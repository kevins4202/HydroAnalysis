index,text
23895,storm surge and coastal flooding predictions can require high resolution of critical flow pathways and barriers typically with simulations using grids meshes with millions of cells elements to represent a coastal region however the cost of this resolution can slow forecasts during a storm to add resolution when and where it is needed previous studies have used adaptive mesh methods which update resolution at single or multiple cells but which require hierarchies of and thresholds for refinement and nesting methods which update resolution at subdomains but which require additional simulations this research proposes a middle way in which predictions from a coarse mesh are mapped mid simulation onto a fine mesh with increased resolution near the storm s projected landfall location the coarse and fine meshes are pre developed thus removing any refinement decisions during the simulation the solution mapping uses a widely used framework thus enabling an efficient interpolation and the same simulation is continued thus eliminating a separate full domain simulation for four historical storms results show efficiency gains of up to 53 percent with minimal accuracy losses relative to a static simulation keywords adcirc inundation hindcasting forecasting unstructured mesh 1 introduction during tropical cyclones and other coastal storms the greatest threat is storm surge the rise of water above the normal astronomical tide in coastal regions with relatively flat floodplains storm surge may lead to intrusion of ocean waters 10 to 20 miles inland conner et al 1957 with devastating effects to infrastructure and ecosystems storm driven coastal flooding can be predicted with numerical models which must represent physical processes and geographical features that influence storm surge from the deeper ocean onto the continental shelf into estuaries and marshes and over low lying coastal floodplains these multi scale processes led to the development of models that can increase resolution of coastal features however there is a continuing challenge to provide high resolution only when and where it is required with the goal of optimizing the efficiency of simulations especially in forecasting applications when model predictions are required to support decision making cheung et al 2003 storm surge and coastal flooding can be predicted with the advanced circulation adcirc modeling system luettich and westerink 2004 westerink et al 2008 which is used for long term planning and design u s army corps of engineers 2018 fema 2021 evaluation of surge mitigation systems interagency performance evaluation task force 2008 and operational forecasting national oceanic atmospheric administration 2021 adcirc uses unstructured finite element meshes in which resolution can vary over 3 to 4 orders of magnitude hope et al 2013 roberts et al 2019 these meshes can be large in their spatial coverage with millions of finite elements to represent one or multiple state coastlines thomas et al 2019 and in their cost requiring several hours even on thousands of computational cores dietrich et al 2012 a significant portion of the cost is due to the inclusion of floodplains and other sub aerial regions which remain dry for most of the simulation and become wet only as the storm makes landfall but which are represented typically by 50 to 90 percent of the total number of finite elements roberts et al 2021 although adcirc meshes can be designed to reduce the number of elements in these floodplains bilskie et al 2020 there is a need to adapt during the simulation to include the floodplains only as they are wetted one possibility is adaptive mesh methods in which meshes are refined dynamically to obtain fine scale solutions in areas of interest e g to follow a tsunami or near landfall of a coastal storm berger et al 2011 mandli and dawson 2014 caviedes voulliéme et al 2020 elements are split or joined either individually or in patches based on gradients in topography or hydrodynamics and thus these methods require a hierarchy of information at varying levels of resolution as well as decisions about how and when to refine or coarsen across that hierarchy kubatko et al 2006 gerhard et al 2015 for structured meshes these methods must overcome the challenges of conservation and well balancing gradation of element sizes and selection of refinement thresholds liang and marche 2009 berger et al 2011 kesserwani and liang 2012 liang et al 2015 hou et al 2018 as well as spurious momentum and smaller time steps caviedes voulliéme et al 2020 for unstructured meshes with triangular elements the methods must also address the challenges of unintentional generation of skewed triangular elements behrens et al 2005 and the structured design of hierarchies for efficient refinement behrens and bader 2009 recent studies have demonstrated successful applications for urban flooding hu et al 2018 and idealized storms and domains beisiegel et al 2020 with speed ups of 70 percent relative to models with static meshes however due partly to the challenges listed above adaptive mesh methods have not yet been applied for realistic storm surge simulations using unstructured meshes another possible adaptive technique is nesting in which a simulation with a fine resolution mesh is forced with results from a separate simulation with a coarser mesh e g for the investigation of tropical cyclones and mid latitude disturbances and in coastal ocean applications ookochi 1972 mathur 1974 hovermale 1976 miyakoda and rosati 1977 oey and chen 1992 for structured meshes nesting has been implemented in one or two directions ways depending on whether information from the fine scale simulation is sent back to the large domain simulation two way nesting has been applied extensively such as in roms agrif penven et al 2006 debreu et al 2008 2012 including for storm surge hindcasts pianezze et al 2020 and forecasts dinapoli et al 2020 for unstructured meshes a one way nesting was tested for two small estuarine systems using an outer large scale coarse mesh and an inner small scale fine mesh taeb and weaver 2019 with run time reductions of 54 to more than 80 and with the solutions showing relatively small deviations from the conventional single domain technique also with unstructured meshes a related technique is subdomain modeling baugh et al 2015 altuntas and baugh 2017 in which a single full domain simulation is used as forcing to repeated simulations on subdomains with local changes e g possible configurations of ground surface and hydraulic barriers to consider design alternatives for a coastal structure these techniques are similar in that they use coarse scale information as forcing to predictions at finer resolution however because the coarse simulation must be performed before or alongside the fine simulation it can increase costs in operations we propose a multi resolution approach to share the advantages of both the adaptive mesh methods and nesting techniques our approach is adaptive in that resolution is increased during the simulation but it does not require a hierarchical mesh refinement and our approach is nested in that results are mapped onto a pre developed higher resolution mesh for the same coastal region but it does not require boundary conditions from a separate full simulation the simulation will start with a mesh without extensive coastal detail while the storm is far away its results will be mapped onto an available mesh with better representation of the coastal region to be affected by the storm and then the simulation will continue with higher resolution predictions of coastal flooding as the storm makes landfall we hypothesize that by switching during a simulation from coarse to fine resolution meshes with the resolution in the fine mesh concentrated only at specific coastal regions influenced by the storm both accuracy and computational gains can be achieved the multi resolution approach is implemented into adcirc for use on its unstructured meshes this approach is most promising for real time forecast applications in the following sections we describe the mechanics of the multi resolution approach and then demonstrate gains in accuracy and efficiency for representative storms 2 methods 2 1 ocean circulation models and technologies adcirc will be used for simulations of storms along the u s southeast and texas coasts starting on a coarse mesh during each simulation a new technology called adcirpolate will allow for regridding of the computed solution from coarse to fine meshes then each simulation will continue on a fine mesh 2 1 1 adcirc to predict coastal flooding we use the advanced circulation adcirc modeling system which has been validated extensively for storms around the world bhaskaran et al 2013 hope et al 2013 suh et al 2015 dietrich et al 2018 adcirc luettich et al 1992 luettich and westerink 2004 westerink et al 2008 is a depth integrated shallow water finite element model capable of simulating tidal circulation and storm surge propagation over large computational domains adcirc is used by the federal emergency management agency fema in the development of flood insurance rate maps federal emergency management agency 2019 by the u s army corps of engineers usace for navigation and storm protection projects u s army corps of engineers 2018 and also by the national oceanic and atmospheric administration noaa for tidal calibrations and incorporation into its vertical datum transformation software vdatum myers et al 2007 in this study the depth averaged barotropic version of adcirc is used because the strong surface stresses during storms cause the water column to be well mixed in shallow nearshore and coastal regions this study will ignore storm induced wave effects i e no coupling with a nearshore wave model for the simulations in this study the adcirc version 54 dev is used in explicit mode with the lumped mass matrix form of the generalized wave continuity equation gwce tanaka et al 2011 a depth dependent quadratic friction law is used to apply bottom drag with the drag coefficient as computed from manning s n values luettich et al 1992 luettich and westerink 2004 the air sea momentum exchange is parameterized as a wind drag garratt 1977 with an upper limit of c d 0 002 similar to other studies dietrich et al 2011 2012 spatially varying attributes are used to represent manning s n values eddy viscosity the primitive weighting in the gwce surface roughness surface canopy elemental slope limiter and advection many of these attributes are derived from land cover data a spatially varying offset surface was also used to account for water level processes on longer time scales like steric and local sea level rise thomas et al 2019 2 1 2 adcirpolate the proposed multi resolution approach uses a technology called adcirpolate samii 2021 to switch simulations between pre developed coarse and fine meshes when a storm is in the open ocean there is uncertainty where it will make landfall at this time a simulation can be started with a mesh with an extensive coverage of the u s coastline but having a relatively coarse resolution of coastal features as the storm approaches the coastline and the landfall location becomes more certain the computed solution is switched to a fine resolution mesh that describes the coastal features in that region in high detail the initiation of the switch is decided by the modeler example criteria for and costs of switching are quantified later in this study the switching technology is implemented via the earth system modeling framework esmf hill et al 2004 and operates on the adcirc hot start files named fort 67 or fort 68 in the adcirc convention which include solution fields for surface elevations depth averaged velocities wet dry states etc at previous and current time steps using the esmf routines the fine destination mesh is masked to identify regions within the coverage of the coarse source mesh e g along the open coast and regions outside the coverage of the coarse source mesh e g inland floodplains then the fields from the coarse source mesh are regridded onto the fine destination mesh for points within the coverage the solution fields are regridded by using bilinear interpolation for points outside the coverage the solution fields are regridded by extrapolation with nearest source to destination i e each destination point is mapped from the closest source point and a source point can be mapped to multiple destination points the regridding is done in parallel and on an arbitrary number of cpus i e it is not confined to the number of cores from either the coarse or fine simulations the resulting hot start file is then used to continue the simulation on the fine mesh the regridding is demonstrated in an example application in the appendix here we note that adcirpolate is not conservative globally or locally the intention is for the fine mesh to include inland water bodies bays estuaries natural and man made channels that are not represented in the coarse mesh and thus the regridding cannot conserve mass and momentum in a global domain wide sense however even for a local single element sense the regridding will not be conservative for instance a channel may be artificially wide in the coarse mesh due to large element sizes but represented at its correct width in the fine mesh or a nearshore bathymetry may be artificially smoothed in the coarse mesh but represented with its submerged ridges and channels in the fine mesh however as we demonstrate in this study the nonconservative mapping does not prevent the follow on simulation from providing accurate predictions of storm surge and coastal flooding 2 1 3 unstructured meshes adcirc uses unstructured finite element meshes to describe the coastal ocean for storm simulations in this study we will start on a coarse mesh called hsofs with coverage of the entire u s coast and then switch to a fine mesh for the coast along either the south atlantic bight sab or texas the coarse mesh will be required for simulations for daily non storm conditions as well as for storms as they develop far from shore this study uses the well validated hurricane surge on demand forecasting system hsofs as the coarse mesh due to its extensive coverage of nearshore regions and coastal floodplains along the u s coast from texas through maine riverside technology aecom 2015 the hsofs mesh has an average resolution of 500 m along the coast with some areas decreasing to 150 m at most locations the mesh extends inland to the 10 m topographic contour it has 1 813 443 vertices and 3 564 104 elements for adcirc simulations on the hsofs mesh the spatially constant horizontal eddy viscosity for the momentum equations was set to 50 m 2 s 1 a time step of 1 s was used and the advective transport terms were enabled to account for nonlinear interactions between surge and tides for storms affecting the coast along the south atlantic bight sab we will switch from hsofs to a new sab mesh this high resolution mesh has detailed coverage from florida through north carolina and was developed by merging five fema regional meshes thomas 2020 the sab mesh has 5 584 241 vertices and 11 066 018 elements its resolution increases to 100 m along the southeastern u s coastline except in a few regions along the carolina coasts the resolution is 10 m in some channels in florida for adcirc simulations on the sab mesh a set of spatially variable parameters was developed using the attributes of the fema regional meshes the advective transport terms were enabled however the sab mesh owing to its high resolution requires a smaller time step of 0 5 s to satisfy the courant friedrichs lewy cfl condition that relates the model time step element size and wave speed luettich et al 1992 for storms affecting the western gulf of mexico we will switch from hsofs to the so called texas mesh which is well validated for storm surge predictions along the texas coast kennedy et al 2011 sebastian et al 2014 it has 3 331 560 vertices and 6 633 623 elements the resolution along the texas coastline is approximately 100 m with increased resolution to 30 m to represent the complexity around the galveston area for adcirc simulations on the texas mesh the time step is 1 s but the advective transport terms are disabled to improve numerical stability 2 2 storms and atmospheric forcing the proposed approach is evaluated with four storms ike 2008 matthew 2016 harvey 2017 and florence 2018 fig 1 these storms are hindcasted by using data assimilated products from oceanweather inc as forcing to adcirc simulations 2 2 1 historical storms ike was a category 4 hurricane on the saffir simpson wind scale made landfall along the upper texas coast with category 2 intensity during september 2008 berg 2009 and pushed surge as high as 4 5 to 6 m on the bolivar peninsula and in parts of texas matthew was a category 5 hurricane that caused widespread impacts all along the u s southeast coast and made landfall with category 1 intensity along the central coast of south carolina during october 2016 stewart 2017 harvey was a category 4 hurricane that made landfall at peak intensity in middle texas coast stalled over southern texas for days and resulted in catastrophic flash and river flooding florence was a category 4 hurricane that made landfall along the southeastern coast of north carolina during september 2018 stewart and berg 2019 and caused significant storm surge flooding in eastern north carolina these storms were selected because of their varied landfall locations tracks and other parameters matthew and florence affected the atlantic coast whereas ike and harvey affected the texas coast matthew s track was shore parallel from florida to north carolina harvey s track started as shore normal but became shore parallel as it stalled over texas and ike s and florence s tracks were shore normal they also had variations in parameters including track orientation to shoreline intensity of winds duration size etc the proposed approach will be tested in these four cases to demonstrate its capability for any storm 2 2 2 atmospheric forcing this study will consider hindcasts of the four storms by using data assimilated surface pressure and wind velocities from oceanweather inc owi these atmospheric products are based on observations from anemometers airborne and land based doppler radar airborne stepped frequency microwave radiometer buoys ships aircraft coastal stations and satellite measurements bunya et al 2010 in situ data sources can be inaccurate at hurricane wind speeds cardone and cox 2009 and also inadequate to resolve the evolution of the critical inner core structure indirect methods using a variety of models are therefore used to describe the evolution of the hurricane wind fields these include simple parametric models holland 1980 steady state dynamical models like the planetary boundary layer pbl model fauver 1970 vickery et al 2000 non steady dynamical methods like noaa s wrf model corbosiero et al 2007 and kinematic methods like the noaa hurricane research wind analysis system h wind powell et al 1996 2010 these methods can also be combined by blending an inner core wind field to a peripheral large scale wind field using the interactive object kinematic analysis ioka system cox et al 1995 cardone and cox 2009 these fields have been used in adcirc hindcasts of katrina dietrich et al 2010 ike hope et al 2013 kennedy et al 2011 gustav dietrich et al 2011 and other storms compared to parametric vortex models like the generalized asymmetric holland model gahm gao et al 2017 used during forecasting owi fields have been shown to be the most realistic representation of the atmospheric forcing during storms thomas et al 2019 depending on the storm the owi fields can be presented with a lower resolution basin wide grid and nested higher resolution regional grids for ike the basin grid covers from 17 93 n to 30 73 n and from 98 03 w to 60 03 w with a spatial resolution of 0 1 whereas the higher resolution region field covers from 28 43 n to 30 185 n and from 96 03 w to 93 03 w with a spatial resolution of 0 015 both covering a period from 1200 utc 05 september 2008 until 0600 utc 14 september 2008 at 15 min intervals for matthew the basin grid covers from 5 n to 47 n and from 99 w to 55 w with a spatial resolution of 0 25 whereas the higher resolution region field covers from 15 n to 40 n and from 82 w to 68 w with a spatial resolution of 0 05 both covering a period from 0000 utc 01 october 2016 until 0000 utc 11 october 2016 at 15 min intervals for harvey the basin grid covers from 5 n to 47 n and from 99 w to 55 w with a spatial resolution of 0 25 whereas the higher resolution region field covers from 18 n to 30 96 n and from 98 w to 80 w with a spatial resolution of 0 08 both covering a period from 1200 utc 13 august 2017 until 0000 utc 15 september 2017 at 15 min intervals for florence the basin grid covers from 5 n to 47 n and from 99 w to 55 w with a spatial resolution of 0 20 whereas the higher resolution region field covers from 31 n to 37 n and from 82 w to 74 w with a spatial resolution of 0 05 both covering a period from 0000 utc 07 september 2018 until 0000 utc 18 september 2018 at 15 min intervals for all storms the surface wind and pressure fields are interpolated in time and space onto the adcirc unstructured meshes 2 3 simulations and analyses for each storm we consider three simulations coarse mixed and fine for the mixed simulations to be beneficial we must identify a parameter to switch late enough to optimize efficiency on the coarse mesh but early enough to optimize accuracy on the fine mesh these optimizations will be evaluated by comparing the computed solutions and via wall clock times 2 3 1 switching parameters for the coarse simulations the hsofs mesh is used for the entire storm and for the fine simulations either the sab or texas mesh is used for the entire storm for the mixed simulations the hsofs mesh is used when the storm is away from a coastline and its path is uncertain but then we switch to the fine mesh for that region as the storm starts to affect water levels along the coast the switching times for the mixed simulations were determined from observed time series of water levels the idea is to identify the time at which the total water levels become influenced by the storm thomas 2020 an ideal switching time will result in near zero differences in water levels overall and thus a minimal loss in accuracy while occurring as late as possible during the storm and thus a maximum gain in efficiency the simulation durations and the switching times for the mixed simulations for all four storms are shown in table 1 for ike switching was done when the eye of the storm was located about 500 km south of dauphin island al at this time the water levels along the tx coastline were 0 70 m at the entrance of galveston bay 0 70 m at corpus christi and 0 54 m at port isabel for matthew switching was done when the storm was located 500 km south of nassau bahamas when the water levels were 0 01 m at vaca key 0 11 m at virginia key and 0 10 m at lake worth pier fl for harvey switching was done when the storm was located 500 km south of galveston bay tx water levels at various points along the tx coastline were 0 40 m at sabine pass 0 54 m at the entrance to the galveston bay 0 39 m at corpus christi and 0 32 m at port isabel for florence switching was done when about the storm eye was located about 477 km off the hatteras inlet nc the water levels at this time were 0 46 m at wrightsville beach 0 26 m at beaufort and 0 25 m at hatteras 2 3 2 error statistics the agreement between observations and predicted results is quantified by using error metrics of root mean squared error e r m s and mean normalized bias b m n thomas et al 2019 the e r m s indicates magnitude of the error and has an ideal value of zero whereas the b m n is a measure of the model s over or under prediction normalized to the observed value and also has an ideal value of zero a positive b m n indicates over prediction while a negative value indicates under prediction by the model the predicted and observed peak water levels are also analyzed with the coefficient of determination r 2 and best fit slope m the r 2 is a statistical measure of how close the data is to the linear regression model and has an ideal value of unity the best fit slope is the slope of the line that best represents the relationship between observed and predicted data and also has an ideal value of unity the accuracy of the approach is also evaluated by comparing the total volume of inundation to that from the coarse and fine simulations for an element this volume is equal to the area of the element multiplied by the average depth of water height of water above the ground surface in the three vertices an element contributes to the total volume only if all the three vertices 1 have ground surface elevations above mean sea level 2 are located in the affected area of the storm and 3 were flooded during the simulation it is noted that because the ground surface is represented at differing resolutions in these simulations the volume of inundation will be affected by the resolution the efficiency of the approach is evaluated by comparing the wall clock time of simulations with and without switching all simulations were completed on the stampede2 computing cluster at the texas advanced computing center using 522 computational cores for the approach a total of the times required for the coarse part of the simulation adcirpolate and the fine part is taken for comparison to avoid any inconsistencies with run times due to hardware competing jobs or network traffic each simulation is run three times on the same number of cores and the minimum of the three run times is then used for comparisons however in operations there is no restriction on the numbers of cores used for the coarse or fine simulations or adcirpolate 3 results and discussion in this study we hindcast four storms each having different parameters like track intensity flooding extent etc for each storm we perform three simulations coarse fine and mixed the results from these simulations are then analyzed to quantify the benefits of the proposed approach 3 1 accuracy benefits 3 1 1 comparisons of predicted flooding extents to examine the effects of higher resolution on predictions of overland flooding difference maps of maximum water levels between the coarse mixed and fine simulations are plotted for all four storms fig 2 differences are shown at the fine mesh resolution by mapping the coarse and mixed results to the fine mesh overall there are significant differences between the coarse and fine maximum water levels for all four storms and these are attributed to the difference in mesh geometry between the coarse and fine meshes for matthew and florence these differences mainly occur inland with near zero differences nearshore and in the open ocean for ike and harvey larger differences are both nearshore and inland with larger magnitude near the landfall location for ike the fine maximum water levels exceeded the coarse results by as much as 1 25 m in the rivers bays and lakes located north of galveston bay texas these large differences were likely due to how the bottom friction is parameterized on the louisiana texas meshes in both meshes with the texas mesh using a lower friction that enabled predictions of the ike forerunner kennedy et al 2011 for all storms the mixed and fine maximum water levels were a good match as evident from the near zero differences in the open ocean nearshore and inland regions differences in the open ocean and nearshore regions were small less than 0 15 m and related to the time of switching in the mixed simulation and differences in inland regions are due to the difference in geometry between the coarse and fine meshes harvey produced maximum inundation levels of 2 5 to 3 m to the north and east of its two landfalls in texas in the backbays between port aransas and matagorda including the copano bay and lavacaa bay texas fig 2 third row left column compared to the coarse results the fine maximum water levels are higher along the coast and inland along the coast these differences are small in the range of 0 1 to 0 2 m the differences inland were 0 15 m in corpus christi bay 0 2 m in san antonio bay 0 35 m in matagorda bay and 0 35 to 0 9 m in lavaca bay these differences are mainly related to the difference in the ground surface representation between the texas mesh and the hsofs mesh the mixed results are a good match to the fine results as evident from the zero differences along the texas coastline the lone exception is a location south of baffin bay away from the landfall location fig 2 third row right column where the mixed maximum water levels are higher than the corresponding fine values by more than 30 cm these differences are caused by differences in how the ground surface is represented between the hsofs and texas meshes and thus independent of the switching process for florence as compared to the coarse results the fine water levels are higher in regions like the albemarle sound atlantic intracoastal waterway core sound currituck sound and upstream all major rivers fig 2 last row middle column these differences were 0 1 to 0 2 m in the core sound 0 2 to 0 6 m upstream of the neuse river 0 1 to 0 2 m in the currituck sound and 0 2 to 1 m up stream of the pamlico river this is attributed to the higher resolution in the fine mesh that better represents bathymetry and in turn a better hydraulic connectivity for water to flow into these complex regions in the coarse mesh the water is not able to flow into the rivers and instead is stuck at downstream locations downstream of the neuse river the coarse water levels were higher by as much as 0 35 m there are almost zero differences in the open ocean along the coast and in the pamlico sound the differences between the mixed and fine results are almost zero fig 2 last row right column as switching has happened well before the storm impacted the nc coast small differences exist in the northeast region of the domain far away from the storm s impact these differences are contributed by the coarse part of the mixed simulation due to the large difference in resolution between the coarse and fine meshes 3 1 2 localized benefits it is expected that the fine mesh resolution will have its largest benefits at inland locations because it will better represent the connectivity of storm surge from the open coast and through the rivers and other channels predicted water levels are compared at inland locations during matthew and ike to highlight the benefits of the approach in matching fine results even at locations far from the coastline for matthew water levels are compared at 3 stations along the savannah river on the ga sc border fig 3 stations 1 and 2 are located upriver where the resolution is insufficient in the coarse mesh as shown by the poor simulation of the tidal signal at these locations before the switch but after the switch the mixed and fine results match well at station 3 closer to the open coast both meshes have sufficient resolution of the storm surge at this location and hence the coarse and mixed results are a good match before and after the switch a similar analysis is made at three stations located north of the trinity and galveston bays in texas during ike fig 4 stations 1 and 2 are located where the coarse mesh does not possess sufficient resolution to represent the san jacinto river therefore the coarse part of the mixed results stays dry but after the switch the mixed water levels match well to the fine results at station 3 the coarse mesh has sufficient resolution to represent the storm surge but the results are slightly different from the fine simulation as the fine mesh has a much higher resolution and thus it predicts flooding better at all three locations the mixed results match the fine results after the switch thus the approach improves predictions of water levels at inland stations where the fine mesh has a better representation of the flow pathways 3 1 3 quantifying accuracy the accuracy of the coarse and mixed results are analyzed by comparing them to the fine solution which is taken as the truth table 2 this allows for an evaluation of accuracy throughout the entire region not only where observations were collected for this comparison the coarse results are mapped onto the fine mesh as a post processing step so comparisons can be made at the same resolution for the mixed simulation the maxima of water levels from all parts of the simulation are considered results are compared only at vertices in the affected area of the storm fl to nc for matthew nc for florence tx for harvey and ike that are not in open ocean depths less than 10 m and that were wetted in both simulations for all four storms the mixed results have a b m n either equal to or close to zero whereas the coarse results have a negative value indicating an overall under prediction by the model the mixed results also have a best fit line slope equal to or close to unity indicating a good match to the fine results the e r m s closer to zero and r 2 closer to unity are also better but the most interesting difference lies in the number of fine mesh vertices that were used for comparison the mixed simulations have comparisons made at much higher numbers of vertices about 218k more vertices for ike 680k for matthew 209k for harvey and 82k for florence indicating much more overland flooding as compared to the coarse simulation this considerably larger number of comparison points during matthew is due to the larger region of analysis fl to nc as compared to individual states for other storms matthew was a shore parallel storm that had effects from fl to nc fig 2 second row left column the advantage of using the mixed approach is evident at points that were dry during the coarse simulation but were wetted in the mixed simulation fig 5 these additional wetted vertices are located along the wetting drying regions like barrier islands sounds as well as upstream rivers where the coarse mesh does not have sufficient resolution these trends in the difference in flooding extent between the coarse and mixed simulations are supported by the total volume of inundation table 3 for all storms the mixed and fine simulations have comparable values thus proving the effectiveness of the approach in matching the fine simulation flooding extent the coarse simulation on the other hand has a much lesser total volume as it floods lesser number of elements and lacks the flooding extents of both the mixed and fine simulations the total volume of inundation for the coarse simulation was less than that for the corresponding fine simulation by 4 17 1 0 9 m 3 for ike 1 61 1 0 9 m 3 for matthew 0 58 1 0 9 m 3 for harvey and 0 66 1 0 9 m 3 for florence however the total volume of inundation for the fine simulation exceeded the value for the corresponding mixed simulation by only 0 73 1 0 9 m 3 for ike 0 06 1 0 9 m 3 for matthew 0 15 1 0 9 m 3 for harvey and 0 04 1 0 9 m 3 for florence the difference in the volume of inundation between the mixed and fine simulations is due to how the coastal region is represented in the coarse mesh segment of the mixed simulation on the fine mesh the peak surge during ike was higher than 5 m along the texas coastline east of galveston bay fig 2 top row left column as this location experienced high shore parallel winds at the time of landfall and afterward this high peak surge led to the largest volume of inundation compared to the other three storms table 3 but even for this high surge the volume of inundation from the mixed and fine simulations differed by only 0 73 1 0 9 m 3 the corresponding difference between the coarse and fine simulations was 4 17 1 0 9 m 3 thus compared to the coarse results the mixed approach allows for a more accurate flooding as well as a much larger flooding extent which can be crucial during forecasting 3 2 efficiency benefits for all four storms the total wall clock time required for the mixed simulation is compared with that for the corresponding fine simulation table 4 there is a wide variety in the wall clock times depending on the size of each mesh by total number of vertices elements and the number of simulation days on each mesh for ike and harvey the fine simulations required 102 to 103 min which was reduced by 3 to 19 percent with the mixed simulations for matthew and florence the fine simulations required 380 to 393 min which was reduced by 38 to 53 percent in the fine simulations adcirpolate required 8 to 12 min or less than 10 percent of each mixed simulation the multi resolution approach had its maximum efficiency during florence with a run time decrease of 53 percent this efficiency is due to florence s shore normal track which allowed switching after 6 days of simulation when the storm first affected the nc coast thus the coarse mesh was used for most of the mixed simulation resulting in a lesser run time there was also a large gain in efficiency during matthew with a run time decrease of 38 percent because matthew had a shore parallel track it affected a larger geographical extent for several days and thus it required the fine mesh to be used for more of its mixed simulation the relative sizes of the coarse and fine meshes by number of vertices elements also affected the efficiency of the approach for storms on the atlantic coast the fine fema sab mesh is more than three times larger than the coarse hsofs mesh whereas for storms on the texas coast the fine texas mesh is about 1 8 times larger than the coarse hsofs mesh because of this there is a smaller potential for efficiency gains for storms on the texas coast for example during harvey the mixed approach required 99 min compared to the fine simulation that required 103 min thus the run time decrease was only 4 percent but overall the multi resolution approach does have an efficiency gain for all storms which is crucial especially in forecasting applications because it can enable ensemble simulations to account for uncertainties in storm parameters 4 conclusions high fidelity predictions of coastal flooding require high resolution of small scale flow pathways and barriers in coastal regions this high resolution can be computationally costly a multi resolution approach was implemented in the adcirc modeling system this approach allows the use of high resolution meshes only when it is required and without re starting the simulation from scratch the approach was evaluated for simulations of four storms that affected the u s coast in different regions the benefits of the approach were evaluated in terms of accuracy and efficiency by comparisons to single simulations on coarse and fine resolution meshes the major findings of this study are 1 if the simulation is switched at an acceptable time during the storm then the flooding predictions will be similar to a full simulation on the higher resolution mesh for all storms the mixed flooding predictions were similar to the fine flooding predictions however they allowed flooding of a larger region as compared to the corresponding coarse simulation this extra flooding coverage was at regions like barrier islands and upstream rivers where the coarse mesh did not have sufficient resolution to provide the required hydraulic connectivity for flooding to occur at inland stations the water levels reacted quickly to the switch onto the fine mesh 2 the multi resolution approach can preserve the accuracy of high resolution predictions of coastal flooding for all for storms the mixed results had a comparable accuracy to the fine results the r 2 and best fit slopes were close to unity the e r m s values were less than 0 18 m and the b m n values were close to zero 3 the multi resolution approach can enhance significantly the efficiency of high resolution predictions of coastal flooding the mixed simulations were more efficient than the fine simulations as measured by wall clock times the efficiency gains were between 3 to 53 percent depending on the relative sizes and simulation durations on the meshes adcirpolate required less than 10 percent of the overall wall clock time the multi resolution approach is most promising for forecasting applications in which the efficiency gains can translate directly into additional time for decision making future work will explore which conditions coastal water levels wind speeds etc can be used to trigger the switch and what is an optimal balance between accuracy losses and efficiency gains credit authorship contribution statement ajimon thomas validation visualization writing original draft j c dietrich supervision project administration writing review editing m loveland investigation validation a samii data curation methodology software c n dawson conceptualization funding acquisition software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this material is based upon work supported by the u s department of homeland security under grant award number 2015 st 061 nd0001 01 the views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies either expressed or implied of the u s department of homeland security this work was also supported by the national science foundation united states of america grant enh 1635784 appendix example of adcirpolate in coastal north carolina to illustrate how adcirpolate can be used to switch meshes during a simulation we provide a small example for coastal north carolina fig 6 beaufort inlet is a barrier island inlet located west of cape lookout and leads south to the atlantic ocean the domain also includes the tidal newport river that flows into the bogue sound and core creek that connects to adams creek a tributary of the neuse river due to the switching with adcirpolate onto a higher resolution mesh the simulation will better represent the flow of surge farther up the newport river meshes with coarse and fine resolution were created by cutting the region from the hsofs and sab meshes by using the surface water modeling system sms https www aquaveo com software sms surface water modeling system introduction the coarse meshhas 3277 vertices with resolution varying from 1377 m at the ocean boundary on the south of the domain to about 550 m at the inlet to 315 m along the core creek and to 350 m at the west end point of the newport river the fine mesh has 22 375 vertices with element spacing varying from 888 to 2500 m at the ocean boundary to 120 m at the inlet to 85 m along the core creek and to 25 m at the west end point of the newport river the difference in resolution causes features to be represented differently between the two meshes fig 7 the bathymetry of the newport river extends about 20 km farther west in the fine mesh the coarse mesh has 2 to 3 elements across the width of the beaufort inlet compared to 10 elements in the fine mesh a tidal signal of period 3 hr and amplitude 1 2 m was added to a surge signal of peak amplitude 2 m and applied as forcing fig 8 top left on the bottom south boundary of the two meshes the total duration was 54 hr the simulation started on the coarse mesh but after 24 hr when the water level on the ocean boundary reached 1 4 m it was switched to the fine mesh for the remainder of the run before switching at the end of the simulation on the coarse mesh fig 7 bottom left the water levels were 1 4 m at the open coast 0 35 to 0 5 m in the inlet and 1 m at the west boundary of the newport river after switching at the start of the simulation on the fine mesh fig 7 bottom right these water levels were mapped to the true coastline inlet and sound and extended into the newport river water levels were analyzed at three stations 1 open coast 2 inlet and 3 channel with locations in fig 7 and for the fine and mixed simulations at the open coast station 1 there was no difference between the simulations because both meshes had a sufficient resolution in open water to represent the combined tide and surge forcing at the inlet station 2 there are differences between the mixed and fine water levels before switching due to differences in geometry between the coarse and fine meshes but after switching the water levels match exactly at the channel station 3 the mixed simulation stays dry for the first 24 hr because the coarse mesh does not have sufficient resolution to represent the fine channel however very shortly after the switch the water levels in mixed increased to match the fine results this was an increase of 0 75 m in just a couple of hours with no oscillations or instabilities in the computed solution thus even when the coarse mesh had locations that were dry the mixed simulation was able to catch up to the fine results 
23895,storm surge and coastal flooding predictions can require high resolution of critical flow pathways and barriers typically with simulations using grids meshes with millions of cells elements to represent a coastal region however the cost of this resolution can slow forecasts during a storm to add resolution when and where it is needed previous studies have used adaptive mesh methods which update resolution at single or multiple cells but which require hierarchies of and thresholds for refinement and nesting methods which update resolution at subdomains but which require additional simulations this research proposes a middle way in which predictions from a coarse mesh are mapped mid simulation onto a fine mesh with increased resolution near the storm s projected landfall location the coarse and fine meshes are pre developed thus removing any refinement decisions during the simulation the solution mapping uses a widely used framework thus enabling an efficient interpolation and the same simulation is continued thus eliminating a separate full domain simulation for four historical storms results show efficiency gains of up to 53 percent with minimal accuracy losses relative to a static simulation keywords adcirc inundation hindcasting forecasting unstructured mesh 1 introduction during tropical cyclones and other coastal storms the greatest threat is storm surge the rise of water above the normal astronomical tide in coastal regions with relatively flat floodplains storm surge may lead to intrusion of ocean waters 10 to 20 miles inland conner et al 1957 with devastating effects to infrastructure and ecosystems storm driven coastal flooding can be predicted with numerical models which must represent physical processes and geographical features that influence storm surge from the deeper ocean onto the continental shelf into estuaries and marshes and over low lying coastal floodplains these multi scale processes led to the development of models that can increase resolution of coastal features however there is a continuing challenge to provide high resolution only when and where it is required with the goal of optimizing the efficiency of simulations especially in forecasting applications when model predictions are required to support decision making cheung et al 2003 storm surge and coastal flooding can be predicted with the advanced circulation adcirc modeling system luettich and westerink 2004 westerink et al 2008 which is used for long term planning and design u s army corps of engineers 2018 fema 2021 evaluation of surge mitigation systems interagency performance evaluation task force 2008 and operational forecasting national oceanic atmospheric administration 2021 adcirc uses unstructured finite element meshes in which resolution can vary over 3 to 4 orders of magnitude hope et al 2013 roberts et al 2019 these meshes can be large in their spatial coverage with millions of finite elements to represent one or multiple state coastlines thomas et al 2019 and in their cost requiring several hours even on thousands of computational cores dietrich et al 2012 a significant portion of the cost is due to the inclusion of floodplains and other sub aerial regions which remain dry for most of the simulation and become wet only as the storm makes landfall but which are represented typically by 50 to 90 percent of the total number of finite elements roberts et al 2021 although adcirc meshes can be designed to reduce the number of elements in these floodplains bilskie et al 2020 there is a need to adapt during the simulation to include the floodplains only as they are wetted one possibility is adaptive mesh methods in which meshes are refined dynamically to obtain fine scale solutions in areas of interest e g to follow a tsunami or near landfall of a coastal storm berger et al 2011 mandli and dawson 2014 caviedes voulliéme et al 2020 elements are split or joined either individually or in patches based on gradients in topography or hydrodynamics and thus these methods require a hierarchy of information at varying levels of resolution as well as decisions about how and when to refine or coarsen across that hierarchy kubatko et al 2006 gerhard et al 2015 for structured meshes these methods must overcome the challenges of conservation and well balancing gradation of element sizes and selection of refinement thresholds liang and marche 2009 berger et al 2011 kesserwani and liang 2012 liang et al 2015 hou et al 2018 as well as spurious momentum and smaller time steps caviedes voulliéme et al 2020 for unstructured meshes with triangular elements the methods must also address the challenges of unintentional generation of skewed triangular elements behrens et al 2005 and the structured design of hierarchies for efficient refinement behrens and bader 2009 recent studies have demonstrated successful applications for urban flooding hu et al 2018 and idealized storms and domains beisiegel et al 2020 with speed ups of 70 percent relative to models with static meshes however due partly to the challenges listed above adaptive mesh methods have not yet been applied for realistic storm surge simulations using unstructured meshes another possible adaptive technique is nesting in which a simulation with a fine resolution mesh is forced with results from a separate simulation with a coarser mesh e g for the investigation of tropical cyclones and mid latitude disturbances and in coastal ocean applications ookochi 1972 mathur 1974 hovermale 1976 miyakoda and rosati 1977 oey and chen 1992 for structured meshes nesting has been implemented in one or two directions ways depending on whether information from the fine scale simulation is sent back to the large domain simulation two way nesting has been applied extensively such as in roms agrif penven et al 2006 debreu et al 2008 2012 including for storm surge hindcasts pianezze et al 2020 and forecasts dinapoli et al 2020 for unstructured meshes a one way nesting was tested for two small estuarine systems using an outer large scale coarse mesh and an inner small scale fine mesh taeb and weaver 2019 with run time reductions of 54 to more than 80 and with the solutions showing relatively small deviations from the conventional single domain technique also with unstructured meshes a related technique is subdomain modeling baugh et al 2015 altuntas and baugh 2017 in which a single full domain simulation is used as forcing to repeated simulations on subdomains with local changes e g possible configurations of ground surface and hydraulic barriers to consider design alternatives for a coastal structure these techniques are similar in that they use coarse scale information as forcing to predictions at finer resolution however because the coarse simulation must be performed before or alongside the fine simulation it can increase costs in operations we propose a multi resolution approach to share the advantages of both the adaptive mesh methods and nesting techniques our approach is adaptive in that resolution is increased during the simulation but it does not require a hierarchical mesh refinement and our approach is nested in that results are mapped onto a pre developed higher resolution mesh for the same coastal region but it does not require boundary conditions from a separate full simulation the simulation will start with a mesh without extensive coastal detail while the storm is far away its results will be mapped onto an available mesh with better representation of the coastal region to be affected by the storm and then the simulation will continue with higher resolution predictions of coastal flooding as the storm makes landfall we hypothesize that by switching during a simulation from coarse to fine resolution meshes with the resolution in the fine mesh concentrated only at specific coastal regions influenced by the storm both accuracy and computational gains can be achieved the multi resolution approach is implemented into adcirc for use on its unstructured meshes this approach is most promising for real time forecast applications in the following sections we describe the mechanics of the multi resolution approach and then demonstrate gains in accuracy and efficiency for representative storms 2 methods 2 1 ocean circulation models and technologies adcirc will be used for simulations of storms along the u s southeast and texas coasts starting on a coarse mesh during each simulation a new technology called adcirpolate will allow for regridding of the computed solution from coarse to fine meshes then each simulation will continue on a fine mesh 2 1 1 adcirc to predict coastal flooding we use the advanced circulation adcirc modeling system which has been validated extensively for storms around the world bhaskaran et al 2013 hope et al 2013 suh et al 2015 dietrich et al 2018 adcirc luettich et al 1992 luettich and westerink 2004 westerink et al 2008 is a depth integrated shallow water finite element model capable of simulating tidal circulation and storm surge propagation over large computational domains adcirc is used by the federal emergency management agency fema in the development of flood insurance rate maps federal emergency management agency 2019 by the u s army corps of engineers usace for navigation and storm protection projects u s army corps of engineers 2018 and also by the national oceanic and atmospheric administration noaa for tidal calibrations and incorporation into its vertical datum transformation software vdatum myers et al 2007 in this study the depth averaged barotropic version of adcirc is used because the strong surface stresses during storms cause the water column to be well mixed in shallow nearshore and coastal regions this study will ignore storm induced wave effects i e no coupling with a nearshore wave model for the simulations in this study the adcirc version 54 dev is used in explicit mode with the lumped mass matrix form of the generalized wave continuity equation gwce tanaka et al 2011 a depth dependent quadratic friction law is used to apply bottom drag with the drag coefficient as computed from manning s n values luettich et al 1992 luettich and westerink 2004 the air sea momentum exchange is parameterized as a wind drag garratt 1977 with an upper limit of c d 0 002 similar to other studies dietrich et al 2011 2012 spatially varying attributes are used to represent manning s n values eddy viscosity the primitive weighting in the gwce surface roughness surface canopy elemental slope limiter and advection many of these attributes are derived from land cover data a spatially varying offset surface was also used to account for water level processes on longer time scales like steric and local sea level rise thomas et al 2019 2 1 2 adcirpolate the proposed multi resolution approach uses a technology called adcirpolate samii 2021 to switch simulations between pre developed coarse and fine meshes when a storm is in the open ocean there is uncertainty where it will make landfall at this time a simulation can be started with a mesh with an extensive coverage of the u s coastline but having a relatively coarse resolution of coastal features as the storm approaches the coastline and the landfall location becomes more certain the computed solution is switched to a fine resolution mesh that describes the coastal features in that region in high detail the initiation of the switch is decided by the modeler example criteria for and costs of switching are quantified later in this study the switching technology is implemented via the earth system modeling framework esmf hill et al 2004 and operates on the adcirc hot start files named fort 67 or fort 68 in the adcirc convention which include solution fields for surface elevations depth averaged velocities wet dry states etc at previous and current time steps using the esmf routines the fine destination mesh is masked to identify regions within the coverage of the coarse source mesh e g along the open coast and regions outside the coverage of the coarse source mesh e g inland floodplains then the fields from the coarse source mesh are regridded onto the fine destination mesh for points within the coverage the solution fields are regridded by using bilinear interpolation for points outside the coverage the solution fields are regridded by extrapolation with nearest source to destination i e each destination point is mapped from the closest source point and a source point can be mapped to multiple destination points the regridding is done in parallel and on an arbitrary number of cpus i e it is not confined to the number of cores from either the coarse or fine simulations the resulting hot start file is then used to continue the simulation on the fine mesh the regridding is demonstrated in an example application in the appendix here we note that adcirpolate is not conservative globally or locally the intention is for the fine mesh to include inland water bodies bays estuaries natural and man made channels that are not represented in the coarse mesh and thus the regridding cannot conserve mass and momentum in a global domain wide sense however even for a local single element sense the regridding will not be conservative for instance a channel may be artificially wide in the coarse mesh due to large element sizes but represented at its correct width in the fine mesh or a nearshore bathymetry may be artificially smoothed in the coarse mesh but represented with its submerged ridges and channels in the fine mesh however as we demonstrate in this study the nonconservative mapping does not prevent the follow on simulation from providing accurate predictions of storm surge and coastal flooding 2 1 3 unstructured meshes adcirc uses unstructured finite element meshes to describe the coastal ocean for storm simulations in this study we will start on a coarse mesh called hsofs with coverage of the entire u s coast and then switch to a fine mesh for the coast along either the south atlantic bight sab or texas the coarse mesh will be required for simulations for daily non storm conditions as well as for storms as they develop far from shore this study uses the well validated hurricane surge on demand forecasting system hsofs as the coarse mesh due to its extensive coverage of nearshore regions and coastal floodplains along the u s coast from texas through maine riverside technology aecom 2015 the hsofs mesh has an average resolution of 500 m along the coast with some areas decreasing to 150 m at most locations the mesh extends inland to the 10 m topographic contour it has 1 813 443 vertices and 3 564 104 elements for adcirc simulations on the hsofs mesh the spatially constant horizontal eddy viscosity for the momentum equations was set to 50 m 2 s 1 a time step of 1 s was used and the advective transport terms were enabled to account for nonlinear interactions between surge and tides for storms affecting the coast along the south atlantic bight sab we will switch from hsofs to a new sab mesh this high resolution mesh has detailed coverage from florida through north carolina and was developed by merging five fema regional meshes thomas 2020 the sab mesh has 5 584 241 vertices and 11 066 018 elements its resolution increases to 100 m along the southeastern u s coastline except in a few regions along the carolina coasts the resolution is 10 m in some channels in florida for adcirc simulations on the sab mesh a set of spatially variable parameters was developed using the attributes of the fema regional meshes the advective transport terms were enabled however the sab mesh owing to its high resolution requires a smaller time step of 0 5 s to satisfy the courant friedrichs lewy cfl condition that relates the model time step element size and wave speed luettich et al 1992 for storms affecting the western gulf of mexico we will switch from hsofs to the so called texas mesh which is well validated for storm surge predictions along the texas coast kennedy et al 2011 sebastian et al 2014 it has 3 331 560 vertices and 6 633 623 elements the resolution along the texas coastline is approximately 100 m with increased resolution to 30 m to represent the complexity around the galveston area for adcirc simulations on the texas mesh the time step is 1 s but the advective transport terms are disabled to improve numerical stability 2 2 storms and atmospheric forcing the proposed approach is evaluated with four storms ike 2008 matthew 2016 harvey 2017 and florence 2018 fig 1 these storms are hindcasted by using data assimilated products from oceanweather inc as forcing to adcirc simulations 2 2 1 historical storms ike was a category 4 hurricane on the saffir simpson wind scale made landfall along the upper texas coast with category 2 intensity during september 2008 berg 2009 and pushed surge as high as 4 5 to 6 m on the bolivar peninsula and in parts of texas matthew was a category 5 hurricane that caused widespread impacts all along the u s southeast coast and made landfall with category 1 intensity along the central coast of south carolina during october 2016 stewart 2017 harvey was a category 4 hurricane that made landfall at peak intensity in middle texas coast stalled over southern texas for days and resulted in catastrophic flash and river flooding florence was a category 4 hurricane that made landfall along the southeastern coast of north carolina during september 2018 stewart and berg 2019 and caused significant storm surge flooding in eastern north carolina these storms were selected because of their varied landfall locations tracks and other parameters matthew and florence affected the atlantic coast whereas ike and harvey affected the texas coast matthew s track was shore parallel from florida to north carolina harvey s track started as shore normal but became shore parallel as it stalled over texas and ike s and florence s tracks were shore normal they also had variations in parameters including track orientation to shoreline intensity of winds duration size etc the proposed approach will be tested in these four cases to demonstrate its capability for any storm 2 2 2 atmospheric forcing this study will consider hindcasts of the four storms by using data assimilated surface pressure and wind velocities from oceanweather inc owi these atmospheric products are based on observations from anemometers airborne and land based doppler radar airborne stepped frequency microwave radiometer buoys ships aircraft coastal stations and satellite measurements bunya et al 2010 in situ data sources can be inaccurate at hurricane wind speeds cardone and cox 2009 and also inadequate to resolve the evolution of the critical inner core structure indirect methods using a variety of models are therefore used to describe the evolution of the hurricane wind fields these include simple parametric models holland 1980 steady state dynamical models like the planetary boundary layer pbl model fauver 1970 vickery et al 2000 non steady dynamical methods like noaa s wrf model corbosiero et al 2007 and kinematic methods like the noaa hurricane research wind analysis system h wind powell et al 1996 2010 these methods can also be combined by blending an inner core wind field to a peripheral large scale wind field using the interactive object kinematic analysis ioka system cox et al 1995 cardone and cox 2009 these fields have been used in adcirc hindcasts of katrina dietrich et al 2010 ike hope et al 2013 kennedy et al 2011 gustav dietrich et al 2011 and other storms compared to parametric vortex models like the generalized asymmetric holland model gahm gao et al 2017 used during forecasting owi fields have been shown to be the most realistic representation of the atmospheric forcing during storms thomas et al 2019 depending on the storm the owi fields can be presented with a lower resolution basin wide grid and nested higher resolution regional grids for ike the basin grid covers from 17 93 n to 30 73 n and from 98 03 w to 60 03 w with a spatial resolution of 0 1 whereas the higher resolution region field covers from 28 43 n to 30 185 n and from 96 03 w to 93 03 w with a spatial resolution of 0 015 both covering a period from 1200 utc 05 september 2008 until 0600 utc 14 september 2008 at 15 min intervals for matthew the basin grid covers from 5 n to 47 n and from 99 w to 55 w with a spatial resolution of 0 25 whereas the higher resolution region field covers from 15 n to 40 n and from 82 w to 68 w with a spatial resolution of 0 05 both covering a period from 0000 utc 01 october 2016 until 0000 utc 11 october 2016 at 15 min intervals for harvey the basin grid covers from 5 n to 47 n and from 99 w to 55 w with a spatial resolution of 0 25 whereas the higher resolution region field covers from 18 n to 30 96 n and from 98 w to 80 w with a spatial resolution of 0 08 both covering a period from 1200 utc 13 august 2017 until 0000 utc 15 september 2017 at 15 min intervals for florence the basin grid covers from 5 n to 47 n and from 99 w to 55 w with a spatial resolution of 0 20 whereas the higher resolution region field covers from 31 n to 37 n and from 82 w to 74 w with a spatial resolution of 0 05 both covering a period from 0000 utc 07 september 2018 until 0000 utc 18 september 2018 at 15 min intervals for all storms the surface wind and pressure fields are interpolated in time and space onto the adcirc unstructured meshes 2 3 simulations and analyses for each storm we consider three simulations coarse mixed and fine for the mixed simulations to be beneficial we must identify a parameter to switch late enough to optimize efficiency on the coarse mesh but early enough to optimize accuracy on the fine mesh these optimizations will be evaluated by comparing the computed solutions and via wall clock times 2 3 1 switching parameters for the coarse simulations the hsofs mesh is used for the entire storm and for the fine simulations either the sab or texas mesh is used for the entire storm for the mixed simulations the hsofs mesh is used when the storm is away from a coastline and its path is uncertain but then we switch to the fine mesh for that region as the storm starts to affect water levels along the coast the switching times for the mixed simulations were determined from observed time series of water levels the idea is to identify the time at which the total water levels become influenced by the storm thomas 2020 an ideal switching time will result in near zero differences in water levels overall and thus a minimal loss in accuracy while occurring as late as possible during the storm and thus a maximum gain in efficiency the simulation durations and the switching times for the mixed simulations for all four storms are shown in table 1 for ike switching was done when the eye of the storm was located about 500 km south of dauphin island al at this time the water levels along the tx coastline were 0 70 m at the entrance of galveston bay 0 70 m at corpus christi and 0 54 m at port isabel for matthew switching was done when the storm was located 500 km south of nassau bahamas when the water levels were 0 01 m at vaca key 0 11 m at virginia key and 0 10 m at lake worth pier fl for harvey switching was done when the storm was located 500 km south of galveston bay tx water levels at various points along the tx coastline were 0 40 m at sabine pass 0 54 m at the entrance to the galveston bay 0 39 m at corpus christi and 0 32 m at port isabel for florence switching was done when about the storm eye was located about 477 km off the hatteras inlet nc the water levels at this time were 0 46 m at wrightsville beach 0 26 m at beaufort and 0 25 m at hatteras 2 3 2 error statistics the agreement between observations and predicted results is quantified by using error metrics of root mean squared error e r m s and mean normalized bias b m n thomas et al 2019 the e r m s indicates magnitude of the error and has an ideal value of zero whereas the b m n is a measure of the model s over or under prediction normalized to the observed value and also has an ideal value of zero a positive b m n indicates over prediction while a negative value indicates under prediction by the model the predicted and observed peak water levels are also analyzed with the coefficient of determination r 2 and best fit slope m the r 2 is a statistical measure of how close the data is to the linear regression model and has an ideal value of unity the best fit slope is the slope of the line that best represents the relationship between observed and predicted data and also has an ideal value of unity the accuracy of the approach is also evaluated by comparing the total volume of inundation to that from the coarse and fine simulations for an element this volume is equal to the area of the element multiplied by the average depth of water height of water above the ground surface in the three vertices an element contributes to the total volume only if all the three vertices 1 have ground surface elevations above mean sea level 2 are located in the affected area of the storm and 3 were flooded during the simulation it is noted that because the ground surface is represented at differing resolutions in these simulations the volume of inundation will be affected by the resolution the efficiency of the approach is evaluated by comparing the wall clock time of simulations with and without switching all simulations were completed on the stampede2 computing cluster at the texas advanced computing center using 522 computational cores for the approach a total of the times required for the coarse part of the simulation adcirpolate and the fine part is taken for comparison to avoid any inconsistencies with run times due to hardware competing jobs or network traffic each simulation is run three times on the same number of cores and the minimum of the three run times is then used for comparisons however in operations there is no restriction on the numbers of cores used for the coarse or fine simulations or adcirpolate 3 results and discussion in this study we hindcast four storms each having different parameters like track intensity flooding extent etc for each storm we perform three simulations coarse fine and mixed the results from these simulations are then analyzed to quantify the benefits of the proposed approach 3 1 accuracy benefits 3 1 1 comparisons of predicted flooding extents to examine the effects of higher resolution on predictions of overland flooding difference maps of maximum water levels between the coarse mixed and fine simulations are plotted for all four storms fig 2 differences are shown at the fine mesh resolution by mapping the coarse and mixed results to the fine mesh overall there are significant differences between the coarse and fine maximum water levels for all four storms and these are attributed to the difference in mesh geometry between the coarse and fine meshes for matthew and florence these differences mainly occur inland with near zero differences nearshore and in the open ocean for ike and harvey larger differences are both nearshore and inland with larger magnitude near the landfall location for ike the fine maximum water levels exceeded the coarse results by as much as 1 25 m in the rivers bays and lakes located north of galveston bay texas these large differences were likely due to how the bottom friction is parameterized on the louisiana texas meshes in both meshes with the texas mesh using a lower friction that enabled predictions of the ike forerunner kennedy et al 2011 for all storms the mixed and fine maximum water levels were a good match as evident from the near zero differences in the open ocean nearshore and inland regions differences in the open ocean and nearshore regions were small less than 0 15 m and related to the time of switching in the mixed simulation and differences in inland regions are due to the difference in geometry between the coarse and fine meshes harvey produced maximum inundation levels of 2 5 to 3 m to the north and east of its two landfalls in texas in the backbays between port aransas and matagorda including the copano bay and lavacaa bay texas fig 2 third row left column compared to the coarse results the fine maximum water levels are higher along the coast and inland along the coast these differences are small in the range of 0 1 to 0 2 m the differences inland were 0 15 m in corpus christi bay 0 2 m in san antonio bay 0 35 m in matagorda bay and 0 35 to 0 9 m in lavaca bay these differences are mainly related to the difference in the ground surface representation between the texas mesh and the hsofs mesh the mixed results are a good match to the fine results as evident from the zero differences along the texas coastline the lone exception is a location south of baffin bay away from the landfall location fig 2 third row right column where the mixed maximum water levels are higher than the corresponding fine values by more than 30 cm these differences are caused by differences in how the ground surface is represented between the hsofs and texas meshes and thus independent of the switching process for florence as compared to the coarse results the fine water levels are higher in regions like the albemarle sound atlantic intracoastal waterway core sound currituck sound and upstream all major rivers fig 2 last row middle column these differences were 0 1 to 0 2 m in the core sound 0 2 to 0 6 m upstream of the neuse river 0 1 to 0 2 m in the currituck sound and 0 2 to 1 m up stream of the pamlico river this is attributed to the higher resolution in the fine mesh that better represents bathymetry and in turn a better hydraulic connectivity for water to flow into these complex regions in the coarse mesh the water is not able to flow into the rivers and instead is stuck at downstream locations downstream of the neuse river the coarse water levels were higher by as much as 0 35 m there are almost zero differences in the open ocean along the coast and in the pamlico sound the differences between the mixed and fine results are almost zero fig 2 last row right column as switching has happened well before the storm impacted the nc coast small differences exist in the northeast region of the domain far away from the storm s impact these differences are contributed by the coarse part of the mixed simulation due to the large difference in resolution between the coarse and fine meshes 3 1 2 localized benefits it is expected that the fine mesh resolution will have its largest benefits at inland locations because it will better represent the connectivity of storm surge from the open coast and through the rivers and other channels predicted water levels are compared at inland locations during matthew and ike to highlight the benefits of the approach in matching fine results even at locations far from the coastline for matthew water levels are compared at 3 stations along the savannah river on the ga sc border fig 3 stations 1 and 2 are located upriver where the resolution is insufficient in the coarse mesh as shown by the poor simulation of the tidal signal at these locations before the switch but after the switch the mixed and fine results match well at station 3 closer to the open coast both meshes have sufficient resolution of the storm surge at this location and hence the coarse and mixed results are a good match before and after the switch a similar analysis is made at three stations located north of the trinity and galveston bays in texas during ike fig 4 stations 1 and 2 are located where the coarse mesh does not possess sufficient resolution to represent the san jacinto river therefore the coarse part of the mixed results stays dry but after the switch the mixed water levels match well to the fine results at station 3 the coarse mesh has sufficient resolution to represent the storm surge but the results are slightly different from the fine simulation as the fine mesh has a much higher resolution and thus it predicts flooding better at all three locations the mixed results match the fine results after the switch thus the approach improves predictions of water levels at inland stations where the fine mesh has a better representation of the flow pathways 3 1 3 quantifying accuracy the accuracy of the coarse and mixed results are analyzed by comparing them to the fine solution which is taken as the truth table 2 this allows for an evaluation of accuracy throughout the entire region not only where observations were collected for this comparison the coarse results are mapped onto the fine mesh as a post processing step so comparisons can be made at the same resolution for the mixed simulation the maxima of water levels from all parts of the simulation are considered results are compared only at vertices in the affected area of the storm fl to nc for matthew nc for florence tx for harvey and ike that are not in open ocean depths less than 10 m and that were wetted in both simulations for all four storms the mixed results have a b m n either equal to or close to zero whereas the coarse results have a negative value indicating an overall under prediction by the model the mixed results also have a best fit line slope equal to or close to unity indicating a good match to the fine results the e r m s closer to zero and r 2 closer to unity are also better but the most interesting difference lies in the number of fine mesh vertices that were used for comparison the mixed simulations have comparisons made at much higher numbers of vertices about 218k more vertices for ike 680k for matthew 209k for harvey and 82k for florence indicating much more overland flooding as compared to the coarse simulation this considerably larger number of comparison points during matthew is due to the larger region of analysis fl to nc as compared to individual states for other storms matthew was a shore parallel storm that had effects from fl to nc fig 2 second row left column the advantage of using the mixed approach is evident at points that were dry during the coarse simulation but were wetted in the mixed simulation fig 5 these additional wetted vertices are located along the wetting drying regions like barrier islands sounds as well as upstream rivers where the coarse mesh does not have sufficient resolution these trends in the difference in flooding extent between the coarse and mixed simulations are supported by the total volume of inundation table 3 for all storms the mixed and fine simulations have comparable values thus proving the effectiveness of the approach in matching the fine simulation flooding extent the coarse simulation on the other hand has a much lesser total volume as it floods lesser number of elements and lacks the flooding extents of both the mixed and fine simulations the total volume of inundation for the coarse simulation was less than that for the corresponding fine simulation by 4 17 1 0 9 m 3 for ike 1 61 1 0 9 m 3 for matthew 0 58 1 0 9 m 3 for harvey and 0 66 1 0 9 m 3 for florence however the total volume of inundation for the fine simulation exceeded the value for the corresponding mixed simulation by only 0 73 1 0 9 m 3 for ike 0 06 1 0 9 m 3 for matthew 0 15 1 0 9 m 3 for harvey and 0 04 1 0 9 m 3 for florence the difference in the volume of inundation between the mixed and fine simulations is due to how the coastal region is represented in the coarse mesh segment of the mixed simulation on the fine mesh the peak surge during ike was higher than 5 m along the texas coastline east of galveston bay fig 2 top row left column as this location experienced high shore parallel winds at the time of landfall and afterward this high peak surge led to the largest volume of inundation compared to the other three storms table 3 but even for this high surge the volume of inundation from the mixed and fine simulations differed by only 0 73 1 0 9 m 3 the corresponding difference between the coarse and fine simulations was 4 17 1 0 9 m 3 thus compared to the coarse results the mixed approach allows for a more accurate flooding as well as a much larger flooding extent which can be crucial during forecasting 3 2 efficiency benefits for all four storms the total wall clock time required for the mixed simulation is compared with that for the corresponding fine simulation table 4 there is a wide variety in the wall clock times depending on the size of each mesh by total number of vertices elements and the number of simulation days on each mesh for ike and harvey the fine simulations required 102 to 103 min which was reduced by 3 to 19 percent with the mixed simulations for matthew and florence the fine simulations required 380 to 393 min which was reduced by 38 to 53 percent in the fine simulations adcirpolate required 8 to 12 min or less than 10 percent of each mixed simulation the multi resolution approach had its maximum efficiency during florence with a run time decrease of 53 percent this efficiency is due to florence s shore normal track which allowed switching after 6 days of simulation when the storm first affected the nc coast thus the coarse mesh was used for most of the mixed simulation resulting in a lesser run time there was also a large gain in efficiency during matthew with a run time decrease of 38 percent because matthew had a shore parallel track it affected a larger geographical extent for several days and thus it required the fine mesh to be used for more of its mixed simulation the relative sizes of the coarse and fine meshes by number of vertices elements also affected the efficiency of the approach for storms on the atlantic coast the fine fema sab mesh is more than three times larger than the coarse hsofs mesh whereas for storms on the texas coast the fine texas mesh is about 1 8 times larger than the coarse hsofs mesh because of this there is a smaller potential for efficiency gains for storms on the texas coast for example during harvey the mixed approach required 99 min compared to the fine simulation that required 103 min thus the run time decrease was only 4 percent but overall the multi resolution approach does have an efficiency gain for all storms which is crucial especially in forecasting applications because it can enable ensemble simulations to account for uncertainties in storm parameters 4 conclusions high fidelity predictions of coastal flooding require high resolution of small scale flow pathways and barriers in coastal regions this high resolution can be computationally costly a multi resolution approach was implemented in the adcirc modeling system this approach allows the use of high resolution meshes only when it is required and without re starting the simulation from scratch the approach was evaluated for simulations of four storms that affected the u s coast in different regions the benefits of the approach were evaluated in terms of accuracy and efficiency by comparisons to single simulations on coarse and fine resolution meshes the major findings of this study are 1 if the simulation is switched at an acceptable time during the storm then the flooding predictions will be similar to a full simulation on the higher resolution mesh for all storms the mixed flooding predictions were similar to the fine flooding predictions however they allowed flooding of a larger region as compared to the corresponding coarse simulation this extra flooding coverage was at regions like barrier islands and upstream rivers where the coarse mesh did not have sufficient resolution to provide the required hydraulic connectivity for flooding to occur at inland stations the water levels reacted quickly to the switch onto the fine mesh 2 the multi resolution approach can preserve the accuracy of high resolution predictions of coastal flooding for all for storms the mixed results had a comparable accuracy to the fine results the r 2 and best fit slopes were close to unity the e r m s values were less than 0 18 m and the b m n values were close to zero 3 the multi resolution approach can enhance significantly the efficiency of high resolution predictions of coastal flooding the mixed simulations were more efficient than the fine simulations as measured by wall clock times the efficiency gains were between 3 to 53 percent depending on the relative sizes and simulation durations on the meshes adcirpolate required less than 10 percent of the overall wall clock time the multi resolution approach is most promising for forecasting applications in which the efficiency gains can translate directly into additional time for decision making future work will explore which conditions coastal water levels wind speeds etc can be used to trigger the switch and what is an optimal balance between accuracy losses and efficiency gains credit authorship contribution statement ajimon thomas validation visualization writing original draft j c dietrich supervision project administration writing review editing m loveland investigation validation a samii data curation methodology software c n dawson conceptualization funding acquisition software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this material is based upon work supported by the u s department of homeland security under grant award number 2015 st 061 nd0001 01 the views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies either expressed or implied of the u s department of homeland security this work was also supported by the national science foundation united states of america grant enh 1635784 appendix example of adcirpolate in coastal north carolina to illustrate how adcirpolate can be used to switch meshes during a simulation we provide a small example for coastal north carolina fig 6 beaufort inlet is a barrier island inlet located west of cape lookout and leads south to the atlantic ocean the domain also includes the tidal newport river that flows into the bogue sound and core creek that connects to adams creek a tributary of the neuse river due to the switching with adcirpolate onto a higher resolution mesh the simulation will better represent the flow of surge farther up the newport river meshes with coarse and fine resolution were created by cutting the region from the hsofs and sab meshes by using the surface water modeling system sms https www aquaveo com software sms surface water modeling system introduction the coarse meshhas 3277 vertices with resolution varying from 1377 m at the ocean boundary on the south of the domain to about 550 m at the inlet to 315 m along the core creek and to 350 m at the west end point of the newport river the fine mesh has 22 375 vertices with element spacing varying from 888 to 2500 m at the ocean boundary to 120 m at the inlet to 85 m along the core creek and to 25 m at the west end point of the newport river the difference in resolution causes features to be represented differently between the two meshes fig 7 the bathymetry of the newport river extends about 20 km farther west in the fine mesh the coarse mesh has 2 to 3 elements across the width of the beaufort inlet compared to 10 elements in the fine mesh a tidal signal of period 3 hr and amplitude 1 2 m was added to a surge signal of peak amplitude 2 m and applied as forcing fig 8 top left on the bottom south boundary of the two meshes the total duration was 54 hr the simulation started on the coarse mesh but after 24 hr when the water level on the ocean boundary reached 1 4 m it was switched to the fine mesh for the remainder of the run before switching at the end of the simulation on the coarse mesh fig 7 bottom left the water levels were 1 4 m at the open coast 0 35 to 0 5 m in the inlet and 1 m at the west boundary of the newport river after switching at the start of the simulation on the fine mesh fig 7 bottom right these water levels were mapped to the true coastline inlet and sound and extended into the newport river water levels were analyzed at three stations 1 open coast 2 inlet and 3 channel with locations in fig 7 and for the fine and mixed simulations at the open coast station 1 there was no difference between the simulations because both meshes had a sufficient resolution in open water to represent the combined tide and surge forcing at the inlet station 2 there are differences between the mixed and fine water levels before switching due to differences in geometry between the coarse and fine meshes but after switching the water levels match exactly at the channel station 3 the mixed simulation stays dry for the first 24 hr because the coarse mesh does not have sufficient resolution to represent the fine channel however very shortly after the switch the water levels in mixed increased to match the fine results this was an increase of 0 75 m in just a couple of hours with no oscillations or instabilities in the computed solution thus even when the coarse mesh had locations that were dry the mixed simulation was able to catch up to the fine results 
23896,the efficacy of an ocean observing analysis and forecasting system for the mid atlantic bight and the gulf of maine is explored using the concept of array modes the analysis forecast system is based on a triply nested configuration of the regional ocean modeling system roms in conjunction with 4 dimensional variational 4d var data assimilation the array modes identify the degrees of freedom df of the signal and of the noise resolved by the observations and are used here to quantify the extent to which the existing network of platforms and instruments are able to observe the ocean across different dynamical regimes ranging from quasi geostrophic through the mesoscale and down to the sub mesoscale the ocean observing system includes the u s national science foundation s ocean observatories initiative pioneer array in general it is found that the df of the signal are largely associated with in situ observations from the pioneer array on the other hand a combination of satellite remote sensing and in situ observations potentially contribute to the df of the noise associated with uncertainties in the measurements the array modes also provide information about the reduction in the expected analysis and forecast error covariance due to assimilating the observations here too observations from the pioneer array are found to significantly influence the veracity of the analyses and forecasts and the circulation is instrumental in propagating observational information to other parts of the model domain an approach is presented in which the array modes are used to quantify the impact of data assimilation on the expected forecast error covariance of forecasts initialized from the 4d var ocean state estimates the advantage of this approach over others in common use is that it is independent of forecast error norm and circumvents the need for generating potentially large and costly ensembles keywords array modes data assimilation 4d var mid atlantic bight pioneer array forecast error covariance 1 introduction regional ocean analysis and forecasting are now well established activities of many national agencies operational centers and research groups worldwide a critical component of such systems is data assimilation which aims to combine ocean observations with a model to yield an estimate of the ocean state that is more reliable than either the observations or model alone since the practice of data assimilation is deeply rooted in estimation theory it also provides an opportunity to assess the properties of the observing system itself in this study we explore the array modes of the ocean observing system in the mid atlantic bight mab and gulf of maine gom in the ne atlantic this observing system supports the u s integrated ocean observing system ioos and forms the backbone of the mid atlantic regional association coastal ocean observing system maracoos the mab is also unique in that it is home to the u s national science foundation nsf ocean observatories initiative ooi pioneer array the pioneer array has been operational since april 2014 and comprises fixed moorings and a fleet of autonomous underwater vehicles that are deployed at the continental shelf break the primary aim of the pioneer array is to increase understanding of the processes responsible for the transport of water masses across the shelf break and their relationship to atmospheric forcing on a range of time scales gawarkiewicz et al 2018 this paper is an extension of the recent studies by levin et al 2019 2020 2021 which document a detailed assessment of the impact of the mab and gom observing system on data assimilation estimates of the ocean environment and shelf break exchange processes in the vicinity of the pioneer array here the array modes of the observing system have been used to delve deeper into the degree to which the information provided by the observations constrain our knowledge of the ocean state the array modes are analogous to the characteristic modes employed in electrical engineering and antenna design array modes were first introduced in oceanography by bennett 1985 and provide information about the field of view and degrees of freedom df of an observing system as well as limitations that are endemic to the data assimilation system more recent applications of array modes in ocean data assimilation include egbert et al 1994 bennett 2002 kurapov et al 2009 and kurapov and özkan haller 2013 while modified forms of the array mode concept have been employed by le hénaff et al 2009 lamouroux et al 2016 and moore et al 2018 this study draws on the properties of and information provided by the array modes as a means of quantifying the efficacy of an ocean analysis forecast system given their central importance the concept of array modes is reviewed in section 2 in relation to 4 dimensional variational 4d var data assimilation the approach employed in this work the concept of reduced rank array modes rams is a practical variant of the array mode concept and is also introduced in section 2 the model used in this study is the regional ocean modeling system roms configured for the mab and gom in conjunction with 4d var as described in section 3 the model comprises a triply nested configuration of roms that resolves circulation scales ranging from quasi geostrophic down to the sub mesoscale the 4d var analyses at each scale are informed by observations that lead to a reduction in the expected error covariance of the resulting ocean state estimates as we will demonstrate the reduction in error covariance can be quantified by drawing on the known properties of the rams as a prelude the hallmark fingerprints of the rams of the mab and gom ocean state estimates across the range of scales captured by the model are first explored in section 4 section 5 focuses on several aspects of the impact of the observations assimilated into the model on the 4d var analyses and ensuing ocean forecasts first section 5 1 describes an alternative and cost effective ram based approach for computing the expected reduction in the analysis and forecast error covariance the spatio temporal nature of the expected error variance reduction in explored in section 5 2 in section 5 3 we demonstrate that much of the expected reduction in error covariance is associated with a single ram furthermore the contribution of each observation to the amplitude of this ram is shown to be a useful and alternative measure of the observation impacts the advantage of this approach over the more conventional adjoint based e g langland and baker 2004 or ensemble based approaches e g liu and kalnay 2008 is that it is independent of forecast error norm and generally more cost effective many of the ideas and results presented here are predicated on identification of the rams as the dfs of the observing system this information is utilized in section 6 to identify the extent to which the 4d var analyses may be overly constrained by errors and uncertainties in the observations a summary conclusions and discussion of potential applications of our work is presented in section 7 2 array modes the theory of array modes will be summarized here in terms of 4 dimensional variational 4d var data assimilation although in principle the same ideas can be applied to other linear data assimilation methodologies with this in mind we will follow standard notation and denote by x the ocean state vector comprising all grid point values of the model prognostic variables given a prior or background estimate of the state vector x b and the n 1 vector of ocean observations y o where n is the number of observations the best linear unbiased estimate or analysis x a is given by 1 x a x b k y o h x b where h is the observation operator that samples x b at the observation locations in space and time and k is the gain matrix in 4d var h includes the nonlinear model the gain can be expressed as 2 k b h t hb h t r 1 where b and r are the background error and observation error covariance matrices respectively the matrix h denotes the tangent linearization of h and in 4d var represents the tangent linear model sampled at the observation points while h t denotes the adjoint of these operations the matrix p hb h t r represents the total error covariance in the space spanned by the observations and is often referred to as the stabilized representer matrix for now let us suppose that all of the observations are of the same type e g in situ temperature observations in this case and since p is a symmetric matrix it can be factorized as w λ w t where w w i is the matrix of orthonormal eigenvectors w i and λ diag λ i are the associated eigenvalues following bennett 1985 the analysis x a in 1 can be re expressed as 3 x a x b i 1 n α i ψ i where ψ i b h t w i are referred to as the array modes with amplitudes α i λ i 1 w i t d and d y o h x b is the innovation vector similar ideas are utilized in antenna theory where it is the eigen spectrum of the impedance matrix associated with an antenna that is considered chen and wang 2015 in that case the array modes or characteristic modes are frequency dependent the extent to which a particular mode is excited by an incident electromagnetic field with frequency ω is proportional to λ i ω 1 therefore when the incident waveform frequency matches the eigenvalue of a specific array mode resonance occurs the array modes of an ocean observing system depend on b r h and in the case of 4d var x b and while dependent on the observation times and locations through h they are independent of the observation values y o however the extent to which each array mode is excited is determined by the innovations d and therefore does depend on the data values themselves in this case there is no frequency dependence of the incident signal from the innovations and the array mode amplitudes α i are proportional to λ i 1 the eigenvectors w i represent the empirical orthogonal functions eofs of the total error covariance in observation space and the eigenvalues are the variance associated with each eof since the array mode amplitudes α i λ i 1 the eofs associated with the largest uncertainty are weighted the least while the eofs that account for the least fraction of error variance are weighted the most this of course makes intuitive sense since the 4d var increments δ x i 1 n α i ψ i then draw most heavily on the array modes associated with the smallest total error variance the array modes themselves ψ i b h t w i represent the projection of the stabilized representer matrix eofs w i into state space via the adjoint operation h t in general ocean observing systems comprise observations of several state variables from a variety of different platforms also since the number of observations n is large most 4d var approaches identify the analysis given by 1 and 2 using iterative methods therefore some form of preconditioning of p is essential and without this the notion of eofs does not make sense in roms the r preconditioned stabilized representer matrix p r 1 hb h t i is factorized using the lanczos formulation of the conjugate gradient cg algorithm according to 4 p m v m t m v m t hb h t where v m v i is the matrix of lanczos vectors v i which represent the normalized cg search directions and t m v m t hb h t p v m is a symmetric positive definite tridiagonal matrix gürol et al 2014 the lanczos vectors are orthonormal according to v m t hb h t v m i m where the norm used is a result of additional restricted preconditioning by b gratton and tshimanga 2009 the subscript m represents the number of cg iterations performed referred to as inner loops in this case a set of array modes and amplitude coefficients can be defined according to 5 ψ i b h t v m φ i 6 α i λ i 1 φ i t v m t hb h t r 1 d λ i 1 ψ i t h t r 1 d where λ i φ i are the eigenpairs of t m since p m in 4 represents a reduced rank approximation of r 1 p moore et al 2018 hereafter mae refer to the ψ i as the reduced rank array modes rams to distinguish them from the full rank case originally considered by bennett 1985 it is also important to note that since 5 and 6 are based on the eigen spectrum of the preconditioned matrix r 1 p the eofs of p m must be interpreted as the vectors that account for fractions of the rescaled total variance in observation space this is also discussed by le hénaff et al 2009 who considered a similar rescaling of p by r 1 2 and refer to the associated eofs mapped back into state space as modal representers in the case considered here there will be only m rams and the analysis increment can be expressed as δ x i 1 m α i ψ i despite the differences in definition several useful complementary interpretations of the array modes and rams exist bennett 1985 notes that the array modes can be viewed as interpolation patterns from observation space to state space onto which the observations project the dependence of the weights α i on λ i 1 dictates that the analysis increments δ x will be most least sensitive to uncertainties in the measurement values that project onto array modes associated with the smallest largest eigenvalues if the eigenvalues λ i are arranged in descending order then ψ 1 represents the most stable interpolation pattern for the observations into state space conversely we should treat with caution the ψ i associated with small eigenvalues since these may introduce non physical noise into the analysis viewed another way the eigen spectrum provides information about the degrees of freedom df for the signal resolved by the observing array and the df for the noise associated with uncertainties in the observations due to measurement errors and errors of representativeness rodgers 2000 since r 1 hb h t and p have the same eigenvectors and their eigenvalues differ by 1 the number of eigenvalues for which λ i 2 provides a measure of the effective number of df of the signal that is resolved by the observing array array modes associated with λ i 2 will be indistinguishable from errors or uncertainties in the observations i e the df of the noise in the data and should be rejected 1 1 from the definition of p m the eigenvalues λ i 1 and the lower bound would correspond to the situation where r 1 hb h t is singular these ideas have been applied by bennett and mcintosh 1984 le hénaff et al 2009 and mae to ocean observing systems and will be revisited in section 6 where a more conservative practical criterion is enforced instead of λ i 2 to prevent overfitting to observation errors furthermore since the expected covariance properties of the array mode amplitudes are known a priori identifying the array modes with the df provides a useful framework for quantifying the impact of data assimilation on ocean forecasts these ideas are exploited in section 5 3 model configuration the roms configuration employed in this study spans the mab and the gom three telescoping nesting layers were used and the geographical extent of each nested grid is shown in fig 1 the model configuration has been described in detail elsewhere levin et al 2019 2020 2021 so only a brief description will be given here the horizontal resolution of the three grids is 7 km grid g1 2 4 km grid g2 and 0 8 km grid g3 respectively in all grids there are 40 terrain following levels stretched so that the thickness of the surface most layers is in the range 0 1 1 8 m and 0 1 3 4 m near the bottom over the continental shelf the innermost refined grid g3 is centered on the nsf ooi pioneer array the g1 open boundaries were constrained using data from the mercator océan real time global analyses lellouche et al 2018 that were adjusted to remove a seasonal bias by comparing with the local regional climatology derived by fleming 2016 corrections for bias were also made to the g1 open boundary mean dynamic topography and seasonal cycle of sea surface height ssh using a regional data assimilative climatological analysis as described by levin et al 2018 and wilkin et al 2018 surface fluxes of momentum heat and freshwater were derived from 3 hourly national centers for environmental prediction ncep north american mesoscale nam fields using the standard bulk formulae of fairall et al 2003 application of the nam atmospheric pressure drives an oceanic dynamic inverted barometer response daily freshwater discharge from 22 rivers was imposed based on gauge observations from the u s geological survey and water survey of canada lópez et al 2020 wilkin et al 2018 all three grids can be run using one or two way nesting which provides appropriate boundary conditions for g2 and g3 the data assimilation system used is the dual formulation of roms 4d var moore et al 2011a gürol et al 2014 a full description of the 4d var system and its configuration can be found in levin et al 2018 2019 2020 2021 and wilkin et al 2018 so only a very brief summary of the salient points is presented here the data assimilated span the period jan 2014 dec 2017 and are summarized in table 1 from levin et al 2020 at the time that these calculations were performed the roms 4d var system did not function across one or two way nested configurations so the following strategy was adopted to assimilate the available observations into the three grids i data were first assimilated into g1 for the full 2014 2017 period using a 3 day assimilation window in this case the model initial conditions surface forcing and open boundary conditions were treated as control variables the background state estimate for each 3 day window was taken to be the analysis at the end of the previous cycle ii step i was repeated for grid g2 using the 4d var analyses from each cycle of g1 as the background open boundary conditions for each 4d var cycle of g2 as in g1 the control variables were the initial conditions surface forcing and open boundary conditions iii step ii was then repeated for grid g3 however in this case the data assimilation window was reduced to 1 day with only the initial conditions and open boundary conditions used as control variables the 4d var analyses from each cycle of g2 were used as the background open boundary conditions for each 4d var cycle of g3 also because of the considerable increase in computational effort 4d var was only run on g3 for the period jan 2014 dec 2015 as discussed in moore et al 2011a the background error covariance b matrix in roms is modeled following the diffusion operator approach of weaver and courtier 2001 table 2 summarizes the decorrelation length scales assumed in b for errors in each control variable on the three model grids used here and these parameter choices are discussed in levin et al 2019 the observation error covariance matrix r was assumed to be a diagonal matrix and table 1 summarizes the errors and uncertainties that were assigned to measurements from each observing platform these errors reflect a combination of measurement error and errors of representativeness i e uncertainties associated with the ability of the model grid to resolve all of the processes that are captured by the observations and are also discussed in levin et al 2019 following andersson and järvinen 1999 quality control was performed during each 4d var cycle specifically the innovation d i associated with each observation is compared to the standard error based on the assumed standard deviations of the background σ b and observation σ o errors for a chosen threshold γ an observation is rejected and not included in the analysis if d i 2 γ 2 σ b 2 σ o 2 the thresholds γ depend on the type of observation and are given in table 2 for the analyses on each grid considered here time series of the total number of observations assimilated into the model on each grid after the formation of super observations during a 4d var cycle are shown in fig 2 also shown are time series of the number of observations from each observing platform the number of observations from each platform is similar across all three grids apart from satellite altimetry the number of altimeter overpasses decreases dramatically going from g1 to g3 due to the reduced geographical extent of each nested grid the performance of the 4d var system on each of the three grids has been documented in detail by levin et al 2019 2020 2021 suffice to say the system performs well across all three grids and interested readers are encouraged to refer to these previous studies for more details an example 4d var analysis from each grid is illustrated in fig 1 which shows sea surface salinity on 16 may 2014 at this time a streamer of saline water associated with a large gulf stream eddy can be seen impinging on the shelf an event that has been studied in detail by zhang and gawarkiewicz 2015 fig 1 shows very clearly how the 4d var circulation estimates can capture the range of scales from quasi geostrophic down to the sub mesoscale secondary circulations as the grid resolution increases 4 reduced rank array modes the incremental 4d var procedure outlined in section 2 is equivalent to minimizing a cost function that represents the squared difference between x a and x b and the observations and x a evaluated at the space time observation locations weighted by the inverse background error and observation error covariance matrices respectively e g courtier et al 1994 the desired best linear unbiased estimate is given by 1 and as outlined in section 2 can be identified using cg methods via a sequence of inner loop iterations in keeping with the usual practice the incremental formulation of 4d var adopted in roms also employs an outer loop and the ocean state about which h and h t are linearized is updated after every m inner loops the roms 4d var analyses described in section 3 were computed using two outer loops and seven inner loops in which case x a x b δ x 1 δ x 2 where the subscript refers to the contribution from each outer loop in this case each δ x can be expanded in terms of the rams appropriate for the outer loop under consideration levin et al 2020 showed that it is in the 1st outer loop that increments are largest and where the observations have the greatest impact on the final analysis therefore in the sequel we will focus on the rams of the first outer loop for the 4d var analyses of section 3 in addition we will demonstrate in section 5 how the rams can be used to quantify the influence of data assimilation on the expected forecast errors and there for mathematical convenience we will focus on a single 4d var outer loop following 5 and 6 and recalling that m 7 for the 4d var analyses considered here there will be seven rams for each outer loop fig 3 shows time series of α 1 and α 7 the absolute value of the amplitudes of the rams associated with the largest eigenvalue λ 1 and smallest eigenvalue λ 7 of p m for the 1st outer loop of each 4d var cycle in the three grids on average α 7 α 1 varies in the range 10 103 although there are some cycles where the ratio can be as high as 105 therefore errors and uncertainties in the innovations d that project onto ram ψ 7 will have considerably more influence on the analysis increments than those that project onto ram ψ 1 to illustrate the typical structure of the rams fig 4 shows the sea surface temperature sst associated with each ram of the 1st outer loop for the 3 day 4d var cycle spanning the interval 17 19 sept 2015 on g2 specifically fig 4 shows the ram sst on 17 sept at the beginning of the assimilation cycle rams ψ 1 fig 4a ψ 2 fig 4b and ψ 3 fig 4c have largest amplitude in the vicinity of the pioneer array conversely rams ψ 4 through ψ 7 generally have more complicated sst structures that span a larger portion of the model domain the eigenvalue spectrum of p m for this 4d var cycle is shown in fig 4h and spans about two orders of magnitude it is important to reiterate that the rams do not depend on the observation values only on their locations in concert with the background state and the error covariances to illustrate this aspect of the rams fig 4i shows the structure of φ 1 and φ 7 the leading and trailing eigenvectors of the lanczos decomposition of the inner loop iterations during the same g2 assimilation cycle according to 5 the leading and trailing rams are given by ψ 1 b h t v m φ 1 and ψ 7 b h t v m φ 7 representing the projection of φ 1 and φ 7 in fig 4i into state space specifically the elements of φ 1 and φ 7 represent weights for the lanczos vectors that form the columns of v m the weighted sums of the lanczos vectors v m φ 1 and v m φ 7 are then mapped into state space by the adjoint observation operator h t which in 4d var involves an integration backwards in time after the mapping into state space the state vector is multiplied by the background error covariance b which has the effect of smoothing the fields according to the decorrelation length scales assumed in table 2 the smooth nature of the ram sst in figs 4a and 4g is very evident for these two modes during the 17 19 sept 2015 4d var cycle the most abundant observations are sst from several different platforms figs 5a c show the innovations associated with three separate overpasses of the avhrr instrument on different days while figs 5d f show the elements of v m φ 1 associated with the same observations there is little or no correspondence between the innovations in fig 5a c and the image of φ 1 mapped into state space shown in figs 5d f however the sst structure of the associated ram ψ 1 cf fig 4a is already evident in figs 5d f conversely figs 5g i show the elements of v m φ 7 at the avhrr observation locations and some features of ram ψ 7 cf fig 4g are already apparent thus the absence of any general correspondence between the ram structures and the innovations further illustrates the their independence from the observation values the structure of ram ψ 1 appears to be closely aligned with the location of the in situ observations for example fig 6 shows the 3 dimensional temperature structure of ψ 1 on 17 sept 2015 on g2 in the vicinity of the pioneer mooring array and the region of the ocean informed by the ram appears to be closely aligned with some of the in situ mooring observations the same is true for the other state vector components of ram ψ 1 not shown the contribution of each ram to the 4d var increment is given by α i ψ i and according to fig 3 it is anticipated that ψ 7 will contribute the most this is confirmed in fig 7 which shows the root mean square rms of sst averaged over all 4d var cycles on each grid associated with ψ 1 and ψ 7 ram ψ 7 clearly dominates which is also the case at depth and for other fields not shown thus much of the detailed structure in the analysis increments is associated with the trailing rams it is important to note that while the φ i are orthogonal the rams are not so fig 7 cannot be interpreted as the contribution of ψ 1 and ψ 7 to the total sst variance of the increments it is important to note that the rams comprise components that are associated with all elements of the 4d var control vector therefore in the case of g1 and g2 this includes fields of surface flux forcing and for the open boundary conditions in the case of all three grids in the interest of brevity we will not discuss these additional components of the rams here taken together figs 4 and 7 indicate that the leading rams of g2 appear to be primarily associated with the pioneer array in contrast the trailing rams span most of the model domain and are largely controlled by the location of remote sensing footprints the rams of g1 also confirm this picture not shown however as demonstrated in section 6 this view is an oversimplification 5 the impact of data assimilation on expected forecast uncertainty 5 1 analysis and forecast error covariance as discussed in section 2 the rams can be interpreted as interpolation patterns for the innovations into state space and provide quantitative information about the sensitivity of the analysis increments to measurement errors and errors of representation see also section 6 this idea can be exploited further to quantify the expected errors in analyses and subsequent forecasts that arise from uncertainties in the innovations the analysis increment at the beginning of the 4d var analysis cycle is given by δ x kd i 1 m α i ψ i therefore uncertainty in the observations and background will manifest as uncertainties in the innovation vector d which enter through the 2nd equality as uncertainties in α i according to 6 for the best linear unbiased estimate e δ x 0 where e is the expectation operator similarly the expected covariance of the increments associated with uncertainties in d is given by c e δ x δ x t k e d d t k t b a where a is the expected analysis error covariance daley 1991 2 2 from the expected covariance of the innovation vector c k e d d t k t k hb h t r k t and using 2 c khb the analysis error covariance matrix a i kh b which shows that a b for any norm and furthermore b a khb c hence c represents the reduction in the background error covariance due to assimilating the observations since a b for any norm see footnote 2 expressing δ x in terms of the rams it can be shown that 3 3 the increment δ x i 1 m α i ψ i so that c e δ x δ x t e i 1 m j 1 m α i α j ψ i ψ j t i 1 m j 1 m e α i α j ψ i ψ j t using 6 e α i α j λ i 1 λ j 1 φ i t v m t hb h t r 1 k e d d t r 1 hb h t v m φ j from 4 e d d t r v m t m v m t hb h t and recalling that v m t hb h t v m i m we can write e α i α j λ i 1 λ j 1 φ i t t m v m t hb h t r 1 hb h t v m φ j recall that t m φ i λ i φ i and that the symmetric tridiagonal matrix t m v m t hb h t r 1 hb h t i v m in which case e α i α j λ j 1 φ i t t m i m φ j λ j 1 λ i 1 φ i t φ j λ j 1 λ i 1 δ i j where δ i j is the kronecker delta function therefore e α i α j 1 λ i 1 δ i j and c i 1 m 1 λ i 1 ψ i ψ i t 7 c i 1 m 1 λ i 1 ψ i ψ i t using the expected covariance properties of d at the end of the 4d var analysis window the analysis increment is well approximated by m δ x where m represents the tangent linear model linearized about the time evolving background state vector therefore 7 can also be used to compute the expected error variance mc m t mb m t ma m t at the end of the analysis cycle by merely replacing each ram ψ i in 7 by the time evolved rams m ψ i during the analysis cycle the surface forcing and open boundary condition components of ψ i are also used as inputs for m the 4d var analysis at the end of the assimilation window is commonly used as the initial condition for a forecast similarly the 4d var increments δ x can be propagated into the forecast interval using m linearized about the 4d var background also extended into the forecast interval and subject to the appropriate forecast boundary conditions and surface forcing while an estimate of the expected analysis error covariance matrix ma m t at the end of the analysis cycle is desirable it is challenging to compute in 4d var e g fisher and courtier 1995 ngodock et al 2020 moore and arango 2021 one reason being that the contribution of the background error covariance mb m t is computationally demanding to calculate reduced rank estimates are one approach although they tend to underestimate the analysis errors fisher and courtier 1995 moore et al 2011b ngodock et al 2020 have demonstrated a monte carlo approach for estimating a based on computing an ensemble of 4d var analyses by perturbing the so called representer coefficients β hb h t r 1 d this approach is related to that used here however in our case we capitalize on the known covariance properties of the ram amplitudes in 7 without the need to compute an explicit ensemble of analyses and forecasts in the case of the expected forecast error covariance some additional computational cost is involved since each ram or equivalently each lanczos vector v i must be propagated to the end of the forecast interval using the tangent linear model m since the computational cost of m in roms is about 50 more than a run of the nonlinear forecast model then for forecast lead times that are similar in length to the assimilation window the additional calculations required are 1 5 m where m is the number inner loops and of course the number of rams in the examples here m 7 so the additional computational burden is equivalent to running an o 10 ensemble of forecasts this is much smaller than the ensemble size required to estimate ma m t using say the randomization method of fisher and courtier 1995 in which a sample size 5000 would be needed to yield an estimate of the leading diagonal accurate to 1 to illustrate the utility of our approach consider the two forecasts illustrated schematically in fig 8 forecast x f a t is initialized from the analysis x a 0 at the end of each 4d var analysis cycle while forecast x f b t is initialized from the background x b τ of the 4d var cycle the forecast x f a t therefore benefits from the data that were assimilated during the 4d var cycle spanning the interval τ 0 while forecast x f b t does not the expected forecast error covariances of x f a t and x f b t are given by m τ t a m t t τ and m τ t b m t t τ respectively where the ordering of the time arguments indicates the direction of integration therefore m τ t c m t t τ m τ t b m t t τ m τ t a m t t τ represents the change in the forecast error covariance associated with assimilating the observations since the covariance information is propagated using m τ t the resulting covariance matrices represent 1st order approximations following this approach forecasts were initialized at the end of the 1st outer loop of each 3 day 4d var cycle on g1 and g2 respectively for the period 1 jan 31 dec 2015 the forecast duration was 10 days for g1 and 7 days for g2 a longer forecast interval was used in g1 because of the larger geographical extent of the model domain for such extended forecast periods true forecast fields are not available for the ncep nam surface forcing therefore for convenience a best time series concatenation of the 1 day forecast ncep nam meteorology and mercator océan open boundary data were used during these forecast experiments in the near real time maracoos system for g1 true 3 day forecast products from the respective operational centers are employed figs 9a c show an example sst forecast for x f b using g2 for the period 20 27 september 2015 the forecast initial condition for sst is shown in fig 9a where a large warm core gulf stream ring dominates the circulation during the subsequent 7 days of the forecast the ring circulation evolves and a filament of cooler fresher water circulates anticyclonically around the eastern and southern margins of the eddy figs 9b and 9c while a train of secondary instabilities develops along the ring s northern edge fig 9d o show the square root of the diagonal elements of m τ t c m t t τ computed using 7 for brevity we will refer to these as the standard deviations but recognize of course that the square root of the difference in the background error and analysis error variances does not represent the difference between the corresponding standard deviations the standard deviations are shown in figs 9d o for sst sea surface salinity sss sea surface height ssh and surface current speed since m τ t c m t t τ represents the difference between the background error covariance and the expected analysis error covariance due to assimilating the observations figs 9d o indicate the reduction in the expected analysis and forecast error variance due to assimilating observations during the 4d var cycle spanning 17 19 september the influence of the circulation is very evident in fig 9 especially in the case of sst figs 9d e f and sss figs 9g h i in particular data assimilation reduces the expected error in the anticyclonic filament of cooler fresher water by 1 c sst and 0 5 sss the expected reductions in error standard deviation in ssh figs 9j k l and surface current speeds figs 9m n o are more modest and are 2 cm and 15 cm s 1 respectively the forecast differences x f a x f b computed from the nonlinear model forecasts for sst and sss for the same period are shown in fig 10 and are fairly large for example fig 10 indicates that with the benefit of data assimilation the forecast initialized from x a generally leads to an increase in temperature and salinity of the filament that is advected anticyclonically around the eddy therefore the corresponding features in fig 9 represent a reduction in the standard deviation of the forecast error associated with these changes regions in fig 9 where the standard deviations are close to zero correspond to locations and fields where data assimilation has little impact on the expected forecast error variance and have an error variance similar to the background this does not mean that the forecast is not accurate only that the forecast is not benefiting in any significant way from the most recent data assimilated into the model the vertical structure of the forecast temperature and salinity along 70 5 w over the upper 250 m of the water column is shown in figs 11a and 11c respectively on forecast day 7 the signature of the warm core ring is very evident with the main thermocline reaching depths 150 m in the core of the ring fig 11a the signature of the cooler lower salinity filament at the southern edge of the eddy is also visible near 39 n figs 11b and 11d show the corresponding vertical structure of the temperature and salinity standard deviations computed from m τ t c m t t τ for the same forecast day elevated values of expected error reduction are prevalent on the continental shelf and following the cooler lower salinity filament and extend to depths below 500 m not shown 5 2 spatio temporal variations the reduction in total expected forecast error variance due to assimilating the observations is t r m τ t c m t t τ which can be thought of as the squared distance between x f b t and x f a t in fig 8 similarly the reduction in total variance associated with a particular forecast variable is given by the appropriate sub trace of m τ t c m t t τ the reduction in total variance at forecast time t relative to that the beginning of the forecast cycle provides a measure of the change in the distance between the forecasts x f b t and x f a t with this in mind fig 12 shows time series of μ t log 10 t r m τ t c m t t τ t r m τ 0 c m t 0 τ versus forecast lead time t cf fig 8 for the sub trace associated with all temperature grid points thus μ t can be viewed as an index of the change in the relative distance between the forecasts time series are presented for all 2015 forecast cycles on g1 and g2 instances for which the relative distance μ t 0 represent forecast times for which the sub trace variance at time t is higher than that of the forecast initial condition at time 0 cf fig 8 in other words the temperature forecasts of x f b t and x f a t are farther apart these situations correspond to cases where the total temperature variance of m τ t b m t t τ m τ t a m t t τ increases with forecast lead time meaning that the forecasts x f a t and x f b t are diverging through time and indicate a persistent benefit of data assimilation for x f a t conversely when the relative distance μ t 0 the squared distance between the forecasts x f a t and x f b t is decreasing over time and the benefits of data assimilation for x f a t are being slowly lost fig 12 indicates that for most forecast cycles and lead times the relative distance μ t 0 and the benefits of data assimilation during the 4d var analysis cycles extends throughout the forecast interval this is particularly true for g1 fig 12a for g2 fig 12b there are more instances when μ t 0 showing more cases when x f a t and x f b t converge in this domain time series of μ t for other state vector fields display similar behavior yet for ssh and velocity the number of instances when μ t 0 is generally higher than for temperature not shown the difference in behavior between g1 and g2 can be understood in terms of the relative horizontal resolution of the two grids while g2 better resolves the mesoscale instabilities than g1 g2 is more susceptible to the growth of forecast errors due to the more energetic higher rossby number circulation being inherently less predictable to the added nonlinearity we would expect the forecast skill of g2 to degrade on shorter time scales than in g1 figs 13a d show the square root of the mean variance associated with sst and sss analyses and 7 day forecasts computed from the average of m τ t c m t t τ over all analysis forecast cycles for 2015 in g2 large reductions in expected error occur in the vicinity of the pioneer array and are associated mainly with the observations collected by the array however there are significant reductions in the expected error farther afield clearly related to persistent circulation features such as the equatorward shelf break jet in the mab the decrease in error can be as large as 3 c in sst and 0 5 in salinity also figs 13e h show the square root of the mean variance for analyses and 10 day forecasts in g1 the expected error reduction in the g1 4d var analyses and forecasts are quantitatively similar to those in g2 and again highlight the local influence of the pioneer array the larger scale circulation effect is also evident in g1 with significant reductions in the expected error extending toward georges bank because 4d var propagates information dynamically upstream to the source region of waters that subsequently flow through the pioneer array 5 3 observation impacts on analysis and forecast error covariance much of the impact of data assimilation on the forecast state given by x f a t x f b t can be attributed to the ram associated with the smallest eigenvalue to illustrate consider again the g2 forecast illustrated in fig 10 for the period 20 27 september 2015 figs 14a and 14b show the sst and sss components of x f a t x f b t associated with ram ψ 7 i e α 7 m τ t ψ 7 on forecast day 7 and are very similar to figs 10c and 10f therefore much can be learned about the impact of data assimilation on the expected forecast errors from ψ 7 alone furthermore the good agreement between figs 14ab and figs 10cf confirms that the tangent linear approximation remains valid over this timescale according to 6 the amplitude of ram ψ 7 is given by the dot product of the innovation vector d with the vector λ 7 1 r 1 hb h t v m φ 7 therefore the contribution of each observation to α 7 can be quantified the forecast differences shown in figs 14a and 14b result from assimilating observations during the preceding 4d var analysis cycle spanning the period 17 19 sept fig 14c shows the contribution of each observation type assimilated during this period to the ram amplitude α 7 the largest contribution is from satellite sst although in situ velocity measurements from the pioneer array moorings are a close second the contribution of in situ temperature observations mainly from pioneer array moorings and gliders are also significant the location of the in situ observations during this 4d var cycle are indicated in fig 6 thus the partitioning of the amplitude of the dominant ram across the different observing platforms is a useful and alternative approach for quantifying the impact of the assimilated observations on the forecast the standard method for quantifying observation impacts in roms follows the adjoint approach of langland and baker 2004 where the impact of each observation on a chosen analysis or forecast metric is computed while this approach is generally quite efficient it requires separate calculations for each metric and forecast lead time the alternative approach that we are advocating here however is independent of any metric and forecast lead time as noted in section 2 the rams depend on the observation locations according to 5 and not on the observation values specifically ψ i b h t v m φ i where the matrix vector product v m φ i represents an eof of the r preconditioned stabilized representer matrix in the usual way these eofs provide information about the in phase and out of phase relationships between various fractions of the total error variance at different observation locations since 5 is a linear equation each ram can be expressed as the linear superposition of the contribution from each element of v m φ i corresponding to specific observations fig 15 shows the contribution of the v m φ 7 eof from information associated with the location of satellite sst in situ temperature and in situ velocity observations to the 7 day sst forecast differences of x f a t x f b t for g2 on 27 sep 2005 in keeping with fig 14 the contributions of eof information at ssh hfr radar and in situ salinity observation locations are small so are not shown consistent with fig 14a the sst component of ψ 7 in fig 15a accounts for much of the forecast change in sst to the north of the warm core ring and on the continental shelf the contributions of in situ temperature fig 15b and velocity fig 15c are mainly in opposition while around the margins of the ring these contributions reinforce each other thus while there are considerable and in some cases opposing overlaps between the contributions of different observation types to ψ 7 the general behavior in fig 15 confirms the observation impacts computed from the ram amplitude α 7 which depends directly on the measurement values with this in mind fig 16 shows the root mean square rms contribution of each observation type to the ram ψ 7 amplitude α 7 on each of the three grids the period considered is jan 2014 dec 2015 which is the overlapping period for which 4d var analyses were computed for all three grids fig 16 indicates that for a given observation type this measure of the observation impact varies considerably across the three grids these grid to grid variations are controlled by several factors that include i variations associated with differences in data coverage for example the number of along track satellite altimeter overpasses decreases dramatically going from g1 to g2 with very few tracks passing over g3 ii variations in horizontal resolution for example the increase in the impact of in situ velocity observations going from g1 to g3 can be attributed to the greater ability of g3 to resolve unbalanced sub mesoscale circulations and better utilize velocity observations from pioneer levin et al 2021 and iii variations in the 4d var background error and observation error covariance matrices b and r the a priori assumptions encapsulated in b and r vary from grid to grid as discussed in levin et al 2020 the parameters used to compute the observation error covariance matrix r and background error covariance matrix b are not the same on the three grids the observation error standard deviations σ o assumed for in situ temperature observations are similar across all three grids and range from 0 6 c on g1 to 0 4 c on g2 and g3 yet a posteriori analysis of the innovation statistics following the diagnostics described by desroziers et al 2005 suggests that σ o should be closer to 1 c as noted in levin et al 2020 the a priori values of σ o for in situ salinity observations were assumed to 0 2 on g1 while the a posteriori innovation statistics indicate that 0 4 is a more appropriate choice the value subsequently adopted for both g2 and g3 this is one reason why the impact of salinity observations declines from g1 to g3 for velocity measurements σ o on g1 was assumed to be 0 6 ms 1 for hf radar surface current estimates and 0 3 ms 1 for moorings but were adjusted downwards to 0 1 ms 1 and 0 04 ms 1 respectively on g2 and g3 to be more in line with the a posteriori innovation statistics the observation impacts in fig 16 are generally consistent with the metric based observation impact calculations presented by levin et al 2019 2020 2021 for the same roms configuration computed using the aforementioned adjoint approach of langland and baker 2004 more discussion about the influence of the factors mentioned above on the metric based observation impacts can be found in levin et al 2020 2021 6 degrees of freedom consider the situation where n error free observations are to be assimilated into an ocean model describing an m dimensional state space where n m in principle these observations can provide at most n independent pieces of information about the rank of the tangent linear observation operator h rodgers 2000 however in the presence of measurement errors and rounding errors in the estimation problem that contribute to ill conditioning the number of independent pieces of information will be less than n thus reducing the effective rank of h eigen analysis of the r preconditioned stabilized representer matrix p r 1 hb h t i can be used to quantify the effective rank of h by identifying the number of eigenvalues λ greater than 2 the corresponding array modes identify the sub space that is effectively informed by the observations formally this identifies the range of h and to coin a phrase from lanczos 1961 represents the part of state space that is activated by the observations the sub space orthogonal to the range is referred to as the null space as discussed by bennett 2002 the number of df of the 4d var cost function is n and is partitioned between the df of the signal and the df of the noise due to the presence of observation error the eigenvectors of p with eigenvalues λ 2 contribute most to the df of the signal while eigenvectors of p with λ 2 contribute most to the df of the noise as shown in section 5 the rams associated with the smallest eof variances λ exert the greatest control on the 4d var analyses and ensuing forecasts it is therefore important to determine whether the smaller scale circulation features associated with these rams e g fig 4g are reliable and physically relevant or whether they contribute primarily to noise in the estimate as noted above the rams can be interpreted as state space vectors that are associated with the df of either the signal or the noise that is resolved by the observing system formally if λ 2 the associated rams cannot be distinguished from observation error however bennett and mcintosh 1984 have argued for a much more conservative criterion in which fewer array modes are admitted to the analysis by rejecting those ψ for which λ i λ 1 0 01 given the practical difficulties and uncertainties in prescribing b and r for large and complex models such a strategy seems very prudent as demonstrated by mae the bennett and mcintosh 1 rule can be used to gauge the extent to which 4d var analyses may suffer from over fitting to errors in the observations following mae fig 17 shows the time series of λ 7 λ 1 during the 1st outer loop for the 4d var analyses on the three grids during the majority of cycles λ 7 λ 1 0 01 on all three grids although there are a significant number of cycles for which λ 7 λ 1 0 01 particularly on g1 and g2 suggesting that we may be dangerously close to over fitting the model to observation errors during these cycles in these cases the observations that control most ram ψ 7 may in fact be exerting an overly large impact on the analyses and forecasts the situation is better on g3 where λ 7 λ 1 exhibits a seasonal cycle with a tendency for potential over fitting during winter months while this aspect of the data assimilation system clearly warrants further attention fig 17 highlights how the rams can identify and monitor endemic issues within the system 7 summary and conclusions in this paper we have explored the properties of the mab and gom ocean observing systems using the rams of a state of the art 4d var data assimilation system in a triply nested configuration of roms central to this work are two complementary interpretations of the rams first they provide information about the df of the observing system in light of a priori assumptions about the background error covariance and the 3 dimensional structures of the rams cf fig 6 provide a clear representation of the field of view of the observing array this property of the rams has been exploited here to quantify the extent to which data assimilation reduces errors in ocean analyses and forecasts second the rams can be interpreted as interpolation patterns for the observations into state space bennett 1985 we capitalize on this exegesis of the rams to elucidate the efficacy of the resulting ocean state estimates in the present case fig 7 indicates that the rams associated with the most stable interpolation patterns on all three grids appear to be most strongly controlled by the in situ observations from the pioneer array it is these observations that will generally contribute most significantly to the df of the signal given that the in situ observing platforms are in principle relocatable an interesting future study would be to explore the extent to which the mab and gom observing system could be reconfigured and optimized to potentially provide a more complete view of the upper ocean circulation array modes have recently been used in this way by le hénaff et al 2009 and lamouroux et al 2016 to evaluate different observing system designs in the bay of biscay remote sensing observations are an essential component of the observing system and in contrast to in situ observations fig 7 suggests that they appear to control the 3 dimensional structures of the least stable rams thus uncertainties in remote sensing data are likely to be the largest contributors to uncertainties in the ocean state estimates and potentially contribute most to the df of the noise however the observation impact calculations discussed below suggest that this is not always the case a novel application of the rams presented here quantifies the extent to which ocean forecasts benefit from data assimilation this was achieved by time evolving the rams from each analysis cycle through the forecast interval and using the known covariance properties of the ram amplitudes to compute the difference between the expected forecast error covariances of forecasts with and without data assimilation namely mb m t ma m t this type of analysis reveals first hand how intimately the expected covariance properties of the forecast errors are tied to the underlying circulation in addition they quantify the extent to which information gained from data assimilation persists throughout the forecast our analyses also reveal the extent to which the observations can inform the forecast both locally and remotely through the circulation dynamics which in this study spans a variety of complex circulation regimes ranging from quasi geostrophic down to the sub mesoscale the pioneer array provides a powerful example of this last point in the present study this is graphically illustrated in fig 13 which indicates the extent to which information from the pioneer array in concert with the other elements of the observing system is conveyed to other parts of the model domain this study represents a proof of concept of the methodology and was applied to cases where forecasts were initialized from 4d var analysis computed using a single outer loop more work is required to adapt the method to the case of multiple outer loops since the ram based approach for quantifying the expected reduction in error covariance is predicated on the tangent linear approximation it is natural to inquire to what extent this approximation remains valid over the 10 day duration of the combined analysis forecast cycles employed here if δ x t denotes the state vector difference between the nonlinear model forecasts x f a t and x f b t and δ x t is the corresponding forecast difference based on the rams then the correlation between δ x t and δ x t versus lead time t provides a quantitative measure of the efficacy of the tangent linear approximation such an analysis on g1 and g2 not shown reveals a pronounced seasonal cycle with the lowest correlations during winter and peak correlations during the summer during winter the cooling of the shelf waters enhances the horizontal temperature gradients in the vicinity of the gulf stream temperature front this in turn will presumably favor faster growth of δ x and δ x via baroclinic instabilities and is most likely a major reason why the tangent linear assumption is less robust during wintertime however much of the wintertime drop in correlation can be attributed to short length scales associated with differences in localized perturbation growth of δ x t and δ x t if both are spatially low pass filtered the average correlations between them are much improved at longer forecast lead times and for surface fields at a 7 day lead time correlations are typically greater than 0 5 therefore we feel confident that the patterns of error covariance such as figs 9 and 11 provide useful information about the regions where forecasts are informed by data assimilation for periods 1 week an alternative approach for quantifying the impact of the observations on the expected analysis and forecast error covariance has also been explored here the procedure is based on the contribution of each observation to the amplitude of the least stable ram in our case ψ 7 what this of course suggests is that rams that may contribute most significantly to the df of the noise are in fact the most impactful on the analyses and forecasts the second equality in 6 shows that the ram amplitudes can be expressed as α 7 λ 7 1 d t r 1 h ψ 7 thus there are two important factors that control the contribution aka impact of different observations on α 7 for the least stable ram first the ram is sampled in observation space i e h ψ 7 therefore it is reasonable to assume that the observations that will exert the most influence on the ram structure will also have a large impact on α since h ψ 7 represents a resampling of the ram at the locations of those very same observations second the resampled ram is rescaled by the inverse observation error variances r 1 which will assign greater weight to observations with small expected errors as discussed in section 5 2 these factors weigh in to differing degrees in the results of fig 16 which shows the rms contribution of different observations to the dot product of the innovation vector d with the vector λ 7 1 r 1 h ψ 7 fig 16 indicates that in situ observations are generally as impactful as sst observations therefore while it is tempting from fig 7b d f to attribute much of the structure of the least stable ram to remote sensing observations fig 16 indicates that in situ observations contribute significantly also the most common approach used at operational centers to quantify observation impacts on analyses and forecasts is metric based and as such the observation impacts can vary across different metrics and through time conversely the alternative approach introduced here is metric independent and quantifies the impact of the observations during any phase of the analysis and forecast cycle while the tangent linear assumption remains valid since the ram amplitudes for a given assimilation cycle are time invariant the utility of the ram based approach will be evaluated in some of the near real time systems currently being run in support of u s ioos and reported later another practical application of the rams applied here is monitoring of the efficacy of the 4d var ocean state estimates specifically an a posteriori analysis of the eigenvalues of the preconditioned stabilized representer matrix associated with 4d var analyses across the three nested grids suggests that the current system configuration may be uncomfortably close to overfitting the model to errors in the observations this overfitting could potentially introduce unphysical features into the analyses and it seems likely based on fig 7 that the primary culprit is satellite observations therefore some adjustments of the near real time analysis forecast system are probably warranted even though we have employed the lenient 1 rule of bennett and mcintosh 1984 the issue of overfitting deserves further attention and should be a cautionary tale for others engaged in ocean data assimilation who may also find that they too are unknowingly flirting with the detrimental influences of observation error the rams are straightforward to compute using the archived output from the 4d var system and we have shown here that they can be a useful tool for monitoring the performance of a data assimilation system and for placing bounds on the expected errors of ensuing forecasts however it is also of interest to speculate on additional important practical applications that capitalize on the properties of the rams specifically since the rams provide flow dependent covariance information i e they are derived from the eofs of the total expected rescaled error covariance matrix they also have considerable potential utility for improving the data assimilation system itself in particular it has been demonstrated in numerical weather prediction e g lorenc et al 2015 that hybrid data assimilation approaches that combine climatological covariance information about the background errors with flow dependent information about errors of the day generally out perform systems that use this information independently hybrid approaches are also an active area of research in oceanography see moore et al 2019 the roms 4d var system falls into the first category in that the background error covariance matrix b is based on climatological information however one can imagine a hybrid approach in which b γ c b c γ a b a where b c is the standard climatological background error covariance and b a is a flow dependent background error covariance based on the rams and that varies from cycle to cycle the coefficients γ c and γ a are weights that can be determined based on theoretical considerations ménétrier and auligné 2015 for example if we let b a c and choose γ c 1 and γ a 1 then we will recover a reduced rank approximation of a although what we really require for data assimilation is ma m t as discussed in section 5 1 ensemble methods are commonly used to estimate flow dependent covariance information however due to the necessarily limited size of the ensemble some form of localization is generally required to eliminate spurious correlations which can be a computationally expensive procedure houtekamer and zhang 2016 one advantage of using the rams to construct b a is that the expected covariance properties of the ram amplitudes is known a priori cf eq 7 which circumvents the obvious need for localization since c in 7 represents the expected covariance arising from an infinite ensemble to illustrate the flow dependent information captured by c fig 18 shows the standard deviations of sss derived from c for the 4d var analyses shown in fig 1 on 16 may 2014 the richness of the field and variance information is very evident and becomes increasingly more complex as the grid resolution increases it would be next to impossible to adequately model the inhomogeneous fields like those in fig 18 using conventional approaches to b c such as diffusion operators as in section 3 therefore the rams offer a straightforward and convenient procedure for supplementing b information about the cross covariances between the different components of the state vector and associated correlation length scales is naturally embedded in c from which a hybrid approach can benefit even though b c is only weakly flow dependent 4d var is forgiving since hb h t in 2 provides implicit flow dependent covariance information it is this information that is mined by the rams and mapped to state space by b h t and which we argue we can capitalize on using the approach that we are advocating here for constructing b a with steady progress in the resolution capabilities of satellite altimeters and radiometers and the advent of new innovative mobile and adaptive observing platforms such as gliders and other autonomous underwater vehicles auvs data assimilation at the ocean sub mesoscale is a new and exciting frontier dense in situ observing systems such as the pioneer array offer extraordinary and unprecedented insight into the sub mesoscale environment synthesizing these data using ocean models and data assimilation however represents a considerable challenge from this perspective fig 18c is particularly exciting since it reveals the remarkable level of detail that can potentially be mined to develop an effective hybrid 4d var approach for roms the sub mesoscale forecast problem on g3 has not been considered here but is nonetheless important and will be the subject of a future study credit authorship contribution statement andrew m moore conceptualization methodology writing original draft writing review editing software validation formal analysis investigation resources project administration julia levin conceptualization methodology writing original draft writing review editing software validation formal analysis investigation data curation hernan g arango methodology writing original draft writing review editing software john wilkin conceptualization methodology writing original draft writing review editing software validation formal analysis investigation resources project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by grants from the national science foundation oce 1459665 and oce 1459646 nasa nnx17ah58g and noaa na16nos0120020 pioneer array data were obtained from the nsf ocean observatories initiative data portal http ooinet oceanobservatories org 
23896,the efficacy of an ocean observing analysis and forecasting system for the mid atlantic bight and the gulf of maine is explored using the concept of array modes the analysis forecast system is based on a triply nested configuration of the regional ocean modeling system roms in conjunction with 4 dimensional variational 4d var data assimilation the array modes identify the degrees of freedom df of the signal and of the noise resolved by the observations and are used here to quantify the extent to which the existing network of platforms and instruments are able to observe the ocean across different dynamical regimes ranging from quasi geostrophic through the mesoscale and down to the sub mesoscale the ocean observing system includes the u s national science foundation s ocean observatories initiative pioneer array in general it is found that the df of the signal are largely associated with in situ observations from the pioneer array on the other hand a combination of satellite remote sensing and in situ observations potentially contribute to the df of the noise associated with uncertainties in the measurements the array modes also provide information about the reduction in the expected analysis and forecast error covariance due to assimilating the observations here too observations from the pioneer array are found to significantly influence the veracity of the analyses and forecasts and the circulation is instrumental in propagating observational information to other parts of the model domain an approach is presented in which the array modes are used to quantify the impact of data assimilation on the expected forecast error covariance of forecasts initialized from the 4d var ocean state estimates the advantage of this approach over others in common use is that it is independent of forecast error norm and circumvents the need for generating potentially large and costly ensembles keywords array modes data assimilation 4d var mid atlantic bight pioneer array forecast error covariance 1 introduction regional ocean analysis and forecasting are now well established activities of many national agencies operational centers and research groups worldwide a critical component of such systems is data assimilation which aims to combine ocean observations with a model to yield an estimate of the ocean state that is more reliable than either the observations or model alone since the practice of data assimilation is deeply rooted in estimation theory it also provides an opportunity to assess the properties of the observing system itself in this study we explore the array modes of the ocean observing system in the mid atlantic bight mab and gulf of maine gom in the ne atlantic this observing system supports the u s integrated ocean observing system ioos and forms the backbone of the mid atlantic regional association coastal ocean observing system maracoos the mab is also unique in that it is home to the u s national science foundation nsf ocean observatories initiative ooi pioneer array the pioneer array has been operational since april 2014 and comprises fixed moorings and a fleet of autonomous underwater vehicles that are deployed at the continental shelf break the primary aim of the pioneer array is to increase understanding of the processes responsible for the transport of water masses across the shelf break and their relationship to atmospheric forcing on a range of time scales gawarkiewicz et al 2018 this paper is an extension of the recent studies by levin et al 2019 2020 2021 which document a detailed assessment of the impact of the mab and gom observing system on data assimilation estimates of the ocean environment and shelf break exchange processes in the vicinity of the pioneer array here the array modes of the observing system have been used to delve deeper into the degree to which the information provided by the observations constrain our knowledge of the ocean state the array modes are analogous to the characteristic modes employed in electrical engineering and antenna design array modes were first introduced in oceanography by bennett 1985 and provide information about the field of view and degrees of freedom df of an observing system as well as limitations that are endemic to the data assimilation system more recent applications of array modes in ocean data assimilation include egbert et al 1994 bennett 2002 kurapov et al 2009 and kurapov and özkan haller 2013 while modified forms of the array mode concept have been employed by le hénaff et al 2009 lamouroux et al 2016 and moore et al 2018 this study draws on the properties of and information provided by the array modes as a means of quantifying the efficacy of an ocean analysis forecast system given their central importance the concept of array modes is reviewed in section 2 in relation to 4 dimensional variational 4d var data assimilation the approach employed in this work the concept of reduced rank array modes rams is a practical variant of the array mode concept and is also introduced in section 2 the model used in this study is the regional ocean modeling system roms configured for the mab and gom in conjunction with 4d var as described in section 3 the model comprises a triply nested configuration of roms that resolves circulation scales ranging from quasi geostrophic down to the sub mesoscale the 4d var analyses at each scale are informed by observations that lead to a reduction in the expected error covariance of the resulting ocean state estimates as we will demonstrate the reduction in error covariance can be quantified by drawing on the known properties of the rams as a prelude the hallmark fingerprints of the rams of the mab and gom ocean state estimates across the range of scales captured by the model are first explored in section 4 section 5 focuses on several aspects of the impact of the observations assimilated into the model on the 4d var analyses and ensuing ocean forecasts first section 5 1 describes an alternative and cost effective ram based approach for computing the expected reduction in the analysis and forecast error covariance the spatio temporal nature of the expected error variance reduction in explored in section 5 2 in section 5 3 we demonstrate that much of the expected reduction in error covariance is associated with a single ram furthermore the contribution of each observation to the amplitude of this ram is shown to be a useful and alternative measure of the observation impacts the advantage of this approach over the more conventional adjoint based e g langland and baker 2004 or ensemble based approaches e g liu and kalnay 2008 is that it is independent of forecast error norm and generally more cost effective many of the ideas and results presented here are predicated on identification of the rams as the dfs of the observing system this information is utilized in section 6 to identify the extent to which the 4d var analyses may be overly constrained by errors and uncertainties in the observations a summary conclusions and discussion of potential applications of our work is presented in section 7 2 array modes the theory of array modes will be summarized here in terms of 4 dimensional variational 4d var data assimilation although in principle the same ideas can be applied to other linear data assimilation methodologies with this in mind we will follow standard notation and denote by x the ocean state vector comprising all grid point values of the model prognostic variables given a prior or background estimate of the state vector x b and the n 1 vector of ocean observations y o where n is the number of observations the best linear unbiased estimate or analysis x a is given by 1 x a x b k y o h x b where h is the observation operator that samples x b at the observation locations in space and time and k is the gain matrix in 4d var h includes the nonlinear model the gain can be expressed as 2 k b h t hb h t r 1 where b and r are the background error and observation error covariance matrices respectively the matrix h denotes the tangent linearization of h and in 4d var represents the tangent linear model sampled at the observation points while h t denotes the adjoint of these operations the matrix p hb h t r represents the total error covariance in the space spanned by the observations and is often referred to as the stabilized representer matrix for now let us suppose that all of the observations are of the same type e g in situ temperature observations in this case and since p is a symmetric matrix it can be factorized as w λ w t where w w i is the matrix of orthonormal eigenvectors w i and λ diag λ i are the associated eigenvalues following bennett 1985 the analysis x a in 1 can be re expressed as 3 x a x b i 1 n α i ψ i where ψ i b h t w i are referred to as the array modes with amplitudes α i λ i 1 w i t d and d y o h x b is the innovation vector similar ideas are utilized in antenna theory where it is the eigen spectrum of the impedance matrix associated with an antenna that is considered chen and wang 2015 in that case the array modes or characteristic modes are frequency dependent the extent to which a particular mode is excited by an incident electromagnetic field with frequency ω is proportional to λ i ω 1 therefore when the incident waveform frequency matches the eigenvalue of a specific array mode resonance occurs the array modes of an ocean observing system depend on b r h and in the case of 4d var x b and while dependent on the observation times and locations through h they are independent of the observation values y o however the extent to which each array mode is excited is determined by the innovations d and therefore does depend on the data values themselves in this case there is no frequency dependence of the incident signal from the innovations and the array mode amplitudes α i are proportional to λ i 1 the eigenvectors w i represent the empirical orthogonal functions eofs of the total error covariance in observation space and the eigenvalues are the variance associated with each eof since the array mode amplitudes α i λ i 1 the eofs associated with the largest uncertainty are weighted the least while the eofs that account for the least fraction of error variance are weighted the most this of course makes intuitive sense since the 4d var increments δ x i 1 n α i ψ i then draw most heavily on the array modes associated with the smallest total error variance the array modes themselves ψ i b h t w i represent the projection of the stabilized representer matrix eofs w i into state space via the adjoint operation h t in general ocean observing systems comprise observations of several state variables from a variety of different platforms also since the number of observations n is large most 4d var approaches identify the analysis given by 1 and 2 using iterative methods therefore some form of preconditioning of p is essential and without this the notion of eofs does not make sense in roms the r preconditioned stabilized representer matrix p r 1 hb h t i is factorized using the lanczos formulation of the conjugate gradient cg algorithm according to 4 p m v m t m v m t hb h t where v m v i is the matrix of lanczos vectors v i which represent the normalized cg search directions and t m v m t hb h t p v m is a symmetric positive definite tridiagonal matrix gürol et al 2014 the lanczos vectors are orthonormal according to v m t hb h t v m i m where the norm used is a result of additional restricted preconditioning by b gratton and tshimanga 2009 the subscript m represents the number of cg iterations performed referred to as inner loops in this case a set of array modes and amplitude coefficients can be defined according to 5 ψ i b h t v m φ i 6 α i λ i 1 φ i t v m t hb h t r 1 d λ i 1 ψ i t h t r 1 d where λ i φ i are the eigenpairs of t m since p m in 4 represents a reduced rank approximation of r 1 p moore et al 2018 hereafter mae refer to the ψ i as the reduced rank array modes rams to distinguish them from the full rank case originally considered by bennett 1985 it is also important to note that since 5 and 6 are based on the eigen spectrum of the preconditioned matrix r 1 p the eofs of p m must be interpreted as the vectors that account for fractions of the rescaled total variance in observation space this is also discussed by le hénaff et al 2009 who considered a similar rescaling of p by r 1 2 and refer to the associated eofs mapped back into state space as modal representers in the case considered here there will be only m rams and the analysis increment can be expressed as δ x i 1 m α i ψ i despite the differences in definition several useful complementary interpretations of the array modes and rams exist bennett 1985 notes that the array modes can be viewed as interpolation patterns from observation space to state space onto which the observations project the dependence of the weights α i on λ i 1 dictates that the analysis increments δ x will be most least sensitive to uncertainties in the measurement values that project onto array modes associated with the smallest largest eigenvalues if the eigenvalues λ i are arranged in descending order then ψ 1 represents the most stable interpolation pattern for the observations into state space conversely we should treat with caution the ψ i associated with small eigenvalues since these may introduce non physical noise into the analysis viewed another way the eigen spectrum provides information about the degrees of freedom df for the signal resolved by the observing array and the df for the noise associated with uncertainties in the observations due to measurement errors and errors of representativeness rodgers 2000 since r 1 hb h t and p have the same eigenvectors and their eigenvalues differ by 1 the number of eigenvalues for which λ i 2 provides a measure of the effective number of df of the signal that is resolved by the observing array array modes associated with λ i 2 will be indistinguishable from errors or uncertainties in the observations i e the df of the noise in the data and should be rejected 1 1 from the definition of p m the eigenvalues λ i 1 and the lower bound would correspond to the situation where r 1 hb h t is singular these ideas have been applied by bennett and mcintosh 1984 le hénaff et al 2009 and mae to ocean observing systems and will be revisited in section 6 where a more conservative practical criterion is enforced instead of λ i 2 to prevent overfitting to observation errors furthermore since the expected covariance properties of the array mode amplitudes are known a priori identifying the array modes with the df provides a useful framework for quantifying the impact of data assimilation on ocean forecasts these ideas are exploited in section 5 3 model configuration the roms configuration employed in this study spans the mab and the gom three telescoping nesting layers were used and the geographical extent of each nested grid is shown in fig 1 the model configuration has been described in detail elsewhere levin et al 2019 2020 2021 so only a brief description will be given here the horizontal resolution of the three grids is 7 km grid g1 2 4 km grid g2 and 0 8 km grid g3 respectively in all grids there are 40 terrain following levels stretched so that the thickness of the surface most layers is in the range 0 1 1 8 m and 0 1 3 4 m near the bottom over the continental shelf the innermost refined grid g3 is centered on the nsf ooi pioneer array the g1 open boundaries were constrained using data from the mercator océan real time global analyses lellouche et al 2018 that were adjusted to remove a seasonal bias by comparing with the local regional climatology derived by fleming 2016 corrections for bias were also made to the g1 open boundary mean dynamic topography and seasonal cycle of sea surface height ssh using a regional data assimilative climatological analysis as described by levin et al 2018 and wilkin et al 2018 surface fluxes of momentum heat and freshwater were derived from 3 hourly national centers for environmental prediction ncep north american mesoscale nam fields using the standard bulk formulae of fairall et al 2003 application of the nam atmospheric pressure drives an oceanic dynamic inverted barometer response daily freshwater discharge from 22 rivers was imposed based on gauge observations from the u s geological survey and water survey of canada lópez et al 2020 wilkin et al 2018 all three grids can be run using one or two way nesting which provides appropriate boundary conditions for g2 and g3 the data assimilation system used is the dual formulation of roms 4d var moore et al 2011a gürol et al 2014 a full description of the 4d var system and its configuration can be found in levin et al 2018 2019 2020 2021 and wilkin et al 2018 so only a very brief summary of the salient points is presented here the data assimilated span the period jan 2014 dec 2017 and are summarized in table 1 from levin et al 2020 at the time that these calculations were performed the roms 4d var system did not function across one or two way nested configurations so the following strategy was adopted to assimilate the available observations into the three grids i data were first assimilated into g1 for the full 2014 2017 period using a 3 day assimilation window in this case the model initial conditions surface forcing and open boundary conditions were treated as control variables the background state estimate for each 3 day window was taken to be the analysis at the end of the previous cycle ii step i was repeated for grid g2 using the 4d var analyses from each cycle of g1 as the background open boundary conditions for each 4d var cycle of g2 as in g1 the control variables were the initial conditions surface forcing and open boundary conditions iii step ii was then repeated for grid g3 however in this case the data assimilation window was reduced to 1 day with only the initial conditions and open boundary conditions used as control variables the 4d var analyses from each cycle of g2 were used as the background open boundary conditions for each 4d var cycle of g3 also because of the considerable increase in computational effort 4d var was only run on g3 for the period jan 2014 dec 2015 as discussed in moore et al 2011a the background error covariance b matrix in roms is modeled following the diffusion operator approach of weaver and courtier 2001 table 2 summarizes the decorrelation length scales assumed in b for errors in each control variable on the three model grids used here and these parameter choices are discussed in levin et al 2019 the observation error covariance matrix r was assumed to be a diagonal matrix and table 1 summarizes the errors and uncertainties that were assigned to measurements from each observing platform these errors reflect a combination of measurement error and errors of representativeness i e uncertainties associated with the ability of the model grid to resolve all of the processes that are captured by the observations and are also discussed in levin et al 2019 following andersson and järvinen 1999 quality control was performed during each 4d var cycle specifically the innovation d i associated with each observation is compared to the standard error based on the assumed standard deviations of the background σ b and observation σ o errors for a chosen threshold γ an observation is rejected and not included in the analysis if d i 2 γ 2 σ b 2 σ o 2 the thresholds γ depend on the type of observation and are given in table 2 for the analyses on each grid considered here time series of the total number of observations assimilated into the model on each grid after the formation of super observations during a 4d var cycle are shown in fig 2 also shown are time series of the number of observations from each observing platform the number of observations from each platform is similar across all three grids apart from satellite altimetry the number of altimeter overpasses decreases dramatically going from g1 to g3 due to the reduced geographical extent of each nested grid the performance of the 4d var system on each of the three grids has been documented in detail by levin et al 2019 2020 2021 suffice to say the system performs well across all three grids and interested readers are encouraged to refer to these previous studies for more details an example 4d var analysis from each grid is illustrated in fig 1 which shows sea surface salinity on 16 may 2014 at this time a streamer of saline water associated with a large gulf stream eddy can be seen impinging on the shelf an event that has been studied in detail by zhang and gawarkiewicz 2015 fig 1 shows very clearly how the 4d var circulation estimates can capture the range of scales from quasi geostrophic down to the sub mesoscale secondary circulations as the grid resolution increases 4 reduced rank array modes the incremental 4d var procedure outlined in section 2 is equivalent to minimizing a cost function that represents the squared difference between x a and x b and the observations and x a evaluated at the space time observation locations weighted by the inverse background error and observation error covariance matrices respectively e g courtier et al 1994 the desired best linear unbiased estimate is given by 1 and as outlined in section 2 can be identified using cg methods via a sequence of inner loop iterations in keeping with the usual practice the incremental formulation of 4d var adopted in roms also employs an outer loop and the ocean state about which h and h t are linearized is updated after every m inner loops the roms 4d var analyses described in section 3 were computed using two outer loops and seven inner loops in which case x a x b δ x 1 δ x 2 where the subscript refers to the contribution from each outer loop in this case each δ x can be expanded in terms of the rams appropriate for the outer loop under consideration levin et al 2020 showed that it is in the 1st outer loop that increments are largest and where the observations have the greatest impact on the final analysis therefore in the sequel we will focus on the rams of the first outer loop for the 4d var analyses of section 3 in addition we will demonstrate in section 5 how the rams can be used to quantify the influence of data assimilation on the expected forecast errors and there for mathematical convenience we will focus on a single 4d var outer loop following 5 and 6 and recalling that m 7 for the 4d var analyses considered here there will be seven rams for each outer loop fig 3 shows time series of α 1 and α 7 the absolute value of the amplitudes of the rams associated with the largest eigenvalue λ 1 and smallest eigenvalue λ 7 of p m for the 1st outer loop of each 4d var cycle in the three grids on average α 7 α 1 varies in the range 10 103 although there are some cycles where the ratio can be as high as 105 therefore errors and uncertainties in the innovations d that project onto ram ψ 7 will have considerably more influence on the analysis increments than those that project onto ram ψ 1 to illustrate the typical structure of the rams fig 4 shows the sea surface temperature sst associated with each ram of the 1st outer loop for the 3 day 4d var cycle spanning the interval 17 19 sept 2015 on g2 specifically fig 4 shows the ram sst on 17 sept at the beginning of the assimilation cycle rams ψ 1 fig 4a ψ 2 fig 4b and ψ 3 fig 4c have largest amplitude in the vicinity of the pioneer array conversely rams ψ 4 through ψ 7 generally have more complicated sst structures that span a larger portion of the model domain the eigenvalue spectrum of p m for this 4d var cycle is shown in fig 4h and spans about two orders of magnitude it is important to reiterate that the rams do not depend on the observation values only on their locations in concert with the background state and the error covariances to illustrate this aspect of the rams fig 4i shows the structure of φ 1 and φ 7 the leading and trailing eigenvectors of the lanczos decomposition of the inner loop iterations during the same g2 assimilation cycle according to 5 the leading and trailing rams are given by ψ 1 b h t v m φ 1 and ψ 7 b h t v m φ 7 representing the projection of φ 1 and φ 7 in fig 4i into state space specifically the elements of φ 1 and φ 7 represent weights for the lanczos vectors that form the columns of v m the weighted sums of the lanczos vectors v m φ 1 and v m φ 7 are then mapped into state space by the adjoint observation operator h t which in 4d var involves an integration backwards in time after the mapping into state space the state vector is multiplied by the background error covariance b which has the effect of smoothing the fields according to the decorrelation length scales assumed in table 2 the smooth nature of the ram sst in figs 4a and 4g is very evident for these two modes during the 17 19 sept 2015 4d var cycle the most abundant observations are sst from several different platforms figs 5a c show the innovations associated with three separate overpasses of the avhrr instrument on different days while figs 5d f show the elements of v m φ 1 associated with the same observations there is little or no correspondence between the innovations in fig 5a c and the image of φ 1 mapped into state space shown in figs 5d f however the sst structure of the associated ram ψ 1 cf fig 4a is already evident in figs 5d f conversely figs 5g i show the elements of v m φ 7 at the avhrr observation locations and some features of ram ψ 7 cf fig 4g are already apparent thus the absence of any general correspondence between the ram structures and the innovations further illustrates the their independence from the observation values the structure of ram ψ 1 appears to be closely aligned with the location of the in situ observations for example fig 6 shows the 3 dimensional temperature structure of ψ 1 on 17 sept 2015 on g2 in the vicinity of the pioneer mooring array and the region of the ocean informed by the ram appears to be closely aligned with some of the in situ mooring observations the same is true for the other state vector components of ram ψ 1 not shown the contribution of each ram to the 4d var increment is given by α i ψ i and according to fig 3 it is anticipated that ψ 7 will contribute the most this is confirmed in fig 7 which shows the root mean square rms of sst averaged over all 4d var cycles on each grid associated with ψ 1 and ψ 7 ram ψ 7 clearly dominates which is also the case at depth and for other fields not shown thus much of the detailed structure in the analysis increments is associated with the trailing rams it is important to note that while the φ i are orthogonal the rams are not so fig 7 cannot be interpreted as the contribution of ψ 1 and ψ 7 to the total sst variance of the increments it is important to note that the rams comprise components that are associated with all elements of the 4d var control vector therefore in the case of g1 and g2 this includes fields of surface flux forcing and for the open boundary conditions in the case of all three grids in the interest of brevity we will not discuss these additional components of the rams here taken together figs 4 and 7 indicate that the leading rams of g2 appear to be primarily associated with the pioneer array in contrast the trailing rams span most of the model domain and are largely controlled by the location of remote sensing footprints the rams of g1 also confirm this picture not shown however as demonstrated in section 6 this view is an oversimplification 5 the impact of data assimilation on expected forecast uncertainty 5 1 analysis and forecast error covariance as discussed in section 2 the rams can be interpreted as interpolation patterns for the innovations into state space and provide quantitative information about the sensitivity of the analysis increments to measurement errors and errors of representation see also section 6 this idea can be exploited further to quantify the expected errors in analyses and subsequent forecasts that arise from uncertainties in the innovations the analysis increment at the beginning of the 4d var analysis cycle is given by δ x kd i 1 m α i ψ i therefore uncertainty in the observations and background will manifest as uncertainties in the innovation vector d which enter through the 2nd equality as uncertainties in α i according to 6 for the best linear unbiased estimate e δ x 0 where e is the expectation operator similarly the expected covariance of the increments associated with uncertainties in d is given by c e δ x δ x t k e d d t k t b a where a is the expected analysis error covariance daley 1991 2 2 from the expected covariance of the innovation vector c k e d d t k t k hb h t r k t and using 2 c khb the analysis error covariance matrix a i kh b which shows that a b for any norm and furthermore b a khb c hence c represents the reduction in the background error covariance due to assimilating the observations since a b for any norm see footnote 2 expressing δ x in terms of the rams it can be shown that 3 3 the increment δ x i 1 m α i ψ i so that c e δ x δ x t e i 1 m j 1 m α i α j ψ i ψ j t i 1 m j 1 m e α i α j ψ i ψ j t using 6 e α i α j λ i 1 λ j 1 φ i t v m t hb h t r 1 k e d d t r 1 hb h t v m φ j from 4 e d d t r v m t m v m t hb h t and recalling that v m t hb h t v m i m we can write e α i α j λ i 1 λ j 1 φ i t t m v m t hb h t r 1 hb h t v m φ j recall that t m φ i λ i φ i and that the symmetric tridiagonal matrix t m v m t hb h t r 1 hb h t i v m in which case e α i α j λ j 1 φ i t t m i m φ j λ j 1 λ i 1 φ i t φ j λ j 1 λ i 1 δ i j where δ i j is the kronecker delta function therefore e α i α j 1 λ i 1 δ i j and c i 1 m 1 λ i 1 ψ i ψ i t 7 c i 1 m 1 λ i 1 ψ i ψ i t using the expected covariance properties of d at the end of the 4d var analysis window the analysis increment is well approximated by m δ x where m represents the tangent linear model linearized about the time evolving background state vector therefore 7 can also be used to compute the expected error variance mc m t mb m t ma m t at the end of the analysis cycle by merely replacing each ram ψ i in 7 by the time evolved rams m ψ i during the analysis cycle the surface forcing and open boundary condition components of ψ i are also used as inputs for m the 4d var analysis at the end of the assimilation window is commonly used as the initial condition for a forecast similarly the 4d var increments δ x can be propagated into the forecast interval using m linearized about the 4d var background also extended into the forecast interval and subject to the appropriate forecast boundary conditions and surface forcing while an estimate of the expected analysis error covariance matrix ma m t at the end of the analysis cycle is desirable it is challenging to compute in 4d var e g fisher and courtier 1995 ngodock et al 2020 moore and arango 2021 one reason being that the contribution of the background error covariance mb m t is computationally demanding to calculate reduced rank estimates are one approach although they tend to underestimate the analysis errors fisher and courtier 1995 moore et al 2011b ngodock et al 2020 have demonstrated a monte carlo approach for estimating a based on computing an ensemble of 4d var analyses by perturbing the so called representer coefficients β hb h t r 1 d this approach is related to that used here however in our case we capitalize on the known covariance properties of the ram amplitudes in 7 without the need to compute an explicit ensemble of analyses and forecasts in the case of the expected forecast error covariance some additional computational cost is involved since each ram or equivalently each lanczos vector v i must be propagated to the end of the forecast interval using the tangent linear model m since the computational cost of m in roms is about 50 more than a run of the nonlinear forecast model then for forecast lead times that are similar in length to the assimilation window the additional calculations required are 1 5 m where m is the number inner loops and of course the number of rams in the examples here m 7 so the additional computational burden is equivalent to running an o 10 ensemble of forecasts this is much smaller than the ensemble size required to estimate ma m t using say the randomization method of fisher and courtier 1995 in which a sample size 5000 would be needed to yield an estimate of the leading diagonal accurate to 1 to illustrate the utility of our approach consider the two forecasts illustrated schematically in fig 8 forecast x f a t is initialized from the analysis x a 0 at the end of each 4d var analysis cycle while forecast x f b t is initialized from the background x b τ of the 4d var cycle the forecast x f a t therefore benefits from the data that were assimilated during the 4d var cycle spanning the interval τ 0 while forecast x f b t does not the expected forecast error covariances of x f a t and x f b t are given by m τ t a m t t τ and m τ t b m t t τ respectively where the ordering of the time arguments indicates the direction of integration therefore m τ t c m t t τ m τ t b m t t τ m τ t a m t t τ represents the change in the forecast error covariance associated with assimilating the observations since the covariance information is propagated using m τ t the resulting covariance matrices represent 1st order approximations following this approach forecasts were initialized at the end of the 1st outer loop of each 3 day 4d var cycle on g1 and g2 respectively for the period 1 jan 31 dec 2015 the forecast duration was 10 days for g1 and 7 days for g2 a longer forecast interval was used in g1 because of the larger geographical extent of the model domain for such extended forecast periods true forecast fields are not available for the ncep nam surface forcing therefore for convenience a best time series concatenation of the 1 day forecast ncep nam meteorology and mercator océan open boundary data were used during these forecast experiments in the near real time maracoos system for g1 true 3 day forecast products from the respective operational centers are employed figs 9a c show an example sst forecast for x f b using g2 for the period 20 27 september 2015 the forecast initial condition for sst is shown in fig 9a where a large warm core gulf stream ring dominates the circulation during the subsequent 7 days of the forecast the ring circulation evolves and a filament of cooler fresher water circulates anticyclonically around the eastern and southern margins of the eddy figs 9b and 9c while a train of secondary instabilities develops along the ring s northern edge fig 9d o show the square root of the diagonal elements of m τ t c m t t τ computed using 7 for brevity we will refer to these as the standard deviations but recognize of course that the square root of the difference in the background error and analysis error variances does not represent the difference between the corresponding standard deviations the standard deviations are shown in figs 9d o for sst sea surface salinity sss sea surface height ssh and surface current speed since m τ t c m t t τ represents the difference between the background error covariance and the expected analysis error covariance due to assimilating the observations figs 9d o indicate the reduction in the expected analysis and forecast error variance due to assimilating observations during the 4d var cycle spanning 17 19 september the influence of the circulation is very evident in fig 9 especially in the case of sst figs 9d e f and sss figs 9g h i in particular data assimilation reduces the expected error in the anticyclonic filament of cooler fresher water by 1 c sst and 0 5 sss the expected reductions in error standard deviation in ssh figs 9j k l and surface current speeds figs 9m n o are more modest and are 2 cm and 15 cm s 1 respectively the forecast differences x f a x f b computed from the nonlinear model forecasts for sst and sss for the same period are shown in fig 10 and are fairly large for example fig 10 indicates that with the benefit of data assimilation the forecast initialized from x a generally leads to an increase in temperature and salinity of the filament that is advected anticyclonically around the eddy therefore the corresponding features in fig 9 represent a reduction in the standard deviation of the forecast error associated with these changes regions in fig 9 where the standard deviations are close to zero correspond to locations and fields where data assimilation has little impact on the expected forecast error variance and have an error variance similar to the background this does not mean that the forecast is not accurate only that the forecast is not benefiting in any significant way from the most recent data assimilated into the model the vertical structure of the forecast temperature and salinity along 70 5 w over the upper 250 m of the water column is shown in figs 11a and 11c respectively on forecast day 7 the signature of the warm core ring is very evident with the main thermocline reaching depths 150 m in the core of the ring fig 11a the signature of the cooler lower salinity filament at the southern edge of the eddy is also visible near 39 n figs 11b and 11d show the corresponding vertical structure of the temperature and salinity standard deviations computed from m τ t c m t t τ for the same forecast day elevated values of expected error reduction are prevalent on the continental shelf and following the cooler lower salinity filament and extend to depths below 500 m not shown 5 2 spatio temporal variations the reduction in total expected forecast error variance due to assimilating the observations is t r m τ t c m t t τ which can be thought of as the squared distance between x f b t and x f a t in fig 8 similarly the reduction in total variance associated with a particular forecast variable is given by the appropriate sub trace of m τ t c m t t τ the reduction in total variance at forecast time t relative to that the beginning of the forecast cycle provides a measure of the change in the distance between the forecasts x f b t and x f a t with this in mind fig 12 shows time series of μ t log 10 t r m τ t c m t t τ t r m τ 0 c m t 0 τ versus forecast lead time t cf fig 8 for the sub trace associated with all temperature grid points thus μ t can be viewed as an index of the change in the relative distance between the forecasts time series are presented for all 2015 forecast cycles on g1 and g2 instances for which the relative distance μ t 0 represent forecast times for which the sub trace variance at time t is higher than that of the forecast initial condition at time 0 cf fig 8 in other words the temperature forecasts of x f b t and x f a t are farther apart these situations correspond to cases where the total temperature variance of m τ t b m t t τ m τ t a m t t τ increases with forecast lead time meaning that the forecasts x f a t and x f b t are diverging through time and indicate a persistent benefit of data assimilation for x f a t conversely when the relative distance μ t 0 the squared distance between the forecasts x f a t and x f b t is decreasing over time and the benefits of data assimilation for x f a t are being slowly lost fig 12 indicates that for most forecast cycles and lead times the relative distance μ t 0 and the benefits of data assimilation during the 4d var analysis cycles extends throughout the forecast interval this is particularly true for g1 fig 12a for g2 fig 12b there are more instances when μ t 0 showing more cases when x f a t and x f b t converge in this domain time series of μ t for other state vector fields display similar behavior yet for ssh and velocity the number of instances when μ t 0 is generally higher than for temperature not shown the difference in behavior between g1 and g2 can be understood in terms of the relative horizontal resolution of the two grids while g2 better resolves the mesoscale instabilities than g1 g2 is more susceptible to the growth of forecast errors due to the more energetic higher rossby number circulation being inherently less predictable to the added nonlinearity we would expect the forecast skill of g2 to degrade on shorter time scales than in g1 figs 13a d show the square root of the mean variance associated with sst and sss analyses and 7 day forecasts computed from the average of m τ t c m t t τ over all analysis forecast cycles for 2015 in g2 large reductions in expected error occur in the vicinity of the pioneer array and are associated mainly with the observations collected by the array however there are significant reductions in the expected error farther afield clearly related to persistent circulation features such as the equatorward shelf break jet in the mab the decrease in error can be as large as 3 c in sst and 0 5 in salinity also figs 13e h show the square root of the mean variance for analyses and 10 day forecasts in g1 the expected error reduction in the g1 4d var analyses and forecasts are quantitatively similar to those in g2 and again highlight the local influence of the pioneer array the larger scale circulation effect is also evident in g1 with significant reductions in the expected error extending toward georges bank because 4d var propagates information dynamically upstream to the source region of waters that subsequently flow through the pioneer array 5 3 observation impacts on analysis and forecast error covariance much of the impact of data assimilation on the forecast state given by x f a t x f b t can be attributed to the ram associated with the smallest eigenvalue to illustrate consider again the g2 forecast illustrated in fig 10 for the period 20 27 september 2015 figs 14a and 14b show the sst and sss components of x f a t x f b t associated with ram ψ 7 i e α 7 m τ t ψ 7 on forecast day 7 and are very similar to figs 10c and 10f therefore much can be learned about the impact of data assimilation on the expected forecast errors from ψ 7 alone furthermore the good agreement between figs 14ab and figs 10cf confirms that the tangent linear approximation remains valid over this timescale according to 6 the amplitude of ram ψ 7 is given by the dot product of the innovation vector d with the vector λ 7 1 r 1 hb h t v m φ 7 therefore the contribution of each observation to α 7 can be quantified the forecast differences shown in figs 14a and 14b result from assimilating observations during the preceding 4d var analysis cycle spanning the period 17 19 sept fig 14c shows the contribution of each observation type assimilated during this period to the ram amplitude α 7 the largest contribution is from satellite sst although in situ velocity measurements from the pioneer array moorings are a close second the contribution of in situ temperature observations mainly from pioneer array moorings and gliders are also significant the location of the in situ observations during this 4d var cycle are indicated in fig 6 thus the partitioning of the amplitude of the dominant ram across the different observing platforms is a useful and alternative approach for quantifying the impact of the assimilated observations on the forecast the standard method for quantifying observation impacts in roms follows the adjoint approach of langland and baker 2004 where the impact of each observation on a chosen analysis or forecast metric is computed while this approach is generally quite efficient it requires separate calculations for each metric and forecast lead time the alternative approach that we are advocating here however is independent of any metric and forecast lead time as noted in section 2 the rams depend on the observation locations according to 5 and not on the observation values specifically ψ i b h t v m φ i where the matrix vector product v m φ i represents an eof of the r preconditioned stabilized representer matrix in the usual way these eofs provide information about the in phase and out of phase relationships between various fractions of the total error variance at different observation locations since 5 is a linear equation each ram can be expressed as the linear superposition of the contribution from each element of v m φ i corresponding to specific observations fig 15 shows the contribution of the v m φ 7 eof from information associated with the location of satellite sst in situ temperature and in situ velocity observations to the 7 day sst forecast differences of x f a t x f b t for g2 on 27 sep 2005 in keeping with fig 14 the contributions of eof information at ssh hfr radar and in situ salinity observation locations are small so are not shown consistent with fig 14a the sst component of ψ 7 in fig 15a accounts for much of the forecast change in sst to the north of the warm core ring and on the continental shelf the contributions of in situ temperature fig 15b and velocity fig 15c are mainly in opposition while around the margins of the ring these contributions reinforce each other thus while there are considerable and in some cases opposing overlaps between the contributions of different observation types to ψ 7 the general behavior in fig 15 confirms the observation impacts computed from the ram amplitude α 7 which depends directly on the measurement values with this in mind fig 16 shows the root mean square rms contribution of each observation type to the ram ψ 7 amplitude α 7 on each of the three grids the period considered is jan 2014 dec 2015 which is the overlapping period for which 4d var analyses were computed for all three grids fig 16 indicates that for a given observation type this measure of the observation impact varies considerably across the three grids these grid to grid variations are controlled by several factors that include i variations associated with differences in data coverage for example the number of along track satellite altimeter overpasses decreases dramatically going from g1 to g2 with very few tracks passing over g3 ii variations in horizontal resolution for example the increase in the impact of in situ velocity observations going from g1 to g3 can be attributed to the greater ability of g3 to resolve unbalanced sub mesoscale circulations and better utilize velocity observations from pioneer levin et al 2021 and iii variations in the 4d var background error and observation error covariance matrices b and r the a priori assumptions encapsulated in b and r vary from grid to grid as discussed in levin et al 2020 the parameters used to compute the observation error covariance matrix r and background error covariance matrix b are not the same on the three grids the observation error standard deviations σ o assumed for in situ temperature observations are similar across all three grids and range from 0 6 c on g1 to 0 4 c on g2 and g3 yet a posteriori analysis of the innovation statistics following the diagnostics described by desroziers et al 2005 suggests that σ o should be closer to 1 c as noted in levin et al 2020 the a priori values of σ o for in situ salinity observations were assumed to 0 2 on g1 while the a posteriori innovation statistics indicate that 0 4 is a more appropriate choice the value subsequently adopted for both g2 and g3 this is one reason why the impact of salinity observations declines from g1 to g3 for velocity measurements σ o on g1 was assumed to be 0 6 ms 1 for hf radar surface current estimates and 0 3 ms 1 for moorings but were adjusted downwards to 0 1 ms 1 and 0 04 ms 1 respectively on g2 and g3 to be more in line with the a posteriori innovation statistics the observation impacts in fig 16 are generally consistent with the metric based observation impact calculations presented by levin et al 2019 2020 2021 for the same roms configuration computed using the aforementioned adjoint approach of langland and baker 2004 more discussion about the influence of the factors mentioned above on the metric based observation impacts can be found in levin et al 2020 2021 6 degrees of freedom consider the situation where n error free observations are to be assimilated into an ocean model describing an m dimensional state space where n m in principle these observations can provide at most n independent pieces of information about the rank of the tangent linear observation operator h rodgers 2000 however in the presence of measurement errors and rounding errors in the estimation problem that contribute to ill conditioning the number of independent pieces of information will be less than n thus reducing the effective rank of h eigen analysis of the r preconditioned stabilized representer matrix p r 1 hb h t i can be used to quantify the effective rank of h by identifying the number of eigenvalues λ greater than 2 the corresponding array modes identify the sub space that is effectively informed by the observations formally this identifies the range of h and to coin a phrase from lanczos 1961 represents the part of state space that is activated by the observations the sub space orthogonal to the range is referred to as the null space as discussed by bennett 2002 the number of df of the 4d var cost function is n and is partitioned between the df of the signal and the df of the noise due to the presence of observation error the eigenvectors of p with eigenvalues λ 2 contribute most to the df of the signal while eigenvectors of p with λ 2 contribute most to the df of the noise as shown in section 5 the rams associated with the smallest eof variances λ exert the greatest control on the 4d var analyses and ensuing forecasts it is therefore important to determine whether the smaller scale circulation features associated with these rams e g fig 4g are reliable and physically relevant or whether they contribute primarily to noise in the estimate as noted above the rams can be interpreted as state space vectors that are associated with the df of either the signal or the noise that is resolved by the observing system formally if λ 2 the associated rams cannot be distinguished from observation error however bennett and mcintosh 1984 have argued for a much more conservative criterion in which fewer array modes are admitted to the analysis by rejecting those ψ for which λ i λ 1 0 01 given the practical difficulties and uncertainties in prescribing b and r for large and complex models such a strategy seems very prudent as demonstrated by mae the bennett and mcintosh 1 rule can be used to gauge the extent to which 4d var analyses may suffer from over fitting to errors in the observations following mae fig 17 shows the time series of λ 7 λ 1 during the 1st outer loop for the 4d var analyses on the three grids during the majority of cycles λ 7 λ 1 0 01 on all three grids although there are a significant number of cycles for which λ 7 λ 1 0 01 particularly on g1 and g2 suggesting that we may be dangerously close to over fitting the model to observation errors during these cycles in these cases the observations that control most ram ψ 7 may in fact be exerting an overly large impact on the analyses and forecasts the situation is better on g3 where λ 7 λ 1 exhibits a seasonal cycle with a tendency for potential over fitting during winter months while this aspect of the data assimilation system clearly warrants further attention fig 17 highlights how the rams can identify and monitor endemic issues within the system 7 summary and conclusions in this paper we have explored the properties of the mab and gom ocean observing systems using the rams of a state of the art 4d var data assimilation system in a triply nested configuration of roms central to this work are two complementary interpretations of the rams first they provide information about the df of the observing system in light of a priori assumptions about the background error covariance and the 3 dimensional structures of the rams cf fig 6 provide a clear representation of the field of view of the observing array this property of the rams has been exploited here to quantify the extent to which data assimilation reduces errors in ocean analyses and forecasts second the rams can be interpreted as interpolation patterns for the observations into state space bennett 1985 we capitalize on this exegesis of the rams to elucidate the efficacy of the resulting ocean state estimates in the present case fig 7 indicates that the rams associated with the most stable interpolation patterns on all three grids appear to be most strongly controlled by the in situ observations from the pioneer array it is these observations that will generally contribute most significantly to the df of the signal given that the in situ observing platforms are in principle relocatable an interesting future study would be to explore the extent to which the mab and gom observing system could be reconfigured and optimized to potentially provide a more complete view of the upper ocean circulation array modes have recently been used in this way by le hénaff et al 2009 and lamouroux et al 2016 to evaluate different observing system designs in the bay of biscay remote sensing observations are an essential component of the observing system and in contrast to in situ observations fig 7 suggests that they appear to control the 3 dimensional structures of the least stable rams thus uncertainties in remote sensing data are likely to be the largest contributors to uncertainties in the ocean state estimates and potentially contribute most to the df of the noise however the observation impact calculations discussed below suggest that this is not always the case a novel application of the rams presented here quantifies the extent to which ocean forecasts benefit from data assimilation this was achieved by time evolving the rams from each analysis cycle through the forecast interval and using the known covariance properties of the ram amplitudes to compute the difference between the expected forecast error covariances of forecasts with and without data assimilation namely mb m t ma m t this type of analysis reveals first hand how intimately the expected covariance properties of the forecast errors are tied to the underlying circulation in addition they quantify the extent to which information gained from data assimilation persists throughout the forecast our analyses also reveal the extent to which the observations can inform the forecast both locally and remotely through the circulation dynamics which in this study spans a variety of complex circulation regimes ranging from quasi geostrophic down to the sub mesoscale the pioneer array provides a powerful example of this last point in the present study this is graphically illustrated in fig 13 which indicates the extent to which information from the pioneer array in concert with the other elements of the observing system is conveyed to other parts of the model domain this study represents a proof of concept of the methodology and was applied to cases where forecasts were initialized from 4d var analysis computed using a single outer loop more work is required to adapt the method to the case of multiple outer loops since the ram based approach for quantifying the expected reduction in error covariance is predicated on the tangent linear approximation it is natural to inquire to what extent this approximation remains valid over the 10 day duration of the combined analysis forecast cycles employed here if δ x t denotes the state vector difference between the nonlinear model forecasts x f a t and x f b t and δ x t is the corresponding forecast difference based on the rams then the correlation between δ x t and δ x t versus lead time t provides a quantitative measure of the efficacy of the tangent linear approximation such an analysis on g1 and g2 not shown reveals a pronounced seasonal cycle with the lowest correlations during winter and peak correlations during the summer during winter the cooling of the shelf waters enhances the horizontal temperature gradients in the vicinity of the gulf stream temperature front this in turn will presumably favor faster growth of δ x and δ x via baroclinic instabilities and is most likely a major reason why the tangent linear assumption is less robust during wintertime however much of the wintertime drop in correlation can be attributed to short length scales associated with differences in localized perturbation growth of δ x t and δ x t if both are spatially low pass filtered the average correlations between them are much improved at longer forecast lead times and for surface fields at a 7 day lead time correlations are typically greater than 0 5 therefore we feel confident that the patterns of error covariance such as figs 9 and 11 provide useful information about the regions where forecasts are informed by data assimilation for periods 1 week an alternative approach for quantifying the impact of the observations on the expected analysis and forecast error covariance has also been explored here the procedure is based on the contribution of each observation to the amplitude of the least stable ram in our case ψ 7 what this of course suggests is that rams that may contribute most significantly to the df of the noise are in fact the most impactful on the analyses and forecasts the second equality in 6 shows that the ram amplitudes can be expressed as α 7 λ 7 1 d t r 1 h ψ 7 thus there are two important factors that control the contribution aka impact of different observations on α 7 for the least stable ram first the ram is sampled in observation space i e h ψ 7 therefore it is reasonable to assume that the observations that will exert the most influence on the ram structure will also have a large impact on α since h ψ 7 represents a resampling of the ram at the locations of those very same observations second the resampled ram is rescaled by the inverse observation error variances r 1 which will assign greater weight to observations with small expected errors as discussed in section 5 2 these factors weigh in to differing degrees in the results of fig 16 which shows the rms contribution of different observations to the dot product of the innovation vector d with the vector λ 7 1 r 1 h ψ 7 fig 16 indicates that in situ observations are generally as impactful as sst observations therefore while it is tempting from fig 7b d f to attribute much of the structure of the least stable ram to remote sensing observations fig 16 indicates that in situ observations contribute significantly also the most common approach used at operational centers to quantify observation impacts on analyses and forecasts is metric based and as such the observation impacts can vary across different metrics and through time conversely the alternative approach introduced here is metric independent and quantifies the impact of the observations during any phase of the analysis and forecast cycle while the tangent linear assumption remains valid since the ram amplitudes for a given assimilation cycle are time invariant the utility of the ram based approach will be evaluated in some of the near real time systems currently being run in support of u s ioos and reported later another practical application of the rams applied here is monitoring of the efficacy of the 4d var ocean state estimates specifically an a posteriori analysis of the eigenvalues of the preconditioned stabilized representer matrix associated with 4d var analyses across the three nested grids suggests that the current system configuration may be uncomfortably close to overfitting the model to errors in the observations this overfitting could potentially introduce unphysical features into the analyses and it seems likely based on fig 7 that the primary culprit is satellite observations therefore some adjustments of the near real time analysis forecast system are probably warranted even though we have employed the lenient 1 rule of bennett and mcintosh 1984 the issue of overfitting deserves further attention and should be a cautionary tale for others engaged in ocean data assimilation who may also find that they too are unknowingly flirting with the detrimental influences of observation error the rams are straightforward to compute using the archived output from the 4d var system and we have shown here that they can be a useful tool for monitoring the performance of a data assimilation system and for placing bounds on the expected errors of ensuing forecasts however it is also of interest to speculate on additional important practical applications that capitalize on the properties of the rams specifically since the rams provide flow dependent covariance information i e they are derived from the eofs of the total expected rescaled error covariance matrix they also have considerable potential utility for improving the data assimilation system itself in particular it has been demonstrated in numerical weather prediction e g lorenc et al 2015 that hybrid data assimilation approaches that combine climatological covariance information about the background errors with flow dependent information about errors of the day generally out perform systems that use this information independently hybrid approaches are also an active area of research in oceanography see moore et al 2019 the roms 4d var system falls into the first category in that the background error covariance matrix b is based on climatological information however one can imagine a hybrid approach in which b γ c b c γ a b a where b c is the standard climatological background error covariance and b a is a flow dependent background error covariance based on the rams and that varies from cycle to cycle the coefficients γ c and γ a are weights that can be determined based on theoretical considerations ménétrier and auligné 2015 for example if we let b a c and choose γ c 1 and γ a 1 then we will recover a reduced rank approximation of a although what we really require for data assimilation is ma m t as discussed in section 5 1 ensemble methods are commonly used to estimate flow dependent covariance information however due to the necessarily limited size of the ensemble some form of localization is generally required to eliminate spurious correlations which can be a computationally expensive procedure houtekamer and zhang 2016 one advantage of using the rams to construct b a is that the expected covariance properties of the ram amplitudes is known a priori cf eq 7 which circumvents the obvious need for localization since c in 7 represents the expected covariance arising from an infinite ensemble to illustrate the flow dependent information captured by c fig 18 shows the standard deviations of sss derived from c for the 4d var analyses shown in fig 1 on 16 may 2014 the richness of the field and variance information is very evident and becomes increasingly more complex as the grid resolution increases it would be next to impossible to adequately model the inhomogeneous fields like those in fig 18 using conventional approaches to b c such as diffusion operators as in section 3 therefore the rams offer a straightforward and convenient procedure for supplementing b information about the cross covariances between the different components of the state vector and associated correlation length scales is naturally embedded in c from which a hybrid approach can benefit even though b c is only weakly flow dependent 4d var is forgiving since hb h t in 2 provides implicit flow dependent covariance information it is this information that is mined by the rams and mapped to state space by b h t and which we argue we can capitalize on using the approach that we are advocating here for constructing b a with steady progress in the resolution capabilities of satellite altimeters and radiometers and the advent of new innovative mobile and adaptive observing platforms such as gliders and other autonomous underwater vehicles auvs data assimilation at the ocean sub mesoscale is a new and exciting frontier dense in situ observing systems such as the pioneer array offer extraordinary and unprecedented insight into the sub mesoscale environment synthesizing these data using ocean models and data assimilation however represents a considerable challenge from this perspective fig 18c is particularly exciting since it reveals the remarkable level of detail that can potentially be mined to develop an effective hybrid 4d var approach for roms the sub mesoscale forecast problem on g3 has not been considered here but is nonetheless important and will be the subject of a future study credit authorship contribution statement andrew m moore conceptualization methodology writing original draft writing review editing software validation formal analysis investigation resources project administration julia levin conceptualization methodology writing original draft writing review editing software validation formal analysis investigation data curation hernan g arango methodology writing original draft writing review editing software john wilkin conceptualization methodology writing original draft writing review editing software validation formal analysis investigation resources project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by grants from the national science foundation oce 1459665 and oce 1459646 nasa nnx17ah58g and noaa na16nos0120020 pioneer array data were obtained from the nsf ocean observatories initiative data portal http ooinet oceanobservatories org 
23897,the implicit equal weights variational particle smoother iewvps is a combination of the particle filter pf and weak constraint 4 dimensional variational 4d var method that inherits the merits of both the iewvps avoids the filter degeneracy of particle filters through an implicit equal weights scheme and reduces the root mean square deviations rmsds by introducing the 4d var method this method has been tested using the lorenz 96 model in a previous study and we now implement it in the regional ocean model system roms which is a realistic and complex ocean model two key problems the representation of the analysis error covariance and the choice of the parameter α were solved during this implementation with an eddy permitting model satellite based sea surface height ssh and sea surface temperature sst observations were assimilated with a set of 40 particles iewvps scheme compared with the ensemble 4d var method the iewvps can reduce the bias introduced by perturbed atmospheric forcing effectively improving temperature simulations in the upper 50 m while maintaining the rmsd of ssh at the same level therefore the cooling effect caused by typhoons in the upper ocean is better characterized under the iewvps scheme than with previously used method the ratio of rmsd to the ensemble spread indicates that the ensemble quality of the iewvps is much better than that of the ensemble 4d var in addition the computational cost of the iewvps is only slightly larger than that of the ensemble 4d var one additional tangent linear model integration one additional nonlinear model integration and perturbation fields inputs outputs are still needed keywords implicit equal weights 4d var particle smoother ocean data assimilation 1 introduction the particle filter pf is a nonlinear data assimilation method that approximates posterior probability density functions pdfs using a set of weighted particles the main problem associated with applying the pf to a real geophysical system is filter degeneracy in high dimensional situations filter degeneracy means that one particle has a weight close to 1 while the weights of the other particles tend toward zero van leeuwen et al 2019 reviewed four categories of methods to prevent filter degeneracy including the proposal density method localization transformation and hybridization although there are still many technical obstacles to make pfs operational in numerical weather prediction centers many researchers and scientists have made their own efforts to implement pfs into real geophysical models ades and van leeuwen 2015 tested the equivalent weights particle filter ewpf van leeuwen 2010 ades and van leeuwen 2013 using a reduced gravity system and found that the ewpf would not disrupt dynamical balances browne and van leeuwen 2015 further tested the ewpf using a coupled hadcm3 model with synthetic observations and filter degeneracy was not observed but the performance of the scheme was dependent on a nudging term browne 2016 the localized particle filter lpf proposed by poterjoy 2016 outperformed the enkf method in the atmospheric weather research and forecasting wrf model with a series of nonlinear observation operators poterjoy and anderson 2016 poterjoy et al 2019 in contrast from the lpf used by poterjoy 2016 the local particle filter introduced by penny and miyoshi 2016 was based on state domain localization a method that is quite similar to the letkf local ensemble transform kalman filter when implemented in the cosmo model the pf and letkf hybrid method improved the forecasting of non gaussian variables robert et al 2018 the localized adaptive particle filter lapf was the first particle filter to be implemented in the global operational system at deutscher wetterdienst dwd potthast et al 2019 and its performance was comparable with that of the letkf method under the framework of a lpf chen et al 2020b proposed a localized weighted ensemble kalman filter lwenkf and applied it to a real ocean data assimilation scheme in the regional ocean modeling system roms comparisons have been made among the lwenkf enkf and lpf indicating that the lwenkf is the most accurate method for unobserved ocean state variables while the accuracies of observed variables are maintained at the same levels as those resulting from the enkf method chen et al 2020a the 4 dimensional variational 4d var method has been widely used in numerical weather predictions due to the benefits of its dynamical constraints and that it allows for additional terms in the cost function e g variational bias correction variational quality control digital filter initialization weak constraint term etc bonavita et al 2017 since the minimization process required by the implicit particle filter method is similar to that used in the variational method atkins et al 2013 explored the inner equivalent connection between the implicit particle filter chorin and tu 2009 and 4d var methods morzfeld et al 2018 introduced a variational particle smoother method that eliminated the impact of filter degeneracy through localization wang et al 2020 illustrated a scheme called the implicit equal weights variational particle smoother iewvps that avoided filter degeneracy through the implicit equal weights scheme proposed by zhu et al 2016 and combined with this scheme the weak constraint 4d var method the lorenz96 experimental results showed that this new scheme outperformed a simple ensemble 4d var scheme in the ensemble quality and reduced the root mean square error of the ensemble members when compared with outputs of the implicit equal weights particle filter iewpf and letkf methods considering the success of the iewvps in the lorenz96 model we further implemented the iewvps scheme in the weak constraint 4d var framework of roms and conducted real ocean satellite data assimilation experiments compared with the simple lorenz96 model roms has more complicated dynamic and thermodynamic constraints and much larger model dimensions the major challenge of the implementation of the iewvps method in a real geophysical system is the representation of the analysis error covariance matrix of the proposal density which is part of the calculation of the equal weights adjustment component furthermore dimensions of the analysis error matrix are dependent not only on model dimensions but also on the length of the time window it is impossible to calculate the analysis error matrix explicitly in a high dimensional system in this study we introduce a random method to solve the problem of calculating the analysis error covariance matrix of this new scheme the response of the upper ocean to a typhoon is used as the test object and satellite based sea surface height ssh and sea surface temperature sst observations are assimilated in the experiments this paper is organized as follows in section 2 we describe the core algorithm and the implementation of the iewvps in the roms weak constraint 4d var framework in detail as well as the random method of deriving the analysis error covariance matrix section 3 introduces detailed experimental information section 4 illustrates the performance when assimilating the ssh and sst observations the conclusions and discussions are included in section 5 2 implementation of implicit equal weights variational particle smoother in roms the iewvps is an extension of the iewpf zhu et al 2016 which is based on the ideas of proposal density implicit sampling chorin and tu 2009 and equal weights van leeuwen 2010 weak constraint 4d var is chosen as a proposal density the implicit part of the scheme follows from drawing samples implicitly from a standard gaussian distributed proposal density instead of the original one the equal weights property is achieved by setting the weight of each particle to a target weight combining these ideas the position of each particle can be expressed as the mode of the chosen proposal density plus a scaled random perturbation denoted as the equal weights adjustment component in this study the core algorithm of the iewvps is introduced in section 2 1 however the equal weights adjustment component requires the square root of the covariance matrix of the chosen proposal density and this matrix is impossible to calculate explicitly due to the high dimension of the system thus we have to approximate the equal weights adjustment component and a random method is used for this purpose which is described in section 2 2 to obtain the position of each particle the equal weights adjustment component operates on each model time step though a nonlinear model operator 2 1 the core algorithm of the iewvps according to bayes theorem the posterior pdf of model state x 0 n given the observations y 1 n during the time window 0 n can be written as follows 1 p x 0 n y 1 n p x 0 n p y 1 n x 0 n p y 1 n where x 0 n x 0 x n t and y 0 n y 0 y n t applying the idea of proposal density we multiply and divide the right hand side in eq 1 by the same factor q x 0 n y 1 n which is the so called proposal density leading to the following equation 2 p x 0 n y 1 n p x 0 n p y 1 n x 0 n p y 1 n q x 0 n y 1 n q x 0 n y 1 n drawing samples from the proposal transition density q x 0 n y 1 n leads the posterior pdf to be expressed as 3 p x 0 n y 1 n 1 n e i 1 n e p y 1 n x i 0 n p x i 0 n p y 1 n q x i 0 n y 1 n δ x 0 n x i 0 n 1 n e i 1 n e w i δ x 0 n x i 0 n where n e is the number of particles and w i is the weight of particle i the term w i can be calculated using the equation below 4 w i p y 1 n x i 0 n p y 1 n p x i 0 n q x i 0 n y 1 n here the assumptions are as follows 1 the errors between x 0 and the initial condition of the background x b are gaussian distributed with covariance b 2 the observation errors have a gaussian distribution with covariance r and the observations are temporally independent and 3 the model errors are gaussian distributed with covariance q under these assumptions the numerator of the weight of each particle can be expressed in terms of the cost function of the weak constraint 4d var 5 p y 1 n x i 0 n p x i 0 n exp j i x i 0 n where j i x i 0 n is the cost function of the weak constraint 4d var for particle i the term j i x i 0 n is expressed as follows 6 j i x i 0 n 1 2 x i 0 x i b b 1 2 1 2 k 1 n y k h k x i k r k 1 2 1 2 k 1 n x i k m k x i k 1 q k 1 2 in which h and m are the nonlinear observation and model operator respectively the notation of x i 0 x i b b 1 2 represents x i 0 x i b t b 1 x i 0 x i b and the same notation is used for the other terms in eq 6 the numerator of the weight for each particle calculated with eq 4 can be further simplified as follows 7 p y 1 n x i 0 n p x i 0 n n x i a 0 n p i exp 1 2 ϕ i where n x a 0 n p c exp 1 2 x 0 n x a 0 n t p 1 x 0 n x a 0 n c is a constant x i a 0 n arg min j i x i 0 n is the minimizer of the weak constraint 4d var cost function 1 2 ϕ i min j i x i 0 n is the minimum value of the cost function p i is the inverse of the hessian of the cost function j i x i 0 n and the superscript a in x a 0 n means analysis the proposal transition density is now chosen to be gaussian with a mean x i a 0 n and covariance p i for each particle such that the following expression can be constructed 8 q x i 0 n y 1 n n x i a 0 n p i the implicit part of the scheme involves implicitly drawing samples from a standard gaussian proposal density q ξ 0 n n 0 i instead of from the original proposal density these two proposal densities are related by the following expression 9 q x 0 n y 1 n q ξ 0 n d x 0 n d ξ 0 n where d x 0 n d ξ 0 n denotes the absolute value of the determinant of the jacobian matrix of the transformation x 0 n g ξ 0 n in the iewvps the transformation is defined as follows 10 x i e 0 n g ξ 0 n x i a 0 n α i 1 2 p i 1 2 ξ i 0 n where α i is a scalar that is to be determined for each particle and the superscript e in x e 0 n means equal weights according to eq 9 the weight of each particle in eq 4 is now given by the following formula 11 w i p y 1 n x i 0 n p x i 0 n p y 1 n q ξ i 0 n d x 0 n d ξ 0 n neglecting p y 1 n and taking the 2 log of both sides gives the following expression 12 2 log w i 2 log p y 1 n x i 0 n p x i 0 n 2 log q ξ i 0 n 2 log d x 0 n d ξ 0 n using sylvester s determinant lemma and the random map of eq 10 d x 0 n d ξ 0 n can be reduced to the following term 13 d x 0 n d ξ 0 n α i n z 2 p i 1 2 1 ξ i 0 n α i 1 2 α i 1 2 ξ i 0 n where n z is the dimension of x 0 n and is depending on the model dimension and the length of the time window using eq 7 the 2 log w i becomes the following expression 14 2 log w i x i 0 n x i a 0 n t p i 1 x i 0 n x i a 0 n ϕ i ξ i 0 n t ξ i 0 n 2 log d x 0 n d ξ 0 n α i 1 ξ i 0 n t ξ i 0 n ϕ i 2 log d x 0 n d ξ 0 n α i 1 ξ i 0 n t ξ i 0 n ϕ i 2 log α i n z 2 2 log 1 ξ i 0 n α i 1 2 α i 1 2 ξ i 0 n 2 log p i 1 2 to satisfy the equal weights property the weight of each particle is set to a target weight w i α i w t a r g e t for this purpose the α i of each particle must satisfy the following equation 15 α i 1 ξ i 0 n t ξ i 0 n 2 log α i n z 2 2 log 1 ξ i 0 n α i 1 2 α i 1 2 ξ i 0 n c i where c i c ϕ i c is a constant and 1 2 ϕ i is the minimum of the weak constraint 4d var cost function if we write ξ i 0 n in a simplified form as ξ i eq 15 has the same form as that used in zhu et al 2016 and skauvold et al 2019 16 α i 1 ξ i t ξ i 2 log α i n z 2 2 log 1 ξ i α i 1 2 α i 1 2 ξ i c i the detailed solution of α i can be found in zhu et al 2016 or skauvold et al 2019 eq 16 has analytical solutions in terms of the lambert w function but it is solved using the newton iterative method for practical reasons if c i 0 then the value of α becomes a single constant solution with a value of 1 when c i 0 there are two branches of α that exist 1 and 1 as explored by zhu et al 2016 sampling on the 1 branch gives a large ensemble spread while sampling on the 1 branch yields the opposite effect a 50 random sampling scheme on the two branches achieves the most stable results in the iewpf but the results are under dispersed in the iewvps wang et al 2020 thus α is sampled on the 1 branch in this study it should be noted that the proposal density distribution has a gap as discussed in zhu et al 2016 that leads to a systematic bias skauvold et al 2019 eliminated this gap by sampling α only on the 1 branch and moving the particles twice in this study we followed the implementation of the iewpf of zhu et al 2016 for two reasons one reason is that no parameter tuning is required in the method of zhu et al 2016 so that it is easy to implement the other is that the sampling ratio between the two branches is treated as a way to tune the ensemble spread 2 2 estimation of the equal weights adjustment component in eq 10 the posterior states are divided into two components the first x i a 0 n is the deterministic part and the second α i 1 2 p i 1 2 ξ i 0 n which we refer to is the equal weights adjustment part once α i is solved x i a 0 n that satisfy the equal weights property can be found using eq 10 however p i 1 2 is impossible to calculate directly due to the high dimensions thus we have to estimate p i 1 2 ξ i 0 n indirectly meaning that we need to first know the structure of p in the roms 4d var system the roms weak constraint 4d var method is an incremental 4d var method that is solved in the observation space moore et al 2011b the entire cost function of the roms weak constraint 4d var method for a time window 0 n can be written as follows 17 j δ x 0 η 1 η n 1 2 δ x 0 b x 1 2 1 2 k 1 n d k h δ x k r k 1 2 1 2 k 1 n η k q k 1 2 where x represents the model states of roms which includes the potential temperature salinity sea surface height and velocities δ x means the increments of state vector x η is the model error b x represents the background error covariance matrix of the initial condition q k is the model error covariance matrix at time k and r k is the observation error covariance at time k the vector d k y h x k is the innovation vector at time k h is an observation operator that transforms the model state vector to the observed variables in the observation space and h is the tangent linearization of h to obtain the expression of p we introduce the control vector δ z δ x 0 t η k t t where k 1 n and rewrite the cost function in a compact form as follows moore et al 2011b 18 j δ z 1 2 δ z t d 1 δ z 1 2 d g δ z t r 1 d g δ z where g h m k t t is considered as a combined forward model and observation operator which propagates the initial state forward in time to match the observation time through tangent linear model m the vector d d k t t is the innovation vector over the assimilation window d and r are block diagonal matrices as expressed below r r 1 r 2 r n d b x q 1 q n in this study r k is considered to be a diagonal matrix at time k b x is assumed to be factorized as b x k b σ x c σ x t k b t where k b is the balance operator c is a univariate correlation matrix and σ x is the matrix of prior standard deviations for all control variables as shown in fig 3d and f for ssh and sst respectively q k is assumed to have the same structure as b x moore et al 2011a but σ q 0 05 σ x in the formulation of eq 18 the hessian matrix s h is the second derivative of the cost function j δ z 19 s h d 1 g t r 1 g the analysis error covariance matrix is the inverse of s h fisher and courtier 1995 20 p d 1 g t r 1 g 1 where time dimension is included in d g r and p if the model dimension is n x and the length of time window is n the dimension of the control vector is n z n x n 1 thus the dimension of p is n z n z which is the same as that of d if the dimension of observations is n y over the time window the dimensions of r and g are n y n y and n y n z respectively in a high dimensional system p and d cannot be calculated and inverted explicitly therefore we have to estimate p indirectly and use it implicitly we only need p 1 2 ξ 0 n for each particle the dimension of p 1 2 ξ 0 n n z n e is much smaller than those of p and p 1 2 we can estimate p 1 2 ξ i 0 n using a random method that was reported in wang et al 2020 as follows extracting d 1 2 out of the parentheses we get 21 p d t 2 d 1 2 g t r 1 g 1 d t 2 i d t 2 g t r 1 g d 1 2 d 1 2 1 d 1 2 i r 1 2 g d 1 2 t r 1 2 g d 1 2 1 d t 2 performing a singular value decomposition svd of r 1 2 g d 1 2 22 r 1 2 g d 1 2 u s v t where u and v are orthogonal unitary matrices containing the left and right singular vectors respectively and s contains the singular values ordered in descending order of magnitude we then obtain the following expression for p 1 2 p d 1 2 i v s u t u s v t 1 d t 2 d 1 2 v i s 2 1 v t d t 2 23 p 1 2 d 1 2 v i s 2 1 2 we cannot calculate p 1 2 directly using eq 23 since large matrix operations are needed and g is difficult to be expressed explicitly instead of using eq 23 we use a random method for convenience we define the ensemble of ξ 0 n as ξ ξ 1 0 n ξ n e 0 n and introduce q 0 i g d 1 2 ξ i g d 1 2 ξ i 1 n e where n e 40 is the ensemble size used in our study the overline represents the ensemble mean instead of performing svd on r 1 2 g d 1 2 we perform svd on r 1 2 q 0 u s v t the random component p 1 2 ξ can be expressed as follows 24 p 1 2 ξ d 1 2 ξ v i s 2 1 2 the dimensions of r 1 2 g d 1 2 are n y n z in eq 23 and the dimension of r 1 2 q 0 are n y n e in eq 24 the latter is much smaller than the dimensions of d or p since the observation dimensions is much smaller than the model dimension this kind of situation usually occurs in ocean data assimilation in eq 24 d 1 2 g and r perform as operators and there is no need to explicitly calculate these values so large matrix operations can be avoided the calculation process flow chart is shown in fig 1 as shown d 1 2 ξ i 0 n appears twice therefore we store it in a netcdf file after the first use overall the whole iewvps process includes 4 steps 1 run a weak constraint 4d var for each particle and then store the minimum of the cost function min j i as well as the initial condition x i 0 at the beginning of time window 2 calculate α in eq 15 using the newton iterative method 3 estimate the random part of each particle p 1 2 ξ i according to the process shown in fig 1 then multiply it with α 1 2 to generate the equal weights adjustment component 4 move each particle from the mode of the proposal density to the equal weights position according to eq 10 this is done by integrating the nonlinear model from x 0 to time n using the equal weights adjustment component as a forcing term we know that the optimization of 4d var is across the entire time window so that particles sampled in the iewvps are from a set of 4d trajectories that are valid at each time step throughout the window not only at the beginning of the time window therefore the equal weights adjustment component operates on each model time step practically and theoretically although we only mentioned the initial condition x 0 at the beginning of the time window in step 1 above the 4d trajectories of the 4d var minimization can be obtained by integrating the nonlinear model m with the model error η as follows 25 x k m k k 1 x k 1 η k it should be noted that the model error is assumed to be additive and not state dependent in the roms weak constraint 4d var as shown in eq 25 in step 4 the equal weights adjustment component across the whole time window are not directly added to the 4d trajectories of the 4d var the 4d trajectories of the 4d var minimization are adjusted using the equal weights adjustment component as follows 26 x k m k k 1 x k 1 η k α 1 2 p 1 2 ξ k the iewvps process on each time step for each particle is shown in fig 2 it should be noted that the adjustment of η k and α 1 2 p 1 2 ξ k could be in opposite direction e g the t 3 and t k in fig 2 3 experiment setups 3 1 roms configuration the model domain spans from 105 e to 128 e and from 15 n to 24 n with a horizontal resolution of 1 6 and 24 vertical levels this is an eddy permitting resolution that can resolve both large scale circulation and mesoscale eddies the surface layers in the model are weakly nonlinear which fits the linear and gaussian assumptions of the enkf and variational methods as mentioned by chen et al 2020b this relatively low resolution is more feasible for the enkf and variational methods but may be challenging for particle filters due to the high dimension of the system thus we can use this resolution to test the performance of the iewvps in weakly nonlinear situations a model free run was initialized using the initial conditions obtained from hycom ncoda hybrid coordinate ocean model navy coupled ocean data assimilation metzger et al 2014 product on 1 jan 2007 and the model was integrated for 10 years without any data assimilation with atmospheric forcings obtained from ecmwf era interim datasets include windstress heat flux and freshwater flux and open boundary conditions obtained from the hycom ncoda product there are 4 purposes for this real simulation 1 to validate the model before data assimilation fig 3 2 to derive statistics regarding the climatic background error standard deviation which is important for 4d var fig 3d and f for ssh and sst 3 to provide a mean sea surface height fig 3b for satellite sea level anomaly sla assimilations and 4 tp provide a dynamical balanced initial condition for assimilation experiments the climatic monthly mean values and standard deviations of the sea surface height ssh and sea surface temperature sst fig 3 were used for the model validation without any data assimilation as shown in fig 3 roms successfully captured the large scale ssh pattern as well as the variabilities in ssh and sst in the northern south china sea nscs this pattern and these variabilities are coincident with the satellite observations 3 2 data assimilation experiments to test performances of the iewvps satellite derived ssh and sst observations are assimilated using the iewvps method and two additional experiments were designed table 1 en4dvar is an ensemble of perturbed weak constraint 4d var similar to the eda strategy of ecmwf bonavita et al 2017 the wcpsas experiment uses a single weak constraint 4d var en4dvar and iewvps are ensemble methods but wcpsas is not the difference between iewvps and en4dvar is that an equal weights adjustment step exists in the iewvps making the whole system a practical particle filter data assimilation system from a practical perspective the model error cannot be neglected thus we do not use the strong constraint 4d var strong constraint assumes that the model error is perfect and without error the ssh data were delayed time and gridded maps of sea level anomaly mlsa from copernicus marine environment monitoring service cmems the sst data were gridded version 2 avhrr only products obtained from noaa these data were developed using optimum interpolation and included a large scale adjustments of satellite biases with respect to the in situ data collected from ships and buoys reynolds et al 2007 both the sshs and ssts are level 4 products and are available daily with a horizontal resolution of 1 4 although the observation errors in gridded products are never uncorrelated in the actual nature we still assume that observation errors are uncorrelated in space and time for simplicity which leads to the fact that the observation error covariance r is a diagonal matrix the observation error is assumed to be a combination of the measurement error and the representation error which are additive measurement errors are considered to be independent of the data source and have the following standard deviations moore et al 2011a 0 4 c for sst and 2 cm for ssh the representativeness error is measured by the standard deviation of the observations that contribute to each super observation to diminish long distance false dependencies the decorrelation length scales of the covariance matrix were set to 50 km in the horizontal direction and 30 m in the vertical direction moore et al 2011a assimilation experiments started from 1 oct 2015 and lasted until 30 nov 2015 with a 1 day assimilation time window in the iewvps and en4dvar experiments the initial and atmospheric forcing ensemble members were generated through an exact second order sampling scheme hoteit et al 2013 chen et al 2020b 27 x i x β n e ϵ i t where x represents the ensemble mean of the initial conditions a 9 year averaged model state or atmospheric forcings windstress and heat flux in this study e is a matrix whose columns consist of n 1 eofs empirical orthogonal functions the term ϵ i is the i th row of a n n 1 random matrix with orthogonal columns and the sum of columns is zero β is an inflation parameter with a value of 1 0 for initial conditions 0 2 for the windstress and 100 for the heat fluxes the ensemble spread of surface net heat flux generated with a factor 100 is approximately 10 w m 2 in all assimilation experiments the observations were cycled assimilated into the model fig 4 4 results the northern south china sea nscs is strongly influenced by monsoons and the kuroshio as a result the large scale circulation is cyclonic with active mesoscale processes moreover the nscs is also an area where typhoons generate or pass through frequently during the period from 1 oct to 10 oct in 2015 typhoon mujigae passed through the nscs and the temperature of the upper layers was lowered by ekman pumping in this research the upper layer states are investigated and compared among different assimilation experiments the surface states at the end of the time window are used for the comparisons while the error statistics of forecasts for november 2015 are also compared 4 1 upper ocean states the sla simulations are shown in fig 5 for the duration of the typhoon the sla was high in the nscs especially along the coast and low in the western pacific the mesoscale eddies in the three assimilation experiments are consistent with the observations the sla differences between the three experiments and satellite products are small approximately 4 cm as shown in fig 5 generally the three assimilation experiments performed similarly in modeling the slas when a typhoon crosses the sea the ekman pumping induced by the typhoon can cause dramatic changes in upper level seawater temperatures especially at the sea surface this sst cooling is shown in fig 6 comand exceeds 1 c around the studied typhoon track the cooling patterns output in the three experiments are similar during the cooling period before 5 oct large differences appeared on 5 oct and the sst cooling was strongest in the iewvps experiment and remained consistent with the avhrr ssts while a warm bias existed in the en4dvar experimental outputs the wcpsas performed similarly to the iewvps in modeling sst cooling however the ssts tended to be overly high in the northeastern part of the modeled region fig 6b after 5 oct the ssts began to recover and large biases can be seen in the western philippines in the wcpsas experiment fig 7b the en4dvar experiment performed better than the wcpsas experiment but the outputs were still overly high according to the avhrr products fig 7c the influences of satellite data are not limited to the surface and can propagate vertically through the cross correlations of the background error covariance matrix and move forward through the model dynamics the temperatures along typhoon mujigae track are shown in fig 8 although the sst cooling reflected in the wcpsas outputs was similar to that output by the en4dvar and iewvps before 5 oct the temperature changes output by the wcpsas in the upper 100 m were worse than those of the en4dvar and iewvps compared with en4dvar the temperatures obtained from iewvps were cooler in upper 50 m and warmer below 100 m fig 9 indicating that the ekman pumping effect was more significant in the iewvps experiment than in the en4dvar experiment compared with the observed ssts the temperatures near 119 e tended to be overly high in the wcpsas and en4dvar outputs these high temperatures also existed in the deterministic part of the iewvps and were improved after an equal weights adjustment in the iewvps fig 9 as a result the temperatures near 119 e in the iewvps outputs were cooler than those output by the en4dvar fig 9 it should be noted that the large biases on 2 oct are mainly caused by the initial ensemble generation scheme calculated through eq 27 when assimilating ssts the error tends to be largest at the thermocline at a depth of approximately 100 m luo et al 2017 this is caused by the net heat input when correcting the cold bias in the mixed layer shu et al 2009 4 2 error statistics the above analyses are qualitative we also quantitatively compared the three experiments to make the comparisons more effective and convincing the assimilation period was extended to 2 months from 1 oct to 30 nov 2015 however only the results obtained in november 2015 were compared to exclude the influence of the initial ensemble generated using eq 27 the root mean square deviations rmsds of the analysis states at the end of the assimilation window that is t n for a window 0 n and the 1 day forecast states were calculated as shown in fig 10 the rmsds of the sshs output in the iewvps and en4dvar experiment were equivalent and slightly smaller than those output in the wcpsas experiment fig 10a and c for the ssts the rmsd at the end of the assimilation window in the iewvps experiment was 0 1 c fig 10b which was smallest among the three experiments and corresponded to a reduction of 52 6 relative to the en4dvar rmsd the sst rmsd of the 1 day forecast fig 10d was larger than that of the analysis states at the beginning of the assimilation window as was expected compared with the en4dvar experimental outputs the sst rmsd of the 1 day forecast in the iewvps experiment was reduced by approximately 10 5 to compare the three experiments more clearly the rmsd time series shown in fig 10 were used to draw box plots fig 11 consistent with fig 10 the ssh rmsds in the three experiments are at the same level among the three methods the sst rmsd of the analysis and 1 day forecast are the smallest in the outputs of the iewvps experiment to be more objective we also compared the output ssts with esacci european space agency sea surface temperature climate change initiative merchant et al 2019 and ukmo sst good et al 2020 products as shown in fig 12 since the horizontal resolutions of the esacci and ukmo sst products are 1 20 and our model resolution was 1 6 we interpolated the esacci and ukmo ssts to our model grids the rmsds relative to the esacci and ukmo ssts were larger than those relative to the avhrr product despite the differences in the sst products the iewvps produced the smallest rmsd among the three assimilation experiments at the end of the assimilation window the sst rmsd of the iewvps was reduced by 12 1 esacci and 12 3 ukmo compared to the en4dvar values fig 12 for the 1 day forecast the sst rmsd of the iewvps was reduced by 6 7 esacci and 6 2 ukmo compared to the en4dvar values in general the ssh rmsds of the three assimilation experiments were similar and the sst rmsds in the iewvps experiment were the smallest especially for the analysis states at the end of the time window during the 1 day forecast step the rmsds increased for both sshs and ssts one factor affecting this result is that large biases appear at open boundaries as shown in figs 5 to 7 the other factor is that the model error is not considered during the forecast step given the same atmospheric forcing and open boundary conditions the rmsds of the forecasts are dependent on the initial conditions which are characterized by the analysis states at the end of the time window fig 10c and d for the purpose of understanding how the equal weights adjustment component works we compared the biases of the ssts the bias is the mean value of the distances between observations and the model simulated values y h x and is shown in fig 13 there are at least four sources accounting for the biases in our experiments the initial conditions perturbed atmospheric forcing open boundary conditions and model errors as shown in fig 13a the bias in the initial conditions is very small a mean value of approximately 0 002 c so the initial conditions can be excluded from the sources of biases however as the model is forced by perturbed atmospheric forcings open boundary conditions and model errors the bias becomes larger when the model propagates over time thus the output ssts were biased at the end of the time window in the en4dvar experiment with a mean value of 0 1 c and a similar pattern was found in the wcpsas experiment we can see that the bias in the iewvps experiment was very small even at the end of the time window fig 13b and the mean value was approximately 0 007 c the sst bias in the en4dvar experiment is approximately 0 093 c greater than that of the iewvps experiment this bias deviation was close to the 0 1 c deviation in the rmsds calculated between the en4dvar and iewvps experiments the rank histograms of the one model run experiment conducted for november 2015 were also compared as shown in fig 14 the rank histograms were generated by ranking the observations in the set of the state ensemble members from lowest to highest a left extreme pattern was found in the en4dvar experiment fig 14a indicating a positive bias hamill 2001 in the iewvps experiment slightly u shaped rank histograms indicated that the ensemble spread was slightly smaller than the rmsd together with fig 13 we can conclude that the equal weights adjustment components can reduce the bias at the end of the time window we know that the full particle weight of each particle consists of the proposal weight and the likelihood weight the implicit equal weights scheme ensures that the full weights are equal for all particles which introducing more freedom in the iewvps than the en4dvar to fit the observations 4 3 an additional experiment the above analyses are based on typhoon mujigae which occurred in 2015 and whose track was almost a straight line in reality some typhoons may turn to the north exhibiting complex air sea interactions typhoon megi which occurred in 2010 was one of these one trial is not enough to show the advantages of the iewvps thus we performed the same experiments as those described above for typhoon megi in 2010 the largest sst cooling occurred on 22 and 23 oct as shown in fig 15 the sst cooling in the iewvps experiment was closer to the avhrr products than those in the en4dvar experiment especially the 2 c isotherm the rmsds of the sshs and ssts are shown in fig 16 the rmsd of sshs in the iewvps experiment was equivalent to that in the en4dvar but the rmsd of sst in the iewvps experiment was much smaller than that in the en4dvar experiment since the ensemble spread is a measure of the uncertainty in the data assimilation method it is better to have a slightly too large spread than a too small spread the ratios of the rmsds to the ensemble spreads for the sshs and ssts output by the iewvps were slightly larger than 1 0 indicating that the ensemble is under dispersed similar to the case of typhoon mujigae a warm bias existed at open boundary and propagated to the interior ocean fig 6b and fig 7b 5 discussions and conclusions in this study the implicit equal weights variational particle smoother iewvps method was implemented in the regional ocean modeling system roms and was applied to the assimilation of ocean satellite data the key problem we solved in the implementation of the iewvps into roms is the estimation of the equal weights stochastic adjustment component we used an implicit random mapping method to estimate the perturbation component which was represented through the tangent linear model and then combined this component with the equal weights parameter α the iewvps uses a modified weak constraint 4d var framework as the proposal transition density in a standard implicit equal weights particle filter iewpf this method combines the merits of the weak constraint 4d var and the iewpf it reduces the rmsd by applying the weak constraint 4d var as the proposal density and prevents filter degeneracy by using an implicit equal weights sampling scheme to verify the effects of the iewvps method we tested the responses of the upper ocean variables during a typhoon case study before and after the assimilation of ocean satellite data satellite remote sensing sst and ssh observations in the northern south china sea area were assimilated in the experiments and the results of the iewvps were compared with those of the en4dvar method and roms weak constraint 4d var wcpsas the ssh rmsds were equivalent among three assimilation experiments while the sst rmsds in the iewvps experiment was the smallest for both the analysis at the end of the time window and 1 day forecasts because the rmsds were reduced at the end of the time window the sst rmsds of the 1 day forecast derived from the iewvps experiment were reduced by 10 5 avhrr 6 7 esacci and 6 2 ukmo relative to the results of en4dvar experiment therefore the temperature cooling induced by ekman pumping in the upper 50 m was best characterized in the iewvps experiment however the largest improvement did not occur during typhoon activity over the sea but occurred after the typhoon made landfall over china excessive temperature arose at open boundaries in the en4dvar and wcpsas experiments this situation was relieved in the iewvps experiment the bias existed at open boundaries is one of the reasons why the rmsds were increased in the 1 day forecasts another reason is that the model errors are considered during the assimilations but are neglected during the forecasts to improve the forecasts model error should be accounted for such as by using the stochastically perturbed parameterization tendencies sppt or stochastic kinetic energy backscatter skeb schemes in the model as done in ecmwf bonavita 2011 to increase the ensemble spread an exact second order sampling scheme hoteit et al 2013 chen et al 2020b was adopted for the atmospheric forcings one drawback of this scheme is that the choice of inflation parameter is artificial the inflation parameter was 0 2 for the windstress and 100 for the heat fluxes these values are not optimal the other drawback is that the independent choices of inflation parameters for different variables windstress and heat fluxes in our study may lead to inconsistent statistical results among variables another drawback of the perturbation scheme used in our study is that the covariance of atmospheric forcing is static and cannot reflect the high frequency variabilities in the atmosphere to improve the ensemble quality the perturbation scheme for atmospheric forcing should be studied further and the uncertainty at open boundaries should also be considered the ensemble spread is also influenced by the sampling ratio of α on the 1 and 1 branches zhu et al 2016 however the α 1 solution is spurious which is leading to a systematic bias this systematic bias can be eliminated by the two stage iewpf proposed by skauvold et al 2019 then the ensemble spread will be dependent on a different parameter β we will replace the implicit equal weights scheme according to skauvold et al 2019 and investigate an adaptive scheme for β to improve the ensemble quality the computational cost of the iewvps is slightly larger than that of the en4dvar since the iewvps is based on the en4dvar a set of weak constraint 4d var additional costs for each particle are needed to calculate of the equal weights adjustment component including α i and p 1 2 ξ i 0 n in total an additional tl tangent linear model integration is needed for the calculation of the equal weights adjustment component and a nl nonlinear model integration is needed to move each proposal particle to its equal weight position for each particle these computational costs are relatively small compared to the total cost of each 4d var 2 nl integrations 30 tl integrations and 30 adjoint model integrations as shown in fig 1 we need to calculate and deliver d 1 2 ξ 0 n the assimilation window in our study is 1 day which equals 120 model steps one time step is 720 s thus we need to deliver n 120 d 1 2 ξ for each particle as outputs if the assimilation window is very long or the time step is very small this cost will become larger one method used to reduce this cost involves delivering d 1 2 ξ outputs every m model steps which in our study m 15 equivalent to 3 h in the simulation the total computational cost of the iewvps scheme was divided into fortran and matlab components the matalb components include the calculation of α i and svd on r 1 2 q 0 the computational cost of the fortran component is 910 83 seconds particle in one cycle and the cost of the matlab component is less than 60 s while the computational cost of each 4d var is 852 18 s therefore the additional computational cost is approximately 120 s for each particle an increment of approximately 14 relative to cost of the 4d var the matlab components will be rewritten in fortran language thus further reducing the computational cost one advantage of the iewvps is that the equal weights stochastic adjustment component maintains the model dynamic balance because of the balance operator in 4d var and the balanced theoretical implicit equal weights sampling framework we know that the major advantage of the enkf is the flow dependent background error covariance matrix therefore one drawback in our study is that the background error covariance matrix b is climatological and static furthermore a flow dependent b matrix based on the pf and 4d var method will be investigated in the future credit authorship contribution statement pinqiang wang conceptualization methodology software writing original draft writing review and editing formal analysis investigation validation visualization data curation mengbin zhu conceptualization methodology formal analysis investigation writing original draft writing review and editing validation supervision funding acquisition yan chen conceptualization methodology formal analysis investigation funding acquisition weimin zhang conceptualization methodology writing review and editing project administration supervision funding acquisition yi yu visualization formal analysis funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the weak constraint 4d var codes in this study is downloaded from http www myroms org satellite sshs are downloaded from http marine copernicus eu satellite ssts are downloaded from ftp eclipse ncdc noaa gov this paper is supported by the national key r d program of china 2018yfc1406202 the national natural science foundation of china nsfc grant nos 42005113 41675097 41830964 42075149 
23897,the implicit equal weights variational particle smoother iewvps is a combination of the particle filter pf and weak constraint 4 dimensional variational 4d var method that inherits the merits of both the iewvps avoids the filter degeneracy of particle filters through an implicit equal weights scheme and reduces the root mean square deviations rmsds by introducing the 4d var method this method has been tested using the lorenz 96 model in a previous study and we now implement it in the regional ocean model system roms which is a realistic and complex ocean model two key problems the representation of the analysis error covariance and the choice of the parameter α were solved during this implementation with an eddy permitting model satellite based sea surface height ssh and sea surface temperature sst observations were assimilated with a set of 40 particles iewvps scheme compared with the ensemble 4d var method the iewvps can reduce the bias introduced by perturbed atmospheric forcing effectively improving temperature simulations in the upper 50 m while maintaining the rmsd of ssh at the same level therefore the cooling effect caused by typhoons in the upper ocean is better characterized under the iewvps scheme than with previously used method the ratio of rmsd to the ensemble spread indicates that the ensemble quality of the iewvps is much better than that of the ensemble 4d var in addition the computational cost of the iewvps is only slightly larger than that of the ensemble 4d var one additional tangent linear model integration one additional nonlinear model integration and perturbation fields inputs outputs are still needed keywords implicit equal weights 4d var particle smoother ocean data assimilation 1 introduction the particle filter pf is a nonlinear data assimilation method that approximates posterior probability density functions pdfs using a set of weighted particles the main problem associated with applying the pf to a real geophysical system is filter degeneracy in high dimensional situations filter degeneracy means that one particle has a weight close to 1 while the weights of the other particles tend toward zero van leeuwen et al 2019 reviewed four categories of methods to prevent filter degeneracy including the proposal density method localization transformation and hybridization although there are still many technical obstacles to make pfs operational in numerical weather prediction centers many researchers and scientists have made their own efforts to implement pfs into real geophysical models ades and van leeuwen 2015 tested the equivalent weights particle filter ewpf van leeuwen 2010 ades and van leeuwen 2013 using a reduced gravity system and found that the ewpf would not disrupt dynamical balances browne and van leeuwen 2015 further tested the ewpf using a coupled hadcm3 model with synthetic observations and filter degeneracy was not observed but the performance of the scheme was dependent on a nudging term browne 2016 the localized particle filter lpf proposed by poterjoy 2016 outperformed the enkf method in the atmospheric weather research and forecasting wrf model with a series of nonlinear observation operators poterjoy and anderson 2016 poterjoy et al 2019 in contrast from the lpf used by poterjoy 2016 the local particle filter introduced by penny and miyoshi 2016 was based on state domain localization a method that is quite similar to the letkf local ensemble transform kalman filter when implemented in the cosmo model the pf and letkf hybrid method improved the forecasting of non gaussian variables robert et al 2018 the localized adaptive particle filter lapf was the first particle filter to be implemented in the global operational system at deutscher wetterdienst dwd potthast et al 2019 and its performance was comparable with that of the letkf method under the framework of a lpf chen et al 2020b proposed a localized weighted ensemble kalman filter lwenkf and applied it to a real ocean data assimilation scheme in the regional ocean modeling system roms comparisons have been made among the lwenkf enkf and lpf indicating that the lwenkf is the most accurate method for unobserved ocean state variables while the accuracies of observed variables are maintained at the same levels as those resulting from the enkf method chen et al 2020a the 4 dimensional variational 4d var method has been widely used in numerical weather predictions due to the benefits of its dynamical constraints and that it allows for additional terms in the cost function e g variational bias correction variational quality control digital filter initialization weak constraint term etc bonavita et al 2017 since the minimization process required by the implicit particle filter method is similar to that used in the variational method atkins et al 2013 explored the inner equivalent connection between the implicit particle filter chorin and tu 2009 and 4d var methods morzfeld et al 2018 introduced a variational particle smoother method that eliminated the impact of filter degeneracy through localization wang et al 2020 illustrated a scheme called the implicit equal weights variational particle smoother iewvps that avoided filter degeneracy through the implicit equal weights scheme proposed by zhu et al 2016 and combined with this scheme the weak constraint 4d var method the lorenz96 experimental results showed that this new scheme outperformed a simple ensemble 4d var scheme in the ensemble quality and reduced the root mean square error of the ensemble members when compared with outputs of the implicit equal weights particle filter iewpf and letkf methods considering the success of the iewvps in the lorenz96 model we further implemented the iewvps scheme in the weak constraint 4d var framework of roms and conducted real ocean satellite data assimilation experiments compared with the simple lorenz96 model roms has more complicated dynamic and thermodynamic constraints and much larger model dimensions the major challenge of the implementation of the iewvps method in a real geophysical system is the representation of the analysis error covariance matrix of the proposal density which is part of the calculation of the equal weights adjustment component furthermore dimensions of the analysis error matrix are dependent not only on model dimensions but also on the length of the time window it is impossible to calculate the analysis error matrix explicitly in a high dimensional system in this study we introduce a random method to solve the problem of calculating the analysis error covariance matrix of this new scheme the response of the upper ocean to a typhoon is used as the test object and satellite based sea surface height ssh and sea surface temperature sst observations are assimilated in the experiments this paper is organized as follows in section 2 we describe the core algorithm and the implementation of the iewvps in the roms weak constraint 4d var framework in detail as well as the random method of deriving the analysis error covariance matrix section 3 introduces detailed experimental information section 4 illustrates the performance when assimilating the ssh and sst observations the conclusions and discussions are included in section 5 2 implementation of implicit equal weights variational particle smoother in roms the iewvps is an extension of the iewpf zhu et al 2016 which is based on the ideas of proposal density implicit sampling chorin and tu 2009 and equal weights van leeuwen 2010 weak constraint 4d var is chosen as a proposal density the implicit part of the scheme follows from drawing samples implicitly from a standard gaussian distributed proposal density instead of the original one the equal weights property is achieved by setting the weight of each particle to a target weight combining these ideas the position of each particle can be expressed as the mode of the chosen proposal density plus a scaled random perturbation denoted as the equal weights adjustment component in this study the core algorithm of the iewvps is introduced in section 2 1 however the equal weights adjustment component requires the square root of the covariance matrix of the chosen proposal density and this matrix is impossible to calculate explicitly due to the high dimension of the system thus we have to approximate the equal weights adjustment component and a random method is used for this purpose which is described in section 2 2 to obtain the position of each particle the equal weights adjustment component operates on each model time step though a nonlinear model operator 2 1 the core algorithm of the iewvps according to bayes theorem the posterior pdf of model state x 0 n given the observations y 1 n during the time window 0 n can be written as follows 1 p x 0 n y 1 n p x 0 n p y 1 n x 0 n p y 1 n where x 0 n x 0 x n t and y 0 n y 0 y n t applying the idea of proposal density we multiply and divide the right hand side in eq 1 by the same factor q x 0 n y 1 n which is the so called proposal density leading to the following equation 2 p x 0 n y 1 n p x 0 n p y 1 n x 0 n p y 1 n q x 0 n y 1 n q x 0 n y 1 n drawing samples from the proposal transition density q x 0 n y 1 n leads the posterior pdf to be expressed as 3 p x 0 n y 1 n 1 n e i 1 n e p y 1 n x i 0 n p x i 0 n p y 1 n q x i 0 n y 1 n δ x 0 n x i 0 n 1 n e i 1 n e w i δ x 0 n x i 0 n where n e is the number of particles and w i is the weight of particle i the term w i can be calculated using the equation below 4 w i p y 1 n x i 0 n p y 1 n p x i 0 n q x i 0 n y 1 n here the assumptions are as follows 1 the errors between x 0 and the initial condition of the background x b are gaussian distributed with covariance b 2 the observation errors have a gaussian distribution with covariance r and the observations are temporally independent and 3 the model errors are gaussian distributed with covariance q under these assumptions the numerator of the weight of each particle can be expressed in terms of the cost function of the weak constraint 4d var 5 p y 1 n x i 0 n p x i 0 n exp j i x i 0 n where j i x i 0 n is the cost function of the weak constraint 4d var for particle i the term j i x i 0 n is expressed as follows 6 j i x i 0 n 1 2 x i 0 x i b b 1 2 1 2 k 1 n y k h k x i k r k 1 2 1 2 k 1 n x i k m k x i k 1 q k 1 2 in which h and m are the nonlinear observation and model operator respectively the notation of x i 0 x i b b 1 2 represents x i 0 x i b t b 1 x i 0 x i b and the same notation is used for the other terms in eq 6 the numerator of the weight for each particle calculated with eq 4 can be further simplified as follows 7 p y 1 n x i 0 n p x i 0 n n x i a 0 n p i exp 1 2 ϕ i where n x a 0 n p c exp 1 2 x 0 n x a 0 n t p 1 x 0 n x a 0 n c is a constant x i a 0 n arg min j i x i 0 n is the minimizer of the weak constraint 4d var cost function 1 2 ϕ i min j i x i 0 n is the minimum value of the cost function p i is the inverse of the hessian of the cost function j i x i 0 n and the superscript a in x a 0 n means analysis the proposal transition density is now chosen to be gaussian with a mean x i a 0 n and covariance p i for each particle such that the following expression can be constructed 8 q x i 0 n y 1 n n x i a 0 n p i the implicit part of the scheme involves implicitly drawing samples from a standard gaussian proposal density q ξ 0 n n 0 i instead of from the original proposal density these two proposal densities are related by the following expression 9 q x 0 n y 1 n q ξ 0 n d x 0 n d ξ 0 n where d x 0 n d ξ 0 n denotes the absolute value of the determinant of the jacobian matrix of the transformation x 0 n g ξ 0 n in the iewvps the transformation is defined as follows 10 x i e 0 n g ξ 0 n x i a 0 n α i 1 2 p i 1 2 ξ i 0 n where α i is a scalar that is to be determined for each particle and the superscript e in x e 0 n means equal weights according to eq 9 the weight of each particle in eq 4 is now given by the following formula 11 w i p y 1 n x i 0 n p x i 0 n p y 1 n q ξ i 0 n d x 0 n d ξ 0 n neglecting p y 1 n and taking the 2 log of both sides gives the following expression 12 2 log w i 2 log p y 1 n x i 0 n p x i 0 n 2 log q ξ i 0 n 2 log d x 0 n d ξ 0 n using sylvester s determinant lemma and the random map of eq 10 d x 0 n d ξ 0 n can be reduced to the following term 13 d x 0 n d ξ 0 n α i n z 2 p i 1 2 1 ξ i 0 n α i 1 2 α i 1 2 ξ i 0 n where n z is the dimension of x 0 n and is depending on the model dimension and the length of the time window using eq 7 the 2 log w i becomes the following expression 14 2 log w i x i 0 n x i a 0 n t p i 1 x i 0 n x i a 0 n ϕ i ξ i 0 n t ξ i 0 n 2 log d x 0 n d ξ 0 n α i 1 ξ i 0 n t ξ i 0 n ϕ i 2 log d x 0 n d ξ 0 n α i 1 ξ i 0 n t ξ i 0 n ϕ i 2 log α i n z 2 2 log 1 ξ i 0 n α i 1 2 α i 1 2 ξ i 0 n 2 log p i 1 2 to satisfy the equal weights property the weight of each particle is set to a target weight w i α i w t a r g e t for this purpose the α i of each particle must satisfy the following equation 15 α i 1 ξ i 0 n t ξ i 0 n 2 log α i n z 2 2 log 1 ξ i 0 n α i 1 2 α i 1 2 ξ i 0 n c i where c i c ϕ i c is a constant and 1 2 ϕ i is the minimum of the weak constraint 4d var cost function if we write ξ i 0 n in a simplified form as ξ i eq 15 has the same form as that used in zhu et al 2016 and skauvold et al 2019 16 α i 1 ξ i t ξ i 2 log α i n z 2 2 log 1 ξ i α i 1 2 α i 1 2 ξ i c i the detailed solution of α i can be found in zhu et al 2016 or skauvold et al 2019 eq 16 has analytical solutions in terms of the lambert w function but it is solved using the newton iterative method for practical reasons if c i 0 then the value of α becomes a single constant solution with a value of 1 when c i 0 there are two branches of α that exist 1 and 1 as explored by zhu et al 2016 sampling on the 1 branch gives a large ensemble spread while sampling on the 1 branch yields the opposite effect a 50 random sampling scheme on the two branches achieves the most stable results in the iewpf but the results are under dispersed in the iewvps wang et al 2020 thus α is sampled on the 1 branch in this study it should be noted that the proposal density distribution has a gap as discussed in zhu et al 2016 that leads to a systematic bias skauvold et al 2019 eliminated this gap by sampling α only on the 1 branch and moving the particles twice in this study we followed the implementation of the iewpf of zhu et al 2016 for two reasons one reason is that no parameter tuning is required in the method of zhu et al 2016 so that it is easy to implement the other is that the sampling ratio between the two branches is treated as a way to tune the ensemble spread 2 2 estimation of the equal weights adjustment component in eq 10 the posterior states are divided into two components the first x i a 0 n is the deterministic part and the second α i 1 2 p i 1 2 ξ i 0 n which we refer to is the equal weights adjustment part once α i is solved x i a 0 n that satisfy the equal weights property can be found using eq 10 however p i 1 2 is impossible to calculate directly due to the high dimensions thus we have to estimate p i 1 2 ξ i 0 n indirectly meaning that we need to first know the structure of p in the roms 4d var system the roms weak constraint 4d var method is an incremental 4d var method that is solved in the observation space moore et al 2011b the entire cost function of the roms weak constraint 4d var method for a time window 0 n can be written as follows 17 j δ x 0 η 1 η n 1 2 δ x 0 b x 1 2 1 2 k 1 n d k h δ x k r k 1 2 1 2 k 1 n η k q k 1 2 where x represents the model states of roms which includes the potential temperature salinity sea surface height and velocities δ x means the increments of state vector x η is the model error b x represents the background error covariance matrix of the initial condition q k is the model error covariance matrix at time k and r k is the observation error covariance at time k the vector d k y h x k is the innovation vector at time k h is an observation operator that transforms the model state vector to the observed variables in the observation space and h is the tangent linearization of h to obtain the expression of p we introduce the control vector δ z δ x 0 t η k t t where k 1 n and rewrite the cost function in a compact form as follows moore et al 2011b 18 j δ z 1 2 δ z t d 1 δ z 1 2 d g δ z t r 1 d g δ z where g h m k t t is considered as a combined forward model and observation operator which propagates the initial state forward in time to match the observation time through tangent linear model m the vector d d k t t is the innovation vector over the assimilation window d and r are block diagonal matrices as expressed below r r 1 r 2 r n d b x q 1 q n in this study r k is considered to be a diagonal matrix at time k b x is assumed to be factorized as b x k b σ x c σ x t k b t where k b is the balance operator c is a univariate correlation matrix and σ x is the matrix of prior standard deviations for all control variables as shown in fig 3d and f for ssh and sst respectively q k is assumed to have the same structure as b x moore et al 2011a but σ q 0 05 σ x in the formulation of eq 18 the hessian matrix s h is the second derivative of the cost function j δ z 19 s h d 1 g t r 1 g the analysis error covariance matrix is the inverse of s h fisher and courtier 1995 20 p d 1 g t r 1 g 1 where time dimension is included in d g r and p if the model dimension is n x and the length of time window is n the dimension of the control vector is n z n x n 1 thus the dimension of p is n z n z which is the same as that of d if the dimension of observations is n y over the time window the dimensions of r and g are n y n y and n y n z respectively in a high dimensional system p and d cannot be calculated and inverted explicitly therefore we have to estimate p indirectly and use it implicitly we only need p 1 2 ξ 0 n for each particle the dimension of p 1 2 ξ 0 n n z n e is much smaller than those of p and p 1 2 we can estimate p 1 2 ξ i 0 n using a random method that was reported in wang et al 2020 as follows extracting d 1 2 out of the parentheses we get 21 p d t 2 d 1 2 g t r 1 g 1 d t 2 i d t 2 g t r 1 g d 1 2 d 1 2 1 d 1 2 i r 1 2 g d 1 2 t r 1 2 g d 1 2 1 d t 2 performing a singular value decomposition svd of r 1 2 g d 1 2 22 r 1 2 g d 1 2 u s v t where u and v are orthogonal unitary matrices containing the left and right singular vectors respectively and s contains the singular values ordered in descending order of magnitude we then obtain the following expression for p 1 2 p d 1 2 i v s u t u s v t 1 d t 2 d 1 2 v i s 2 1 v t d t 2 23 p 1 2 d 1 2 v i s 2 1 2 we cannot calculate p 1 2 directly using eq 23 since large matrix operations are needed and g is difficult to be expressed explicitly instead of using eq 23 we use a random method for convenience we define the ensemble of ξ 0 n as ξ ξ 1 0 n ξ n e 0 n and introduce q 0 i g d 1 2 ξ i g d 1 2 ξ i 1 n e where n e 40 is the ensemble size used in our study the overline represents the ensemble mean instead of performing svd on r 1 2 g d 1 2 we perform svd on r 1 2 q 0 u s v t the random component p 1 2 ξ can be expressed as follows 24 p 1 2 ξ d 1 2 ξ v i s 2 1 2 the dimensions of r 1 2 g d 1 2 are n y n z in eq 23 and the dimension of r 1 2 q 0 are n y n e in eq 24 the latter is much smaller than the dimensions of d or p since the observation dimensions is much smaller than the model dimension this kind of situation usually occurs in ocean data assimilation in eq 24 d 1 2 g and r perform as operators and there is no need to explicitly calculate these values so large matrix operations can be avoided the calculation process flow chart is shown in fig 1 as shown d 1 2 ξ i 0 n appears twice therefore we store it in a netcdf file after the first use overall the whole iewvps process includes 4 steps 1 run a weak constraint 4d var for each particle and then store the minimum of the cost function min j i as well as the initial condition x i 0 at the beginning of time window 2 calculate α in eq 15 using the newton iterative method 3 estimate the random part of each particle p 1 2 ξ i according to the process shown in fig 1 then multiply it with α 1 2 to generate the equal weights adjustment component 4 move each particle from the mode of the proposal density to the equal weights position according to eq 10 this is done by integrating the nonlinear model from x 0 to time n using the equal weights adjustment component as a forcing term we know that the optimization of 4d var is across the entire time window so that particles sampled in the iewvps are from a set of 4d trajectories that are valid at each time step throughout the window not only at the beginning of the time window therefore the equal weights adjustment component operates on each model time step practically and theoretically although we only mentioned the initial condition x 0 at the beginning of the time window in step 1 above the 4d trajectories of the 4d var minimization can be obtained by integrating the nonlinear model m with the model error η as follows 25 x k m k k 1 x k 1 η k it should be noted that the model error is assumed to be additive and not state dependent in the roms weak constraint 4d var as shown in eq 25 in step 4 the equal weights adjustment component across the whole time window are not directly added to the 4d trajectories of the 4d var the 4d trajectories of the 4d var minimization are adjusted using the equal weights adjustment component as follows 26 x k m k k 1 x k 1 η k α 1 2 p 1 2 ξ k the iewvps process on each time step for each particle is shown in fig 2 it should be noted that the adjustment of η k and α 1 2 p 1 2 ξ k could be in opposite direction e g the t 3 and t k in fig 2 3 experiment setups 3 1 roms configuration the model domain spans from 105 e to 128 e and from 15 n to 24 n with a horizontal resolution of 1 6 and 24 vertical levels this is an eddy permitting resolution that can resolve both large scale circulation and mesoscale eddies the surface layers in the model are weakly nonlinear which fits the linear and gaussian assumptions of the enkf and variational methods as mentioned by chen et al 2020b this relatively low resolution is more feasible for the enkf and variational methods but may be challenging for particle filters due to the high dimension of the system thus we can use this resolution to test the performance of the iewvps in weakly nonlinear situations a model free run was initialized using the initial conditions obtained from hycom ncoda hybrid coordinate ocean model navy coupled ocean data assimilation metzger et al 2014 product on 1 jan 2007 and the model was integrated for 10 years without any data assimilation with atmospheric forcings obtained from ecmwf era interim datasets include windstress heat flux and freshwater flux and open boundary conditions obtained from the hycom ncoda product there are 4 purposes for this real simulation 1 to validate the model before data assimilation fig 3 2 to derive statistics regarding the climatic background error standard deviation which is important for 4d var fig 3d and f for ssh and sst 3 to provide a mean sea surface height fig 3b for satellite sea level anomaly sla assimilations and 4 tp provide a dynamical balanced initial condition for assimilation experiments the climatic monthly mean values and standard deviations of the sea surface height ssh and sea surface temperature sst fig 3 were used for the model validation without any data assimilation as shown in fig 3 roms successfully captured the large scale ssh pattern as well as the variabilities in ssh and sst in the northern south china sea nscs this pattern and these variabilities are coincident with the satellite observations 3 2 data assimilation experiments to test performances of the iewvps satellite derived ssh and sst observations are assimilated using the iewvps method and two additional experiments were designed table 1 en4dvar is an ensemble of perturbed weak constraint 4d var similar to the eda strategy of ecmwf bonavita et al 2017 the wcpsas experiment uses a single weak constraint 4d var en4dvar and iewvps are ensemble methods but wcpsas is not the difference between iewvps and en4dvar is that an equal weights adjustment step exists in the iewvps making the whole system a practical particle filter data assimilation system from a practical perspective the model error cannot be neglected thus we do not use the strong constraint 4d var strong constraint assumes that the model error is perfect and without error the ssh data were delayed time and gridded maps of sea level anomaly mlsa from copernicus marine environment monitoring service cmems the sst data were gridded version 2 avhrr only products obtained from noaa these data were developed using optimum interpolation and included a large scale adjustments of satellite biases with respect to the in situ data collected from ships and buoys reynolds et al 2007 both the sshs and ssts are level 4 products and are available daily with a horizontal resolution of 1 4 although the observation errors in gridded products are never uncorrelated in the actual nature we still assume that observation errors are uncorrelated in space and time for simplicity which leads to the fact that the observation error covariance r is a diagonal matrix the observation error is assumed to be a combination of the measurement error and the representation error which are additive measurement errors are considered to be independent of the data source and have the following standard deviations moore et al 2011a 0 4 c for sst and 2 cm for ssh the representativeness error is measured by the standard deviation of the observations that contribute to each super observation to diminish long distance false dependencies the decorrelation length scales of the covariance matrix were set to 50 km in the horizontal direction and 30 m in the vertical direction moore et al 2011a assimilation experiments started from 1 oct 2015 and lasted until 30 nov 2015 with a 1 day assimilation time window in the iewvps and en4dvar experiments the initial and atmospheric forcing ensemble members were generated through an exact second order sampling scheme hoteit et al 2013 chen et al 2020b 27 x i x β n e ϵ i t where x represents the ensemble mean of the initial conditions a 9 year averaged model state or atmospheric forcings windstress and heat flux in this study e is a matrix whose columns consist of n 1 eofs empirical orthogonal functions the term ϵ i is the i th row of a n n 1 random matrix with orthogonal columns and the sum of columns is zero β is an inflation parameter with a value of 1 0 for initial conditions 0 2 for the windstress and 100 for the heat fluxes the ensemble spread of surface net heat flux generated with a factor 100 is approximately 10 w m 2 in all assimilation experiments the observations were cycled assimilated into the model fig 4 4 results the northern south china sea nscs is strongly influenced by monsoons and the kuroshio as a result the large scale circulation is cyclonic with active mesoscale processes moreover the nscs is also an area where typhoons generate or pass through frequently during the period from 1 oct to 10 oct in 2015 typhoon mujigae passed through the nscs and the temperature of the upper layers was lowered by ekman pumping in this research the upper layer states are investigated and compared among different assimilation experiments the surface states at the end of the time window are used for the comparisons while the error statistics of forecasts for november 2015 are also compared 4 1 upper ocean states the sla simulations are shown in fig 5 for the duration of the typhoon the sla was high in the nscs especially along the coast and low in the western pacific the mesoscale eddies in the three assimilation experiments are consistent with the observations the sla differences between the three experiments and satellite products are small approximately 4 cm as shown in fig 5 generally the three assimilation experiments performed similarly in modeling the slas when a typhoon crosses the sea the ekman pumping induced by the typhoon can cause dramatic changes in upper level seawater temperatures especially at the sea surface this sst cooling is shown in fig 6 comand exceeds 1 c around the studied typhoon track the cooling patterns output in the three experiments are similar during the cooling period before 5 oct large differences appeared on 5 oct and the sst cooling was strongest in the iewvps experiment and remained consistent with the avhrr ssts while a warm bias existed in the en4dvar experimental outputs the wcpsas performed similarly to the iewvps in modeling sst cooling however the ssts tended to be overly high in the northeastern part of the modeled region fig 6b after 5 oct the ssts began to recover and large biases can be seen in the western philippines in the wcpsas experiment fig 7b the en4dvar experiment performed better than the wcpsas experiment but the outputs were still overly high according to the avhrr products fig 7c the influences of satellite data are not limited to the surface and can propagate vertically through the cross correlations of the background error covariance matrix and move forward through the model dynamics the temperatures along typhoon mujigae track are shown in fig 8 although the sst cooling reflected in the wcpsas outputs was similar to that output by the en4dvar and iewvps before 5 oct the temperature changes output by the wcpsas in the upper 100 m were worse than those of the en4dvar and iewvps compared with en4dvar the temperatures obtained from iewvps were cooler in upper 50 m and warmer below 100 m fig 9 indicating that the ekman pumping effect was more significant in the iewvps experiment than in the en4dvar experiment compared with the observed ssts the temperatures near 119 e tended to be overly high in the wcpsas and en4dvar outputs these high temperatures also existed in the deterministic part of the iewvps and were improved after an equal weights adjustment in the iewvps fig 9 as a result the temperatures near 119 e in the iewvps outputs were cooler than those output by the en4dvar fig 9 it should be noted that the large biases on 2 oct are mainly caused by the initial ensemble generation scheme calculated through eq 27 when assimilating ssts the error tends to be largest at the thermocline at a depth of approximately 100 m luo et al 2017 this is caused by the net heat input when correcting the cold bias in the mixed layer shu et al 2009 4 2 error statistics the above analyses are qualitative we also quantitatively compared the three experiments to make the comparisons more effective and convincing the assimilation period was extended to 2 months from 1 oct to 30 nov 2015 however only the results obtained in november 2015 were compared to exclude the influence of the initial ensemble generated using eq 27 the root mean square deviations rmsds of the analysis states at the end of the assimilation window that is t n for a window 0 n and the 1 day forecast states were calculated as shown in fig 10 the rmsds of the sshs output in the iewvps and en4dvar experiment were equivalent and slightly smaller than those output in the wcpsas experiment fig 10a and c for the ssts the rmsd at the end of the assimilation window in the iewvps experiment was 0 1 c fig 10b which was smallest among the three experiments and corresponded to a reduction of 52 6 relative to the en4dvar rmsd the sst rmsd of the 1 day forecast fig 10d was larger than that of the analysis states at the beginning of the assimilation window as was expected compared with the en4dvar experimental outputs the sst rmsd of the 1 day forecast in the iewvps experiment was reduced by approximately 10 5 to compare the three experiments more clearly the rmsd time series shown in fig 10 were used to draw box plots fig 11 consistent with fig 10 the ssh rmsds in the three experiments are at the same level among the three methods the sst rmsd of the analysis and 1 day forecast are the smallest in the outputs of the iewvps experiment to be more objective we also compared the output ssts with esacci european space agency sea surface temperature climate change initiative merchant et al 2019 and ukmo sst good et al 2020 products as shown in fig 12 since the horizontal resolutions of the esacci and ukmo sst products are 1 20 and our model resolution was 1 6 we interpolated the esacci and ukmo ssts to our model grids the rmsds relative to the esacci and ukmo ssts were larger than those relative to the avhrr product despite the differences in the sst products the iewvps produced the smallest rmsd among the three assimilation experiments at the end of the assimilation window the sst rmsd of the iewvps was reduced by 12 1 esacci and 12 3 ukmo compared to the en4dvar values fig 12 for the 1 day forecast the sst rmsd of the iewvps was reduced by 6 7 esacci and 6 2 ukmo compared to the en4dvar values in general the ssh rmsds of the three assimilation experiments were similar and the sst rmsds in the iewvps experiment were the smallest especially for the analysis states at the end of the time window during the 1 day forecast step the rmsds increased for both sshs and ssts one factor affecting this result is that large biases appear at open boundaries as shown in figs 5 to 7 the other factor is that the model error is not considered during the forecast step given the same atmospheric forcing and open boundary conditions the rmsds of the forecasts are dependent on the initial conditions which are characterized by the analysis states at the end of the time window fig 10c and d for the purpose of understanding how the equal weights adjustment component works we compared the biases of the ssts the bias is the mean value of the distances between observations and the model simulated values y h x and is shown in fig 13 there are at least four sources accounting for the biases in our experiments the initial conditions perturbed atmospheric forcing open boundary conditions and model errors as shown in fig 13a the bias in the initial conditions is very small a mean value of approximately 0 002 c so the initial conditions can be excluded from the sources of biases however as the model is forced by perturbed atmospheric forcings open boundary conditions and model errors the bias becomes larger when the model propagates over time thus the output ssts were biased at the end of the time window in the en4dvar experiment with a mean value of 0 1 c and a similar pattern was found in the wcpsas experiment we can see that the bias in the iewvps experiment was very small even at the end of the time window fig 13b and the mean value was approximately 0 007 c the sst bias in the en4dvar experiment is approximately 0 093 c greater than that of the iewvps experiment this bias deviation was close to the 0 1 c deviation in the rmsds calculated between the en4dvar and iewvps experiments the rank histograms of the one model run experiment conducted for november 2015 were also compared as shown in fig 14 the rank histograms were generated by ranking the observations in the set of the state ensemble members from lowest to highest a left extreme pattern was found in the en4dvar experiment fig 14a indicating a positive bias hamill 2001 in the iewvps experiment slightly u shaped rank histograms indicated that the ensemble spread was slightly smaller than the rmsd together with fig 13 we can conclude that the equal weights adjustment components can reduce the bias at the end of the time window we know that the full particle weight of each particle consists of the proposal weight and the likelihood weight the implicit equal weights scheme ensures that the full weights are equal for all particles which introducing more freedom in the iewvps than the en4dvar to fit the observations 4 3 an additional experiment the above analyses are based on typhoon mujigae which occurred in 2015 and whose track was almost a straight line in reality some typhoons may turn to the north exhibiting complex air sea interactions typhoon megi which occurred in 2010 was one of these one trial is not enough to show the advantages of the iewvps thus we performed the same experiments as those described above for typhoon megi in 2010 the largest sst cooling occurred on 22 and 23 oct as shown in fig 15 the sst cooling in the iewvps experiment was closer to the avhrr products than those in the en4dvar experiment especially the 2 c isotherm the rmsds of the sshs and ssts are shown in fig 16 the rmsd of sshs in the iewvps experiment was equivalent to that in the en4dvar but the rmsd of sst in the iewvps experiment was much smaller than that in the en4dvar experiment since the ensemble spread is a measure of the uncertainty in the data assimilation method it is better to have a slightly too large spread than a too small spread the ratios of the rmsds to the ensemble spreads for the sshs and ssts output by the iewvps were slightly larger than 1 0 indicating that the ensemble is under dispersed similar to the case of typhoon mujigae a warm bias existed at open boundary and propagated to the interior ocean fig 6b and fig 7b 5 discussions and conclusions in this study the implicit equal weights variational particle smoother iewvps method was implemented in the regional ocean modeling system roms and was applied to the assimilation of ocean satellite data the key problem we solved in the implementation of the iewvps into roms is the estimation of the equal weights stochastic adjustment component we used an implicit random mapping method to estimate the perturbation component which was represented through the tangent linear model and then combined this component with the equal weights parameter α the iewvps uses a modified weak constraint 4d var framework as the proposal transition density in a standard implicit equal weights particle filter iewpf this method combines the merits of the weak constraint 4d var and the iewpf it reduces the rmsd by applying the weak constraint 4d var as the proposal density and prevents filter degeneracy by using an implicit equal weights sampling scheme to verify the effects of the iewvps method we tested the responses of the upper ocean variables during a typhoon case study before and after the assimilation of ocean satellite data satellite remote sensing sst and ssh observations in the northern south china sea area were assimilated in the experiments and the results of the iewvps were compared with those of the en4dvar method and roms weak constraint 4d var wcpsas the ssh rmsds were equivalent among three assimilation experiments while the sst rmsds in the iewvps experiment was the smallest for both the analysis at the end of the time window and 1 day forecasts because the rmsds were reduced at the end of the time window the sst rmsds of the 1 day forecast derived from the iewvps experiment were reduced by 10 5 avhrr 6 7 esacci and 6 2 ukmo relative to the results of en4dvar experiment therefore the temperature cooling induced by ekman pumping in the upper 50 m was best characterized in the iewvps experiment however the largest improvement did not occur during typhoon activity over the sea but occurred after the typhoon made landfall over china excessive temperature arose at open boundaries in the en4dvar and wcpsas experiments this situation was relieved in the iewvps experiment the bias existed at open boundaries is one of the reasons why the rmsds were increased in the 1 day forecasts another reason is that the model errors are considered during the assimilations but are neglected during the forecasts to improve the forecasts model error should be accounted for such as by using the stochastically perturbed parameterization tendencies sppt or stochastic kinetic energy backscatter skeb schemes in the model as done in ecmwf bonavita 2011 to increase the ensemble spread an exact second order sampling scheme hoteit et al 2013 chen et al 2020b was adopted for the atmospheric forcings one drawback of this scheme is that the choice of inflation parameter is artificial the inflation parameter was 0 2 for the windstress and 100 for the heat fluxes these values are not optimal the other drawback is that the independent choices of inflation parameters for different variables windstress and heat fluxes in our study may lead to inconsistent statistical results among variables another drawback of the perturbation scheme used in our study is that the covariance of atmospheric forcing is static and cannot reflect the high frequency variabilities in the atmosphere to improve the ensemble quality the perturbation scheme for atmospheric forcing should be studied further and the uncertainty at open boundaries should also be considered the ensemble spread is also influenced by the sampling ratio of α on the 1 and 1 branches zhu et al 2016 however the α 1 solution is spurious which is leading to a systematic bias this systematic bias can be eliminated by the two stage iewpf proposed by skauvold et al 2019 then the ensemble spread will be dependent on a different parameter β we will replace the implicit equal weights scheme according to skauvold et al 2019 and investigate an adaptive scheme for β to improve the ensemble quality the computational cost of the iewvps is slightly larger than that of the en4dvar since the iewvps is based on the en4dvar a set of weak constraint 4d var additional costs for each particle are needed to calculate of the equal weights adjustment component including α i and p 1 2 ξ i 0 n in total an additional tl tangent linear model integration is needed for the calculation of the equal weights adjustment component and a nl nonlinear model integration is needed to move each proposal particle to its equal weight position for each particle these computational costs are relatively small compared to the total cost of each 4d var 2 nl integrations 30 tl integrations and 30 adjoint model integrations as shown in fig 1 we need to calculate and deliver d 1 2 ξ 0 n the assimilation window in our study is 1 day which equals 120 model steps one time step is 720 s thus we need to deliver n 120 d 1 2 ξ for each particle as outputs if the assimilation window is very long or the time step is very small this cost will become larger one method used to reduce this cost involves delivering d 1 2 ξ outputs every m model steps which in our study m 15 equivalent to 3 h in the simulation the total computational cost of the iewvps scheme was divided into fortran and matlab components the matalb components include the calculation of α i and svd on r 1 2 q 0 the computational cost of the fortran component is 910 83 seconds particle in one cycle and the cost of the matlab component is less than 60 s while the computational cost of each 4d var is 852 18 s therefore the additional computational cost is approximately 120 s for each particle an increment of approximately 14 relative to cost of the 4d var the matlab components will be rewritten in fortran language thus further reducing the computational cost one advantage of the iewvps is that the equal weights stochastic adjustment component maintains the model dynamic balance because of the balance operator in 4d var and the balanced theoretical implicit equal weights sampling framework we know that the major advantage of the enkf is the flow dependent background error covariance matrix therefore one drawback in our study is that the background error covariance matrix b is climatological and static furthermore a flow dependent b matrix based on the pf and 4d var method will be investigated in the future credit authorship contribution statement pinqiang wang conceptualization methodology software writing original draft writing review and editing formal analysis investigation validation visualization data curation mengbin zhu conceptualization methodology formal analysis investigation writing original draft writing review and editing validation supervision funding acquisition yan chen conceptualization methodology formal analysis investigation funding acquisition weimin zhang conceptualization methodology writing review and editing project administration supervision funding acquisition yi yu visualization formal analysis funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the weak constraint 4d var codes in this study is downloaded from http www myroms org satellite sshs are downloaded from http marine copernicus eu satellite ssts are downloaded from ftp eclipse ncdc noaa gov this paper is supported by the national key r d program of china 2018yfc1406202 the national natural science foundation of china nsfc grant nos 42005113 41675097 41830964 42075149 
23898,waves in large lakes put coastal communities and vessels under threat and accurate wave predictions are needed for early warnings while physics based numerical wave models such as wavewatch iii ww3 are useful to provide spatial information to supplement in situ observations they require intensive computational resources an attractive alternative is machine learning ml methods which can potentially improve the performance of numerical wave models while only requiring a small fraction of the computational cost in this study we applied novel ml methods based on xgboost and a long short term memory lstm recurrent neural network for predicting wave height and period under the near idealized wave growth conditions of lake erie data sets of significant wave height h peak wave period t p and surface wind from two offshore buoys from 1994 to 2017 were processed for model training and testing we trained and validated the ml models with the data sets from 1994 to 2015 and then used the trained models to predict significant wave height and peak period for 2016 and 2017 the xgboost model yielded the best overall performance with mean absolute percentage error mape values of 15 6 22 9 in h and 8 3 13 4 in t p the lstm model yielded mape values of 23 4 30 8 in h and 9 1 13 6 in t p an unstructured grid ww3 applied to lake erie yielded mape values of 15 3 21 0 in h and 12 5 19 3 in t p however ww3 underestimated h and t p during strong wind events with relative biases of 11 76 to 14 15 in h and 15 59 to 19 68 in t p xgboost and lstm improve on these predictions with relative biases of 2 56 to 10 61 in h and 8 08 to 10 13 in t p an ensemble mean of these three models yielded lower scatter scores than the members with mape values of 13 3 17 3 in h and 8 0 13 0 in t p although it did not improve the bias the ml models ran significantly faster than ww3 for this 2 year run on the same computing environment ww3 needed 24 h with 60 cpus whereas the trained lstm needed 0 24 s on 1 cpu and the trained xgboost needed only 0 03 s on 1 cpu 1 introduction accurate predictions of wave conditions are important for the offshore industry shipping and for mitigating coastal hazards such as dune erosion various wave models have been developed and implemented in ocean and coastal regions following mahjoobi and etemad shahidi 2008 and pirhooshyaran et al 2020 these can be divided into three broad categories namely i physics based models which feature parameterizations of the wave action balance equation ii statistical and machine learning models hereafter ml models also known as soft computing methods which instead use the data structure for prediction without explicit description of physics and iii hybrid approaches which combine the first two categories physics based models have been developed during the course of three generations starting with simple parameterizations of the relationships between wind wave height and wave period to the current third generation in which four wave nonlinear interaction and various other source terms are explicitly modeled in frequency directional space e g wam wamdi group 1988 swan booij et al 1999 and wavewatch iii hereafter ww3 tolman et al 2002 for lake erie one of the five great lakes first and second generation physics based wave models used simple parameterization to account for the evolution of wave height and period due to wind forcing schwab et al 1984 third generation wave models such as swan and mike21 sw sørensen et al 2004 which explicitly represent three and four wave interactions have subsequently been applied to lake erie moeini and etemad shahidi 2007 a coupled wave hydrodynamic model based on the unstructured grid finite volume community ocean model fvcom chen et al 2013 and a third generation wave model based on swan qi et al 2009 was applied to lake erie to investigate wave climatology and inter basin wave interactions niu and xia 2016 alves et al 2014 implemented the third generation ww3 model as the dynamical core of the national oceanic and atmospheric administration s noaa great lakes operational wave forecasting system while physics based wave models generally provide satisfactory results in comparison with observations the physics they embody are known to underperform under highly nonlinear extreme events such as storm peaks e g alves et al 2014 furthermore a practical problem especially in an operational environment is their high computational expense statistical and ml models soft computing methods were first applied to significant wave height prediction by deo and sridhar naidu 1999 deo et al 2001 and agrawal and deo 2002 these researchers applied small fully connected feed forward artificial neural networks anns as well as autoregressive moving average arma or autoregressive integrated moving average arima stochastic time series models to predict waves at point locations using as input previous wave conditions univariate modeling or wind observations multivariate modeling mandal and prabaharan 2006 introduced the use of recurrent neural networks rnns to wave forecasting they applied an rnn to predict significant wave heights based on a time series history of observed significant wave heights more recently srinivasan et al 2017 compared feed forward networks and rnns in the context of multivariate wave modeling using observed wind as input and found the rnns to have better skill all these studies applied neural networks with only one hidden layer and limited data sets of only 1 2 years mahjoobi and etemad shahidi 2008 and etemad shahidi and mahjoobi 2009 introduced the use of single classification and regression trees using c5 0 cart and m5 training algorithms as an alternative to neural network based approaches the former study considered 5 years of wind and wave data on lake michigan from the national data buoy center ndbc station 45007 the authors found better performance for regression trees than classification trees and that anns performed slightly better than either tree model the authors note that as with anns tree models do not require knowledge of the underlying physical process to make predictions however they provide more insight into the problem than anns since they represent understandable rules mahjoobi et al 2008 and malekmohamadi et al 2011 compared a range of other ml approaches including support vector machines bayesian networks fuzzy inference systems and adaptive neuro fuzzy inference systems to anns in lake ontario and lake superior they found comparable accuracy between these alternative methods and anns except for bayesian networks which performed poorly however it should be noted that the wind and wave data used in their study covered less than two years and the ann used had only one hidden layer with three neurons recent years have seen breakthroughs in the development of ml training algorithms hardware and a massive increase in data peres et al 2015 used observed winds u v at or near three coastal wave buoy stations as input and observed significant wave heights h as output to train a single hidden layer fully connected feed forward neural network for each station fig 1a the data set covered a 20 year period from 1989 to 2008 the model input included observed wind from locations upwind of the output station as well as at time lags to account for the fetch and duration in wind wave generation james et al 2018 trained a fully connected feed forward neural network to produce a field of coastal wave heights h 1 h n using as input three offshore wave characteristics height h period t and direction d a 12 2 field of wind vectors u v a 357 2 field of ocean currents u v and wave height fields from swan simulations as target variables fig 1b recently feng et al 2020 used fully connected a feed forward neural network to model waves in lake michigan a number of more complex neural network applications have furthermore been proposed mahmoodi et al 2017 compared feed forward and cascade forward networks but found better performance in the former kumar et al 2018 investigated the use of ensembles of extreme learning machines to reduce the variation caused by random initialization prahlada and deka 2015 and dixit and londhe 2016 combined anns with wavelet analysis to better model extreme wave events pirhooshyaran and snyder 2020 introduced the use of sequence to sequence networks for the prediction wave height and output power feature selection and the reconstruction of missing wave observations using data from nearby stations pirhooshyaran et al 2020 expanded this work by including an ensemble of sequence to sequence networks introducing a new resampling technique and improving feature selection they demonstrate the superiority of this approach as the forecast lead time increases hybrid approaches combine the strengths of physics based models and ml this includes the use of ml models to perform data assimilation on the output of physics based models e g makarynskyy 2006 londhe et al 2016 and the creation of nonlinear ensemble means e g campos et al 2020 in the present study we investigate two different ml approaches for learning the nonlinear relationships between observed wind data significant wave height and peak wave period with the aim to correct the underprediction of the latter two quantities by physics based spectral wave models in particular at storm peaks the first is an ensemble tree based model xgboost chen and guestrin 2016 which is based on the random forest model breiman 2001 xgboost is an optimized distributed gradient boosting library designed to be highly efficient flexible and portable it provides a parallel tree boosting that solves many data science problems such as regression e g qian et al 2020 and classification e g merembayeva et al 2019 fast and accurately xgboost has built in l1 and l2 regularization which prevents the model from overfitting it is furthermore robust having a capability to handle missing values the second model is from the rnn family used here to capture the dependency of significant wave height and peak wave period on the time series structure of recently observed winds specifically we applied the long short term memory lstm hochreiter and schmidhuber 1997 implemented using the keras high level framework with a tensorflow backend chollet 2017 to compare with physics based models we include simulations with the unstructured mode of ww3 roland and ardhuin 2014 the field case considered here is lake erie which is an enclosed basin with a wind climate directed along its major axis hourly metocean data are available at ndbc stations 45005 and 45142 at opposite ends of the fetch for the period 1994 2017 it therefore represents near idealized conditions under which to compare the modeling of wind wave growth by ml and physics based models the xgboost fig 1c and lstm fig 1d models were trained using observed wind u v significant wave height h and peak wave period t p at these two data buoys fig 2 and subsequently evaluated against holdout data and the physics based model ww3 this paper is structured as follows section 2 describes the field case and data set used to train and evaluate the two ml models sections 3 and 4 provide details of respectively the xgboost and lstm models section 5 describes the physics based ww3 model and the configuration applied here in section 6 the performance of the two ml models and ww3 as well as their ensemble mean are intercompared and in section 7 model sensitivity is investigated section 8 closes with conclusions drawn from these results 2 field case data set the field case considered here for the training and evaluation of our ml models is lake erie one of the great lakes that is shared between the united states and canada fig 2 lake erie is narrow with a length of 338 km and a width of 92 km and has a wsw ene orientation it is relatively shallow with a maximum depth of 64 m but with mean depths of only 7 3 m in the western basin 18 3 m in the central basin and 24 4 m in the eastern basin lake erie has several offshore buoys as part of its observational network of which two have collected continuous hourly data from 1994 to 2017 during the ice free spring summer and fall seasons located on the western end of the lake near toledo oh is station 45005 9 8 m depth maintained by noaa s national data buoy center ndbc located on the eastern end of the lake near buffalo ny is station 45142 27 m depth maintained by the environment and climate change canada the historical and real time measurements from these two offshore buoys are available from ndbc 2020 this data was cleaned by i dropping records where the wind speed significant wave height and peak wave period were not available simultaneously as required for training and evaluation ii dropping records where the significant wave height is very low 0 05 m indicating low energy noise and iii dropping records where the peak wave period had a quality flag of 1 indicating doubtful or erroneous values or an unphysical value for this small lake 14 s in addition based on visual inspection some years were excluded due to the doubtful quality of the peak period observations at station 45142 namely 1996 2003 2006 and 2010 figs 3 to 5 show the distributions of the cleaned monthly significant wave height peak wave period and wind speed during the 24 year observational period considered fig 6 shows the observed wind climate in terms of wind roses at ndbc buoys 45005 and 45142 from fig 3 we can see that the distribution of significant wave heights is strongly skewed to the right higher values with the third quartile below 1 5 m but with outliers representing storm peaks of up to 5 2 m there is clear seasonality present with the largest significant wave heights recorded in spring april may and fall october november similarly fig 4 shows peak wave periods to be skewed to the right and display seasonality with the highest values recorded in the spring and fall fig 5 shows that this is correlated to similar seasonal wind speed magnitude patterns which is to be expected considering that all wind wave conditions in this shallow lake are locally generated figs 3 and 4 furthermore show that extreme significant wave heights and peak periods tend to be higher at station 45142 than station 45005 considering that the dominant wind direction is from the wsw ssw fig 6 these larger wave heights are due to the longer fetch available to the downwind station 45142 note that the extreme depth limitation of 7 3 m at station 45005 also imposes a depth limitation to the maximum possible wave height which is relaxed at the deeper buoy 45142 the input variables that were selected to construct our ml models are firstly the observed wind speed and direction at stations 45005 and 45142 these are transformed into the wind vector components u and v at each station second following peres et al 2015 we add lags of these wind vectors to capture storm duration considering the 338 km length of the lake and a maximum observed wind speed of 20 6 m s during the study period fully developed waves would be generated after a duration of about 24 h following bretschneider 1958 therefore lags at 3 hourly intervals of up to 24 h were included the exact number of lags included was determined as part of hyperparameter tuning see below this yields a maximum of 18 input variables per observation station or 18 2 in total the observed significant wave height and peak wave period at stations 45005 and 45142 are used as the output target variables note that in general wave conditions are also affected by lake ice coverage and thickness and hence lake surface temperature which could serve as additional input variables however here we only consider wave growth during ice free months april to early december so that these variables are not included in order to train and evaluate our ml models on this data we split the data set into training validation and test partitions considering that these are time series data in which serial correlation exists between successive observations the usual approach of random assignment to the three partitions is not followed instead continuous periods of the time series are assigned to each partition e g peres et al 2015 the bulk of the data set namely the years 1994 2012 is assigned to the training partition in order to achieve reliable estimates of the trainable model parameters next the years 2014 2015 are assigned to the validation partition which is used in the selection of model hyperparameters such as tree depth and to check for overfitting on the training data finally the years 2016 2017 are assigned to the test partition this latter portion of the data set will be used to test how well the ml models generalize to unseen conditions at the two locations for which they were trained the test partition will also be used to assess the performance of the physics based model ww3 at these two stations the resulting split between the three partitions is 28154 records for training 5193 for validation and 4493 for testing or about 74 14 12 fig 7 compares the distribution of observed significant wave height and peak wave period in the train validation and test partitions at stations 45005 and 45142 we can see that the median and interquartile distributions are similar between the training validation and test partitions however we note that in terms of peak events outliers the test partition is less energetic than the train and validation partitions 3 xgboost model 3 1 model description xgboost chen and guestrin 2016 is mostly used for supervised learning problems where one uses training data to predict a target variable xgboost implements ml algorithms under the gradient boosting framework it uses the same tree ensemble model as the random forest model breiman 2001 consisting of a set of classification or regression trees random forest and boosted trees are basically the same models the only difference is how one trains them mathematically an ensemble tree model can be written in the form e g chen and guestrin 2016 1 y ˆ i k 1 k f k x i f k f where k is the number of trees and f x is a tree function in the function space f f x w q x the data set d x i y i has n examples and m features x i r m y i r the structure of each tree is given by q which maps an example x i to the corresponding leaf index each tree function f k corresponds to a structure q and leaf weights w for a total of t leaves the objective loss function with parameters θ to be optimized is given by 2 l θ i l y i y ˆ i k ω f k where l is a differentiable convex loss function that measures the difference between the prediction y ˆ i and the target y i ω f is the regularization term which penalizes the complexity of the model gradient tree boosting is achieved by training the model in an additive manner tree functions f t are greedily added to successive iterations to minimize the loss function 2 let y ˆ i t 1 and y ˆ i t be the predictions at iterations t 1 and t respectively of the i th instance setting y ˆ i t y ˆ i t 1 f t x i the loss function at the t th iteration will be 3 l t i 1 n l y i y ˆ i t 1 f t x i ω f t recalling the taylor expansion f x δ x f x f x δ x 1 2 f x δ x 2 we obtain 4 l y i y ˆ i t 1 f t x i l y i y ˆ i t 1 g i f t x i 1 2 h i f t 2 x i then we have 5 l t i 1 n l y i y ˆ i t 1 g i f t x i 1 2 h i f t 2 x i ω f t constant where g i y ˆ i t 1 l y i y ˆ i t 1 and h i y ˆ i t 1 2 l y i y ˆ i t 1 define the penalty on the complexity of the model with the regularization term ω f γ t 1 2 λ j 1 t w j 2 with t the number of leaves then with constant terms removed the simplified objective loss function at iteration t is 6 l t i 1 n g i f t x i 1 2 h i f t 2 x i γ t 1 2 λ j 1 t w j 2 define the instance set of leaf j as i j i q x i j then 7 l t j 1 t i i j g i w j 1 2 i i j h i λ w j 2 γ t define g j i i j g i and h j i i j h i then 8 l t j 1 t g j w j 1 2 h j λ w j 2 γ t and then 9 argmin x l 1 2 j 1 t g j 2 h j λ γ t where the optimal weight w j of leaf j is given by w j g j 2 h j λ in summary the boosted tree algorithm is the following 1 add a new tree in each iteration 2 at the beginning of each iteration calculate g i y ˆ i t 1 l y i y ˆ i t 1 and h i y ˆ i t 1 2 l y i y ˆ i t 1 3 use the statistics to greedily grow a tree f t x with l 1 2 j 1 t g j 2 h j λ γ t 4 add f x to the model y ˆ i t y ˆ i t 1 ε f t x i where ε is called shrinkage 3 2 configuration and hyperparameter tuning the xgboost model used hourly wind significant wave height and peak wave period from observations at the buoy stations in western 45005 and eastern 45142 lake erie fig 2 the observed wind speed and direction at the two stations were used as input x i and significant wave height and peak wave period were used as model output y i for this regression problem we set the learning objective as regression with squared loss three hyperparameters were optimized in the xgboost model namely t d maximum tree depth w minimum child weight and e learning rate eta considering the limited number of hyperparameters a direct search strategy was followed for the optimization the search space was defined as t d 5 6 7 8 9 10 11 12 w 3 4 5 6 7 8 and e 0 005 0 01 0 05 0 1 0 2 one parameter was tuned at a time with other parameters fixed until all parameters were tuned the process was repeated three times mean absolute percentage error mape was set as the metric for training and the mean absolute error mae as the evaluation metric for cross validation table 1 shows that model accuracy improved with increasing maximum tree depth t d up to 10 but was quite insensitive to the minimum child weight w the combination of t d 10 and w 5 yielded a minimum mae 0 07466 m table 2 shows that the optimal learning rate e 0 05 yielded a minimum mae 0 0747 m this model training ran on a cpu intel xeon cpu e2176m 2 70 ghz for about 3 min per hyperparameter combination and it converged at around 4000 epochs 3 3 xgboost results the trained xgboost model was used to predict wind wave conditions for the unseen test period of 2016 2017 it needed only 0 03 s for this prediction using 1 cpu figs 8 and 9 show the time series and error statistics the prediction for station 45005 yielded a mae of 0 074 m in h and 0 309 s in t p with maximum absolute errors of 0 853 m and 2 613 s respectively this corresponds to mape values of 15 611 in h and 8 322 in t p the downwind station 45142 yielded larger errors namely a mae of 0 100 m in h and 0 516 s in t p with maximum absolute errors of 1 045 m and 3 774 s respectively this corresponds to larger mape values of 22 890 in h and 13 445 in t p scatterplot comparisons with other the models will be discussed in section 6 4 long short term memory lstm model 4 1 model description our second model is a specialized class of artificial neural network namely a recurrent neural network rnn this class of network differs from the standard feed forward type in that the time dependency of the input variables is explicitly modeled rnn models are composed of a sequence of identical neural network cells each one representing one time step each of these units passes its nonlinear activation to the unit of the next time step thus allowing a system memory to build up this system memory is useful for modeling sequences such as time series of physical phenomena and natural language as motivated by the early work of mandal and prabaharan 2006 rnns are therefore well suited to the problem of wave height prediction a phenomenon that is known to depend on a time series history of winds here we apply a popular variety of the rnn namely the long short term memory lstm network by hochreiter and schmidhuber 1997 the structure of one cell of the lstm is expressed in the following set of equations see olah 2015 and chollet 2017 for graphical representations 10 f t σ w f a t 1 x t b f 11 i t σ w i a t 1 x t b i 12 o t σ w o a t 1 x t b o 13 c t tanh w c a t 1 x t b c 14 c t i t c t f t c t 1 15 a t o t tanh c t the input to the lstm consists of a time series vector x x 1 x 2 x t for t 1 t time steps in the series the vector a a 1 a 2 a t represents the network activation that is passed from one time step to the next thus representing the short term memory of the system a second longer term memory is represented by the vector c c 1 c 2 c t which is also passed from one time step to the next the distinction is that c or the carry state is not necessarily updated at every time step unlike the activation a as such c can carry information across several time steps influencing model outputs based on inputs much earlier in the time series the updating of c and its influence on the activation a is determined by a set of binary gates called the forget gate f t given by 10 the update gate i t 11 and the output gate o t 12 in each of these expressions σ is the sigmoid function w is the weight matrix and b is the bias vector at each time step a candidate c t for updating the previous carry state c t 1 is computed using 13 in 14 based on the outcomes of the update and forget gates c t is then updated to the candidate c t or just takes the same value as the previous carry state c t 1 thus continuing the long term memory finally the new activation a t is computed based on the value of the output gate and the current carry state c t using 15 the activation a t is then used to determine the output y t of the given time step via e g a rectified linear unit relu or softmax activation function however it is also passed forward to the next time step along with the carry state this lstm cell can thus be repeated for each time step in the sequence to be modeled as will be shown in the next section to build our lstm application in the keras framework the lstm cell described in the previous section was arranged in a time sequence covering the 3 hourly time lags of wind observation going back to a maximum of 24 h namely t 24 21 18 15 12 9 6 3 0 this results in a horizontal sequence of up to 9 lstm units for the wind input vectors x t to x t each unit passes along its activation a t and carry status c t to the unit at the next time step fig 10 the maximum number of lags t which represents the duration of wave generation will be determined via hyperparameter optimization below each wind input vector x t u 1 v 1 u 2 v 2 contains the wind velocity components at the two stations 45005 and 45142 at each time lag these input sequences were generated by moving a time window through the data set each time capturing a sequence of t wind input vectors this results in a training data set with an input size of m x t w 28154 t 4 where m x is the number of training sequences of length t and w 4 the number of wind vector components for the validation and test partitions the corresponding input sizes are 5193 t 4 and 4493 t 4 respectively at each time step t a series of four lstm cells are stacked each passing its activation to the next to increase complexity of the nonlinear relationships that can be modeled each lstm unit contains n neurons which is the second hyperparameter to optimize as described above inputs are received at each time step t 1 t the output however is only generated at the last time step t corresponding to the current time at which an estimate of the significant wave height is required at this time step the activation of the fourth lstm layer is passed to a four neuron dense layer with rectified linear unit relu activations this produces non negative regression outputs of the significant wave heights and peak periods y t h 1 h 2 t p 1 t p 2 at stations 45005 and 45142 4 2 hyperparameter tuning as described above the lstm structure features a sequence of n neurons per layer and t wind observations fig 10 this neural network was trained using a mini batch size of b and rmsprop optimization with a learning rate of l we therefore have four hyperparameters for this model namely n t b and l which were determined using derivative free optimization similar to the approach for the xgboost model a direct search strategy was followed the search space was defined as n 12 14 16 18 20 22 24 t 3 6 9 b 16 32 64 128 and l 0 001 0 01 0 1 for each hyperparameter combination the lstm model was trained for a maximum of 100 epochs and the epoch at which the minimum validation partition mape was found was recorded this minimum represents a balance between bias reduction and variance overfitting for that hyperparameter combination moving through the search space the combination for which the minimum validation set mape was found is identified as the optimum set of hyperparameters tables 3 and 4 show the results of this optimization for different dimensions of the search space table 3 shows a preference in the optimization towards using an input wind history of 6 time steps i e t 15 12 9 6 3 0 in particular when using slower learning rates furthermore the slowest learning rate of l 0 001 consistently yields the best results table 4 shows that the optimization is not sensitive to mini batch size this is because for a larger mini batch size the optimization responded by using more epochs to reach a minimum mape and vice versa an optimal combination of n 16 t 6 b 32 and l 0 001 was found which is identified in bold in both tables this model training ran on a cpu intel xeon cpu e2176m 2 70 ghz for about 3 5 min per hyperparameter combination fig 11 shows the training of this model in terms of aggregate mape over both stations 45005 and 45142 on the training and validation partitions up to the optimal epoch it can be seen that the error of both the training and validation partitions reduce rapidly during the first few epochs over the next decades of epochs the training partition score reduces steadily the score of the validation set also reduces on average although not as smoothly considering the smaller sample size for the computation of these metrics the mape of the validation set reaches a 3 point moving average minimum at 63 epochs suggesting that the model is not overfit after 63 epochs the model achieves an aggregate mape 15 995 over both stations for the training partition years 1994 2013 an mape 14 770 for the validation partition years 2014 2015 and an mape 15 328 for the unseen test partition years 2016 2017 we can therefore conclude that the lstm model is indeed not overfit and seems to generalize well yielding somewhat better results for the unseen test partition than the training and validation partition 4 3 lstm results figs 12 and 13 show the predicted lstm significant wave height and peak period and their error statistics for the unseen test partition years 2016 2017 the lstm model provides a good representation of the observed h and t p in both the low and high energy regimes importantly the lstm model captures all major wave events including the storm peaks at the upwind station 45005 the lstm model yields an mae of 0 103 m in h and 0 326 s in t p with maximum absolute errors of 0 885 m and 2 487 s respectively this corresponds to mape values of 23 421 in h and 9 072 s in t p as was seen for the xgboost model above errors tend to be larger at the downwind station 45142 an mae of 0 125 m in h and 0 499 s in t p with maximum absolute errors of 1 906 m and 3 831 s respectively this corresponds to mape values of 30 775 in h and 13 614 in t p the trained lstm needed only 0 24 s on 1 cpu to perform this 2 year prediction 5 wavewatch iii spectral model 5 1 model description the unstructured mesh used for the ww3 runs is shown in fig 14 which was generated using aquaveo s surface water modeling system sms the grid size distribution was configured as a function of the 3 arc second bathymetry data from the national centers for environmental information ncei the model bathymetry was obtained by interpolating the observed bathymetry onto each unstructured grid node using the inverse distance method high resolution noaa coastline data was applied to delineate the land boundary the resulting mesh is composed of 11509 triangular elements and 6106 nodes in the horizontal the resolution varies from approximately 100 m near the shore to 2 5 km offshore the model has distribution referenced to the great lakes low water datum of 173 5 m surface wind observations were interpolated to create hourly gridded surface meteorological analyses of wind the observations are from the great lakes weather data and marine observations glerl 2020 which includes coastal and offshore meteorological stations these data were corrected for over water conditions and interpolated along with available in lake buoys to the model grid fig 15 shows scatter and quantile quantile q q plots comparing the observed winds with the analysis winds used in ww3 from the q q plots we can see that the analysis wind fields match the observation at 45005 well although they slightly underestimate values greater than 10 m s conversely they slightly overestimate the observations at 45142 during strong events of around 15 m s we can therefore conclude that the wind fields applied to ww3 have a generally high accuracy and only a limited bias the ww3 simulation starts at 12 00 gmt on 01 january 2016 model results are output on an hourly interval at the same time step of the buoy data the model took 24 h on 60 cpus intel xeon cpu e5 2603 v4 1 70 ghz to complete the prediction for 2016 2017 the model settings were chosen to match those of the current noaa operational model for the great lakes alves et al 2014 which are wind input and dissipation source terms based on ardhuin et al 2010 flux computation included in the model source terms the third order propagation scheme with tolman 2002 averaging technique generalized multiple dia gmd nonlinear interaction tolman 2013 the empirical linear jonswap parameterization of bottom friction hasselmann et al 1973 depth induced breaking according to battjes and janssen 1978 linear time interpolation for wind interpolation and approximately linear speed interpolation for wind space interpolation no triad interactions used no bottom scattering used no supplemental source term used the model was compiled with netcdf version 4 as a stand alone program the model used message passing interface mpi with a distributed memory for more details refer to alves et al 2014 we used this model configuration as a benchmark and no additional tuning was done 5 2 ww3 results the ww3 model was implemented to predict significant wave height and peak wave period for 2016 2017 fig 16 shows the time series of modeled and observed significant wave height for station 45005 a mae of 0 082 m and a maximum absolute error of 0 873 m were found for station 45142 the model yielded an mae of 0 119 m and a maximum absolute error of 1 100 m this corresponds to mape values of 15 325 and 21 008 for the two stations respectively fig 17 shows the corresponding results for peak wave period at station 45005 a mae of 0 408 s and a maximum absolute error of 2 604 s were found and at station 45142 a mae of 0 653 s and a maximum absolute error of 5 864 s were found the mape values are 12 506 and 19 318 for the two stations respectively these figures show that ww3 tends to underestimate wave height and period peaks during strong wind events spikes on the figures we note that the slightly underestimated model winds at station 45005 during strong wind events fig 15 left may be the cause of the underestimation of these peaks however this cannot explain the behavior at the downwind station 45142 where the model winds slightly overestimate observations during strong wind events fig 15 right while the wave height and period peaks are still underestimated we tested an alternative wind data set from an operational weather forecast model by the high resolution rapid refresh hrrr benjamin et al 2016a b in ww3 simulations and the issue with underestimating the peak significant wave height still persisted and the maximum absolute errors were slightly worse than the results with the interpolated winds from the observations 6 model intercomparison 6 1 significant wave height fig 18 compares the performance of the two ml models and ww3 in terms of scatter plots of significant wave height at stations 45005 and 45142 for the test partition 2016 2017 these results are summarized in table 5 in addition to the mape and mae discussed above we also consider the scatter index si correlation coefficient cc bias relative bias and regression fit the xgboost model fig 18 top row displays a tight grouping around the line of perfect agreement in particular in the lower range of wave heights where the data density is the highest warmer colors we see a low scatter mae 0 074 m mape 15 61 si 0 160 cc 0 958 and small overall bias 0 011 m or 1 81 against the observations at the upwind station 45005 to assess the performance during storm conditions the bias is also computed over observations greater than the 95th percentile diamond markers score included in parentheses which also displays a small bias 0 042 m or 2 56 the regression fit through the results indicate a slope close to unity 0 974 and an intercept of close to zero 0 005 m at the downwind station 45142 the scatter increases mae 0 100 m mape 21 89 si 0 220 cc 0 970 and the overall bias increases to 0 063 m 9 28 we see a comparable bias of 0 239 m 10 61 over the data points greater than the 95th percentile this underprediction of h is reflected in the lower regression slope of 0 895 we note that the larger errors at the downwind station 45142 compared to the upwind 45005 is presumably due to the added complexity of its longer fetch over this elongated lake which is a nonlinear function of the wind direction refer figs 2 and 7 the lstm model fig 18 center row shows similar but weaker performance than the xgboost model in terms of significant wave height at the upwind station 45005 the scatter is larger mae 0 103 m mape 23 42 si 0 221 cc 0 931 and the overall bias 0 037 m or 5 91 and the bias of values greater than the 95th percentile 0 054 m or 3 34 are somewhat higher the regression slope is close to unity 0 989 and the intercept close to zero 0 030 m at the downwind station 45142 the scatter increases relative to xgboost and ww3 mae 0 125 m mape 30 78 si 0 269 cc 0 948 and the overall bias increases to 0 059 m 8 71 however for values greater than the 95th percentile the negative bias is the lowest of the three models at 0 201 m 8 93 this underestimation is reflected in the regression slope of 0 909 which is nonetheless the closest to unity of the three models the performance of ww3 fig 18 bottom row in terms of scatter is slightly poorer than xgboost and better than lstm both at station 45005 mae 0 082 m mape 15 33 si 0 180 cc 0 950 and 45142 mae 0 119 m mape 21 01 si 0 241 cc 0 963 in terms of overall bias ww3 performs similar or better than the two ml models with values of 0 024 m 3 91 and 0 002 m 0 33 at stations 45005 and 45142 respectively however these overall bias scores obscure a pattern of underestimation in the upper range of wave heights the bias values over values greater than the 95th percentile are significantly negative at 0 191 m 11 76 and 0 318 m 14 15 at stations 45005 and 45142 respectively this underestimation is reflected in the poorer regression slopes of 0 834 and 0 769 for these stations respectively conversely an overestimation in the lower ranges can be seen from the regression intercepts which at 0 079 m and 0 135 m indicates a marked deviation from zero fig 19 presents the corresponding q q plots in order to compare the distributions of the simulated significant wave heights to those of the observations in particular the upper quantiles right hand tail of each distribution represent the behavior of the modeled storm peaks the distributions of the xgboost and lstm model at stations 45005 and 45142 follow those of the observations closely however some negative bias can be seen towards higher values at 45142 reflecting a tendency to underestimate storm peaks this result might be surprising considering that ml models such as neural networks are typically unbiased when properly trained the finding that the lower quantiles agree better with observations in these ml models can be explained by the fact that the majority of the available training data cover this lower significant wave height range 75th percentile 0 8 m see fig 7 this concentration of data within the lower wave height range can also be seen from the color mapping in fig 18 improved model fitting at these upper quantiles can be obtained by including more storm peak observations in the training set or alternatively by using weighted samples with greater weights at these higher values not shown furthermore mahjoobi and etemad shahidi 2008 prahlada and deka 2015 and dixit and londhe 2016 suggest dividing the data into multiple parts and training a separate model on the higher wave height partition this is conceptually similar to threshold autoregressive models tar in time series statistics e g shumway and stoffer 2017 by comparison the quantiles of the ww3 model have a clear low bias compared to observations which becomes progressively larger with increasing significant wave heights this indicates that the ww3 results are left skewed compared to the distribution of the observed wave heights i e that the extremes storm peaks are underpredicted by the model cavaleri 2009 has identified the underprediction of storm peaks as the most challenging aspect of physics based wave models due to the difficulty of observing detailed wave dynamics under extreme conditions the theories and empirical expressions incorporated into these models are often based on moderate storm conditions and may not include potential nonlinear effects occurring during extreme events as a result these models can have difficulty in generalizing to such conditions 6 2 peak wave period fig 20 compares the performance of the two ml models and ww3 in terms of scatter plots of peak wave period at stations 45005 and 45142 for the test partition 2016 2017 these results are summarized in table 6 the xgboost model fig 20 top row yields the lowest scatter of the three models considered although values do increase moving from the upwind station 45005 mae 0 309 s mape 8 32 si 0 116 cc 0 866 to the downwind station 45142 mae 0 516 s mape 13 45 si 0 172 cc 0 835 overall bias levels remain equally low between the two stations at 0 033 s 0 87 and 0 061 s 1 47 respectively although they become more prominently negative for values greater than the 95th percentile parentheses at 0 574 s 9 62 and 0 690 s 10 13 respectively as a result the regression slopes are relatively low at 0 730 and 0 707 for stations 45005 and 45142 respectively the lstm model fig 20 center row yields scatter results that are somewhat larger than those of xgboost again increasing from station 45005 mae 0 326 s mape 9 07 si 0 120 cc 0 860 to the downwind station 45142 mae 0 499 s mape 13 61 si 0 166 cc 0 859 however the lstm model captures the peak periods of storm waves better so that the biases over values greater than the 95th percentile are the lowest of the three models with 0 482 s 8 08 and 0 538 s 7 89 at stations 45005 and 45142 respectively this is reflected in the regression slopes of 0 758 and 0 826 respectively which are the best of the three models by comparison the ww3 model fig 20 bottom row yields the poorest results on all metrics considered the scatter results are larger than both the xgboost and lstm both at station 45005 mae 0 408 s mape 12 51 si 0 147 cc 0 865 and 45142 mae 0 653 s mape 19 32 si 0 216 cc 0 812 ww3 displays overall negative biases of 0 339 s 9 02 and 0 474 s 11 51 at 45005 and 45142 respectively with particularly large values of 0 929 s 15 59 and 1 342 s 19 68 respectively for values greater than the 95th percentile this results in low regression slope values at station 45005 0 744 and particularly at station 45142 0 626 this general underprediction of peak wave periods by ww3 is highlighted in the q q plots fig 21 where its quantiles can be seen to be below the line of perfect agreement in both the mid and upper ranges by contrast the xgboost and lstm models display good distributions throughout it can thus be concluded that these two ml models yield a clear improvement in the prediction of peak wave period at the two stations considered 6 3 model ensemble in the model comparisons above we have seen an overall best performance of the xgboost compared to the other two models however we have also seen that the lstm yielded the best performance in terms of values greater that the 95th percentile storm peaks in both significant wave height and peak wave period therefore following pirhooshyaran et al 2020 we investigate the application of an ensemble of these three models in order to assess if their combined performance can improve on those of the individual members fig 22 and the last two columns of tables 5 and 6 show the results of the ensemble mean of the three component models the top row of fig 22 shows that the ensemble mean has a low scatter and low overall bias the scatter values at stations 45005 mae 0 068 m mape 13 33 si 0 151 cc 0 964 and 45142 mae 0 090 m mape 17 27 si 0 195 cc 0 977 are the lowest of the three component models considered see also table 5 by comparison the bias results in particular those of the values greater that the 95th percentile parentheses are an improvement over those of ww3 but they fall short of the improvements seen in the xgboost and lstm members a similar outcome is seen for the peak wave period fig 22 bottom row the scatter results of the ensemble mean improve on those of the individual members both at station 45005 mae 0 287 s mape 7 96 si 0 108 cc 0 904 and station 45142 mae 0 482 s mape 12 98 si 0 157 cc 0 885 however the ensemble mean bias both overall and values greater that the 95th percentile performs better than ww3 but worse than the stronger ensemble members lstm and xgboost the q q plots in fig 23 confirm these bias results of the ensemble mean for the significant wave height top row we see that the distribution has improved relative to that of ww3 compare fig 19 bottom row but still represents an underestimation relative to those of the lstm and xgboost fig 19 center and top rows similarly for the peak wave period fig 23 bottom row the ensemble mean improves on the results of ww3 compare fig 21 bottom row but performs poorer than the lstm and xgboost component models fig 21 center and top rows we can therefore conclude that the ensemble mean generally improves the component model performance in terms of scatter but does not yield the best results in terms of correcting the negative bias during storm peaks 7 model sensitivity in the previous section we showed that advanced ml models can be effective in predicting significant wave height and peak wave period at point locations however it is well known that such data driven models are limited in their generality to the data sets on which they were trained here we investigate the models sensitivity to data set size to investigate the effect of data set size we conducted another training for the xgboost and lstm models in which we limited the training partition size to only 5 years 2011 2015 using the trained models to predict significant wave height for the same test period as before 2016 2017 with this retrained xgboost model the mae for station 45005 is 0 080 m with a maximum absolute error of 0 910 m for station 45142 the mae is 0 101 m with a maximum absolute error of 1 036 m fig 24 shows that the original model trained with 15 years of data performs somewhat better than the one trained with only 5 years of data however the performance of the model fit on the longer training period is not much better which means that it is close to saturation on the training data after about 5 years this is typical for a tree based model it builds up enough tree representations when it sees enough samples most importantly the largest and the smallest for comparison fig 25 shows the results of the lstm model that was trained on the shorter period of 5 years with this retrained model the mae for station 45005 is 0 090 m with a maximum absolute error of 0 887 m for station 45142 the mae is 0 147 m with a maximum absolute error of 1 764 m these mae results are better than those of the lstm trained on the 15 year data set for the upwind station 45005 but worse for the more geographically complex downwind station 45142 we can therefore conclude that unlike the xgboost model the additional data used in the original lstm training did improve the quality of the model thus xgboost would be a better choice for this wave modeling application if only a few years of training data would be available 8 conclusions in this study ml methods based on xgboost and lstm were applied to wind wave prediction in lake erie and were compared to the physics based spectral wave model ww3 this enclosed basin with a wind climate directed along its major axis represents near idealized conditions under which to compare the modeling of wind wave growth by these models buoy data at the upwind ndbc 45005 and downwind ndbc 45142 from 1994 to 2017 were processed for model training validation and testing we used 1994 2013 data for training the model applying observed u 10 wind velocity as model input and observed significant wave height and peak wave period as the target variables data from 2014 2015 was applied for model validation and we subsequently used the trained models to evaluate significant wave height and peak period for the unseen test period of 2016 2017 from the results of this study the following conclusions can be drawn 1 the physics based ww3 model was found to perform well in terms of significant wave height with a low overall bias and a low scatter expressed in a mean absolute percentage error mape of 15 33 at station 45005 and 21 01 at station 45142 however ww3 tends to underestimate wave height during storm events expressed in relative biases of 11 76 and 14 15 at stations 45005 and 45142 respectively computed over wave heights exceeding the 95th percentile the scatter in peak wave period was acceptable with a mape of 12 51 and 19 32 at stations 45005 and 45142 respectively however the overall relative bias at these stations was quite large namely 9 02 and 11 51 respectively increasing to 15 59 and 19 68 during storm peaks 2 the trained xgboost model yields significant wave height predictions with comparable scatter to ww3 and extreme values are less underestimated with a relative bias of 2 56 at station 45005 and 10 61 at 45142 for values above the 95th percentile furthermore the scatter and bias in peak wave period are lower than for ww3 with overall relative bias values of 0 87 and 1 47 at 45005 and 45142 respectively and values of 9 62 and 10 13 during storm events 3 the trained lstm model yields significant wave height predictions with larger scatter and overall bias than ww3 and xgboost however it yields improved relative bias during storm conditions compared to ww3 with values of 3 34 and 8 93 at stations 45005 and 45142 respectively for values exceeding the 95th percentile for peak wave period it yields the lowest relative bias of the three models during storm conditions namely 8 08 and 7 89 for stations 45005 and 45142 respectively we conclude that for such extreme events fetch and duration effects are import to incorporate done here via the lstm s long and short term memories 4 ensembling the three studied models was found to be a good strategy since it combines some of their strengths the overall scatter of the ensemble mean was lower than any of the component models with mape values of 13 33 and 17 27 in significant wave height and mape values of 7 96 and 12 98 in peak wave period at stations 45005 and 45142 respectively however the bias of the ensemble mean fell short of the lowest values amongst the component models for significant wave height during storm events it yielded relative biases of 5 89 and 11 23 at stations 45005 and 45142 respectively and for peak wave period during storm events it yielded relative biases 11 10 and 12 57 respectively which are greater than found for either xgboost or lstm 5 a benefit of the tree based xgboost model is that it can also perform well on a smaller set of training data training our model on only 5 years of data yields test partition results that are comparable to those found after training the model with the full 15 years of training data this suggests that the xgboost model fitting has become saturated after processing about 5 years of training examples 6 the ml models evaluated here represent a significant reduction in computational times compared to traditional physics based spectral wave models to complete the 2016 2017 prediction on the same computing environment ww3 needed 24 h with 60 cpus whereas the trained xgboost needed only 0 03 s for predicting with 1 cpu and the trained lstm required 0 24 s on 1 cpu it should be mentioned however that the xgboost and lstm models built here output only significant wave height and peak period and only at the point locations for which they were trained by contrast ww3 is capable of outputting complete fields of significant wave height wave period wave direction and the full directional wave spectrum nonetheless james et al 2018 have shown that ml approaches as studied here can readily be extended to outputting fields of wave variables if sufficient area covering training data would be available these authors show how significant wave height results from physics based models such as swan can be used for this purpose however biases from these physics based models such as those shown in figs 18 21 would need to be removed before using their output as training data for the ml models in this regard the reader is reminded that the comparisons presented here were made with respect to the operational ww3 model for the great lakes as benchmark it is possible that the performance of the physics based ww3 could be improved for lake erie with a local tuning using the data utilized for training our ml models we note furthermore that even though this study was conducted in an enclosed basin to isolate the process of wind wave growth the xgboost and lstm models evaluated here are equally applicable to open coast and offshore conditions for such more generalized conditions currents and offshore wave boundary conditions would have to be included as predictors for shallow water coastal applications water level variation from tides and surge should also be included as predictors credit authorship contribution statement haoguo hu conception and design of study acquisition of data analysis and or interpretation of data writing original draft andré j van der westhuysen conception and design of study analysis and or interpretation of data writing original draft philip chu conception and design of study writing review editing ayumi fujisaki manome conception and design of study acquisition of data analysis and or interpretation of data writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was funded by the noaa great lakes environmental research laboratory awarded to the cooperative institute for great lakes research ciglr through the noaa cooperative agreement with the university of michigan na17oar4320152 this is glerl contribution 1981 and ciglr contribution 1182 the authors would also like to thank the reviewers for their constructive comments which improved the quality of this manuscript 
23898,waves in large lakes put coastal communities and vessels under threat and accurate wave predictions are needed for early warnings while physics based numerical wave models such as wavewatch iii ww3 are useful to provide spatial information to supplement in situ observations they require intensive computational resources an attractive alternative is machine learning ml methods which can potentially improve the performance of numerical wave models while only requiring a small fraction of the computational cost in this study we applied novel ml methods based on xgboost and a long short term memory lstm recurrent neural network for predicting wave height and period under the near idealized wave growth conditions of lake erie data sets of significant wave height h peak wave period t p and surface wind from two offshore buoys from 1994 to 2017 were processed for model training and testing we trained and validated the ml models with the data sets from 1994 to 2015 and then used the trained models to predict significant wave height and peak period for 2016 and 2017 the xgboost model yielded the best overall performance with mean absolute percentage error mape values of 15 6 22 9 in h and 8 3 13 4 in t p the lstm model yielded mape values of 23 4 30 8 in h and 9 1 13 6 in t p an unstructured grid ww3 applied to lake erie yielded mape values of 15 3 21 0 in h and 12 5 19 3 in t p however ww3 underestimated h and t p during strong wind events with relative biases of 11 76 to 14 15 in h and 15 59 to 19 68 in t p xgboost and lstm improve on these predictions with relative biases of 2 56 to 10 61 in h and 8 08 to 10 13 in t p an ensemble mean of these three models yielded lower scatter scores than the members with mape values of 13 3 17 3 in h and 8 0 13 0 in t p although it did not improve the bias the ml models ran significantly faster than ww3 for this 2 year run on the same computing environment ww3 needed 24 h with 60 cpus whereas the trained lstm needed 0 24 s on 1 cpu and the trained xgboost needed only 0 03 s on 1 cpu 1 introduction accurate predictions of wave conditions are important for the offshore industry shipping and for mitigating coastal hazards such as dune erosion various wave models have been developed and implemented in ocean and coastal regions following mahjoobi and etemad shahidi 2008 and pirhooshyaran et al 2020 these can be divided into three broad categories namely i physics based models which feature parameterizations of the wave action balance equation ii statistical and machine learning models hereafter ml models also known as soft computing methods which instead use the data structure for prediction without explicit description of physics and iii hybrid approaches which combine the first two categories physics based models have been developed during the course of three generations starting with simple parameterizations of the relationships between wind wave height and wave period to the current third generation in which four wave nonlinear interaction and various other source terms are explicitly modeled in frequency directional space e g wam wamdi group 1988 swan booij et al 1999 and wavewatch iii hereafter ww3 tolman et al 2002 for lake erie one of the five great lakes first and second generation physics based wave models used simple parameterization to account for the evolution of wave height and period due to wind forcing schwab et al 1984 third generation wave models such as swan and mike21 sw sørensen et al 2004 which explicitly represent three and four wave interactions have subsequently been applied to lake erie moeini and etemad shahidi 2007 a coupled wave hydrodynamic model based on the unstructured grid finite volume community ocean model fvcom chen et al 2013 and a third generation wave model based on swan qi et al 2009 was applied to lake erie to investigate wave climatology and inter basin wave interactions niu and xia 2016 alves et al 2014 implemented the third generation ww3 model as the dynamical core of the national oceanic and atmospheric administration s noaa great lakes operational wave forecasting system while physics based wave models generally provide satisfactory results in comparison with observations the physics they embody are known to underperform under highly nonlinear extreme events such as storm peaks e g alves et al 2014 furthermore a practical problem especially in an operational environment is their high computational expense statistical and ml models soft computing methods were first applied to significant wave height prediction by deo and sridhar naidu 1999 deo et al 2001 and agrawal and deo 2002 these researchers applied small fully connected feed forward artificial neural networks anns as well as autoregressive moving average arma or autoregressive integrated moving average arima stochastic time series models to predict waves at point locations using as input previous wave conditions univariate modeling or wind observations multivariate modeling mandal and prabaharan 2006 introduced the use of recurrent neural networks rnns to wave forecasting they applied an rnn to predict significant wave heights based on a time series history of observed significant wave heights more recently srinivasan et al 2017 compared feed forward networks and rnns in the context of multivariate wave modeling using observed wind as input and found the rnns to have better skill all these studies applied neural networks with only one hidden layer and limited data sets of only 1 2 years mahjoobi and etemad shahidi 2008 and etemad shahidi and mahjoobi 2009 introduced the use of single classification and regression trees using c5 0 cart and m5 training algorithms as an alternative to neural network based approaches the former study considered 5 years of wind and wave data on lake michigan from the national data buoy center ndbc station 45007 the authors found better performance for regression trees than classification trees and that anns performed slightly better than either tree model the authors note that as with anns tree models do not require knowledge of the underlying physical process to make predictions however they provide more insight into the problem than anns since they represent understandable rules mahjoobi et al 2008 and malekmohamadi et al 2011 compared a range of other ml approaches including support vector machines bayesian networks fuzzy inference systems and adaptive neuro fuzzy inference systems to anns in lake ontario and lake superior they found comparable accuracy between these alternative methods and anns except for bayesian networks which performed poorly however it should be noted that the wind and wave data used in their study covered less than two years and the ann used had only one hidden layer with three neurons recent years have seen breakthroughs in the development of ml training algorithms hardware and a massive increase in data peres et al 2015 used observed winds u v at or near three coastal wave buoy stations as input and observed significant wave heights h as output to train a single hidden layer fully connected feed forward neural network for each station fig 1a the data set covered a 20 year period from 1989 to 2008 the model input included observed wind from locations upwind of the output station as well as at time lags to account for the fetch and duration in wind wave generation james et al 2018 trained a fully connected feed forward neural network to produce a field of coastal wave heights h 1 h n using as input three offshore wave characteristics height h period t and direction d a 12 2 field of wind vectors u v a 357 2 field of ocean currents u v and wave height fields from swan simulations as target variables fig 1b recently feng et al 2020 used fully connected a feed forward neural network to model waves in lake michigan a number of more complex neural network applications have furthermore been proposed mahmoodi et al 2017 compared feed forward and cascade forward networks but found better performance in the former kumar et al 2018 investigated the use of ensembles of extreme learning machines to reduce the variation caused by random initialization prahlada and deka 2015 and dixit and londhe 2016 combined anns with wavelet analysis to better model extreme wave events pirhooshyaran and snyder 2020 introduced the use of sequence to sequence networks for the prediction wave height and output power feature selection and the reconstruction of missing wave observations using data from nearby stations pirhooshyaran et al 2020 expanded this work by including an ensemble of sequence to sequence networks introducing a new resampling technique and improving feature selection they demonstrate the superiority of this approach as the forecast lead time increases hybrid approaches combine the strengths of physics based models and ml this includes the use of ml models to perform data assimilation on the output of physics based models e g makarynskyy 2006 londhe et al 2016 and the creation of nonlinear ensemble means e g campos et al 2020 in the present study we investigate two different ml approaches for learning the nonlinear relationships between observed wind data significant wave height and peak wave period with the aim to correct the underprediction of the latter two quantities by physics based spectral wave models in particular at storm peaks the first is an ensemble tree based model xgboost chen and guestrin 2016 which is based on the random forest model breiman 2001 xgboost is an optimized distributed gradient boosting library designed to be highly efficient flexible and portable it provides a parallel tree boosting that solves many data science problems such as regression e g qian et al 2020 and classification e g merembayeva et al 2019 fast and accurately xgboost has built in l1 and l2 regularization which prevents the model from overfitting it is furthermore robust having a capability to handle missing values the second model is from the rnn family used here to capture the dependency of significant wave height and peak wave period on the time series structure of recently observed winds specifically we applied the long short term memory lstm hochreiter and schmidhuber 1997 implemented using the keras high level framework with a tensorflow backend chollet 2017 to compare with physics based models we include simulations with the unstructured mode of ww3 roland and ardhuin 2014 the field case considered here is lake erie which is an enclosed basin with a wind climate directed along its major axis hourly metocean data are available at ndbc stations 45005 and 45142 at opposite ends of the fetch for the period 1994 2017 it therefore represents near idealized conditions under which to compare the modeling of wind wave growth by ml and physics based models the xgboost fig 1c and lstm fig 1d models were trained using observed wind u v significant wave height h and peak wave period t p at these two data buoys fig 2 and subsequently evaluated against holdout data and the physics based model ww3 this paper is structured as follows section 2 describes the field case and data set used to train and evaluate the two ml models sections 3 and 4 provide details of respectively the xgboost and lstm models section 5 describes the physics based ww3 model and the configuration applied here in section 6 the performance of the two ml models and ww3 as well as their ensemble mean are intercompared and in section 7 model sensitivity is investigated section 8 closes with conclusions drawn from these results 2 field case data set the field case considered here for the training and evaluation of our ml models is lake erie one of the great lakes that is shared between the united states and canada fig 2 lake erie is narrow with a length of 338 km and a width of 92 km and has a wsw ene orientation it is relatively shallow with a maximum depth of 64 m but with mean depths of only 7 3 m in the western basin 18 3 m in the central basin and 24 4 m in the eastern basin lake erie has several offshore buoys as part of its observational network of which two have collected continuous hourly data from 1994 to 2017 during the ice free spring summer and fall seasons located on the western end of the lake near toledo oh is station 45005 9 8 m depth maintained by noaa s national data buoy center ndbc located on the eastern end of the lake near buffalo ny is station 45142 27 m depth maintained by the environment and climate change canada the historical and real time measurements from these two offshore buoys are available from ndbc 2020 this data was cleaned by i dropping records where the wind speed significant wave height and peak wave period were not available simultaneously as required for training and evaluation ii dropping records where the significant wave height is very low 0 05 m indicating low energy noise and iii dropping records where the peak wave period had a quality flag of 1 indicating doubtful or erroneous values or an unphysical value for this small lake 14 s in addition based on visual inspection some years were excluded due to the doubtful quality of the peak period observations at station 45142 namely 1996 2003 2006 and 2010 figs 3 to 5 show the distributions of the cleaned monthly significant wave height peak wave period and wind speed during the 24 year observational period considered fig 6 shows the observed wind climate in terms of wind roses at ndbc buoys 45005 and 45142 from fig 3 we can see that the distribution of significant wave heights is strongly skewed to the right higher values with the third quartile below 1 5 m but with outliers representing storm peaks of up to 5 2 m there is clear seasonality present with the largest significant wave heights recorded in spring april may and fall october november similarly fig 4 shows peak wave periods to be skewed to the right and display seasonality with the highest values recorded in the spring and fall fig 5 shows that this is correlated to similar seasonal wind speed magnitude patterns which is to be expected considering that all wind wave conditions in this shallow lake are locally generated figs 3 and 4 furthermore show that extreme significant wave heights and peak periods tend to be higher at station 45142 than station 45005 considering that the dominant wind direction is from the wsw ssw fig 6 these larger wave heights are due to the longer fetch available to the downwind station 45142 note that the extreme depth limitation of 7 3 m at station 45005 also imposes a depth limitation to the maximum possible wave height which is relaxed at the deeper buoy 45142 the input variables that were selected to construct our ml models are firstly the observed wind speed and direction at stations 45005 and 45142 these are transformed into the wind vector components u and v at each station second following peres et al 2015 we add lags of these wind vectors to capture storm duration considering the 338 km length of the lake and a maximum observed wind speed of 20 6 m s during the study period fully developed waves would be generated after a duration of about 24 h following bretschneider 1958 therefore lags at 3 hourly intervals of up to 24 h were included the exact number of lags included was determined as part of hyperparameter tuning see below this yields a maximum of 18 input variables per observation station or 18 2 in total the observed significant wave height and peak wave period at stations 45005 and 45142 are used as the output target variables note that in general wave conditions are also affected by lake ice coverage and thickness and hence lake surface temperature which could serve as additional input variables however here we only consider wave growth during ice free months april to early december so that these variables are not included in order to train and evaluate our ml models on this data we split the data set into training validation and test partitions considering that these are time series data in which serial correlation exists between successive observations the usual approach of random assignment to the three partitions is not followed instead continuous periods of the time series are assigned to each partition e g peres et al 2015 the bulk of the data set namely the years 1994 2012 is assigned to the training partition in order to achieve reliable estimates of the trainable model parameters next the years 2014 2015 are assigned to the validation partition which is used in the selection of model hyperparameters such as tree depth and to check for overfitting on the training data finally the years 2016 2017 are assigned to the test partition this latter portion of the data set will be used to test how well the ml models generalize to unseen conditions at the two locations for which they were trained the test partition will also be used to assess the performance of the physics based model ww3 at these two stations the resulting split between the three partitions is 28154 records for training 5193 for validation and 4493 for testing or about 74 14 12 fig 7 compares the distribution of observed significant wave height and peak wave period in the train validation and test partitions at stations 45005 and 45142 we can see that the median and interquartile distributions are similar between the training validation and test partitions however we note that in terms of peak events outliers the test partition is less energetic than the train and validation partitions 3 xgboost model 3 1 model description xgboost chen and guestrin 2016 is mostly used for supervised learning problems where one uses training data to predict a target variable xgboost implements ml algorithms under the gradient boosting framework it uses the same tree ensemble model as the random forest model breiman 2001 consisting of a set of classification or regression trees random forest and boosted trees are basically the same models the only difference is how one trains them mathematically an ensemble tree model can be written in the form e g chen and guestrin 2016 1 y ˆ i k 1 k f k x i f k f where k is the number of trees and f x is a tree function in the function space f f x w q x the data set d x i y i has n examples and m features x i r m y i r the structure of each tree is given by q which maps an example x i to the corresponding leaf index each tree function f k corresponds to a structure q and leaf weights w for a total of t leaves the objective loss function with parameters θ to be optimized is given by 2 l θ i l y i y ˆ i k ω f k where l is a differentiable convex loss function that measures the difference between the prediction y ˆ i and the target y i ω f is the regularization term which penalizes the complexity of the model gradient tree boosting is achieved by training the model in an additive manner tree functions f t are greedily added to successive iterations to minimize the loss function 2 let y ˆ i t 1 and y ˆ i t be the predictions at iterations t 1 and t respectively of the i th instance setting y ˆ i t y ˆ i t 1 f t x i the loss function at the t th iteration will be 3 l t i 1 n l y i y ˆ i t 1 f t x i ω f t recalling the taylor expansion f x δ x f x f x δ x 1 2 f x δ x 2 we obtain 4 l y i y ˆ i t 1 f t x i l y i y ˆ i t 1 g i f t x i 1 2 h i f t 2 x i then we have 5 l t i 1 n l y i y ˆ i t 1 g i f t x i 1 2 h i f t 2 x i ω f t constant where g i y ˆ i t 1 l y i y ˆ i t 1 and h i y ˆ i t 1 2 l y i y ˆ i t 1 define the penalty on the complexity of the model with the regularization term ω f γ t 1 2 λ j 1 t w j 2 with t the number of leaves then with constant terms removed the simplified objective loss function at iteration t is 6 l t i 1 n g i f t x i 1 2 h i f t 2 x i γ t 1 2 λ j 1 t w j 2 define the instance set of leaf j as i j i q x i j then 7 l t j 1 t i i j g i w j 1 2 i i j h i λ w j 2 γ t define g j i i j g i and h j i i j h i then 8 l t j 1 t g j w j 1 2 h j λ w j 2 γ t and then 9 argmin x l 1 2 j 1 t g j 2 h j λ γ t where the optimal weight w j of leaf j is given by w j g j 2 h j λ in summary the boosted tree algorithm is the following 1 add a new tree in each iteration 2 at the beginning of each iteration calculate g i y ˆ i t 1 l y i y ˆ i t 1 and h i y ˆ i t 1 2 l y i y ˆ i t 1 3 use the statistics to greedily grow a tree f t x with l 1 2 j 1 t g j 2 h j λ γ t 4 add f x to the model y ˆ i t y ˆ i t 1 ε f t x i where ε is called shrinkage 3 2 configuration and hyperparameter tuning the xgboost model used hourly wind significant wave height and peak wave period from observations at the buoy stations in western 45005 and eastern 45142 lake erie fig 2 the observed wind speed and direction at the two stations were used as input x i and significant wave height and peak wave period were used as model output y i for this regression problem we set the learning objective as regression with squared loss three hyperparameters were optimized in the xgboost model namely t d maximum tree depth w minimum child weight and e learning rate eta considering the limited number of hyperparameters a direct search strategy was followed for the optimization the search space was defined as t d 5 6 7 8 9 10 11 12 w 3 4 5 6 7 8 and e 0 005 0 01 0 05 0 1 0 2 one parameter was tuned at a time with other parameters fixed until all parameters were tuned the process was repeated three times mean absolute percentage error mape was set as the metric for training and the mean absolute error mae as the evaluation metric for cross validation table 1 shows that model accuracy improved with increasing maximum tree depth t d up to 10 but was quite insensitive to the minimum child weight w the combination of t d 10 and w 5 yielded a minimum mae 0 07466 m table 2 shows that the optimal learning rate e 0 05 yielded a minimum mae 0 0747 m this model training ran on a cpu intel xeon cpu e2176m 2 70 ghz for about 3 min per hyperparameter combination and it converged at around 4000 epochs 3 3 xgboost results the trained xgboost model was used to predict wind wave conditions for the unseen test period of 2016 2017 it needed only 0 03 s for this prediction using 1 cpu figs 8 and 9 show the time series and error statistics the prediction for station 45005 yielded a mae of 0 074 m in h and 0 309 s in t p with maximum absolute errors of 0 853 m and 2 613 s respectively this corresponds to mape values of 15 611 in h and 8 322 in t p the downwind station 45142 yielded larger errors namely a mae of 0 100 m in h and 0 516 s in t p with maximum absolute errors of 1 045 m and 3 774 s respectively this corresponds to larger mape values of 22 890 in h and 13 445 in t p scatterplot comparisons with other the models will be discussed in section 6 4 long short term memory lstm model 4 1 model description our second model is a specialized class of artificial neural network namely a recurrent neural network rnn this class of network differs from the standard feed forward type in that the time dependency of the input variables is explicitly modeled rnn models are composed of a sequence of identical neural network cells each one representing one time step each of these units passes its nonlinear activation to the unit of the next time step thus allowing a system memory to build up this system memory is useful for modeling sequences such as time series of physical phenomena and natural language as motivated by the early work of mandal and prabaharan 2006 rnns are therefore well suited to the problem of wave height prediction a phenomenon that is known to depend on a time series history of winds here we apply a popular variety of the rnn namely the long short term memory lstm network by hochreiter and schmidhuber 1997 the structure of one cell of the lstm is expressed in the following set of equations see olah 2015 and chollet 2017 for graphical representations 10 f t σ w f a t 1 x t b f 11 i t σ w i a t 1 x t b i 12 o t σ w o a t 1 x t b o 13 c t tanh w c a t 1 x t b c 14 c t i t c t f t c t 1 15 a t o t tanh c t the input to the lstm consists of a time series vector x x 1 x 2 x t for t 1 t time steps in the series the vector a a 1 a 2 a t represents the network activation that is passed from one time step to the next thus representing the short term memory of the system a second longer term memory is represented by the vector c c 1 c 2 c t which is also passed from one time step to the next the distinction is that c or the carry state is not necessarily updated at every time step unlike the activation a as such c can carry information across several time steps influencing model outputs based on inputs much earlier in the time series the updating of c and its influence on the activation a is determined by a set of binary gates called the forget gate f t given by 10 the update gate i t 11 and the output gate o t 12 in each of these expressions σ is the sigmoid function w is the weight matrix and b is the bias vector at each time step a candidate c t for updating the previous carry state c t 1 is computed using 13 in 14 based on the outcomes of the update and forget gates c t is then updated to the candidate c t or just takes the same value as the previous carry state c t 1 thus continuing the long term memory finally the new activation a t is computed based on the value of the output gate and the current carry state c t using 15 the activation a t is then used to determine the output y t of the given time step via e g a rectified linear unit relu or softmax activation function however it is also passed forward to the next time step along with the carry state this lstm cell can thus be repeated for each time step in the sequence to be modeled as will be shown in the next section to build our lstm application in the keras framework the lstm cell described in the previous section was arranged in a time sequence covering the 3 hourly time lags of wind observation going back to a maximum of 24 h namely t 24 21 18 15 12 9 6 3 0 this results in a horizontal sequence of up to 9 lstm units for the wind input vectors x t to x t each unit passes along its activation a t and carry status c t to the unit at the next time step fig 10 the maximum number of lags t which represents the duration of wave generation will be determined via hyperparameter optimization below each wind input vector x t u 1 v 1 u 2 v 2 contains the wind velocity components at the two stations 45005 and 45142 at each time lag these input sequences were generated by moving a time window through the data set each time capturing a sequence of t wind input vectors this results in a training data set with an input size of m x t w 28154 t 4 where m x is the number of training sequences of length t and w 4 the number of wind vector components for the validation and test partitions the corresponding input sizes are 5193 t 4 and 4493 t 4 respectively at each time step t a series of four lstm cells are stacked each passing its activation to the next to increase complexity of the nonlinear relationships that can be modeled each lstm unit contains n neurons which is the second hyperparameter to optimize as described above inputs are received at each time step t 1 t the output however is only generated at the last time step t corresponding to the current time at which an estimate of the significant wave height is required at this time step the activation of the fourth lstm layer is passed to a four neuron dense layer with rectified linear unit relu activations this produces non negative regression outputs of the significant wave heights and peak periods y t h 1 h 2 t p 1 t p 2 at stations 45005 and 45142 4 2 hyperparameter tuning as described above the lstm structure features a sequence of n neurons per layer and t wind observations fig 10 this neural network was trained using a mini batch size of b and rmsprop optimization with a learning rate of l we therefore have four hyperparameters for this model namely n t b and l which were determined using derivative free optimization similar to the approach for the xgboost model a direct search strategy was followed the search space was defined as n 12 14 16 18 20 22 24 t 3 6 9 b 16 32 64 128 and l 0 001 0 01 0 1 for each hyperparameter combination the lstm model was trained for a maximum of 100 epochs and the epoch at which the minimum validation partition mape was found was recorded this minimum represents a balance between bias reduction and variance overfitting for that hyperparameter combination moving through the search space the combination for which the minimum validation set mape was found is identified as the optimum set of hyperparameters tables 3 and 4 show the results of this optimization for different dimensions of the search space table 3 shows a preference in the optimization towards using an input wind history of 6 time steps i e t 15 12 9 6 3 0 in particular when using slower learning rates furthermore the slowest learning rate of l 0 001 consistently yields the best results table 4 shows that the optimization is not sensitive to mini batch size this is because for a larger mini batch size the optimization responded by using more epochs to reach a minimum mape and vice versa an optimal combination of n 16 t 6 b 32 and l 0 001 was found which is identified in bold in both tables this model training ran on a cpu intel xeon cpu e2176m 2 70 ghz for about 3 5 min per hyperparameter combination fig 11 shows the training of this model in terms of aggregate mape over both stations 45005 and 45142 on the training and validation partitions up to the optimal epoch it can be seen that the error of both the training and validation partitions reduce rapidly during the first few epochs over the next decades of epochs the training partition score reduces steadily the score of the validation set also reduces on average although not as smoothly considering the smaller sample size for the computation of these metrics the mape of the validation set reaches a 3 point moving average minimum at 63 epochs suggesting that the model is not overfit after 63 epochs the model achieves an aggregate mape 15 995 over both stations for the training partition years 1994 2013 an mape 14 770 for the validation partition years 2014 2015 and an mape 15 328 for the unseen test partition years 2016 2017 we can therefore conclude that the lstm model is indeed not overfit and seems to generalize well yielding somewhat better results for the unseen test partition than the training and validation partition 4 3 lstm results figs 12 and 13 show the predicted lstm significant wave height and peak period and their error statistics for the unseen test partition years 2016 2017 the lstm model provides a good representation of the observed h and t p in both the low and high energy regimes importantly the lstm model captures all major wave events including the storm peaks at the upwind station 45005 the lstm model yields an mae of 0 103 m in h and 0 326 s in t p with maximum absolute errors of 0 885 m and 2 487 s respectively this corresponds to mape values of 23 421 in h and 9 072 s in t p as was seen for the xgboost model above errors tend to be larger at the downwind station 45142 an mae of 0 125 m in h and 0 499 s in t p with maximum absolute errors of 1 906 m and 3 831 s respectively this corresponds to mape values of 30 775 in h and 13 614 in t p the trained lstm needed only 0 24 s on 1 cpu to perform this 2 year prediction 5 wavewatch iii spectral model 5 1 model description the unstructured mesh used for the ww3 runs is shown in fig 14 which was generated using aquaveo s surface water modeling system sms the grid size distribution was configured as a function of the 3 arc second bathymetry data from the national centers for environmental information ncei the model bathymetry was obtained by interpolating the observed bathymetry onto each unstructured grid node using the inverse distance method high resolution noaa coastline data was applied to delineate the land boundary the resulting mesh is composed of 11509 triangular elements and 6106 nodes in the horizontal the resolution varies from approximately 100 m near the shore to 2 5 km offshore the model has distribution referenced to the great lakes low water datum of 173 5 m surface wind observations were interpolated to create hourly gridded surface meteorological analyses of wind the observations are from the great lakes weather data and marine observations glerl 2020 which includes coastal and offshore meteorological stations these data were corrected for over water conditions and interpolated along with available in lake buoys to the model grid fig 15 shows scatter and quantile quantile q q plots comparing the observed winds with the analysis winds used in ww3 from the q q plots we can see that the analysis wind fields match the observation at 45005 well although they slightly underestimate values greater than 10 m s conversely they slightly overestimate the observations at 45142 during strong events of around 15 m s we can therefore conclude that the wind fields applied to ww3 have a generally high accuracy and only a limited bias the ww3 simulation starts at 12 00 gmt on 01 january 2016 model results are output on an hourly interval at the same time step of the buoy data the model took 24 h on 60 cpus intel xeon cpu e5 2603 v4 1 70 ghz to complete the prediction for 2016 2017 the model settings were chosen to match those of the current noaa operational model for the great lakes alves et al 2014 which are wind input and dissipation source terms based on ardhuin et al 2010 flux computation included in the model source terms the third order propagation scheme with tolman 2002 averaging technique generalized multiple dia gmd nonlinear interaction tolman 2013 the empirical linear jonswap parameterization of bottom friction hasselmann et al 1973 depth induced breaking according to battjes and janssen 1978 linear time interpolation for wind interpolation and approximately linear speed interpolation for wind space interpolation no triad interactions used no bottom scattering used no supplemental source term used the model was compiled with netcdf version 4 as a stand alone program the model used message passing interface mpi with a distributed memory for more details refer to alves et al 2014 we used this model configuration as a benchmark and no additional tuning was done 5 2 ww3 results the ww3 model was implemented to predict significant wave height and peak wave period for 2016 2017 fig 16 shows the time series of modeled and observed significant wave height for station 45005 a mae of 0 082 m and a maximum absolute error of 0 873 m were found for station 45142 the model yielded an mae of 0 119 m and a maximum absolute error of 1 100 m this corresponds to mape values of 15 325 and 21 008 for the two stations respectively fig 17 shows the corresponding results for peak wave period at station 45005 a mae of 0 408 s and a maximum absolute error of 2 604 s were found and at station 45142 a mae of 0 653 s and a maximum absolute error of 5 864 s were found the mape values are 12 506 and 19 318 for the two stations respectively these figures show that ww3 tends to underestimate wave height and period peaks during strong wind events spikes on the figures we note that the slightly underestimated model winds at station 45005 during strong wind events fig 15 left may be the cause of the underestimation of these peaks however this cannot explain the behavior at the downwind station 45142 where the model winds slightly overestimate observations during strong wind events fig 15 right while the wave height and period peaks are still underestimated we tested an alternative wind data set from an operational weather forecast model by the high resolution rapid refresh hrrr benjamin et al 2016a b in ww3 simulations and the issue with underestimating the peak significant wave height still persisted and the maximum absolute errors were slightly worse than the results with the interpolated winds from the observations 6 model intercomparison 6 1 significant wave height fig 18 compares the performance of the two ml models and ww3 in terms of scatter plots of significant wave height at stations 45005 and 45142 for the test partition 2016 2017 these results are summarized in table 5 in addition to the mape and mae discussed above we also consider the scatter index si correlation coefficient cc bias relative bias and regression fit the xgboost model fig 18 top row displays a tight grouping around the line of perfect agreement in particular in the lower range of wave heights where the data density is the highest warmer colors we see a low scatter mae 0 074 m mape 15 61 si 0 160 cc 0 958 and small overall bias 0 011 m or 1 81 against the observations at the upwind station 45005 to assess the performance during storm conditions the bias is also computed over observations greater than the 95th percentile diamond markers score included in parentheses which also displays a small bias 0 042 m or 2 56 the regression fit through the results indicate a slope close to unity 0 974 and an intercept of close to zero 0 005 m at the downwind station 45142 the scatter increases mae 0 100 m mape 21 89 si 0 220 cc 0 970 and the overall bias increases to 0 063 m 9 28 we see a comparable bias of 0 239 m 10 61 over the data points greater than the 95th percentile this underprediction of h is reflected in the lower regression slope of 0 895 we note that the larger errors at the downwind station 45142 compared to the upwind 45005 is presumably due to the added complexity of its longer fetch over this elongated lake which is a nonlinear function of the wind direction refer figs 2 and 7 the lstm model fig 18 center row shows similar but weaker performance than the xgboost model in terms of significant wave height at the upwind station 45005 the scatter is larger mae 0 103 m mape 23 42 si 0 221 cc 0 931 and the overall bias 0 037 m or 5 91 and the bias of values greater than the 95th percentile 0 054 m or 3 34 are somewhat higher the regression slope is close to unity 0 989 and the intercept close to zero 0 030 m at the downwind station 45142 the scatter increases relative to xgboost and ww3 mae 0 125 m mape 30 78 si 0 269 cc 0 948 and the overall bias increases to 0 059 m 8 71 however for values greater than the 95th percentile the negative bias is the lowest of the three models at 0 201 m 8 93 this underestimation is reflected in the regression slope of 0 909 which is nonetheless the closest to unity of the three models the performance of ww3 fig 18 bottom row in terms of scatter is slightly poorer than xgboost and better than lstm both at station 45005 mae 0 082 m mape 15 33 si 0 180 cc 0 950 and 45142 mae 0 119 m mape 21 01 si 0 241 cc 0 963 in terms of overall bias ww3 performs similar or better than the two ml models with values of 0 024 m 3 91 and 0 002 m 0 33 at stations 45005 and 45142 respectively however these overall bias scores obscure a pattern of underestimation in the upper range of wave heights the bias values over values greater than the 95th percentile are significantly negative at 0 191 m 11 76 and 0 318 m 14 15 at stations 45005 and 45142 respectively this underestimation is reflected in the poorer regression slopes of 0 834 and 0 769 for these stations respectively conversely an overestimation in the lower ranges can be seen from the regression intercepts which at 0 079 m and 0 135 m indicates a marked deviation from zero fig 19 presents the corresponding q q plots in order to compare the distributions of the simulated significant wave heights to those of the observations in particular the upper quantiles right hand tail of each distribution represent the behavior of the modeled storm peaks the distributions of the xgboost and lstm model at stations 45005 and 45142 follow those of the observations closely however some negative bias can be seen towards higher values at 45142 reflecting a tendency to underestimate storm peaks this result might be surprising considering that ml models such as neural networks are typically unbiased when properly trained the finding that the lower quantiles agree better with observations in these ml models can be explained by the fact that the majority of the available training data cover this lower significant wave height range 75th percentile 0 8 m see fig 7 this concentration of data within the lower wave height range can also be seen from the color mapping in fig 18 improved model fitting at these upper quantiles can be obtained by including more storm peak observations in the training set or alternatively by using weighted samples with greater weights at these higher values not shown furthermore mahjoobi and etemad shahidi 2008 prahlada and deka 2015 and dixit and londhe 2016 suggest dividing the data into multiple parts and training a separate model on the higher wave height partition this is conceptually similar to threshold autoregressive models tar in time series statistics e g shumway and stoffer 2017 by comparison the quantiles of the ww3 model have a clear low bias compared to observations which becomes progressively larger with increasing significant wave heights this indicates that the ww3 results are left skewed compared to the distribution of the observed wave heights i e that the extremes storm peaks are underpredicted by the model cavaleri 2009 has identified the underprediction of storm peaks as the most challenging aspect of physics based wave models due to the difficulty of observing detailed wave dynamics under extreme conditions the theories and empirical expressions incorporated into these models are often based on moderate storm conditions and may not include potential nonlinear effects occurring during extreme events as a result these models can have difficulty in generalizing to such conditions 6 2 peak wave period fig 20 compares the performance of the two ml models and ww3 in terms of scatter plots of peak wave period at stations 45005 and 45142 for the test partition 2016 2017 these results are summarized in table 6 the xgboost model fig 20 top row yields the lowest scatter of the three models considered although values do increase moving from the upwind station 45005 mae 0 309 s mape 8 32 si 0 116 cc 0 866 to the downwind station 45142 mae 0 516 s mape 13 45 si 0 172 cc 0 835 overall bias levels remain equally low between the two stations at 0 033 s 0 87 and 0 061 s 1 47 respectively although they become more prominently negative for values greater than the 95th percentile parentheses at 0 574 s 9 62 and 0 690 s 10 13 respectively as a result the regression slopes are relatively low at 0 730 and 0 707 for stations 45005 and 45142 respectively the lstm model fig 20 center row yields scatter results that are somewhat larger than those of xgboost again increasing from station 45005 mae 0 326 s mape 9 07 si 0 120 cc 0 860 to the downwind station 45142 mae 0 499 s mape 13 61 si 0 166 cc 0 859 however the lstm model captures the peak periods of storm waves better so that the biases over values greater than the 95th percentile are the lowest of the three models with 0 482 s 8 08 and 0 538 s 7 89 at stations 45005 and 45142 respectively this is reflected in the regression slopes of 0 758 and 0 826 respectively which are the best of the three models by comparison the ww3 model fig 20 bottom row yields the poorest results on all metrics considered the scatter results are larger than both the xgboost and lstm both at station 45005 mae 0 408 s mape 12 51 si 0 147 cc 0 865 and 45142 mae 0 653 s mape 19 32 si 0 216 cc 0 812 ww3 displays overall negative biases of 0 339 s 9 02 and 0 474 s 11 51 at 45005 and 45142 respectively with particularly large values of 0 929 s 15 59 and 1 342 s 19 68 respectively for values greater than the 95th percentile this results in low regression slope values at station 45005 0 744 and particularly at station 45142 0 626 this general underprediction of peak wave periods by ww3 is highlighted in the q q plots fig 21 where its quantiles can be seen to be below the line of perfect agreement in both the mid and upper ranges by contrast the xgboost and lstm models display good distributions throughout it can thus be concluded that these two ml models yield a clear improvement in the prediction of peak wave period at the two stations considered 6 3 model ensemble in the model comparisons above we have seen an overall best performance of the xgboost compared to the other two models however we have also seen that the lstm yielded the best performance in terms of values greater that the 95th percentile storm peaks in both significant wave height and peak wave period therefore following pirhooshyaran et al 2020 we investigate the application of an ensemble of these three models in order to assess if their combined performance can improve on those of the individual members fig 22 and the last two columns of tables 5 and 6 show the results of the ensemble mean of the three component models the top row of fig 22 shows that the ensemble mean has a low scatter and low overall bias the scatter values at stations 45005 mae 0 068 m mape 13 33 si 0 151 cc 0 964 and 45142 mae 0 090 m mape 17 27 si 0 195 cc 0 977 are the lowest of the three component models considered see also table 5 by comparison the bias results in particular those of the values greater that the 95th percentile parentheses are an improvement over those of ww3 but they fall short of the improvements seen in the xgboost and lstm members a similar outcome is seen for the peak wave period fig 22 bottom row the scatter results of the ensemble mean improve on those of the individual members both at station 45005 mae 0 287 s mape 7 96 si 0 108 cc 0 904 and station 45142 mae 0 482 s mape 12 98 si 0 157 cc 0 885 however the ensemble mean bias both overall and values greater that the 95th percentile performs better than ww3 but worse than the stronger ensemble members lstm and xgboost the q q plots in fig 23 confirm these bias results of the ensemble mean for the significant wave height top row we see that the distribution has improved relative to that of ww3 compare fig 19 bottom row but still represents an underestimation relative to those of the lstm and xgboost fig 19 center and top rows similarly for the peak wave period fig 23 bottom row the ensemble mean improves on the results of ww3 compare fig 21 bottom row but performs poorer than the lstm and xgboost component models fig 21 center and top rows we can therefore conclude that the ensemble mean generally improves the component model performance in terms of scatter but does not yield the best results in terms of correcting the negative bias during storm peaks 7 model sensitivity in the previous section we showed that advanced ml models can be effective in predicting significant wave height and peak wave period at point locations however it is well known that such data driven models are limited in their generality to the data sets on which they were trained here we investigate the models sensitivity to data set size to investigate the effect of data set size we conducted another training for the xgboost and lstm models in which we limited the training partition size to only 5 years 2011 2015 using the trained models to predict significant wave height for the same test period as before 2016 2017 with this retrained xgboost model the mae for station 45005 is 0 080 m with a maximum absolute error of 0 910 m for station 45142 the mae is 0 101 m with a maximum absolute error of 1 036 m fig 24 shows that the original model trained with 15 years of data performs somewhat better than the one trained with only 5 years of data however the performance of the model fit on the longer training period is not much better which means that it is close to saturation on the training data after about 5 years this is typical for a tree based model it builds up enough tree representations when it sees enough samples most importantly the largest and the smallest for comparison fig 25 shows the results of the lstm model that was trained on the shorter period of 5 years with this retrained model the mae for station 45005 is 0 090 m with a maximum absolute error of 0 887 m for station 45142 the mae is 0 147 m with a maximum absolute error of 1 764 m these mae results are better than those of the lstm trained on the 15 year data set for the upwind station 45005 but worse for the more geographically complex downwind station 45142 we can therefore conclude that unlike the xgboost model the additional data used in the original lstm training did improve the quality of the model thus xgboost would be a better choice for this wave modeling application if only a few years of training data would be available 8 conclusions in this study ml methods based on xgboost and lstm were applied to wind wave prediction in lake erie and were compared to the physics based spectral wave model ww3 this enclosed basin with a wind climate directed along its major axis represents near idealized conditions under which to compare the modeling of wind wave growth by these models buoy data at the upwind ndbc 45005 and downwind ndbc 45142 from 1994 to 2017 were processed for model training validation and testing we used 1994 2013 data for training the model applying observed u 10 wind velocity as model input and observed significant wave height and peak wave period as the target variables data from 2014 2015 was applied for model validation and we subsequently used the trained models to evaluate significant wave height and peak period for the unseen test period of 2016 2017 from the results of this study the following conclusions can be drawn 1 the physics based ww3 model was found to perform well in terms of significant wave height with a low overall bias and a low scatter expressed in a mean absolute percentage error mape of 15 33 at station 45005 and 21 01 at station 45142 however ww3 tends to underestimate wave height during storm events expressed in relative biases of 11 76 and 14 15 at stations 45005 and 45142 respectively computed over wave heights exceeding the 95th percentile the scatter in peak wave period was acceptable with a mape of 12 51 and 19 32 at stations 45005 and 45142 respectively however the overall relative bias at these stations was quite large namely 9 02 and 11 51 respectively increasing to 15 59 and 19 68 during storm peaks 2 the trained xgboost model yields significant wave height predictions with comparable scatter to ww3 and extreme values are less underestimated with a relative bias of 2 56 at station 45005 and 10 61 at 45142 for values above the 95th percentile furthermore the scatter and bias in peak wave period are lower than for ww3 with overall relative bias values of 0 87 and 1 47 at 45005 and 45142 respectively and values of 9 62 and 10 13 during storm events 3 the trained lstm model yields significant wave height predictions with larger scatter and overall bias than ww3 and xgboost however it yields improved relative bias during storm conditions compared to ww3 with values of 3 34 and 8 93 at stations 45005 and 45142 respectively for values exceeding the 95th percentile for peak wave period it yields the lowest relative bias of the three models during storm conditions namely 8 08 and 7 89 for stations 45005 and 45142 respectively we conclude that for such extreme events fetch and duration effects are import to incorporate done here via the lstm s long and short term memories 4 ensembling the three studied models was found to be a good strategy since it combines some of their strengths the overall scatter of the ensemble mean was lower than any of the component models with mape values of 13 33 and 17 27 in significant wave height and mape values of 7 96 and 12 98 in peak wave period at stations 45005 and 45142 respectively however the bias of the ensemble mean fell short of the lowest values amongst the component models for significant wave height during storm events it yielded relative biases of 5 89 and 11 23 at stations 45005 and 45142 respectively and for peak wave period during storm events it yielded relative biases 11 10 and 12 57 respectively which are greater than found for either xgboost or lstm 5 a benefit of the tree based xgboost model is that it can also perform well on a smaller set of training data training our model on only 5 years of data yields test partition results that are comparable to those found after training the model with the full 15 years of training data this suggests that the xgboost model fitting has become saturated after processing about 5 years of training examples 6 the ml models evaluated here represent a significant reduction in computational times compared to traditional physics based spectral wave models to complete the 2016 2017 prediction on the same computing environment ww3 needed 24 h with 60 cpus whereas the trained xgboost needed only 0 03 s for predicting with 1 cpu and the trained lstm required 0 24 s on 1 cpu it should be mentioned however that the xgboost and lstm models built here output only significant wave height and peak period and only at the point locations for which they were trained by contrast ww3 is capable of outputting complete fields of significant wave height wave period wave direction and the full directional wave spectrum nonetheless james et al 2018 have shown that ml approaches as studied here can readily be extended to outputting fields of wave variables if sufficient area covering training data would be available these authors show how significant wave height results from physics based models such as swan can be used for this purpose however biases from these physics based models such as those shown in figs 18 21 would need to be removed before using their output as training data for the ml models in this regard the reader is reminded that the comparisons presented here were made with respect to the operational ww3 model for the great lakes as benchmark it is possible that the performance of the physics based ww3 could be improved for lake erie with a local tuning using the data utilized for training our ml models we note furthermore that even though this study was conducted in an enclosed basin to isolate the process of wind wave growth the xgboost and lstm models evaluated here are equally applicable to open coast and offshore conditions for such more generalized conditions currents and offshore wave boundary conditions would have to be included as predictors for shallow water coastal applications water level variation from tides and surge should also be included as predictors credit authorship contribution statement haoguo hu conception and design of study acquisition of data analysis and or interpretation of data writing original draft andré j van der westhuysen conception and design of study analysis and or interpretation of data writing original draft philip chu conception and design of study writing review editing ayumi fujisaki manome conception and design of study acquisition of data analysis and or interpretation of data writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was funded by the noaa great lakes environmental research laboratory awarded to the cooperative institute for great lakes research ciglr through the noaa cooperative agreement with the university of michigan na17oar4320152 this is glerl contribution 1981 and ciglr contribution 1182 the authors would also like to thank the reviewers for their constructive comments which improved the quality of this manuscript 
23899,flash rips and surf eddies are transient horizontal structures of the order of 10 to 100 m which can be generated in the surfzone in the absence of bathymetric irregularities they are traditionally evaluated in a depth averaged setting which involves intrinsic horizontal shear instabilities and the direct generation of vorticity by short crested waves in this article we revisit the processes of surf eddy generation with a new three dimensional wave resolution model croco and provide a plausible demonstration of new 3d non hydrostatic instability and turbulent cascade we first present a quick overview of a compressible free surface approach suitable for nearshore dynamics its ability to simulate the propagation of surface gravity waves and nearshore wave driven circulation is validated by two laboratory experiments next we present a real world application from grand popo beach benin forced by waves with frequency and directional spreading the generation of surf eddies by the 3d model differs from depth averaged models due to the vertical shear associated with shallow breaking waves in this case the generation of eddies from both horizontal shear instability and the breaking of short crested waves is hampered the former by stretching the alongshore current and the latter by inhibiting the inverse energy cascade instead the vertical shear flow is subjected to forced wave group variability and kelvin helmholtz type instability at an inflection point primary and secondary instabilities generate spanwise and streamwise vorticity connecting small scale eddies to larger horizontal surfzone structures streamwise filaments appearing as 5 m wide ribs or mini rips can extend beyond the surfzone but with moderate energy these results appear consistent with the velocity spectra and observed patterns of tracers and suspended sediments at grand popo beach the timescale associated with the mean shear induced turbulence is several times the wave period and suggests an intermediate range between breaker induced turbulence and large scale surf eddies keywords surfzone rip currents 3d instability turbulent cascade wave resolving rans model 1 introduction flash rips and surf eddies are generally defined as transient horizontal structures of size ranging between water depth and surfzone width i e of order 10 100 m which are generated in the surfzone in the absence of bathymetric irregularities e g johnson and pattiaratchi 2004 they are studied separately from stationary rip currents confined to deeper channels between sandbars bowen 1969 macmahan et al 2006 marchesiello et al 2015 they are also separated from breaker induced rollers that scale with wave height cox and anderson 2001 and even smaller vortices of the fully developed turbulent bore svendsen and madsen 1984 however the separation between surf eddies and turbulence is uncertain longo et al 2002 and the possibility of intermediate scales and processes linking horizontal and vertical vorticity generation has been suggested e g short et al 1993 describing ephemeral and shallow mini rips over australian low tide terrace beaches but not clearly demonstrated because it is difficult to sample transient rip currents with sufficient spatial resolution lippmann et al 2016 henderson et al 2017 our concepts largely rely on numerical models three types of processes stand out horizontal 2d shear instability of longshore currents short crested wave vorticity generation here called peregrine process tridimensional 3d shear instability the horizontal shear instability of longshore currents was the earliest process proposed for eddy generation describing the intrinsic variability of wave induced currents bowen and holman 1989 dodd et al 1992 allen et al 1996 slinn et al 1998 özkan haller and kirby 1999 dodd et al 2000 uchiyama et al 2009 this process has generally been studied with wave averaged shallow water models in which the momentum transfer from waves to currents is fully parametrized its importance has faded over the last decade not only due to the prevalence of the peregrine process but also to the contradictory results given by three dimensional wave averaged models newberger and allen 2007 splinter and slinn 2003 the second process largely due to peregrine 1998 is the current nearshore community views of driving mechanisms for wave averaged circulation in the surfzone kirby and derakhti 2019 boussinesq equations for weakly dispersive intermediate and shallow water waves provide a conceptual model for the action of spatially varying wave breaking i e short crested waves johnson and pattiaratchi 2006 bonneton et al 2010 feddersen et al 2011 clark et al 2012 feddersen 2014 in this model small vortices result from generation by differential breaking and combine over time into larger eddies through an inverse cascade mechanism consistent with 2d turbulence the surfzone is thus a production center for eddies with scales roughly ranging from 10 to 100 m in addition the coastal boundary imposes that eddies and associated filaments can only go offshore providing a mechanism for enhanced cross shore dispersion of various tracers the theoretical framework from depth integrated models neglect the effect of vertical shear following the advent of robust 3d formulations of wave averaged equations mcwilliams et al 2004 ardhuin et al 2008 a number of 3d modeling studies have emerged in the last decade newberger and allen 2007 uchiyama et al 2010 kumar et al 2012 marchesiello et al 2015 uchiyama et al 2017 mcwilliams et al 2018 akan et al 2020 they show a modulation of nearshore circulation when wave breaking occurs in a shallow surface layer however short crested wave breaking is generally neglected in these wave averaged studies or addressed in ad hoc manners and all real scale applications to date are performed using hydrostatic assumption thus underestimating horizontal vorticity motions nonhydrostatic dynamics are essential in our third listed process of surf eddy generation they are mostly studied in laboratory experiment nadaoka et al 1989 and laboratory scale large eddy simulations les using 2 5d cfd models applied to individual wave breaking lin and liu 1998 li and dalrymple 1998 watanabe and saeki 1999 watanabe et al 2005 lubin and glockner 2015 these previous studies show that the spanwise mostly alongshore component of vorticity is an important aspect of the breaking process surface breaking produces traveling rolls through a primary instability which can evolve through secondary instability to produce streamwise vorticity transitioning toward fully tridimensional turbulence however 2 5d cfd models are computationally very expensive and applied to individual breaking waves with only few alongshore wavelengths of the secondary instability precluding any evaluation of eddy statistics in addition these studies do not always clearly distinguish whether the instability is associated with the instantaneous plunging and rebounding jet produced by breakers or with the mean shear flow caused by momentum transfer yet the two processes may be sorted by their timescale i e smaller than the wave period for the rebounding jet watanabe et al 2005 and longer for the mean shear turbulence li and dalrymple 1998 if confirmed the latter could therefore be an intermediate phenomenon between breaker induced turbulence and large scale surf eddies 3d nonhydrostatic processes are usually studied independently of the two others by separate research communities and rarely compared in terms of scales magnitude and interaction the only attempt was made by splinter and slinn 2003 in a proceeding report using a 3d nonhydrostatic model where breaking acceleration is introduced as a body force they show that a simulation with deep breaking reproduces 2d model solutions while the more realistic shallow breaking process seems to disrupt the formation of horizontal shear instability at the expense of vertical shear instability however their domain size does not allow statistical comparisons and the profile of breaking acceleration is imposed not computed from a wave resolving model the present study is a step forward compared with this first work also addressing the case of short crested wave generation note that kumar and feddersen 2017 studied transient eddies produced by a 3d nearshore circulation model forced by short crested waves computed beforehand with a boussinesq model however wave forcing is prescribed as a depth uniform body force i e as deep breaking and could not produce vertical shear of the cross shore flow their hydrostatic assumption also precluded the model from vertical shear instabilities croco coastal and regional ocean community model is a new oceanic modeling system built upon roms shchepetkin and mcwilliams 2005 debreu et al 2012 with an added non boussinesq kernel auclair et al 2018 it solves reynolds averaged navier stokes equations rans on a free surface and terrain following grid and is designed to study realistic fine scale processes from the regional ocean to the littoral zone particular attention is paid to numerical accuracy high performance computing optimization scalability portability and ease of access www croco ocean org this paper presents a quick overview of the nonhydrostatic croco solver with a non boussinesq compressible approach before embarking in its application to nearshore dynamics first its ability to simulate the propagation of surface gravity waves near shore breaking and the resulting vertical circulation is validated against small and large scale laboratory experiments second we present a 3d wave resolving real case simulation of transient rips in the presence or not of short crested waves and strong alongshore currents we discuss fundamental differences in the generation of surf eddies by 3d wave resolving models compared with depth averaged models with a focus on the vertical structure of currents produced by shallow breaking and associated turbulence we conclude on the limitation of simplified vorticity evolution equations in which only the vertical part is considered when so much activity resides in the horizontal vorticity governed by 3d non hydrostatic equations 2 model description because of limited computational resources 3d wave resolving models are still rarely used to study nearshore dynamics in realistic environments les applications appeared in the 1990s and are generally restricted to 2 5d laboratory scale experiments of individual wave breaking early applications used the volume of fluid vof method for free surface tracking e g lin and liu 1998 watanabe and saeki 1999 watanabe et al 2005 derakhti and kirby 2014 larsen et al 2020 this model type with cartesian coordinate where the free surface crosses computational cells arbitrarily fails to precisely apply the pressure boundary condition on the free surface affecting the model accuracy more recently several 3d wave resolving free surface and terrain following rans models have emerged for the nearshore zone e g swash zijlema et al 2011 and nhwave ma et al 2012 derakhti et al 2016 based on earlier attempts e g lin and li 2002 in this case the explicit overturning of the free surface is excluded and the breaking wave is modeled instead with a single valued free surface which follows a shock process and resembles a dissipating bore despite the absence of explicit overturning replaced by parametrized turbulence these models can be accurate as well as computationally efficient orders of magnitude cheaper in the study of waves and wave driven mean and transient circulation croco belongs to this class of models but unlike other attempts resolves the compressible navier stokes equations auclair et al 2018 a compressible approach preserves the hyperbolic nature of navier stokes equations and does not require a global elliptic solver with incremental pressure corrections to ensure the incompressible mass balance as a result it avoids splitting errors between pressure and velocity and approximations made on free surface conditions zijlema et al 2011 derakhti et al 2016 thereby preserving amplitude and nonlinear dispersive properties of surface waves at the same time the absence of global computations by an elliptic solver makes parallelization and optimization procedures much more efficient the cost of solving acoustic waves is managed with a time splitting technique and semi implicit time discretization introduced below the development of croco around the regional oceanic modeling system roms has advantages for realistic applications it benefits from capabilities long developed in oceanic models high performance computing high order discretization coupling with biogeochemistry and sediment models pre processing tools for rapid generation of model input various online and offline diagnostics the nonhydrostatic model version can thus be applied without much effort to realistic highly nonlinear regimes e g large internal solitons and hydraulic jumps hilt et al 2020 kelvin helmholtz instabilities penney et al 2020 langmuir turbulence herman et al 2020 or wave induced nearshore circulation as in the present study it is naturally suited for bridging ocean and coastal sciences e g addressing surf shelf exchange processes in a 3d rotating and stratified framework in addition both wave resolving and wave averaged uchiyama et al 2010 marchesiello et al 2015 model equations are available within the same code which has potential advantages for evaluating the parametrizations of wave current interactions in wave averaged models 2 1 free surface compressible ocean model equations the full set of navier stokes equations for a free surface ocean is explicitly integrated in the nonhydrostatic non boussinesq compressible version of croco built on the code structure of roms primitive equation solver in the compressible approach auclair et al 2018 acoustic waves are solved explicitly to avoid boussinesq degeneracy which inevitably leads to a 3d poisson system in nonhydrostatic incompressible methods detrimental to computational costs and accuracy of free surface model implementation non boussinesq equations include the momentum and continuity equations the surface kinematic relation for free surface heat salt or other tracer c conservation equations and the equation of state which reads in cartesian coordinates 1 ρ u t ρ v u ρ f v ρ f w p x f u d u λ v x 2 ρ v t ρ v v ρ f u p y f v d v λ v y 3 ρ w t ρ v w ρ f u p z ρ g f w d w λ v z 4 ρ t ρ v 5 η t w f z η v z η η 6 ρ c t ρ v c f c d c u v w are the x y z components of vector velocity v η is the free surface p the total pressure ρ the density f x y and f x y are the traditional and non traditional coriolis parameters function of latitude g is acceleration of gravity d u d v d c are eddy diffusion terms requiring second moment turbulence closure models f u f v f c are forcing terms λ is the second bulk viscosity associated with compressibility used to damp acoustic waves 2 2 time splitting principle in the above set of equations a relation between ρ and p is required to that end and as part of a time splitting approach density is decomposed into slow and fast components based on a first order linear decomposition with respect to total pressure in the following s and f subscripts refer to slow and fast mode components respectively 7 ρ ρ s t s p s ρ p t s δ p ρ f c s 2 p f o δ p 2 8 p p a t m z η ρ s ρ 0 g d z s l o w ρ 0 g η z δ p p f f a s t c s is the speed of sound and δ p p f is the nonhydrostatic pressure the navier stokes equations are then integrated with two different time steps within the time splitting approach inherited from roms the slow mode integration is similar to roms with the addition of the slow part of vertical momentum equation while fast mode integration is in 3d and includes the compressible terms of momentum and continuity equations in vector form 9 ρ v t ρ v v 2 ρ ω v z η f ρ s ρ 0 g d z f v d v s l o w ρ 0 g η f p f ρ f g λ v f a s t 10 ρ f t ρ s t ρ v 11 p f c s 2 ρ f 12 η f t w f z η v f z η η f 13 ρ c s t ρ v c s f c d c 14 ρ s ρ t s s s η f 15 ρ ρ s ρ f the momentum is integrated both in slow and fast modes but the right hand side of the equation is split into two parts a slow part made of slowly varying terms advection coriolis force baroclinic pressure force and viscous dissipation and a fast part made of fast varying terms the surface induced and compressible pressure force weight and dissipation associated with bulk viscosity numerical integration is therefore done twice once with a large time step keeping the fast part constant and once with a smaller time step keeping the slow part constant this is much more computationally efficient than integrating the whole set of equations at the same fast time step more details can be found in auclair et al 2018 1 1 auclair et al 2018 presented a first implementation of the compressible approach involving a 3 level time splitting internal external and acoustic croco was simplified to only retain a slow and a fast time level where acoustic waves are solved together with the external depth averaged mode this procedure is more computationally efficient note that acoustic waves can become pseudo acoustic if their phase speed c s is artificially reduced c s is a model parameter in this case high frequency processes associated with bulk compressibility may be unphysical but an accurate solution for slower nonhydrostatic dynamics can be preserved while relaxing cfl constraints in our nearshore applications a c s value of 200 m s instead of 1500 m s makes almost no difference for the physical solution but allows a great reduction in the computation time by almost half 2 3 discretized equations for nearshore applications in this study motions are produced by an offshore wave maker in a non rotating homogeneous fluid in this case the coriolis force baroclinic pressure force and all surface fluxes are null there is no temperature or salinity stratification so that slow density ρ s is constant in time and space croco is discretized on a c grid with finite difference methods for slow and fast modes that are detailed elsewhere shchepetkin and mcwilliams 2005 soufflet et al 2016 in short the slow mode time stepping algorithm is a leapfrog adams moulton predictor corrector scheme that is third order accurate for integrating advective terms the fast mode is integrated with a generalized forward backward scheme which is also third order accurate vertical flux terms that do not require accuracy vertical diffusion term in the slow mode and all acoustic terms of w equation in the fast mode are computed with an implicit time stepping to increase computational stability horizontal and vertical advection terms are discretized using the weno5 z improved version of the 5th order weighted essentially non oscillatory scheme borges et al 2008 which is popular for hyperbolic problems containing both shocks and smooth structures weno5 z naturally copes with dispersive numerical modes as well as shocks caused by breaking waves with no need for ad hoc criteria 2 4 turbulence closure along with the numerical treatment of breaking waves a k ϵ or k ω model solving the closure equations for turbulent kinetic energy k and dissipation ϵ or dissipation rate ω ϵ k 1 is used as part of a generic length scale gls method warner et al 2005 in the absence of buoyancy forcing the turbulence equations express a balance between transport diffusion shear production and dissipation 16 ρ k t ρ v k d k ρ p ϵ 17 ρ ϵ t ρ v ϵ d ϵ ρ ϵ k c ϵ 1 p c ϵ 2 ϵ or 18 ρ ω t ρ v ω d ω ρ ω k c ω 1 p c ω 2 ϵ the eddy viscosity ν t c μ l k 1 2 is derived from these equations with coefficient c μ dependent on stability functions and mixing length l k 3 2 ϵ 1 l is resolution independent which is consistent with a rans rather than les approach the shear production term for k is p 2 ν t s i j s i j with the mean strain rate tensor s i j 1 2 u i x j u j x i using einstein notation all turbulence model parameters are given in warner et al 2005 based on burchard et al 1998 for k ϵ and wilcox 1988 for k ω the only present modification in the k ϵ model concerns the surface mixing length a model boundary condition which is briefly discussed in the validation section 3 2 for this reason and for its robustness through resolutions and benchmarks the k ω model will be our standard turbulence model however we note as mayer and madsen 2000 and larsen and fuhrman 2018 that this model tends to produce excessive mixing in potential flow regions i e on the innershelf this problem will be addressed in further studies including realistic conditions of stratification and wind forcing 2 5 wave maker at offshore boundary the wave maker forces a spectrum of 3d linear waves at the offshore boundary as in zijlema et al 2011 rather than as an interior source term wei et al 1999 the spectrum has frequency and directional spreading similar to feddersen et al 2011 19 η b c y t i a i j d j cos k y i j y ω i t ϕ i j 20 with d j e θ j θ m σ θ 2 and d j 1 21 u b c y z t η b c y t ω p cos θ m cosh k p z h sinh k p h 22 v b c y z t η b c y t ω p sin θ m cosh k p z h sinh k p h where x y z are cross shore alongshore and vertical directions respectively i j are indices of spectral distribution in frequency and direction respectively a i is the amplitude at each frequency ω i from a given statistical distribution e g jonswap section 3 2 and section 4 1 k y i j k i sin θ j is the alongshore wavenumber where k i is the linear theory wavenumber ω i 2 g k i tanh k i h with h the mean water depth θ j is wave angle θ m is the mean wave direction and σ θ the directional spread around the mean ω p and k p are peak frequency and wavenumber d j is a normalized frequency dependent directional distribution ϕ i j is a uniformly distributed random phase here w b c is set to zero and our tests show only weak sensitivity to this choice depth averaged barotropic velocities u v must be provided as well in the wave maker because they are prognostic variables of our split explicit model advanced together with the fast acoustic mode normal depth averaged velocity u is complemented at the boundary by an anti stokes compensation flow opposite to stokes drift and thus closing the volume budget we do not impose the depth averaged value of u b c directly but the value of the incoming characteristic of the shallow water system as in flather type conditions marchesiello et al 2001 blayo and debreu 2005 23 u u b c g h η η b c this allows infragravity waves generated inside the domain to propagate out as long waves while ensuring a near conservation of mass and energy through the open boundary likewise the baroclinic components u b c v b c w b c are applied via an adaptive radiation condition which helps short waves and 3d flow perturbations to leave the domain with only a small effect on the interior solution marchesiello et al 2001 3 validation in flume experiments 3 1 globex experiment as a first step toward 3d modeling we present here a validation of wave propagation and breaking using a wave flume experiment the gently sloping beach experiments globex 2 2 globex data is freely available at zenodo org record 4009405 were performed in the scheldt flume of deltares delft the netherlands in 2012 and described in michallet et al 2014 the project objective was to collect high resolution space time data of the cross shore evolution of short and infragravity waves on a gentle slope for a range of wave conditions the flume is 110 m long 1 m wide and 1 2 m high the waves were generated with a piston type wave maker equipped to minimize reflections from the wave paddle a concrete beach with a weakly reflexive 1 80 slope was constructed with its toe at 16 57 m from the wave maker all experiments were run with a still water depth of 0 85 m and shoreline at x 84 57 m the material that was laying loose on the concrete bed before the flume was filled with water had a median grain size d50 0 75 mm sea surface elevation measurements were taken at 190 locations repeating an experiment ten times while relocating the 21 wave gauges together with velocity measurements at 43 locations mostly but not always at 1 cm above bed to focus on the undertow the sampling frequency of the instruments during these experiments is 128 hz here we focus on experiment b3 corresponding to second order stokes wave generation of bichromatic frequencies simulated with a boussinesq model in michallet et al 2014 the characteristics are as follows a 1 0 09 m a 2 0 01 m f 1 0 420 hz f 2 0 462 hz short wave peak period t p 2 f 1 f 2 2 27 s and group period t g 1 f 2 f 1 23 81 s the signal had a total duration of 75 min the model is set up with the same conditions as the wave flume experiment second order bichromatic waves are generated at the offshore boundary with shore normal direction and zero directional spread a no slip condition is imposed on the lateral wall boundaries of the canal so that transverse modes are precluded the grid spacing is d x 1 cm with 10 vertical levels evenly spaced between the free surface and bottom a simulation with 20 levels gave similar results while the solution is moderately degraded mostly in higher moments with coarser horizontal resolution d x 3 6 and 12 cm which shows good convergence properties the model time step is d t 0 15 ms the minimum depth is 1 mm on the shore the position of which varies with the swash oscillation relying on a wetting drying scheme warner et al 2013 for bottom drag the logarithmic law of the wall is used with roughness length z 0 d 50 12 0 0625 mm fig 1 compares an hovmuller plot x time of data and model sea level η and undertow u b the model u b is interpolated at the measurement depth when some data is missing in the measurements it is also removed from the model output the general structure reflecting wave speed and frequencies wave packets surf and swash zones are all very similar model data correlations are high with 0 85 and 0 80 respectively for η and u b and root mean square errors are 2 7 cm and 12 2 cm s some scattering in the undertow data is noticeable according to michallet et al 2014 it may be attributed to the presence of secondary motions generated by transverse waves at the break point where the transverse mode 1 seiche can be excited at frequency f 1 f 2 it may also be due to variations in the depth level of flow measurements a snapshot of wave field across the flume during runup fig 2 highlights the main processes of propagation nonlinear wave wave interactions shoaling breaking roller propagation and runup model data correlation is high as already mentioned and non linearity is apparent in both cases in the increasingly non sinusoidal shape of short waves as they approach the shore elgar and guza 1985 they first develop short high wave crests with increasing skewness asymmetry about the horizontal axis measuring crest trough shape and as they break transition into the characteristic saw tooth shape with asymmetry about the vertical axis wave statistics first second and third moments for η and u b are shown in fig 3 mean standard deviation or h s for η skewness ϕ 3 ϕ 2 1 5 and asymmetry h ϕ 3 ϕ 2 1 5 h is hilbert transform the model sea level statistics left of fig 3 closely resemble the measurement data including high order moments showing the transition from skewness to asymmetry across the shoaling and surf zones with two peaks in the asymmetry profile corresponding to outer and inner surf zone evolutions for the mean field of measured undertow a few scattered points lying far outside the standard deviation are corrected using a polynomial fit the model appears to replicate the observed cross shore undertow profile top right of fig 3 the undertow is part of a vertical recirculation associated with breaking induced surface onshore flow here we call undertow the bottom return flow 10 cm s in this experiment which includes the eulerian anti stokes compensation flow preserving lagrangian flow continuity the latter is the only undertow component captured by depth averaged models and is relatively weak on the order of 1 cm s in this simulation in fig 3 the model also correctly represents high order moments which show profiles similar to those of the sea level skewness and asymmetry from sensitivity tests it appears that a realistic reproduction of h s cross shore evolution in the surfzone benefits from using a shock capturing scheme weno5 z the results are degraded not shown when replacing weno5 z with up5 a non monotonic linear upstream biased 5th order advection scheme menesguen et al 2018 this is in line with analogies between breaking waves bores and hydraulic jumps that may be treated as a shock cienfuegos et al 2010 tissier et al 2012 lubin and chanson 2017 the preservation of steep wave fronts by shock capturing schemes can help generate asymmetry even to the excess depending on resolution tissier et al 2012 however this is balanced here by additional eddy viscosity from the turbulence model which is otherwise required below wave trough level to capture the right vertical shear as shown next 3 2 large scale lip flume experiment the undertow may be considered as a proxy for vertical shear which will appear as an essential parameter of surf eddy generation in the next section to further confirm the model s ability to simulate this shear we now present a comparison with the european large installation plan lip experiments designed for profile validation and carried out at full scale in delft hydraulics s delta flume roelvink and reniers 1995 it will also be a test for our numerical wave maker in its ability to generate a spectrum of random waves the flume is 225 m long 5 m wide and 7 m deep in lip three types of experiments were designed with different wave conditions which subsequently resulted in a stable a erosive b and accretive c beach state here we use the erosive experiment lip11d 1b the wave conditions were a jonswap narrow banded random wave spectrum generated by a wave paddle with characteristics h s 1 4 m t p 5 s and peak enhancement factor γ 3 3 under this wave forcing a sandbar formed and slowly migrated across the initial beach profile of slope 1 30 consisting of a median grain size of 0 22 mm z 0 d 50 12 0 02 mm a movable carriage was placed 10 cm above the bed to capture the depth varying structure of the currents at 10 locations along the flume with a given accuracy of 2 cm s we use measurements taken after 8 h in experiment 1b and averaged over one hour the model setup is adapted from the globex experiment to the lip experiment in particular a jonswap wave spectrum similar to the experiment is generated with shore normal direction and zero directional spread the grid spacing is d x 25 cm with 10 vertical levels evenly spaced between the free surface and bottom here again a simulation with 20 levels gave similar results and a test of coarser horizontal resolutions d x 50 cm and 1 m is presented below the model time step is d t 25 ms fig 4 shows a comparison of the model with data using our standard configuration the match with measured currents is very good throughout the complex morphology of the beach the waves start to break before the sandbar but the breaking is more intense on the sandbar where the surf is strongest on the onshore side the resulting undertow has a strong shear and maximum intensity of about 30 cm s the resolution test 25 cm 50 cm and 1 m shows a mean error of about 3 cm s at all resolution close to the measurement error of 2 cm s the results are thus consistent at all resolutions despite no adjustment of any parameter with the small scale globex experiment it confirms the validity of a rans approach for estimating the mixing length of breaking induced turbulence which will be distinguished from mean shear induced turbulence in the next section turbulent kinetic energy and eddy viscosity estimated by the k ω model in the breaker zone have the expected structure fig 4 top and magnitude ν t 0 01 h g h svendsen 1987 cox et al 1994 interestingly the transport terms in the closure equations tend to reduce mixing at break point by redistributing the turbulent energy thus allowing a more intense shear to be maintained not shown the k ϵ model works almost as well as the k ω model with respect to mean current profiles but the comparison is improved by imposing a high value on the surface mixing length 0 2 m as in wave averaged models feddersen and trowbridge 2005 kumar et al 2012 the k ω model may thus be a better choice for surface wave breaking possibly due to a more accurate near wall treatment mayer and madsen 2000 devolder et al 2018 larsen et al 2020 however as mentioned this model produces a greater amount of mixing in potential flow regions outside the surf zone mainly due to the divergence part of the mean strain rate tensor mayer and madsen 2000 while an extensive study of surfzone turbulence is beyond the scope of this paper we conclude that our combination of numerical and physical closures with off the box parameters although perfectible provides a realistic and robust framework for the horizontal and the vertical circulation in the surfzone 4 natural beach application we now turn to a full 3d experiment with longshore uniform bathymetry the configuration is derived from grand popo beach 6 2 n 1 7 e benin in the gulf of guinea fig 5 this stretch of coast presents a longshore uniform low tide terrace and steep upper shoreface almar et al 2014 2016 and a sandy wave dominated and microtidal environment exposed to s sw long period swells generated at high latitudes in the south atlantic almar et al 2015a a field experiment was conducted at grand popo beach from 10 to 18 march 2014 almar et al 2014 derian and almar 2017 for our setup we focus on conditions in the middle of the afternoon of march 13 2014 the weather tides and wave conditions were ideal weak winds and wind waves well separated from a narrow band swell with significant wave height h s 1 15 m peak period t p 11 s and wave incidence d 10 from shore normal direction measured from an acoustic doppler current profiler moored in 10 m depth the water was at mid neap tide level low tide terrace at about 1 m depth promoting a narrow surfzone less than 50 m wide a nortek high frequency acoustic doppler velocimeter adv with sampling rate of 8 hz was deployed in the surfzone in the middle of the terrace measuring currents about 0 5 m from the bottom a dye release was conducted to monitor the dispersion induced by flash rips coupled with uav flights stb ds6 hexacopter at an elevation of 100 m the drone camera nikon d700 was looking down with a vertical angle and recorded 4256 2832 px scenes at 1 hz almar et al 2014 derian and almar 2017 4 1 model setup the model parameters are summarized in table 1 the domain is 542 m alongshore by 240 m across shore with periodic alongshore boundary conditions in order to prevent distortion when oblique waves are used with periodic conditions the alongshore size is adjusted according to peak wavelength and mean wave direction this method proved to perform well even with long crested waves the grid spacing is d x d y 0 5 m there are 10 vertical levels evenly spaced between the free surface and bottom the model time step is d t 0 01 s the bathymetry is longshore uniform and built with continuous functions to smoothly fit the low tide terrace structure observed during the survey of almar et al 2014 2018 the depth h is 8 m offshore to 1 mm at shore level the position of which varies with the swash oscillation wetting drying scheme of warner et al 2013 the depth on the terrace is about 1 m which corresponds to the mid tide conditions of the afternoon of march 13 2014 the wave maker is set with following parameters h s 1 15 m t p 11 s d 0 or 10 and directional spread σ θ 30 a jonswap spectrum is constructed with these parameters and a peak enhancement factor γ of 3 3 the weno5 z scheme is used again with the k ω turbulence model bottom roughness is z 0 0 01 mm which may seem low but gives a drag coefficient c d 0 002 in the surfzone c d κ log z 1 z 0 2 with κ 0 41 and z 1 h 10 the first level height above bed a usual value in depth averaged models chen et al 2003 feddersen et al 2011 we follow the practice of these models here to reproduce their results within the pseudo 2d approach described below it is of little consequence for 3d simulations because as will be shown they are much less sensitive to bottom drag than 2d models section 5 the model is run for an hour starting from rest and adjusting through a rapid spin up phase fig 6 presents a snapshot of sea level that shows realistic features short crested waves generated at an angle refract and break producing rollers swash and some reflection croco comes with capabilities for water quality marine ecosystem and sediment modeling in the present study they are used with simple settings first we introduce a passive tracer in the swash zone for comparison with dye releases made during the beach survey second a suspended sediment model blaas et al 2007 warner et al 2008 allows the rip patterns to be compared with aerial photos taken during the survey we use a single fine sand class with settling velocity of 1 cm s for resuspension taking one sediment bed layer for simplicity only two parameters are needed critical shear stress and erosion rate at the seafloor expressed in the erosion flux blaas et al 2007 24 e e 0 1 p τ b τ c r τ c r for τ b τ c r τ b is the bottom shear stress computed by the model e 0 is an empirical erosion rate set to 10 5 kg m2 s p is the sediment porosity 0 41 τ c r is the critical shear stress i e the threshold for initiation of sediment motion set to 0 01n m2 4 2 shallow vs deep breaking and a boussinesq model the peregrine vorticity generation process only requires short crested waves with no need for unstable longshore currents generated by oblique waves boussinesq models are very efficient in this process but an important question for us is whether a 3d model will remain so an essential difference between the two types of model is the depth penetration of wave breaking in a 2d boussinesq model deep breaking is implicitly assumed as momentum is transferred instantaneously to the depth averaged flow however this is a rough assumption as the breaking induced flow is produced essentially above trough level where the onshore flow is located while turbulence generated at the surface spreads downward by diffusion with a limited mixing length of 10 to 30 percent of the water depth svendsen 1987 cox et al 1994 mocke 2001 longo et al 2002 uchiyama et al 2010 show that the deep breaking assumption is inconsistent with the cross shore velocity profiles measured during duck94 splinter and slinn 2003 also suggest that transient dynamics produced by deep breaking may collapse in the more realistic case of shallow breaking the results presented here are in agreement with these studies and we propose to assess the role of 3d dynamics on surf eddies by comparing simulations forced by shallow and deep breaking deep breaking will constitute a pseudo 2d model whose results can be compared with a boussinesq model solution section 4 5 boussinesq type models see barthelemy 2004 for a review are common tools to simulate weakly dispersive waves and their transformations from the ocean to the swash zone several developments allowed their application to a wide range of scales from surfzone processes to ocean basin scale tsunami propagation kirby 2016 here for a verification of our pseudo 2d croco version we use funwave tvd shi et al 2012 it solves the fully nonlinear boussinesq equations using a hybrid finite volume finite difference scheme parametrizations are similar to croco with wave breaking handled by a shock capturing tvd scheme making the need of explicit criterion unnecessary and a quadratic drag formulation with c d 0 002 for bottom friction note that newer generation models than funwave are available with better dispersive properties using green naghdi equations lannes and bonneton 2009 or incorporating an additional enstrophy equation kazakova and richard 2019 but funwave is widely used and share with this class of models the essential depth averaged assumption see table 2 to force deep breaking in a 3d wave resolving model we can artificially enforce strong deep vertical mixing around the breaker zone by multiplying the vertical eddy viscosity by 10 from values of ν t 0 01 0 05 m 2 s fig 7 shows a time and longshore average of cross shore and alongshore currents in the case of shallow and deep breaking shallow breaking is computed by the 3d model with no explicit constrain on penetration scale but a parametrization of eddy viscosity induced by breakers and currents it drives a shallow onshore flow about 40 cm deep and an offshore near bottom undertow resulting in strong vertical shear of about 1 s 1 with artificially strong vertical viscosity momentum in the breaker zone is almost instantly mixed to the bottom and the cross shore flow is reduced to the part required by mass conservation the anti stokes compensation flow and consistent with depth averaged models the longshore flow driven by oblique waves and a number of other simulations with shallow and deep breaking table 2 will be analyzed and compared in the following sections 4 3 reference simulations and comparison with data to introduce the 3d processes of flash rip generation we present simulations with shallow and deep breaking 3d sc d10 and 2d sc d10 representing mid tides conditions on march 13 2014 in grand popo beach fig 8 compares the vertical vorticity of surface flow ω z v s x u s y phase averaged over two peak periods 22 s with deep breaking left panel the vortical field is rich with large filaments and surf eddies of 50 100 m scale that are generated from short crested waves similar to boussinesq model solutions however the full 3d model with shallow breaking offers a radically new solution fig 8 right panel some of the large scale fluctuations are present but over shadowed by shorter scales this mode presents itself as rib structures or mini rips following a relevant observation by short et al 1993 already mentioned with short longshore wavelength of about 5 m and period about 1 min we now compare the two simulations with data collected during the survey of march 2014 flash rips did not appear particularly large to the survey team at high or mid tide but a higher frequency signal was present and interpreted as swash rips castelle et al 2014 scott et al 2018 floc h et al 2018 dye experiments revealed filament generation but of relatively short scales although the survey was not extensive enough to draw definite conclusions the adv data as well as the released dye and suspended sediment patterns suggest a dynamical regime closer to 3d shallow breaking than pseudo 2d deep breaking simulations as will be seen the dye experiment presented in fig 9 illustrates both the structure of the alongshore flow and scales of flash rips emerging from the surfzone careful analysis of observed versus modeled dye evolution as in hally rosendahl and feddersen 2016 is beyond the scope of this paper but useful information can be gained from a simpler analysis fig 9 presents two consecutive aerial photos at 116s interval and the corresponding snapshots of tracer simulations with the full 3d model we do not expect an exact match between observed and modeled rips considering the chaotic nature of these phenomena 3 3 during the survey a few attempts of dye release were made before obtaining a clear filament patch similarly for the model we selected one occurrence among few tracer patches initialized at regular interval along the coast in the swash zone even though all tracer patches eventually ended up with similar v shape and similar scales of evolution there was variability in the evolution and we selected the most visually comparable filament with drone photos but scales and structures are meaningful in both cases a thin filament of about 5 m expands quickly seaward at a speed of about 0 5 m s reaching about 70 m from shore using sequential photos of the tracer from the drone camera it is also possible to extract a simplified cross shore profile of longshore drift velocities see derian and almar 2017 for more extensive lagrangian calculations the result is presented in fig 10 together with the adv measurement of mean longshore current over the terrace and model solutions with deep and shallow breaking the estimated longshore flow has an asymmetric v shape similar to the full 3d model solution with a peak velocity of about 0 5 m s in the inner surfzone and error bar of about 0 1 m s derian and almar 2017 deep breaking solutions have a more symmetric profile centered in the outer surfzone the profiles in both simulations are a result of cross shore advection with deep breaking advection is weak and the longshore flow remains centered in the breaker zone fig 7 this is a common bias of depth averaged models larson et al 2002 another qualitative comparison of patterns can be made looking at surfzone suspended sediments in the aerial photo fig 11 left panel the contrasts in the photo are enhanced to better expose suspended sediments brown color which is seen weakly extending beyond the surfzone snapshots of the model s surface sediment concentration are also shown after 15 min of simulation with shallow breaking center panel sediments tend to resuspend in the breaker zone and mix efficiently within the surf zone but only weakly extend to the innershelf the rib structure is apparent at the seaward front of sediment concentration it is also apparent in white streaks representing alongshore surface current convergence that have a structure similar to the foam lines in the aerial photo overall the patterns are similar to the observations particularly in the upper part of the photo where there is less foam or sunglint the same suspended sediment simulation with deep breaking gives very different results fig 11 right panel resuspension is now maximum in the inner surfzone as for eddy energy section 4 5 filaments and eddies are more coherent larger their growth slower but extend further seaward mixing in the surfzone is less efficient than for the shallow breaking case but shelf surf exchange is more intense due to filament extension for a more quantitative local comparison we now turn to adv measurements of horizontal velocities u h u 2 v 2 fig 12 presents the power spectral density psd of velocity fluctuations using welch s noise reduction method for the model and adv data in the middle of the terrace left panel and for the model alone over the outer terrace slope right panel the short wave spectrum around the peak period 11 s is well represented given the jonswap approximation made for the model wave maker at a lower frequency a good fit with the data is also given by the full 3d solution while the deep breaking simulation exhibits two opposite biases in successive frequency ranges valid at 95 confidence level which are even more pronounced near the terrace slope these biases are consistent with those noted in feddersen et al 2011 the first is an underestimation of energy by the pseudo 2d model in the 30 100s period range this band is consistent with visual inspection of rip structure oscillation in animated vorticity fields the 3d model seems to correct the deficit particularly in the lower frequencies note that this energy range for 3d eddies overlaps that of infragravity waves making it difficult to separate the two phenomena from observations alone at very low frequency vlf for timescales between 2 and 15 min a second bias of opposite sign is observed in the pseudo 2d model solution in this range the eddies produced by short crested waves have more energy in the deep breaking case than in the 3d case as predicted by inspection of vorticity and suspended sediment the comparison with the data therefore suggests that the vlf energy is overestimated by depth averaged models consistent with feddersen et al 2011 and newberger and allen 2007 we conclude from this section that the observations at grand popo beach are in better agreement with a complete 3d solution of surf eddies that includes the presence of 3d rib structures next we analyze their generation process 4 4 structure and production of vertical shear instability the surfzone eddy variability seen in the 3d model solution is truly three dimensional vertical vorticity is only one manifestation but horizontal vorticity is the main player fig 13 presents the q field defined by q 1 2 u i x j u j x i using einstein summation convention over the three dimensions q is commonly used to enhance detection of vortical flows here we split cross shore and alongshore q components q y u z w x 1 2 u x 2 in red and q x v z w y 1 2 v y 2 in green then normalize them and only plot positive isosurface values 0 02 for clarity negative values give counter rotating features the result is strikingly consistent with instabilities of a transitional mixing layer metcalfe et al 1987 lesieur 1990 the transition being constrained by surfzone width q y shows spanwise rolls created from the primary instability while q x identify streamwise ribs that are transverse counter rotating vortices from secondary instability assumingly growing from perturbations generated between the rolls in the braid region note that streamwise designates the shear direction which is cross shore rather than the oblique wave direction the reason is that the bottom flow aligns with the surface flow in the alongshore direction forming only a weak mean vertical shear fig 7 however the rips can take an oblique direction when advected by the mean longshore flow they extend seaward beyond the surfzone while stretching in the vertical direction filaments of vertical vorticity or q z not shown also have similar rib structures to q x recognizable in the surface vorticity plot of fig 8 the mean shear flow is composed of the wave mean surface onshore flow and associated seaward undertow fig 7 the inflected velocity profile is inviscidly unstable to small perturbations and unstable modes of kelvin helmholtz type can emerge the spanwise rolls are large vertical eddies that rapidly evolve into transverse streamwise rib vortices connected by braid regions and stretched seaward and downward this picture is reminiscent of descriptions based on measurements and simulations at the laboratory scale nadaoka et al 1989 watanabe et al 2005 lubin and glockner 2015 but here the instabilities are generated by the wave mean shear flow rather than by direct breaking breaking induced turbulence is parametrized it is therefore more consistent with the instability of the undertow profile described by li and dalrymple 1998 according to linear stability analyses the wavelength of primary shear instability setting the distance between spanwise rolls is an order of magnitude larger than the mixing layer width δ in free shear layers michalke 1964 1965 wavelength frequency and growth rate of the most unstable modes are 14 δ 0 015 u δ and 0 1 u δ respectively the secondary instability wavelength is of the same order as that of the primary instability 2 3 in pierrehumbert and widnall 1982 if the mixing layer width is taken as the vorticity thickness δ δ u u z m a x 50 cm then the wavelength of both roll and rib structures is about 5 m consistent with our simulation note that given an effective resolution of 5 10 d x for croco soufflet et al 2016 a grid resolution of 0 5 m can be considered eddy resolving for 3d instabilities under current conditions as for frequency the mixing layer size would be associated with modes around 0 015 hz 60 s period a range usually reserved to surf beat the model with 3d instability has energy in this range that is lacking in the deep breaking case see previous section and fig 12 but we will see that nonlinear interactions can spread this energy around the injection scale section 4 6 note that the ribs sometimes develop localized pairing which may be evidence of subharmonic resonance another instability associated with shear layers craik 1971 pierrehumbert and widnall 1982 herbert 1983 in order to isolate the mechanism of eddy mean flow interaction we analyze a solution forced with monochromatic shore normal long crested waves 3d mono d0 in table 2 this simulation has constant wave forcing in both space and time when averaging over the wave period 11 s in this case the same rib structure is generated fig 14 but without the large scale alongshore variation seen in the full solution fig 8 right panel a comparison of this simulation with other test cases will be presented in the next section here we analyze the mechanism and patterns of shear production the eke source terms u i u j u i x j represent the energy spent by the mean flow to feed the instability leading to rolls and ribs the largest of these terms is the vertical shear production u w u z metcalfe et al 1987 which is shown in fig 15 there is a lesser contribution from cross shore convergence u u u x all other 7 combinations are negligible the main site of shear production is in the breaker zone fig 15 and the maximum values are located at the inflection point in the mean velocity profile represented by a magenta line in agreement with mixing layer instability theory the figure also shows a cross section of mean eddy kinetic energy e k e 1 2 u 2 v 2 w 2 where u v w are fluctuations of phase averaged velocities with respect to the time mean flow presented in fig 7 eke and shear production have a similar spatial pattern although high eke values extend from the production center in all directions the primary rolls are thus produced in the outer surfzone but turbulent energy is diffused by the mean and eddy flow across the water depth and toward both the inner surfzone and innershelf eke transport is stronger at the surface and streamwise filaments extend farther offshore at the surface than at the bottom despite some amount of vertical stretching as they leave the terrace for comparison the mean subgrid scale turbulent kinetic energy tke produced by the k ω closure equations is also shown in fig 15 bottom eke and tke have a similar structure while eke amplitude is about a quarter of that of tke the mean shear turbulence intensity is thus a significant part of total 3d turbulence generated by breaking waves we expect that part of tke parametrized by the closure model could be transferred to eke if a higher resolution was used 4 5 short crested waves and the peregrine process an important question of our study concerns the effect of wave variations frequency and directional spreading on flash rip generation in a full 3d model to address this question it is useful to simplify the problem and progressively add the multiple conditions of variability in this section we analyze shore normal short crested wave simulations with shallow or deep breaking 3d sc d0 or 2d sc d0 shore normal conditions prevent the formation of a longshore current and associated horizontal shear instability in addition we look at long crested wave solutions to isolate the effect of 3d instabilities i e the monochromatic solution 3d mono d0 presented in the previous section and a similar case with jonswap frequency spectrum 3d lc d0 this latter comparison will help evaluate the effect of frequency spreading on eddy variability before addressing the effect of directional spreading fig 16 compares vertical vorticity for shore normal short crested wave cases as for oblique waves deep breaking leads to a rich vortical field with large filaments extending far offshore similarly to boussinesq models this is confirmed here with a comparison between funwave tvd and pseudo 2d croco applied to the same configuration a difference between boussinesq and pseudo 2d solutions is the effect of 3d dynamics over the innershelf in the latter case where surface intensified offshore eddies and filaments present a more fragmented aspect due to a forward energy cascade uchiyama et al 2017 mcwilliams et al 2018 however the full 3d nonhydrostatic model with shallow breaking fig 16 right panel shows again different patterns from both boussinesq and pseudo 2d solutions with regular rib structures having a shorter alongshore scale and a more limited cross shore extension fig 17 presents eke cross sections for all shore normal wave experiments a striking element of these figures is the presence of large surface and bottom eke in the shallow breaking cases this pattern is not a result of shear production because it is absent from the monochromatic case see previous section and fig 15 therefore it can only result from wave groups associated with frequency spreading through wave height modulation wave groups produce variability in the surface onshore flow and associated undertow the variability amounts to about half the integrated mean eke however it is much smaller in the deep breaking case consistent with depth averaged model results de schipper et al 2014 we now turn to the effect of directional spreading in fig 17 short crested waves produced by directional spreading extend eke production over a wider surf zone than long crested waves where eke is confined to the breaker zone however the seaward extension is significantly larger in the pseudo 2d model confirming the impression made from vorticity inspection further confirmation is given by vertical eke integration and normalization by mean depth fig 18 it highlights 3 distinct regions the inner and outer surfzones and innershelf the top panel presents unfiltered data in this case 3d instability and wave group forcing dominate eddy production in the surfzone the deep breaking solution has a larger cross shore expansion extending out to the innershelf where eke levels are twice as high as in the shallow breaking case this is even clearer using a low pass filter on velocity fluctuations removing a large part of variability from 3d instability and wave group forcing bottom panel of fig 18 what remains is closer to the usual definition of surfzone eddies as very low frequency features there is now a maximum in the inner surfzone consistent with findings from previous boussinesq model studies that filaments forced by short crested waves originate in the inner surfzone then spread offshore forming eddies that grow in scale johnson and pattiaratchi 2006 however the difference of eke profiles between deep and shallow breaking cases is reminiscent of the overestimation of shelf surf exchange by boussinesq models see profiles of dye concentration in fig 10 of hally rosendahl and feddersen 2016 4 6 2d and 3d surfzone turbulent cascades if there is energy produced by short crested waves in the inner surfzone of the shallow breaking case the question is why does it produce fewer large filaments than the deep breaking case we found an answer in computing the spectrum energy flux in 2d turbulence the flux of energy is negative and small fluctuations can grow into larger coherent structures this process is usually involved to explain the growth of filaments and eddies from variable wave forcing e g johnson and pattiaratchi 2006 feddersen 2014 an inverse cascade in the surfzone has recently been confirmed by observations elgar and raubenheimer 2020 but how efficient is it exactly to answer this question we performed a wavenumber spectral flux analysis for pseudo 2d and 3d simulations 2d sc d0 and 3d sc d0 fig 19 the spectral flux is computed as in marchesiello et al 2011 by spectral integration of v advection term consistently with 2d turbulence there is a strong inverse cascade of kinetic energy negative flux in the pseudo 2d model starting from the scale of injection corresponding to short crested wave forcing wavelength of 30 m here and there is no direct cascade toward smaller scales in the 3d case the turbulent regime is different the negative flux of energy produced by variable wave forcing is present but significantly reduced in addition there is a second injection at smaller scales that corresponds to the most unstable mode of 3d instability wavelength of about 5 m this small scale energy travels both backward and forward across the spectrum and thus widens the range of variability associated with 3d instability in the 3d long crested wave case 3d lc d0 a similar spectral flux is produced at small scales but there is no large scale inverse cascade due to missing injection by short crested waves this analysis confirms that the growth of filaments and eddies associated with the peregrine process heavily relies on a 2d inverse cascade but this cascade is impaired by 3d dynamics in this case vorticity fluctuations generated by waves with finite alongshore extent produce less coherent horizontal structures than in the 2d paradigm built on depth averaged models johnson and pattiaratchi 2006 the extent of this inhibition process depends on the intensity of turbulent mixing sensitivity analysis using reduced viscosity shows that lower viscosity leads to a reduction of the 2d inverse cascade at vlf but to an increase of energy production and fluxes at smaller scales the opposite is true when increasing viscosity therefore breaking waves can transfer energy into both 2d and 3d transient circulations but the distribution of energy between the two regimes is regulated by turbulent mixing which occurs at a higher frequency these regimes are not mutually exclusive at least within some range of turbulence intensity the realism of which still needs to be better evaluated 4 7 oblique waves and horizontal shear instability the precedent experiments with shore normal waves show that shallow breaking tends to hinder the generation of large 2d eddies by short crested waves while sustaining forced and intrinsic 3d surfzone eddy dynamics we now address the case of oblique waves with direction d 10 from linear stability analysis of a 2d problem bowen and holman 1989 the wavelength frequency and growth rate of the most unstable shear waves are λ h 2 5 l and f h 0 07 v l σ h 0 15 v l where v is the longshore current magnitude and l is the longshore current half width outer shear for a narrow shoreline intensified jet typical of grand popo at mid tide almar et al 2014 2015b shear can be strong 0 05 s 1 implying a minimum shear wave period of 5 min wavelength of 80 m and growth time σ h 1 3 min when forcing long crested waves σ θ 0 with deep breaking 2d lc d10 croco recovers results that are typical of wave averaged shallow water models or 3d models with deep breaking as in kumar and feddersen 2017 fig 20 left panel shows an active horizontal shear instability producing shear waves with wavelength consistent with the linear theory shear waves propagate with the longshore current as they become nonlinearly unstable generating filaments and eddies that extend offshore when both horizontal shear instability and short crested wave vorticity generation are active with deep breaking eddies and filaments are more prominent 3rd panel of fig 20 however with shallow breaking the horizontal shear instability appears weaker and is again replaced by rib structures both horizontal processes are thus reduced by the vertical shear inspection of the cross shore profile of mean longshore currents fig 7 may help to understand how shallow breaking undermines horizontal shear instability with deep breaking cross shore advection is inactive and the longshore current remains trapped over the terrace slope which is steep in grand popo around 1 10 and its outer shear is strong with shallow breaking however the longshore current is advected by the cross shore circulation stretching its profile across the terrace so as to minimizes the outer shear then the instability growth rate becomes too weak to overcome friction from turbulence or bottom drag this may explain for example the puzzled observation by newberger and allen 2007 that their 3d wave averaged model produced no horizontal shear instability unlike many previous 2d modeling studies allen et al 1996 slinn et al 1998 uchiyama et al 2009 this inhibitory process is verified in fig 21 showing cross shore profiles of the mean and eddy flow averaged over time and the alongshore direction for the four cases of oblique waves horizontal shear instability is best assessed with the term for horizontal shear production of turbulent kinetic energy u v v x in all cases the shear production is clearly correlated with the outer and inner slopes of the mean longshore current with higher production in the outer shear deep breaking cases show higher shear production and greater eke centered on the outer slope of the terrace where the shear is greatest in the case of horizontal shear instability alone 2d lc d10 the magnitude is lower than that of 2d sc d10 despite similar shear intensity this indicates an amplification of shear instability by short crested waves as they drive transient intensification of longshore currents in this case also the eke maximum extends farther offshore than expected from shear production possibly due to mean and eddy advection but short crested waves provide the most efficient process for innershelf eddy activity the shallow breaking cases 3d lc d10 and 3d sc d10 also feature horizontal shear production but weaker and in shallower water where friction is more prevalent as a result eke is significantly reduced short crested waves 3d sc d10 appear to amplify the inner surfzone energy compared with 3d lc d10 but in both cases the offshore energy is considerably reduced 5 discussion and conclusion flash rips and surfzone eddies are traditionally conceived within a depth averaged framework that involves intrinsic horizontal shear instabilities or and direct short crested wave vorticity generation they are revisited in this study using a 3d nonhydrostatic wave resolving model applied to a natural beach with ideal longshore uniform topography grand popo beach benin we first presented a quick overview of a new free surface compressible approach adapted to wave resolved nearshore dynamics its ability to simulate surface gravity wave propagation nearshore breaking and the resulting circulation is validated against small and large scale laboratory experiments then the model is applied to the nearshore circulation generated at grand popo beach by waves with frequency and directional spreading we assume on the basis of the comparison with boussinesq solutions that the essential difference between 2d and 3d models is reduced to the vertical profile of breaking induced acceleration i e deep or shallow breaking this allows a direct comparison of 2d and 3d frameworks within the same model equations and setup the generation of transient rips by the 3d model is shown to differ from that produced by depth averaged models owing to the vertical structure of currents produced by surface intensified acceleration processes of both horizontal shear instability and short crested wave breaking are limited in our 3d model by the cross shore vertical recirculation which can restrict an otherwise strong inverse cascade variable wave forcing in space and time tends to increase flow variability in the surfzone especially at the surface and bottom but it does not fully translate into large scale rips streaming far offshore usual 2d mechanisms may thus be weaker than expected but complemented by a kelvin helmholtz type instability generated at the inflection point of the mean vertical shear flow the latter generates rib structures with spanwise and streamwise alongshore and cross shore vorticity of intermediate scale between turbulence and large horizontal eddies timescale several times the wave period encroaching on the infragravity wave range and wavelength around 5 m streamwise filaments extend beyond the surfzone but with lower intensity than usual vlf rips the offshore mean eke can be halved comforted by observed energy spectra and patterns of tracer and sediment concentrations at grand popo beach our study may call into question the accuracy of nearshore depth averaged models it may explain in particular the evidence of overestimation by these models of the shelf surf exchange spydell and feddersen 2009 hally rosendahl and feddersen 2016 or vlf variability feddersen et al 2011 our results are representative of mid tide conditions of a low tide terrace with moderate wave heights interestingly the rib structures that are described here are comparable to the mini rips described for similar conditions by short et al 1993 under typical mid tide conditions with waves breaking across the bar a low friendly surf zone is produced waves are less than 1 m and most water appears to head toward the shore in fact it is return seaward also both by reflection of the beach face and via the mini rips even if no rip channel are present the rips however are usually weak ephemeral and shallow in order to assess the ubiquity of mini rips in the nearshore zone future 3d studies should explore different nearshore conditions several sensitivity tests were performed in this study which we only briefly report pending further exploration of the model parameter space in the future nevertheless they provide useful material for discussion we first tested the effect of wave amplitude as it affects the breaking induced flow and turbulence a simulation forced with twice as large waves h s from 1 15 to 2 30 m did not fundamentally change the results in case of higher waves breaking induced turbulence can reach deeper depths but the cross shore flow acceleration is also stronger so the result on vertical shear is uncertain next we tested a different beach profile from the steep slope of grand popo to a more gentle slope smaller iribarren number similar to duck beach as in noyes et al 2005 here again the results were similar as the intensity of the mean vertical shear does not appear to be too sensitive to the beach slope the bottom roughness length z 0 was another relevant parameter as z 0 is increased from 0 01 to 1 mm the drag coefficient goes from about 0 002 to 0 008 simulations with deep breaking were very sensitive to these values and the largest roughness value can completely shutdown horizontal shear instability due to shorter frictional time decreased from 5 to 2 min i e shorter than growth time of about 3 min and also damp vortical generation by short crested waves on the contrary full 3d solutions with shallow breaking are only weakly sensitive to bottom roughness because of surface intensified currents and shorter growth time of 3d instability these tests inspire greater confidence in our results while highlighting the overestimated importance given to bottom drag in studies using depth averaged models e g allen et al 1996 eddy viscosity provides the largest source of sensitivity in our results artificially changing the eddy viscosity coefficient lead to qualitatively similar results but with significant variation in the intensity of processes an increase of eddy viscosity in the surfzone reduces the vertical shear damps vertical shear instability as the frictional time h 2 ν t becomes closer to the growth time of about 1 min and intensifies the inverse turbulent cascade in the same time cross shore advection is reduced so that the wave averaged longshore flow is sharper and in deeper waters therefore more sensitive to horizontal shear instability therefore there is a relationship between the intensity of turbulence and sorting of energy that enters 2d and 3d dynamical regimes given the present model uncertainty we can only infer that these regimes are not mutually exclusive and should coexist more validation in different settings will be needed to assess their relative importance finally the present modeling study shed light on the mechanistic process that could inhibit the generation of surf eddies by short crested waves vertical vorticity generation by the peregrine process can be written as 25 ω z t f b r y c where f b r is the breaking force extending to the bottom in a depth averaged model and y c is the along crest direction using a parametrization for breaker acceleration clark et al 2012 propose a scaling relation for vorticity generation of a single wave as h s 3 h 2 5 at a maximum in the outer surfzone however flash rip generation originates in the inner surfzone in depth averaged models johnson and pattiaratchi 2006 as well as in our simulations therefore a transient vorticity source in the outer surfzone is not sufficient to generate the expected local horizontal recirculation with offshore filament it needs a coastal boundary and an inverse energy cascade that transform vorticity fluctuations into larger scale coherent structures as shown by our spectral flux analysis in a 3d regime this cascade can be reduced due to vortex tilting by the shear flow mcwilliams et al 2018 and the variability generated in the inner surfzone does not fully translate into large rips jetting offshore in conclusion our results suggest that nearshore dynamics and transport processes may be affected by nonhydrostatic dynamics not only for surface gravity waves and small scale turbulence as is well known but also for larger scale vortical motions we expect this conclusion to be qualitatively valid in other applications but further studies should explore the range of parameters encountered in the global coastal ocean most importantly they should pay special attention to how these parameters affect the mean cross shore current profiles as a key to 3d transient dynamics credit authorship contribution statement francis auclair conceptualization methodology software laurent debreu methodology software james mcwilliams conceptualization methodology rafael almar validation investigation rachid benshila software franck dumas project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research has received support from a consortium of french national research agency as part of croco s development project gdr croco and from the service hydrographique et oceanographique de la marine shom 20cp05 it was granted access to the hpc resources of calmip supercomputing center under allocation p19069 the fieldwork received support by the french insu ec2co program grand popo experiment we thank h michallet for sharing the globex data now freely available at zenodo org record 4009405 and dano roelvink for sharing the lip data apart from these all data were acquired by the authors and the croco source code is freely available at www croco ocean org both observational and modeling data are available upon request 
23899,flash rips and surf eddies are transient horizontal structures of the order of 10 to 100 m which can be generated in the surfzone in the absence of bathymetric irregularities they are traditionally evaluated in a depth averaged setting which involves intrinsic horizontal shear instabilities and the direct generation of vorticity by short crested waves in this article we revisit the processes of surf eddy generation with a new three dimensional wave resolution model croco and provide a plausible demonstration of new 3d non hydrostatic instability and turbulent cascade we first present a quick overview of a compressible free surface approach suitable for nearshore dynamics its ability to simulate the propagation of surface gravity waves and nearshore wave driven circulation is validated by two laboratory experiments next we present a real world application from grand popo beach benin forced by waves with frequency and directional spreading the generation of surf eddies by the 3d model differs from depth averaged models due to the vertical shear associated with shallow breaking waves in this case the generation of eddies from both horizontal shear instability and the breaking of short crested waves is hampered the former by stretching the alongshore current and the latter by inhibiting the inverse energy cascade instead the vertical shear flow is subjected to forced wave group variability and kelvin helmholtz type instability at an inflection point primary and secondary instabilities generate spanwise and streamwise vorticity connecting small scale eddies to larger horizontal surfzone structures streamwise filaments appearing as 5 m wide ribs or mini rips can extend beyond the surfzone but with moderate energy these results appear consistent with the velocity spectra and observed patterns of tracers and suspended sediments at grand popo beach the timescale associated with the mean shear induced turbulence is several times the wave period and suggests an intermediate range between breaker induced turbulence and large scale surf eddies keywords surfzone rip currents 3d instability turbulent cascade wave resolving rans model 1 introduction flash rips and surf eddies are generally defined as transient horizontal structures of size ranging between water depth and surfzone width i e of order 10 100 m which are generated in the surfzone in the absence of bathymetric irregularities e g johnson and pattiaratchi 2004 they are studied separately from stationary rip currents confined to deeper channels between sandbars bowen 1969 macmahan et al 2006 marchesiello et al 2015 they are also separated from breaker induced rollers that scale with wave height cox and anderson 2001 and even smaller vortices of the fully developed turbulent bore svendsen and madsen 1984 however the separation between surf eddies and turbulence is uncertain longo et al 2002 and the possibility of intermediate scales and processes linking horizontal and vertical vorticity generation has been suggested e g short et al 1993 describing ephemeral and shallow mini rips over australian low tide terrace beaches but not clearly demonstrated because it is difficult to sample transient rip currents with sufficient spatial resolution lippmann et al 2016 henderson et al 2017 our concepts largely rely on numerical models three types of processes stand out horizontal 2d shear instability of longshore currents short crested wave vorticity generation here called peregrine process tridimensional 3d shear instability the horizontal shear instability of longshore currents was the earliest process proposed for eddy generation describing the intrinsic variability of wave induced currents bowen and holman 1989 dodd et al 1992 allen et al 1996 slinn et al 1998 özkan haller and kirby 1999 dodd et al 2000 uchiyama et al 2009 this process has generally been studied with wave averaged shallow water models in which the momentum transfer from waves to currents is fully parametrized its importance has faded over the last decade not only due to the prevalence of the peregrine process but also to the contradictory results given by three dimensional wave averaged models newberger and allen 2007 splinter and slinn 2003 the second process largely due to peregrine 1998 is the current nearshore community views of driving mechanisms for wave averaged circulation in the surfzone kirby and derakhti 2019 boussinesq equations for weakly dispersive intermediate and shallow water waves provide a conceptual model for the action of spatially varying wave breaking i e short crested waves johnson and pattiaratchi 2006 bonneton et al 2010 feddersen et al 2011 clark et al 2012 feddersen 2014 in this model small vortices result from generation by differential breaking and combine over time into larger eddies through an inverse cascade mechanism consistent with 2d turbulence the surfzone is thus a production center for eddies with scales roughly ranging from 10 to 100 m in addition the coastal boundary imposes that eddies and associated filaments can only go offshore providing a mechanism for enhanced cross shore dispersion of various tracers the theoretical framework from depth integrated models neglect the effect of vertical shear following the advent of robust 3d formulations of wave averaged equations mcwilliams et al 2004 ardhuin et al 2008 a number of 3d modeling studies have emerged in the last decade newberger and allen 2007 uchiyama et al 2010 kumar et al 2012 marchesiello et al 2015 uchiyama et al 2017 mcwilliams et al 2018 akan et al 2020 they show a modulation of nearshore circulation when wave breaking occurs in a shallow surface layer however short crested wave breaking is generally neglected in these wave averaged studies or addressed in ad hoc manners and all real scale applications to date are performed using hydrostatic assumption thus underestimating horizontal vorticity motions nonhydrostatic dynamics are essential in our third listed process of surf eddy generation they are mostly studied in laboratory experiment nadaoka et al 1989 and laboratory scale large eddy simulations les using 2 5d cfd models applied to individual wave breaking lin and liu 1998 li and dalrymple 1998 watanabe and saeki 1999 watanabe et al 2005 lubin and glockner 2015 these previous studies show that the spanwise mostly alongshore component of vorticity is an important aspect of the breaking process surface breaking produces traveling rolls through a primary instability which can evolve through secondary instability to produce streamwise vorticity transitioning toward fully tridimensional turbulence however 2 5d cfd models are computationally very expensive and applied to individual breaking waves with only few alongshore wavelengths of the secondary instability precluding any evaluation of eddy statistics in addition these studies do not always clearly distinguish whether the instability is associated with the instantaneous plunging and rebounding jet produced by breakers or with the mean shear flow caused by momentum transfer yet the two processes may be sorted by their timescale i e smaller than the wave period for the rebounding jet watanabe et al 2005 and longer for the mean shear turbulence li and dalrymple 1998 if confirmed the latter could therefore be an intermediate phenomenon between breaker induced turbulence and large scale surf eddies 3d nonhydrostatic processes are usually studied independently of the two others by separate research communities and rarely compared in terms of scales magnitude and interaction the only attempt was made by splinter and slinn 2003 in a proceeding report using a 3d nonhydrostatic model where breaking acceleration is introduced as a body force they show that a simulation with deep breaking reproduces 2d model solutions while the more realistic shallow breaking process seems to disrupt the formation of horizontal shear instability at the expense of vertical shear instability however their domain size does not allow statistical comparisons and the profile of breaking acceleration is imposed not computed from a wave resolving model the present study is a step forward compared with this first work also addressing the case of short crested wave generation note that kumar and feddersen 2017 studied transient eddies produced by a 3d nearshore circulation model forced by short crested waves computed beforehand with a boussinesq model however wave forcing is prescribed as a depth uniform body force i e as deep breaking and could not produce vertical shear of the cross shore flow their hydrostatic assumption also precluded the model from vertical shear instabilities croco coastal and regional ocean community model is a new oceanic modeling system built upon roms shchepetkin and mcwilliams 2005 debreu et al 2012 with an added non boussinesq kernel auclair et al 2018 it solves reynolds averaged navier stokes equations rans on a free surface and terrain following grid and is designed to study realistic fine scale processes from the regional ocean to the littoral zone particular attention is paid to numerical accuracy high performance computing optimization scalability portability and ease of access www croco ocean org this paper presents a quick overview of the nonhydrostatic croco solver with a non boussinesq compressible approach before embarking in its application to nearshore dynamics first its ability to simulate the propagation of surface gravity waves near shore breaking and the resulting vertical circulation is validated against small and large scale laboratory experiments second we present a 3d wave resolving real case simulation of transient rips in the presence or not of short crested waves and strong alongshore currents we discuss fundamental differences in the generation of surf eddies by 3d wave resolving models compared with depth averaged models with a focus on the vertical structure of currents produced by shallow breaking and associated turbulence we conclude on the limitation of simplified vorticity evolution equations in which only the vertical part is considered when so much activity resides in the horizontal vorticity governed by 3d non hydrostatic equations 2 model description because of limited computational resources 3d wave resolving models are still rarely used to study nearshore dynamics in realistic environments les applications appeared in the 1990s and are generally restricted to 2 5d laboratory scale experiments of individual wave breaking early applications used the volume of fluid vof method for free surface tracking e g lin and liu 1998 watanabe and saeki 1999 watanabe et al 2005 derakhti and kirby 2014 larsen et al 2020 this model type with cartesian coordinate where the free surface crosses computational cells arbitrarily fails to precisely apply the pressure boundary condition on the free surface affecting the model accuracy more recently several 3d wave resolving free surface and terrain following rans models have emerged for the nearshore zone e g swash zijlema et al 2011 and nhwave ma et al 2012 derakhti et al 2016 based on earlier attempts e g lin and li 2002 in this case the explicit overturning of the free surface is excluded and the breaking wave is modeled instead with a single valued free surface which follows a shock process and resembles a dissipating bore despite the absence of explicit overturning replaced by parametrized turbulence these models can be accurate as well as computationally efficient orders of magnitude cheaper in the study of waves and wave driven mean and transient circulation croco belongs to this class of models but unlike other attempts resolves the compressible navier stokes equations auclair et al 2018 a compressible approach preserves the hyperbolic nature of navier stokes equations and does not require a global elliptic solver with incremental pressure corrections to ensure the incompressible mass balance as a result it avoids splitting errors between pressure and velocity and approximations made on free surface conditions zijlema et al 2011 derakhti et al 2016 thereby preserving amplitude and nonlinear dispersive properties of surface waves at the same time the absence of global computations by an elliptic solver makes parallelization and optimization procedures much more efficient the cost of solving acoustic waves is managed with a time splitting technique and semi implicit time discretization introduced below the development of croco around the regional oceanic modeling system roms has advantages for realistic applications it benefits from capabilities long developed in oceanic models high performance computing high order discretization coupling with biogeochemistry and sediment models pre processing tools for rapid generation of model input various online and offline diagnostics the nonhydrostatic model version can thus be applied without much effort to realistic highly nonlinear regimes e g large internal solitons and hydraulic jumps hilt et al 2020 kelvin helmholtz instabilities penney et al 2020 langmuir turbulence herman et al 2020 or wave induced nearshore circulation as in the present study it is naturally suited for bridging ocean and coastal sciences e g addressing surf shelf exchange processes in a 3d rotating and stratified framework in addition both wave resolving and wave averaged uchiyama et al 2010 marchesiello et al 2015 model equations are available within the same code which has potential advantages for evaluating the parametrizations of wave current interactions in wave averaged models 2 1 free surface compressible ocean model equations the full set of navier stokes equations for a free surface ocean is explicitly integrated in the nonhydrostatic non boussinesq compressible version of croco built on the code structure of roms primitive equation solver in the compressible approach auclair et al 2018 acoustic waves are solved explicitly to avoid boussinesq degeneracy which inevitably leads to a 3d poisson system in nonhydrostatic incompressible methods detrimental to computational costs and accuracy of free surface model implementation non boussinesq equations include the momentum and continuity equations the surface kinematic relation for free surface heat salt or other tracer c conservation equations and the equation of state which reads in cartesian coordinates 1 ρ u t ρ v u ρ f v ρ f w p x f u d u λ v x 2 ρ v t ρ v v ρ f u p y f v d v λ v y 3 ρ w t ρ v w ρ f u p z ρ g f w d w λ v z 4 ρ t ρ v 5 η t w f z η v z η η 6 ρ c t ρ v c f c d c u v w are the x y z components of vector velocity v η is the free surface p the total pressure ρ the density f x y and f x y are the traditional and non traditional coriolis parameters function of latitude g is acceleration of gravity d u d v d c are eddy diffusion terms requiring second moment turbulence closure models f u f v f c are forcing terms λ is the second bulk viscosity associated with compressibility used to damp acoustic waves 2 2 time splitting principle in the above set of equations a relation between ρ and p is required to that end and as part of a time splitting approach density is decomposed into slow and fast components based on a first order linear decomposition with respect to total pressure in the following s and f subscripts refer to slow and fast mode components respectively 7 ρ ρ s t s p s ρ p t s δ p ρ f c s 2 p f o δ p 2 8 p p a t m z η ρ s ρ 0 g d z s l o w ρ 0 g η z δ p p f f a s t c s is the speed of sound and δ p p f is the nonhydrostatic pressure the navier stokes equations are then integrated with two different time steps within the time splitting approach inherited from roms the slow mode integration is similar to roms with the addition of the slow part of vertical momentum equation while fast mode integration is in 3d and includes the compressible terms of momentum and continuity equations in vector form 9 ρ v t ρ v v 2 ρ ω v z η f ρ s ρ 0 g d z f v d v s l o w ρ 0 g η f p f ρ f g λ v f a s t 10 ρ f t ρ s t ρ v 11 p f c s 2 ρ f 12 η f t w f z η v f z η η f 13 ρ c s t ρ v c s f c d c 14 ρ s ρ t s s s η f 15 ρ ρ s ρ f the momentum is integrated both in slow and fast modes but the right hand side of the equation is split into two parts a slow part made of slowly varying terms advection coriolis force baroclinic pressure force and viscous dissipation and a fast part made of fast varying terms the surface induced and compressible pressure force weight and dissipation associated with bulk viscosity numerical integration is therefore done twice once with a large time step keeping the fast part constant and once with a smaller time step keeping the slow part constant this is much more computationally efficient than integrating the whole set of equations at the same fast time step more details can be found in auclair et al 2018 1 1 auclair et al 2018 presented a first implementation of the compressible approach involving a 3 level time splitting internal external and acoustic croco was simplified to only retain a slow and a fast time level where acoustic waves are solved together with the external depth averaged mode this procedure is more computationally efficient note that acoustic waves can become pseudo acoustic if their phase speed c s is artificially reduced c s is a model parameter in this case high frequency processes associated with bulk compressibility may be unphysical but an accurate solution for slower nonhydrostatic dynamics can be preserved while relaxing cfl constraints in our nearshore applications a c s value of 200 m s instead of 1500 m s makes almost no difference for the physical solution but allows a great reduction in the computation time by almost half 2 3 discretized equations for nearshore applications in this study motions are produced by an offshore wave maker in a non rotating homogeneous fluid in this case the coriolis force baroclinic pressure force and all surface fluxes are null there is no temperature or salinity stratification so that slow density ρ s is constant in time and space croco is discretized on a c grid with finite difference methods for slow and fast modes that are detailed elsewhere shchepetkin and mcwilliams 2005 soufflet et al 2016 in short the slow mode time stepping algorithm is a leapfrog adams moulton predictor corrector scheme that is third order accurate for integrating advective terms the fast mode is integrated with a generalized forward backward scheme which is also third order accurate vertical flux terms that do not require accuracy vertical diffusion term in the slow mode and all acoustic terms of w equation in the fast mode are computed with an implicit time stepping to increase computational stability horizontal and vertical advection terms are discretized using the weno5 z improved version of the 5th order weighted essentially non oscillatory scheme borges et al 2008 which is popular for hyperbolic problems containing both shocks and smooth structures weno5 z naturally copes with dispersive numerical modes as well as shocks caused by breaking waves with no need for ad hoc criteria 2 4 turbulence closure along with the numerical treatment of breaking waves a k ϵ or k ω model solving the closure equations for turbulent kinetic energy k and dissipation ϵ or dissipation rate ω ϵ k 1 is used as part of a generic length scale gls method warner et al 2005 in the absence of buoyancy forcing the turbulence equations express a balance between transport diffusion shear production and dissipation 16 ρ k t ρ v k d k ρ p ϵ 17 ρ ϵ t ρ v ϵ d ϵ ρ ϵ k c ϵ 1 p c ϵ 2 ϵ or 18 ρ ω t ρ v ω d ω ρ ω k c ω 1 p c ω 2 ϵ the eddy viscosity ν t c μ l k 1 2 is derived from these equations with coefficient c μ dependent on stability functions and mixing length l k 3 2 ϵ 1 l is resolution independent which is consistent with a rans rather than les approach the shear production term for k is p 2 ν t s i j s i j with the mean strain rate tensor s i j 1 2 u i x j u j x i using einstein notation all turbulence model parameters are given in warner et al 2005 based on burchard et al 1998 for k ϵ and wilcox 1988 for k ω the only present modification in the k ϵ model concerns the surface mixing length a model boundary condition which is briefly discussed in the validation section 3 2 for this reason and for its robustness through resolutions and benchmarks the k ω model will be our standard turbulence model however we note as mayer and madsen 2000 and larsen and fuhrman 2018 that this model tends to produce excessive mixing in potential flow regions i e on the innershelf this problem will be addressed in further studies including realistic conditions of stratification and wind forcing 2 5 wave maker at offshore boundary the wave maker forces a spectrum of 3d linear waves at the offshore boundary as in zijlema et al 2011 rather than as an interior source term wei et al 1999 the spectrum has frequency and directional spreading similar to feddersen et al 2011 19 η b c y t i a i j d j cos k y i j y ω i t ϕ i j 20 with d j e θ j θ m σ θ 2 and d j 1 21 u b c y z t η b c y t ω p cos θ m cosh k p z h sinh k p h 22 v b c y z t η b c y t ω p sin θ m cosh k p z h sinh k p h where x y z are cross shore alongshore and vertical directions respectively i j are indices of spectral distribution in frequency and direction respectively a i is the amplitude at each frequency ω i from a given statistical distribution e g jonswap section 3 2 and section 4 1 k y i j k i sin θ j is the alongshore wavenumber where k i is the linear theory wavenumber ω i 2 g k i tanh k i h with h the mean water depth θ j is wave angle θ m is the mean wave direction and σ θ the directional spread around the mean ω p and k p are peak frequency and wavenumber d j is a normalized frequency dependent directional distribution ϕ i j is a uniformly distributed random phase here w b c is set to zero and our tests show only weak sensitivity to this choice depth averaged barotropic velocities u v must be provided as well in the wave maker because they are prognostic variables of our split explicit model advanced together with the fast acoustic mode normal depth averaged velocity u is complemented at the boundary by an anti stokes compensation flow opposite to stokes drift and thus closing the volume budget we do not impose the depth averaged value of u b c directly but the value of the incoming characteristic of the shallow water system as in flather type conditions marchesiello et al 2001 blayo and debreu 2005 23 u u b c g h η η b c this allows infragravity waves generated inside the domain to propagate out as long waves while ensuring a near conservation of mass and energy through the open boundary likewise the baroclinic components u b c v b c w b c are applied via an adaptive radiation condition which helps short waves and 3d flow perturbations to leave the domain with only a small effect on the interior solution marchesiello et al 2001 3 validation in flume experiments 3 1 globex experiment as a first step toward 3d modeling we present here a validation of wave propagation and breaking using a wave flume experiment the gently sloping beach experiments globex 2 2 globex data is freely available at zenodo org record 4009405 were performed in the scheldt flume of deltares delft the netherlands in 2012 and described in michallet et al 2014 the project objective was to collect high resolution space time data of the cross shore evolution of short and infragravity waves on a gentle slope for a range of wave conditions the flume is 110 m long 1 m wide and 1 2 m high the waves were generated with a piston type wave maker equipped to minimize reflections from the wave paddle a concrete beach with a weakly reflexive 1 80 slope was constructed with its toe at 16 57 m from the wave maker all experiments were run with a still water depth of 0 85 m and shoreline at x 84 57 m the material that was laying loose on the concrete bed before the flume was filled with water had a median grain size d50 0 75 mm sea surface elevation measurements were taken at 190 locations repeating an experiment ten times while relocating the 21 wave gauges together with velocity measurements at 43 locations mostly but not always at 1 cm above bed to focus on the undertow the sampling frequency of the instruments during these experiments is 128 hz here we focus on experiment b3 corresponding to second order stokes wave generation of bichromatic frequencies simulated with a boussinesq model in michallet et al 2014 the characteristics are as follows a 1 0 09 m a 2 0 01 m f 1 0 420 hz f 2 0 462 hz short wave peak period t p 2 f 1 f 2 2 27 s and group period t g 1 f 2 f 1 23 81 s the signal had a total duration of 75 min the model is set up with the same conditions as the wave flume experiment second order bichromatic waves are generated at the offshore boundary with shore normal direction and zero directional spread a no slip condition is imposed on the lateral wall boundaries of the canal so that transverse modes are precluded the grid spacing is d x 1 cm with 10 vertical levels evenly spaced between the free surface and bottom a simulation with 20 levels gave similar results while the solution is moderately degraded mostly in higher moments with coarser horizontal resolution d x 3 6 and 12 cm which shows good convergence properties the model time step is d t 0 15 ms the minimum depth is 1 mm on the shore the position of which varies with the swash oscillation relying on a wetting drying scheme warner et al 2013 for bottom drag the logarithmic law of the wall is used with roughness length z 0 d 50 12 0 0625 mm fig 1 compares an hovmuller plot x time of data and model sea level η and undertow u b the model u b is interpolated at the measurement depth when some data is missing in the measurements it is also removed from the model output the general structure reflecting wave speed and frequencies wave packets surf and swash zones are all very similar model data correlations are high with 0 85 and 0 80 respectively for η and u b and root mean square errors are 2 7 cm and 12 2 cm s some scattering in the undertow data is noticeable according to michallet et al 2014 it may be attributed to the presence of secondary motions generated by transverse waves at the break point where the transverse mode 1 seiche can be excited at frequency f 1 f 2 it may also be due to variations in the depth level of flow measurements a snapshot of wave field across the flume during runup fig 2 highlights the main processes of propagation nonlinear wave wave interactions shoaling breaking roller propagation and runup model data correlation is high as already mentioned and non linearity is apparent in both cases in the increasingly non sinusoidal shape of short waves as they approach the shore elgar and guza 1985 they first develop short high wave crests with increasing skewness asymmetry about the horizontal axis measuring crest trough shape and as they break transition into the characteristic saw tooth shape with asymmetry about the vertical axis wave statistics first second and third moments for η and u b are shown in fig 3 mean standard deviation or h s for η skewness ϕ 3 ϕ 2 1 5 and asymmetry h ϕ 3 ϕ 2 1 5 h is hilbert transform the model sea level statistics left of fig 3 closely resemble the measurement data including high order moments showing the transition from skewness to asymmetry across the shoaling and surf zones with two peaks in the asymmetry profile corresponding to outer and inner surf zone evolutions for the mean field of measured undertow a few scattered points lying far outside the standard deviation are corrected using a polynomial fit the model appears to replicate the observed cross shore undertow profile top right of fig 3 the undertow is part of a vertical recirculation associated with breaking induced surface onshore flow here we call undertow the bottom return flow 10 cm s in this experiment which includes the eulerian anti stokes compensation flow preserving lagrangian flow continuity the latter is the only undertow component captured by depth averaged models and is relatively weak on the order of 1 cm s in this simulation in fig 3 the model also correctly represents high order moments which show profiles similar to those of the sea level skewness and asymmetry from sensitivity tests it appears that a realistic reproduction of h s cross shore evolution in the surfzone benefits from using a shock capturing scheme weno5 z the results are degraded not shown when replacing weno5 z with up5 a non monotonic linear upstream biased 5th order advection scheme menesguen et al 2018 this is in line with analogies between breaking waves bores and hydraulic jumps that may be treated as a shock cienfuegos et al 2010 tissier et al 2012 lubin and chanson 2017 the preservation of steep wave fronts by shock capturing schemes can help generate asymmetry even to the excess depending on resolution tissier et al 2012 however this is balanced here by additional eddy viscosity from the turbulence model which is otherwise required below wave trough level to capture the right vertical shear as shown next 3 2 large scale lip flume experiment the undertow may be considered as a proxy for vertical shear which will appear as an essential parameter of surf eddy generation in the next section to further confirm the model s ability to simulate this shear we now present a comparison with the european large installation plan lip experiments designed for profile validation and carried out at full scale in delft hydraulics s delta flume roelvink and reniers 1995 it will also be a test for our numerical wave maker in its ability to generate a spectrum of random waves the flume is 225 m long 5 m wide and 7 m deep in lip three types of experiments were designed with different wave conditions which subsequently resulted in a stable a erosive b and accretive c beach state here we use the erosive experiment lip11d 1b the wave conditions were a jonswap narrow banded random wave spectrum generated by a wave paddle with characteristics h s 1 4 m t p 5 s and peak enhancement factor γ 3 3 under this wave forcing a sandbar formed and slowly migrated across the initial beach profile of slope 1 30 consisting of a median grain size of 0 22 mm z 0 d 50 12 0 02 mm a movable carriage was placed 10 cm above the bed to capture the depth varying structure of the currents at 10 locations along the flume with a given accuracy of 2 cm s we use measurements taken after 8 h in experiment 1b and averaged over one hour the model setup is adapted from the globex experiment to the lip experiment in particular a jonswap wave spectrum similar to the experiment is generated with shore normal direction and zero directional spread the grid spacing is d x 25 cm with 10 vertical levels evenly spaced between the free surface and bottom here again a simulation with 20 levels gave similar results and a test of coarser horizontal resolutions d x 50 cm and 1 m is presented below the model time step is d t 25 ms fig 4 shows a comparison of the model with data using our standard configuration the match with measured currents is very good throughout the complex morphology of the beach the waves start to break before the sandbar but the breaking is more intense on the sandbar where the surf is strongest on the onshore side the resulting undertow has a strong shear and maximum intensity of about 30 cm s the resolution test 25 cm 50 cm and 1 m shows a mean error of about 3 cm s at all resolution close to the measurement error of 2 cm s the results are thus consistent at all resolutions despite no adjustment of any parameter with the small scale globex experiment it confirms the validity of a rans approach for estimating the mixing length of breaking induced turbulence which will be distinguished from mean shear induced turbulence in the next section turbulent kinetic energy and eddy viscosity estimated by the k ω model in the breaker zone have the expected structure fig 4 top and magnitude ν t 0 01 h g h svendsen 1987 cox et al 1994 interestingly the transport terms in the closure equations tend to reduce mixing at break point by redistributing the turbulent energy thus allowing a more intense shear to be maintained not shown the k ϵ model works almost as well as the k ω model with respect to mean current profiles but the comparison is improved by imposing a high value on the surface mixing length 0 2 m as in wave averaged models feddersen and trowbridge 2005 kumar et al 2012 the k ω model may thus be a better choice for surface wave breaking possibly due to a more accurate near wall treatment mayer and madsen 2000 devolder et al 2018 larsen et al 2020 however as mentioned this model produces a greater amount of mixing in potential flow regions outside the surf zone mainly due to the divergence part of the mean strain rate tensor mayer and madsen 2000 while an extensive study of surfzone turbulence is beyond the scope of this paper we conclude that our combination of numerical and physical closures with off the box parameters although perfectible provides a realistic and robust framework for the horizontal and the vertical circulation in the surfzone 4 natural beach application we now turn to a full 3d experiment with longshore uniform bathymetry the configuration is derived from grand popo beach 6 2 n 1 7 e benin in the gulf of guinea fig 5 this stretch of coast presents a longshore uniform low tide terrace and steep upper shoreface almar et al 2014 2016 and a sandy wave dominated and microtidal environment exposed to s sw long period swells generated at high latitudes in the south atlantic almar et al 2015a a field experiment was conducted at grand popo beach from 10 to 18 march 2014 almar et al 2014 derian and almar 2017 for our setup we focus on conditions in the middle of the afternoon of march 13 2014 the weather tides and wave conditions were ideal weak winds and wind waves well separated from a narrow band swell with significant wave height h s 1 15 m peak period t p 11 s and wave incidence d 10 from shore normal direction measured from an acoustic doppler current profiler moored in 10 m depth the water was at mid neap tide level low tide terrace at about 1 m depth promoting a narrow surfzone less than 50 m wide a nortek high frequency acoustic doppler velocimeter adv with sampling rate of 8 hz was deployed in the surfzone in the middle of the terrace measuring currents about 0 5 m from the bottom a dye release was conducted to monitor the dispersion induced by flash rips coupled with uav flights stb ds6 hexacopter at an elevation of 100 m the drone camera nikon d700 was looking down with a vertical angle and recorded 4256 2832 px scenes at 1 hz almar et al 2014 derian and almar 2017 4 1 model setup the model parameters are summarized in table 1 the domain is 542 m alongshore by 240 m across shore with periodic alongshore boundary conditions in order to prevent distortion when oblique waves are used with periodic conditions the alongshore size is adjusted according to peak wavelength and mean wave direction this method proved to perform well even with long crested waves the grid spacing is d x d y 0 5 m there are 10 vertical levels evenly spaced between the free surface and bottom the model time step is d t 0 01 s the bathymetry is longshore uniform and built with continuous functions to smoothly fit the low tide terrace structure observed during the survey of almar et al 2014 2018 the depth h is 8 m offshore to 1 mm at shore level the position of which varies with the swash oscillation wetting drying scheme of warner et al 2013 the depth on the terrace is about 1 m which corresponds to the mid tide conditions of the afternoon of march 13 2014 the wave maker is set with following parameters h s 1 15 m t p 11 s d 0 or 10 and directional spread σ θ 30 a jonswap spectrum is constructed with these parameters and a peak enhancement factor γ of 3 3 the weno5 z scheme is used again with the k ω turbulence model bottom roughness is z 0 0 01 mm which may seem low but gives a drag coefficient c d 0 002 in the surfzone c d κ log z 1 z 0 2 with κ 0 41 and z 1 h 10 the first level height above bed a usual value in depth averaged models chen et al 2003 feddersen et al 2011 we follow the practice of these models here to reproduce their results within the pseudo 2d approach described below it is of little consequence for 3d simulations because as will be shown they are much less sensitive to bottom drag than 2d models section 5 the model is run for an hour starting from rest and adjusting through a rapid spin up phase fig 6 presents a snapshot of sea level that shows realistic features short crested waves generated at an angle refract and break producing rollers swash and some reflection croco comes with capabilities for water quality marine ecosystem and sediment modeling in the present study they are used with simple settings first we introduce a passive tracer in the swash zone for comparison with dye releases made during the beach survey second a suspended sediment model blaas et al 2007 warner et al 2008 allows the rip patterns to be compared with aerial photos taken during the survey we use a single fine sand class with settling velocity of 1 cm s for resuspension taking one sediment bed layer for simplicity only two parameters are needed critical shear stress and erosion rate at the seafloor expressed in the erosion flux blaas et al 2007 24 e e 0 1 p τ b τ c r τ c r for τ b τ c r τ b is the bottom shear stress computed by the model e 0 is an empirical erosion rate set to 10 5 kg m2 s p is the sediment porosity 0 41 τ c r is the critical shear stress i e the threshold for initiation of sediment motion set to 0 01n m2 4 2 shallow vs deep breaking and a boussinesq model the peregrine vorticity generation process only requires short crested waves with no need for unstable longshore currents generated by oblique waves boussinesq models are very efficient in this process but an important question for us is whether a 3d model will remain so an essential difference between the two types of model is the depth penetration of wave breaking in a 2d boussinesq model deep breaking is implicitly assumed as momentum is transferred instantaneously to the depth averaged flow however this is a rough assumption as the breaking induced flow is produced essentially above trough level where the onshore flow is located while turbulence generated at the surface spreads downward by diffusion with a limited mixing length of 10 to 30 percent of the water depth svendsen 1987 cox et al 1994 mocke 2001 longo et al 2002 uchiyama et al 2010 show that the deep breaking assumption is inconsistent with the cross shore velocity profiles measured during duck94 splinter and slinn 2003 also suggest that transient dynamics produced by deep breaking may collapse in the more realistic case of shallow breaking the results presented here are in agreement with these studies and we propose to assess the role of 3d dynamics on surf eddies by comparing simulations forced by shallow and deep breaking deep breaking will constitute a pseudo 2d model whose results can be compared with a boussinesq model solution section 4 5 boussinesq type models see barthelemy 2004 for a review are common tools to simulate weakly dispersive waves and their transformations from the ocean to the swash zone several developments allowed their application to a wide range of scales from surfzone processes to ocean basin scale tsunami propagation kirby 2016 here for a verification of our pseudo 2d croco version we use funwave tvd shi et al 2012 it solves the fully nonlinear boussinesq equations using a hybrid finite volume finite difference scheme parametrizations are similar to croco with wave breaking handled by a shock capturing tvd scheme making the need of explicit criterion unnecessary and a quadratic drag formulation with c d 0 002 for bottom friction note that newer generation models than funwave are available with better dispersive properties using green naghdi equations lannes and bonneton 2009 or incorporating an additional enstrophy equation kazakova and richard 2019 but funwave is widely used and share with this class of models the essential depth averaged assumption see table 2 to force deep breaking in a 3d wave resolving model we can artificially enforce strong deep vertical mixing around the breaker zone by multiplying the vertical eddy viscosity by 10 from values of ν t 0 01 0 05 m 2 s fig 7 shows a time and longshore average of cross shore and alongshore currents in the case of shallow and deep breaking shallow breaking is computed by the 3d model with no explicit constrain on penetration scale but a parametrization of eddy viscosity induced by breakers and currents it drives a shallow onshore flow about 40 cm deep and an offshore near bottom undertow resulting in strong vertical shear of about 1 s 1 with artificially strong vertical viscosity momentum in the breaker zone is almost instantly mixed to the bottom and the cross shore flow is reduced to the part required by mass conservation the anti stokes compensation flow and consistent with depth averaged models the longshore flow driven by oblique waves and a number of other simulations with shallow and deep breaking table 2 will be analyzed and compared in the following sections 4 3 reference simulations and comparison with data to introduce the 3d processes of flash rip generation we present simulations with shallow and deep breaking 3d sc d10 and 2d sc d10 representing mid tides conditions on march 13 2014 in grand popo beach fig 8 compares the vertical vorticity of surface flow ω z v s x u s y phase averaged over two peak periods 22 s with deep breaking left panel the vortical field is rich with large filaments and surf eddies of 50 100 m scale that are generated from short crested waves similar to boussinesq model solutions however the full 3d model with shallow breaking offers a radically new solution fig 8 right panel some of the large scale fluctuations are present but over shadowed by shorter scales this mode presents itself as rib structures or mini rips following a relevant observation by short et al 1993 already mentioned with short longshore wavelength of about 5 m and period about 1 min we now compare the two simulations with data collected during the survey of march 2014 flash rips did not appear particularly large to the survey team at high or mid tide but a higher frequency signal was present and interpreted as swash rips castelle et al 2014 scott et al 2018 floc h et al 2018 dye experiments revealed filament generation but of relatively short scales although the survey was not extensive enough to draw definite conclusions the adv data as well as the released dye and suspended sediment patterns suggest a dynamical regime closer to 3d shallow breaking than pseudo 2d deep breaking simulations as will be seen the dye experiment presented in fig 9 illustrates both the structure of the alongshore flow and scales of flash rips emerging from the surfzone careful analysis of observed versus modeled dye evolution as in hally rosendahl and feddersen 2016 is beyond the scope of this paper but useful information can be gained from a simpler analysis fig 9 presents two consecutive aerial photos at 116s interval and the corresponding snapshots of tracer simulations with the full 3d model we do not expect an exact match between observed and modeled rips considering the chaotic nature of these phenomena 3 3 during the survey a few attempts of dye release were made before obtaining a clear filament patch similarly for the model we selected one occurrence among few tracer patches initialized at regular interval along the coast in the swash zone even though all tracer patches eventually ended up with similar v shape and similar scales of evolution there was variability in the evolution and we selected the most visually comparable filament with drone photos but scales and structures are meaningful in both cases a thin filament of about 5 m expands quickly seaward at a speed of about 0 5 m s reaching about 70 m from shore using sequential photos of the tracer from the drone camera it is also possible to extract a simplified cross shore profile of longshore drift velocities see derian and almar 2017 for more extensive lagrangian calculations the result is presented in fig 10 together with the adv measurement of mean longshore current over the terrace and model solutions with deep and shallow breaking the estimated longshore flow has an asymmetric v shape similar to the full 3d model solution with a peak velocity of about 0 5 m s in the inner surfzone and error bar of about 0 1 m s derian and almar 2017 deep breaking solutions have a more symmetric profile centered in the outer surfzone the profiles in both simulations are a result of cross shore advection with deep breaking advection is weak and the longshore flow remains centered in the breaker zone fig 7 this is a common bias of depth averaged models larson et al 2002 another qualitative comparison of patterns can be made looking at surfzone suspended sediments in the aerial photo fig 11 left panel the contrasts in the photo are enhanced to better expose suspended sediments brown color which is seen weakly extending beyond the surfzone snapshots of the model s surface sediment concentration are also shown after 15 min of simulation with shallow breaking center panel sediments tend to resuspend in the breaker zone and mix efficiently within the surf zone but only weakly extend to the innershelf the rib structure is apparent at the seaward front of sediment concentration it is also apparent in white streaks representing alongshore surface current convergence that have a structure similar to the foam lines in the aerial photo overall the patterns are similar to the observations particularly in the upper part of the photo where there is less foam or sunglint the same suspended sediment simulation with deep breaking gives very different results fig 11 right panel resuspension is now maximum in the inner surfzone as for eddy energy section 4 5 filaments and eddies are more coherent larger their growth slower but extend further seaward mixing in the surfzone is less efficient than for the shallow breaking case but shelf surf exchange is more intense due to filament extension for a more quantitative local comparison we now turn to adv measurements of horizontal velocities u h u 2 v 2 fig 12 presents the power spectral density psd of velocity fluctuations using welch s noise reduction method for the model and adv data in the middle of the terrace left panel and for the model alone over the outer terrace slope right panel the short wave spectrum around the peak period 11 s is well represented given the jonswap approximation made for the model wave maker at a lower frequency a good fit with the data is also given by the full 3d solution while the deep breaking simulation exhibits two opposite biases in successive frequency ranges valid at 95 confidence level which are even more pronounced near the terrace slope these biases are consistent with those noted in feddersen et al 2011 the first is an underestimation of energy by the pseudo 2d model in the 30 100s period range this band is consistent with visual inspection of rip structure oscillation in animated vorticity fields the 3d model seems to correct the deficit particularly in the lower frequencies note that this energy range for 3d eddies overlaps that of infragravity waves making it difficult to separate the two phenomena from observations alone at very low frequency vlf for timescales between 2 and 15 min a second bias of opposite sign is observed in the pseudo 2d model solution in this range the eddies produced by short crested waves have more energy in the deep breaking case than in the 3d case as predicted by inspection of vorticity and suspended sediment the comparison with the data therefore suggests that the vlf energy is overestimated by depth averaged models consistent with feddersen et al 2011 and newberger and allen 2007 we conclude from this section that the observations at grand popo beach are in better agreement with a complete 3d solution of surf eddies that includes the presence of 3d rib structures next we analyze their generation process 4 4 structure and production of vertical shear instability the surfzone eddy variability seen in the 3d model solution is truly three dimensional vertical vorticity is only one manifestation but horizontal vorticity is the main player fig 13 presents the q field defined by q 1 2 u i x j u j x i using einstein summation convention over the three dimensions q is commonly used to enhance detection of vortical flows here we split cross shore and alongshore q components q y u z w x 1 2 u x 2 in red and q x v z w y 1 2 v y 2 in green then normalize them and only plot positive isosurface values 0 02 for clarity negative values give counter rotating features the result is strikingly consistent with instabilities of a transitional mixing layer metcalfe et al 1987 lesieur 1990 the transition being constrained by surfzone width q y shows spanwise rolls created from the primary instability while q x identify streamwise ribs that are transverse counter rotating vortices from secondary instability assumingly growing from perturbations generated between the rolls in the braid region note that streamwise designates the shear direction which is cross shore rather than the oblique wave direction the reason is that the bottom flow aligns with the surface flow in the alongshore direction forming only a weak mean vertical shear fig 7 however the rips can take an oblique direction when advected by the mean longshore flow they extend seaward beyond the surfzone while stretching in the vertical direction filaments of vertical vorticity or q z not shown also have similar rib structures to q x recognizable in the surface vorticity plot of fig 8 the mean shear flow is composed of the wave mean surface onshore flow and associated seaward undertow fig 7 the inflected velocity profile is inviscidly unstable to small perturbations and unstable modes of kelvin helmholtz type can emerge the spanwise rolls are large vertical eddies that rapidly evolve into transverse streamwise rib vortices connected by braid regions and stretched seaward and downward this picture is reminiscent of descriptions based on measurements and simulations at the laboratory scale nadaoka et al 1989 watanabe et al 2005 lubin and glockner 2015 but here the instabilities are generated by the wave mean shear flow rather than by direct breaking breaking induced turbulence is parametrized it is therefore more consistent with the instability of the undertow profile described by li and dalrymple 1998 according to linear stability analyses the wavelength of primary shear instability setting the distance between spanwise rolls is an order of magnitude larger than the mixing layer width δ in free shear layers michalke 1964 1965 wavelength frequency and growth rate of the most unstable modes are 14 δ 0 015 u δ and 0 1 u δ respectively the secondary instability wavelength is of the same order as that of the primary instability 2 3 in pierrehumbert and widnall 1982 if the mixing layer width is taken as the vorticity thickness δ δ u u z m a x 50 cm then the wavelength of both roll and rib structures is about 5 m consistent with our simulation note that given an effective resolution of 5 10 d x for croco soufflet et al 2016 a grid resolution of 0 5 m can be considered eddy resolving for 3d instabilities under current conditions as for frequency the mixing layer size would be associated with modes around 0 015 hz 60 s period a range usually reserved to surf beat the model with 3d instability has energy in this range that is lacking in the deep breaking case see previous section and fig 12 but we will see that nonlinear interactions can spread this energy around the injection scale section 4 6 note that the ribs sometimes develop localized pairing which may be evidence of subharmonic resonance another instability associated with shear layers craik 1971 pierrehumbert and widnall 1982 herbert 1983 in order to isolate the mechanism of eddy mean flow interaction we analyze a solution forced with monochromatic shore normal long crested waves 3d mono d0 in table 2 this simulation has constant wave forcing in both space and time when averaging over the wave period 11 s in this case the same rib structure is generated fig 14 but without the large scale alongshore variation seen in the full solution fig 8 right panel a comparison of this simulation with other test cases will be presented in the next section here we analyze the mechanism and patterns of shear production the eke source terms u i u j u i x j represent the energy spent by the mean flow to feed the instability leading to rolls and ribs the largest of these terms is the vertical shear production u w u z metcalfe et al 1987 which is shown in fig 15 there is a lesser contribution from cross shore convergence u u u x all other 7 combinations are negligible the main site of shear production is in the breaker zone fig 15 and the maximum values are located at the inflection point in the mean velocity profile represented by a magenta line in agreement with mixing layer instability theory the figure also shows a cross section of mean eddy kinetic energy e k e 1 2 u 2 v 2 w 2 where u v w are fluctuations of phase averaged velocities with respect to the time mean flow presented in fig 7 eke and shear production have a similar spatial pattern although high eke values extend from the production center in all directions the primary rolls are thus produced in the outer surfzone but turbulent energy is diffused by the mean and eddy flow across the water depth and toward both the inner surfzone and innershelf eke transport is stronger at the surface and streamwise filaments extend farther offshore at the surface than at the bottom despite some amount of vertical stretching as they leave the terrace for comparison the mean subgrid scale turbulent kinetic energy tke produced by the k ω closure equations is also shown in fig 15 bottom eke and tke have a similar structure while eke amplitude is about a quarter of that of tke the mean shear turbulence intensity is thus a significant part of total 3d turbulence generated by breaking waves we expect that part of tke parametrized by the closure model could be transferred to eke if a higher resolution was used 4 5 short crested waves and the peregrine process an important question of our study concerns the effect of wave variations frequency and directional spreading on flash rip generation in a full 3d model to address this question it is useful to simplify the problem and progressively add the multiple conditions of variability in this section we analyze shore normal short crested wave simulations with shallow or deep breaking 3d sc d0 or 2d sc d0 shore normal conditions prevent the formation of a longshore current and associated horizontal shear instability in addition we look at long crested wave solutions to isolate the effect of 3d instabilities i e the monochromatic solution 3d mono d0 presented in the previous section and a similar case with jonswap frequency spectrum 3d lc d0 this latter comparison will help evaluate the effect of frequency spreading on eddy variability before addressing the effect of directional spreading fig 16 compares vertical vorticity for shore normal short crested wave cases as for oblique waves deep breaking leads to a rich vortical field with large filaments extending far offshore similarly to boussinesq models this is confirmed here with a comparison between funwave tvd and pseudo 2d croco applied to the same configuration a difference between boussinesq and pseudo 2d solutions is the effect of 3d dynamics over the innershelf in the latter case where surface intensified offshore eddies and filaments present a more fragmented aspect due to a forward energy cascade uchiyama et al 2017 mcwilliams et al 2018 however the full 3d nonhydrostatic model with shallow breaking fig 16 right panel shows again different patterns from both boussinesq and pseudo 2d solutions with regular rib structures having a shorter alongshore scale and a more limited cross shore extension fig 17 presents eke cross sections for all shore normal wave experiments a striking element of these figures is the presence of large surface and bottom eke in the shallow breaking cases this pattern is not a result of shear production because it is absent from the monochromatic case see previous section and fig 15 therefore it can only result from wave groups associated with frequency spreading through wave height modulation wave groups produce variability in the surface onshore flow and associated undertow the variability amounts to about half the integrated mean eke however it is much smaller in the deep breaking case consistent with depth averaged model results de schipper et al 2014 we now turn to the effect of directional spreading in fig 17 short crested waves produced by directional spreading extend eke production over a wider surf zone than long crested waves where eke is confined to the breaker zone however the seaward extension is significantly larger in the pseudo 2d model confirming the impression made from vorticity inspection further confirmation is given by vertical eke integration and normalization by mean depth fig 18 it highlights 3 distinct regions the inner and outer surfzones and innershelf the top panel presents unfiltered data in this case 3d instability and wave group forcing dominate eddy production in the surfzone the deep breaking solution has a larger cross shore expansion extending out to the innershelf where eke levels are twice as high as in the shallow breaking case this is even clearer using a low pass filter on velocity fluctuations removing a large part of variability from 3d instability and wave group forcing bottom panel of fig 18 what remains is closer to the usual definition of surfzone eddies as very low frequency features there is now a maximum in the inner surfzone consistent with findings from previous boussinesq model studies that filaments forced by short crested waves originate in the inner surfzone then spread offshore forming eddies that grow in scale johnson and pattiaratchi 2006 however the difference of eke profiles between deep and shallow breaking cases is reminiscent of the overestimation of shelf surf exchange by boussinesq models see profiles of dye concentration in fig 10 of hally rosendahl and feddersen 2016 4 6 2d and 3d surfzone turbulent cascades if there is energy produced by short crested waves in the inner surfzone of the shallow breaking case the question is why does it produce fewer large filaments than the deep breaking case we found an answer in computing the spectrum energy flux in 2d turbulence the flux of energy is negative and small fluctuations can grow into larger coherent structures this process is usually involved to explain the growth of filaments and eddies from variable wave forcing e g johnson and pattiaratchi 2006 feddersen 2014 an inverse cascade in the surfzone has recently been confirmed by observations elgar and raubenheimer 2020 but how efficient is it exactly to answer this question we performed a wavenumber spectral flux analysis for pseudo 2d and 3d simulations 2d sc d0 and 3d sc d0 fig 19 the spectral flux is computed as in marchesiello et al 2011 by spectral integration of v advection term consistently with 2d turbulence there is a strong inverse cascade of kinetic energy negative flux in the pseudo 2d model starting from the scale of injection corresponding to short crested wave forcing wavelength of 30 m here and there is no direct cascade toward smaller scales in the 3d case the turbulent regime is different the negative flux of energy produced by variable wave forcing is present but significantly reduced in addition there is a second injection at smaller scales that corresponds to the most unstable mode of 3d instability wavelength of about 5 m this small scale energy travels both backward and forward across the spectrum and thus widens the range of variability associated with 3d instability in the 3d long crested wave case 3d lc d0 a similar spectral flux is produced at small scales but there is no large scale inverse cascade due to missing injection by short crested waves this analysis confirms that the growth of filaments and eddies associated with the peregrine process heavily relies on a 2d inverse cascade but this cascade is impaired by 3d dynamics in this case vorticity fluctuations generated by waves with finite alongshore extent produce less coherent horizontal structures than in the 2d paradigm built on depth averaged models johnson and pattiaratchi 2006 the extent of this inhibition process depends on the intensity of turbulent mixing sensitivity analysis using reduced viscosity shows that lower viscosity leads to a reduction of the 2d inverse cascade at vlf but to an increase of energy production and fluxes at smaller scales the opposite is true when increasing viscosity therefore breaking waves can transfer energy into both 2d and 3d transient circulations but the distribution of energy between the two regimes is regulated by turbulent mixing which occurs at a higher frequency these regimes are not mutually exclusive at least within some range of turbulence intensity the realism of which still needs to be better evaluated 4 7 oblique waves and horizontal shear instability the precedent experiments with shore normal waves show that shallow breaking tends to hinder the generation of large 2d eddies by short crested waves while sustaining forced and intrinsic 3d surfzone eddy dynamics we now address the case of oblique waves with direction d 10 from linear stability analysis of a 2d problem bowen and holman 1989 the wavelength frequency and growth rate of the most unstable shear waves are λ h 2 5 l and f h 0 07 v l σ h 0 15 v l where v is the longshore current magnitude and l is the longshore current half width outer shear for a narrow shoreline intensified jet typical of grand popo at mid tide almar et al 2014 2015b shear can be strong 0 05 s 1 implying a minimum shear wave period of 5 min wavelength of 80 m and growth time σ h 1 3 min when forcing long crested waves σ θ 0 with deep breaking 2d lc d10 croco recovers results that are typical of wave averaged shallow water models or 3d models with deep breaking as in kumar and feddersen 2017 fig 20 left panel shows an active horizontal shear instability producing shear waves with wavelength consistent with the linear theory shear waves propagate with the longshore current as they become nonlinearly unstable generating filaments and eddies that extend offshore when both horizontal shear instability and short crested wave vorticity generation are active with deep breaking eddies and filaments are more prominent 3rd panel of fig 20 however with shallow breaking the horizontal shear instability appears weaker and is again replaced by rib structures both horizontal processes are thus reduced by the vertical shear inspection of the cross shore profile of mean longshore currents fig 7 may help to understand how shallow breaking undermines horizontal shear instability with deep breaking cross shore advection is inactive and the longshore current remains trapped over the terrace slope which is steep in grand popo around 1 10 and its outer shear is strong with shallow breaking however the longshore current is advected by the cross shore circulation stretching its profile across the terrace so as to minimizes the outer shear then the instability growth rate becomes too weak to overcome friction from turbulence or bottom drag this may explain for example the puzzled observation by newberger and allen 2007 that their 3d wave averaged model produced no horizontal shear instability unlike many previous 2d modeling studies allen et al 1996 slinn et al 1998 uchiyama et al 2009 this inhibitory process is verified in fig 21 showing cross shore profiles of the mean and eddy flow averaged over time and the alongshore direction for the four cases of oblique waves horizontal shear instability is best assessed with the term for horizontal shear production of turbulent kinetic energy u v v x in all cases the shear production is clearly correlated with the outer and inner slopes of the mean longshore current with higher production in the outer shear deep breaking cases show higher shear production and greater eke centered on the outer slope of the terrace where the shear is greatest in the case of horizontal shear instability alone 2d lc d10 the magnitude is lower than that of 2d sc d10 despite similar shear intensity this indicates an amplification of shear instability by short crested waves as they drive transient intensification of longshore currents in this case also the eke maximum extends farther offshore than expected from shear production possibly due to mean and eddy advection but short crested waves provide the most efficient process for innershelf eddy activity the shallow breaking cases 3d lc d10 and 3d sc d10 also feature horizontal shear production but weaker and in shallower water where friction is more prevalent as a result eke is significantly reduced short crested waves 3d sc d10 appear to amplify the inner surfzone energy compared with 3d lc d10 but in both cases the offshore energy is considerably reduced 5 discussion and conclusion flash rips and surfzone eddies are traditionally conceived within a depth averaged framework that involves intrinsic horizontal shear instabilities or and direct short crested wave vorticity generation they are revisited in this study using a 3d nonhydrostatic wave resolving model applied to a natural beach with ideal longshore uniform topography grand popo beach benin we first presented a quick overview of a new free surface compressible approach adapted to wave resolved nearshore dynamics its ability to simulate surface gravity wave propagation nearshore breaking and the resulting circulation is validated against small and large scale laboratory experiments then the model is applied to the nearshore circulation generated at grand popo beach by waves with frequency and directional spreading we assume on the basis of the comparison with boussinesq solutions that the essential difference between 2d and 3d models is reduced to the vertical profile of breaking induced acceleration i e deep or shallow breaking this allows a direct comparison of 2d and 3d frameworks within the same model equations and setup the generation of transient rips by the 3d model is shown to differ from that produced by depth averaged models owing to the vertical structure of currents produced by surface intensified acceleration processes of both horizontal shear instability and short crested wave breaking are limited in our 3d model by the cross shore vertical recirculation which can restrict an otherwise strong inverse cascade variable wave forcing in space and time tends to increase flow variability in the surfzone especially at the surface and bottom but it does not fully translate into large scale rips streaming far offshore usual 2d mechanisms may thus be weaker than expected but complemented by a kelvin helmholtz type instability generated at the inflection point of the mean vertical shear flow the latter generates rib structures with spanwise and streamwise alongshore and cross shore vorticity of intermediate scale between turbulence and large horizontal eddies timescale several times the wave period encroaching on the infragravity wave range and wavelength around 5 m streamwise filaments extend beyond the surfzone but with lower intensity than usual vlf rips the offshore mean eke can be halved comforted by observed energy spectra and patterns of tracer and sediment concentrations at grand popo beach our study may call into question the accuracy of nearshore depth averaged models it may explain in particular the evidence of overestimation by these models of the shelf surf exchange spydell and feddersen 2009 hally rosendahl and feddersen 2016 or vlf variability feddersen et al 2011 our results are representative of mid tide conditions of a low tide terrace with moderate wave heights interestingly the rib structures that are described here are comparable to the mini rips described for similar conditions by short et al 1993 under typical mid tide conditions with waves breaking across the bar a low friendly surf zone is produced waves are less than 1 m and most water appears to head toward the shore in fact it is return seaward also both by reflection of the beach face and via the mini rips even if no rip channel are present the rips however are usually weak ephemeral and shallow in order to assess the ubiquity of mini rips in the nearshore zone future 3d studies should explore different nearshore conditions several sensitivity tests were performed in this study which we only briefly report pending further exploration of the model parameter space in the future nevertheless they provide useful material for discussion we first tested the effect of wave amplitude as it affects the breaking induced flow and turbulence a simulation forced with twice as large waves h s from 1 15 to 2 30 m did not fundamentally change the results in case of higher waves breaking induced turbulence can reach deeper depths but the cross shore flow acceleration is also stronger so the result on vertical shear is uncertain next we tested a different beach profile from the steep slope of grand popo to a more gentle slope smaller iribarren number similar to duck beach as in noyes et al 2005 here again the results were similar as the intensity of the mean vertical shear does not appear to be too sensitive to the beach slope the bottom roughness length z 0 was another relevant parameter as z 0 is increased from 0 01 to 1 mm the drag coefficient goes from about 0 002 to 0 008 simulations with deep breaking were very sensitive to these values and the largest roughness value can completely shutdown horizontal shear instability due to shorter frictional time decreased from 5 to 2 min i e shorter than growth time of about 3 min and also damp vortical generation by short crested waves on the contrary full 3d solutions with shallow breaking are only weakly sensitive to bottom roughness because of surface intensified currents and shorter growth time of 3d instability these tests inspire greater confidence in our results while highlighting the overestimated importance given to bottom drag in studies using depth averaged models e g allen et al 1996 eddy viscosity provides the largest source of sensitivity in our results artificially changing the eddy viscosity coefficient lead to qualitatively similar results but with significant variation in the intensity of processes an increase of eddy viscosity in the surfzone reduces the vertical shear damps vertical shear instability as the frictional time h 2 ν t becomes closer to the growth time of about 1 min and intensifies the inverse turbulent cascade in the same time cross shore advection is reduced so that the wave averaged longshore flow is sharper and in deeper waters therefore more sensitive to horizontal shear instability therefore there is a relationship between the intensity of turbulence and sorting of energy that enters 2d and 3d dynamical regimes given the present model uncertainty we can only infer that these regimes are not mutually exclusive and should coexist more validation in different settings will be needed to assess their relative importance finally the present modeling study shed light on the mechanistic process that could inhibit the generation of surf eddies by short crested waves vertical vorticity generation by the peregrine process can be written as 25 ω z t f b r y c where f b r is the breaking force extending to the bottom in a depth averaged model and y c is the along crest direction using a parametrization for breaker acceleration clark et al 2012 propose a scaling relation for vorticity generation of a single wave as h s 3 h 2 5 at a maximum in the outer surfzone however flash rip generation originates in the inner surfzone in depth averaged models johnson and pattiaratchi 2006 as well as in our simulations therefore a transient vorticity source in the outer surfzone is not sufficient to generate the expected local horizontal recirculation with offshore filament it needs a coastal boundary and an inverse energy cascade that transform vorticity fluctuations into larger scale coherent structures as shown by our spectral flux analysis in a 3d regime this cascade can be reduced due to vortex tilting by the shear flow mcwilliams et al 2018 and the variability generated in the inner surfzone does not fully translate into large rips jetting offshore in conclusion our results suggest that nearshore dynamics and transport processes may be affected by nonhydrostatic dynamics not only for surface gravity waves and small scale turbulence as is well known but also for larger scale vortical motions we expect this conclusion to be qualitatively valid in other applications but further studies should explore the range of parameters encountered in the global coastal ocean most importantly they should pay special attention to how these parameters affect the mean cross shore current profiles as a key to 3d transient dynamics credit authorship contribution statement francis auclair conceptualization methodology software laurent debreu methodology software james mcwilliams conceptualization methodology rafael almar validation investigation rachid benshila software franck dumas project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research has received support from a consortium of french national research agency as part of croco s development project gdr croco and from the service hydrographique et oceanographique de la marine shom 20cp05 it was granted access to the hpc resources of calmip supercomputing center under allocation p19069 the fieldwork received support by the french insu ec2co program grand popo experiment we thank h michallet for sharing the globex data now freely available at zenodo org record 4009405 and dano roelvink for sharing the lip data apart from these all data were acquired by the authors and the croco source code is freely available at www croco ocean org both observational and modeling data are available upon request 
