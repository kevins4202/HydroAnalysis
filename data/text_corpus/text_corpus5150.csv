index,text
25750,this paper proposes a novel optimization approach for multi scenario multi objective robust decision making as well as an alternative way for scenario discovery and identifying vulnerable scenarios even before any solution generation to demonstrate and test the novel approach we use the classic shallow lake problem we compare the results obtained with the novel approach to those obtained with previously used approaches we show that the novel approach guarantees the feasibility and robust efficiency of the produced solutions under all selected scenarios while decreasing computation cost addresses the scenario dependency issues and enables the decision makers to explore the trade off between optimality feasibility in any selected scenario and robustness across a broader range of scenarios we also find that the lake problem is ill suited for reflecting trade offs in robust performance over the set of scenarios and pareto optimality in any specific scenario highlighting the need for novel benchmark problems to properly evaluate novel approaches keywords multi objective optimization scenario planning deep uncertainty robust decision making scalarizing functions reference points 1 introduction decision making in complex environmental problems typically involves several conflicting objectives to be considered simultaneously there is no single optimal solution for these multi objective problems because of the conflicting objectives instead several so called pareto optimal solutions reflecting different trade offs between the conflicting objectives can be found in such multi objective decision problems decision support tools can help decision makers in balancing between conflicting objectives the task of decision makers in environmental problems is further complicated by the presence of uncertainty to mitigate the potential negative consequences of uncertainty it has been argued that decisions should have limited sensitivity to the consequences of uncertainty so called robust decision and perform relatively well in a broad range of future states of the world or scenarios lempert et al 2006 in environmental systems the level of uncertainty is high and probabilities over the various alternative states of the world can only be approximately assessed this kind of uncertainty is sometimes also known as deep uncertainty bankes 2002 lempert et al 2003 kwakkel et al 2010 walker et al 2013 shavazipour and stewart 2019 uncertainties about future climate change and socio economic conditions are two examples of deep uncertainty in environmental problems therefore decision makers in complex environmental problems are facing a multi objective optimization problem to be solved in the presence of deep uncertainty where the performance of a decision should be evaluated according to all objectives in all plausible scenarios shavazipour and stewart 2019 shavazipour et al 2020 stewart et al 2013 this kind of decision problem is also known as a multi scenario multi objective decision making problem scenarios in this paper represent different plausible future realizations of the deep uncertainties maier et al 2016 in practice it is almost impossible to find a decision that is pareto optimal or even feasible in all plausible scenarios by a feasible solution in multi scenario multi objective optimization problems we mean a solution that is feasible i e satisfies all constraints in all scenarios accordingly decision makers seek robust solutions that are sufficiently good in a broad range of scenarios i e robust satisficing this introduces an additional trade off between pareto optimality and feasibility in any given scenario and robustness over a set of scenarios in this paper we refer to this as the trade off between scenarios recently different approaches have been introduced for environmental multi objective optimization problems under deep uncertainty such as many objective robust decision making mordm kasprzyk et al 2013 multi scenario mordm watson and kasprzyk 2017 and multi objective robust optimization moro hamarat et al 2014 kwakkel et al 2015 trindade et al 2017 all these approaches are based on the robust decision making rdm framework lempert et al 2006 groves and lempert 2007 rdm is an iterative approach where pre specified policy alternatives solutions are stress tested over a wide range of scenarios in order to determine conditions under which each solution fails to perform adequately then the policy alternatives are refined to find the most robust solution s in light of these failure conditions mordm was introduced as an extension to rdm to help in generating a promising set of candidate solutions as input to the stress testing for decision problems involving multiple objectives these solutions are produced using multi objective optimization given a single reference scenario i e only optimizing in the feasible region of a single scenario this inadvertently introduces scenario dependency of the generated solutions given that the pareto approximation only includes solutions optimized in and feasible for that single scenario which reduces both the robustness which can be attained during stress testing eker and kwakkel 2018 giudici et al 2020 bartholomew and kwakkel 2020 as well as the feasibility of the candidate solutions in other scenarios to reduce this shortcoming watson and kasprzyk 2017 proposed multi scenario mordm which repeats the process of identifying candidate solutions prior to stress testing for several scenarios expanding on this eker and kwakkel 2018 introduced a more systematic scenario selection procedure that ensures high diversity among the scenarios which are used for the identification of candidate solutions however solutions generated with multi scenario mordm are still highly dependent on the selected scenarios this is because multi scenario mordm does the search separately for few selected scenarios without checking the feasibility and performance of the solutions in the other scenarios during the optimization process therefore there is no guarantee that the solutions generated are feasible in any other scenario besides gathering solutions generated by single scenario optimizations cannot guarantee optimal robustness either of course the feasibility and the performance of the solutions will be checked later in the robustness analysis however many of the solutions found in that way may have inferior performance i e be dominated in some scenarios or even be infeasible which means wasting computational resources in finding poor solutions that will be eliminated later in the robustness analysis in contrast moro hamarat et al 2014 kwakkel et al 2015 trindade et al 2017 only concentrates on robustness by optimizing the robustness measures as objective functions over a set of scenarios however this simulation optimization approach is computationally demanding and possibly intractable even for small sets of scenarios eker and kwakkel 2018 bartholomew and kwakkel 2020 giudici et al 2020 furthermore utilizing different robustness measures results in different solutions this highlights the meta choice of selecting the most appropriate robustness measures which might require a separate study giudici et al 2020 kwakkel et al 2016 mcphail et al 2018 besides the existing trade offs between objectives in different scenarios cannot be explicitly verified in worst case min max and aggregation based robustness measures which are often used in moro for instance the overall robustness may be affected excessively because of poor performance in a few scenarios ben tal et al 2017 roos den hertog 2020 shavazipour and stewart 2019 shavazipour et al 2020 bartholomew and kwakkel 2020 compared mordm multi scenario mordm and moro and confirmed that the more robustness is considered in the search for candidate solutions prior to stress testing the more robust the solutions will be nevertheless there remains a trade off between optimality and feasibility in any given scenario and robustness over the set of scenarios this trade off is sometimes also known as the price of robustness bertsimas and sim 2004 schöbel and zhou kangas 2021 there is currently no approach for the search phase of mordm that enables decision makers to explore this trade off explicitly note that adding scenarios to a multi objective optimization problem adds dimensions to the problem indeed the resulting multi scenario multi objective optimization problem includes all objective scenario combinations in which the dimension of the space grows exponentially e g in a problem with four objectives and five scenarios the space becomes 4 5 20 dimensional this means that a solution may have an outstanding performance on one objective in one particular scenario but its performance on other objectives may be poor or the solution may even be infeasible in some scenarios the previous variants of mordm identify solutions in the objective space of a single scenario problem then they test the performance of these solutions on the uncertainty space constructed by an ensemble of random scenarios this may imply losing robust solutions as well as the chance of exploring the mentioned trade off in parallel to the continuous refinement of rdm via mordm multi scenario mordm and moro the concept of robustness in multi objective optimization has been receiving theoretical attention as well this has resulted in various novel theoretical concepts such as min max robustness ehrgott et al 2014 highly dranichak and wiecek 2019 flimsy bitran 1980 kuhn et al 2016 and lightly robust efficiency ide and schöbel 2016 regret robustness xidonas et al 2017 and multi scenario efficiency botte and schöbel 2019 shavazipour and stewart 2019 shavazipour et al 2020 we refer the interested readers to botte and schöbel 2019 ide and schöbel 2016 and schöbel and zhou kangas 2021 for a review and comparison of different theoretical robustness concepts in multi objective optimization although not all of these concepts and methods were primarily developed to deal with deep uncertainty still to some extent they can be utilized for this purpose as a complement to the existing approaches in decision making under deep uncertainty dmdu among these concepts multi scenario efficiency defined particularly for a discrete uncertainty space i e constructed with a finite number of scenarios is similar to the concept of robustness utilized in the deep uncertainty literature the main difference between these two bodies of literature in how they use robustness concepts lies in where they are evaluating the robustness of a candidate solution in mathematical optimization robustness is often utilized as an a priori criterion or soft constraint in searching for candidate solutions leading to a particular set of solutions following that criterion constraint i e we are only looking for robust efficient solutions in contrast in dmdu the robustness of solutions is typically an attribute of a generated solution measured in an a posteriori manner i e after the search phase as a result robustness in dmdu is used as an a posteriori measure for ranking already generated solutions in mathematical optimization all the robust efficient solutions are compromise solutions distinguishable by different trade offs between objective s in various scenarios as the central common assumption none of these two bodies of literature consider the probability of scenario occurrence in their definitions and models in a multi scenario multi objective decision making problem ideally candidate decisions are evaluated in terms of all objectives in all or at least a representative set of selected scenarios shavazipour and stewart 2019 shavazipour et al 2020 stewart et al 2013 this kind of an assessment helps identifying solutions that are not only feasible in all selected scenarios but also robust efficient that is the used approach should guarantees that there exists no other solution which is not worse on all objectives in all selected scenarios and is better on at least one objective in one scenario botte and schöbel 2019 shavazipour et al 2021 among the previously proposed methods developed to handle multiple objectives under deep uncertainty only moro can to some extent guarantee the robust efficiency of all generated solutions without any extra filtering in all selected scenarios e g by considering all scenario specific constraints within the optimization model the primary aim of this paper is to build a bridge between the literature on mathematical multi objective optimization which has a strong theoretical foundation and the robust decision making literature which has shown successful real world applications to the best of our knowledge this is the very first step in this regard by drawing on the theoretical developments in mathematical multi objective optimization we can address the issues of robust efficiency feasibility and the price of robustness which affect existing approaches for the search phase within the mordm framework in this paper we propose a novel multi scenario multi objective robust optimization approach called multi scenario moro hereinafter by incorporating uncertainties in the optimization phase and identify solutions that perform well in some selected scenarios in this way the performance of solutions in terms of all objectives in all selected scenarios are evaluated within a single optimization problem as a result the pareto optimal solutions for considered scenarios can be found which are not only feasible in all selected scenarios but also robust efficient if any feasible solution is available in other words we combine all single scenario multi objective optimization problems into a meta optimization problem and simultaneously consider the evaluation of the objective functions in multiple scenarios indeed our objective functions include all the objective scenario combinations called meta objective meta criteria stewart et al 2013 subject to constraints satisfaction in all considered scenarios the proposed multi scenario moro has both a lower computation cost and increased robustness consideration during the search process it generates less scenario dependent solutions for our proposed approach and likewise for other approaches for multi scenario mordm selected scenarios should reflect the system vulnerabilities and or the main decision maker s preferences giudici et al 2020 the classic shallow lake problem first introduced by carpenter et al 1999 has been very often used to demonstrate test and compare methodological developments for decision making under deep uncertainty kwakkel 2017 lempert and collins 2007 singh et al 2015 bartholomew and kwakkel 2020 eker and kwakkel 2018 quinn et al 2017 singh et al 2015 2015 ward et al 2015 it is a standard benchmark problem reflecting the required characteristics of real world environmental problems such as tipping points affected by deeply uncertain parameters and multiple conflicting objectives therefore we use the shallow lake problem to demonstrate our novel approach and compare it with existing approaches in brief the main contributions of this paper are 1 proposing a multi scenario moro approach which utilizes a different solution method from the mathematical multi objective optimization literature to produce candidate solutions that reduce the computational cost and also can guarantee pareto optimality 2 paving the way to explore the trade offs between scenario specific pareto optimality and robustness by considering different numbers of scenarios in the optimization model and 3 introducing a novel way of scenario analysis to determine vulnerable scenarios using ideal points best possible achievements on each objective in each scenario and information about feasible regions in various scenarios since the proposed scenario analysis does not need any prior knowledge of solutions and their robustness the decision makers can gain insight into the problem before solution determination the rest of the paper is organized as follows section 2 includes a brief description of multi scenario multi objective optimization problems and the solution method utilizing in this study as well as the proposed multi scenario moro approach the lake problem as our case study and the multi scenario formulation of it are described in section 3 in section 4 we illustrate more details about how the proposed multi scenario moro can be applied and compare the results with the state of the art methods in the literature finally we discuss the feasibility robustness and computational costs of multi scenario moro regarding the different number of scenario considerations in section 5 before concluding in section 6 2 methods 2 1 multi scenario multi objective optimization a multi scenario multi objective optimization problem msmop also called all in one or scenario based multi objective optimization problem with k 2 objective functions and s 2 scenarios can be formulated as follows shavazipour et al 2021 1 minimize f 1 p x f k p x p ω subject to x s r n where the scenario space set ω is constructed by s plausible scenarios and each scenario includes k objective functions objective functions in scenario p p 1 s are described by f ip i 1 k x x 1 x n t is a vector of n decision variables in the feasible region s in the decision space r n s r n defined by constraint functions z p f 1 p x f k p x t p 1 s called an objective vector is the image of a decision vector x in the objective space r k under the conditions of scenario p a decision vector x s is called pareto optimal also called non dominated in scenario p if under the conditions of scenario p there does not exists another x s such that for all i f ip x f ip x and f jp x f jp x for at least one index j the image of the set of pareto optimal decision vectors in the objective space is sometimes called a pareto front x s is weakly pareto optimal in scenario p if there does not exists another x s such that for all i f i p x f i p x a preferred solution refers to a pareto optimal solution satisfying decision maker s preferences in terms of all k s meta criteria for any two objective vectors z p z p r k in scenario p we say that z p dominates z p if and only if for all i z i p z i p and z p j z j p for at least one index j the best and the worst possible values for individual objectives in the pareto front are components of an ideal point z i d e a l z 11 i d e a l z k s i d e a l t and a nadir point z n a d i r z 11 n a d i r z k s n a d t respectively while ideal points can be simply calculated by solving a relevant single scenario single objective optimization problem computing nadir points is difficult in practice however their estimations can either be provided by the decision maker or approximated for instant through a pay off table see e g miettinen 1999 and references therein also z i p u t o z i p i d e a l ε i 1 k p 1 s are components of an objective vector z p u t o r n called a utopian objective vector in scenario p where ε 0 is a relatively small scalar it is strictly better than the ideal point 2 2 generating candidate solutions achievement scalarizing function over the years many different methods have been proposed to solve multi objective optimization problems the two most popular type of methods are 1 multiple criteria decision making mcdm e g chankong and haimes 1983 miettinen 1999 and 2 evolutionary multi objective optimization emo e g coello et al 2007 deb 2001 a major advantage of emo algorithms is that they generate a set of approximated pareto optimal solutions in a single run of the algorithm however emo algorithms tend to be inefficient when the number of objectives increases in contrast mcdm method guarantee pareto optimality have a strong theoretical foundation and no limitation regarding the number of objectives many mcdm methods transform the original problem into a single objective optimization problems using a so called scalarizing function considering the decision maker s preferences see e g miettinen 1999 for more information on different mcdm methods and miettinen and mäkelä 2002 ruiz et al 2009 for a comparison of various scalarizing functions so far all variants of mordm have utilized emo algorithms to the best of our knowledge mcdm methods have not yet been utilized within the mordm framework as also mentioned in kasprzyk et al 2013 the main reason is related to the use of a priori importance weights as one form of preference information because of some concerns about the accuracy of these weights before the decision maker observes a broader set of solutions and gains a better understanding about the non convexity continuity of the pareto front and potential non linear relations between variables parameters however on the one hand a priori methods are designed to be used in problems in which the decision maker has a good enough understanding of the problem interdependencies of the objectives and possible outcomes and is able or wishes to express his her expertise a priori on the other hand there are also other types of mcdm methods interactive and a posteriori methods see e g miettinen 1999 furthermore when the number of objectives grows emo algorithms cannot efficiently approximate the pareto front thus they cannot be used to solve msmops which usually have tens or hundreds of objectives to overcome this issue in this paper we utilize one of the most popular scalarizing functions i e an achievement scalarizing function wierzbicki 1986 more specifically we use the following achievement scalarizing function which includes an augmentation term to avoid weakly pareto optimal solutions wierzbicki 1986 2 min x max i 1 k w i f i x z i ε i 1 k w i f i x z i s t x s where w i i 1 k are the weights for normalization as preference information set by a decision maker z i i 1 k known as aspiration levels represent desirable objective function values the vector of k aspiration levels is called a reference point a reference point in the objective space can be feasible or infeasible in any case a scalarizing function like 2 can identify the closest pareto optimal solution to the given reference point accordingly utilizing different reference points tends to lead to different pareto optimal solutions however sometimes the same pareto optimal solution may be associated with multiple reference points the multiplier ε is a small positive number and ε i 1 k w i f i x z i is an augmentation term ensuring pareto optimality thus the optimal solution to problem 2 is a pareto optimal solution to the original multi objective optimization problem wierzbicki 1986 miettinen 1999 when we have multiple scenarios the above formulation has an additional dimension 3 min x max i 1 k p 1 s w i p f i p x z i p ε i 1 n p 1 s w i p f i p x z i p s t x s where w ip represents the weight for objective i in scenario p and z i p is the aspiration level for the ith objective function in the pth scenario solving the scalarized problem 3 provides a single optimal solution which is pareto optimal for the original problem 1 different solutions can be generated by using different reference points therefore in contrast to emo methods to produce a set of pareto optimal solutions one needs to repeatedly solve the optimization problem using different reference points nonetheless considering the decision maker s preferences reference points here not only gives rise to generating solutions that lie in the areas of interest to the decision makers but it can also confine the search area and thus reduce computational cost in general a decision maker may provide preferences a priori choose a preferred solution among the provided set of solutions in a posteriori method or be iteratively involved using an interactive approach miettinen 1999 achievement scalarizing functions can be applied in any of these three ways in this paper following all variants of rdm we utilize scalarizing functions in an a posteriori method therefore we need to predetermine several reference points to produce different pareto optimal solutions amongst various techniques that have been developed for setting the reference points we utilize the method introduced by mueller gritschneder et al 2009 2 3 the proposed multi scenario multi objective robust optimization approach mo rdm is an iterative approach for finding robust solution s it consists of four steps 1 model specification 2 solution identification 3 computational exploration i e re evaluation of candidate solutions in a broad range of plausible scenarios and 4 scenario discovery in which vulnerable scenarios are identified and this information can be used to modify the model and or generate new solutions this process continues until the decision maker is satisfied with a set of solution s lempert et al 2006 kasprzyk et al 2013 there exist three different approaches for identifying policy alternatives in the second step of mordm single scenario kasprzyk et al 2013 multiple single scenario eker and kwakkel 2018 watson and kasprzyk 2017 and robust optimization hamarat et al 2014 kwakkel et al 2015 trindade et al 2017 however they have shortcomings e g scenario dependency in the first two variants and inability to reflect the trade offs between scenarios in all three variants to overcome these weaknesses we propose a novel multi scenario multi objective robust optimization approach that simultaneously considers multiple objectives in multiple scenarios not an indirect aggregated value over a set of scenarios i e the proposed multi scenario moro approach performs the search in a combined multi scenario multi objective space in this way all the generated solutions are robust efficient in all selected scenarios which increases robustness and reduces scenario dependency indeed we propose to use the multi scenario multi objective optimization approach model 1 to generate solutions in the second step search phase of the mo rdm yet the proposed multi scenario moro involves four iterative steps portraying in fig 1 and detailed as follows 1 model specification determining the components of a decision making problem such as the decisions to be made decision variables certain and uncertain parameters and relations between them how to measure performance like objective functions in an optimization problem problem constraints etc 2 solution determination this step which is the main contribution of this study divides into three sub steps as also shown in fig 1 a scenario selection similar to the previous variants of multi scenario mordm we need to select a set of scenarios to be considered within the optimization this study follows the state of the art scenario selection method proposed by eker and kwakkel 2018 although any other approaches can be utilized the decision maker can set the number of scenarios to be considered in the optimization problem based on preferences computation cost complexity or other considerations b multi scenario multi objective optimization problem formulation in this step to identify the candidate solutions we formulate using the information specified in the previous steps and solve a multi scenario multi objective optimization problem that simultaneously considers multiple objectives and multiple scenarios within a single optimization problem of the form 1 by changing the number of scenarios considered in this problem the decision maker can explore the trade offs between all objectives in all selected scenarios the higher the number of scenarios considered within the optimization problem the more robust the identified solutions will be however increasing the number of scenarios considered within the problem can reduce the chance of feasibility and or optimality in any given scenario c solution process since the total number of objective scenario combinations meta objectives utilizing in msmop is often significantly high we utilize the scalarizing function 3 to generate pareto optimal solutions by solving it multiple times by any appropriate single objective solver incorporating different reference points 3 uncertainty robustness analysis in this study following the previous variants of mordm the solutions identified in step 2 are re evaluated over a wider range of plausible scenarios to assess their robustness and investigate the impacts of deep uncertainty on the objective functions 4 scenario discovery analysis scenario discovery methods aim at identifying sub spaces subsets in the uncertainty space set ω where candidate solutions perform poorly different algorithms have been developed for this purpose in the literature e g bryant and lempert 2010 dalal et al 2013 kwakkel and jaxa rozen 2016 however as an alternative to available scenario discovery methods we propose a novel method to determine vulnerable scenarios using ideal points and information about feasible regions in various scenarios to this end first we calculate the ideal points for all objective functions in all new randomly generated scenarios in the previous step note that these randomly generated scenarios may not be the same as the scenarios considered in the optimization problem thus we need to calculate the best possible values for each objective hereinafter called ideal values in each randomly generated scenario these ideal values are computed by solving the associated single scenario single objective optimization problem described in section 2 1 comparing the differences between the ideal values in various scenarios will help us identify vulnerable scenarios and the combinations of the deeply uncertain parameters causing the poor performances in these vulnerable scenarios this novel way of scenario analysis discovery will be illustrated in more detail through the case study in section 4 3 2 4 robustness measures and trade offs analysis following recent studies and for comparison purposes in this paper we use the mean standard deviation hamarat et al 2014 and the domain criterion starr 1963 to measure the robustness for each objective and avoid objectives aggregation to compare the robustness trade offs between the objectives the mean standard deviation measure is used to compare the results with eker and kwakkel 2018 while the domain criterion measure is utilized mostly for scenario analyzes and relevant discussions it is also used in comparison with the results of quinn et al 2017 these two robustness measures which are used in robustness analyses and scenario discovery not in optimization are briefly described in this section 2 4 1 mean standard deviation signal to noise ratio the mean standard deviation measure for solution j in objective function i representing by r ij can be formulated as follows eker and kwakkel 2018 4 r i j μ f i j p 1 σ f i j p 1 if f i is to be maximized p 1 s μ f i j p 1 σ f i j p 1 if f i is to be minimized p 1 s where μ is the mean over the set of scenarios for the ith objective function in the case of implementing the solution j and σ is the standard deviation 2 4 2 domain criterion the domain criterion is a satisficing robustness measure introduced by starr 1963 which directly applies the decision maker s preferences on minimum acceptable values for each objective function this measure mirrors the fraction percentage of all considered scenarios in which the minimum acceptable thresholds are met i e the percentage number of scenarios in which the solutions are meeting the criterion the robustness value lies between 0 and 1 where 1 shows that the given criterion is met in all scenarios for the relevant candidate solution and 0 means that the given criterion is not met in any scenario 3 case study the shallow lake problem to demonstrate the proposed multi scenario moro we use the shallow lake problem carpenter et al 1999 which is often used to demonstrate and benchmark methodological developments for decision making under deep uncertainty bartholomew and kwakkel 2020 eker and kwakkel 2018 hadka et al 2015 lempert and collins 2007 quinn et al 2017 singh et al 2015 ward et al 2015 in this stylized decision making problem a city is located next to a shallow lake anthropogenic pollution produced by the city goes into the lake if a eutrophication threshold is passed the lake irreversible transitions to a eutrophic state the decision problem is to decide on an annual pollution control strategy that gives rise to the highest economic benefits without passing a critical threshold to complicate the decision problem next to the controllable anthropogenic inflow there is also uncontrollable natural inflow phosphorus pollution levels can be calculated by the following dimensionless differential equation quinn et al 2017 5 p t 1 b p t 1 p t 1 q 1 p t 1 q x t 1 ζ t 1 where p represents the phosphorus level in the lake x describes the phosphorus anthropogenic pollution input ζ logn μ σ 2 refers to the natural pollution input t indicates the time period and b and q are the parameters of the lake model which control the rate at which pollution is lost from the lake and recycled from the sediment 3 1 objective functions following the literature on the shallow lake problem bartholomew and kwakkel 2020 eker and kwakkel 2018 hadka et al 2015 quinn et al 2017 ward et al 2015 we consider four conflicting objectives as follows 3 1 1 economic utility to be maximized the first objective function is the economic utility by releasing anthropogenic phosphorus pollution into the lake following quinn et al 2017 and ward et al 2015 the economic utility is computed as the average of the discounted benefit in n simulations of t years of random natural inflows in the first objective function 6 f 1 x 1 n n 1 n t 0 t 1 α x t n δ t α is an economic constant fixed at 0 04 δ is a discount rate and x tn is the decision variable describing the phosphorus pollution level that can be released in year t for the nth random natural inflows simulation realization 3 1 2 phosphorus pollution to be minimized minimizing the maximum average phosphorus level is considered as an environmental objective for water quality targets this objective function is naturally conflicting with the economic objective if p tn x represents the concentration of the phosphorus pollution in year t for the nth random natural inflows simulation the second objective function is 7 f 2 x m a x t 1 t 1 n n 1 n p t n 3 1 3 inertia to be maximized to avoid extremely rapid declines in phosphorus pollution in one year which needs a massive amount of investments in infrastructure and to control the maintain decision inertia the decision maker can set an annual reduction limit i limit on phosphorus pollution therefore in the third objective function 8 f 3 x 1 n t 1 n 1 n t 1 t 1 φ t n w h e r e φ t n 1 x t 1 n x t n i l i m i t 0 o t h e r w i s e the inertia of a decision is maximized inertia is defined as the average fraction of t 1 planning years over n random natural inflows simulations where inter annual pollution declines are lower than i limit of the maximum possible reduction in this paper following eker and kwakkel 2018 quinn et al 2017 ward et al 2015 i limit is set as 0 02 i e 20 of the maximum possible reduction 3 1 4 reliability to be maximized the last objective function 9 f 4 x 1 n t n 1 n t 1 t θ t n w h e r e θ t n 1 p t n p c r i t 0 p t n p c r i t also called an average reliability of a decision reflects the decision maker s desire in abstaining from the eutrophication of the lake which occurs if the concentration of the phosphorus in the lake passes a critical threshold p crit if the phosphorus level in the lake lies below p crit in a given period the reliability index θ tn is 1 and 0 otherwise thus maximizing the reliability means maximizing the periods out of t and across n simulations in which the phosphorus level in the lake stays below the critical threshold p crit 3 2 uncertainties and scenario selection two different degrees levels of uncertainty see e g shavazipour and stewart 2019 or kwakkel and walker 2010 for the definitions of various degrees or levels of uncertainty i e mild also called stochastic and deep are present in the shallow lake problem the mild uncertainty in natural pollution inflow ζ is handled by the average values of random samples generating by a log normal distribution the mean μ and standard deviation σ of the log normal distribution the discount factor δ the natural recycling rate q and the loss rate b are the deeply uncertain parameters in this problem following previous work bartholomew and kwakkel 2020 eker and kwakkel 2018 quinn et al 2017 these five deeply uncertain parameters and their ranges are shown in table 1 a combination of the values of these five deeply uncertain parameters sampled from their ranges generates a scenario from the scenario space ω for scenario selection we followed eker and kwakkel 2018 in selecting four more scenarios in addition to the baseline scenario and thus consider the same scenarios by utilizing the same values for deeply uncertain parameters constructing those four scenarios for more information about how these four scenarios were selected see eker and kwakkel 2018 section 4 1 pages 205 207 the last five columns of table 1 denote these five selected scenarios 3 3 multi scenario inter temporal open loop control formulation for the lake problem different variants of the shallow lake problem have been proposed in the literature the widely known ones are inter temporal open loop control eker and kwakkel 2018 hadka et al 2015 quinn et al 2017 singh et al 2015 ward et al 2015 direct policy search quinn et al 2017 and planned adaptive direct policy search bartholomew and kwakkel 2020 in this paper we use the often used inter temporal open loop control version in a multi scenario manner which includes t decision variables the optimization formulation for this multi scenario inter temporal open loop control version of the lake problem is 10 minimize f 1 p x f 2 p x f 3 p x f 4 p x p ω s t 0 0001 x t 0 1 f o r a l l t where x x 0 x 1 x t 1 is a vector of decision variables t indicates the length of the planning horizon x t represents the amount of phosphorus pollution to be released in year t which is limited to 0 1 as before f ip refers to objective function i i 1 2 3 4 in scenario p and ω is the scenario space 4 results in this section we illustrate the proposed multi scenario moro step by step following the steps shown in fig 1 describes in section 2 3 to assess the efficacy of the novel approach we compare the results with those of the previous studies 4 1 steps 1 and 2 problem setting scenario selection and generating candidate solutions we consider the formulation of the lake problem 10 with four objective functions in five scenarios for t 100 years and n 100 random realizations of the natural inflows we follow the selection approach of eker and kwakkel 2018 and thus use the same scenarios these five scenarios are presented in table 1 therefore to generate solutions we need to solve a multi scenario multi objective optimization problem with 100 decision variables and 20 objective functions 5 scenarios 4 objectives per scenario based on the above mentioned settings and the estimated worst possible values of the objective functions 6 9 the nadir points for the second objective pollution were set as 15 in all five scenarios and for the other three objectives in all selected scenarios 0 ideal points presenting in table 2 were calculated by solving the relevant single scenario single objective optimization problems for each objective function in each considered scenario the utopian values were calculated by adding for objectives to be maximized or subtracting for objective to be minimized a small scalar of 0 0001 to from the ideal points an ideal point represents the optimal performance that can be reached for each objective in a given scenario for instance as shown in table 2 the best possible performance for the first objective utility in the fourth scenario was 0 581 which is about two third of the maximum potential performance in the first three scenarios and around one third of the performance in the best case scenario s 5 these ideal values are represented the effect of deep uncertainty and highlight some of the problem s limitations even before identifying the solution candidate given this setup we use the achievement scalarizing function and solve the resulting optimization problem using the sequential least squares programming slsqp algorithm kraft 1994 available from the scipy module oliphant 2007 to generate different pareto optimal solutions we followed the mueller gritschneder et al 2009 method to pre specify with the 50 reference points and weights w i p 1 z i p n a d i r z i p u t o i 1 4 p 1 5 we get 50 different pareto optimal solutions for the original multi objective problem in practice the number of solutions is to be set by the decision maker here we chose 50 for comparison purposes as it is the number of solutions considered by eker and kwakkel 2018 in their robustness analysis note that the 50 solutions plotting in eker and kwakkel 2018 were the brushed solutions after all the filtering however a set of 50 or even fewer solutions generated by the proposed multi scenario moro can reasonably out perform these because of the simultaneous consideration of the five selected scenarios in multi scenario moro there is no need for further filtering in this way we can simultaneously save computational resources and improve the quality of the solutions we will return to this later in this section each poly line of the parallel coordinate plot in fig 2 represents the performance of a single solution on all four objectives in each of the five scenarios different colors in fig 2 distinguish the solutions in terms of their performance on the utility objective in the fifth scenario s 5 the higher the utility the lighter the color yellow in the colored version a higher utility leads to a higher pollution which results in lower reliability values the opposite is visible for solutions with higher reliability values shown with darker colors dark purple in the colored version which highlights the trade off between pollution reliability and utility another observation is that there is no significant trade off between scenarios in each objective particularly between the performances in utility and inertia in different scenarios for example the highest values for utility in different scenarios are from the same solution or inertia values in all five scenarios are similar in each solution visible by horizontal lines for inertia over the five scenarios in fig 2 this particular characteristic of the lake problem may prevent the explicit study of the trade offs between scenarios and cause some difficulty in robustness and trade off comparisons we return to this point in section 5 4 2 step 3 robustness analysis to re evaluate the 50 solutions and assess their robustness we generate an ensemble of 1000 randomly generated scenarios using latin hypercube sampling the 50 solutions are re evaluated over these scenarios to analyze and compare their robustness across a broader range of scenarios based on this ensemble of 1000 scenarios the robustness of the candidate solutions is determined using the domain criterion and the mean standard deviation 4 2 1 robustness trade offs with mean standard deviation in this section we use the mean standard deviation as the robustness measure to compare the results identified with multi scenario moro solution j in each objective function i using 4 fig 3 shows the mean standard robustness trade offs of the generated candidate solutions over the 1000 random scenarios the color code is similar to the previous figure i e the higher the utility robustness the lighter the color also conflicts between the robustness values in reliability and utility are vivid when the poly lines cross between the last two columns representing the robustness trade offs between these two objectives note that since we are minimizing pollution lower values of robustness are better for this objective thus the relevant column representing the robustness of pollution are inverted in the plot to unify the robustness improvement direction which is upwards lines higher up in the plot describe solutions with higher robustness on all objectives fig 4 compares the robustness trade offs of the solutions generated by the proposed multi scenario moro the solutions of eker and kwakkel 2018 and the solutions produced by quinn et al 2017 as seen in this figure the solutions generated by multi scenario moro result in a wider variety of robustness trade offs compared to the solutions produced by the other methods for example the maximum robustness value for utility in multi scenario mordm eker and kwakkel 2018 and mordm quinn et al 2017 were respectively 1 21 and 1 16 while almost a half of the solutions generated by multi scenario moro provided better values up to 1 51 similar patterns are valid for the robustness of all three other objectives moreover as mentioned the trade offs between reliability and utility are evident among the solutions produced by multi scenario moro these trade offs are hardly visible with the solutions of the other methods there are two reasons why the other methods could not find solutions with wider robustness trade offs 1 scenario dependency of their solutions since their search area has a lower dimension limiting the search to a hyperplane constructed by one scenario at a time for instance solutions with exceptionally low performance in one scenario i e dominated solutions in that scenario may have high performance in many other scenarios i e non dominated in many different scenarios these solutions are not identified as non dominated solutions when the search is confined to only a single scenario space 2 some of the solutions which represent wider trade offs may be eliminated from the final list after applying the reliability constraint quinn et al 2017 eker and kwakkel 2018 with a similar reason to the previous item 4 2 2 robustness trade offs with domain criterion the second robustness measure we use is the domain criterion the following criteria are considered based on previous studies using the lake problem quinn et al 2017 bartholomew and kwakkel 2020 1 utility 0 2 2 reliability 0 95 3 pollution critical point p crit 4 inertia 0 99 in practice these criteria would be set by a decision maker for each criterion from the above list we calculated the number of solutions meeting that criteria after re evaluation over the ensemble of 1000 randomly generated scenarios in section 4 2 then ranked and sorted them based on their robustness in that criterion over the rank sorted solutions the robustness scores on the following criteria are described in fig 5 fig 6 compares the distributions of the robustness for the 50 candidate solutions generated by multi scenario moro the 50 brushed solutions of eker and kwakkel 2018 and the 86 brushed solutions produced by quinn et al 2017 note that all the solutions are re evaluated over the same ensemble of 1000 randomly generated scenarios as seen in fig 6 b in 60 of the generated solutions by multi scenario moro and in 26 of the solutions of the multi scenario mordm eker and kwakkel 2018 the robustness values for utility were 1 meaning that the utility values met the domain criterion of 0 2 in all 1000 random scenarios for these solutions in contrast none of the solutions of the mordm quinn et al 2017 reached this value the utility robustness value of 1 the maximum value of the utility robustness gained by a solution of quinn et al 2017 was 62 9 the robustness value of 1 for inertia is observed in about 34 of the solutions generated by multi scenario moro while only one solution amongst the ones produced by the mordm quinn et al 2017 could obtain a similar value of robustness for inertia see fig 6 d all the solutions generated by the multi scenario mordm eker and kwakkel 2018 have the robustness value of 0 for inertia nevertheless no solution among the generated solutions by either approach provides the robustness value 1 for reliability and pollution fig 6 a and c the maximum robustness percentage of reliability and pollution among the solutions of multi scenario moro were 76 9 and 76 7 respectively corresponding values amongst the solutions of quinn et al 2017 were respectively 63 4 and 63 1 also the maximum robustness percentage of reliability and pollution obtained by the solutions of eker and kwakkel 2018 were 63 7 and 63 4 respectively this result is also in line with the results of a similar analysis in quinn et al 2017 and bartholomew and kwakkel 2020 in general two main reasons may cause these results first some parts of the pareto front are left unexplored i e the set of generated solutions is not diverse enough to cover the entire front second there may be no feasible solution in those scenarios that can satisfy the given criterion in the next section through scenario analysis we will show that there is no feasible solution meeting the domain criterion on reliability and pollution as can also be seen in fig 6 a and c in about 23 of the scenarios this means that in around 23 of the scenarios the reliability criterion cannot be satisfied the values for the deeply uncertain parameters of these scenarios are presented in fig 8 moreover comparing the left side plots a and c in fig 6 demonstrates a strong correlation between the robustness of reliability and pollution particularly amidst the solutions of multi scenario moro which is expected as minimum pollution values give rise to high reliability values this strong correlation is even more visible by tracing the straight lines between pollution and reliability in fig 5 apart from these insights in fig 5 we also see that the existing trade offs between reliability and utility are again visible the color codes are the same as fig 3 i e the higher the utility robustness the lighter the colors comparing the robustness of the solutions generated by multi scenario moro and the ones produced by the two other methods i e mordm quinn et al 2017 and multi scenario mordm eker and kwakkel 2018 with the domain criterion measure as portrayed in fig 7 confirms the superiority of multi scenario moro for instance multi scenario moro identifies pareto optimal solutions that provide a broader range of robustness trade offs they also help decision makers to gain more insights into the problem than the previous variants of mordm in the next section we investigate in more detail the feasibility of the domain criteria through scenario analysis furthermore we analyze vulnerable scenarios to identify the combinations of deeply uncertain parameters causing poor performance in those scenarios 4 3 step 4 scenario analysis discovery to check the feasibility of meeting the domain criterion for each objective function in any scenario first we calculate the ideal points for all four objective functions in all 1000 scenarios effectively we are searching for the best possible values for each objective under the conditions of each scenario i e what is the best that could happen in every scenario the ideal points can be calculated by solving a single scenario single objective problem using for each objective function in each scenario this required solving 4 1000 4000 problems for our case study however the total computation time for solving all these problems is less than a couple of hours on a personal laptop and we only need to calculate the ideal points once indeed the ideal point calculations are related to the best worst case discovery halim et al 2016 table 3 represents the minimum and maximum values for each objective function among the components of the ideal points across all 1000 scenarios i e the best possible values for each objective in the best and worst case scenario describing the best and the worst performances for each objective as seen in this table the corresponding ideal values for reliability in some or at least in one scenarios are very close to 0 min 0 04 this means that in those scenarios there is no feasible solution even in the feasible region of the single scenario single objective problem with a reliability higher than 0 04 which is far less than the domain criterion for reliability at 0 95 similarly the ideal values for pollution in some or at least one scenarios are more than 10 which is also far more than the maximum values of the critical points in any scenarios i e 0 9165 counting the scenarios with the reliability of less than 0 95 indicates 231 1000 scenarios which confirms the claim that no feasible solutions meet the reliability criterion reliability 0 95 in about 23 of the scenarios fig 8 shows the combination of deeply uncertain parameter values listed in table 1 leading to poor performance in the ensemble of 1000 scenarios each dot represents a scenario orange dots correspond to scenarios where the reliability criterion is not met while blue dots belong to scenarios that met the reliability criterion reliability 0 95 similar to the results of the sensitivity analysis of quinn et al 2017 the first plot at the bottom left describes the area in which some nonlinear combination of small values of q and b results in poor performance on the reliability objective of course other uncertain parameters also have some impacts for instance higher values of mean natural pollution can also contribute to a failure on reliability even for higher values of q if b is not large enough overall it seems that for b 0 3 we rarely have a failure on the reliability objective giving the decision maker a new insight into the problem this point is more visible in fig 9 showing the vulnerable combinations of deeply uncertain parameters as seen in this figure only few failures were observable for b 0 3 and none for b 0 34 also no failure was recorded for δ 0 98 investigating the feasible region and ideal values for each objective across an ensemble of scenarios fosters the understanding of the behavior of deeply uncertain parameters in combination with each other the directed search kwakkel 2017 moallemi et al 2020 in some extreme areas of the uncertainty space provides us with detailed insights into the system dynamics in these areas because calculating the ideal values and the proposed scenario analysis do not need any prior knowledge about the solutions and their robustness this kind of analysis can be done even before solution generation thus one can get more insight into the problem and modify the model or preferences if needed before determining solutions potentially saving time and energy 5 discussion as mentioned in the introduction previous variants of mordm only considered a single scenario at a time in the search for the candidate solutions leaving moro aside therefore the feasibility of the generated solutions in a different scenario is questionable and in the best case the solutions are scenario dependent if not infeasible for example quinn et al 2017 added a hard constraint of reliability 0 85 to the intertemporal model of the lake problem in a reference scenario which is the same as s 5 in this study then they solved the four objective optimization model with the borg moea hadka and reed 2013 to generate candidate solutions that were subsequently stress tested re evaluated across 1000 scenarios for robustness analysis they used the domain criterion as their robustness measure and their second criterion was reliability 0 95 they showed that their generated solutions met this criterion only in around 60 of scenarios see figure 8 in quinn et al 2017 that is quite similar to the results described in figs 6 a and 7 however they could not find any failure mechanism on the reliability criterion the reason is that there exists no feasible solution with a reliability 0 95 in around 23 of the scenarios as described in section 4 3 identifying such a failure mechanism is almost impossible if only one scenario is considered in the search phase as another example bartholomew and kwakkel 2020 set the domain criterion of utility 0 75 let us set this criterion as a hard constraint in the model suppose we separately solve the lake problem with this constraint for each selected scenario in that case we cannot find any feasible solution for the optimization problem related to the fourth scenario in which the ideal values for the utility are less than 0 75 see table 2 utility has undesired values in more than 33 of the scenarios therefore if we generate solutions based on any other four scenarios none of the solutions can meet this constraint utility 0 75 in the fourth scenario i e they are infeasible in this scenario even though they are feasible in all other four scenarios accordingly some solutions identified by the previous variant of mordm which does not consider this scenario or some similar scenarios are infeasible in some scenarios in terms of satisfying the constraint of utility 0 75 in other words if the set of scenarios considering as part of the search phase of mordm does not contain scenarios causing a particular type of failure mechanism we cannot find solutions that can cope with this failure in general identifying solutions for some particular scenario cannot guarantee the feasibility of the solutions in any other scenario i e scenario dependent solutions may not be feasible in some other scenarios this feasibility robustness i e the solution is feasible in all or in a wide variety of scenarios is an essential factor that must be somehow checked or guaranteed in the search phase to the best of our knowledge this concept of robustness has not received much attention from the authors developing methods for dealing with deep uncertainty one reason for this may be the particular characteristics of the lake problem as the most popular benchmark problem for methodological developments for decision making under deep uncertainty it is weak in representing the trade offs between scenarios consequently the simultaneous consideration of multiple scenarios within the optimization problem like multi scenario moro helps in verifying the feasibility of the generated solutions in various scenarios during the search process however one cannot consider an infinite number of scenarios therefore it is vital to study the effects of the number of scenarios considered within the optimization problem and explore the trade offs between the overall robustness and optimality in any given scenario this investigation is performed in the next sections 5 1 effects of the number of scenarios in this section we study the effects of different numbers of scenarios within multi scenario moro again we consider the lake problem 10 with four objectives but with a different number of scenarios namely 1 5 9 and 50 scenarios these problems which respectively include 4 20 36 and 200 meta objectives are solved utilizing achievement scalarizing functions with 50 reference points to generate different pareto optimal solutions note that when we compare all generated solutions in a particular dimension of the scenario space e g 1d or single scenario comparisons in table 4 some solutions that were non dominated in the higher dimension may dominate in the lower dimensions and no longer lie in the pareto set these solutions are removed and therefore the number of solutions considered in comparisons may be less than fifty we compare the solutions found through the above mentioned models for the five scenarios described in table 1 in the model with nine scenarios in addition to these five scenarios four extra scenarios the same scenarios utilized by bartholomew and kwakkel 2020 table 2 page 127 are also considered moreover we consider 41 additional scenarios from the set of randomly generated scenarios in the model with 50 scenarios table 4 portrays the results of this comparison the number of scenarios utilizing in the optimization problem has been shown in column 1 while the second column shows the total number of solutions in the pareto set for each model in the 5d scenario space constructing by the first five scenarios the percentage of the solutions that remain in the pareto set when evaluated in each scenario is described in columns 3 to 7 we merge all the solutions found by all models multi scenario multi objective optimization problems with 1 5 9 and 50 scenarios identify the non dominated solutions in each scenario and classify them according to the models that generated them as seen in table 4 in general the percentage of the solutions that remain in the pareto set for any single scenario decreases by increasing the number of scenarios considered within the optimization problem displaying the price of robustness this observation is also in line with the results of bartholomew and kwakkel 2020 moreover the solutions generated by the five scenarios optimization problem perform relatively well especially in the first four scenarios in comparison with the results from the other optimization problems therefore there is not much loss in the optimality in the first four scenarios compared to the solutions generating by the other models in contrast the optimality loss the price of robustness is high in the model with 9 and 50 scenarios note that not all solutions to each single scenario optimization problem remain in the pareto set of that particular scenario mainly because of the stochastic nature of the natural flows in the lake problem this random variation of the natural flows causes some dominance issues in the non dominated sorting calculation i e multiple evaluations of a decision may give rise to some close but not the same values for the objective functions in fact some solutions are dominated because of the random values set by the model for the natural flows in each evaluation not because of the existence of any better solutions this issue also questions the suitability of the lake problem for robustness and trade off comparisons 5 2 robustness over the randomly generated scenarios the domain criterion robustness of the solutions generated by 50 9 5 and single scenario models is presented in fig 10 the larger the number of scenarios considered within the optimization problem especially for more than five scenarios the higher the robustness values on reliability pollution and inertia objectives after re evaluation this means that the optimization problem involving 50 scenarios largely dominates the solutions found for the 9 scenario optimization problem the inverse is observed for the utility objective it seems that by increasing the number of scenarios that is simultaneously considered within the optimization problem the solutions are increasingly biased towards higher reliability at the expense of utility the individual performance of the solutions found for the 5 scenario optimization formulation as shown in fig 10 with blue lines show a moderate behavior in all objective functions the robustness of the solutions produced by the 5 scenario formulation lies somewhere in the middle of the others demonstrating more balanced solutions however utilizing different reference points for each optimization formulation can generate various solutions that can completely change the story indeed the decision maker can steer the solution process towards the area of interest by providing relevant reference points therefore determining the reference points is an important step in multi scenario moro furthermore the trade off diversity between the objectives and the robustness ranges is not visible in fig 10 highlighting the need for a different visualization accordingly fig 11 is used to investigate these two issues as seen in fig 11 a similar pattern is observed across the different optimization formulations for both the domain criterion and the mean standard deviation robustness measure the robustness ranges are almost the same across formulations while the trade off diversity between objectives varies the main reason again is because of the set of chosen reference points to pre determine the reference points we use the method proposed by mueller gritschneder et al 2009 in which the extreme points together with some evenly distributed points on the convex hull of all extreme points are considered as the reference points to ensure diversity nonetheless in higher dimensions i e when we consider more scenarios in the optimization problem to cover the whole space one needs to generate more solutions compared to lower dimensional formulations for example the number of extreme points in the model with four objectives and 1 5 9 and 50 scenarios is 4 20 36 and 200 respectively therefore as we generate 50 solutions for each formulation we cannot cover the whole objective space in the higher dimensions particularly for the 50 scenario formulation this explanation justifies the extreme distribution of the robustness of the solutions generated by the 50 scenarios formulation the lack of diversity in the solutions produced by some single scenario formulations can be justified by the limited search area comparing to the optimization formulations for five and nine scenarios nevertheless once again the particular characteristics of the lake problem prevent further investigations and comparison in the robustness of the solutions generating by different models to sum up there is no significant difference in the robustness of the solutions generated by various models since there is no significant trade off between scenarios in each objective function in the lake problem because the feasible region in all scenarios stays the same any pareto optimal solution can be generated by any model if an appropriate reference point is set therefore determining the reference points and the number of solutions to be generated are more vital than the number of scenarios considered in the optimization model of the lake problem as a special case 5 3 computation cost another vital matter is the computational cost of multi scenario moro bartholomew and kwakkel 2020 giudici et al 2020 we examine the effects of considering more scenarios within the optimization phase of the proposed approach on computational cost the number of function evaluations nfe and the processing time for generating 50 solutions by multi scenario moro with 1 5 9 and 50 scenarios are described in table 5 all these models were solved 50 times once for each reference point in a laptop with intel core i7 cpu and 16 gb ram overall as expected the computation costs increased when the number of scenarios grew during the experiments we noticed that the number of function evaluations and or the time of evaluation varied from one reference point to another for example the most time consuming calculations were related to the reference points that directed the search into the area in which the utility objective is maximizing also the computational cost in most of the extremes was lower than the cost of identifying the more balanced solutions this is the reason why the computation cost in the 50 scenario model was lower than in some other models as discussed earlier in section 4 the solutions generated by multi scenario moro without any additional filtering have a similar robustness to the solutions produced by the previous variants of mordm after extra filtering the proposed multi scenario moro however identified these solutions with fewer function evaluations as seen in table 5 the number of function evaluations was less than 145 000 in s 5 the reference scenario which is much less than 200 000 function evaluations that were used by quinn et al 2017 in mordm moreover the mordm approach used in quinn et al 2017 can hardly generate more robust solutions if ever even with more function evaluations mainly because no information about other scenarios can be considered within its optimization model this is also true in the case of separate consideration of multiple scenarios as performed in multi scenario mordm eker and kwakkel 2018 in contrast by increasing the computational resources like nfe in the proposed multi scenario moro one can include more scenarios within the optimization model that boost the robustness of the generated solutions furthermore as mentioned above we ran the proposed multi scenario moro on a personal laptop which is significantly slower than the high performance computer resources often used to solve different variants of mordm e g as utilized in bartholomew and kwakkel 2020 therefore multi scenario moro is computationally more efficient than the previous approaches for the search phase of mordm while considering more scenarios within the optimization formulation and has other advantages such as representing a wider variety of robustness trade offs 6 conclusions in various disciplines there has been a growing interest in robust multi objective optimization this topic has in parallel been explored by researchers in both mathematical multi objective optimization and decision making under deep uncertainty the former focuses mostly on theory developments while the latter concentrates on practical problems we believe that integrating the developments of these two fields can address some current issues in robust multi objective optimization one of the widely used model based decision support frameworks in dmdu is many objective robust decision making mordm a critical step within the mordm framework is the search phase where candidate solutions are identified using multi objective optimization in this step typically one solves one or more single scenario multi objective optimization problems to produce a large set of promising solutions to be stress tested under uncertainty however this solution set might not be feasible or be dominated in other scenarios as an alternative others have proposed to optimize robustness directly but this leaves the trade off between optimality within individual scenarios and robustness over the scenario set unexplored to address these gaps in this paper we have proposed a new multi scenario multi objective robust optimization approach called multi scenario moro drawing on the concept of scalarizing functions from mathematical multi objective optimization in the novel approach the performance of solutions in terms of all objectives in all selected scenarios is evaluated within a single optimization problem therefore the generated solutions are feasible in all selected scenarios and robust efficient furthermore the proposed multi scenario moro enhances the robustness of the generated solutions reduces scenario dependency and produces a wider variety of robustness trade offs than the previous variants of mordm multi scenario moro also provides the opportunity of exploring trade offs between optimality feasibility in any given scenario and robustness over a broader range of scenarios by considering different numbers of scenarios within the optimization problem which helps the decision maker in discovering balanced solutions the computation cost of multi scenario moro is low compared to previous approaches however there is a need for more experience in different real life environmental problems as we observed in this study the lake problem as a widely used benchmark problem in robustness comparisons cannot reflect the trade offs between scenarios therefore there is a need for new benchmark problems that reflect trade offs between scenarios this will be one of our future research directions we also proposed a novel approach for scenario discovery based on the basic concepts of mathematical multi objective optimization to determine vulnerable scenarios before generating any solution the decision maker can learn about vulnerability and the sources of failures even before policy determination paving the way for considering other solution methods like a priori and particularly interactive multi objective optimization methods miettinen et al 2008 2016 miettinen 1999 this interesting topic is also in our future research interests last but not least in this study we used the inter temporal open loop formulation including static periodical decision variables of the lake problem for demonstrating our method and for comparisons because it is easy to understand and is supposed to present the relationship between scenarios and robustness of solutions nonetheless the proposed multi scenario moro can also be applied to solve adaptive formulations such as direct policy search quinn et al 2017 and planned adaptive direct policy search bartholomew and kwakkel 2020 we also believe that the best way to deal with deep uncertainty is dynamic robustness and adaptive approaches as another interesting future direction our proposed multi scenario moro can also be combined with dynamic adaptive policy pathways haasnoot et al 2013 in this way we can design a dynamic multi stage multi scenario moro approach to identify the best combination of the initial decisions and scenario relevant possible adaptation decisions shavazipour and stewart 2019 shavazipour et al 2020 software availability all code used for this research can be found at https github com industrial optimization group multi scenario multi objective robust optimization under deep uncertainty git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partly funded by the academy of finland grants no 322221 and 311877 this research is related to the thematic research area decision analytics utilizing causal models and multiobjective optimization demo jyu fi demo of the university of jyvaskyla 
25750,this paper proposes a novel optimization approach for multi scenario multi objective robust decision making as well as an alternative way for scenario discovery and identifying vulnerable scenarios even before any solution generation to demonstrate and test the novel approach we use the classic shallow lake problem we compare the results obtained with the novel approach to those obtained with previously used approaches we show that the novel approach guarantees the feasibility and robust efficiency of the produced solutions under all selected scenarios while decreasing computation cost addresses the scenario dependency issues and enables the decision makers to explore the trade off between optimality feasibility in any selected scenario and robustness across a broader range of scenarios we also find that the lake problem is ill suited for reflecting trade offs in robust performance over the set of scenarios and pareto optimality in any specific scenario highlighting the need for novel benchmark problems to properly evaluate novel approaches keywords multi objective optimization scenario planning deep uncertainty robust decision making scalarizing functions reference points 1 introduction decision making in complex environmental problems typically involves several conflicting objectives to be considered simultaneously there is no single optimal solution for these multi objective problems because of the conflicting objectives instead several so called pareto optimal solutions reflecting different trade offs between the conflicting objectives can be found in such multi objective decision problems decision support tools can help decision makers in balancing between conflicting objectives the task of decision makers in environmental problems is further complicated by the presence of uncertainty to mitigate the potential negative consequences of uncertainty it has been argued that decisions should have limited sensitivity to the consequences of uncertainty so called robust decision and perform relatively well in a broad range of future states of the world or scenarios lempert et al 2006 in environmental systems the level of uncertainty is high and probabilities over the various alternative states of the world can only be approximately assessed this kind of uncertainty is sometimes also known as deep uncertainty bankes 2002 lempert et al 2003 kwakkel et al 2010 walker et al 2013 shavazipour and stewart 2019 uncertainties about future climate change and socio economic conditions are two examples of deep uncertainty in environmental problems therefore decision makers in complex environmental problems are facing a multi objective optimization problem to be solved in the presence of deep uncertainty where the performance of a decision should be evaluated according to all objectives in all plausible scenarios shavazipour and stewart 2019 shavazipour et al 2020 stewart et al 2013 this kind of decision problem is also known as a multi scenario multi objective decision making problem scenarios in this paper represent different plausible future realizations of the deep uncertainties maier et al 2016 in practice it is almost impossible to find a decision that is pareto optimal or even feasible in all plausible scenarios by a feasible solution in multi scenario multi objective optimization problems we mean a solution that is feasible i e satisfies all constraints in all scenarios accordingly decision makers seek robust solutions that are sufficiently good in a broad range of scenarios i e robust satisficing this introduces an additional trade off between pareto optimality and feasibility in any given scenario and robustness over a set of scenarios in this paper we refer to this as the trade off between scenarios recently different approaches have been introduced for environmental multi objective optimization problems under deep uncertainty such as many objective robust decision making mordm kasprzyk et al 2013 multi scenario mordm watson and kasprzyk 2017 and multi objective robust optimization moro hamarat et al 2014 kwakkel et al 2015 trindade et al 2017 all these approaches are based on the robust decision making rdm framework lempert et al 2006 groves and lempert 2007 rdm is an iterative approach where pre specified policy alternatives solutions are stress tested over a wide range of scenarios in order to determine conditions under which each solution fails to perform adequately then the policy alternatives are refined to find the most robust solution s in light of these failure conditions mordm was introduced as an extension to rdm to help in generating a promising set of candidate solutions as input to the stress testing for decision problems involving multiple objectives these solutions are produced using multi objective optimization given a single reference scenario i e only optimizing in the feasible region of a single scenario this inadvertently introduces scenario dependency of the generated solutions given that the pareto approximation only includes solutions optimized in and feasible for that single scenario which reduces both the robustness which can be attained during stress testing eker and kwakkel 2018 giudici et al 2020 bartholomew and kwakkel 2020 as well as the feasibility of the candidate solutions in other scenarios to reduce this shortcoming watson and kasprzyk 2017 proposed multi scenario mordm which repeats the process of identifying candidate solutions prior to stress testing for several scenarios expanding on this eker and kwakkel 2018 introduced a more systematic scenario selection procedure that ensures high diversity among the scenarios which are used for the identification of candidate solutions however solutions generated with multi scenario mordm are still highly dependent on the selected scenarios this is because multi scenario mordm does the search separately for few selected scenarios without checking the feasibility and performance of the solutions in the other scenarios during the optimization process therefore there is no guarantee that the solutions generated are feasible in any other scenario besides gathering solutions generated by single scenario optimizations cannot guarantee optimal robustness either of course the feasibility and the performance of the solutions will be checked later in the robustness analysis however many of the solutions found in that way may have inferior performance i e be dominated in some scenarios or even be infeasible which means wasting computational resources in finding poor solutions that will be eliminated later in the robustness analysis in contrast moro hamarat et al 2014 kwakkel et al 2015 trindade et al 2017 only concentrates on robustness by optimizing the robustness measures as objective functions over a set of scenarios however this simulation optimization approach is computationally demanding and possibly intractable even for small sets of scenarios eker and kwakkel 2018 bartholomew and kwakkel 2020 giudici et al 2020 furthermore utilizing different robustness measures results in different solutions this highlights the meta choice of selecting the most appropriate robustness measures which might require a separate study giudici et al 2020 kwakkel et al 2016 mcphail et al 2018 besides the existing trade offs between objectives in different scenarios cannot be explicitly verified in worst case min max and aggregation based robustness measures which are often used in moro for instance the overall robustness may be affected excessively because of poor performance in a few scenarios ben tal et al 2017 roos den hertog 2020 shavazipour and stewart 2019 shavazipour et al 2020 bartholomew and kwakkel 2020 compared mordm multi scenario mordm and moro and confirmed that the more robustness is considered in the search for candidate solutions prior to stress testing the more robust the solutions will be nevertheless there remains a trade off between optimality and feasibility in any given scenario and robustness over the set of scenarios this trade off is sometimes also known as the price of robustness bertsimas and sim 2004 schöbel and zhou kangas 2021 there is currently no approach for the search phase of mordm that enables decision makers to explore this trade off explicitly note that adding scenarios to a multi objective optimization problem adds dimensions to the problem indeed the resulting multi scenario multi objective optimization problem includes all objective scenario combinations in which the dimension of the space grows exponentially e g in a problem with four objectives and five scenarios the space becomes 4 5 20 dimensional this means that a solution may have an outstanding performance on one objective in one particular scenario but its performance on other objectives may be poor or the solution may even be infeasible in some scenarios the previous variants of mordm identify solutions in the objective space of a single scenario problem then they test the performance of these solutions on the uncertainty space constructed by an ensemble of random scenarios this may imply losing robust solutions as well as the chance of exploring the mentioned trade off in parallel to the continuous refinement of rdm via mordm multi scenario mordm and moro the concept of robustness in multi objective optimization has been receiving theoretical attention as well this has resulted in various novel theoretical concepts such as min max robustness ehrgott et al 2014 highly dranichak and wiecek 2019 flimsy bitran 1980 kuhn et al 2016 and lightly robust efficiency ide and schöbel 2016 regret robustness xidonas et al 2017 and multi scenario efficiency botte and schöbel 2019 shavazipour and stewart 2019 shavazipour et al 2020 we refer the interested readers to botte and schöbel 2019 ide and schöbel 2016 and schöbel and zhou kangas 2021 for a review and comparison of different theoretical robustness concepts in multi objective optimization although not all of these concepts and methods were primarily developed to deal with deep uncertainty still to some extent they can be utilized for this purpose as a complement to the existing approaches in decision making under deep uncertainty dmdu among these concepts multi scenario efficiency defined particularly for a discrete uncertainty space i e constructed with a finite number of scenarios is similar to the concept of robustness utilized in the deep uncertainty literature the main difference between these two bodies of literature in how they use robustness concepts lies in where they are evaluating the robustness of a candidate solution in mathematical optimization robustness is often utilized as an a priori criterion or soft constraint in searching for candidate solutions leading to a particular set of solutions following that criterion constraint i e we are only looking for robust efficient solutions in contrast in dmdu the robustness of solutions is typically an attribute of a generated solution measured in an a posteriori manner i e after the search phase as a result robustness in dmdu is used as an a posteriori measure for ranking already generated solutions in mathematical optimization all the robust efficient solutions are compromise solutions distinguishable by different trade offs between objective s in various scenarios as the central common assumption none of these two bodies of literature consider the probability of scenario occurrence in their definitions and models in a multi scenario multi objective decision making problem ideally candidate decisions are evaluated in terms of all objectives in all or at least a representative set of selected scenarios shavazipour and stewart 2019 shavazipour et al 2020 stewart et al 2013 this kind of an assessment helps identifying solutions that are not only feasible in all selected scenarios but also robust efficient that is the used approach should guarantees that there exists no other solution which is not worse on all objectives in all selected scenarios and is better on at least one objective in one scenario botte and schöbel 2019 shavazipour et al 2021 among the previously proposed methods developed to handle multiple objectives under deep uncertainty only moro can to some extent guarantee the robust efficiency of all generated solutions without any extra filtering in all selected scenarios e g by considering all scenario specific constraints within the optimization model the primary aim of this paper is to build a bridge between the literature on mathematical multi objective optimization which has a strong theoretical foundation and the robust decision making literature which has shown successful real world applications to the best of our knowledge this is the very first step in this regard by drawing on the theoretical developments in mathematical multi objective optimization we can address the issues of robust efficiency feasibility and the price of robustness which affect existing approaches for the search phase within the mordm framework in this paper we propose a novel multi scenario multi objective robust optimization approach called multi scenario moro hereinafter by incorporating uncertainties in the optimization phase and identify solutions that perform well in some selected scenarios in this way the performance of solutions in terms of all objectives in all selected scenarios are evaluated within a single optimization problem as a result the pareto optimal solutions for considered scenarios can be found which are not only feasible in all selected scenarios but also robust efficient if any feasible solution is available in other words we combine all single scenario multi objective optimization problems into a meta optimization problem and simultaneously consider the evaluation of the objective functions in multiple scenarios indeed our objective functions include all the objective scenario combinations called meta objective meta criteria stewart et al 2013 subject to constraints satisfaction in all considered scenarios the proposed multi scenario moro has both a lower computation cost and increased robustness consideration during the search process it generates less scenario dependent solutions for our proposed approach and likewise for other approaches for multi scenario mordm selected scenarios should reflect the system vulnerabilities and or the main decision maker s preferences giudici et al 2020 the classic shallow lake problem first introduced by carpenter et al 1999 has been very often used to demonstrate test and compare methodological developments for decision making under deep uncertainty kwakkel 2017 lempert and collins 2007 singh et al 2015 bartholomew and kwakkel 2020 eker and kwakkel 2018 quinn et al 2017 singh et al 2015 2015 ward et al 2015 it is a standard benchmark problem reflecting the required characteristics of real world environmental problems such as tipping points affected by deeply uncertain parameters and multiple conflicting objectives therefore we use the shallow lake problem to demonstrate our novel approach and compare it with existing approaches in brief the main contributions of this paper are 1 proposing a multi scenario moro approach which utilizes a different solution method from the mathematical multi objective optimization literature to produce candidate solutions that reduce the computational cost and also can guarantee pareto optimality 2 paving the way to explore the trade offs between scenario specific pareto optimality and robustness by considering different numbers of scenarios in the optimization model and 3 introducing a novel way of scenario analysis to determine vulnerable scenarios using ideal points best possible achievements on each objective in each scenario and information about feasible regions in various scenarios since the proposed scenario analysis does not need any prior knowledge of solutions and their robustness the decision makers can gain insight into the problem before solution determination the rest of the paper is organized as follows section 2 includes a brief description of multi scenario multi objective optimization problems and the solution method utilizing in this study as well as the proposed multi scenario moro approach the lake problem as our case study and the multi scenario formulation of it are described in section 3 in section 4 we illustrate more details about how the proposed multi scenario moro can be applied and compare the results with the state of the art methods in the literature finally we discuss the feasibility robustness and computational costs of multi scenario moro regarding the different number of scenario considerations in section 5 before concluding in section 6 2 methods 2 1 multi scenario multi objective optimization a multi scenario multi objective optimization problem msmop also called all in one or scenario based multi objective optimization problem with k 2 objective functions and s 2 scenarios can be formulated as follows shavazipour et al 2021 1 minimize f 1 p x f k p x p ω subject to x s r n where the scenario space set ω is constructed by s plausible scenarios and each scenario includes k objective functions objective functions in scenario p p 1 s are described by f ip i 1 k x x 1 x n t is a vector of n decision variables in the feasible region s in the decision space r n s r n defined by constraint functions z p f 1 p x f k p x t p 1 s called an objective vector is the image of a decision vector x in the objective space r k under the conditions of scenario p a decision vector x s is called pareto optimal also called non dominated in scenario p if under the conditions of scenario p there does not exists another x s such that for all i f ip x f ip x and f jp x f jp x for at least one index j the image of the set of pareto optimal decision vectors in the objective space is sometimes called a pareto front x s is weakly pareto optimal in scenario p if there does not exists another x s such that for all i f i p x f i p x a preferred solution refers to a pareto optimal solution satisfying decision maker s preferences in terms of all k s meta criteria for any two objective vectors z p z p r k in scenario p we say that z p dominates z p if and only if for all i z i p z i p and z p j z j p for at least one index j the best and the worst possible values for individual objectives in the pareto front are components of an ideal point z i d e a l z 11 i d e a l z k s i d e a l t and a nadir point z n a d i r z 11 n a d i r z k s n a d t respectively while ideal points can be simply calculated by solving a relevant single scenario single objective optimization problem computing nadir points is difficult in practice however their estimations can either be provided by the decision maker or approximated for instant through a pay off table see e g miettinen 1999 and references therein also z i p u t o z i p i d e a l ε i 1 k p 1 s are components of an objective vector z p u t o r n called a utopian objective vector in scenario p where ε 0 is a relatively small scalar it is strictly better than the ideal point 2 2 generating candidate solutions achievement scalarizing function over the years many different methods have been proposed to solve multi objective optimization problems the two most popular type of methods are 1 multiple criteria decision making mcdm e g chankong and haimes 1983 miettinen 1999 and 2 evolutionary multi objective optimization emo e g coello et al 2007 deb 2001 a major advantage of emo algorithms is that they generate a set of approximated pareto optimal solutions in a single run of the algorithm however emo algorithms tend to be inefficient when the number of objectives increases in contrast mcdm method guarantee pareto optimality have a strong theoretical foundation and no limitation regarding the number of objectives many mcdm methods transform the original problem into a single objective optimization problems using a so called scalarizing function considering the decision maker s preferences see e g miettinen 1999 for more information on different mcdm methods and miettinen and mäkelä 2002 ruiz et al 2009 for a comparison of various scalarizing functions so far all variants of mordm have utilized emo algorithms to the best of our knowledge mcdm methods have not yet been utilized within the mordm framework as also mentioned in kasprzyk et al 2013 the main reason is related to the use of a priori importance weights as one form of preference information because of some concerns about the accuracy of these weights before the decision maker observes a broader set of solutions and gains a better understanding about the non convexity continuity of the pareto front and potential non linear relations between variables parameters however on the one hand a priori methods are designed to be used in problems in which the decision maker has a good enough understanding of the problem interdependencies of the objectives and possible outcomes and is able or wishes to express his her expertise a priori on the other hand there are also other types of mcdm methods interactive and a posteriori methods see e g miettinen 1999 furthermore when the number of objectives grows emo algorithms cannot efficiently approximate the pareto front thus they cannot be used to solve msmops which usually have tens or hundreds of objectives to overcome this issue in this paper we utilize one of the most popular scalarizing functions i e an achievement scalarizing function wierzbicki 1986 more specifically we use the following achievement scalarizing function which includes an augmentation term to avoid weakly pareto optimal solutions wierzbicki 1986 2 min x max i 1 k w i f i x z i ε i 1 k w i f i x z i s t x s where w i i 1 k are the weights for normalization as preference information set by a decision maker z i i 1 k known as aspiration levels represent desirable objective function values the vector of k aspiration levels is called a reference point a reference point in the objective space can be feasible or infeasible in any case a scalarizing function like 2 can identify the closest pareto optimal solution to the given reference point accordingly utilizing different reference points tends to lead to different pareto optimal solutions however sometimes the same pareto optimal solution may be associated with multiple reference points the multiplier ε is a small positive number and ε i 1 k w i f i x z i is an augmentation term ensuring pareto optimality thus the optimal solution to problem 2 is a pareto optimal solution to the original multi objective optimization problem wierzbicki 1986 miettinen 1999 when we have multiple scenarios the above formulation has an additional dimension 3 min x max i 1 k p 1 s w i p f i p x z i p ε i 1 n p 1 s w i p f i p x z i p s t x s where w ip represents the weight for objective i in scenario p and z i p is the aspiration level for the ith objective function in the pth scenario solving the scalarized problem 3 provides a single optimal solution which is pareto optimal for the original problem 1 different solutions can be generated by using different reference points therefore in contrast to emo methods to produce a set of pareto optimal solutions one needs to repeatedly solve the optimization problem using different reference points nonetheless considering the decision maker s preferences reference points here not only gives rise to generating solutions that lie in the areas of interest to the decision makers but it can also confine the search area and thus reduce computational cost in general a decision maker may provide preferences a priori choose a preferred solution among the provided set of solutions in a posteriori method or be iteratively involved using an interactive approach miettinen 1999 achievement scalarizing functions can be applied in any of these three ways in this paper following all variants of rdm we utilize scalarizing functions in an a posteriori method therefore we need to predetermine several reference points to produce different pareto optimal solutions amongst various techniques that have been developed for setting the reference points we utilize the method introduced by mueller gritschneder et al 2009 2 3 the proposed multi scenario multi objective robust optimization approach mo rdm is an iterative approach for finding robust solution s it consists of four steps 1 model specification 2 solution identification 3 computational exploration i e re evaluation of candidate solutions in a broad range of plausible scenarios and 4 scenario discovery in which vulnerable scenarios are identified and this information can be used to modify the model and or generate new solutions this process continues until the decision maker is satisfied with a set of solution s lempert et al 2006 kasprzyk et al 2013 there exist three different approaches for identifying policy alternatives in the second step of mordm single scenario kasprzyk et al 2013 multiple single scenario eker and kwakkel 2018 watson and kasprzyk 2017 and robust optimization hamarat et al 2014 kwakkel et al 2015 trindade et al 2017 however they have shortcomings e g scenario dependency in the first two variants and inability to reflect the trade offs between scenarios in all three variants to overcome these weaknesses we propose a novel multi scenario multi objective robust optimization approach that simultaneously considers multiple objectives in multiple scenarios not an indirect aggregated value over a set of scenarios i e the proposed multi scenario moro approach performs the search in a combined multi scenario multi objective space in this way all the generated solutions are robust efficient in all selected scenarios which increases robustness and reduces scenario dependency indeed we propose to use the multi scenario multi objective optimization approach model 1 to generate solutions in the second step search phase of the mo rdm yet the proposed multi scenario moro involves four iterative steps portraying in fig 1 and detailed as follows 1 model specification determining the components of a decision making problem such as the decisions to be made decision variables certain and uncertain parameters and relations between them how to measure performance like objective functions in an optimization problem problem constraints etc 2 solution determination this step which is the main contribution of this study divides into three sub steps as also shown in fig 1 a scenario selection similar to the previous variants of multi scenario mordm we need to select a set of scenarios to be considered within the optimization this study follows the state of the art scenario selection method proposed by eker and kwakkel 2018 although any other approaches can be utilized the decision maker can set the number of scenarios to be considered in the optimization problem based on preferences computation cost complexity or other considerations b multi scenario multi objective optimization problem formulation in this step to identify the candidate solutions we formulate using the information specified in the previous steps and solve a multi scenario multi objective optimization problem that simultaneously considers multiple objectives and multiple scenarios within a single optimization problem of the form 1 by changing the number of scenarios considered in this problem the decision maker can explore the trade offs between all objectives in all selected scenarios the higher the number of scenarios considered within the optimization problem the more robust the identified solutions will be however increasing the number of scenarios considered within the problem can reduce the chance of feasibility and or optimality in any given scenario c solution process since the total number of objective scenario combinations meta objectives utilizing in msmop is often significantly high we utilize the scalarizing function 3 to generate pareto optimal solutions by solving it multiple times by any appropriate single objective solver incorporating different reference points 3 uncertainty robustness analysis in this study following the previous variants of mordm the solutions identified in step 2 are re evaluated over a wider range of plausible scenarios to assess their robustness and investigate the impacts of deep uncertainty on the objective functions 4 scenario discovery analysis scenario discovery methods aim at identifying sub spaces subsets in the uncertainty space set ω where candidate solutions perform poorly different algorithms have been developed for this purpose in the literature e g bryant and lempert 2010 dalal et al 2013 kwakkel and jaxa rozen 2016 however as an alternative to available scenario discovery methods we propose a novel method to determine vulnerable scenarios using ideal points and information about feasible regions in various scenarios to this end first we calculate the ideal points for all objective functions in all new randomly generated scenarios in the previous step note that these randomly generated scenarios may not be the same as the scenarios considered in the optimization problem thus we need to calculate the best possible values for each objective hereinafter called ideal values in each randomly generated scenario these ideal values are computed by solving the associated single scenario single objective optimization problem described in section 2 1 comparing the differences between the ideal values in various scenarios will help us identify vulnerable scenarios and the combinations of the deeply uncertain parameters causing the poor performances in these vulnerable scenarios this novel way of scenario analysis discovery will be illustrated in more detail through the case study in section 4 3 2 4 robustness measures and trade offs analysis following recent studies and for comparison purposes in this paper we use the mean standard deviation hamarat et al 2014 and the domain criterion starr 1963 to measure the robustness for each objective and avoid objectives aggregation to compare the robustness trade offs between the objectives the mean standard deviation measure is used to compare the results with eker and kwakkel 2018 while the domain criterion measure is utilized mostly for scenario analyzes and relevant discussions it is also used in comparison with the results of quinn et al 2017 these two robustness measures which are used in robustness analyses and scenario discovery not in optimization are briefly described in this section 2 4 1 mean standard deviation signal to noise ratio the mean standard deviation measure for solution j in objective function i representing by r ij can be formulated as follows eker and kwakkel 2018 4 r i j μ f i j p 1 σ f i j p 1 if f i is to be maximized p 1 s μ f i j p 1 σ f i j p 1 if f i is to be minimized p 1 s where μ is the mean over the set of scenarios for the ith objective function in the case of implementing the solution j and σ is the standard deviation 2 4 2 domain criterion the domain criterion is a satisficing robustness measure introduced by starr 1963 which directly applies the decision maker s preferences on minimum acceptable values for each objective function this measure mirrors the fraction percentage of all considered scenarios in which the minimum acceptable thresholds are met i e the percentage number of scenarios in which the solutions are meeting the criterion the robustness value lies between 0 and 1 where 1 shows that the given criterion is met in all scenarios for the relevant candidate solution and 0 means that the given criterion is not met in any scenario 3 case study the shallow lake problem to demonstrate the proposed multi scenario moro we use the shallow lake problem carpenter et al 1999 which is often used to demonstrate and benchmark methodological developments for decision making under deep uncertainty bartholomew and kwakkel 2020 eker and kwakkel 2018 hadka et al 2015 lempert and collins 2007 quinn et al 2017 singh et al 2015 ward et al 2015 in this stylized decision making problem a city is located next to a shallow lake anthropogenic pollution produced by the city goes into the lake if a eutrophication threshold is passed the lake irreversible transitions to a eutrophic state the decision problem is to decide on an annual pollution control strategy that gives rise to the highest economic benefits without passing a critical threshold to complicate the decision problem next to the controllable anthropogenic inflow there is also uncontrollable natural inflow phosphorus pollution levels can be calculated by the following dimensionless differential equation quinn et al 2017 5 p t 1 b p t 1 p t 1 q 1 p t 1 q x t 1 ζ t 1 where p represents the phosphorus level in the lake x describes the phosphorus anthropogenic pollution input ζ logn μ σ 2 refers to the natural pollution input t indicates the time period and b and q are the parameters of the lake model which control the rate at which pollution is lost from the lake and recycled from the sediment 3 1 objective functions following the literature on the shallow lake problem bartholomew and kwakkel 2020 eker and kwakkel 2018 hadka et al 2015 quinn et al 2017 ward et al 2015 we consider four conflicting objectives as follows 3 1 1 economic utility to be maximized the first objective function is the economic utility by releasing anthropogenic phosphorus pollution into the lake following quinn et al 2017 and ward et al 2015 the economic utility is computed as the average of the discounted benefit in n simulations of t years of random natural inflows in the first objective function 6 f 1 x 1 n n 1 n t 0 t 1 α x t n δ t α is an economic constant fixed at 0 04 δ is a discount rate and x tn is the decision variable describing the phosphorus pollution level that can be released in year t for the nth random natural inflows simulation realization 3 1 2 phosphorus pollution to be minimized minimizing the maximum average phosphorus level is considered as an environmental objective for water quality targets this objective function is naturally conflicting with the economic objective if p tn x represents the concentration of the phosphorus pollution in year t for the nth random natural inflows simulation the second objective function is 7 f 2 x m a x t 1 t 1 n n 1 n p t n 3 1 3 inertia to be maximized to avoid extremely rapid declines in phosphorus pollution in one year which needs a massive amount of investments in infrastructure and to control the maintain decision inertia the decision maker can set an annual reduction limit i limit on phosphorus pollution therefore in the third objective function 8 f 3 x 1 n t 1 n 1 n t 1 t 1 φ t n w h e r e φ t n 1 x t 1 n x t n i l i m i t 0 o t h e r w i s e the inertia of a decision is maximized inertia is defined as the average fraction of t 1 planning years over n random natural inflows simulations where inter annual pollution declines are lower than i limit of the maximum possible reduction in this paper following eker and kwakkel 2018 quinn et al 2017 ward et al 2015 i limit is set as 0 02 i e 20 of the maximum possible reduction 3 1 4 reliability to be maximized the last objective function 9 f 4 x 1 n t n 1 n t 1 t θ t n w h e r e θ t n 1 p t n p c r i t 0 p t n p c r i t also called an average reliability of a decision reflects the decision maker s desire in abstaining from the eutrophication of the lake which occurs if the concentration of the phosphorus in the lake passes a critical threshold p crit if the phosphorus level in the lake lies below p crit in a given period the reliability index θ tn is 1 and 0 otherwise thus maximizing the reliability means maximizing the periods out of t and across n simulations in which the phosphorus level in the lake stays below the critical threshold p crit 3 2 uncertainties and scenario selection two different degrees levels of uncertainty see e g shavazipour and stewart 2019 or kwakkel and walker 2010 for the definitions of various degrees or levels of uncertainty i e mild also called stochastic and deep are present in the shallow lake problem the mild uncertainty in natural pollution inflow ζ is handled by the average values of random samples generating by a log normal distribution the mean μ and standard deviation σ of the log normal distribution the discount factor δ the natural recycling rate q and the loss rate b are the deeply uncertain parameters in this problem following previous work bartholomew and kwakkel 2020 eker and kwakkel 2018 quinn et al 2017 these five deeply uncertain parameters and their ranges are shown in table 1 a combination of the values of these five deeply uncertain parameters sampled from their ranges generates a scenario from the scenario space ω for scenario selection we followed eker and kwakkel 2018 in selecting four more scenarios in addition to the baseline scenario and thus consider the same scenarios by utilizing the same values for deeply uncertain parameters constructing those four scenarios for more information about how these four scenarios were selected see eker and kwakkel 2018 section 4 1 pages 205 207 the last five columns of table 1 denote these five selected scenarios 3 3 multi scenario inter temporal open loop control formulation for the lake problem different variants of the shallow lake problem have been proposed in the literature the widely known ones are inter temporal open loop control eker and kwakkel 2018 hadka et al 2015 quinn et al 2017 singh et al 2015 ward et al 2015 direct policy search quinn et al 2017 and planned adaptive direct policy search bartholomew and kwakkel 2020 in this paper we use the often used inter temporal open loop control version in a multi scenario manner which includes t decision variables the optimization formulation for this multi scenario inter temporal open loop control version of the lake problem is 10 minimize f 1 p x f 2 p x f 3 p x f 4 p x p ω s t 0 0001 x t 0 1 f o r a l l t where x x 0 x 1 x t 1 is a vector of decision variables t indicates the length of the planning horizon x t represents the amount of phosphorus pollution to be released in year t which is limited to 0 1 as before f ip refers to objective function i i 1 2 3 4 in scenario p and ω is the scenario space 4 results in this section we illustrate the proposed multi scenario moro step by step following the steps shown in fig 1 describes in section 2 3 to assess the efficacy of the novel approach we compare the results with those of the previous studies 4 1 steps 1 and 2 problem setting scenario selection and generating candidate solutions we consider the formulation of the lake problem 10 with four objective functions in five scenarios for t 100 years and n 100 random realizations of the natural inflows we follow the selection approach of eker and kwakkel 2018 and thus use the same scenarios these five scenarios are presented in table 1 therefore to generate solutions we need to solve a multi scenario multi objective optimization problem with 100 decision variables and 20 objective functions 5 scenarios 4 objectives per scenario based on the above mentioned settings and the estimated worst possible values of the objective functions 6 9 the nadir points for the second objective pollution were set as 15 in all five scenarios and for the other three objectives in all selected scenarios 0 ideal points presenting in table 2 were calculated by solving the relevant single scenario single objective optimization problems for each objective function in each considered scenario the utopian values were calculated by adding for objectives to be maximized or subtracting for objective to be minimized a small scalar of 0 0001 to from the ideal points an ideal point represents the optimal performance that can be reached for each objective in a given scenario for instance as shown in table 2 the best possible performance for the first objective utility in the fourth scenario was 0 581 which is about two third of the maximum potential performance in the first three scenarios and around one third of the performance in the best case scenario s 5 these ideal values are represented the effect of deep uncertainty and highlight some of the problem s limitations even before identifying the solution candidate given this setup we use the achievement scalarizing function and solve the resulting optimization problem using the sequential least squares programming slsqp algorithm kraft 1994 available from the scipy module oliphant 2007 to generate different pareto optimal solutions we followed the mueller gritschneder et al 2009 method to pre specify with the 50 reference points and weights w i p 1 z i p n a d i r z i p u t o i 1 4 p 1 5 we get 50 different pareto optimal solutions for the original multi objective problem in practice the number of solutions is to be set by the decision maker here we chose 50 for comparison purposes as it is the number of solutions considered by eker and kwakkel 2018 in their robustness analysis note that the 50 solutions plotting in eker and kwakkel 2018 were the brushed solutions after all the filtering however a set of 50 or even fewer solutions generated by the proposed multi scenario moro can reasonably out perform these because of the simultaneous consideration of the five selected scenarios in multi scenario moro there is no need for further filtering in this way we can simultaneously save computational resources and improve the quality of the solutions we will return to this later in this section each poly line of the parallel coordinate plot in fig 2 represents the performance of a single solution on all four objectives in each of the five scenarios different colors in fig 2 distinguish the solutions in terms of their performance on the utility objective in the fifth scenario s 5 the higher the utility the lighter the color yellow in the colored version a higher utility leads to a higher pollution which results in lower reliability values the opposite is visible for solutions with higher reliability values shown with darker colors dark purple in the colored version which highlights the trade off between pollution reliability and utility another observation is that there is no significant trade off between scenarios in each objective particularly between the performances in utility and inertia in different scenarios for example the highest values for utility in different scenarios are from the same solution or inertia values in all five scenarios are similar in each solution visible by horizontal lines for inertia over the five scenarios in fig 2 this particular characteristic of the lake problem may prevent the explicit study of the trade offs between scenarios and cause some difficulty in robustness and trade off comparisons we return to this point in section 5 4 2 step 3 robustness analysis to re evaluate the 50 solutions and assess their robustness we generate an ensemble of 1000 randomly generated scenarios using latin hypercube sampling the 50 solutions are re evaluated over these scenarios to analyze and compare their robustness across a broader range of scenarios based on this ensemble of 1000 scenarios the robustness of the candidate solutions is determined using the domain criterion and the mean standard deviation 4 2 1 robustness trade offs with mean standard deviation in this section we use the mean standard deviation as the robustness measure to compare the results identified with multi scenario moro solution j in each objective function i using 4 fig 3 shows the mean standard robustness trade offs of the generated candidate solutions over the 1000 random scenarios the color code is similar to the previous figure i e the higher the utility robustness the lighter the color also conflicts between the robustness values in reliability and utility are vivid when the poly lines cross between the last two columns representing the robustness trade offs between these two objectives note that since we are minimizing pollution lower values of robustness are better for this objective thus the relevant column representing the robustness of pollution are inverted in the plot to unify the robustness improvement direction which is upwards lines higher up in the plot describe solutions with higher robustness on all objectives fig 4 compares the robustness trade offs of the solutions generated by the proposed multi scenario moro the solutions of eker and kwakkel 2018 and the solutions produced by quinn et al 2017 as seen in this figure the solutions generated by multi scenario moro result in a wider variety of robustness trade offs compared to the solutions produced by the other methods for example the maximum robustness value for utility in multi scenario mordm eker and kwakkel 2018 and mordm quinn et al 2017 were respectively 1 21 and 1 16 while almost a half of the solutions generated by multi scenario moro provided better values up to 1 51 similar patterns are valid for the robustness of all three other objectives moreover as mentioned the trade offs between reliability and utility are evident among the solutions produced by multi scenario moro these trade offs are hardly visible with the solutions of the other methods there are two reasons why the other methods could not find solutions with wider robustness trade offs 1 scenario dependency of their solutions since their search area has a lower dimension limiting the search to a hyperplane constructed by one scenario at a time for instance solutions with exceptionally low performance in one scenario i e dominated solutions in that scenario may have high performance in many other scenarios i e non dominated in many different scenarios these solutions are not identified as non dominated solutions when the search is confined to only a single scenario space 2 some of the solutions which represent wider trade offs may be eliminated from the final list after applying the reliability constraint quinn et al 2017 eker and kwakkel 2018 with a similar reason to the previous item 4 2 2 robustness trade offs with domain criterion the second robustness measure we use is the domain criterion the following criteria are considered based on previous studies using the lake problem quinn et al 2017 bartholomew and kwakkel 2020 1 utility 0 2 2 reliability 0 95 3 pollution critical point p crit 4 inertia 0 99 in practice these criteria would be set by a decision maker for each criterion from the above list we calculated the number of solutions meeting that criteria after re evaluation over the ensemble of 1000 randomly generated scenarios in section 4 2 then ranked and sorted them based on their robustness in that criterion over the rank sorted solutions the robustness scores on the following criteria are described in fig 5 fig 6 compares the distributions of the robustness for the 50 candidate solutions generated by multi scenario moro the 50 brushed solutions of eker and kwakkel 2018 and the 86 brushed solutions produced by quinn et al 2017 note that all the solutions are re evaluated over the same ensemble of 1000 randomly generated scenarios as seen in fig 6 b in 60 of the generated solutions by multi scenario moro and in 26 of the solutions of the multi scenario mordm eker and kwakkel 2018 the robustness values for utility were 1 meaning that the utility values met the domain criterion of 0 2 in all 1000 random scenarios for these solutions in contrast none of the solutions of the mordm quinn et al 2017 reached this value the utility robustness value of 1 the maximum value of the utility robustness gained by a solution of quinn et al 2017 was 62 9 the robustness value of 1 for inertia is observed in about 34 of the solutions generated by multi scenario moro while only one solution amongst the ones produced by the mordm quinn et al 2017 could obtain a similar value of robustness for inertia see fig 6 d all the solutions generated by the multi scenario mordm eker and kwakkel 2018 have the robustness value of 0 for inertia nevertheless no solution among the generated solutions by either approach provides the robustness value 1 for reliability and pollution fig 6 a and c the maximum robustness percentage of reliability and pollution among the solutions of multi scenario moro were 76 9 and 76 7 respectively corresponding values amongst the solutions of quinn et al 2017 were respectively 63 4 and 63 1 also the maximum robustness percentage of reliability and pollution obtained by the solutions of eker and kwakkel 2018 were 63 7 and 63 4 respectively this result is also in line with the results of a similar analysis in quinn et al 2017 and bartholomew and kwakkel 2020 in general two main reasons may cause these results first some parts of the pareto front are left unexplored i e the set of generated solutions is not diverse enough to cover the entire front second there may be no feasible solution in those scenarios that can satisfy the given criterion in the next section through scenario analysis we will show that there is no feasible solution meeting the domain criterion on reliability and pollution as can also be seen in fig 6 a and c in about 23 of the scenarios this means that in around 23 of the scenarios the reliability criterion cannot be satisfied the values for the deeply uncertain parameters of these scenarios are presented in fig 8 moreover comparing the left side plots a and c in fig 6 demonstrates a strong correlation between the robustness of reliability and pollution particularly amidst the solutions of multi scenario moro which is expected as minimum pollution values give rise to high reliability values this strong correlation is even more visible by tracing the straight lines between pollution and reliability in fig 5 apart from these insights in fig 5 we also see that the existing trade offs between reliability and utility are again visible the color codes are the same as fig 3 i e the higher the utility robustness the lighter the colors comparing the robustness of the solutions generated by multi scenario moro and the ones produced by the two other methods i e mordm quinn et al 2017 and multi scenario mordm eker and kwakkel 2018 with the domain criterion measure as portrayed in fig 7 confirms the superiority of multi scenario moro for instance multi scenario moro identifies pareto optimal solutions that provide a broader range of robustness trade offs they also help decision makers to gain more insights into the problem than the previous variants of mordm in the next section we investigate in more detail the feasibility of the domain criteria through scenario analysis furthermore we analyze vulnerable scenarios to identify the combinations of deeply uncertain parameters causing poor performance in those scenarios 4 3 step 4 scenario analysis discovery to check the feasibility of meeting the domain criterion for each objective function in any scenario first we calculate the ideal points for all four objective functions in all 1000 scenarios effectively we are searching for the best possible values for each objective under the conditions of each scenario i e what is the best that could happen in every scenario the ideal points can be calculated by solving a single scenario single objective problem using for each objective function in each scenario this required solving 4 1000 4000 problems for our case study however the total computation time for solving all these problems is less than a couple of hours on a personal laptop and we only need to calculate the ideal points once indeed the ideal point calculations are related to the best worst case discovery halim et al 2016 table 3 represents the minimum and maximum values for each objective function among the components of the ideal points across all 1000 scenarios i e the best possible values for each objective in the best and worst case scenario describing the best and the worst performances for each objective as seen in this table the corresponding ideal values for reliability in some or at least in one scenarios are very close to 0 min 0 04 this means that in those scenarios there is no feasible solution even in the feasible region of the single scenario single objective problem with a reliability higher than 0 04 which is far less than the domain criterion for reliability at 0 95 similarly the ideal values for pollution in some or at least one scenarios are more than 10 which is also far more than the maximum values of the critical points in any scenarios i e 0 9165 counting the scenarios with the reliability of less than 0 95 indicates 231 1000 scenarios which confirms the claim that no feasible solutions meet the reliability criterion reliability 0 95 in about 23 of the scenarios fig 8 shows the combination of deeply uncertain parameter values listed in table 1 leading to poor performance in the ensemble of 1000 scenarios each dot represents a scenario orange dots correspond to scenarios where the reliability criterion is not met while blue dots belong to scenarios that met the reliability criterion reliability 0 95 similar to the results of the sensitivity analysis of quinn et al 2017 the first plot at the bottom left describes the area in which some nonlinear combination of small values of q and b results in poor performance on the reliability objective of course other uncertain parameters also have some impacts for instance higher values of mean natural pollution can also contribute to a failure on reliability even for higher values of q if b is not large enough overall it seems that for b 0 3 we rarely have a failure on the reliability objective giving the decision maker a new insight into the problem this point is more visible in fig 9 showing the vulnerable combinations of deeply uncertain parameters as seen in this figure only few failures were observable for b 0 3 and none for b 0 34 also no failure was recorded for δ 0 98 investigating the feasible region and ideal values for each objective across an ensemble of scenarios fosters the understanding of the behavior of deeply uncertain parameters in combination with each other the directed search kwakkel 2017 moallemi et al 2020 in some extreme areas of the uncertainty space provides us with detailed insights into the system dynamics in these areas because calculating the ideal values and the proposed scenario analysis do not need any prior knowledge about the solutions and their robustness this kind of analysis can be done even before solution generation thus one can get more insight into the problem and modify the model or preferences if needed before determining solutions potentially saving time and energy 5 discussion as mentioned in the introduction previous variants of mordm only considered a single scenario at a time in the search for the candidate solutions leaving moro aside therefore the feasibility of the generated solutions in a different scenario is questionable and in the best case the solutions are scenario dependent if not infeasible for example quinn et al 2017 added a hard constraint of reliability 0 85 to the intertemporal model of the lake problem in a reference scenario which is the same as s 5 in this study then they solved the four objective optimization model with the borg moea hadka and reed 2013 to generate candidate solutions that were subsequently stress tested re evaluated across 1000 scenarios for robustness analysis they used the domain criterion as their robustness measure and their second criterion was reliability 0 95 they showed that their generated solutions met this criterion only in around 60 of scenarios see figure 8 in quinn et al 2017 that is quite similar to the results described in figs 6 a and 7 however they could not find any failure mechanism on the reliability criterion the reason is that there exists no feasible solution with a reliability 0 95 in around 23 of the scenarios as described in section 4 3 identifying such a failure mechanism is almost impossible if only one scenario is considered in the search phase as another example bartholomew and kwakkel 2020 set the domain criterion of utility 0 75 let us set this criterion as a hard constraint in the model suppose we separately solve the lake problem with this constraint for each selected scenario in that case we cannot find any feasible solution for the optimization problem related to the fourth scenario in which the ideal values for the utility are less than 0 75 see table 2 utility has undesired values in more than 33 of the scenarios therefore if we generate solutions based on any other four scenarios none of the solutions can meet this constraint utility 0 75 in the fourth scenario i e they are infeasible in this scenario even though they are feasible in all other four scenarios accordingly some solutions identified by the previous variant of mordm which does not consider this scenario or some similar scenarios are infeasible in some scenarios in terms of satisfying the constraint of utility 0 75 in other words if the set of scenarios considering as part of the search phase of mordm does not contain scenarios causing a particular type of failure mechanism we cannot find solutions that can cope with this failure in general identifying solutions for some particular scenario cannot guarantee the feasibility of the solutions in any other scenario i e scenario dependent solutions may not be feasible in some other scenarios this feasibility robustness i e the solution is feasible in all or in a wide variety of scenarios is an essential factor that must be somehow checked or guaranteed in the search phase to the best of our knowledge this concept of robustness has not received much attention from the authors developing methods for dealing with deep uncertainty one reason for this may be the particular characteristics of the lake problem as the most popular benchmark problem for methodological developments for decision making under deep uncertainty it is weak in representing the trade offs between scenarios consequently the simultaneous consideration of multiple scenarios within the optimization problem like multi scenario moro helps in verifying the feasibility of the generated solutions in various scenarios during the search process however one cannot consider an infinite number of scenarios therefore it is vital to study the effects of the number of scenarios considered within the optimization problem and explore the trade offs between the overall robustness and optimality in any given scenario this investigation is performed in the next sections 5 1 effects of the number of scenarios in this section we study the effects of different numbers of scenarios within multi scenario moro again we consider the lake problem 10 with four objectives but with a different number of scenarios namely 1 5 9 and 50 scenarios these problems which respectively include 4 20 36 and 200 meta objectives are solved utilizing achievement scalarizing functions with 50 reference points to generate different pareto optimal solutions note that when we compare all generated solutions in a particular dimension of the scenario space e g 1d or single scenario comparisons in table 4 some solutions that were non dominated in the higher dimension may dominate in the lower dimensions and no longer lie in the pareto set these solutions are removed and therefore the number of solutions considered in comparisons may be less than fifty we compare the solutions found through the above mentioned models for the five scenarios described in table 1 in the model with nine scenarios in addition to these five scenarios four extra scenarios the same scenarios utilized by bartholomew and kwakkel 2020 table 2 page 127 are also considered moreover we consider 41 additional scenarios from the set of randomly generated scenarios in the model with 50 scenarios table 4 portrays the results of this comparison the number of scenarios utilizing in the optimization problem has been shown in column 1 while the second column shows the total number of solutions in the pareto set for each model in the 5d scenario space constructing by the first five scenarios the percentage of the solutions that remain in the pareto set when evaluated in each scenario is described in columns 3 to 7 we merge all the solutions found by all models multi scenario multi objective optimization problems with 1 5 9 and 50 scenarios identify the non dominated solutions in each scenario and classify them according to the models that generated them as seen in table 4 in general the percentage of the solutions that remain in the pareto set for any single scenario decreases by increasing the number of scenarios considered within the optimization problem displaying the price of robustness this observation is also in line with the results of bartholomew and kwakkel 2020 moreover the solutions generated by the five scenarios optimization problem perform relatively well especially in the first four scenarios in comparison with the results from the other optimization problems therefore there is not much loss in the optimality in the first four scenarios compared to the solutions generating by the other models in contrast the optimality loss the price of robustness is high in the model with 9 and 50 scenarios note that not all solutions to each single scenario optimization problem remain in the pareto set of that particular scenario mainly because of the stochastic nature of the natural flows in the lake problem this random variation of the natural flows causes some dominance issues in the non dominated sorting calculation i e multiple evaluations of a decision may give rise to some close but not the same values for the objective functions in fact some solutions are dominated because of the random values set by the model for the natural flows in each evaluation not because of the existence of any better solutions this issue also questions the suitability of the lake problem for robustness and trade off comparisons 5 2 robustness over the randomly generated scenarios the domain criterion robustness of the solutions generated by 50 9 5 and single scenario models is presented in fig 10 the larger the number of scenarios considered within the optimization problem especially for more than five scenarios the higher the robustness values on reliability pollution and inertia objectives after re evaluation this means that the optimization problem involving 50 scenarios largely dominates the solutions found for the 9 scenario optimization problem the inverse is observed for the utility objective it seems that by increasing the number of scenarios that is simultaneously considered within the optimization problem the solutions are increasingly biased towards higher reliability at the expense of utility the individual performance of the solutions found for the 5 scenario optimization formulation as shown in fig 10 with blue lines show a moderate behavior in all objective functions the robustness of the solutions produced by the 5 scenario formulation lies somewhere in the middle of the others demonstrating more balanced solutions however utilizing different reference points for each optimization formulation can generate various solutions that can completely change the story indeed the decision maker can steer the solution process towards the area of interest by providing relevant reference points therefore determining the reference points is an important step in multi scenario moro furthermore the trade off diversity between the objectives and the robustness ranges is not visible in fig 10 highlighting the need for a different visualization accordingly fig 11 is used to investigate these two issues as seen in fig 11 a similar pattern is observed across the different optimization formulations for both the domain criterion and the mean standard deviation robustness measure the robustness ranges are almost the same across formulations while the trade off diversity between objectives varies the main reason again is because of the set of chosen reference points to pre determine the reference points we use the method proposed by mueller gritschneder et al 2009 in which the extreme points together with some evenly distributed points on the convex hull of all extreme points are considered as the reference points to ensure diversity nonetheless in higher dimensions i e when we consider more scenarios in the optimization problem to cover the whole space one needs to generate more solutions compared to lower dimensional formulations for example the number of extreme points in the model with four objectives and 1 5 9 and 50 scenarios is 4 20 36 and 200 respectively therefore as we generate 50 solutions for each formulation we cannot cover the whole objective space in the higher dimensions particularly for the 50 scenario formulation this explanation justifies the extreme distribution of the robustness of the solutions generated by the 50 scenarios formulation the lack of diversity in the solutions produced by some single scenario formulations can be justified by the limited search area comparing to the optimization formulations for five and nine scenarios nevertheless once again the particular characteristics of the lake problem prevent further investigations and comparison in the robustness of the solutions generating by different models to sum up there is no significant difference in the robustness of the solutions generated by various models since there is no significant trade off between scenarios in each objective function in the lake problem because the feasible region in all scenarios stays the same any pareto optimal solution can be generated by any model if an appropriate reference point is set therefore determining the reference points and the number of solutions to be generated are more vital than the number of scenarios considered in the optimization model of the lake problem as a special case 5 3 computation cost another vital matter is the computational cost of multi scenario moro bartholomew and kwakkel 2020 giudici et al 2020 we examine the effects of considering more scenarios within the optimization phase of the proposed approach on computational cost the number of function evaluations nfe and the processing time for generating 50 solutions by multi scenario moro with 1 5 9 and 50 scenarios are described in table 5 all these models were solved 50 times once for each reference point in a laptop with intel core i7 cpu and 16 gb ram overall as expected the computation costs increased when the number of scenarios grew during the experiments we noticed that the number of function evaluations and or the time of evaluation varied from one reference point to another for example the most time consuming calculations were related to the reference points that directed the search into the area in which the utility objective is maximizing also the computational cost in most of the extremes was lower than the cost of identifying the more balanced solutions this is the reason why the computation cost in the 50 scenario model was lower than in some other models as discussed earlier in section 4 the solutions generated by multi scenario moro without any additional filtering have a similar robustness to the solutions produced by the previous variants of mordm after extra filtering the proposed multi scenario moro however identified these solutions with fewer function evaluations as seen in table 5 the number of function evaluations was less than 145 000 in s 5 the reference scenario which is much less than 200 000 function evaluations that were used by quinn et al 2017 in mordm moreover the mordm approach used in quinn et al 2017 can hardly generate more robust solutions if ever even with more function evaluations mainly because no information about other scenarios can be considered within its optimization model this is also true in the case of separate consideration of multiple scenarios as performed in multi scenario mordm eker and kwakkel 2018 in contrast by increasing the computational resources like nfe in the proposed multi scenario moro one can include more scenarios within the optimization model that boost the robustness of the generated solutions furthermore as mentioned above we ran the proposed multi scenario moro on a personal laptop which is significantly slower than the high performance computer resources often used to solve different variants of mordm e g as utilized in bartholomew and kwakkel 2020 therefore multi scenario moro is computationally more efficient than the previous approaches for the search phase of mordm while considering more scenarios within the optimization formulation and has other advantages such as representing a wider variety of robustness trade offs 6 conclusions in various disciplines there has been a growing interest in robust multi objective optimization this topic has in parallel been explored by researchers in both mathematical multi objective optimization and decision making under deep uncertainty the former focuses mostly on theory developments while the latter concentrates on practical problems we believe that integrating the developments of these two fields can address some current issues in robust multi objective optimization one of the widely used model based decision support frameworks in dmdu is many objective robust decision making mordm a critical step within the mordm framework is the search phase where candidate solutions are identified using multi objective optimization in this step typically one solves one or more single scenario multi objective optimization problems to produce a large set of promising solutions to be stress tested under uncertainty however this solution set might not be feasible or be dominated in other scenarios as an alternative others have proposed to optimize robustness directly but this leaves the trade off between optimality within individual scenarios and robustness over the scenario set unexplored to address these gaps in this paper we have proposed a new multi scenario multi objective robust optimization approach called multi scenario moro drawing on the concept of scalarizing functions from mathematical multi objective optimization in the novel approach the performance of solutions in terms of all objectives in all selected scenarios is evaluated within a single optimization problem therefore the generated solutions are feasible in all selected scenarios and robust efficient furthermore the proposed multi scenario moro enhances the robustness of the generated solutions reduces scenario dependency and produces a wider variety of robustness trade offs than the previous variants of mordm multi scenario moro also provides the opportunity of exploring trade offs between optimality feasibility in any given scenario and robustness over a broader range of scenarios by considering different numbers of scenarios within the optimization problem which helps the decision maker in discovering balanced solutions the computation cost of multi scenario moro is low compared to previous approaches however there is a need for more experience in different real life environmental problems as we observed in this study the lake problem as a widely used benchmark problem in robustness comparisons cannot reflect the trade offs between scenarios therefore there is a need for new benchmark problems that reflect trade offs between scenarios this will be one of our future research directions we also proposed a novel approach for scenario discovery based on the basic concepts of mathematical multi objective optimization to determine vulnerable scenarios before generating any solution the decision maker can learn about vulnerability and the sources of failures even before policy determination paving the way for considering other solution methods like a priori and particularly interactive multi objective optimization methods miettinen et al 2008 2016 miettinen 1999 this interesting topic is also in our future research interests last but not least in this study we used the inter temporal open loop formulation including static periodical decision variables of the lake problem for demonstrating our method and for comparisons because it is easy to understand and is supposed to present the relationship between scenarios and robustness of solutions nonetheless the proposed multi scenario moro can also be applied to solve adaptive formulations such as direct policy search quinn et al 2017 and planned adaptive direct policy search bartholomew and kwakkel 2020 we also believe that the best way to deal with deep uncertainty is dynamic robustness and adaptive approaches as another interesting future direction our proposed multi scenario moro can also be combined with dynamic adaptive policy pathways haasnoot et al 2013 in this way we can design a dynamic multi stage multi scenario moro approach to identify the best combination of the initial decisions and scenario relevant possible adaptation decisions shavazipour and stewart 2019 shavazipour et al 2020 software availability all code used for this research can be found at https github com industrial optimization group multi scenario multi objective robust optimization under deep uncertainty git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partly funded by the academy of finland grants no 322221 and 311877 this research is related to the thematic research area decision analytics utilizing causal models and multiobjective optimization demo jyu fi demo of the university of jyvaskyla 
25751,with the advancement of computer science artificial intelligence ai is being incorporated into many fields to increase prediction performance disaster management is one of the main fields embracing the techniques of ai it is essential to forecast the occurrence of disasters in advance to take the necessary mitigation steps and reduce damage to life and property therefore many types of research are conducted to predict such events due to climate change in advance using hydrological mathematical and ai based approaches this paper presents a comparison of three major accepted ai based approaches in flood and drought forecasting in this study fluvial floods are measured by the runoff change in rivers whereas meteorological droughts are measured using the standard precipitation index spi the performance of the convolutional neural network cnn long short term memory network lstm and wavelet decomposition functions combined with the adaptive neuro fuzzy inference system wanfis are compared in flood and drought forecasting with five statistical performance criteria and accepted flood and drought indicators used for comparison extending to two climatic regions arid and tropical the results suggest that the cnn performs best in flood forecasting with wanfis for meteorological drought forecasting regardless of the climate of the region under study besides the results demonstrate the increased accuracy of the cnn in applications with multiple features in the input graphical abstract image 1 keywords forecasting floods droughts artificial intelligence convolutional neural network long short term memory network 1 introduction analysis of extreme events has become crucial over time due to their social ecological and economic impacts on society and the environment climate change due to human induced activities has led to fluctuations in the temperature and rainfall patterns resulting in unfavourable circumstances such as sea level rises and extreme events budhathoki et al 2020 and there is an increased risk due to urbanisation and the expansion of urban centres and infrastructures stott 2016 in particular extreme events such as widespread floods and severe droughts are popular among researchers and have been thoroughly observed analysed and forecasted to minimise the consequences water sector decision making in arid and semi arid regions is effectively navigated by an explicit forecasting model for streamflow since it supports the decision makers by providing valuable details to minimise the after effects of extreme events such as floods and droughts on humans and infrastructure osman et al 2020 kisi and cimen 2011 several research studies reveal the ubiquitous occurrence of floods droughts and extensive damage to human lives and various socio economic development arduino et al 2005 the flood in china during 1931 causing the loss of more than 4 million lives is an example of one of the oldest floods recorded courtney 2018 the yellow river flood occurring in 1887 in henan china is another example of a severe flood killing at least 900 000 people trimble et al 2007 the floods occurring during 2019 in thailand were responsible for 815 deaths and 46 5 billion dollars in economic damage affecting 13 6 million people during the same season the floods occurring in australia caused australia s gdp to reduce by 30 billion australian dollars flood costs tipped to top 30b 2011 in 2013 uttarakhand province in northern india was flooded killing 5 748 people and trapping around 300 000 pilgrims kala 2014 the indonesian flood occurring around the same time is another example causing the death of 119 people and destroying more than 110 00 homes droughts on the other hand cause long term unrest for mankind both socially and economically it is estimated that the drought occurring during 2011 in somalia resulted in the deaths of 260 000 living beings taylor 2020 droughts in argentina have been occurring for the past 20 years reducing the country s annual revenue by 400 billion the droughts occurring in cambodia at the end of 2015 affected the lives of 2 5 million people with 25 districts experiencing water shortages south east asia drought 2015 2017 2020 the brazilian drought occurring between 2014 and 2017 has been identified as the deadliest of the century reducing the country s revenue through agriculture by 50 and significantly affecting its gdp at large furthermore the australian sub continent is one of the regions most prone to droughts braganza 2019 the millennium droughts occurring during 2018 were caused by 11 below average rainfall previous research has identified the need for the forecasting of extreme events to establish adaptation and mitigation procedures beforehand to mitigate future damage this forecasting initiative represents an attempt to quantify the magnitude frequency and other miscellaneous factors involved in these events cancelliere et al 2006 mathematical models have been developed using both probabilistic and deterministic approaches for flood forecasting wmo 2011 hence conventional models tend to be categorised as probabilistic merely relying on statistical relationships in past hydrological variables and deterministic models that consider physical processes effects the probabilistic data driven models for flood forecasting use the bayesian and regression approaches which are dependent on time luchetta and maneeti 2003 the auto regressive moving average arma is a reliable example of these hydrological models chung and salas 2000 modified and integrated conventional and bayesian models have been extensively used for flood forecasting li et al 2021 wu et al 2020 in similarity to flood forecasting droughts have been predicted using probability distribution models at the early stage yevjevich 1967 the introduction of palmer s drought index pmdi extended the use of stochastic models to predict droughts rao and padmanabhan 1984 later indicators such as the standard precipitation index spi and standard precipitation evapotranspiration index spei were introduced for use on arma models in drought forecasting azizi et al 2019 mossad and alazba 2015 effective drought index edi also became popular in later research for quantising droughts masinde 2013 the introduction of neural networks into the computer engineering world has taken the study of extreme events to a novel and accurate level before the twentieth century experiments were carried out on the ouse river basin in the united kingdom to forecast river flows an artificial neural network ann has been modelled with a simple backpropagation training algorithm to achieve accuracies of no less than 70 see 1997 subsequently ann models have been modified to achieve more accurate forecasts nayak et al 2005 studies also report that the ann model performs well in tropical and sub tropical climates pradhan et al 2020 the anfis support vector machine svm and wavelet neural networks wnn are some of the major ai techniques recently introduced yaseen et al 2015 shirmohammadi et al 2013 both fis and anfis models have been tested using different training models and architectures and proven to be as effective for forecasting rezaeianzadeh 2013 lohani et al 2014 comparisons with hydrological models such as arma have increased the accuracy and performance of the ai based techniques with the introduction of deep learning researchers have experimented on the performance of models such as convolutional neural networks cnn and recurrent neural networks rnn in flood forecasting padmawar 2019 li et al 2020 li et al 2020 2020 the timeline for drought forecasting takes a similar pattern the use of machine learning techniques such as wnn and anfis has resulted in greater prediction accuracy nourani et al 2014 complex algorithms such as genetic programming gp and the firefly algorithm ffa have also been used to achieve better performance mehr 2014 existing research has drifted towards the use of deep learning techniques such as cnn and long short term memory network lstm for forecasting droughts with spi and spei showing significantly increased accuracy ding et al 2019 poornima and pushpalatha 2019 the ai models require longer development times and different strategies compared to mathematical models although ai based models require a very short execution time to perform a given forecast campolo 1999 besides the ai models show increased accuracy in flood forecasting despite the non linearity in input data mehr 2014 moreover flood forecasting accuracy is increased due to higher lead times one of the most important factors is that the accuracies of the forecasts do not depend on the size of the river basin or the region concerned but rather the patterns in the past data fed into the models kao et al 2020 the increased performance of ai models suggests that their use is preferred over conventional mathematical models however it is evident that most of the past literature compares ai models but lacks performance comparison between the same model in two different climatic regions mehdizadeh et al 2019 ali et al 2018 furthermore the research conducted only focuses on forecasting in a single geographic area where the performance across varying climates has not been studied this study aims to compare three chosen ai techniques cnn lstm and wanfis in the forecasting of floods and droughts as previously mentioned cnn and lstm are proven to be highly accurate models for addressing regression problems the robust performance of these two indicators for sustaining large and contrasting data has also made their application useful besides anfis is a well accepted technique for forecasting problems while the use of wdf in the wanfis increases model performance belayneh 2012 hence these three models have been chosen for this study to determine a superior model for two selected basins the forecasting experiment is applied to two different climatic regions the lower darling river basin located in an arid region and the sekong river basin located in a tropical region although the last major flood in the darling river occurred in 1942 due to its arid climate the river has completely dried up on many occasions especially during 1960 followed by severe droughts in the area darling river history darling river run 2019 the flow rate falling below 16 m3 s in 1993 is a good example of the drought occurrence in the sekong catchment whereas the subsequent floods in 1996 raising the flow rate to 4 000 m3 s is a perfect example meynell 2014 hence the two chosen basins are believed to have varying climates however the floods forecasted are limited to pluvial whereas only meteorological drought forecasting is evaluated the results of this study can help to understand the behaviour and performance of novel ai techniques on different event forecasting while also comparing the effect under different climates 2 materials and methods 2 1 study area the lower darling river basin in australia has been chosen in this study to represent the arid climate while the sekong river basin in south east asia represents the tropical climate the lower darling river is located in new south wales and covers an area of 106 146 km2 this is approximately 3 of the total size of the murray darling river basin the river starts from the menindee lakes leading up to the murray river at wentworth fig 1 a shows the exact location of the lower darling river the sekong catchment starts from the central mountains of vietnam passes through lao pdr and joins the mekong river basin in cambodia covering a total basin area of 28 414 km2 the location of the sekong river catchment is shown in fig 1b the average rainfall in the lower darling river basin is below 280 mm year leaving the climate dry throughout the year the mean daily temperature of the lower darling basin ranges from 30 to 34 c the notable feature of the sekong basin is the mean annual rainfall ranging from 2 300 to 2 700 mm year which is comparatively higher compared to that of the lower darling the mean daily temperature of the sekong basin varies from 21 to 28 c while precipitation accounts for an annual discharge of 102 and 512 m3 s in the lower darling and sekong respectively table 1 compares the major differences between the two basins considering the climate basin area topography settlement and land use 2 2 data as presented in fig 1 eight meteorological stations and four hydrological stations were chosen to represent the lower darling river basin whereas for sekong five meteorological stations and three hydrological stations were used the sources and features of the data collected for training and testing the ai models are illustrated in table 2 the downloaded data were found to be complete and approved by the meteorological departments of the respective countries nine deciles of the total data collected for each basin were then used to train the networks with the remainder used for testing the data collected were divided into two at a ratio of 9 1 to be used for training and testing table 3 denotes the periods of each type of data used for the two applications 2 3 methodology this study focuses on forecasting unit time ahead predictions for both extreme events with the unit time lag determined based on previous literature and data availability flood and drought data collected from both regions were converted into quantitative indicators subsequently the converted data was used to train and validate the models the performance of these models were finally analysed based on several statistical and hydrological indicators the overall methodology of the study is shown in fig 2 2 3 1 quantitative indicators the status of a pluvial flood can also be assessed from several indicators derived from the runoff fernandez et al 2012 hence runoff and rainfall were chosen as quantitative representors of floods in artificial intelligence models the goal was to predict single unit time discharge data after feeding a set of previous discharge and rainfall into the network subject to data availability one day was considered as the unit time lag while quantitative data for seven days were fed to forecast the next day s output however the rainfall was converted into the antecedent precipitation index api before feeding into the network using eq 1 the api conversion followed the original rainfall most accurately with a window of seven days the api is a continuous measure of the wetness of the catchment of interest and is calculated daily the api rises when the most recent rainfall is higher meaning that the rain is likely to runoff it is a recommended tool for use instead of rainfall in flood prediction applications filho et al 2020 1 a p i d i 0 n k n p d n where k is a predefined constant which is equal to 0 92 for arid regions and 0 85 for tropical regions and p d is the rainfall of d th day in mm filho et al 2020 conversion from precipitation to api scaled down the non linearity in precipitation data boosting the accuracy fig 3 shows the format of the input and output vectors used to calibrate and validate the models multiple quantitative indicators exist for droughts hayes et al 1999 since this study only focuses on meteorological droughts spi was chosen to represent the drought status in the ai models spi is an index used to characterise meteorological droughts in a particular range of periods standardized precipitation index spi ncar climate data guide 2020 the derivation of spi consists of a series of steps rainfall data is modelled as a gamma distribution using eq 2 the cumulative probability of each rainfall event is calculated using the distribution probabilities are converted into a standardized normal distribution 2 g x 1 β α f α x α 1 e x β where x 0 is the cardinality of precipitation and f x is the standard gamma function of x the α and β parameters can be estimated using eqs 3 and 4 where α ˆ and β ˆ denote the estimations 3 α ˆ 1 4 x 1 1 4 x 3 4 x 4 β ˆ x α ˆ in similarity to flood forecasting the goal was to predict one step ahead spi followed by an input of spis of a past specific period since this study focuses on meteorological droughts caused by rainfall a shorter time period had to be selected hence tests were carried out selecting one week one month and three months or spi calculation one month spi provided variations similar to the original rainfall data while one week spi provided arbitrary fluctuations in the spi series hence one month was selected the low accuracy observed for one week spi was caused by less data being used to identify patterns in the series besides 24 months of spi data was selected for feeding into the networks to incorporate important flood details into the analysis four proven indicators were selected annual average runoff average discharge calculated annually annual peak runoff maximum discharge for a given year q05 runoff threshold for high flows only more than 0 5 of the total discharge values recorded lie above this discharge value hence a discharge value greater than q05 is expected to be a high flow indicating the strong possibility of a flood q95 runoff threshold for low flows more than 95 of the total discharge values recorded lie above this discharge value hence a discharge value greater than q95 is expected to be a low flow indicating no possibility of flood annual average and peak discharges were calculated using statistical equations with the last two indicators obtained after constructing the flow duration curve for each dataset on the other hand drought severity and duration are two accepted indicators of droughts in previous research zhang et al 2019 severity represents the magnitude of the drought occurring also represented in the spi while the duration refers to the period during which the drought occurs as shown in fig 4 according to zhang et al 2015 an spi value below 1 0 indicates the possibility of moderate drought hence drought severity was calculated as the area between the spi curve and 1 0 with an spi less than 1 0 the drought duration was calculated as the amount of time that the spi was less than 1 0 2 3 2 artificial intelligence model formulation and training the ai models were chosen from previous studies depending on novelty and performance the cnn and lstm are recognised as two major deep learning techniques with increased accuracy and numerous applications besides the anfis is a proven technique used in event forecasting shirmohammadi 2013 confirmed the increased accuracy of anfis outputs when combined with wavelet decomposition functions wdf hence the combination was selected as the third model for this study referred to as wanfis cnn s are optimal architectures designed to detect patterns in 1d and 2d data since they can be customised according to the number and type of layers in the application these architectures can be categorised into two depending on the classification and regression problem in this study cnns were formulated to address the regression problem with architectures formed for the separate forecasting of flood and drought as shown in fig 5 a and b the model used for flood forecasting consists of three hidden layers the first being a 1x2 convolutional layer a fully connected layer was added next with a regression layer used to calculate the final output the drought forecasting cnn was formulated using four hidden layers the input layer was followed by a convolutional layer with a 1x6 filter the output of this layer was averaged using a 1x3 pooling layer in similarity to the flood forecasting model the output of pooling layers was fed to a fully connected layer followed by a regression layer the networks were designed using the neural network toolbox in matlab with root mean square propagation to train the models for 100 epochs data from all the stations were used to train a single network in the case of drought forecasting for each basin however due to the contrasting mean discharge among stations separate networks were used for each station to forecast floods hence four cnns were used for the four stations in lower darling with three for the sekong basin the lstm network is a derivation of the rnn hochreiter and schmidhuber 1997 as the name suggests this model is capable of sustaining previous outputs in the memory for a considerable number of iterations using recurrent layers in a typical vanilla ann an input is converted into a certain output using a particular transformation with the size of both input and outputs fixed however rnn repeats the transformation process using a set of inputs to produce a series of outputs meanwhile the next input is processed based on the previous input and output in the repeated process in similarity to cnn two lstm models were designed for flood forecasting and drought forecasting however the architecture of the model was similar with both addressing a regression problem fig 5c shows the architecture used in this study the input vector was fed into an lstm layer consisting of 50 hidden units the number of hidden units was determined experimentally and found to be equal for both models in similarity to the cnn fully connected and regression layers were added before the output layer the training procedure and number of trained networks were also similar for both models the anfis is a derivation of the takagi sugeno fuzzy inference system fis this network is formed by combining ann with fis the final combination consists of five hidden layers the first layer determines a membership function corresponding to the input which is converted into a strength in the second layer the process in the first layer is also termed fuzzification the third layer normalises these strengths whereas the fourth layer calculates the output strength this strength is converted into an output value in the right units where the process is called defuzzification the input vectors were denoised using the wavelet decomposition functions before feeding into the network the format of the wavelet function is shown in eq 5 5 t m n 2 m n i 0 n 1 g 2 m i n x i where m 3 g 1 and n 2 are constants models for flood and drought forecasting differed only in the first layer the membership functions were obtained for flood inputs using grid partitioning with subtractive clustering used to identify the membership clusters for spi inputs the models were run in matlab using the anfis toolbox with genfis architecture grid partitioning used for flood forecasting including multiple zero valued vectors whereas genfis2 subtractive clustering was incorporated into the drought forecasting models the least squares and gradient descent algorithms were used to train the models up to a maximum of 100 iterations similar numbers of wanfis networks cnns and lstms were used for each application flood and drought forecasting 2 3 3 performance analysis the performance of the models was evaluated using two methods first statistical parameters were used to obtain the correlations and differences between the observed and predicted data from the models the strength of the relationships between the two series were analysed using the coefficient of determination r 2 nash sutcliffe coefficient n s e and wilmott s index w i yaseen et al 2017 lin et al 2017 willmott et al 2011 the errors in the predicted data were quantised using relative absolute error r a e with the relative square error r s e representing the relationship strength and the nse the continuous difference in a series the respective equations for calculating the indices and ranges are shown in table 4 notations n x y and x represent the amount of data observed value predicted value and observed average respectively an r 2 value greater than 0 85 shows a very good relationship between two series where a value greater than 0 75 represents a good relationship rauf and ghumman 2018 the relationship could be considered acceptable if the value is greater than 0 4 on the other hand wi determines the goodness of fit between two data series this is a novel indicator for measuring continuous strength and hence used to validate the results in addition rae and rse relative error metrics for outputting measurements in a certain range were used to quantise the errors produced by the models for the second comparison flood and drought indicators were computed to obtain the observed data in the testing period and the outputs of the three models the differences between the observed and predicted flood indicators were compared mathematically with those for drought compared graphically 3 results and discussion 3 1 performance of flood forecasting models the statistical parameters for comparing the predicted and observed discharges at the lower darling catchment for each station are shown in fig 6 the r 2 values indicate that most of the predictions are very good exceptions can be seen in the lstm and wanfis forecasts for menindee and pooncarie specifically at pooncarie the predictions were observed to be unsatisfactory these observations are well proven by n s e and w i at the burtundy menindee and wentworth stations at pooncarie the lstm forecast was unsatisfactory and the wanfis forecast merely acceptable considering the raw discharge data from all the basins the number of zero valued inputs to the model were observed to affect the accuracy of models the merely acceptable outputs at pooncarie and the high accuracy outputs at wentworth evidence the aforementioned observation the r a e values show that the cnn produced the least square errors between observed and forecasted data at all stations except for wentworth where the r a e values were very small the r s e was observed to provide the lowest cnn outputs at menindee and pooncarie only however the r s e values are negligibly small for all model outputs at burtundy and wentworth comparing the overall results cnn was found to produce the best results in burtundy menindee and pooncarie besides the wanfis produced the best results at wentworth where the average runoff was comparatively higher table 5 shows the percentage difference in flood indicators at the lower darling river basin compared to the observed indicators the cnn was the most accurate for all indicators at burtundy and pooncarie however the difference in predicted high flow thresholds was less compared to those for predicted low flow thresholds at burtundy and pooncarie the lowest percentage difference among the indicators was for the peak discharges the increased accuracy in predicting high runoff values can be observed from these results at karoola the cnn produced the least forecasting difference for q95 and mean discharge more accurate higher runoff values were predicted by the lstm and wanfis at this station the cnn produced the lowest percentage error in the prediction of q05 corresponding to the high flow rate potentially causing floods the peak discharge forecasts were also satisfactory but lower in accuracy compared to the lstm and wanfis however the cnn failed to forecast the lower runoff values at wentworth which discharges a higher mean flow of water compared to the other stations these observations prove that the cnn produced the best forecasts at stations with low mean runoff the results at the outlet were observed to be different nonetheless the cnn provided satisfactory results at the outlet station suggesting that it is the best model out of the three compared for pluvial flood forecasting in arid regions the calculated performance indicators for comparing the flood forecasting models with observed discharges at the sekong river basin are shown in fig 7 very good behaviour is observed in all models at pakse and siem pang as proven by the w i however only r 2 shows a good prediction at atteipu station comparing the indicator outputs between models w i shows that the cnn has the best performance at every station nevertheless r 2 and r m s e show that the cnn works best only at atteipu and pakse stations while at siem pang wanfis produced the best correlation and fit the r s e and r a e indicate that cnn provided the fewest forecasting errors at atteipu and pakse while the r s e disagreed with this observation at siem pang indicating wanfis was the best model the cnn was observed to provide the most accurate forecasting at stations with lower mean runoff while the wanfis forecasted better at stations with higher mean runoff the percentage difference between the predicted and observed flood indicators are shown in table 6 comparatively the cnn produced indicators with the least errors for the testing data at pakse and siem pang stations nevertheless the error percentages are observed to be indirectly proportional to the magnitude of the runoff considering the indicator forecasts at siem pang q95 produced the highest percentage error while those for q05 and peak discharges gradually reduced this pattern is in contrast to that observed at atteipu besides the cnn predicted the high and low flow thresholds most accurately at atteipu while the wanfis predicted annual peak discharge more accurately these results prove that cnn provides high accuracy in forecasting floods at sekong although the statistical parameters showed the highest accuracy in forecasts at the outlet the cnn could also be observed to produce satisfactory results hence the overall results suggest the use of cnn for forecasting pluvial floods in both tropical and arid regions 3 2 performance of drought forecasting models the statistical parameters for evaluating the performance of drought forecasting models at lower darling are presented in fig 8 the r 2 values show that wanfis produced very good forecasts at all stations while the cnn provided both good and very good forecasts however the lstm performance was rated good by r 2 at almost all stations whereas the performance was only satisfactory at pooncarie the variation can be seen in both computed n s e and w i however w i suggests that all three methods produce a very good performance comparisons between the models using these parameters show that the wanfis produced the best performance in spi prediction the error measurements also confirm the above observations regardless of the station both r m s e and m a e indicate that the wanfis produced the least forecasting errors the left portion of the line chart in fig 9 shows the computed drought indicators for observed and forecasted spi series in the lower darling basin the overall plot indicates that both the wanfis and cnn have the least difference compared with the observed specifically wanfis reveals the exact annual severity and duration values at burtundy pooncarie and wentworth stations while cnn shows the least number of differences compared to the observed data at karoola and toora however the lstm projections deviate from the observed in all cases except for toora the detailed plots for drought severity and duration show that lstm projected the occurrence of floods in several years where there had previously been no evidence of floods moreover this analysis demonstrates superior performance by wanfis over cnn when each year is observed separately in conclusion the two indicator sets suggest that the wanfis be used for drought forecasting in arid regions the statistical parameters computed to compare the performance of drought forecasting models at the sekong river basin are shown in fig 10 according to r 2 the forecasts provided by the wanfis showed very good correlation compared to observed data at nikohm km nonghine and siem pang stations as confirmed by both the n s e and w i however according to the r 2 criterion the wanfis only provided merely acceptable forecasts at pakse and atteipu on the other hand cnn produced good results only at nonghine and siem pang according to all three criteria whereas at atteipu and nikohm km the forecasts were only satisfactory it is noticeable from these indicators that lstm only provided merely acceptable results at nonghine the wanfis provided the lowest spi forecasting errors for nikohm km nonghine and siem pang according to both indicators cnn forecasts produced the lowest error at atteipu the r a e plots contradict the outcome of r s e at pakse showing that the wanfis produced fewer forecasting errors than the cnn however considering the overall indicator outputs the wanfis forecasts followed the observed data in most cases fig 11 shows the drought indicators for observed data and model forecasts in the sekong river basin at all stations the wanfis demonstrated the best correlated plot compared to the observed data both the cnn and lstm projected contrasting annual drought severity and drought duration compared to the observed data furthermore the detailed plots drawn yearly confirm that the wanfis demonstrates the best relationship with the least number of incorrect predictions concerning the possibility of floods these results indicate that the wanfis is better than the deep learning models for spi drought forecasting in tropical regions the cnn was found to produce the best performance for pluvial flood forecasting in both regions with contrasting climatic conditions in the case of meteorological drought forecasting the wanfis performed best flood forecasting was conducted by making use of two independent properties rainfall and runoff as inputs whereas only spi was used for drought forecasting as a derivation of rainfall interestingly the cnn performs better in applications with multiple independent properties deep learning models such as the cnn have been found to perform better at forecasting than machine learning models in cases where multiple properties affect the input however the anfis models combined with the wavelet decomposition functions performed better in predicting climate and analysing the patterns of a single property due to the smoothing of input data nevertheless the small differences in statistical performance indicators in the case of flood forecasting in this study were mainly due to less training data being available in the sekong river basin the results confirm the findings of padmawar 2019 who stated that the accuracy of flood identification and forecasting has increased even in the presence of high data non linearity the results also support the ability of the cnn to identify flood features wang et al 2019 however the results do not support the evidence provided by le 2019 that lstms increased accuracy in a similar way to cnn models besides belayneh 2012 claimed that wanfis models provided high forecasting accuracy in the presence of higher lead times while the analysis confirmed that fewer features were present in the drought forecasting models the fact that the introduction of wdf for data preprocessing leads to increased accuracy in ai models was also acknowledged by the study results shirmohammadi 2013 4 conclusions this study evaluated the performance of three well known ai models for pluvial floods and meteorological drought forecasting two regions with contrasting climates were compared namely the lower darling river basin situated in an arid region of australia and the sekong river basin located in a tropical region of cambodia initially the quantitative indicators of floods and droughts were identified the two driving factors of discharge and rainfall were selected to represent floods in ai models while an spi was used to represent droughts the cnn lstm and wanfis were modelled to output unit times ahead of forecasting using a previous dataset the cnn was used to perform flood forecasts and wanfis to perform drought forecasts however the forecasts were limited to one day ahead and one month ahead for floods and droughts respectively this was mainly due to the design of models being unsuitable for predicting both rainfall and discharge the performances were analysed using five statistical parameters and several extreme event indicators for each basin distribution analysis using quartiles and five evaluation criteria namely r 2 n s e w i r s e and r a e were used to assess performance furthermore two generalised indicators of drought severity and duration were used for performance comparison while four generalised indicators were used for flood forecasting the observations revealed that the cnn performed best in flood forecasting whereas the wanfis performed best in drought forecasting it can be concluded that the climatic conditions of an area barely affect the performance of ai models in flood and drought forecasting although the study areas lie in arid and tropical regions all methods demonstrated a similar response for both regions this may be due to the dependency on past climatic data in the region deep learning models such as the cnn are better at forecasting than machine learning models in cases where multiple properties affect the input of the models however anfis models combined with the wavelet decomposition functions tend to perform better in predicting climate and analysing the patterns of a single property due to the smoothing of input data in future studies it is recommended that complex drought indicators such as spei and pdi be used to analyse model performance in hydrological drought forecasting to reduce the limitations involved in flood forecasting this study could be extended by using hourly data and considering snowmelt however the outputs suggest using a machine learning model with wavelet decomposition functions to predict extreme events that can be denoted by a single property in the case of extreme events denoted by multiple properties the results of this study suggest that deep learning models allow the behaviour of such events to be identified declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to express their sincere gratitude to the usaid funded peer project connecting climate change hydrology and fisheries for energy and food security in lower mekong basin thailand and cambodia project 6 436 carried out in co operation with the asian institute of technology stockholm environmental institute inland fisheries research and development institute and arizona state university 
25751,with the advancement of computer science artificial intelligence ai is being incorporated into many fields to increase prediction performance disaster management is one of the main fields embracing the techniques of ai it is essential to forecast the occurrence of disasters in advance to take the necessary mitigation steps and reduce damage to life and property therefore many types of research are conducted to predict such events due to climate change in advance using hydrological mathematical and ai based approaches this paper presents a comparison of three major accepted ai based approaches in flood and drought forecasting in this study fluvial floods are measured by the runoff change in rivers whereas meteorological droughts are measured using the standard precipitation index spi the performance of the convolutional neural network cnn long short term memory network lstm and wavelet decomposition functions combined with the adaptive neuro fuzzy inference system wanfis are compared in flood and drought forecasting with five statistical performance criteria and accepted flood and drought indicators used for comparison extending to two climatic regions arid and tropical the results suggest that the cnn performs best in flood forecasting with wanfis for meteorological drought forecasting regardless of the climate of the region under study besides the results demonstrate the increased accuracy of the cnn in applications with multiple features in the input graphical abstract image 1 keywords forecasting floods droughts artificial intelligence convolutional neural network long short term memory network 1 introduction analysis of extreme events has become crucial over time due to their social ecological and economic impacts on society and the environment climate change due to human induced activities has led to fluctuations in the temperature and rainfall patterns resulting in unfavourable circumstances such as sea level rises and extreme events budhathoki et al 2020 and there is an increased risk due to urbanisation and the expansion of urban centres and infrastructures stott 2016 in particular extreme events such as widespread floods and severe droughts are popular among researchers and have been thoroughly observed analysed and forecasted to minimise the consequences water sector decision making in arid and semi arid regions is effectively navigated by an explicit forecasting model for streamflow since it supports the decision makers by providing valuable details to minimise the after effects of extreme events such as floods and droughts on humans and infrastructure osman et al 2020 kisi and cimen 2011 several research studies reveal the ubiquitous occurrence of floods droughts and extensive damage to human lives and various socio economic development arduino et al 2005 the flood in china during 1931 causing the loss of more than 4 million lives is an example of one of the oldest floods recorded courtney 2018 the yellow river flood occurring in 1887 in henan china is another example of a severe flood killing at least 900 000 people trimble et al 2007 the floods occurring during 2019 in thailand were responsible for 815 deaths and 46 5 billion dollars in economic damage affecting 13 6 million people during the same season the floods occurring in australia caused australia s gdp to reduce by 30 billion australian dollars flood costs tipped to top 30b 2011 in 2013 uttarakhand province in northern india was flooded killing 5 748 people and trapping around 300 000 pilgrims kala 2014 the indonesian flood occurring around the same time is another example causing the death of 119 people and destroying more than 110 00 homes droughts on the other hand cause long term unrest for mankind both socially and economically it is estimated that the drought occurring during 2011 in somalia resulted in the deaths of 260 000 living beings taylor 2020 droughts in argentina have been occurring for the past 20 years reducing the country s annual revenue by 400 billion the droughts occurring in cambodia at the end of 2015 affected the lives of 2 5 million people with 25 districts experiencing water shortages south east asia drought 2015 2017 2020 the brazilian drought occurring between 2014 and 2017 has been identified as the deadliest of the century reducing the country s revenue through agriculture by 50 and significantly affecting its gdp at large furthermore the australian sub continent is one of the regions most prone to droughts braganza 2019 the millennium droughts occurring during 2018 were caused by 11 below average rainfall previous research has identified the need for the forecasting of extreme events to establish adaptation and mitigation procedures beforehand to mitigate future damage this forecasting initiative represents an attempt to quantify the magnitude frequency and other miscellaneous factors involved in these events cancelliere et al 2006 mathematical models have been developed using both probabilistic and deterministic approaches for flood forecasting wmo 2011 hence conventional models tend to be categorised as probabilistic merely relying on statistical relationships in past hydrological variables and deterministic models that consider physical processes effects the probabilistic data driven models for flood forecasting use the bayesian and regression approaches which are dependent on time luchetta and maneeti 2003 the auto regressive moving average arma is a reliable example of these hydrological models chung and salas 2000 modified and integrated conventional and bayesian models have been extensively used for flood forecasting li et al 2021 wu et al 2020 in similarity to flood forecasting droughts have been predicted using probability distribution models at the early stage yevjevich 1967 the introduction of palmer s drought index pmdi extended the use of stochastic models to predict droughts rao and padmanabhan 1984 later indicators such as the standard precipitation index spi and standard precipitation evapotranspiration index spei were introduced for use on arma models in drought forecasting azizi et al 2019 mossad and alazba 2015 effective drought index edi also became popular in later research for quantising droughts masinde 2013 the introduction of neural networks into the computer engineering world has taken the study of extreme events to a novel and accurate level before the twentieth century experiments were carried out on the ouse river basin in the united kingdom to forecast river flows an artificial neural network ann has been modelled with a simple backpropagation training algorithm to achieve accuracies of no less than 70 see 1997 subsequently ann models have been modified to achieve more accurate forecasts nayak et al 2005 studies also report that the ann model performs well in tropical and sub tropical climates pradhan et al 2020 the anfis support vector machine svm and wavelet neural networks wnn are some of the major ai techniques recently introduced yaseen et al 2015 shirmohammadi et al 2013 both fis and anfis models have been tested using different training models and architectures and proven to be as effective for forecasting rezaeianzadeh 2013 lohani et al 2014 comparisons with hydrological models such as arma have increased the accuracy and performance of the ai based techniques with the introduction of deep learning researchers have experimented on the performance of models such as convolutional neural networks cnn and recurrent neural networks rnn in flood forecasting padmawar 2019 li et al 2020 li et al 2020 2020 the timeline for drought forecasting takes a similar pattern the use of machine learning techniques such as wnn and anfis has resulted in greater prediction accuracy nourani et al 2014 complex algorithms such as genetic programming gp and the firefly algorithm ffa have also been used to achieve better performance mehr 2014 existing research has drifted towards the use of deep learning techniques such as cnn and long short term memory network lstm for forecasting droughts with spi and spei showing significantly increased accuracy ding et al 2019 poornima and pushpalatha 2019 the ai models require longer development times and different strategies compared to mathematical models although ai based models require a very short execution time to perform a given forecast campolo 1999 besides the ai models show increased accuracy in flood forecasting despite the non linearity in input data mehr 2014 moreover flood forecasting accuracy is increased due to higher lead times one of the most important factors is that the accuracies of the forecasts do not depend on the size of the river basin or the region concerned but rather the patterns in the past data fed into the models kao et al 2020 the increased performance of ai models suggests that their use is preferred over conventional mathematical models however it is evident that most of the past literature compares ai models but lacks performance comparison between the same model in two different climatic regions mehdizadeh et al 2019 ali et al 2018 furthermore the research conducted only focuses on forecasting in a single geographic area where the performance across varying climates has not been studied this study aims to compare three chosen ai techniques cnn lstm and wanfis in the forecasting of floods and droughts as previously mentioned cnn and lstm are proven to be highly accurate models for addressing regression problems the robust performance of these two indicators for sustaining large and contrasting data has also made their application useful besides anfis is a well accepted technique for forecasting problems while the use of wdf in the wanfis increases model performance belayneh 2012 hence these three models have been chosen for this study to determine a superior model for two selected basins the forecasting experiment is applied to two different climatic regions the lower darling river basin located in an arid region and the sekong river basin located in a tropical region although the last major flood in the darling river occurred in 1942 due to its arid climate the river has completely dried up on many occasions especially during 1960 followed by severe droughts in the area darling river history darling river run 2019 the flow rate falling below 16 m3 s in 1993 is a good example of the drought occurrence in the sekong catchment whereas the subsequent floods in 1996 raising the flow rate to 4 000 m3 s is a perfect example meynell 2014 hence the two chosen basins are believed to have varying climates however the floods forecasted are limited to pluvial whereas only meteorological drought forecasting is evaluated the results of this study can help to understand the behaviour and performance of novel ai techniques on different event forecasting while also comparing the effect under different climates 2 materials and methods 2 1 study area the lower darling river basin in australia has been chosen in this study to represent the arid climate while the sekong river basin in south east asia represents the tropical climate the lower darling river is located in new south wales and covers an area of 106 146 km2 this is approximately 3 of the total size of the murray darling river basin the river starts from the menindee lakes leading up to the murray river at wentworth fig 1 a shows the exact location of the lower darling river the sekong catchment starts from the central mountains of vietnam passes through lao pdr and joins the mekong river basin in cambodia covering a total basin area of 28 414 km2 the location of the sekong river catchment is shown in fig 1b the average rainfall in the lower darling river basin is below 280 mm year leaving the climate dry throughout the year the mean daily temperature of the lower darling basin ranges from 30 to 34 c the notable feature of the sekong basin is the mean annual rainfall ranging from 2 300 to 2 700 mm year which is comparatively higher compared to that of the lower darling the mean daily temperature of the sekong basin varies from 21 to 28 c while precipitation accounts for an annual discharge of 102 and 512 m3 s in the lower darling and sekong respectively table 1 compares the major differences between the two basins considering the climate basin area topography settlement and land use 2 2 data as presented in fig 1 eight meteorological stations and four hydrological stations were chosen to represent the lower darling river basin whereas for sekong five meteorological stations and three hydrological stations were used the sources and features of the data collected for training and testing the ai models are illustrated in table 2 the downloaded data were found to be complete and approved by the meteorological departments of the respective countries nine deciles of the total data collected for each basin were then used to train the networks with the remainder used for testing the data collected were divided into two at a ratio of 9 1 to be used for training and testing table 3 denotes the periods of each type of data used for the two applications 2 3 methodology this study focuses on forecasting unit time ahead predictions for both extreme events with the unit time lag determined based on previous literature and data availability flood and drought data collected from both regions were converted into quantitative indicators subsequently the converted data was used to train and validate the models the performance of these models were finally analysed based on several statistical and hydrological indicators the overall methodology of the study is shown in fig 2 2 3 1 quantitative indicators the status of a pluvial flood can also be assessed from several indicators derived from the runoff fernandez et al 2012 hence runoff and rainfall were chosen as quantitative representors of floods in artificial intelligence models the goal was to predict single unit time discharge data after feeding a set of previous discharge and rainfall into the network subject to data availability one day was considered as the unit time lag while quantitative data for seven days were fed to forecast the next day s output however the rainfall was converted into the antecedent precipitation index api before feeding into the network using eq 1 the api conversion followed the original rainfall most accurately with a window of seven days the api is a continuous measure of the wetness of the catchment of interest and is calculated daily the api rises when the most recent rainfall is higher meaning that the rain is likely to runoff it is a recommended tool for use instead of rainfall in flood prediction applications filho et al 2020 1 a p i d i 0 n k n p d n where k is a predefined constant which is equal to 0 92 for arid regions and 0 85 for tropical regions and p d is the rainfall of d th day in mm filho et al 2020 conversion from precipitation to api scaled down the non linearity in precipitation data boosting the accuracy fig 3 shows the format of the input and output vectors used to calibrate and validate the models multiple quantitative indicators exist for droughts hayes et al 1999 since this study only focuses on meteorological droughts spi was chosen to represent the drought status in the ai models spi is an index used to characterise meteorological droughts in a particular range of periods standardized precipitation index spi ncar climate data guide 2020 the derivation of spi consists of a series of steps rainfall data is modelled as a gamma distribution using eq 2 the cumulative probability of each rainfall event is calculated using the distribution probabilities are converted into a standardized normal distribution 2 g x 1 β α f α x α 1 e x β where x 0 is the cardinality of precipitation and f x is the standard gamma function of x the α and β parameters can be estimated using eqs 3 and 4 where α ˆ and β ˆ denote the estimations 3 α ˆ 1 4 x 1 1 4 x 3 4 x 4 β ˆ x α ˆ in similarity to flood forecasting the goal was to predict one step ahead spi followed by an input of spis of a past specific period since this study focuses on meteorological droughts caused by rainfall a shorter time period had to be selected hence tests were carried out selecting one week one month and three months or spi calculation one month spi provided variations similar to the original rainfall data while one week spi provided arbitrary fluctuations in the spi series hence one month was selected the low accuracy observed for one week spi was caused by less data being used to identify patterns in the series besides 24 months of spi data was selected for feeding into the networks to incorporate important flood details into the analysis four proven indicators were selected annual average runoff average discharge calculated annually annual peak runoff maximum discharge for a given year q05 runoff threshold for high flows only more than 0 5 of the total discharge values recorded lie above this discharge value hence a discharge value greater than q05 is expected to be a high flow indicating the strong possibility of a flood q95 runoff threshold for low flows more than 95 of the total discharge values recorded lie above this discharge value hence a discharge value greater than q95 is expected to be a low flow indicating no possibility of flood annual average and peak discharges were calculated using statistical equations with the last two indicators obtained after constructing the flow duration curve for each dataset on the other hand drought severity and duration are two accepted indicators of droughts in previous research zhang et al 2019 severity represents the magnitude of the drought occurring also represented in the spi while the duration refers to the period during which the drought occurs as shown in fig 4 according to zhang et al 2015 an spi value below 1 0 indicates the possibility of moderate drought hence drought severity was calculated as the area between the spi curve and 1 0 with an spi less than 1 0 the drought duration was calculated as the amount of time that the spi was less than 1 0 2 3 2 artificial intelligence model formulation and training the ai models were chosen from previous studies depending on novelty and performance the cnn and lstm are recognised as two major deep learning techniques with increased accuracy and numerous applications besides the anfis is a proven technique used in event forecasting shirmohammadi 2013 confirmed the increased accuracy of anfis outputs when combined with wavelet decomposition functions wdf hence the combination was selected as the third model for this study referred to as wanfis cnn s are optimal architectures designed to detect patterns in 1d and 2d data since they can be customised according to the number and type of layers in the application these architectures can be categorised into two depending on the classification and regression problem in this study cnns were formulated to address the regression problem with architectures formed for the separate forecasting of flood and drought as shown in fig 5 a and b the model used for flood forecasting consists of three hidden layers the first being a 1x2 convolutional layer a fully connected layer was added next with a regression layer used to calculate the final output the drought forecasting cnn was formulated using four hidden layers the input layer was followed by a convolutional layer with a 1x6 filter the output of this layer was averaged using a 1x3 pooling layer in similarity to the flood forecasting model the output of pooling layers was fed to a fully connected layer followed by a regression layer the networks were designed using the neural network toolbox in matlab with root mean square propagation to train the models for 100 epochs data from all the stations were used to train a single network in the case of drought forecasting for each basin however due to the contrasting mean discharge among stations separate networks were used for each station to forecast floods hence four cnns were used for the four stations in lower darling with three for the sekong basin the lstm network is a derivation of the rnn hochreiter and schmidhuber 1997 as the name suggests this model is capable of sustaining previous outputs in the memory for a considerable number of iterations using recurrent layers in a typical vanilla ann an input is converted into a certain output using a particular transformation with the size of both input and outputs fixed however rnn repeats the transformation process using a set of inputs to produce a series of outputs meanwhile the next input is processed based on the previous input and output in the repeated process in similarity to cnn two lstm models were designed for flood forecasting and drought forecasting however the architecture of the model was similar with both addressing a regression problem fig 5c shows the architecture used in this study the input vector was fed into an lstm layer consisting of 50 hidden units the number of hidden units was determined experimentally and found to be equal for both models in similarity to the cnn fully connected and regression layers were added before the output layer the training procedure and number of trained networks were also similar for both models the anfis is a derivation of the takagi sugeno fuzzy inference system fis this network is formed by combining ann with fis the final combination consists of five hidden layers the first layer determines a membership function corresponding to the input which is converted into a strength in the second layer the process in the first layer is also termed fuzzification the third layer normalises these strengths whereas the fourth layer calculates the output strength this strength is converted into an output value in the right units where the process is called defuzzification the input vectors were denoised using the wavelet decomposition functions before feeding into the network the format of the wavelet function is shown in eq 5 5 t m n 2 m n i 0 n 1 g 2 m i n x i where m 3 g 1 and n 2 are constants models for flood and drought forecasting differed only in the first layer the membership functions were obtained for flood inputs using grid partitioning with subtractive clustering used to identify the membership clusters for spi inputs the models were run in matlab using the anfis toolbox with genfis architecture grid partitioning used for flood forecasting including multiple zero valued vectors whereas genfis2 subtractive clustering was incorporated into the drought forecasting models the least squares and gradient descent algorithms were used to train the models up to a maximum of 100 iterations similar numbers of wanfis networks cnns and lstms were used for each application flood and drought forecasting 2 3 3 performance analysis the performance of the models was evaluated using two methods first statistical parameters were used to obtain the correlations and differences between the observed and predicted data from the models the strength of the relationships between the two series were analysed using the coefficient of determination r 2 nash sutcliffe coefficient n s e and wilmott s index w i yaseen et al 2017 lin et al 2017 willmott et al 2011 the errors in the predicted data were quantised using relative absolute error r a e with the relative square error r s e representing the relationship strength and the nse the continuous difference in a series the respective equations for calculating the indices and ranges are shown in table 4 notations n x y and x represent the amount of data observed value predicted value and observed average respectively an r 2 value greater than 0 85 shows a very good relationship between two series where a value greater than 0 75 represents a good relationship rauf and ghumman 2018 the relationship could be considered acceptable if the value is greater than 0 4 on the other hand wi determines the goodness of fit between two data series this is a novel indicator for measuring continuous strength and hence used to validate the results in addition rae and rse relative error metrics for outputting measurements in a certain range were used to quantise the errors produced by the models for the second comparison flood and drought indicators were computed to obtain the observed data in the testing period and the outputs of the three models the differences between the observed and predicted flood indicators were compared mathematically with those for drought compared graphically 3 results and discussion 3 1 performance of flood forecasting models the statistical parameters for comparing the predicted and observed discharges at the lower darling catchment for each station are shown in fig 6 the r 2 values indicate that most of the predictions are very good exceptions can be seen in the lstm and wanfis forecasts for menindee and pooncarie specifically at pooncarie the predictions were observed to be unsatisfactory these observations are well proven by n s e and w i at the burtundy menindee and wentworth stations at pooncarie the lstm forecast was unsatisfactory and the wanfis forecast merely acceptable considering the raw discharge data from all the basins the number of zero valued inputs to the model were observed to affect the accuracy of models the merely acceptable outputs at pooncarie and the high accuracy outputs at wentworth evidence the aforementioned observation the r a e values show that the cnn produced the least square errors between observed and forecasted data at all stations except for wentworth where the r a e values were very small the r s e was observed to provide the lowest cnn outputs at menindee and pooncarie only however the r s e values are negligibly small for all model outputs at burtundy and wentworth comparing the overall results cnn was found to produce the best results in burtundy menindee and pooncarie besides the wanfis produced the best results at wentworth where the average runoff was comparatively higher table 5 shows the percentage difference in flood indicators at the lower darling river basin compared to the observed indicators the cnn was the most accurate for all indicators at burtundy and pooncarie however the difference in predicted high flow thresholds was less compared to those for predicted low flow thresholds at burtundy and pooncarie the lowest percentage difference among the indicators was for the peak discharges the increased accuracy in predicting high runoff values can be observed from these results at karoola the cnn produced the least forecasting difference for q95 and mean discharge more accurate higher runoff values were predicted by the lstm and wanfis at this station the cnn produced the lowest percentage error in the prediction of q05 corresponding to the high flow rate potentially causing floods the peak discharge forecasts were also satisfactory but lower in accuracy compared to the lstm and wanfis however the cnn failed to forecast the lower runoff values at wentworth which discharges a higher mean flow of water compared to the other stations these observations prove that the cnn produced the best forecasts at stations with low mean runoff the results at the outlet were observed to be different nonetheless the cnn provided satisfactory results at the outlet station suggesting that it is the best model out of the three compared for pluvial flood forecasting in arid regions the calculated performance indicators for comparing the flood forecasting models with observed discharges at the sekong river basin are shown in fig 7 very good behaviour is observed in all models at pakse and siem pang as proven by the w i however only r 2 shows a good prediction at atteipu station comparing the indicator outputs between models w i shows that the cnn has the best performance at every station nevertheless r 2 and r m s e show that the cnn works best only at atteipu and pakse stations while at siem pang wanfis produced the best correlation and fit the r s e and r a e indicate that cnn provided the fewest forecasting errors at atteipu and pakse while the r s e disagreed with this observation at siem pang indicating wanfis was the best model the cnn was observed to provide the most accurate forecasting at stations with lower mean runoff while the wanfis forecasted better at stations with higher mean runoff the percentage difference between the predicted and observed flood indicators are shown in table 6 comparatively the cnn produced indicators with the least errors for the testing data at pakse and siem pang stations nevertheless the error percentages are observed to be indirectly proportional to the magnitude of the runoff considering the indicator forecasts at siem pang q95 produced the highest percentage error while those for q05 and peak discharges gradually reduced this pattern is in contrast to that observed at atteipu besides the cnn predicted the high and low flow thresholds most accurately at atteipu while the wanfis predicted annual peak discharge more accurately these results prove that cnn provides high accuracy in forecasting floods at sekong although the statistical parameters showed the highest accuracy in forecasts at the outlet the cnn could also be observed to produce satisfactory results hence the overall results suggest the use of cnn for forecasting pluvial floods in both tropical and arid regions 3 2 performance of drought forecasting models the statistical parameters for evaluating the performance of drought forecasting models at lower darling are presented in fig 8 the r 2 values show that wanfis produced very good forecasts at all stations while the cnn provided both good and very good forecasts however the lstm performance was rated good by r 2 at almost all stations whereas the performance was only satisfactory at pooncarie the variation can be seen in both computed n s e and w i however w i suggests that all three methods produce a very good performance comparisons between the models using these parameters show that the wanfis produced the best performance in spi prediction the error measurements also confirm the above observations regardless of the station both r m s e and m a e indicate that the wanfis produced the least forecasting errors the left portion of the line chart in fig 9 shows the computed drought indicators for observed and forecasted spi series in the lower darling basin the overall plot indicates that both the wanfis and cnn have the least difference compared with the observed specifically wanfis reveals the exact annual severity and duration values at burtundy pooncarie and wentworth stations while cnn shows the least number of differences compared to the observed data at karoola and toora however the lstm projections deviate from the observed in all cases except for toora the detailed plots for drought severity and duration show that lstm projected the occurrence of floods in several years where there had previously been no evidence of floods moreover this analysis demonstrates superior performance by wanfis over cnn when each year is observed separately in conclusion the two indicator sets suggest that the wanfis be used for drought forecasting in arid regions the statistical parameters computed to compare the performance of drought forecasting models at the sekong river basin are shown in fig 10 according to r 2 the forecasts provided by the wanfis showed very good correlation compared to observed data at nikohm km nonghine and siem pang stations as confirmed by both the n s e and w i however according to the r 2 criterion the wanfis only provided merely acceptable forecasts at pakse and atteipu on the other hand cnn produced good results only at nonghine and siem pang according to all three criteria whereas at atteipu and nikohm km the forecasts were only satisfactory it is noticeable from these indicators that lstm only provided merely acceptable results at nonghine the wanfis provided the lowest spi forecasting errors for nikohm km nonghine and siem pang according to both indicators cnn forecasts produced the lowest error at atteipu the r a e plots contradict the outcome of r s e at pakse showing that the wanfis produced fewer forecasting errors than the cnn however considering the overall indicator outputs the wanfis forecasts followed the observed data in most cases fig 11 shows the drought indicators for observed data and model forecasts in the sekong river basin at all stations the wanfis demonstrated the best correlated plot compared to the observed data both the cnn and lstm projected contrasting annual drought severity and drought duration compared to the observed data furthermore the detailed plots drawn yearly confirm that the wanfis demonstrates the best relationship with the least number of incorrect predictions concerning the possibility of floods these results indicate that the wanfis is better than the deep learning models for spi drought forecasting in tropical regions the cnn was found to produce the best performance for pluvial flood forecasting in both regions with contrasting climatic conditions in the case of meteorological drought forecasting the wanfis performed best flood forecasting was conducted by making use of two independent properties rainfall and runoff as inputs whereas only spi was used for drought forecasting as a derivation of rainfall interestingly the cnn performs better in applications with multiple independent properties deep learning models such as the cnn have been found to perform better at forecasting than machine learning models in cases where multiple properties affect the input however the anfis models combined with the wavelet decomposition functions performed better in predicting climate and analysing the patterns of a single property due to the smoothing of input data nevertheless the small differences in statistical performance indicators in the case of flood forecasting in this study were mainly due to less training data being available in the sekong river basin the results confirm the findings of padmawar 2019 who stated that the accuracy of flood identification and forecasting has increased even in the presence of high data non linearity the results also support the ability of the cnn to identify flood features wang et al 2019 however the results do not support the evidence provided by le 2019 that lstms increased accuracy in a similar way to cnn models besides belayneh 2012 claimed that wanfis models provided high forecasting accuracy in the presence of higher lead times while the analysis confirmed that fewer features were present in the drought forecasting models the fact that the introduction of wdf for data preprocessing leads to increased accuracy in ai models was also acknowledged by the study results shirmohammadi 2013 4 conclusions this study evaluated the performance of three well known ai models for pluvial floods and meteorological drought forecasting two regions with contrasting climates were compared namely the lower darling river basin situated in an arid region of australia and the sekong river basin located in a tropical region of cambodia initially the quantitative indicators of floods and droughts were identified the two driving factors of discharge and rainfall were selected to represent floods in ai models while an spi was used to represent droughts the cnn lstm and wanfis were modelled to output unit times ahead of forecasting using a previous dataset the cnn was used to perform flood forecasts and wanfis to perform drought forecasts however the forecasts were limited to one day ahead and one month ahead for floods and droughts respectively this was mainly due to the design of models being unsuitable for predicting both rainfall and discharge the performances were analysed using five statistical parameters and several extreme event indicators for each basin distribution analysis using quartiles and five evaluation criteria namely r 2 n s e w i r s e and r a e were used to assess performance furthermore two generalised indicators of drought severity and duration were used for performance comparison while four generalised indicators were used for flood forecasting the observations revealed that the cnn performed best in flood forecasting whereas the wanfis performed best in drought forecasting it can be concluded that the climatic conditions of an area barely affect the performance of ai models in flood and drought forecasting although the study areas lie in arid and tropical regions all methods demonstrated a similar response for both regions this may be due to the dependency on past climatic data in the region deep learning models such as the cnn are better at forecasting than machine learning models in cases where multiple properties affect the input of the models however anfis models combined with the wavelet decomposition functions tend to perform better in predicting climate and analysing the patterns of a single property due to the smoothing of input data in future studies it is recommended that complex drought indicators such as spei and pdi be used to analyse model performance in hydrological drought forecasting to reduce the limitations involved in flood forecasting this study could be extended by using hourly data and considering snowmelt however the outputs suggest using a machine learning model with wavelet decomposition functions to predict extreme events that can be denoted by a single property in the case of extreme events denoted by multiple properties the results of this study suggest that deep learning models allow the behaviour of such events to be identified declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to express their sincere gratitude to the usaid funded peer project connecting climate change hydrology and fisheries for energy and food security in lower mekong basin thailand and cambodia project 6 436 carried out in co operation with the asian institute of technology stockholm environmental institute inland fisheries research and development institute and arizona state university 
25752,drainage network extraction is essential for different research and applications however traditional methods have low efficiency low accuracy for flat regions and difficulties in detecting channel heads although deep learning techniques have been used to solve these problems different challenges remain unsolved therefore we introduced distributed representations of aspect features to facilitate the deep learning model calculating the flow direction adopted a semantic segmentation model u net to improve the accuracy and efficiency in predicting flow directions and in pixel classifications and used postprocessing to delineate the flowlines our proposed framework achieved state of the art results compared with the traditional methods and the published deep learning based methods further case study results demonstrated that our framework can extract drainage networks with high accuracy for rivers of different widths flowing through terrains of different characteristics this framework requiring no parameters provided by users can also produce waterbody polygons and allow cyclic graphs in the drainage network keywords drainage network extraction deep learning semantic segmentation digital elevation model 1 introduction drainage networks describe the topography and morphology of channels and catchments bai et al 2015 howard 1994 o callaghan and mark 1984 delineate drainage basins together with geographic features cheng et al 2016 mathew et al 2016 wu et al 2019 and transport water energy and minerals in fluvial processes li et al 2020 o callaghan and mark 1984 in this context drainage networks are crucial for geomorphology o callaghan and mark 1984 and are widely used in different fields such as the basin evolution simulation howard 1994 surface hydrologic modeling sheng et al 2017 vivoni et al 2011 wu et al 2019 river pollutant tracking buchanan et al 2013 wu et al 2019 wu and chen 2013 and regional flood simulation graf 1977 smith et al 2002 due to the importance of drainage networks in these fields many different frameworks methods and algorithms for extracting drainage networks have been developed in the past decades bai et al 2015 band 1986 choi 2012 jenson and domingue 1988 li et al 2020 o callaghan and mark 1984 pelletier 2013 rulli 2010 turcotte et al 2001 wang and liu 2006 wu et al 2019 yoeli 1984 the framework mostly used for drainage network extraction is rooted in the simulation of surface flow from digital elevation models dems and consists of six steps bai et al 2015 wu et al 2019 see fig 1 first the depression filling pretreatment changes the elevation of the pixels to ensure that the surface flow in any pixel can spill out to an outlet pixel bai et al 2015 grimaldi et al 2007 šamanović et al 2016 then for the flow direction determination algorithms like deterministic 8 d8 bai et al 2015 o callaghan and mark 1984 quinn et al 1991 soille and gratin 1994 are used to assign one of the eight flow directions to each pixel next the flow accumulation is used to calculate the sub catchment area which is the upslope area of each pixel bai et al 2015 o callaghan and mark 1984 subsequently drainage channel extraction marks the pixels with areas higher than a stated threshold named the critical source area csa as the channel pixels bai et al 2015 o callaghan and mark 1984 wu et al 2019 later the geographic feature vectorization enables drainage networks to be determined in a vector form bai et al 2015 finally the topographic parameter calculation allows us to derive the h s order and hillslope area bai et al 2015 providing most drainage network products and solving multiple common problems in drainage network extraction this framework or part of it has been used in many different applications jenson and domingue 1988 martz and garbrecht 1992 tarboton et al 1991 however many problems have arisen to prevent the traditional methods from satisfying the needs in practice the first problem is the computational efficiency of the depression filling pretreatment bai et al 2015 in the early works jenson and domingue 1988 o callaghan and mark 1984 depression pixels and their corresponding pouring out paths are found by flow direction extraction and searching algorithms o callaghan and mark 1984 followed by changing the elevation of the pixels along the path to remove the depressions o callaghan and mark 1984 due to the nested searching the computational complexity sipser 1996 of this method reaches o n 2 bai et al 2015 unaffordable when the number of pixels is largely increased as a result of the advent of easy access to the high resolution digital elevation models dems in recent years snyder 2012 xie et al 2020 p 7 although some improvements have been proposed such as simulating a flooding process to make the depression pixels inundated with a complexity of o n 1 5 moran and vezina 1993 planchon and darboux 2002 or exploiting the spill elevation and the least cost search to reduce the complexity to o n l o g n bai et al 2015 wang and liu 2006 wu et al 2019 the depression filling pretreatment is still time consuming the second problem is the unreliability of the critical source area csa method used in drainage channel extraction in detail the channel pixels are argued as being predominantly transported limited with gradual slope decline while in contrast the hillslope pixels are controlled by detachment and undergo parallel retreat howard 1994 these distinctions are hard to be grasped by the size of sub catchments with a single threshold li et al 2020 passalacqua et al 2010 to resolve this problem some studies introduced geometric features such as topographic curvature and used geodesic minimization principles to recognize channel pixels passalacqua et al 2010 sangireddy et al 2016 and other studies used the relationship between the local slope and catchment area howard 1994 montgomery and dietrich 1992 to classify pixels however the first solution only uses a small set of predefined features li et al 2020 while other geometric features also have the potential to be used furthermore the second solution still uses a single threshold which can change with the spatial and temporal variation in the hydrological and erosional processes montgomery and dietrich 1992 the last one is the misalignments of d8 derived drainage networks and the photogrammetrically mapped stream networks lindsay 2016 especially in the flat low relief areas in fine resolution dems wu et al 2019 this is because a pixel in a flat region has eight neighboring pixels of the same elevation but the d8 algorithm assigns the flow direction according to the steepest slope without the dominant steepest slope a predefined default direction will be chosen o callaghan and mark 1984 resulting in incorrect streams delineated to solve this problem several drainage enforcement technologies especially stream burning have been developed graham et al 1999 hellweger and maidment 1997 lindsay 2016 maidment and saunders 1996 saunders and maidment 1995 wu et al 2019 to alter the elevations of pixels in dems based on the surveyed stream networks so as to align the extracted drainage networks with the actual ones lindsay 2016 wu et al 2019 nevertheless the photogrammetrically mapped stream networks are hard to access in many regions the elevation offset of the stream burning is difficult to determine lindsay 2016 incorrect parallel streams and topological errors can be caused by stream burning lindsay 2016 wu et al 2019 and terrain analyses are hindered by stream burning yadav and hatfield 2018 with the fast development of deep learning recently krizhevsky et al 2017 which leads to breakthroughs in processing images video speech and audio lecun et al 2015 we therefore have a solution to the above three problems deep learning models composed of multiple processing layers and can discover the structure in the topographic datasets lecun et al 2015 can achieve great expressive power for mapping dems to drainage networks barron 1994 cybenko 1989 funahashi 1989 hornik et al 1989 lu et al 2017 for instance a set of filters can be adopted to smoothen dems by removing depressions geometric features can be extracted automatically to distinguish channel and hillslope pixels and multi scale information can be integrated together to help navigate flows through flat regions in addition by virtue of local connections zhou et al 2018 shared weights zhou et al 2018 and end to end training of deep learning models the computation involved in the entire data processing will be more efficient in both time and space also in an integrated way with these advantages deep learning technique is adopted to extract drainage information in recent years see stanislawski et al 2018 but the results are still preliminary and limited thereby hindering its practical application for instance the flow direction calculation which involves undifferentiable transformation and is difficult for the deep learning model to calculate to be further discussed in the method section is lacked in addition the sliding window model used in their study i e stanislawski et al 2018 which crops the dem image to small patches and predicts the classes of the central pixels in each patch is inefficient and have difficulty in precisely locating pixels because the convolution used in the model is spatial insensitive chen et al 2014 2017 2018a 2018b also they use one pixel flowlines to train the model however the pixels features inside the waterbody polygons in wide rivers might be similar causing inadequate training and inaccurate results thus we propose a new framework of deep learning enhanced drainage network extraction to resolve these problems in this study which include three parts the input layer the semantic segmentation model and postprocessing we first introduce the distributed representations of aspect features generated from the input layer to facilitate the semantic segmentation model deep learning calculating the flow direction where the aspect is the direction that a slope faces raaflaub and collins 2006 in addition we let the model predict the flow direction and pixel classes for each pixel at the same time to solve the problem of lacking flow direction prediction to tackle the problem caused by the inability of the sliding window model while extracting drainage networks we adopt u net ronneberger et al 2015 which is a sophisticated deep learning semantic segmentation model to classify all the pixels at the same time to help more accurately locate the pixels in addition for the problem caused by using one pixel flowlines for model training we propose an integrated solution where the model can be trained to distinguish the hillslope ground water body and flowlines pixels and the resultant flowlines are delineated through postprocessing 2 dataset and preprocessing as shown in fig 2 we used the national hydrography dataset plus nhdplus high resolution with 10 m spacing in the contiguous united states u s geological survey 2019 covering the regions with hydrologic units from 01 to 18 to train and test the framework proposed in our study in this dataset the 3dep snyder 2012 elevation raster elev cm tif see fig 2 b with a vertical resolution of 100 cm the flow direction raster fdr tif see fig 2 c produced from the hydrologically conditioned dems and the surface water raster swnet tif see fig 2 d classified with hillslope ground flowline and waterbody pixels were mainly used moreover the water boundary vector data wbd were rasterized to act as masks when training and testing models during the training phase the nine flow directions in fig 2 c and the hillslope ground flowline and waterbody pixel classes in fig 3 d were used as the ground truth with the high resolution dem dataset and that the flow direction and pixel class are generated based on the dem refined with removing depressions and stream burning systematic errors can be minimized in our framework however given that adding a constant to the elevations could result in the same drainage network this dataset cannot be directly used in our framework because the absolute elevation has to be normalized moreover the total size of the selected data is too large to be efficiently handled in the computation for the first concern we therefore used the topographic position index tpi de reu et al 2013 stanislawski et al 2018 of elevations which is the difference between a pixel s elevation and its local average elevation inside a 7 p i x e l s 7 p i x e l s square herein around the pixel de reu et al 2013 to reflect the local variation of the elevation then to deal with the second problem we cropped the data into 512 512 pixels 5 12 5 12 k m 2 samples using sliding window without overlapping image overlapping was not used because the national hydrography dataset plus nhdplus high resolution is a large scale dataset which contains enough redundancy and thus the context information loss caused by image splitting is negligible during model training from the cropped samples 504 gigabytes we randomly selected 70 197822 samples as the training dataset 20 56520 samples as the validation dataset and 10 28261 samples as the test dataset through the random selection each sub dataset contained a similar percentage of each type of terrain e g flatland or mountain 3 the proposed framework fig 3 depicts a schematic diagram of the proposed framework of the deep learning enhanced drainage network extraction consisting of three parts the input layer the semantic segmentation model and the post processing the details are delineated below 3 1 input layer as shown in fig 3 a the input layer provides the features of the aspect i e the direction that a slope faces as a part of the input to enhance the deep learning models to extract flow directions the aspect is an important intermediate variable in the flow direction calculation nevertheless deriving an aspect from the gradient using steepest neighbor algorithms popularly used in drainage network extraction needs the operation of argmax which is undifferentiable and hard for the deep learning models to process therefore with the aspect features as the input deep learning models can circumvent the problem of undifferentiable transformation to focus on managing filling depressions and refining aspects to get the flow directions to obtain the aspect features as the input for the deep learning model the deterministic 8 d8 algorithm o callaghan and mark 1984 was used to derive the aspect in the form of one hot vector harris and harris 2015 as shown in fig 3 a in this process we first calculated the slope of a pixel in the eight directions based on its eight neighbors using the following equation 1 d i j δ i δ j e l e v i j e l e v i δ i j δ j 1 d i a g δ i δ j 2 1 1 d i a g δ i δ j δ i δ j 1 0 1 2 a n d δ i δ j 0 in the left of this equation d denotes the slope between point i j and its neighbor i δ i j δ j where the i axis points to the south and j axis points to the east in the right of this equation e l e v i j is the elevation of point i j not the tpi value and 1 d i a g δ i δ j is the indicator function that is equal to 1 if the neighbor indexed by δ i δ j is in the diagonal direction such as in the northwest direction and is equal to 0 for the remaining cases we then selected the direction with the largest non negative slopes and assigned the corresponding one hot vector based on table 1 to each pixel if more than one candidate exists the one with the minimum angle in the clockwise direction with respect to the east direction is chosen however if all the slopes are less than zero the pixel is assigned the vector corresponding to sink and acts as a depression the above procedure has a computational complexity of o n and as the slope calculation for each pixel is decoupled this procedure can be run in parallel to enhance efficiency although the aspect generated by the d8 algorithm can be used directly the associated one hot vector ignores the similarities among directions and become redundant thus we adopt the distributed representation instead which discovers the similarity between data and encodes data into a dense vector mikolov et al 2013 the distributed representation constructs a many to many relationship between the data and vector elements rather than saving all the information in one element as in the one hot vector each element describes one kind of common feature such that the data with similar features tend to be close in the vector space mikolov et al 2013 in addition similar to principal component analysis pca strang 1993 the many to many relationship transforms the data to low dimensional space and saves memories to implement distributed representation we created learnable vectors in three dimensions for each of the nine direction aspects see the distributed representation in table 1 and optimized the vectors using the backpropagation during the model training process devlin et al 2018 vaswani et al 2017 specifically a matrix a with three rows and nine columns was created and the distributed representation of the aspect f d r was calculated using f d r a f o h where the f o h was the one hot vector of the aspect as shown in table 1 and the 27 parameters inside the matrix a were optimized during the training process concatenating the distributed representations of the aspect features with the tpi features generated in the data preprocessing we obtain the input for the following models 3 2 semantic segmentation model as shown in fig 3 b the semantic segmentation model was used to replace sliding window models in drainage network extraction because sliding window models have two drawbacks the first drawback is using a local region patch around a pixel to classify the pixel and not exploiting the redundancy between overlapping patches causing the model to be computationally expensive vigueras guillén et al 2019 the second drawback is in losing location information of the pixels through the downsampling layers and fully connected layers causing a reduction in localization accuracy chen et al 2018a to overcome the first drawback the semantic segmentation model was used to retain the spatial resolution of the input and classify all pixels inside the region at once ronneberger et al 2015 chen et al 2018a as to the second drawback the semantic segmentation model uses the skip layers to recover the location information badrinarayanan et al 2017 ronneberger et al 2015 or reduce the use of the downsampling operation chen et al 2018a in this context semantic segmentation can handle high resolution digital elevation models with high efficiency and can extract drainage network with high quality in this study u net ronneberger et al 2015 was selected as the semantic segmentation model because the encoder decoder structure adopted by u net is reported to produce sharp object boundaries badrinarayanan et al 2017 chen et al 2018b the architecture of u net is depicted in fig 4 where the left and right sides of the u net correspond to the encoder and decoder respectively ronneberger et al 2015 in the encoder the model extracts features using doubleconv blocks and enlarges the receptive field i e the local region used to classify a pixel using maxpooling2d layers ronneberger et al 2015 in the decoder part the model recovers the loss of spatial resolution caused by maxpooling2d layers using bilinear upsampling layers and recovers the location information by skip connection layers the dash line arrows details of these components are discussed below the first component is the doubleconv block which exists in both the encoder and decoder parts and transforms the input and extract semantic features as the output the doubleconv block is constructed from a series of convolution activation and batch normalization layers among them the convolution layer conv2d filters data using the following equation 2 i k x y i m n k x m y n d m d n where i is the 2d data as the input and k is a kernel function represented by a linear transformation the constraint of this equation is k x m y n 0 if m c 1 or n c 2 where c 1 and c 2 are two constants from this equation the output of a pixel can be thought of as a weighted average of its neighboring c 1 c 2 pixels input thus the convolution can be trained to smoothen the input data and reduce the number of artificial depressions furthermore as shown by eq 3 prince 2012 3 x i i k i k x i the derivative of i k which is the smoothed data and can be calculated by applying a convolution to the data i is based on the derivative of the kernel k as k k x i is also a kernel function that can be trained using the backpropagation algorithm the derivative filters can be learned to produce geometric features like the slope and curvature which are useful for drainage network extraction montgomery and dietrich 1992 passalacqua et al 2010 sangireddy et al 2016 the next layer of the doubleconv block is the batch normalization layer batchnorm2d ioffe 2017 ioffe and szegedy 2015 which is used to facilitate the training process and to increase the generality of the models in detail batch normalization first normalizes a feature map using the mean and standard deviation of that feature from a set of samples then applies a learnable linear transformation on the normalized data to produce the result due to this process suppressing the variation of the input data between training steps the optimization process is accelerated and the hyperparameters can be chosen less carefully in the initialization ioffe and szegedy 2015 moreover as this method rescales and shifts each pixel s features equally it can keep the topographic structures and does not affect drainage network extraction the last layer of the doubleconv block is the activation layer that applies a nonlinear transformation to the data by combining multiple convolution and activation layers the model can approximate any borel measurable function barron 1994 cybenko 1989 funahashi 1989 hornik et al 1989 lu et al 2017 therefore useful nonlinear operators for drainage network extraction can be learned such as the perona malik diffusion filter to enhance the valley and ridge features in the dems passalacqua et al 2010 facilitating the drainage network extraction in our study we chose the rectified linear unit relu as the activation layer which returns the maximum between the input value and 0 further relu can prevent gradient exploding vanishing which is harmful to deep learning models and accelerate the training speed xu et al 2015 the second component of the u net is the maxpooling2d layer which exists between two doubleconv blocks in the encoder in this layer four pixels adjacent to each other are merged to one pixel using their maximum value badrinarayanan et al 2017 which corresponds to the important features like the topographic breaklines ridges and streams danielson and gesch 2011 moreover the spatial resolution of the features is downscaled by a factor of two and enlarges the local region that can be used to classify a pixel besides the downscaling also increases the variation of features such as the elevation in the flat relief regions thereby improving the accuracy of the drainage network extraction in such regions the last components of the u net are the blinearupsample layer and the skip connection layer which are between two doubleconv blocks the dash line arrows in the decoder the bilinearupsample layer uses bilinear interpolation to upsample feature maps by a factor of two making the classification of all pixels in the original image possible the skip connection layer concatenates the feature maps in the upstream and the feature maps produced by the bilinearupsample layer the upstream feature maps low level feature maps are less affected by the maxpooling2d layer so they contain more location information in contrast the feature maps obtained by the bilinearupsample layer high level feature maps contain more context information but less location information therefore fusing these two kinds of feature maps can improve both the accuracy of the pixel locating and the classification after processing the input data by a sequence of the above blocks and layers we obtained the feature map which is linearly separable pan and yang 2010 for estimating the flow directions and pixel classes by applying the softmax operation on each pixel we got the probability of the water flow direction in a pixel and the probability of the pixel class i e hillslope waterbody or flowline pixels note that the flow direction prediction and the pixel classification were learned at the same time with shared weights which is multi task learning as what is learned for each task can help other task be learned better caruana 1997 furthermore to optimize these weights cross entropy was used as the loss function for both the flow direction prediction and the pixel classification in particular the cross entropy values for the two tasks were averaged for each pixel and subsequently the resulted values were further averaged over pixels to get the final loss value for model optimization 3 3 postprocessing for delineating flowlines as noted the pixels features inside the waterbody polygons in wide rivers might be similar resulting in the detection of flowline pixels being inefficient and inaccurate therefore in such cases we first let the model extract waterbody polygons i e the union of the waterbody pixels and the flowline pixels extracted by the model and then use postprocessing to get the midlines of the polygons as the flowlines fig 3 c depicts the postprocessing which consists of two parts in the skeletonization part the pixels classified as waterbody or flowline pixels using softmax values see fig 5 a and b were selected and the zhang suen thinning algorithm zhang and suen 1984 was applied to get the midlines however some midlines had no outlet and formed endorheic waterbody which might be noises so we used the endorheic waterbody filter to remove them see fig 5 c in this filter we first calculated the connected components of the midline pixels then removed the components without outlets producing the flowlines see fig 5 d further for narrow rivers without the waterbody pixels the postprocessing will return the flowline pixels directly with the midline being the flowline itself therefore the proposed postprocessing is a general method and can be used in both wide and narrow rivers making the framework more straightforward note that although the pixels inside the waterbody polygons i e combining the waterbody pixels and the flowline pixels are what we intend to obtain the end we still let the semantic segmentation model predict the waterbody pixels and the flowline pixels separately during the training phase the reason is that the flowline and waterbody pixels might be easier to be extracted respectively in the mountain and flat regions that is the features used for the flowline and waterbody pixel extraction could be different therefore letting the deep learning models classify pixels into these two classes in the training phase may improve the accuracy of the waterbody polygons subsequently generated from the union of the two classes 3 4 data imbalance alleviation as indicated in fig 2 the number of the hillslope pixels is 16 times the total number of the flowline and waterbody pixels causing significant data imbalance this affects the drainage network extraction because the deep learning model is optimized using the following equation 4 min θ l θ σ i 1 n l y i ˆ y i n where l θ is the expected value of the loss function l y i ˆ y i and θ represents the parameters to be trained to minimize l θ l y i ˆ y i is based on the predicted class y i ˆ and the ground truth class y i for pixel i and n is the total number of pixels in the dataset from this equation the flowline pixels and waterbody pixels that occupy only a tiny portion of the dataset 6 04 see fig 2 make a little bit of contributions in the optimizing process which in turn prevents the model from recognizing these pixels inside the water polygons and from extracting the drainage network to solve this problem we adopted an undersampling strategy in each training step the hillslope pixels were sampled randomly through which the number of ground pixels for training was set to be equal to the total number of the flowline and waterbody pixels 3 5 settings of model training table 2 details the hyperparameters used in the training process and with these settings all models can converge in three days as shown in table 2 adamw loshchilov and hutter 2017 is chosen to optimize our models weights because it computes adaptive learning rates for each parameter achieving a faster convergence speed kingma and ba 2014 and allowing the effortless selection of hyperparameters furthermore compared with its original version adam kingma and ba 2014 adamw decouples the weight decay function from the optimization steps and applies regularization on the weights with large gradients loshchilov and hutter 2017 improving the generalization performance of the models as the adamw optimizer can benefit from the scheduled learning rate multiplier loshchilov and hutter 2017 we introduce a one cycle learning rate scheduler smith and topin 2017 to the model this scheduler anneals the learning rate from one small value to a large maximum value and then from that value to a very small value during the training process with this scheduler the model training process accelerates and the model s weights get regularized by the large maximum learning rate smith and topin 2017 3 6 evaluation metrics to evaluate our models in predicting both the flow directions and pixel classes we used the metrics described in the following paragraphs however before evaluating the prediction of pixel classes the waterbody pixels and flowline pixels were first merged into a single class as the pixels inside the waterbody polygons the reason is that the midlines of the waterbody polygons were extracted by the postprocessing as the flowlines see section 3 3 and the better the waterbody polygons are generated by the models the better the midlines can be extracted by the postprocessing for approximating the flowlines in this context the metrics based on the pixels inside the waterbody polygons can be used to evaluate the models performance as a whole including the effectiveness of postprocessing merging these two pixel classes in the testing phase is also consistent with traditional drainage network extraction studies e g see wu et al 2019 and du et al 2017 in these studies the polygons generated by buffering the extracted flowlines were used to measure the spatial concurrences between the generated flowlines and the ground truth wu et al 2019 where the polygons generated by the buffering can be considered analogous to the waterbody polygons in our paper we first calculated the accuracy using the following equation 5 a c c y ˆ y σ i 1 n 1 y i ˆ y i n where y ˆ and y are the sets of the predicted results and ground truths and y i ˆ and y i are the predicted and ground truth labels for pixel i 1 is an indicator function returning 1 if the condition inside the parenthesis is satisfied and returning 0 if not and n is the total number of pixels in the dataset the equation estimates the proportion of the labels predicted correctly but it is unable to reflect the improvement of a model compared to the random prediction cohen 1960 mchugh 2012 because a model predicting all pixels as hillslope pixels constituting 93 96 of the dataset will still get high accuracy to achieve a more objective evaluation we applied the undersampling strategy see section 3 4 in the test step of predicting pixel classes in detail the hillslope pixels in the test dataset were randomly selected to make the number equal to the total number of the flowline and waterbody pixels for predicting pixel classes i e pixels inside waterbody polygons furthermore the selected ground pixels in the test dataset were set to be the same for all the model cases so as to make the evaluations be conducted under the same benchmark this strategy also follows the one used in stanislawski et al 2018 further we adopted cohen s kappa coefficient cohen 1960 as another metrics using the following equation wu et al 2019 6 k c a c c σ j 1 k p j ˆ p j 1 σ j 1 k p j ˆ p j where a c c is the accuracy calculated using eq 5 p j ˆ and p j are the predicted and ground truth marginal probabilities of label j the probability that a random sample belongs to label j and k is the number of classes in detail σ j 1 k p j ˆ p j quantifies the proportion of pixels for which the correct prediction is achieved by chance cohen 1960 and a c c σ j 1 k p j ˆ p j estimates how many pixels that cannot be predicted by chance can be predicted correctly by the model from eq 6 the kappa coefficient ranges from 1 to 1 while values below 0 are unlikely in practice cohen 1960 mchugh 2012 and a higher value means a better model mchugh 2012 the range 0 21 0 39 corresponds to minimal agreement between the predictions and the ground truths 0 40 0 59 to weak agreement 0 60 0 79 to moderate agreement 0 80 0 90 to strong agreement and 0 90 1 00 to almost perfection agreement mchugh 2012 besides the above two metrics we also calculated the precision p recall r and f1 scores to evaluate our framework to calculate these metrics the true positive tp false positive fp and false negative fn must be calculated for a selected class the true positive tp is the number of samples correctly predicted as that class the false positive fp is the number of samples that are predicted as that class but do not belong to that class the false negative fn is the number of samples that belong to that class but are predicted as other classes with these numbers precision p recall r and f1 scores can be calculated using the following equations 7 p t p t p f p 8 r t p t p f n 9 f 1 2 p r p r for pixels inside waterbody polygons waterbody pixels flowline pixels p evaluates the proportion of real waterbody polygon pixels inside the predicted waterbody polygon pixels r evaluates the proportion of waterbody polygon pixels that have been found by the model and f 1 is the harmonic mean of p and r and can evaluate the models overall performance 4 results and discussion 4 1 evaluation of the input layer of the proposed framework we first evaluated the performance and effectiveness of the input layer in the proposed framework see fig 3 as the most important output generated from the input layer is the distributed representations of the aspect features we used two cases to evaluate the necessity of obtaining such features as shown in table 3 case 1 is our proposed framework with the distributed representations of the aspect features particularly removing from the output of the input layer and case 2 just follows our proposed framework the comparison of the predicted flow direction from these two cases indicates that introducing the distributed representations of the aspect features helps the framework achieve a better performance in all the evaluation metrics the prediction gains significant improvements of around 7 in the accuracy macro precision macro recall and macro f1 scores the associated kappa value is also increased from the moderate agreement 0 74 to the strong agreement 0 82 furthermore in terms of the pixel classification as shown in table 4 introducing the distributed representations of the aspect features can also help the framework gain a marginal improvement on accuracy wp recall and wp f1 scores the kappa value is also increased from the moderate agreement 0 78 to the strong agreement 0 81 these support our design of the input layer in the framework however there are many other geometric features that could also be introduced to the input layer and therefore whether or not the aspect and tpi features are sufficient for our proposed framework of deep learning enhanced drainage network extraction needs to be further examined for this purpose we designed case 3 which included other geometric features that might be useful for drainage network extraction the first feature we selected for case 3 was the curvature because it acts as an excellent indicator for pixel classifications o neil et al 2020 passalacqua et al 2010 sangireddy et al 2016 where positive values practically mean channel pixels and negative values usually represent hillslope pixels passalacqua et al 2010 the second feature is the slope since the different erosional mechanisms of the hillslopes and the channels produce different slope patterns howard 1994 and slope declines may occur along channels howard 1994 the last feature is c κ s 2 where κ denotes the curvature and s denotes the slope because it may simulate the slope catchment relationship c a s 2 montgomery and dietrich 1992 where a represents the catchment area the geometric curvature rather than the laplacian curvature was used as the former has a higher correlation with the drainage skeleton passalacqua et al 2010 we retrieved the gradient and computed the length of the gradient vector to calculate the slope as shown in tables 3 and 4 introducing these extra features has no significant and even no marginal improvement on both the predicted flow direction and pixel classification the reason might be attributed to that the convolutional layers in the deep learning models can learn to extract those geometric features as shown in the semantic segmentation model in the method section therefore it is plausible to suggest that the aspect and tpi features are sufficient for our proposed framework to extract the drainage network 4 2 evaluation of semantic segmentation model next we evaluated the choice of the semantic segmentation model in our framework fig 3 we chose to use u net because of the merits of the encoder decoder structure badrinarayanan et al 2017 chen et al 2018b however u net is not the only semantic segmentation model that uses the encoder decoder structure and there are also other models that can be used chen et al 2018a 2018b therefore we designed cases 4 and 5 for comparisons in case 4 the segnet badrinarayanan et al 2017 which also has an encoder decoder structure and has been used in wetland identification o neil et al 2020 was adopted to replace the u net the architecture of the segnet is very close to the u net except that the pooling indices instead of the upstream low level feature maps are transported by the skip connection layers badrinarayanan et al 2017 the pooling indices store the locations of the pixels containing the values kept by the maxpooling2d layers so the decoder can upsample data using these indices without the use of bilinear interpolation badrinarayanan et al 2017 as the pooling indices contain information about the object s boundary locations the feature map upsampled in this way is believed to involve the necessary information for recovering the resolution in addition saving indices use less memory than storing feature maps which also help reduce the number of parameters for training badrinarayanan et al 2017 however results in table 3 show that the use of the segnet reduces the accuracy macro precision macro recall and macro f1 scores by around 5 compared with case 2 our proposed framework and it also reduces the kappa to the moderate agreement 0 76 this deterioration shows that the low level feature enhanced bilinear interpolation used in u net maybe a better choice than the pooling indices guided interpolation in segnet this might be due to that some information contained in the low level features that are useful for flow direction extraction is lost while the location information is contained in the pooling indices specifically the pooling indices guided upsampling does not fuse the low level and high level features which in turn causes the feature maps only contains the high level information and the low level features become blurred or even disappear chen et al 2021 as a result the low level features required for the flow direction extraction such as the local slope cannot be used by the segnet thereby giving a low accuracy in predicting the flow direction when compared to the results predicted by the u net further the deeplabv3 chen et al 2018b was used to replace u net in case 5 because the deeplabv3 was reported to achieve state of the art performance in multiple semantic segmentation tasks chen et al 2018b the deeplabv3 replaces the maxpooling2d layers with the atrous convolution layers chen et al 2017 compared to the conventional convolution the atrous convolution has a group of kernel elements set to constant zero therefore the atrous convolution can have a larger kernel size than the convolution with the same number of learnable elements and produces a larger receptive field achieving the same goal as the maxpooling2d layer in light of this the deeplabv3 can maintain the resolution of feature maps without the use of maxpooling2d layer and retain the location information during the process besides the deeplabv3 also organizes atrous convolution layers with different receptive fields in parallel to generate feature maps with multi scale information so the deeplabv3 also has the potential to be used to extract drainage networks in flat regions however the results in table 3 show that the deeplabv3 case 5 has significantly lower performance around 13 in the accuracy macro precision macro recall and macro f1scores than u net case 2 our proposed framework in the flow direction extraction predictions furthermore the kappa value of flow direction extraction of the deeplabv3 is in moderate agreement 0 66 much lower than the u net 0 82 this reduction shows that the atrous convolution may not be suitable for flow direction extraction while satisfying most semantic segmentation tasks needs due to the following reasons the flow direction vectors are continuous over space but the kernel of the atrous convolution is not continuous and may produce discrete results wang et al 2018 in addition the flow direction can change significantly over a small region but the atrous convolution cannot well handle high frequency information yu et al 2017 besides the unsuitableness of atrous convolution we further noticed that the final segmentation map of the deeplabv3 is obtained by bilinearly interpolating the feature map four times directly therefore the local information may be lost through this process bai et al 2021 which also causes the low performance of the deeplabv3 in drainage network extraction 4 3 performance compared to the previous work the above evaluations support our design of the framework and selection of semantic segmentation models therefore we can compare our proposed framework s performance with the published method that also uses deep learning techniques to extract drainage network from digital elevation models stanislawski et al 2018 the previous study used a sliding window model to extract the drainage network which uses the patch local region around a pixel to classify the pixel so we cropped our data samples to 65 65 patches for the model training further as the previous work did not extract the flow direction it had to rely on traditional methods to get the flow direction thus in our case we used the flow direction extracted by the traditional method as well i e steps 1 and 2 in fig 1 to evaluate the performance of the flow direction extraction in the previous study and we use the priority flood method for the depression filling pretreatment zhou et al 2016 case 6 in tables 3 and 4 is the sliding window method with the traditional flow direction extraction and the evaluation is summarized below in terms of flow direction extraction the results in table 3 show that our proposed framework case 2 outperforms case 6 in the accuracy macro precision macro recall and macro f1 scores by around 5 and increases the kappa value from the moderate agreement 0 76 to the strong agreement 0 82 in terms of pixel classifications the results in table 4 show that our proposed framework compared to case 6 has better improvements around 5 and also increases the kappa value from 0 73 to 0 81 note that we also have used nine days to train the sliding window model three times longer than the training time of the semantic segmentation model used in our framework 4 4 evaluation of the performance of the proposed framework in different case studies although our proposed framework outperformed other models or methods in various metrics further evaluations on its applicability for different cases are continued in this section the generated flowlines and the ground truth ones are used in the comparison for different case studies as shown in fig 6 we first implemented our proposed framework on a wide river case the grandfather flowage 2269 on the wisconsin river wisconsin usa as shown in fig 6 a with the grandmother dam in the downstream direction highlighted by the orange rectangle e in fig 6 a the river width varies from 250 m to 600 m together with the small local variations of the elevation standard deviation is 8 07 m in a 26 km2 area it is challenging to extract drainage networks in this region however fig 6 a shows that the results generated by our framework match closely with the ground truth flowlines furthermore our framework produces the correct flowlines around the lake island as indicated by the orange rectangle a in comparison the ground truth ignores the flowlines in the north of the island as the traditional method cannot extract cyclic graphs moreover our method provides a more accurate morphological structure of the river in the orange rectangle c correctly delineates the meandering river in the orange rectangle d and successfully extracts the flowline through the grandmother dam in the orange rectangle b there appears to be a ground truth tributary that has not been recognized by our framework however the tributary is not obvious in the basemap so the results generated from our framework can still be considered acceptable fig 6 b shows a river of medium width the flint river near crisp county georgia usa with the warwick dam in the upstream direction the river width is around 50 m which is five pixels in the dataset in addition the standard deviation of the elevation in this region in a 26 km2 area is 5 95 m even smaller than in the previous case so this region is also challenging for our proposed framework to extract flowlines however the outcomes are quite satisfied the difference between the flowlines obtained by our framework and the ground truth is less than 2 pixels at most of the areas through visual examinations moreover the meandering creeks in the orange rectangles a and b are correctly extracted even if they are covered by a forest the lake area is also successfully connected to the river channel in the orange rectangle c and our framework does not make any false prediction in the engineered lands orange rectangle d these support our framework is capable of dealing with a river of medium width in the flat region and the framework also works well in different complicated environments in terms of topography and morphology besides wide rivers and flat relief regions we also tested our framework for narrow rivers and mountain regions fig 6 c shows the first of narrow rivers at the bob moore creek near lemhi county idaho usa with a standard deviation of elevation at 275 12 m in a 26 km2 area this region is less challenging for extracting drainage networks even for the traditional methods however the channel heads are difficult to detect by the conventional methods with a single threshold as discussed in the introduction section besides it is hard to distinguish roads from the channels in these regions by the traditional methods sangireddy et al 2016 and some roads even cross the river as shown in the orange rectangle c however both of these two problems can be solved by our proposed framework as indicated by the orange rectangles a and b the differences between the channel heads obtained from the framework and the ground truth are less than seven pixels 70 m furthermore as indicated by the orange rectangle c the meandering roads do not bias the framework s predictions and the framework results around the river road junctions are correct and clear considering the good agreement between the framework s outcomes and ground truths our proposed framework also has good performance in this region fig 6 d presents a mountain case located around elkhorn peak bannock county idaho usa with a standard deviation of elevation at 204 95 m in a 26 km2 area similar to fig 6 c the flowlines generated by our framework are close to the ground truths and the channel heads as shown in the orange rectangles a and b are predicted with an error of less than 100 m furthermore the prediction of the framework is not biased by an engineered land farmland in the orange rectangle c and the flowlines can still be correctly extracted 4 5 evaluation of the efficiency of the proposed framework our proposed framework only takes less than 10 min to extract the drainage network in the test dataset that contains 7 408 451 584 pixels 740845 16 km2 in our facility so it will take about 30 h to extract the global drainage networks which is much more efficient than the traditional framework 5 conclusions this study proposed a new framework of deep learning enhanced drainage network extraction including three parts the input layer the semantic segmentation model deep learning model and postprocessing for delineating flowlines we introduced the distributed representations of the aspect features generated from the input layer to facilitate the semantic segmentation model deep learning model calculating the flow direction after comparing the performance of different semantic segmentation models we adopted u net which was trained to distinguish the hillslope ground water body and flowlines pixels and classify all the pixels in the image same time to locate the pixels more accurately also we let the model predict the flow direction and pixel classes for each pixel at the same time to solve the problem of lacking flow direction prediction the resultant flowlines are delineated through postprocessing including skeletonization and endorheic waterbody filtering the proposed framework achieves state of the art performance compared to previous studies using deep learning techniques in extracting drainage networks from the digital elevation models through different case studies and comparisons with associated ground truths the proposed framework has demonstrated the capability of extracting flowlines channel lines with high accuracy for rivers of different widths flowing through regions with different features of terrains and even through engineered landscapes the errors of the channel heads obtained by this framework are also low these support the supremacy effectiveness and accuracy of our proposed framework in drainage network extraction further this proposed framework has several advantages compared to the traditional methods the first advantage is that the framework can extract the waterbody polygons with high accuracy in contrast most of the traditional methods except for some methods using geometric features e g sangireddy et al 2016 extract only the flowlines therefore the proposed framework can provide more morphological information which is of benefit in relevant researches and applications the second advantage is that the drainage network extracted by the proposed framework allows cyclic graphs see fig 6a while the traditional methods tend to produce dendritic graphs as cyclic graphs are inevitable in a river system our framework s drainage network output is more objective the third advantage is that the proposed framework needs no threshold values or parameters provided by the user so this framework can be applied to automatically process large scale datasets thereby costing less and achieving more the fourth advantage is that this framework is very efficient as it will just take about 30 h to extract the global drainage networks these advantages provide broad development and application opportunities for the proposed framework software availability name deeplearningdraingenetworkextractor software required gdal 2 2 3 opencv 3 2 0 python 3 7 9 python packages pytorch 1 4 0 torchvision 0 5 0 pytorch lightning 1 0 6 gdal numpy opencv python opencv contrib python scipy tqdm networkx richdem availability https github com maoxin deep learning drainage network extractor git declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests yu hsing wang reports financial support was provided by the innovation technology fund of hksar yu hsing wang reports financial support was provided by the hkust smart sustainable campus project tiejian li reports financial support was provided by the national key research and development program of china acknowledgments this study was supported by the innovation technology fund its 101 19 of hksar the hkust smart sustainable campus ssc project and the national key research and development program of china 2016yfe0201900 the authors are grateful to the reviewers for their valuable comments any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the authors or funders 
25752,drainage network extraction is essential for different research and applications however traditional methods have low efficiency low accuracy for flat regions and difficulties in detecting channel heads although deep learning techniques have been used to solve these problems different challenges remain unsolved therefore we introduced distributed representations of aspect features to facilitate the deep learning model calculating the flow direction adopted a semantic segmentation model u net to improve the accuracy and efficiency in predicting flow directions and in pixel classifications and used postprocessing to delineate the flowlines our proposed framework achieved state of the art results compared with the traditional methods and the published deep learning based methods further case study results demonstrated that our framework can extract drainage networks with high accuracy for rivers of different widths flowing through terrains of different characteristics this framework requiring no parameters provided by users can also produce waterbody polygons and allow cyclic graphs in the drainage network keywords drainage network extraction deep learning semantic segmentation digital elevation model 1 introduction drainage networks describe the topography and morphology of channels and catchments bai et al 2015 howard 1994 o callaghan and mark 1984 delineate drainage basins together with geographic features cheng et al 2016 mathew et al 2016 wu et al 2019 and transport water energy and minerals in fluvial processes li et al 2020 o callaghan and mark 1984 in this context drainage networks are crucial for geomorphology o callaghan and mark 1984 and are widely used in different fields such as the basin evolution simulation howard 1994 surface hydrologic modeling sheng et al 2017 vivoni et al 2011 wu et al 2019 river pollutant tracking buchanan et al 2013 wu et al 2019 wu and chen 2013 and regional flood simulation graf 1977 smith et al 2002 due to the importance of drainage networks in these fields many different frameworks methods and algorithms for extracting drainage networks have been developed in the past decades bai et al 2015 band 1986 choi 2012 jenson and domingue 1988 li et al 2020 o callaghan and mark 1984 pelletier 2013 rulli 2010 turcotte et al 2001 wang and liu 2006 wu et al 2019 yoeli 1984 the framework mostly used for drainage network extraction is rooted in the simulation of surface flow from digital elevation models dems and consists of six steps bai et al 2015 wu et al 2019 see fig 1 first the depression filling pretreatment changes the elevation of the pixels to ensure that the surface flow in any pixel can spill out to an outlet pixel bai et al 2015 grimaldi et al 2007 šamanović et al 2016 then for the flow direction determination algorithms like deterministic 8 d8 bai et al 2015 o callaghan and mark 1984 quinn et al 1991 soille and gratin 1994 are used to assign one of the eight flow directions to each pixel next the flow accumulation is used to calculate the sub catchment area which is the upslope area of each pixel bai et al 2015 o callaghan and mark 1984 subsequently drainage channel extraction marks the pixels with areas higher than a stated threshold named the critical source area csa as the channel pixels bai et al 2015 o callaghan and mark 1984 wu et al 2019 later the geographic feature vectorization enables drainage networks to be determined in a vector form bai et al 2015 finally the topographic parameter calculation allows us to derive the h s order and hillslope area bai et al 2015 providing most drainage network products and solving multiple common problems in drainage network extraction this framework or part of it has been used in many different applications jenson and domingue 1988 martz and garbrecht 1992 tarboton et al 1991 however many problems have arisen to prevent the traditional methods from satisfying the needs in practice the first problem is the computational efficiency of the depression filling pretreatment bai et al 2015 in the early works jenson and domingue 1988 o callaghan and mark 1984 depression pixels and their corresponding pouring out paths are found by flow direction extraction and searching algorithms o callaghan and mark 1984 followed by changing the elevation of the pixels along the path to remove the depressions o callaghan and mark 1984 due to the nested searching the computational complexity sipser 1996 of this method reaches o n 2 bai et al 2015 unaffordable when the number of pixels is largely increased as a result of the advent of easy access to the high resolution digital elevation models dems in recent years snyder 2012 xie et al 2020 p 7 although some improvements have been proposed such as simulating a flooding process to make the depression pixels inundated with a complexity of o n 1 5 moran and vezina 1993 planchon and darboux 2002 or exploiting the spill elevation and the least cost search to reduce the complexity to o n l o g n bai et al 2015 wang and liu 2006 wu et al 2019 the depression filling pretreatment is still time consuming the second problem is the unreliability of the critical source area csa method used in drainage channel extraction in detail the channel pixels are argued as being predominantly transported limited with gradual slope decline while in contrast the hillslope pixels are controlled by detachment and undergo parallel retreat howard 1994 these distinctions are hard to be grasped by the size of sub catchments with a single threshold li et al 2020 passalacqua et al 2010 to resolve this problem some studies introduced geometric features such as topographic curvature and used geodesic minimization principles to recognize channel pixels passalacqua et al 2010 sangireddy et al 2016 and other studies used the relationship between the local slope and catchment area howard 1994 montgomery and dietrich 1992 to classify pixels however the first solution only uses a small set of predefined features li et al 2020 while other geometric features also have the potential to be used furthermore the second solution still uses a single threshold which can change with the spatial and temporal variation in the hydrological and erosional processes montgomery and dietrich 1992 the last one is the misalignments of d8 derived drainage networks and the photogrammetrically mapped stream networks lindsay 2016 especially in the flat low relief areas in fine resolution dems wu et al 2019 this is because a pixel in a flat region has eight neighboring pixels of the same elevation but the d8 algorithm assigns the flow direction according to the steepest slope without the dominant steepest slope a predefined default direction will be chosen o callaghan and mark 1984 resulting in incorrect streams delineated to solve this problem several drainage enforcement technologies especially stream burning have been developed graham et al 1999 hellweger and maidment 1997 lindsay 2016 maidment and saunders 1996 saunders and maidment 1995 wu et al 2019 to alter the elevations of pixels in dems based on the surveyed stream networks so as to align the extracted drainage networks with the actual ones lindsay 2016 wu et al 2019 nevertheless the photogrammetrically mapped stream networks are hard to access in many regions the elevation offset of the stream burning is difficult to determine lindsay 2016 incorrect parallel streams and topological errors can be caused by stream burning lindsay 2016 wu et al 2019 and terrain analyses are hindered by stream burning yadav and hatfield 2018 with the fast development of deep learning recently krizhevsky et al 2017 which leads to breakthroughs in processing images video speech and audio lecun et al 2015 we therefore have a solution to the above three problems deep learning models composed of multiple processing layers and can discover the structure in the topographic datasets lecun et al 2015 can achieve great expressive power for mapping dems to drainage networks barron 1994 cybenko 1989 funahashi 1989 hornik et al 1989 lu et al 2017 for instance a set of filters can be adopted to smoothen dems by removing depressions geometric features can be extracted automatically to distinguish channel and hillslope pixels and multi scale information can be integrated together to help navigate flows through flat regions in addition by virtue of local connections zhou et al 2018 shared weights zhou et al 2018 and end to end training of deep learning models the computation involved in the entire data processing will be more efficient in both time and space also in an integrated way with these advantages deep learning technique is adopted to extract drainage information in recent years see stanislawski et al 2018 but the results are still preliminary and limited thereby hindering its practical application for instance the flow direction calculation which involves undifferentiable transformation and is difficult for the deep learning model to calculate to be further discussed in the method section is lacked in addition the sliding window model used in their study i e stanislawski et al 2018 which crops the dem image to small patches and predicts the classes of the central pixels in each patch is inefficient and have difficulty in precisely locating pixels because the convolution used in the model is spatial insensitive chen et al 2014 2017 2018a 2018b also they use one pixel flowlines to train the model however the pixels features inside the waterbody polygons in wide rivers might be similar causing inadequate training and inaccurate results thus we propose a new framework of deep learning enhanced drainage network extraction to resolve these problems in this study which include three parts the input layer the semantic segmentation model and postprocessing we first introduce the distributed representations of aspect features generated from the input layer to facilitate the semantic segmentation model deep learning calculating the flow direction where the aspect is the direction that a slope faces raaflaub and collins 2006 in addition we let the model predict the flow direction and pixel classes for each pixel at the same time to solve the problem of lacking flow direction prediction to tackle the problem caused by the inability of the sliding window model while extracting drainage networks we adopt u net ronneberger et al 2015 which is a sophisticated deep learning semantic segmentation model to classify all the pixels at the same time to help more accurately locate the pixels in addition for the problem caused by using one pixel flowlines for model training we propose an integrated solution where the model can be trained to distinguish the hillslope ground water body and flowlines pixels and the resultant flowlines are delineated through postprocessing 2 dataset and preprocessing as shown in fig 2 we used the national hydrography dataset plus nhdplus high resolution with 10 m spacing in the contiguous united states u s geological survey 2019 covering the regions with hydrologic units from 01 to 18 to train and test the framework proposed in our study in this dataset the 3dep snyder 2012 elevation raster elev cm tif see fig 2 b with a vertical resolution of 100 cm the flow direction raster fdr tif see fig 2 c produced from the hydrologically conditioned dems and the surface water raster swnet tif see fig 2 d classified with hillslope ground flowline and waterbody pixels were mainly used moreover the water boundary vector data wbd were rasterized to act as masks when training and testing models during the training phase the nine flow directions in fig 2 c and the hillslope ground flowline and waterbody pixel classes in fig 3 d were used as the ground truth with the high resolution dem dataset and that the flow direction and pixel class are generated based on the dem refined with removing depressions and stream burning systematic errors can be minimized in our framework however given that adding a constant to the elevations could result in the same drainage network this dataset cannot be directly used in our framework because the absolute elevation has to be normalized moreover the total size of the selected data is too large to be efficiently handled in the computation for the first concern we therefore used the topographic position index tpi de reu et al 2013 stanislawski et al 2018 of elevations which is the difference between a pixel s elevation and its local average elevation inside a 7 p i x e l s 7 p i x e l s square herein around the pixel de reu et al 2013 to reflect the local variation of the elevation then to deal with the second problem we cropped the data into 512 512 pixels 5 12 5 12 k m 2 samples using sliding window without overlapping image overlapping was not used because the national hydrography dataset plus nhdplus high resolution is a large scale dataset which contains enough redundancy and thus the context information loss caused by image splitting is negligible during model training from the cropped samples 504 gigabytes we randomly selected 70 197822 samples as the training dataset 20 56520 samples as the validation dataset and 10 28261 samples as the test dataset through the random selection each sub dataset contained a similar percentage of each type of terrain e g flatland or mountain 3 the proposed framework fig 3 depicts a schematic diagram of the proposed framework of the deep learning enhanced drainage network extraction consisting of three parts the input layer the semantic segmentation model and the post processing the details are delineated below 3 1 input layer as shown in fig 3 a the input layer provides the features of the aspect i e the direction that a slope faces as a part of the input to enhance the deep learning models to extract flow directions the aspect is an important intermediate variable in the flow direction calculation nevertheless deriving an aspect from the gradient using steepest neighbor algorithms popularly used in drainage network extraction needs the operation of argmax which is undifferentiable and hard for the deep learning models to process therefore with the aspect features as the input deep learning models can circumvent the problem of undifferentiable transformation to focus on managing filling depressions and refining aspects to get the flow directions to obtain the aspect features as the input for the deep learning model the deterministic 8 d8 algorithm o callaghan and mark 1984 was used to derive the aspect in the form of one hot vector harris and harris 2015 as shown in fig 3 a in this process we first calculated the slope of a pixel in the eight directions based on its eight neighbors using the following equation 1 d i j δ i δ j e l e v i j e l e v i δ i j δ j 1 d i a g δ i δ j 2 1 1 d i a g δ i δ j δ i δ j 1 0 1 2 a n d δ i δ j 0 in the left of this equation d denotes the slope between point i j and its neighbor i δ i j δ j where the i axis points to the south and j axis points to the east in the right of this equation e l e v i j is the elevation of point i j not the tpi value and 1 d i a g δ i δ j is the indicator function that is equal to 1 if the neighbor indexed by δ i δ j is in the diagonal direction such as in the northwest direction and is equal to 0 for the remaining cases we then selected the direction with the largest non negative slopes and assigned the corresponding one hot vector based on table 1 to each pixel if more than one candidate exists the one with the minimum angle in the clockwise direction with respect to the east direction is chosen however if all the slopes are less than zero the pixel is assigned the vector corresponding to sink and acts as a depression the above procedure has a computational complexity of o n and as the slope calculation for each pixel is decoupled this procedure can be run in parallel to enhance efficiency although the aspect generated by the d8 algorithm can be used directly the associated one hot vector ignores the similarities among directions and become redundant thus we adopt the distributed representation instead which discovers the similarity between data and encodes data into a dense vector mikolov et al 2013 the distributed representation constructs a many to many relationship between the data and vector elements rather than saving all the information in one element as in the one hot vector each element describes one kind of common feature such that the data with similar features tend to be close in the vector space mikolov et al 2013 in addition similar to principal component analysis pca strang 1993 the many to many relationship transforms the data to low dimensional space and saves memories to implement distributed representation we created learnable vectors in three dimensions for each of the nine direction aspects see the distributed representation in table 1 and optimized the vectors using the backpropagation during the model training process devlin et al 2018 vaswani et al 2017 specifically a matrix a with three rows and nine columns was created and the distributed representation of the aspect f d r was calculated using f d r a f o h where the f o h was the one hot vector of the aspect as shown in table 1 and the 27 parameters inside the matrix a were optimized during the training process concatenating the distributed representations of the aspect features with the tpi features generated in the data preprocessing we obtain the input for the following models 3 2 semantic segmentation model as shown in fig 3 b the semantic segmentation model was used to replace sliding window models in drainage network extraction because sliding window models have two drawbacks the first drawback is using a local region patch around a pixel to classify the pixel and not exploiting the redundancy between overlapping patches causing the model to be computationally expensive vigueras guillén et al 2019 the second drawback is in losing location information of the pixels through the downsampling layers and fully connected layers causing a reduction in localization accuracy chen et al 2018a to overcome the first drawback the semantic segmentation model was used to retain the spatial resolution of the input and classify all pixels inside the region at once ronneberger et al 2015 chen et al 2018a as to the second drawback the semantic segmentation model uses the skip layers to recover the location information badrinarayanan et al 2017 ronneberger et al 2015 or reduce the use of the downsampling operation chen et al 2018a in this context semantic segmentation can handle high resolution digital elevation models with high efficiency and can extract drainage network with high quality in this study u net ronneberger et al 2015 was selected as the semantic segmentation model because the encoder decoder structure adopted by u net is reported to produce sharp object boundaries badrinarayanan et al 2017 chen et al 2018b the architecture of u net is depicted in fig 4 where the left and right sides of the u net correspond to the encoder and decoder respectively ronneberger et al 2015 in the encoder the model extracts features using doubleconv blocks and enlarges the receptive field i e the local region used to classify a pixel using maxpooling2d layers ronneberger et al 2015 in the decoder part the model recovers the loss of spatial resolution caused by maxpooling2d layers using bilinear upsampling layers and recovers the location information by skip connection layers the dash line arrows details of these components are discussed below the first component is the doubleconv block which exists in both the encoder and decoder parts and transforms the input and extract semantic features as the output the doubleconv block is constructed from a series of convolution activation and batch normalization layers among them the convolution layer conv2d filters data using the following equation 2 i k x y i m n k x m y n d m d n where i is the 2d data as the input and k is a kernel function represented by a linear transformation the constraint of this equation is k x m y n 0 if m c 1 or n c 2 where c 1 and c 2 are two constants from this equation the output of a pixel can be thought of as a weighted average of its neighboring c 1 c 2 pixels input thus the convolution can be trained to smoothen the input data and reduce the number of artificial depressions furthermore as shown by eq 3 prince 2012 3 x i i k i k x i the derivative of i k which is the smoothed data and can be calculated by applying a convolution to the data i is based on the derivative of the kernel k as k k x i is also a kernel function that can be trained using the backpropagation algorithm the derivative filters can be learned to produce geometric features like the slope and curvature which are useful for drainage network extraction montgomery and dietrich 1992 passalacqua et al 2010 sangireddy et al 2016 the next layer of the doubleconv block is the batch normalization layer batchnorm2d ioffe 2017 ioffe and szegedy 2015 which is used to facilitate the training process and to increase the generality of the models in detail batch normalization first normalizes a feature map using the mean and standard deviation of that feature from a set of samples then applies a learnable linear transformation on the normalized data to produce the result due to this process suppressing the variation of the input data between training steps the optimization process is accelerated and the hyperparameters can be chosen less carefully in the initialization ioffe and szegedy 2015 moreover as this method rescales and shifts each pixel s features equally it can keep the topographic structures and does not affect drainage network extraction the last layer of the doubleconv block is the activation layer that applies a nonlinear transformation to the data by combining multiple convolution and activation layers the model can approximate any borel measurable function barron 1994 cybenko 1989 funahashi 1989 hornik et al 1989 lu et al 2017 therefore useful nonlinear operators for drainage network extraction can be learned such as the perona malik diffusion filter to enhance the valley and ridge features in the dems passalacqua et al 2010 facilitating the drainage network extraction in our study we chose the rectified linear unit relu as the activation layer which returns the maximum between the input value and 0 further relu can prevent gradient exploding vanishing which is harmful to deep learning models and accelerate the training speed xu et al 2015 the second component of the u net is the maxpooling2d layer which exists between two doubleconv blocks in the encoder in this layer four pixels adjacent to each other are merged to one pixel using their maximum value badrinarayanan et al 2017 which corresponds to the important features like the topographic breaklines ridges and streams danielson and gesch 2011 moreover the spatial resolution of the features is downscaled by a factor of two and enlarges the local region that can be used to classify a pixel besides the downscaling also increases the variation of features such as the elevation in the flat relief regions thereby improving the accuracy of the drainage network extraction in such regions the last components of the u net are the blinearupsample layer and the skip connection layer which are between two doubleconv blocks the dash line arrows in the decoder the bilinearupsample layer uses bilinear interpolation to upsample feature maps by a factor of two making the classification of all pixels in the original image possible the skip connection layer concatenates the feature maps in the upstream and the feature maps produced by the bilinearupsample layer the upstream feature maps low level feature maps are less affected by the maxpooling2d layer so they contain more location information in contrast the feature maps obtained by the bilinearupsample layer high level feature maps contain more context information but less location information therefore fusing these two kinds of feature maps can improve both the accuracy of the pixel locating and the classification after processing the input data by a sequence of the above blocks and layers we obtained the feature map which is linearly separable pan and yang 2010 for estimating the flow directions and pixel classes by applying the softmax operation on each pixel we got the probability of the water flow direction in a pixel and the probability of the pixel class i e hillslope waterbody or flowline pixels note that the flow direction prediction and the pixel classification were learned at the same time with shared weights which is multi task learning as what is learned for each task can help other task be learned better caruana 1997 furthermore to optimize these weights cross entropy was used as the loss function for both the flow direction prediction and the pixel classification in particular the cross entropy values for the two tasks were averaged for each pixel and subsequently the resulted values were further averaged over pixels to get the final loss value for model optimization 3 3 postprocessing for delineating flowlines as noted the pixels features inside the waterbody polygons in wide rivers might be similar resulting in the detection of flowline pixels being inefficient and inaccurate therefore in such cases we first let the model extract waterbody polygons i e the union of the waterbody pixels and the flowline pixels extracted by the model and then use postprocessing to get the midlines of the polygons as the flowlines fig 3 c depicts the postprocessing which consists of two parts in the skeletonization part the pixels classified as waterbody or flowline pixels using softmax values see fig 5 a and b were selected and the zhang suen thinning algorithm zhang and suen 1984 was applied to get the midlines however some midlines had no outlet and formed endorheic waterbody which might be noises so we used the endorheic waterbody filter to remove them see fig 5 c in this filter we first calculated the connected components of the midline pixels then removed the components without outlets producing the flowlines see fig 5 d further for narrow rivers without the waterbody pixels the postprocessing will return the flowline pixels directly with the midline being the flowline itself therefore the proposed postprocessing is a general method and can be used in both wide and narrow rivers making the framework more straightforward note that although the pixels inside the waterbody polygons i e combining the waterbody pixels and the flowline pixels are what we intend to obtain the end we still let the semantic segmentation model predict the waterbody pixels and the flowline pixels separately during the training phase the reason is that the flowline and waterbody pixels might be easier to be extracted respectively in the mountain and flat regions that is the features used for the flowline and waterbody pixel extraction could be different therefore letting the deep learning models classify pixels into these two classes in the training phase may improve the accuracy of the waterbody polygons subsequently generated from the union of the two classes 3 4 data imbalance alleviation as indicated in fig 2 the number of the hillslope pixels is 16 times the total number of the flowline and waterbody pixels causing significant data imbalance this affects the drainage network extraction because the deep learning model is optimized using the following equation 4 min θ l θ σ i 1 n l y i ˆ y i n where l θ is the expected value of the loss function l y i ˆ y i and θ represents the parameters to be trained to minimize l θ l y i ˆ y i is based on the predicted class y i ˆ and the ground truth class y i for pixel i and n is the total number of pixels in the dataset from this equation the flowline pixels and waterbody pixels that occupy only a tiny portion of the dataset 6 04 see fig 2 make a little bit of contributions in the optimizing process which in turn prevents the model from recognizing these pixels inside the water polygons and from extracting the drainage network to solve this problem we adopted an undersampling strategy in each training step the hillslope pixels were sampled randomly through which the number of ground pixels for training was set to be equal to the total number of the flowline and waterbody pixels 3 5 settings of model training table 2 details the hyperparameters used in the training process and with these settings all models can converge in three days as shown in table 2 adamw loshchilov and hutter 2017 is chosen to optimize our models weights because it computes adaptive learning rates for each parameter achieving a faster convergence speed kingma and ba 2014 and allowing the effortless selection of hyperparameters furthermore compared with its original version adam kingma and ba 2014 adamw decouples the weight decay function from the optimization steps and applies regularization on the weights with large gradients loshchilov and hutter 2017 improving the generalization performance of the models as the adamw optimizer can benefit from the scheduled learning rate multiplier loshchilov and hutter 2017 we introduce a one cycle learning rate scheduler smith and topin 2017 to the model this scheduler anneals the learning rate from one small value to a large maximum value and then from that value to a very small value during the training process with this scheduler the model training process accelerates and the model s weights get regularized by the large maximum learning rate smith and topin 2017 3 6 evaluation metrics to evaluate our models in predicting both the flow directions and pixel classes we used the metrics described in the following paragraphs however before evaluating the prediction of pixel classes the waterbody pixels and flowline pixels were first merged into a single class as the pixels inside the waterbody polygons the reason is that the midlines of the waterbody polygons were extracted by the postprocessing as the flowlines see section 3 3 and the better the waterbody polygons are generated by the models the better the midlines can be extracted by the postprocessing for approximating the flowlines in this context the metrics based on the pixels inside the waterbody polygons can be used to evaluate the models performance as a whole including the effectiveness of postprocessing merging these two pixel classes in the testing phase is also consistent with traditional drainage network extraction studies e g see wu et al 2019 and du et al 2017 in these studies the polygons generated by buffering the extracted flowlines were used to measure the spatial concurrences between the generated flowlines and the ground truth wu et al 2019 where the polygons generated by the buffering can be considered analogous to the waterbody polygons in our paper we first calculated the accuracy using the following equation 5 a c c y ˆ y σ i 1 n 1 y i ˆ y i n where y ˆ and y are the sets of the predicted results and ground truths and y i ˆ and y i are the predicted and ground truth labels for pixel i 1 is an indicator function returning 1 if the condition inside the parenthesis is satisfied and returning 0 if not and n is the total number of pixels in the dataset the equation estimates the proportion of the labels predicted correctly but it is unable to reflect the improvement of a model compared to the random prediction cohen 1960 mchugh 2012 because a model predicting all pixels as hillslope pixels constituting 93 96 of the dataset will still get high accuracy to achieve a more objective evaluation we applied the undersampling strategy see section 3 4 in the test step of predicting pixel classes in detail the hillslope pixels in the test dataset were randomly selected to make the number equal to the total number of the flowline and waterbody pixels for predicting pixel classes i e pixels inside waterbody polygons furthermore the selected ground pixels in the test dataset were set to be the same for all the model cases so as to make the evaluations be conducted under the same benchmark this strategy also follows the one used in stanislawski et al 2018 further we adopted cohen s kappa coefficient cohen 1960 as another metrics using the following equation wu et al 2019 6 k c a c c σ j 1 k p j ˆ p j 1 σ j 1 k p j ˆ p j where a c c is the accuracy calculated using eq 5 p j ˆ and p j are the predicted and ground truth marginal probabilities of label j the probability that a random sample belongs to label j and k is the number of classes in detail σ j 1 k p j ˆ p j quantifies the proportion of pixels for which the correct prediction is achieved by chance cohen 1960 and a c c σ j 1 k p j ˆ p j estimates how many pixels that cannot be predicted by chance can be predicted correctly by the model from eq 6 the kappa coefficient ranges from 1 to 1 while values below 0 are unlikely in practice cohen 1960 mchugh 2012 and a higher value means a better model mchugh 2012 the range 0 21 0 39 corresponds to minimal agreement between the predictions and the ground truths 0 40 0 59 to weak agreement 0 60 0 79 to moderate agreement 0 80 0 90 to strong agreement and 0 90 1 00 to almost perfection agreement mchugh 2012 besides the above two metrics we also calculated the precision p recall r and f1 scores to evaluate our framework to calculate these metrics the true positive tp false positive fp and false negative fn must be calculated for a selected class the true positive tp is the number of samples correctly predicted as that class the false positive fp is the number of samples that are predicted as that class but do not belong to that class the false negative fn is the number of samples that belong to that class but are predicted as other classes with these numbers precision p recall r and f1 scores can be calculated using the following equations 7 p t p t p f p 8 r t p t p f n 9 f 1 2 p r p r for pixels inside waterbody polygons waterbody pixels flowline pixels p evaluates the proportion of real waterbody polygon pixels inside the predicted waterbody polygon pixels r evaluates the proportion of waterbody polygon pixels that have been found by the model and f 1 is the harmonic mean of p and r and can evaluate the models overall performance 4 results and discussion 4 1 evaluation of the input layer of the proposed framework we first evaluated the performance and effectiveness of the input layer in the proposed framework see fig 3 as the most important output generated from the input layer is the distributed representations of the aspect features we used two cases to evaluate the necessity of obtaining such features as shown in table 3 case 1 is our proposed framework with the distributed representations of the aspect features particularly removing from the output of the input layer and case 2 just follows our proposed framework the comparison of the predicted flow direction from these two cases indicates that introducing the distributed representations of the aspect features helps the framework achieve a better performance in all the evaluation metrics the prediction gains significant improvements of around 7 in the accuracy macro precision macro recall and macro f1 scores the associated kappa value is also increased from the moderate agreement 0 74 to the strong agreement 0 82 furthermore in terms of the pixel classification as shown in table 4 introducing the distributed representations of the aspect features can also help the framework gain a marginal improvement on accuracy wp recall and wp f1 scores the kappa value is also increased from the moderate agreement 0 78 to the strong agreement 0 81 these support our design of the input layer in the framework however there are many other geometric features that could also be introduced to the input layer and therefore whether or not the aspect and tpi features are sufficient for our proposed framework of deep learning enhanced drainage network extraction needs to be further examined for this purpose we designed case 3 which included other geometric features that might be useful for drainage network extraction the first feature we selected for case 3 was the curvature because it acts as an excellent indicator for pixel classifications o neil et al 2020 passalacqua et al 2010 sangireddy et al 2016 where positive values practically mean channel pixels and negative values usually represent hillslope pixels passalacqua et al 2010 the second feature is the slope since the different erosional mechanisms of the hillslopes and the channels produce different slope patterns howard 1994 and slope declines may occur along channels howard 1994 the last feature is c κ s 2 where κ denotes the curvature and s denotes the slope because it may simulate the slope catchment relationship c a s 2 montgomery and dietrich 1992 where a represents the catchment area the geometric curvature rather than the laplacian curvature was used as the former has a higher correlation with the drainage skeleton passalacqua et al 2010 we retrieved the gradient and computed the length of the gradient vector to calculate the slope as shown in tables 3 and 4 introducing these extra features has no significant and even no marginal improvement on both the predicted flow direction and pixel classification the reason might be attributed to that the convolutional layers in the deep learning models can learn to extract those geometric features as shown in the semantic segmentation model in the method section therefore it is plausible to suggest that the aspect and tpi features are sufficient for our proposed framework to extract the drainage network 4 2 evaluation of semantic segmentation model next we evaluated the choice of the semantic segmentation model in our framework fig 3 we chose to use u net because of the merits of the encoder decoder structure badrinarayanan et al 2017 chen et al 2018b however u net is not the only semantic segmentation model that uses the encoder decoder structure and there are also other models that can be used chen et al 2018a 2018b therefore we designed cases 4 and 5 for comparisons in case 4 the segnet badrinarayanan et al 2017 which also has an encoder decoder structure and has been used in wetland identification o neil et al 2020 was adopted to replace the u net the architecture of the segnet is very close to the u net except that the pooling indices instead of the upstream low level feature maps are transported by the skip connection layers badrinarayanan et al 2017 the pooling indices store the locations of the pixels containing the values kept by the maxpooling2d layers so the decoder can upsample data using these indices without the use of bilinear interpolation badrinarayanan et al 2017 as the pooling indices contain information about the object s boundary locations the feature map upsampled in this way is believed to involve the necessary information for recovering the resolution in addition saving indices use less memory than storing feature maps which also help reduce the number of parameters for training badrinarayanan et al 2017 however results in table 3 show that the use of the segnet reduces the accuracy macro precision macro recall and macro f1 scores by around 5 compared with case 2 our proposed framework and it also reduces the kappa to the moderate agreement 0 76 this deterioration shows that the low level feature enhanced bilinear interpolation used in u net maybe a better choice than the pooling indices guided interpolation in segnet this might be due to that some information contained in the low level features that are useful for flow direction extraction is lost while the location information is contained in the pooling indices specifically the pooling indices guided upsampling does not fuse the low level and high level features which in turn causes the feature maps only contains the high level information and the low level features become blurred or even disappear chen et al 2021 as a result the low level features required for the flow direction extraction such as the local slope cannot be used by the segnet thereby giving a low accuracy in predicting the flow direction when compared to the results predicted by the u net further the deeplabv3 chen et al 2018b was used to replace u net in case 5 because the deeplabv3 was reported to achieve state of the art performance in multiple semantic segmentation tasks chen et al 2018b the deeplabv3 replaces the maxpooling2d layers with the atrous convolution layers chen et al 2017 compared to the conventional convolution the atrous convolution has a group of kernel elements set to constant zero therefore the atrous convolution can have a larger kernel size than the convolution with the same number of learnable elements and produces a larger receptive field achieving the same goal as the maxpooling2d layer in light of this the deeplabv3 can maintain the resolution of feature maps without the use of maxpooling2d layer and retain the location information during the process besides the deeplabv3 also organizes atrous convolution layers with different receptive fields in parallel to generate feature maps with multi scale information so the deeplabv3 also has the potential to be used to extract drainage networks in flat regions however the results in table 3 show that the deeplabv3 case 5 has significantly lower performance around 13 in the accuracy macro precision macro recall and macro f1scores than u net case 2 our proposed framework in the flow direction extraction predictions furthermore the kappa value of flow direction extraction of the deeplabv3 is in moderate agreement 0 66 much lower than the u net 0 82 this reduction shows that the atrous convolution may not be suitable for flow direction extraction while satisfying most semantic segmentation tasks needs due to the following reasons the flow direction vectors are continuous over space but the kernel of the atrous convolution is not continuous and may produce discrete results wang et al 2018 in addition the flow direction can change significantly over a small region but the atrous convolution cannot well handle high frequency information yu et al 2017 besides the unsuitableness of atrous convolution we further noticed that the final segmentation map of the deeplabv3 is obtained by bilinearly interpolating the feature map four times directly therefore the local information may be lost through this process bai et al 2021 which also causes the low performance of the deeplabv3 in drainage network extraction 4 3 performance compared to the previous work the above evaluations support our design of the framework and selection of semantic segmentation models therefore we can compare our proposed framework s performance with the published method that also uses deep learning techniques to extract drainage network from digital elevation models stanislawski et al 2018 the previous study used a sliding window model to extract the drainage network which uses the patch local region around a pixel to classify the pixel so we cropped our data samples to 65 65 patches for the model training further as the previous work did not extract the flow direction it had to rely on traditional methods to get the flow direction thus in our case we used the flow direction extracted by the traditional method as well i e steps 1 and 2 in fig 1 to evaluate the performance of the flow direction extraction in the previous study and we use the priority flood method for the depression filling pretreatment zhou et al 2016 case 6 in tables 3 and 4 is the sliding window method with the traditional flow direction extraction and the evaluation is summarized below in terms of flow direction extraction the results in table 3 show that our proposed framework case 2 outperforms case 6 in the accuracy macro precision macro recall and macro f1 scores by around 5 and increases the kappa value from the moderate agreement 0 76 to the strong agreement 0 82 in terms of pixel classifications the results in table 4 show that our proposed framework compared to case 6 has better improvements around 5 and also increases the kappa value from 0 73 to 0 81 note that we also have used nine days to train the sliding window model three times longer than the training time of the semantic segmentation model used in our framework 4 4 evaluation of the performance of the proposed framework in different case studies although our proposed framework outperformed other models or methods in various metrics further evaluations on its applicability for different cases are continued in this section the generated flowlines and the ground truth ones are used in the comparison for different case studies as shown in fig 6 we first implemented our proposed framework on a wide river case the grandfather flowage 2269 on the wisconsin river wisconsin usa as shown in fig 6 a with the grandmother dam in the downstream direction highlighted by the orange rectangle e in fig 6 a the river width varies from 250 m to 600 m together with the small local variations of the elevation standard deviation is 8 07 m in a 26 km2 area it is challenging to extract drainage networks in this region however fig 6 a shows that the results generated by our framework match closely with the ground truth flowlines furthermore our framework produces the correct flowlines around the lake island as indicated by the orange rectangle a in comparison the ground truth ignores the flowlines in the north of the island as the traditional method cannot extract cyclic graphs moreover our method provides a more accurate morphological structure of the river in the orange rectangle c correctly delineates the meandering river in the orange rectangle d and successfully extracts the flowline through the grandmother dam in the orange rectangle b there appears to be a ground truth tributary that has not been recognized by our framework however the tributary is not obvious in the basemap so the results generated from our framework can still be considered acceptable fig 6 b shows a river of medium width the flint river near crisp county georgia usa with the warwick dam in the upstream direction the river width is around 50 m which is five pixels in the dataset in addition the standard deviation of the elevation in this region in a 26 km2 area is 5 95 m even smaller than in the previous case so this region is also challenging for our proposed framework to extract flowlines however the outcomes are quite satisfied the difference between the flowlines obtained by our framework and the ground truth is less than 2 pixels at most of the areas through visual examinations moreover the meandering creeks in the orange rectangles a and b are correctly extracted even if they are covered by a forest the lake area is also successfully connected to the river channel in the orange rectangle c and our framework does not make any false prediction in the engineered lands orange rectangle d these support our framework is capable of dealing with a river of medium width in the flat region and the framework also works well in different complicated environments in terms of topography and morphology besides wide rivers and flat relief regions we also tested our framework for narrow rivers and mountain regions fig 6 c shows the first of narrow rivers at the bob moore creek near lemhi county idaho usa with a standard deviation of elevation at 275 12 m in a 26 km2 area this region is less challenging for extracting drainage networks even for the traditional methods however the channel heads are difficult to detect by the conventional methods with a single threshold as discussed in the introduction section besides it is hard to distinguish roads from the channels in these regions by the traditional methods sangireddy et al 2016 and some roads even cross the river as shown in the orange rectangle c however both of these two problems can be solved by our proposed framework as indicated by the orange rectangles a and b the differences between the channel heads obtained from the framework and the ground truth are less than seven pixels 70 m furthermore as indicated by the orange rectangle c the meandering roads do not bias the framework s predictions and the framework results around the river road junctions are correct and clear considering the good agreement between the framework s outcomes and ground truths our proposed framework also has good performance in this region fig 6 d presents a mountain case located around elkhorn peak bannock county idaho usa with a standard deviation of elevation at 204 95 m in a 26 km2 area similar to fig 6 c the flowlines generated by our framework are close to the ground truths and the channel heads as shown in the orange rectangles a and b are predicted with an error of less than 100 m furthermore the prediction of the framework is not biased by an engineered land farmland in the orange rectangle c and the flowlines can still be correctly extracted 4 5 evaluation of the efficiency of the proposed framework our proposed framework only takes less than 10 min to extract the drainage network in the test dataset that contains 7 408 451 584 pixels 740845 16 km2 in our facility so it will take about 30 h to extract the global drainage networks which is much more efficient than the traditional framework 5 conclusions this study proposed a new framework of deep learning enhanced drainage network extraction including three parts the input layer the semantic segmentation model deep learning model and postprocessing for delineating flowlines we introduced the distributed representations of the aspect features generated from the input layer to facilitate the semantic segmentation model deep learning model calculating the flow direction after comparing the performance of different semantic segmentation models we adopted u net which was trained to distinguish the hillslope ground water body and flowlines pixels and classify all the pixels in the image same time to locate the pixels more accurately also we let the model predict the flow direction and pixel classes for each pixel at the same time to solve the problem of lacking flow direction prediction the resultant flowlines are delineated through postprocessing including skeletonization and endorheic waterbody filtering the proposed framework achieves state of the art performance compared to previous studies using deep learning techniques in extracting drainage networks from the digital elevation models through different case studies and comparisons with associated ground truths the proposed framework has demonstrated the capability of extracting flowlines channel lines with high accuracy for rivers of different widths flowing through regions with different features of terrains and even through engineered landscapes the errors of the channel heads obtained by this framework are also low these support the supremacy effectiveness and accuracy of our proposed framework in drainage network extraction further this proposed framework has several advantages compared to the traditional methods the first advantage is that the framework can extract the waterbody polygons with high accuracy in contrast most of the traditional methods except for some methods using geometric features e g sangireddy et al 2016 extract only the flowlines therefore the proposed framework can provide more morphological information which is of benefit in relevant researches and applications the second advantage is that the drainage network extracted by the proposed framework allows cyclic graphs see fig 6a while the traditional methods tend to produce dendritic graphs as cyclic graphs are inevitable in a river system our framework s drainage network output is more objective the third advantage is that the proposed framework needs no threshold values or parameters provided by the user so this framework can be applied to automatically process large scale datasets thereby costing less and achieving more the fourth advantage is that this framework is very efficient as it will just take about 30 h to extract the global drainage networks these advantages provide broad development and application opportunities for the proposed framework software availability name deeplearningdraingenetworkextractor software required gdal 2 2 3 opencv 3 2 0 python 3 7 9 python packages pytorch 1 4 0 torchvision 0 5 0 pytorch lightning 1 0 6 gdal numpy opencv python opencv contrib python scipy tqdm networkx richdem availability https github com maoxin deep learning drainage network extractor git declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests yu hsing wang reports financial support was provided by the innovation technology fund of hksar yu hsing wang reports financial support was provided by the hkust smart sustainable campus project tiejian li reports financial support was provided by the national key research and development program of china acknowledgments this study was supported by the innovation technology fund its 101 19 of hksar the hkust smart sustainable campus ssc project and the national key research and development program of china 2016yfe0201900 the authors are grateful to the reviewers for their valuable comments any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the authors or funders 
25753,agricultural best management practices bmps are popular approaches to reduce nonpoint source nps pollutant losses hydrologic models that can simulate impacts of bmps at the field scale can help guide the selection of bmps furthermore high performance computing techniques have significant potential for scaling spatial simulations and reducing model runtimes in this study a parallel modeling framework for the agricultural policy environmental extender apex model was developed for large scale high resolution spatially distributed model simulations it provides a tool for conducting bmp evaluations at field scale with a distributed architecture and automatic model setup of apex sample results demonstrated the capability of the framework for distributed and semi distributed modeling and illustrated the performance of parallelization this framework can help provide guidance for decision makers on agricultural bmps with large scale water quality assessments and nps nutrient loading reductions keywords apex distributed modeling best management practices nonpoint source pollution parallel computing 1 introduction agricultural land is a diffuse source of sediment excess nitrogen n and phosphorus p and pesticide losses that have been linked to significant environmental issues such as harmful algal blooms habs hypoxia and general ecological degradation national science technology council 2016 these issues not only have negative impacts on waterbody ecology but also have adverse social impacts including domestic water supply pollution and tourism decline scavia et al 2017 agricultural best management practices bmps are popular approaches to reduce nonpoint source nps pollutant losses in such cases effective bmp implementation at any spatial scale typically involves detailed model simulations of bmps and potential locations before one is selected and ultimately implemented feng et al 2019 as opposed to watershed scale bmp evaluations that usually focus on overall effectiveness field scale bmp evaluations can provide better guidance on costs and performance since most bmps function at this scale zhang and zhang 2011 however few models capable of simulating watershed scale impacts can also simulate realistic impacts of bmps and bmp placement at the field scale within the larger watershed hantush et al 2005 improved understanding of hydrologic and nutrient cycling processes in recent years has facilitated field scale water quality model development for better prediction of nutrient loss and bmp effectiveness evaluation harmel et al 2018 there is therefore a need to develop methods i e software and databases to deploy these rapidly improving field scale models at larger spatial scales and finer spatial resolutions many of the publicly distributed nps capable models were evaluated in an epa sponsored model review hantush et al 2005 there more than 60 water quality models characteristics were compared across several categories including simulation capabilities model architectures problem domains and more almost all of these models were process based but their spatial simulation capabilities and architectures varied field and watershed scale models were split almost half and half lumped spatial approaches were most common while semi distributed and distributed approaches were less common only a handful of models exhibited a gridded distributed architecture of the process based models only about one third supported agricultural management practice simulations and about the same fraction supported bmp placement within their various spatial architectures of that fraction only four models supported bmp simulations in high detail which was defined as including most processes related to bmps of those only the agricultural nonpoint source pollution agnps young et al 1989 and annualized agricultural nonpoint source pollution annagnps yuan et al 2002 models have been widely used agnps only supports single event simulations whereas annagnps supports continuous daily simulations in short not many models represent the realistic complexity of bmps and their impacts and of those models that do simulate bmps in high detail they are difficult to scale beyond watershed areas of just a few square kilometers the computational efficiency of hydrologic and water quality models remains a significant constraint for natural resource modeling and therefore management and conservation this becomes particularly obvious when simulating hydrologic or hydraulic systems at larger spatial scales or fine spatial resolutions maxwell 2013 despite the well known limitations of hydrologic hydraulic or water quality models most still rely on single core central processing unit cpu architectures in this regard parallel or high performance computing hpc clusters and techniques have significant potential for scaling spatial simulations in hydrology and reducing model runtimes kollet et al 2010 a significant amount of research has been conducted to explore the use of hpc platforms for hydrologic modeling applications with pre developed hpc packages or libraries hwang et al 2014 and several studies of monte carlo simulation schemes in groundwater modeling have focused on parallelizing the simulation process cheng et al 2014 abdelaziz and le 2014 huang et al 2018 thain et al 2005 sarris et al 2021 however parallel schemes for surface hydrologic models both empirical and physical model types are not widely deployed but are increasingly needed this is especially true for agricultural and nps models of watershed and field scale architectures with bmp simulation capabilities hantush et al 2005 1 1 apex model background the agricultural policy environmental extender apex model willams et al 2000 is a field scale hydrologic model developed to simulate runoff soil loss nutrient losses and pesticide losses originating from farms or agricultural lands gassman et al 2010 apex has been applied in several previous studies on nps pollution mitigation such as simulating macropore phosphorus loss ford et al 2017 evaluating the effectiveness of conservation practices francesconi et al 2014 tuppad et al 2010 smith et al 2015 and evaluating evapotranspiration for dryland cropping systems tadesse et al 2018 apex supports bmp simulation in moderate detail and it has been used at relatively small spatial scales through its community of user interfaces there are five interfaces providing support to users for setup and execution of the apex model including arcapex tuppad et al 2010 winapex i apex apex web interface feng et al 2019 and geoapexol feng et al 2020 arcapex winapex and i apex all require users to have profound expertise on how to set up and execute the model arcapex is embedded in the arcgis software package and uses multiple spatial tools in arcgis winapex is a stand alone windows pc based interface that needs to be downloaded and installed two recently developed tools by our group named the apex web interface and geoapexol are both equipped with web interfaces and background geospatial databases the apex web interface can automatically simulate a farm field with user selection of area location soil and management currently it simulates one field at a time and therefore is not suitable for simulating larger spatial scales or multiple fields geoapexol incorporates online maps web gis technologies and spatial databases so simulation can be readily conducted with limited expertise within a few minutes without extra downloaded or installed software requiring only an internet browser and internet access however like the apex web interface it can only simulate one user selected area for either field or small watershed scales which limits its application to small study areas or individual fields 2 objectives based on the limitations identified and briefly discussed above the following objectives were formulated 1 develop a modeling framework that can automatically prepare model inputs setup and execute simulations in parallel gather and summarize results and generate graphical output 2 prepare and disseminate necessary database s executable scripts tutorials and documentation for application support and 3 demonstrate significantly reduced computational time and general capabilities of the new framework in large or relatively high resolution study areas typical of water quality modeling projects 2 1 the proposed framework the proposed apex multiprocessing program apexmp framework supports spatially distributed and semi distributed approaches of modeling as well as different results reporting options i e tabular summaries and mapping products it is designed to conduct modeling for large spatial scales and or fine spatial resolutions using parallel computing with the most current apex model while version 1501 is the most recent model version it is also expected that future updates to the core apex model would be compatible with apexmp in many cases only requiring the executable to be replaced fig 1 provides a visual representation of apexmp execution options namely the option to engage distributed or semi distributed routines and the option to generate geospatial outputs in addition to the standard tabular outputs a brief explanation of each option is described below as mentioned above the semi distributed approach means apexmp will group cells with similar physical properties first and then simulate them together this is a semi distributed approach because apexmp performs some lumping of model inputs to reduce model complexity and runtime where it may be the least costly in terms of accuracy and performance the spatially distributed approach means apexmp will simulate every individual cell in the study area which is a distributed approach because model input details are represented regularly and equally thus the latter approach would have significantly more simulations and longer simulation times than the former as is typical of distributed modeling approaches after the execution approach has been determined users can specify apexmp create map outputs in addition to standard output tables based on user specification the appropriate combinations of modules will be determined by apexmp automatically apexmp users are expected to understand differences of the two execution approaches and their respective advantages or limitations as no decision support is provided in the software package to indicate whether any approach is suitable for any particular simulation a detailed comparison of simulation results using both execution approaches is discussed later graphical outputs for each approach are also provided to demonstrate apexmp capabilities 3 methodology 3 1 apexmp framework components apexmp includes four scripts modules for each of the two major approaches spatially distributed and semi distributed these include preparation execution gathering and mapping modules users are required to provide a shapefile which defines the study area and model boundaries this is then used to extract information from the default apexmp databases or optionally from a user s own database in the same format as the default database apexmp was developed with the python language supported by pandas numpy gdal fiona and taudem packages described in the software manual 3 2 preparation modules and databases the preparation modules have two options as shown in fig 1 since the distributed option requires significantly more runs than the semi distributed option it better suits modeling a relatively small study area with distributed simulation the semi distributed option that requires fewer runs better fits modeling large areas with a user prepared study area shapefile the distributed preparation module uses the gdalwrap and gdal translate functions of the geospatial data abstraction library gdal for clipping raster layers and converting the clipped raster layers from tiff files to ascii datasets the d8 flow direction tools d8flowdir of the terrain analysis using dem taudem library are applied to calculate slope using the dem layer for the semi distributed option the study area map is used to clip the shapefile of counties in the study area with gdal the shapefiles of all the counties in the study area are disaggregated from the clipped shapefile with the fiona library the shapefile of each county is then used as a mask to clip and transfer the raster layers with a looping function the raster data shown in table 1 are cropped to the same area s and slope layer s are created with the d8flowdir tool all the prepared data including zip code for the semi distributed option only soil group land use slope and dem are ascii format files which can be read as text files with headers of spatial information and contain the main body of data for each cell such data format facilitates the overlapping process in preparing input data for the apex model as shown in table 1 the database provided for the contiguous us in the apexmp package includes 1 raster layers of elevation dem land use cdl soil ssurgo and zip codes 2 vector layers of all us counties 3 tables of soil database and county zip code location and 4 climate and management files prepared for apex all spatial layers have the same projection and all the raster layers have the same resolution so that data can be extracted and overlapped spatially for updating apex files all crops in the cdl have been clipped and other land use types such as urban water and forest have been eliminated as this tool was designed for simulation of agricultural bmps the county zip code location tabular file includes the coordinates of zip codes associated with counties the zip code layer the shapefile of all counties and the county zip code location tabular file are used for the semi distributed option in order to group cells with similar physical properties within the same area the soil database tabular file includes soil properties for all the mukey map unit key special identification for each soil group to link the spatial and tabular data the climate database uses monthly statistics table information generated by climate generator cligen software zhang and garbrecht 2003 with the source data of over 2700 weather stations across the u s for 1974 to 2013 from the national climatic data center ncdc https www ncdc noaa gov cdo web three management scenario files included in the apexmp package are traditional cropping tilled fallow and trees from feng et al feng et al 2019 the traditional cropping scenarios of the top ten crops in the contiguous us which occupy 93 of crop areas were either extracted from or created by the apex web interface feng et al 2019 the tilled fallow scenario has no crop planted but is tilled every month pine tree is used for the tree scenario users that want to use bmps could use other apex tools to generate different scenarios or simply modify the management files they are all text files manually based on the apex user manual more detailed information on the soil climate and management databases can be found in the previous study of feng et al feng et al 2019 3 3 execution modules with processed data from the preparation modules the execution modules with the apex default input files and apex executable will conduct simulations for the four combinations of modules as mentioned in section 1 3 and shown in fig 1 the semi distributed map option means cells with similar physical properties will be merged and simulated together and finally the results will be mapped the semi distributed option has the same setting as the previous one for simulation but only reads the results without mapping the distributed map option simulates all cells in parallel and then reads and maps the results and the distributed option performs all the runs but no mapping for the results the execution modules consist of the main function and several sub functions providing an efficient structure for maintenance detailed workflow for the two options is shown in fig 2 the main function starts with calling the sub functions for reading and overlapping multiple spatial layer information and creating a data list called cellslist hereinafter that contains data of all the cells next the function scans each cell of the layers to eliminate cells containing the no data value for the semi distributed option cells with the same state county zip code soil group land use and slope group are considered as one subarea the total area average slope x and y coordinates of spatial center and the average elevation of the cells in the semi distributed option are then calculated and recorded for the distributed option the x and y coordinates soil group land use slope and elevation column and row numbers of all the cells are read and recorded the next part of the main function includes reorganizing the cellslist creating an in memory soil database and creating a job pool for parallel model setup and runs the cellslist is organized with desired orders of different data and formatted as required data types and then since the management scenario information is not spatial data it is attached to the end of the cellslist with the scenario id for query of the scenario files an in memory soil database is created by refining the soil database for only the soil groups in the cellslist with the pandas library to reduce the query time the job pool of the parallel processing is created with the run number cellslist and in memory soil database the primary function of the execution modules is to perform the major model tasks of setup and running in parallel first a suitable number of cores is determined such that the processes could be executed in the background while remaining cpu resources can be used to conduct normal operations without noticeable impedance then each subsequent process i e setup and running has associated job lists created to be fed to the available parallel cores model setup involves primarily the creation or retrieval of inputs for each job to be executed the setup procedure calls multiple sub functions to accomplish a range of tasks including soil database queries elimination of water soil types run folder creation default file copies executable file copies and file modifications that require data from the cellslist and in memory soil database the simulation process creates individual run logs and error files initiates the model run for a given run folder and finally removes unnecessary files to reduce the storage size an override for retaining all files or deleting unnecessary files can be specified by the user in the apexmp configuration file the final part of the main function is reading and writing the input data and output results to a result file a tabular result file with the headings of all the required data and results is created the input data from cellslist for each run number is also read and recorded in the result file next the desired apex result files for annual average are read or calculated and then recorded in the result file start time and end time for each part of the main function and the whole main function are recorded so runtime comparisons can be made between different options different resolutions or between parallel and series simulations 3 4 gathering and mapping modules as shown in fig 1 the gathering and mapping modules have two options one with the mapping option and the other with tables only users can choose to process the results by only gathering table outputs or by gathering table outputs and then mapping the results in the beginning the result file s generated by the execution modules are read and the results of cells with no data are eliminated soil data of such cells are categorized as non cropped landscape and thus are not simulated but are recorded e g soil cells have no soil property data when they are categorized as water urban or other values next the input data simulation results calculated nps pollution reductions between bmps and a baseline and the rankings of the reductions for all the cells of different variables are separated into multiple tabular files processes that are distinct between different options are 1 nothing for the distributed option 2 the results of the semi distributed option need to be disaggregated to individual cells with their own column and row numbers for both the semi distributed and semi distributed map options 3 for the semi distributed map and distributed map options the column and row numbers for each cell associated with different variables and their rankings are first rearranged to create python numpy arrays and then the base map all 0s for each cell are updated with gdal open and write functions 4 results and discussion for demonstration purposes apexmp was applied to a hydrologic unit code huc 12 watershed large study area in northwest ohio and northeast indiana huc 12 number 041000030301 fig 3 catchment area was 94 22 km2 and with the 30 m 30 m resolution spatial data there were 132 038 grid cells all options of the four modules described in the methodology section were demonstrated as described in the methods section apexmp serves as a framework to achieve automated and parallelized model setup and execution thus with the same parameter set in default apex input files and using the apexmp dataset there would be no difference between the results generated by apexmp and the apex standalone model the results of a zoomed in area of the watershed were clipped out for comparison between the distributed and semi distributed options section 3 1 and the performance testing was based on results for the whole watershed section 3 2 future development and research directions for apexmp are discussed in section 3 3 4 1 spatial results analysis the results of nps pollution reductions from the baseline to the land use change scenario were compared between the distributed and semi distributed map options fig 3 the tilled fallow scenario with the most anticipated nps pollution was selected as the baseline and the pine tree scenario with the least anticipated nps pollution was used as the a land use change scenario to obtain the greatest reductions the spatial distributions 30 m 30 m of the total nitrogen reductions based on the 30 year 1985 2014 annual average simulations for both options are illustrated as a small zoomed in rectangular area in fig 3 the spatial distributions of both distributed and semi distributed results were generally in agreement as the correlation coefficient r was 0 89 this finding supports the case that when simulating large areas if the computational capability is insufficient choosing the semi distributed option can provide reasonable spatial distributions of the results 4 2 parallelization performance testing the pc that was used for the testing has a dual cpu setup i e two cpus which are both intel xeon cpu e5 2670 2 60 ghz 8 physical cores each or 16 virtual cores each it runs a windows x64 bit version and the apex executable file is also a 64 bit version the number of simulations and the total runtimes when using different numbers of cores are reported in table 2 a total of 12 unique runs were executed to evaluate the parallelization performance of apexmp these consisted of two execution strategies distributed and semi distributed corresponding to distributed and semi distributed spatial representations and six unique core counts 1 2 4 8 16 and 32 used in execution as recorded in table 2 for the distributed approach the total runtime for the 67 110 runs decreased from 71 089 s with 1 core to 3 833 s with 32 cores which resulted in a 94 6 reduction of total runtime for the semi distributed approach the total runtime was reduced from 826 s with 1 core to 74 s with 32 cores which also represented at a substantial decrease even though apexmp has not been applied with supercomputing which it can be it has already reduced the runtime to 1 10 or 1 20 of the original with a pc therefore for the same timeframe users could initiate 10 to 20 runs with apexmp comparing to the apex standalone model assuming the apex standalone model has already been setup and is ready to run if not then the manual setup would take extra time depending on user expertise and data availability as shown in table 2 the total number of runs was decreased from 67 110 to 730 after grouping the grid cells with similar physical properties and simulating them together the semi distributed approach furthermore according to the results shown in section 3 1 those generated by the semi distributed option had no large difference from those for the distributed option in the demonstration area thus if the option of more cores or supercomputing is not feasible choosing the semi distributed option would also significantly reduce the runtime for the same core s the total runtimes of the semi distributed option were about 1 2 of those of the distributed approach 4 3 future development and research directions several opportunities with the current apexmp exist to include more functions improve simulation accuracy or improve ease of use planned future developments include 1 an optimization module with the ability to calibrate and validate the model against observed data 2 routing methods and an algorithm for connection of cells to make the simulation more accurate 3 a simple user interface to make apexmp easier to use and 4 with limited modifications apexmp could be used with other hydrologic models to compare the computational efficiency and the research areas could be expanded e g l thia is mainly applied for urban land use so apexmp with l thia could be applied to urban areas chen et al 2017 at this time default databases have been prepared for the contiguous united states for the data of areas outside of the contiguous us 1 elevation and land use maps climate and management data could be modified to the same resolution and spatial projection raster data or the same formats as provided in the apexmp package climate and management data 2 for the soil map and soil attribute table some adaptation is likely needed as such data often do not exist in places outside of the us or are in different formats 3 for the zip code and county maps and county zip code location table some modification would be needed under our guidance to work for apexmp such data availability problems exist for all popular models developed in specific locations and then applied to other places with some adaptation and modification of the data apexmp is fully capable of being applied to areas outside of the contiguous us with the current apexmp potential future research directions include 1 with more information on crop management practices bmps and recommendation of parameters in other regions apexmp could be applied to cropland of the entire contiguous us 2 more temporal scales other than average annual in this study and hydrologic variables other than precipitation runoff soil loss total nitrogen and total phosphorus can be included for future research objectives 5 conclusions a parallel modeling framework was developed to make large scale high resolution apex model simulations more practical and efficient the proposed framework package includes national datasets of elevation land use and soil management scenarios and a climate database an apex model executable file a tutorial code documentation and additional apexmp dependencies with one study area map and choice of module options the framework can be executed automatically for conducting agricultural bmp evaluations at field scale with a distributed architecture contiguous spatial maps of designated hydrologic variables reductions for bmps compared to the baseline and reduction rankings among all the cells could be acquired as shown in the results the parallelized algorithms of the framework increase computational efficiency and reduce execution time significantly author contribution conceptualization feng pan qingyu feng and ryan mcgehee methodology feng pan qingyu feng and ryan mcgehee software feng pan qingyu feng and ryan mcgehee validation feng pan and ryan mcgehee formal analysis feng pan and ryan mcgehee investigation feng pan and ryan mcgehee resources feng pan qingyu feng and ryan mcgehee data curation feng pan writing original draft preparation feng pan writing review and editing feng pan qingyu feng ryan mcgehee bernard engel dennis flanagan and jingqiu chen visualization feng pan ryan mcgehee and jingqiu chen supervision bernard engel and dennis flanagan project administration bernard engel and dennis flanagan funding acquisition bernard engel all authors have read and agreed to the published version of the manuscript software and data availability name of software apexmp developers feng pan qingyu feng ryan mcgehee software required taudem and gdal included in the package and tested on windows pcs with windows 10 64 bit system programming language python packages available https github com ryanpmcg apexmp database available needs author s permission to access and download https app globus org file manager origin id 001e9a6c 512e 11eb 87b7 02187389bd35 origin path 2f first available december 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank dr anurag srivastava of the department of soil and water systems university of idaho and dr jaehek jeong at the blackland research center temple texas for providing technical support feedback and tests of the interface 
25753,agricultural best management practices bmps are popular approaches to reduce nonpoint source nps pollutant losses hydrologic models that can simulate impacts of bmps at the field scale can help guide the selection of bmps furthermore high performance computing techniques have significant potential for scaling spatial simulations and reducing model runtimes in this study a parallel modeling framework for the agricultural policy environmental extender apex model was developed for large scale high resolution spatially distributed model simulations it provides a tool for conducting bmp evaluations at field scale with a distributed architecture and automatic model setup of apex sample results demonstrated the capability of the framework for distributed and semi distributed modeling and illustrated the performance of parallelization this framework can help provide guidance for decision makers on agricultural bmps with large scale water quality assessments and nps nutrient loading reductions keywords apex distributed modeling best management practices nonpoint source pollution parallel computing 1 introduction agricultural land is a diffuse source of sediment excess nitrogen n and phosphorus p and pesticide losses that have been linked to significant environmental issues such as harmful algal blooms habs hypoxia and general ecological degradation national science technology council 2016 these issues not only have negative impacts on waterbody ecology but also have adverse social impacts including domestic water supply pollution and tourism decline scavia et al 2017 agricultural best management practices bmps are popular approaches to reduce nonpoint source nps pollutant losses in such cases effective bmp implementation at any spatial scale typically involves detailed model simulations of bmps and potential locations before one is selected and ultimately implemented feng et al 2019 as opposed to watershed scale bmp evaluations that usually focus on overall effectiveness field scale bmp evaluations can provide better guidance on costs and performance since most bmps function at this scale zhang and zhang 2011 however few models capable of simulating watershed scale impacts can also simulate realistic impacts of bmps and bmp placement at the field scale within the larger watershed hantush et al 2005 improved understanding of hydrologic and nutrient cycling processes in recent years has facilitated field scale water quality model development for better prediction of nutrient loss and bmp effectiveness evaluation harmel et al 2018 there is therefore a need to develop methods i e software and databases to deploy these rapidly improving field scale models at larger spatial scales and finer spatial resolutions many of the publicly distributed nps capable models were evaluated in an epa sponsored model review hantush et al 2005 there more than 60 water quality models characteristics were compared across several categories including simulation capabilities model architectures problem domains and more almost all of these models were process based but their spatial simulation capabilities and architectures varied field and watershed scale models were split almost half and half lumped spatial approaches were most common while semi distributed and distributed approaches were less common only a handful of models exhibited a gridded distributed architecture of the process based models only about one third supported agricultural management practice simulations and about the same fraction supported bmp placement within their various spatial architectures of that fraction only four models supported bmp simulations in high detail which was defined as including most processes related to bmps of those only the agricultural nonpoint source pollution agnps young et al 1989 and annualized agricultural nonpoint source pollution annagnps yuan et al 2002 models have been widely used agnps only supports single event simulations whereas annagnps supports continuous daily simulations in short not many models represent the realistic complexity of bmps and their impacts and of those models that do simulate bmps in high detail they are difficult to scale beyond watershed areas of just a few square kilometers the computational efficiency of hydrologic and water quality models remains a significant constraint for natural resource modeling and therefore management and conservation this becomes particularly obvious when simulating hydrologic or hydraulic systems at larger spatial scales or fine spatial resolutions maxwell 2013 despite the well known limitations of hydrologic hydraulic or water quality models most still rely on single core central processing unit cpu architectures in this regard parallel or high performance computing hpc clusters and techniques have significant potential for scaling spatial simulations in hydrology and reducing model runtimes kollet et al 2010 a significant amount of research has been conducted to explore the use of hpc platforms for hydrologic modeling applications with pre developed hpc packages or libraries hwang et al 2014 and several studies of monte carlo simulation schemes in groundwater modeling have focused on parallelizing the simulation process cheng et al 2014 abdelaziz and le 2014 huang et al 2018 thain et al 2005 sarris et al 2021 however parallel schemes for surface hydrologic models both empirical and physical model types are not widely deployed but are increasingly needed this is especially true for agricultural and nps models of watershed and field scale architectures with bmp simulation capabilities hantush et al 2005 1 1 apex model background the agricultural policy environmental extender apex model willams et al 2000 is a field scale hydrologic model developed to simulate runoff soil loss nutrient losses and pesticide losses originating from farms or agricultural lands gassman et al 2010 apex has been applied in several previous studies on nps pollution mitigation such as simulating macropore phosphorus loss ford et al 2017 evaluating the effectiveness of conservation practices francesconi et al 2014 tuppad et al 2010 smith et al 2015 and evaluating evapotranspiration for dryland cropping systems tadesse et al 2018 apex supports bmp simulation in moderate detail and it has been used at relatively small spatial scales through its community of user interfaces there are five interfaces providing support to users for setup and execution of the apex model including arcapex tuppad et al 2010 winapex i apex apex web interface feng et al 2019 and geoapexol feng et al 2020 arcapex winapex and i apex all require users to have profound expertise on how to set up and execute the model arcapex is embedded in the arcgis software package and uses multiple spatial tools in arcgis winapex is a stand alone windows pc based interface that needs to be downloaded and installed two recently developed tools by our group named the apex web interface and geoapexol are both equipped with web interfaces and background geospatial databases the apex web interface can automatically simulate a farm field with user selection of area location soil and management currently it simulates one field at a time and therefore is not suitable for simulating larger spatial scales or multiple fields geoapexol incorporates online maps web gis technologies and spatial databases so simulation can be readily conducted with limited expertise within a few minutes without extra downloaded or installed software requiring only an internet browser and internet access however like the apex web interface it can only simulate one user selected area for either field or small watershed scales which limits its application to small study areas or individual fields 2 objectives based on the limitations identified and briefly discussed above the following objectives were formulated 1 develop a modeling framework that can automatically prepare model inputs setup and execute simulations in parallel gather and summarize results and generate graphical output 2 prepare and disseminate necessary database s executable scripts tutorials and documentation for application support and 3 demonstrate significantly reduced computational time and general capabilities of the new framework in large or relatively high resolution study areas typical of water quality modeling projects 2 1 the proposed framework the proposed apex multiprocessing program apexmp framework supports spatially distributed and semi distributed approaches of modeling as well as different results reporting options i e tabular summaries and mapping products it is designed to conduct modeling for large spatial scales and or fine spatial resolutions using parallel computing with the most current apex model while version 1501 is the most recent model version it is also expected that future updates to the core apex model would be compatible with apexmp in many cases only requiring the executable to be replaced fig 1 provides a visual representation of apexmp execution options namely the option to engage distributed or semi distributed routines and the option to generate geospatial outputs in addition to the standard tabular outputs a brief explanation of each option is described below as mentioned above the semi distributed approach means apexmp will group cells with similar physical properties first and then simulate them together this is a semi distributed approach because apexmp performs some lumping of model inputs to reduce model complexity and runtime where it may be the least costly in terms of accuracy and performance the spatially distributed approach means apexmp will simulate every individual cell in the study area which is a distributed approach because model input details are represented regularly and equally thus the latter approach would have significantly more simulations and longer simulation times than the former as is typical of distributed modeling approaches after the execution approach has been determined users can specify apexmp create map outputs in addition to standard output tables based on user specification the appropriate combinations of modules will be determined by apexmp automatically apexmp users are expected to understand differences of the two execution approaches and their respective advantages or limitations as no decision support is provided in the software package to indicate whether any approach is suitable for any particular simulation a detailed comparison of simulation results using both execution approaches is discussed later graphical outputs for each approach are also provided to demonstrate apexmp capabilities 3 methodology 3 1 apexmp framework components apexmp includes four scripts modules for each of the two major approaches spatially distributed and semi distributed these include preparation execution gathering and mapping modules users are required to provide a shapefile which defines the study area and model boundaries this is then used to extract information from the default apexmp databases or optionally from a user s own database in the same format as the default database apexmp was developed with the python language supported by pandas numpy gdal fiona and taudem packages described in the software manual 3 2 preparation modules and databases the preparation modules have two options as shown in fig 1 since the distributed option requires significantly more runs than the semi distributed option it better suits modeling a relatively small study area with distributed simulation the semi distributed option that requires fewer runs better fits modeling large areas with a user prepared study area shapefile the distributed preparation module uses the gdalwrap and gdal translate functions of the geospatial data abstraction library gdal for clipping raster layers and converting the clipped raster layers from tiff files to ascii datasets the d8 flow direction tools d8flowdir of the terrain analysis using dem taudem library are applied to calculate slope using the dem layer for the semi distributed option the study area map is used to clip the shapefile of counties in the study area with gdal the shapefiles of all the counties in the study area are disaggregated from the clipped shapefile with the fiona library the shapefile of each county is then used as a mask to clip and transfer the raster layers with a looping function the raster data shown in table 1 are cropped to the same area s and slope layer s are created with the d8flowdir tool all the prepared data including zip code for the semi distributed option only soil group land use slope and dem are ascii format files which can be read as text files with headers of spatial information and contain the main body of data for each cell such data format facilitates the overlapping process in preparing input data for the apex model as shown in table 1 the database provided for the contiguous us in the apexmp package includes 1 raster layers of elevation dem land use cdl soil ssurgo and zip codes 2 vector layers of all us counties 3 tables of soil database and county zip code location and 4 climate and management files prepared for apex all spatial layers have the same projection and all the raster layers have the same resolution so that data can be extracted and overlapped spatially for updating apex files all crops in the cdl have been clipped and other land use types such as urban water and forest have been eliminated as this tool was designed for simulation of agricultural bmps the county zip code location tabular file includes the coordinates of zip codes associated with counties the zip code layer the shapefile of all counties and the county zip code location tabular file are used for the semi distributed option in order to group cells with similar physical properties within the same area the soil database tabular file includes soil properties for all the mukey map unit key special identification for each soil group to link the spatial and tabular data the climate database uses monthly statistics table information generated by climate generator cligen software zhang and garbrecht 2003 with the source data of over 2700 weather stations across the u s for 1974 to 2013 from the national climatic data center ncdc https www ncdc noaa gov cdo web three management scenario files included in the apexmp package are traditional cropping tilled fallow and trees from feng et al feng et al 2019 the traditional cropping scenarios of the top ten crops in the contiguous us which occupy 93 of crop areas were either extracted from or created by the apex web interface feng et al 2019 the tilled fallow scenario has no crop planted but is tilled every month pine tree is used for the tree scenario users that want to use bmps could use other apex tools to generate different scenarios or simply modify the management files they are all text files manually based on the apex user manual more detailed information on the soil climate and management databases can be found in the previous study of feng et al feng et al 2019 3 3 execution modules with processed data from the preparation modules the execution modules with the apex default input files and apex executable will conduct simulations for the four combinations of modules as mentioned in section 1 3 and shown in fig 1 the semi distributed map option means cells with similar physical properties will be merged and simulated together and finally the results will be mapped the semi distributed option has the same setting as the previous one for simulation but only reads the results without mapping the distributed map option simulates all cells in parallel and then reads and maps the results and the distributed option performs all the runs but no mapping for the results the execution modules consist of the main function and several sub functions providing an efficient structure for maintenance detailed workflow for the two options is shown in fig 2 the main function starts with calling the sub functions for reading and overlapping multiple spatial layer information and creating a data list called cellslist hereinafter that contains data of all the cells next the function scans each cell of the layers to eliminate cells containing the no data value for the semi distributed option cells with the same state county zip code soil group land use and slope group are considered as one subarea the total area average slope x and y coordinates of spatial center and the average elevation of the cells in the semi distributed option are then calculated and recorded for the distributed option the x and y coordinates soil group land use slope and elevation column and row numbers of all the cells are read and recorded the next part of the main function includes reorganizing the cellslist creating an in memory soil database and creating a job pool for parallel model setup and runs the cellslist is organized with desired orders of different data and formatted as required data types and then since the management scenario information is not spatial data it is attached to the end of the cellslist with the scenario id for query of the scenario files an in memory soil database is created by refining the soil database for only the soil groups in the cellslist with the pandas library to reduce the query time the job pool of the parallel processing is created with the run number cellslist and in memory soil database the primary function of the execution modules is to perform the major model tasks of setup and running in parallel first a suitable number of cores is determined such that the processes could be executed in the background while remaining cpu resources can be used to conduct normal operations without noticeable impedance then each subsequent process i e setup and running has associated job lists created to be fed to the available parallel cores model setup involves primarily the creation or retrieval of inputs for each job to be executed the setup procedure calls multiple sub functions to accomplish a range of tasks including soil database queries elimination of water soil types run folder creation default file copies executable file copies and file modifications that require data from the cellslist and in memory soil database the simulation process creates individual run logs and error files initiates the model run for a given run folder and finally removes unnecessary files to reduce the storage size an override for retaining all files or deleting unnecessary files can be specified by the user in the apexmp configuration file the final part of the main function is reading and writing the input data and output results to a result file a tabular result file with the headings of all the required data and results is created the input data from cellslist for each run number is also read and recorded in the result file next the desired apex result files for annual average are read or calculated and then recorded in the result file start time and end time for each part of the main function and the whole main function are recorded so runtime comparisons can be made between different options different resolutions or between parallel and series simulations 3 4 gathering and mapping modules as shown in fig 1 the gathering and mapping modules have two options one with the mapping option and the other with tables only users can choose to process the results by only gathering table outputs or by gathering table outputs and then mapping the results in the beginning the result file s generated by the execution modules are read and the results of cells with no data are eliminated soil data of such cells are categorized as non cropped landscape and thus are not simulated but are recorded e g soil cells have no soil property data when they are categorized as water urban or other values next the input data simulation results calculated nps pollution reductions between bmps and a baseline and the rankings of the reductions for all the cells of different variables are separated into multiple tabular files processes that are distinct between different options are 1 nothing for the distributed option 2 the results of the semi distributed option need to be disaggregated to individual cells with their own column and row numbers for both the semi distributed and semi distributed map options 3 for the semi distributed map and distributed map options the column and row numbers for each cell associated with different variables and their rankings are first rearranged to create python numpy arrays and then the base map all 0s for each cell are updated with gdal open and write functions 4 results and discussion for demonstration purposes apexmp was applied to a hydrologic unit code huc 12 watershed large study area in northwest ohio and northeast indiana huc 12 number 041000030301 fig 3 catchment area was 94 22 km2 and with the 30 m 30 m resolution spatial data there were 132 038 grid cells all options of the four modules described in the methodology section were demonstrated as described in the methods section apexmp serves as a framework to achieve automated and parallelized model setup and execution thus with the same parameter set in default apex input files and using the apexmp dataset there would be no difference between the results generated by apexmp and the apex standalone model the results of a zoomed in area of the watershed were clipped out for comparison between the distributed and semi distributed options section 3 1 and the performance testing was based on results for the whole watershed section 3 2 future development and research directions for apexmp are discussed in section 3 3 4 1 spatial results analysis the results of nps pollution reductions from the baseline to the land use change scenario were compared between the distributed and semi distributed map options fig 3 the tilled fallow scenario with the most anticipated nps pollution was selected as the baseline and the pine tree scenario with the least anticipated nps pollution was used as the a land use change scenario to obtain the greatest reductions the spatial distributions 30 m 30 m of the total nitrogen reductions based on the 30 year 1985 2014 annual average simulations for both options are illustrated as a small zoomed in rectangular area in fig 3 the spatial distributions of both distributed and semi distributed results were generally in agreement as the correlation coefficient r was 0 89 this finding supports the case that when simulating large areas if the computational capability is insufficient choosing the semi distributed option can provide reasonable spatial distributions of the results 4 2 parallelization performance testing the pc that was used for the testing has a dual cpu setup i e two cpus which are both intel xeon cpu e5 2670 2 60 ghz 8 physical cores each or 16 virtual cores each it runs a windows x64 bit version and the apex executable file is also a 64 bit version the number of simulations and the total runtimes when using different numbers of cores are reported in table 2 a total of 12 unique runs were executed to evaluate the parallelization performance of apexmp these consisted of two execution strategies distributed and semi distributed corresponding to distributed and semi distributed spatial representations and six unique core counts 1 2 4 8 16 and 32 used in execution as recorded in table 2 for the distributed approach the total runtime for the 67 110 runs decreased from 71 089 s with 1 core to 3 833 s with 32 cores which resulted in a 94 6 reduction of total runtime for the semi distributed approach the total runtime was reduced from 826 s with 1 core to 74 s with 32 cores which also represented at a substantial decrease even though apexmp has not been applied with supercomputing which it can be it has already reduced the runtime to 1 10 or 1 20 of the original with a pc therefore for the same timeframe users could initiate 10 to 20 runs with apexmp comparing to the apex standalone model assuming the apex standalone model has already been setup and is ready to run if not then the manual setup would take extra time depending on user expertise and data availability as shown in table 2 the total number of runs was decreased from 67 110 to 730 after grouping the grid cells with similar physical properties and simulating them together the semi distributed approach furthermore according to the results shown in section 3 1 those generated by the semi distributed option had no large difference from those for the distributed option in the demonstration area thus if the option of more cores or supercomputing is not feasible choosing the semi distributed option would also significantly reduce the runtime for the same core s the total runtimes of the semi distributed option were about 1 2 of those of the distributed approach 4 3 future development and research directions several opportunities with the current apexmp exist to include more functions improve simulation accuracy or improve ease of use planned future developments include 1 an optimization module with the ability to calibrate and validate the model against observed data 2 routing methods and an algorithm for connection of cells to make the simulation more accurate 3 a simple user interface to make apexmp easier to use and 4 with limited modifications apexmp could be used with other hydrologic models to compare the computational efficiency and the research areas could be expanded e g l thia is mainly applied for urban land use so apexmp with l thia could be applied to urban areas chen et al 2017 at this time default databases have been prepared for the contiguous united states for the data of areas outside of the contiguous us 1 elevation and land use maps climate and management data could be modified to the same resolution and spatial projection raster data or the same formats as provided in the apexmp package climate and management data 2 for the soil map and soil attribute table some adaptation is likely needed as such data often do not exist in places outside of the us or are in different formats 3 for the zip code and county maps and county zip code location table some modification would be needed under our guidance to work for apexmp such data availability problems exist for all popular models developed in specific locations and then applied to other places with some adaptation and modification of the data apexmp is fully capable of being applied to areas outside of the contiguous us with the current apexmp potential future research directions include 1 with more information on crop management practices bmps and recommendation of parameters in other regions apexmp could be applied to cropland of the entire contiguous us 2 more temporal scales other than average annual in this study and hydrologic variables other than precipitation runoff soil loss total nitrogen and total phosphorus can be included for future research objectives 5 conclusions a parallel modeling framework was developed to make large scale high resolution apex model simulations more practical and efficient the proposed framework package includes national datasets of elevation land use and soil management scenarios and a climate database an apex model executable file a tutorial code documentation and additional apexmp dependencies with one study area map and choice of module options the framework can be executed automatically for conducting agricultural bmp evaluations at field scale with a distributed architecture contiguous spatial maps of designated hydrologic variables reductions for bmps compared to the baseline and reduction rankings among all the cells could be acquired as shown in the results the parallelized algorithms of the framework increase computational efficiency and reduce execution time significantly author contribution conceptualization feng pan qingyu feng and ryan mcgehee methodology feng pan qingyu feng and ryan mcgehee software feng pan qingyu feng and ryan mcgehee validation feng pan and ryan mcgehee formal analysis feng pan and ryan mcgehee investigation feng pan and ryan mcgehee resources feng pan qingyu feng and ryan mcgehee data curation feng pan writing original draft preparation feng pan writing review and editing feng pan qingyu feng ryan mcgehee bernard engel dennis flanagan and jingqiu chen visualization feng pan ryan mcgehee and jingqiu chen supervision bernard engel and dennis flanagan project administration bernard engel and dennis flanagan funding acquisition bernard engel all authors have read and agreed to the published version of the manuscript software and data availability name of software apexmp developers feng pan qingyu feng ryan mcgehee software required taudem and gdal included in the package and tested on windows pcs with windows 10 64 bit system programming language python packages available https github com ryanpmcg apexmp database available needs author s permission to access and download https app globus org file manager origin id 001e9a6c 512e 11eb 87b7 02187389bd35 origin path 2f first available december 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank dr anurag srivastava of the department of soil and water systems university of idaho and dr jaehek jeong at the blackland research center temple texas for providing technical support feedback and tests of the interface 
25754,faced with persistent flooding and water quality challenges water managers are now seeking to build digital twins of surface water systems that combine sensor data with online models to better understand and control system dynamics towards this goal this study presents pipedream an end to end simulation engine for real time modeling and state estimation in natural urban drainage networks the engine combines i a new hydraulic solver based on the one dimensional saint venant equations and ii a kalman filtering scheme that efficiently updates hydraulic states based on observed data using sensor data from a real world watershed we find that the simulation engine is effective at both interpolating hydraulic states and forecasting future states based on current measurements by providing a complete real time view of system hydraulics our software will enable rapid detection of flooding improved characterization of maintenance and remediation needs and robust real time control for both small scale stormwater and large scale river reservoir networks keywords hydraulic modeling data assimilation storm water kalman filtering continuous modeling real time sensing and control 1 introduction in the wake of growing urban populations aging infrastructure and more frequent extreme weather events many cities are struggling to manage stormwater related challenges such as flash flooding and combined sewer overflows kerkez et al 2016 engineers have traditionally responded to these challenges by expanding stormwater infrastructure and implementing best management practices rosenberg et al 2010 however despite the high costs of these interventions it is often difficult to evaluate their benefits and impacts due to i a lack of real time data and ii a limited understanding of system dynamics kerkez et al 2016 wong and kerkez 2016 interventions may fail to achieve performance targets due to design oversights insufficient maintenance or changing hydrologic conditions blecken et al 2015 wright and marchese 2017 in certain pathological cases measures aimed at improving flood control and urban water quality may actually worsen the problems they are intended to solve criss 2001 emerson et al 2005 in the absence of continuous monitoring these issues may remain undetected until negative impacts have already accrued to ensure that stormwater systems achieve desired outcomes water managers are now seeking to build digital twins of real world networks that use embedded sensors and online models to monitor system dynamics in real time while these digital twins vary widely in scope and sophistication they share the general goal of integrating sensor data hydrodynamic models and data assimilation techniques in order to assess system performance and determine effective control strategies using these technologies operators can detect abnormal conditions within sewer networks and then dispatch maintenance crews to make repairs before damage occurs wright and marchese 2017 montestruque and lemmon 2015 moreover when combined with real time control rtc continuous monitoring has shown impressive results in reducing combined sewer overflows montestruque and lemmon 2015 gelormino and ricker 1994 van nooijen and kolechkina 2013 puig et al 2009 andersen et al 1997 reducing operational costs wright and marchese 2017 montestruque and lemmon 2015 and improving urban water quality gaborit et al 2013 middleton and barrett 2008 jacopin et al 2001 carpenter et al 2013 while digital twins promise to address many of stormwater management s biggest pain points adoption has been hampered by a lack of sufficient tools and theory for online hydraulic modeling and real time data assimilation in practice hydraulic models are primarily executed in batch mode for the purposes of sizing pipes and evaluating infrastructure expansions karmous edwards 2019 popular stormwater models are oriented towards this use case and relatively few software packages provide support for online modeling or data assimilation consequently for real world systems that seek to implement continuous monitoring the underlying process model is often substantially simplified for instance assuming that the stormwater network behaves as a series of cascading linear reservoirs puig et al 2009 these simplifications may introduce uncertainty when interpolating or forecasting system states which in turn may impair the performance of real time control strategies new tools are needed to enable online state estimation and control capabilities while at the same time ensuring that system dynamics are accurately represented to pave the way for continuous monitoring and control of urban drainage systems we introduce pipedream a software toolkit for building digital twins of stormwater networks this toolkit consists of i an efficient hydraulic solver based on the full one dimensional saint venant equations and ii an implicit kalman filtering approach that updates system states based on streaming sensor data see fig 1 using the full one dimensional saint venant equations ensures that the model adequately captures the physics of a wide variety of real world stormwater networks similarly kalman filtering enables fast assimilation of sensor data while at the same time respecting the dynamics of the physical system pipedream can be run in online mode advancing forward in sync with the real world system and assimilating sensor measurements in real time moreover the toolkit provides a robust interface for executing dynamic controls through the use of adjustable orifices weirs and pumps taken together the software described in this paper provides an end to end framework for real time monitoring and control that will enable system operators to better manage stormwater infrastructure 2 prior work previous efforts towards interactive stormwater modeling have largely focused on developing enhancements to existing models such as the epa stormwater management model swmm huber et al 2005 one of the earliest of these efforts was matswmm a matlab python library aimed at simulating real time control strategies for stormwater systems riaño briceño et al 2016 the matswmm library provides bindings for the swmm hydrodynamic solver and also offers tools for implementing control strategies however at the time of writing the project is no longer actively maintained pyswmm is an actively maintained python library that also provides interactive bindings for swmm allowing users to modify system states and simulate real time control strategies mcdonnell et al 2020 while pyswmm has been used to great effect in simulating real time control strategies within industry its application to continuous monitoring has been limited by the fact that swmm does not offer a state space representation of system dynamics making it difficult to apply model based data assimilation techniques like kalman filtering 1 1 although it is possible to assimilate data using a surrogate model when the underlying process model is unavailable todini 2001 this approach often lacks the performance guarantees of model based data assimilation approaches simon 2006 on the data assimilation side a significant body of work has explored the problem of state parameter estimation in hydrologic systems much of this research focuses on the problem of streamflow forecasting in large river basins liu et al 2012 these studies seek to improve streamflow forecasts by using data assimilation techniques to correct initial soil moisture conditions seo et al 2003 thirel et al 2010 weerts et al 2010 snow water storage dechant and moradkhani 2011 and rainfall forcings seo et al 2003 a variety of data assimilation approaches have been investigated including variational data assimilation seo et al 2003 best linear unbiased estimation thirel et al 2010 particle filtering dechant and moradkhani 2011 van leeuwen 2009 and ensemble kalman filtering weerts et al 2010 dechant and moradkhani 2011 pauwels and lannoy 2009 while most of the literature focuses on offline data assimilation a few studies have examined data assimilation in a real time context schwanenberg 2011 develop a data assimilation approach based on ensemble kalman filtering to assist with real time control of large delta river systems schwanenberg et al 2011 however their approach uses the kinematic wave approximation of the saint venant equations rendering it unsuitable for stormwater networks with bidirectional flow in the context of stormwater networks the application of data assimilation has been more limited in practice efforts towards incorporating sensor data into stormwater models have mainly focused on model parameter calibration dotto et al 2012 schaefli et al 2007 schoups and vrugt 2010 thyer et al 2009 renard et al 2010 online state estimation has generally been achieved through model specific deterministic methods that are difficult to reconcile with modern state estimation theory hansen et al 2014 borup 2014 studies that explore more formal state estimation techniques generally rely on simplified conceptual models hutton et al 2014 for instance present a method for applying a kalman filter to a simplified stormwater model based on a series of cascading linear reservoirs hutton et al 2014 while breinholt et al 2011 2012 investigate the application of extended kalman filtering to a lumped conceptual urban rainfall runoff model breinholt et al 2011 breinholt et al 2012 recent work has demonstrated the use of data assimilation for soft sensing in urban drainage systems using lumped surrogate models with ensemble kalman filtering enkf lund et al 2019 dynamic updating of existing urban drainage models via enkf palmitessa et al 2020 and short term water level forecasting using neural networks palmitessa et al 2021 while this line of research shows the potential for data assimilation to improve our understanding of stormwater system performance more work is needed to integrate robust data assimilation techniques with our best knowledge of system dynamics as represented by physically based models to our knowledge there is currently no fully physically based interactive stormwater model that provides real time data assimilation capabilities to fill this need we present pipedream a new digital twin model for stormwater networks 3 methods in this section we describe the development of the pipedream toolkit including the theory behind the hydrodynamic model its numerical implementation and the development of a state estimation procedure for fusing real time sensor data first we develop and implement a robust hydraulic solver that facilitates data assimilation by providing a state space model of the stormwater system section 3 1 next we combine the hydraulic solver with an implicit kalman filter to facilitate real time assimilation of sensor data into the dynamical model section 3 4 finally drawing on sensor data from a real world stormwater network we test the data assimilation framework by evaluating its efficacy at interpolating and forecasting system states section 3 8 3 1 overview of the hydraulic solver to enable real time state estimation in urban drainage systems we first develop and implement a new hydraulic solver that i enables real time interactive usage and ii provides a state space representation of system dynamics the hydraulic solver developed in this study is based on an implicit staggered grid scheme for solving the one dimensional saint venant equations in sewer channel networks ji 1998 this formulation has demonstrated impressive accuracy and stability in previous applications and variants of the scheme are used in a number of proprietary software packages including gssha and sewercat zahner 2004 freni et al 2003 more crucially for this study this numerical scheme facilitates the use of robust state estimation techniques like kalman filtering by enabling the system dynamics to be cast in the form of an implicit state space model in this section we discuss the basic solver formulation and show how the model can be reinterpreted as an implicit locally linear state space system we also present several improvements to the original solver formulation that enhance model stability enable representation of backwater effects and extend the applicability of the model to systems with dynamic control structures the hydraulic solver is a finite difference model with four distinct types of computational elements links junctions superlinks and superjunctions ji 1998 links are finite volumes that represent sections of conduit or open channel junctions connect links together and may represent manholes grade changes or simply extra computational elements superlinks are collections of links joined end to end by junctions with no branching superjunctions connect one or more superlinks together and may represent storage basins branching locations or invert discontinuities to facilitate the computation of pressurized flow each link is equipped with a priessman slot a fictitious narrow groove located at the crown of the link that allows the one dimensional unsteady open channel flow equations to be applied to surcharged pipes cunge 1980 fig 2 shows an example network adapted from ji 1998 ji 1998 with all basic elements of the model labeled flows within the channel network are modeled using the one dimensional saint venant equations this pair of nonlinear partial differential equations consists of two parts the continuity equation 1 which describes the mass balance for a finite volume and the momentum equation 2 which describes the force balance 1 a t q x q i n 2 q t x q u g a h x s 0 s f s l 0 where q is discharge a is the cross sectional area of flow u is the average velocity h is depth x is distance t is time q in is the lateral inflow per unit width and s o s f and s l represent the channel bottom slope friction slope and local head loss slope respectively using a staggered grid formulation the continuity equation is applied to each junction indexed by ik while the momentum equation applied to each link indexed by ik the equations are discretized using a backward euler type implicit scheme see sections s2 and s3 in the supplementary information for derivations the continuity equation is discretized as 3 q i k t δ t q i 1 k t δ t b i k δ x i k 2 b i 1 k δ x i 1 k 2 a s i k h i k t δ t h i k t δ t q i n i k while the momentum equation is discretized as 4 q i k t δ t q i k t δ x i k δ t u i 1 k q i 1 k t δ t u i k q i k t δ t g a i k h i 1 k t δ t h i k t δ t g a i k s o i k δ x i k g a i k s f i k s l i k δ x 0 where b is the top width of flow a s is the junction surface area and q in is the exogenous flow input the boundary conditions for each superlink are supplied by the upstream and downstream superjunction heads assuming weir like flow at the superlink inlet and outlet 5 q c a 2 g δ h where c is the inlet outlet discharge coefficient and δh is the difference in head between the superjunction and the adjacent superlink boundary junction the hydraulic model solves for all unknowns simultaneously at each time step by embedding the solutions to the saint venant equations into a system of implicit linear equations wherein all unknowns are defined in terms of the unknown superjunction heads 1 first the discretized saint venant equations are reformulated into recurrence relations that relate junction heads and link flows within each superlink 2 the assumption of orifice like flow between superjunctions and superlinks is used to establish boundary conditions for the superlink inlets and outlets 3 combining the recurrence relations together with the superlink boundary conditions the system is reformulated as a sparse matrix equation with all unknowns expressed in terms of the unknown superjunction heads 4 after solving for the unknown superjunction heads the internal depths and flows within each superlink are recovered by substituting the superjunction boundary heads into the previously developed recurrence relations this solution procedure affords a balance between stability and computational efficiency while the implicit discretization scheme helps to ensure stability the use of recurrence relations reduces the size of the solution matrix and increases computation speed compared to the more conventional four point implicit scheme ji 1998 however in addition to its computational advantages this scheme also facilitates data assimilation by casting the solver in the form of a locally linear state space system the pipedream toolkit exploits this fact by applying a kalman filter to fuse sensor measurements into the dynamical model with the kalman filter active the solution procedure is modified to assimilate sensor data at each time step 1 first the solver steps forward in time computing the hydraulic heads at all superjunctions 2 the kalman filter corrects the superjunction heads at monitored locations by fusing sensor data into the model 3 internal depths and flows are recomputed based on the corrected superjunction heads thereby propagating the correction to the rest of the system the following sections describe the theory and implementation of the hydraulic solver and kalman filter in detail and also describe improvements to the solver that were implemented in this study 3 2 enhancements to the hydraulic solver to enable representation of a broader array of real world stormwater networks we make substantial additions to the original numerical scheme these changes allow for modeling of real world stormwater networks that would be either impractical or impossible under the original formulation 3 2 1 control structures to facilitate simulation and execution of real time controls we modify the numerical scheme to enable representation of orifices weirs and pumps see sections s9 s10 and s11 the governing equations for these control structures are embedded directly into the system solution matrix facilitating the use of classical state estimation and control algorithms for linear time varying systems 3 2 2 mobile computational elements the original numerical scheme suffers from instability when modeling backwater effects we correct this problem by introducing mobile computational elements that track discontinuities in the water surface profile see section s12 3 2 3 accuracy improvements high gradient conditions were found to induce mass balance errors under the original formulation to reduce these errors we re derive the recurrence relations and superlink boundary conditions to eliminate some error generating assumptions see sections s4 and s5 3 2 4 adaptive step size control implicit solvers typically maintain better stability at large time steps when compared to explicit solvers especially during near steady state conditions to exploit this strength we implement an adaptive time stepping algorithm that allows the solver to use a small timestep during highly transient conditions and a large timestep during weakly transient conditions see section s13 söderlind 2003 3 2 5 infiltration runoff coupling to enable the use of pipedream as a standalone stormwater management model we implement an infiltration module based on the well known green ampt formulation green and ampt 1911 this module is coupled bi directionally with the hydraulic solver to enable more accurate representation of overland flow and runoff generation see section s14 3 3 constructing the state space system through derivation of the solution matrix equation we show that the hydraulic model is equivalent to a locally linear time varying state space system which in turn allows for the application of algorithms from signal processing and control theory such as feedback control modal analysis and as we show in this paper kalman filtering in the following section we derive the solution matrix equation and show how it is equivalent to a locally linear state space system detailed derivations including the discretization of governing equations and the development of the recurrence relations are omitted from this section and can instead be located in the supplementary information si document the solution matrix equation is derived by applying the continuity equation to all superjunctions and then using recurrence relations to express all unknown variables in terms of the superjunction heads focusing on a single superjunction j the continuity equation requires that the change in storage volume over a time interval δt equal the sum of inflows minus outflows 6 ℓ 1 n k d j q d k ℓ t δ t m 1 n k u j q u k m t δ t q i n j t δ t a s j h j t δ t h j t δ t where q dk is the discharge at the downstream end of superlink k q uk is the discharge at the upstream end of superlink k q in j is the external inflow into superjunction j a sj is the surface area of superjunction j and h j is the total head at superjunction j nkd j represents the number of superlinks with their downstream ends attached to superjunction j while nku j represents the number of superlinks with their upstream ends attached to superjunction j thus the first two terms represent the sum of inflows minus outflows from all superlinks attached to superjunction j using the recurrence relations developed in si section s4 the discharge at the upstream and downstream ends of each superlink can be described using the following linear functions of the upstream and downstream superjunction heads at time t δt 7 q u k t δ t α u k h j u k t δ t β u k h j d k t δ t χ u k 8 q d k t δ t α d k h j u k t δ t β d k h j d k t δ t χ d k where h juk represents the head at the superjunction upstream of superlink k and h jdk represents the head at the superjunction downstream of superlink k the α β and χ coefficients are functions of the depths and flows inside each superlink that incorporate the solutions to the continuity and momentum equations in both the forward and backward directions substituting these linearized expressions into the continuity balance for superjunction j yields the linear equation 9 f j j h j t δ t ℓ 1 n k d j φ j j u k ℓ h j u k ℓ t δ t m 1 n k u j ψ j j d k m h j d k m t δ t a s j δ t h j t q i n j t δ t g j where 10 f j j a s j δ t m 1 n k u j α u k m ℓ 1 n k d j β d k ℓ 11 φ j j u k ℓ α d k ℓ 12 ψ j j d k m β u k m 13 g j ℓ 1 n k d j χ d k ℓ m 1 n k u j χ u k m applying these equations to each superjunction j yields the following implicit state space equation 14 z t x t δ t y t x t u t δ t s t where x t is the state vector of superjunction heads at the current time step and x t δt is the state vector of superjunction heads at the next time step the elements of z t are defined such that z t j j f j j z t j j u k ℓ φ j j u k ℓ and z t j j d k m ψ j j d k m with all other elements of z t equal to zero similarly the elements of s t are defined such that s t j g j y t is a diagonal matrix representing the current superjunction storage 15 y t diag a s j δ t j 1 2 m the exogenous input u t δt represents the external inflow e g runoff to each superjunction j 16 u t δ t q i n 1 t δ t q i n 2 t δ t q i n m t δ t t to illustrate the construction of the solution matrix in concrete terms we include here the matrix equation for the example network in fig 2 with flow inputs on superjunctions 1 and 3 and head boundary conditions on superjunctions 4 and 6 as originally described in ji 1998 17 f 1 1 ψ 1 2 0 0 0 0 φ 2 1 f 2 2 ψ 2 3 0 ψ 2 5 0 0 φ 3 2 f 3 3 ψ 3 4 φ 3 5 0 0 0 0 1 0 0 0 φ 5 2 ψ 5 3 0 f 5 5 ψ 5 6 0 0 0 0 0 1 h 1 t δ t h 2 t δ t h 3 t δ t h 4 t δ t h 5 t δ t h 6 t δ t a s 1 δ t h 1 t g 1 q i n 1 t δ t a s 2 δ t h 2 t g 2 a s 3 δ t h 3 t g 3 q i n 3 t δ t h b c 4 t δ t a s 5 δ t h 4 t g 5 h b c 6 t δ t this sparse system is solved for the unknown left hand vector of superjunction heads at time step t δt once the superjunction heads are known the depths and discharges at the upstream and downstream superlink boundaries are computed finally the recurrence relations are used to solve for the internal depths and flows within each superlink 3 4 kalman filter having defined the system dynamics in terms of an implicit locally linear system a kalman filter is formulated to fuse sensor data with the dynamical model kalman filtering is a recursive bayesian estimation algorithm that i uses a dynamical system model to generate a prior estimate of system states then ii updates this prior with observed data to produce a posterior estimate kalman 1960 it can be shown that the kalman filter is the optimal linear estimator for system states when the dynamical system model is perfectly accurate and the noise is white and gaussian with known covariance kalman 1960 in the following section we present the general kalman recursion for a linear time varying system and show how it is applied to the hydraulic model developed in the previous section as a starting point note that the implicit state equation 14 may be rewritten as 18 x t δ t a t x t b t u t δ t y t where a t z t 1 y t b t z t 1 and y t z t 1 s t 2 2 note that for the case where z t is very large and or ill conditioned an implicit version of the kalman filter may be preferred to inverting z t directly skliar and ramirez 1997 we may thus express the internal states and observed outputs of the system in terms of a hidden state equation along with an observed output equation we will assume that the state equation is corrupted by some stochastic disturbance w t while the output equation is corrupted by measurement noise v t 19 x t δ t a t x t b t u t δ t y t w t 20 z t h t x t v t here x t is the n dimensional state vector at the current time step u t is an ℓ dimensional input vector w t is a p dimensional stochastic disturbance z t is the m dimensional observed state and v t is the m dimensional vector of measurement noise a t is the n n state transition matrix b t is an n ℓ input transition matrix y t is an n 1 vector of constants and h t is the m n measurement transition matrix the stochastic disturbance w t and measurement noise v t are assumed to be random vectors of zero mean gaussian white noise with known covariance under these conditions the minimum mean squared error estimator of the state x t is given by 21 x t δ t a t x t b t u t δ t y t l t δ t z t δ t z t δ t 22 z t δ t h t δ t a t x t b t u t δ t y t here we recursively correct the estimate x t by feeding back the difference between the observed measurement z t and our estimate of the observed output z t as predicted by the dynamical model the weighting factor that is applied to this difference l t is the optimal kalman gain and is given by 23 l t p t h t t h t t p t h t v t 1 where v t is the covariance of the measurement noise v t p t e x t x t x t x t t is the estimation error covariance matrix which is given recursively as 24 p t δ t a t p t p t h t t h t p t c t t v t 1 h t p t a t t w t where w t is the covariance of the stochastic disturbance w t the kalman gain l t and the error covariance p t are re computed at each time step and then used to recursively correct the estimate of system states x t the data fusion procedure proceeds as follows at each time step the hydraulic solver is advanced to determine the hydraulic heads and update the state transition matrices next the kalman recursion is applied to correct the hydraulic heads at time step t δt based on observed data finally the superlink boundary conditions and internal states are computed based on the updated superjunction heads this process is repeated indefinitely potentially in real time until the simulation is terminated 3 5 hydrology to establish pipedream as a standalone stormwater model we include a hydrologic module that computes infiltration and runoff using the green ampt method at each time step the integrated form of the green ampt equation is solved to estimate the cumulative infiltration depth for a soil element indexed by f 25 h f t δ t k s δ t h f t ψ f θ d log h f t δ t ψ f θ d log h f t ψ f θ d where h f t δ t is the cumulative infiltration depth at time t δt m k s is the saturated hydraulic conductivity m s ψ f is the suction head of the wetting front m and θ d is the soil moisture deficit unitless the infiltration rate and runoff are then computed from the cumulative infiltration depth the details of the runoff computation are presented in section s14 soil elements in the infiltration model may be coupled with any junction link or superjunction in the hydraulic model this design enables representation of processes where hydraulics and infiltration are interdependent overland flow for instance is represented by coupling infiltration elements with a wide rectangular superlink low impact development may be simulated by combining an infiltration element with a superjunction and an appropriate hydraulic control structure the coupling of hydraulics and infiltration distinguishes pipedream from existing models like swmm in which infiltration and hydraulics are treated as separate processes that are computed sequentially 3 6 software architecture fig 1 illustrates the complete architecture of the pipedream toolkit including the hydraulic solver the hydrologic module and the kalman filter the schematic indicates i the major components of the toolkit ii the ways in which these components exchange data and iii the inputs and outputs to each component here we briefly describe the software architecture with a focus on model inputs and outputs and how they interact with the hydraulic hydrologic and data assimilation components of the pipedream toolkit respectively inputs to a pipedream simulation include hydraulic and hydrologic forcings hydraulic control actions and sensor data for assimilation these inputs may be provided in either streaming or bulk format inputs to the hydrologic module include rainfall forcings which are used to compute runoff and infiltration inputs to the hydraulic model include direct inflows such as those contributed by wastewater treatment facilities boundary heads such as those imposed by tidal boundary conditions and real time controls which include weir and orifice positions as well as pumping actions as discussed earlier runoff from the hydrologic model is an input to the hydraulic model while ponding computed by the hydraulic model is an input to the hydrologic model the kalman filter accepts inputs in the form of depth or pressure sensor data at superjunctions and fuses these measurements into the hydraulic solver outputs of a simulation include hydraulic and hydrologic states at each timestep hydraulic states include hydraulic heads at superjunctions discharges in internal links and depths at internal junctions hydrologic states include infiltration and runoff rates for each soil element all model states may be programmatically accessed and modified by the user at any time this design allows for the pipedream toolkit to easily interface with external software such as visualization tools weather forecast apis and even scada software in addition to providing programmatic access to system states bulk simulation results at sampled time intervals may also be written to static files for subsequent analysis 3 7 implementation the pipedream toolkit is implemented in the python programming language which provides a powerful interpreter environment for interactive use van rossum 1995 acceleration of numerical code is realized using the numba just in time compiler which compiles native python code to machine code that achieves speeds comparable to code written in c or fortran lam et al 2015 we accelerate the solution of the system matrix equation by using a banded matrix solver upon model initialization the system matrix is automatically permuted into a banded form using the reverse cuthill mckee algorithm cuthill and mckee 1969 all code and data for this study are available at github com mdbartos pipedream 3 8 model validation we assess the pipedream toolkit by applying our state estimation methodology to a real world stormwater network and then evaluating the extent to which the kalman filter improves the accuracy of interpolated and forecasted system states first a real world catchment is selected and real time depth data is collected at four sites we then construct a pipedream model of the catchment and force the model with a real world storm event we then use a holdout cross validation approach to measure the extent to which fusing sensor data at selected sites reduces error at the holdout sites we also evaluate the ability of the kalman filter to forecast system states by fusing sensor measurements at 1 h intervals and quantifying the reduction in error over the remainder of each hour 3 3 a 1 h interval is chosen for the forecasting assessment because this sampling frequency is typical for telemetered stream gage data from a wireless sensor network these two applications interpolation and forecasting represent important use cases for real time state estimation with continuous monitoring our case study focuses on a 5 85 km2 urban watershed located in the midwestern united states fig 3 this watershed is the subject of a long term monitoring project led by the authors and thus features roughly two years of continuous sensor data bartos et al 2018 roughly 47 of the catchment is impervious with a majority of the impervious area located towards the downstream half of the catchment the representation of the channel network is derived from survey data and engineering drawings that describe the network topology hydraulic geometries storage curves and various hydraulic and hydrologic parameters needed to properly model catchment dynamics to characterize the response of the catchment wireless ultrasonic depth sensors are installed at four locations in series along the mainstem of the watershed numbered in increasing order from 1 to 4 in the downstream direction these sensors continuously report the distance to the water surface at an adaptive sampling rate that ranges from roughly 2 min to 1 h with a manufacturer specified reading to reading error of approximately 1 mm sensors at sites 1 and 2 measure the water level in two relatively large retention basins the sensor at site 3 measures the water level in an outlet box directly downstream of a third retention basin overflow from this retention basin drains into the outlet box through a rectangular weir that is approximately 5 m wide finally the sensor at site 4 measures the water level at the downstream end of a rectangular flume the rectangular flume is roughly 5 m wide and is connected at the upstream end to a fourth retention basin raw sensor data is preprocessed using a combination of manual and automatic filtering techniques invalid readings and sensor spikes are flagged and removed using an automated filtering routine consisting of range checks and threshold tests on the second derivative of the sensor signal after the initial quality control distance measurements are converted to water depth estimates using field measurements of the sensor offset to the channel bottom we apply the new hydraulic model to a real world storm event occurring on august 8 2017 this rain event is selected because i all sensor sites were active and reporting during this storm and ii the peak depth of the generated hydrograph is close to the median peak depth for the period of available data meaning that the storm event is representative of a typical storm event in this location precipitation intensity data are collected from two weather stations operated by weather underground near the catchment of interest weather underground 2019 runoff is generated from the rainfall using the green ampt formulation and then fed into the hydraulic model for the purposes of this analysis we assume uniform rainfall intensity over the catchment and use the average intensity between the two gages as input to the model the kalman filter is then applied to fuse depth sensor data into the hydraulic model we evaluate the extent to which kalman filtering improves interpolation and forecasting of depths and flows throughout the network we also perform a sensitivity analysis to gage the effect of uncertain parameters on the results namely we assess the sensitivity of the results to the assumed measurement and process covariances which reflect the relative confidence in the sensor data and model output respectively see section s16 4 results 4 1 interpolation using a holdout cross validation approach we find that the kalman filter is effective at interpolating system states at ungaged locations fig 4 shows the result of the holdout cross validation assessment for this experiment the filter is applied to sensor sites 1 and 3 and the output of the updated model is compared with sensor measurements at sites 2 and 4 from table 1 it can be seen that the filter reduces error at both holdout sites while the model performs well on its own the kalman filter reduces the mean absolute error mae at site 2 by 55 and at site 4 by 44 for site 2 a majority of the error is reduced at the peak of the hydrograph while at site 4 a majority of the error is reduced at the falling limb because the filter improves model accuracy even at locations where it is not directly applied the holdout assessment suggests that the kalman filter pushes the system closer to its actual state rather than simply overfitting individual sites to measured data the kalman filter is capable of correcting error introduced by uncertain dynamical inputs distinguishing it from a calibration only approach from fig 4 it can be seen that the model without filtering over predicts the discharge at sites 1 and 2 but under predicts at sites 3 and 4 this result suggests that the spatial heterogeneity of runoff is a major source of error this type of error is difficult to counter with model calibration alone given that calibration tends to target static parameters of the system such as channel roughness coefficients and impervious area percentages however spatial variability in runoff may also result from spatial variability in the originating rainfall field in contrast to continuous calibration the kalman filter handles this contingency by correcting system states in real time adding and removing mass from the system to match field observations this approach is robust to both parameter and input uncertainty making it especially suitable for real time applications in which the driving input is often uncertain or unknown 4 2 forecasting by adaptively correcting system dynamics kalman filtering pushes the hydraulic system closer to its measured state and thus enables improved forecasting of system behavior however not all sites offer the same forecasting benefit and sensor sites must be selected judiciously to maximize the accuracy of the forecast fig 5 shows the result of using the kalman filter to forecast system states at 1 h intervals in this application sensor data for each site is fused at the first minute of each hour indicated by circular markers and the model is then propagated forward in time to forecast system behavior for the remainder of the hour the effectiveness of the forecast is then gaged based on the reduction in mae and the length of time that the correction persists in general the most effective sites for forecasting are those for which the volume of storage is large compared to the volume of water entering and exiting the control volume when the filter is applied to correct the hydraulic head at a large retention pond site 1 as shown in fig 5 left the effect of the correction persists for the remainder of the hourly interval in other words applying the correction significantly changes the trajectory of the hydraulic head compared to the trajectory produced by the model alone the correction imposed by the filter increases the accuracy of the forecasted states measured by mae compared to the model only forecast sites with larger storage capacity are most effective because small changes in hydraulic head result in relatively large changes to the mass and energy balance of the system essentially allowing greater control over the state space by contrast sites with smaller storage capacity offer less forecasting benefit given that the effect of the correction is quickly overpowered by dynamics originating from elsewhere in the system at the outlet flume site 4 for example the proportion of mass entering and leaving the control volume at each time step is relatively large compared to the mass within the control volume itself thus correcting the system state at this location does not significantly alter the amount of mass or energy in the system and the effect of the correction persists for only a short time after the filter is applied as shown in fig 5 right the trajectory of the hydraulic head quickly returns to the model only trajectory after each application of the filter thus sensor data from sites with little storage capacity must be fused at a rapid frequency on the order of the time step used by the hydraulic solver in order to produce noticeable forecasting benefit 4 3 computational performance the pipedream model offers significantly improved model run times compared to pyswmm an existing interactive solver for stormwater networks model run time comparisons are conducted on networks α β and ε from the pystorms benchmarking library with network sizes ranging from 26 to 210 nodes superjunctions rimer et al 2019 when compared against pyswmm pipedream ran between 15 and 190 times faster with performance gains increasing for larger numbers of computational elements for many networks this multiple order of magnitude run time improvement could mean the difference between offline usage and true real time execution when compared with swmm5 in batch execution pipedream is currently about 2 1 3 4 times slower for the benchmark scenarios chosen however pipedream was able to achieve more consistently stable results in particular for the β network pipedream was able to achieve stable results while swmm was not even when using a small timestep 0 1 s table 2 shows a detailed breakdown of model performance for all scenarios figs s4 s6 compare model outputs for pipedream and swmm under each scenario additional information about each test network and storm scenario may be found in section s15 5 discussion by enabling real time interpolation and forecasting of hydraulic states pipedream provides a powerful new tool for urban flash flood nowcasting the dynamics of urban flash flooding are complex and spatially heterogeneous to the effect that there is no existing model that is capable of reliably forecasting flash floods in urban catchments hapuarachchi et al 2011 while many cities use gage networks to help detect flooding and communicate flood alerts gages are generally restricted to larger streams leaving significant blind spots in the drainage network habibi et al 2019 to address this problem the data assimilation methodology presented in this paper will allow emergency managers to better estimate localized flooding at ungaged locations by interpolating hydraulic states from locations where sensor data is available these high resolution flood estimates will enable new forms of rapid flood response such as localized alerts for motorists and targeted dispatch of emergency services moreover by correcting system states in real time the pipedream toolkit will assist with flood forecasting enabling emergency managers to more accurately predict downstream flooding at longer lead times especially in cases where flooding is primarily driven by upstream transport in addition to detecting localized floods the pipedream toolkit will also assist in identifying maintenance emergencies timely and accurate diagnosis of maintenance issues is essential for effective stormwater management short term maintenance emergencies such as storm drain blockages can lead to localized flooding while long term maintenance issues such as sediment accumulation can degrade the overall performance of the stormwater system wright and marchese 2017 however distinguishing true maintenance emergencies from spurious sensor faults remains a persistent challenge especially when attempting to diagnose anomalies from sensor data alone ni et al 2009 pipedream handles this ambiguity by combining sensor data with a dynamical model of the stormwater system by offering two independent estimates of system states pipedream makes it easier to track the source of anomalies and differentiate true emergencies from false positives moreover pipedream natively encodes the relative confidence in sensor model outputs through the measurement process covariances respectively allowing users to incorporate prior information about sensor faults directly into the data assimilation procedure perhaps most importantly the pipedream toolkit provides a foundation for real time control of urban drainage systems many cities are now implementing or seeking to implement real time control systems for urban drainage systems in order to improve performance cut costs or mitigate stormwater related hazards in both modeling studies and real world deployments real time control has shown proven results in mitigating combined sewer overflows and improving urban water quality montestruque and lemmon 2015 gelormino and ricker 1994 van nooijen and kolechkina 2013 puig et al 2009 andersen et al 1997 gaborit et al 2013 middleton and barrett 2008 jacopin et al 2001 carpenter et al 2013 however effective real time control is predicated on an accurate representation of system states by fusing sensor data with an accurate physically based process model the pipedream toolkit provides a firm basis for control whether for the purposes of simulation or real world execution crucially pipedream integrates control structures into the internal state space model facilitating native use of classical control algorithms such as model predictive control and linear quadratic regulation 5 1 future work the pipedream toolkit provides practitioners and researchers with a powerful new tool for building digital twins of stormwater systems however important questions remain about how this tool should best be applied to real world use cases we recommend the following future work focused on characterizing and improving the scalability and reliability of the model on the issue of scalability future work should examine the performance of pipedream for very large systems e g tens of thousands of elements in this study we evaluate model performance for networks of up to 210 superjunctions while pipedream scales favorably in comparison with swmm and pyswmm for networks of this size larger networks may present problems for implicit solvers that must solve a large system of equations simultaneously for these networks specialized algorithms for solving sparse matrix equations may be required saad 2003 in the context of data assimilation extensions to the conventional kalman filter may be required for very large systems for systems with many states and small process noise covariance for instance the error covariance matrix may become ill conditioned under these conditions the square root form of the kalman filter is often preferred verhaegen and dooren 1986 for even larger systems the error covariance matrix may become impractical to compute and an ensemble kalman filter enkf may be required evensen 1994 future work should investigate these extensions to the kalman filter algorithm and how they might be integrated into the existing modeling framework on the subject of model accuracy and reliability future work should examine how alternative numerical schemes for the saint venant equations perform in the context of real time data assimilation for this study we employed a first order implicit scheme with an upwind formulation for the advection term this scheme is highly stable and is thus well suited for running continuous simulations with data assimilation however higher order schemes may offer greater accuracy and result in reduced mass balance error future work should investigate these tradeoffs in the context of real time simulations future work should also examine the boundary conditions for the hydraulic solver in greater detail the assumption of weir like flow at the superlink boundaries helps to accelerate the solution of the system matrix however this formulation ignores the inertial component of the momentum balance at the interface between the superlinks and superjunctions inertial forces may be significant at the downstream end of conduits under supercritical flow conditions future work should investigate alternative formulations of the superlink boundary conditions that mitigate this source of error 6 conclusions in this study we develop a new toolkit for building real time digital twins of urban and natural drainage systems this toolkit consists of a robust hydraulic solver based on the full one dimensional saint venant equations along with an implicit kalman filtering methodology that facilitates assimilation of real time sensor data drawing on sensor data from a real world stormwater network we find that the implicit kalman filter is effective at both interpolating system states within the network and forecasting future states based on current measurements by providing a physically based methodology for state estimation in stormwater networks this toolkit will enable system operators to pre emptively detect and repair blockages leaks and other maintenance emergencies moreover by improving interpolation and forecasting of system states our toolkit will provide a strong foundation for model based real time control schemes such as model predictive control and linear quadratic regulation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments support for this project was provided by the j robert beyster computational innovation graduate fellowship the national science foundation earthcube initiative grant 1639640 and the national science foundation smart and connected communities program grant 1737432 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105120 data availability code and data links are available at https github com mdbartos pipedream permanent doi 10 5281 zenodo 3950373 
25754,faced with persistent flooding and water quality challenges water managers are now seeking to build digital twins of surface water systems that combine sensor data with online models to better understand and control system dynamics towards this goal this study presents pipedream an end to end simulation engine for real time modeling and state estimation in natural urban drainage networks the engine combines i a new hydraulic solver based on the one dimensional saint venant equations and ii a kalman filtering scheme that efficiently updates hydraulic states based on observed data using sensor data from a real world watershed we find that the simulation engine is effective at both interpolating hydraulic states and forecasting future states based on current measurements by providing a complete real time view of system hydraulics our software will enable rapid detection of flooding improved characterization of maintenance and remediation needs and robust real time control for both small scale stormwater and large scale river reservoir networks keywords hydraulic modeling data assimilation storm water kalman filtering continuous modeling real time sensing and control 1 introduction in the wake of growing urban populations aging infrastructure and more frequent extreme weather events many cities are struggling to manage stormwater related challenges such as flash flooding and combined sewer overflows kerkez et al 2016 engineers have traditionally responded to these challenges by expanding stormwater infrastructure and implementing best management practices rosenberg et al 2010 however despite the high costs of these interventions it is often difficult to evaluate their benefits and impacts due to i a lack of real time data and ii a limited understanding of system dynamics kerkez et al 2016 wong and kerkez 2016 interventions may fail to achieve performance targets due to design oversights insufficient maintenance or changing hydrologic conditions blecken et al 2015 wright and marchese 2017 in certain pathological cases measures aimed at improving flood control and urban water quality may actually worsen the problems they are intended to solve criss 2001 emerson et al 2005 in the absence of continuous monitoring these issues may remain undetected until negative impacts have already accrued to ensure that stormwater systems achieve desired outcomes water managers are now seeking to build digital twins of real world networks that use embedded sensors and online models to monitor system dynamics in real time while these digital twins vary widely in scope and sophistication they share the general goal of integrating sensor data hydrodynamic models and data assimilation techniques in order to assess system performance and determine effective control strategies using these technologies operators can detect abnormal conditions within sewer networks and then dispatch maintenance crews to make repairs before damage occurs wright and marchese 2017 montestruque and lemmon 2015 moreover when combined with real time control rtc continuous monitoring has shown impressive results in reducing combined sewer overflows montestruque and lemmon 2015 gelormino and ricker 1994 van nooijen and kolechkina 2013 puig et al 2009 andersen et al 1997 reducing operational costs wright and marchese 2017 montestruque and lemmon 2015 and improving urban water quality gaborit et al 2013 middleton and barrett 2008 jacopin et al 2001 carpenter et al 2013 while digital twins promise to address many of stormwater management s biggest pain points adoption has been hampered by a lack of sufficient tools and theory for online hydraulic modeling and real time data assimilation in practice hydraulic models are primarily executed in batch mode for the purposes of sizing pipes and evaluating infrastructure expansions karmous edwards 2019 popular stormwater models are oriented towards this use case and relatively few software packages provide support for online modeling or data assimilation consequently for real world systems that seek to implement continuous monitoring the underlying process model is often substantially simplified for instance assuming that the stormwater network behaves as a series of cascading linear reservoirs puig et al 2009 these simplifications may introduce uncertainty when interpolating or forecasting system states which in turn may impair the performance of real time control strategies new tools are needed to enable online state estimation and control capabilities while at the same time ensuring that system dynamics are accurately represented to pave the way for continuous monitoring and control of urban drainage systems we introduce pipedream a software toolkit for building digital twins of stormwater networks this toolkit consists of i an efficient hydraulic solver based on the full one dimensional saint venant equations and ii an implicit kalman filtering approach that updates system states based on streaming sensor data see fig 1 using the full one dimensional saint venant equations ensures that the model adequately captures the physics of a wide variety of real world stormwater networks similarly kalman filtering enables fast assimilation of sensor data while at the same time respecting the dynamics of the physical system pipedream can be run in online mode advancing forward in sync with the real world system and assimilating sensor measurements in real time moreover the toolkit provides a robust interface for executing dynamic controls through the use of adjustable orifices weirs and pumps taken together the software described in this paper provides an end to end framework for real time monitoring and control that will enable system operators to better manage stormwater infrastructure 2 prior work previous efforts towards interactive stormwater modeling have largely focused on developing enhancements to existing models such as the epa stormwater management model swmm huber et al 2005 one of the earliest of these efforts was matswmm a matlab python library aimed at simulating real time control strategies for stormwater systems riaño briceño et al 2016 the matswmm library provides bindings for the swmm hydrodynamic solver and also offers tools for implementing control strategies however at the time of writing the project is no longer actively maintained pyswmm is an actively maintained python library that also provides interactive bindings for swmm allowing users to modify system states and simulate real time control strategies mcdonnell et al 2020 while pyswmm has been used to great effect in simulating real time control strategies within industry its application to continuous monitoring has been limited by the fact that swmm does not offer a state space representation of system dynamics making it difficult to apply model based data assimilation techniques like kalman filtering 1 1 although it is possible to assimilate data using a surrogate model when the underlying process model is unavailable todini 2001 this approach often lacks the performance guarantees of model based data assimilation approaches simon 2006 on the data assimilation side a significant body of work has explored the problem of state parameter estimation in hydrologic systems much of this research focuses on the problem of streamflow forecasting in large river basins liu et al 2012 these studies seek to improve streamflow forecasts by using data assimilation techniques to correct initial soil moisture conditions seo et al 2003 thirel et al 2010 weerts et al 2010 snow water storage dechant and moradkhani 2011 and rainfall forcings seo et al 2003 a variety of data assimilation approaches have been investigated including variational data assimilation seo et al 2003 best linear unbiased estimation thirel et al 2010 particle filtering dechant and moradkhani 2011 van leeuwen 2009 and ensemble kalman filtering weerts et al 2010 dechant and moradkhani 2011 pauwels and lannoy 2009 while most of the literature focuses on offline data assimilation a few studies have examined data assimilation in a real time context schwanenberg 2011 develop a data assimilation approach based on ensemble kalman filtering to assist with real time control of large delta river systems schwanenberg et al 2011 however their approach uses the kinematic wave approximation of the saint venant equations rendering it unsuitable for stormwater networks with bidirectional flow in the context of stormwater networks the application of data assimilation has been more limited in practice efforts towards incorporating sensor data into stormwater models have mainly focused on model parameter calibration dotto et al 2012 schaefli et al 2007 schoups and vrugt 2010 thyer et al 2009 renard et al 2010 online state estimation has generally been achieved through model specific deterministic methods that are difficult to reconcile with modern state estimation theory hansen et al 2014 borup 2014 studies that explore more formal state estimation techniques generally rely on simplified conceptual models hutton et al 2014 for instance present a method for applying a kalman filter to a simplified stormwater model based on a series of cascading linear reservoirs hutton et al 2014 while breinholt et al 2011 2012 investigate the application of extended kalman filtering to a lumped conceptual urban rainfall runoff model breinholt et al 2011 breinholt et al 2012 recent work has demonstrated the use of data assimilation for soft sensing in urban drainage systems using lumped surrogate models with ensemble kalman filtering enkf lund et al 2019 dynamic updating of existing urban drainage models via enkf palmitessa et al 2020 and short term water level forecasting using neural networks palmitessa et al 2021 while this line of research shows the potential for data assimilation to improve our understanding of stormwater system performance more work is needed to integrate robust data assimilation techniques with our best knowledge of system dynamics as represented by physically based models to our knowledge there is currently no fully physically based interactive stormwater model that provides real time data assimilation capabilities to fill this need we present pipedream a new digital twin model for stormwater networks 3 methods in this section we describe the development of the pipedream toolkit including the theory behind the hydrodynamic model its numerical implementation and the development of a state estimation procedure for fusing real time sensor data first we develop and implement a robust hydraulic solver that facilitates data assimilation by providing a state space model of the stormwater system section 3 1 next we combine the hydraulic solver with an implicit kalman filter to facilitate real time assimilation of sensor data into the dynamical model section 3 4 finally drawing on sensor data from a real world stormwater network we test the data assimilation framework by evaluating its efficacy at interpolating and forecasting system states section 3 8 3 1 overview of the hydraulic solver to enable real time state estimation in urban drainage systems we first develop and implement a new hydraulic solver that i enables real time interactive usage and ii provides a state space representation of system dynamics the hydraulic solver developed in this study is based on an implicit staggered grid scheme for solving the one dimensional saint venant equations in sewer channel networks ji 1998 this formulation has demonstrated impressive accuracy and stability in previous applications and variants of the scheme are used in a number of proprietary software packages including gssha and sewercat zahner 2004 freni et al 2003 more crucially for this study this numerical scheme facilitates the use of robust state estimation techniques like kalman filtering by enabling the system dynamics to be cast in the form of an implicit state space model in this section we discuss the basic solver formulation and show how the model can be reinterpreted as an implicit locally linear state space system we also present several improvements to the original solver formulation that enhance model stability enable representation of backwater effects and extend the applicability of the model to systems with dynamic control structures the hydraulic solver is a finite difference model with four distinct types of computational elements links junctions superlinks and superjunctions ji 1998 links are finite volumes that represent sections of conduit or open channel junctions connect links together and may represent manholes grade changes or simply extra computational elements superlinks are collections of links joined end to end by junctions with no branching superjunctions connect one or more superlinks together and may represent storage basins branching locations or invert discontinuities to facilitate the computation of pressurized flow each link is equipped with a priessman slot a fictitious narrow groove located at the crown of the link that allows the one dimensional unsteady open channel flow equations to be applied to surcharged pipes cunge 1980 fig 2 shows an example network adapted from ji 1998 ji 1998 with all basic elements of the model labeled flows within the channel network are modeled using the one dimensional saint venant equations this pair of nonlinear partial differential equations consists of two parts the continuity equation 1 which describes the mass balance for a finite volume and the momentum equation 2 which describes the force balance 1 a t q x q i n 2 q t x q u g a h x s 0 s f s l 0 where q is discharge a is the cross sectional area of flow u is the average velocity h is depth x is distance t is time q in is the lateral inflow per unit width and s o s f and s l represent the channel bottom slope friction slope and local head loss slope respectively using a staggered grid formulation the continuity equation is applied to each junction indexed by ik while the momentum equation applied to each link indexed by ik the equations are discretized using a backward euler type implicit scheme see sections s2 and s3 in the supplementary information for derivations the continuity equation is discretized as 3 q i k t δ t q i 1 k t δ t b i k δ x i k 2 b i 1 k δ x i 1 k 2 a s i k h i k t δ t h i k t δ t q i n i k while the momentum equation is discretized as 4 q i k t δ t q i k t δ x i k δ t u i 1 k q i 1 k t δ t u i k q i k t δ t g a i k h i 1 k t δ t h i k t δ t g a i k s o i k δ x i k g a i k s f i k s l i k δ x 0 where b is the top width of flow a s is the junction surface area and q in is the exogenous flow input the boundary conditions for each superlink are supplied by the upstream and downstream superjunction heads assuming weir like flow at the superlink inlet and outlet 5 q c a 2 g δ h where c is the inlet outlet discharge coefficient and δh is the difference in head between the superjunction and the adjacent superlink boundary junction the hydraulic model solves for all unknowns simultaneously at each time step by embedding the solutions to the saint venant equations into a system of implicit linear equations wherein all unknowns are defined in terms of the unknown superjunction heads 1 first the discretized saint venant equations are reformulated into recurrence relations that relate junction heads and link flows within each superlink 2 the assumption of orifice like flow between superjunctions and superlinks is used to establish boundary conditions for the superlink inlets and outlets 3 combining the recurrence relations together with the superlink boundary conditions the system is reformulated as a sparse matrix equation with all unknowns expressed in terms of the unknown superjunction heads 4 after solving for the unknown superjunction heads the internal depths and flows within each superlink are recovered by substituting the superjunction boundary heads into the previously developed recurrence relations this solution procedure affords a balance between stability and computational efficiency while the implicit discretization scheme helps to ensure stability the use of recurrence relations reduces the size of the solution matrix and increases computation speed compared to the more conventional four point implicit scheme ji 1998 however in addition to its computational advantages this scheme also facilitates data assimilation by casting the solver in the form of a locally linear state space system the pipedream toolkit exploits this fact by applying a kalman filter to fuse sensor measurements into the dynamical model with the kalman filter active the solution procedure is modified to assimilate sensor data at each time step 1 first the solver steps forward in time computing the hydraulic heads at all superjunctions 2 the kalman filter corrects the superjunction heads at monitored locations by fusing sensor data into the model 3 internal depths and flows are recomputed based on the corrected superjunction heads thereby propagating the correction to the rest of the system the following sections describe the theory and implementation of the hydraulic solver and kalman filter in detail and also describe improvements to the solver that were implemented in this study 3 2 enhancements to the hydraulic solver to enable representation of a broader array of real world stormwater networks we make substantial additions to the original numerical scheme these changes allow for modeling of real world stormwater networks that would be either impractical or impossible under the original formulation 3 2 1 control structures to facilitate simulation and execution of real time controls we modify the numerical scheme to enable representation of orifices weirs and pumps see sections s9 s10 and s11 the governing equations for these control structures are embedded directly into the system solution matrix facilitating the use of classical state estimation and control algorithms for linear time varying systems 3 2 2 mobile computational elements the original numerical scheme suffers from instability when modeling backwater effects we correct this problem by introducing mobile computational elements that track discontinuities in the water surface profile see section s12 3 2 3 accuracy improvements high gradient conditions were found to induce mass balance errors under the original formulation to reduce these errors we re derive the recurrence relations and superlink boundary conditions to eliminate some error generating assumptions see sections s4 and s5 3 2 4 adaptive step size control implicit solvers typically maintain better stability at large time steps when compared to explicit solvers especially during near steady state conditions to exploit this strength we implement an adaptive time stepping algorithm that allows the solver to use a small timestep during highly transient conditions and a large timestep during weakly transient conditions see section s13 söderlind 2003 3 2 5 infiltration runoff coupling to enable the use of pipedream as a standalone stormwater management model we implement an infiltration module based on the well known green ampt formulation green and ampt 1911 this module is coupled bi directionally with the hydraulic solver to enable more accurate representation of overland flow and runoff generation see section s14 3 3 constructing the state space system through derivation of the solution matrix equation we show that the hydraulic model is equivalent to a locally linear time varying state space system which in turn allows for the application of algorithms from signal processing and control theory such as feedback control modal analysis and as we show in this paper kalman filtering in the following section we derive the solution matrix equation and show how it is equivalent to a locally linear state space system detailed derivations including the discretization of governing equations and the development of the recurrence relations are omitted from this section and can instead be located in the supplementary information si document the solution matrix equation is derived by applying the continuity equation to all superjunctions and then using recurrence relations to express all unknown variables in terms of the superjunction heads focusing on a single superjunction j the continuity equation requires that the change in storage volume over a time interval δt equal the sum of inflows minus outflows 6 ℓ 1 n k d j q d k ℓ t δ t m 1 n k u j q u k m t δ t q i n j t δ t a s j h j t δ t h j t δ t where q dk is the discharge at the downstream end of superlink k q uk is the discharge at the upstream end of superlink k q in j is the external inflow into superjunction j a sj is the surface area of superjunction j and h j is the total head at superjunction j nkd j represents the number of superlinks with their downstream ends attached to superjunction j while nku j represents the number of superlinks with their upstream ends attached to superjunction j thus the first two terms represent the sum of inflows minus outflows from all superlinks attached to superjunction j using the recurrence relations developed in si section s4 the discharge at the upstream and downstream ends of each superlink can be described using the following linear functions of the upstream and downstream superjunction heads at time t δt 7 q u k t δ t α u k h j u k t δ t β u k h j d k t δ t χ u k 8 q d k t δ t α d k h j u k t δ t β d k h j d k t δ t χ d k where h juk represents the head at the superjunction upstream of superlink k and h jdk represents the head at the superjunction downstream of superlink k the α β and χ coefficients are functions of the depths and flows inside each superlink that incorporate the solutions to the continuity and momentum equations in both the forward and backward directions substituting these linearized expressions into the continuity balance for superjunction j yields the linear equation 9 f j j h j t δ t ℓ 1 n k d j φ j j u k ℓ h j u k ℓ t δ t m 1 n k u j ψ j j d k m h j d k m t δ t a s j δ t h j t q i n j t δ t g j where 10 f j j a s j δ t m 1 n k u j α u k m ℓ 1 n k d j β d k ℓ 11 φ j j u k ℓ α d k ℓ 12 ψ j j d k m β u k m 13 g j ℓ 1 n k d j χ d k ℓ m 1 n k u j χ u k m applying these equations to each superjunction j yields the following implicit state space equation 14 z t x t δ t y t x t u t δ t s t where x t is the state vector of superjunction heads at the current time step and x t δt is the state vector of superjunction heads at the next time step the elements of z t are defined such that z t j j f j j z t j j u k ℓ φ j j u k ℓ and z t j j d k m ψ j j d k m with all other elements of z t equal to zero similarly the elements of s t are defined such that s t j g j y t is a diagonal matrix representing the current superjunction storage 15 y t diag a s j δ t j 1 2 m the exogenous input u t δt represents the external inflow e g runoff to each superjunction j 16 u t δ t q i n 1 t δ t q i n 2 t δ t q i n m t δ t t to illustrate the construction of the solution matrix in concrete terms we include here the matrix equation for the example network in fig 2 with flow inputs on superjunctions 1 and 3 and head boundary conditions on superjunctions 4 and 6 as originally described in ji 1998 17 f 1 1 ψ 1 2 0 0 0 0 φ 2 1 f 2 2 ψ 2 3 0 ψ 2 5 0 0 φ 3 2 f 3 3 ψ 3 4 φ 3 5 0 0 0 0 1 0 0 0 φ 5 2 ψ 5 3 0 f 5 5 ψ 5 6 0 0 0 0 0 1 h 1 t δ t h 2 t δ t h 3 t δ t h 4 t δ t h 5 t δ t h 6 t δ t a s 1 δ t h 1 t g 1 q i n 1 t δ t a s 2 δ t h 2 t g 2 a s 3 δ t h 3 t g 3 q i n 3 t δ t h b c 4 t δ t a s 5 δ t h 4 t g 5 h b c 6 t δ t this sparse system is solved for the unknown left hand vector of superjunction heads at time step t δt once the superjunction heads are known the depths and discharges at the upstream and downstream superlink boundaries are computed finally the recurrence relations are used to solve for the internal depths and flows within each superlink 3 4 kalman filter having defined the system dynamics in terms of an implicit locally linear system a kalman filter is formulated to fuse sensor data with the dynamical model kalman filtering is a recursive bayesian estimation algorithm that i uses a dynamical system model to generate a prior estimate of system states then ii updates this prior with observed data to produce a posterior estimate kalman 1960 it can be shown that the kalman filter is the optimal linear estimator for system states when the dynamical system model is perfectly accurate and the noise is white and gaussian with known covariance kalman 1960 in the following section we present the general kalman recursion for a linear time varying system and show how it is applied to the hydraulic model developed in the previous section as a starting point note that the implicit state equation 14 may be rewritten as 18 x t δ t a t x t b t u t δ t y t where a t z t 1 y t b t z t 1 and y t z t 1 s t 2 2 note that for the case where z t is very large and or ill conditioned an implicit version of the kalman filter may be preferred to inverting z t directly skliar and ramirez 1997 we may thus express the internal states and observed outputs of the system in terms of a hidden state equation along with an observed output equation we will assume that the state equation is corrupted by some stochastic disturbance w t while the output equation is corrupted by measurement noise v t 19 x t δ t a t x t b t u t δ t y t w t 20 z t h t x t v t here x t is the n dimensional state vector at the current time step u t is an ℓ dimensional input vector w t is a p dimensional stochastic disturbance z t is the m dimensional observed state and v t is the m dimensional vector of measurement noise a t is the n n state transition matrix b t is an n ℓ input transition matrix y t is an n 1 vector of constants and h t is the m n measurement transition matrix the stochastic disturbance w t and measurement noise v t are assumed to be random vectors of zero mean gaussian white noise with known covariance under these conditions the minimum mean squared error estimator of the state x t is given by 21 x t δ t a t x t b t u t δ t y t l t δ t z t δ t z t δ t 22 z t δ t h t δ t a t x t b t u t δ t y t here we recursively correct the estimate x t by feeding back the difference between the observed measurement z t and our estimate of the observed output z t as predicted by the dynamical model the weighting factor that is applied to this difference l t is the optimal kalman gain and is given by 23 l t p t h t t h t t p t h t v t 1 where v t is the covariance of the measurement noise v t p t e x t x t x t x t t is the estimation error covariance matrix which is given recursively as 24 p t δ t a t p t p t h t t h t p t c t t v t 1 h t p t a t t w t where w t is the covariance of the stochastic disturbance w t the kalman gain l t and the error covariance p t are re computed at each time step and then used to recursively correct the estimate of system states x t the data fusion procedure proceeds as follows at each time step the hydraulic solver is advanced to determine the hydraulic heads and update the state transition matrices next the kalman recursion is applied to correct the hydraulic heads at time step t δt based on observed data finally the superlink boundary conditions and internal states are computed based on the updated superjunction heads this process is repeated indefinitely potentially in real time until the simulation is terminated 3 5 hydrology to establish pipedream as a standalone stormwater model we include a hydrologic module that computes infiltration and runoff using the green ampt method at each time step the integrated form of the green ampt equation is solved to estimate the cumulative infiltration depth for a soil element indexed by f 25 h f t δ t k s δ t h f t ψ f θ d log h f t δ t ψ f θ d log h f t ψ f θ d where h f t δ t is the cumulative infiltration depth at time t δt m k s is the saturated hydraulic conductivity m s ψ f is the suction head of the wetting front m and θ d is the soil moisture deficit unitless the infiltration rate and runoff are then computed from the cumulative infiltration depth the details of the runoff computation are presented in section s14 soil elements in the infiltration model may be coupled with any junction link or superjunction in the hydraulic model this design enables representation of processes where hydraulics and infiltration are interdependent overland flow for instance is represented by coupling infiltration elements with a wide rectangular superlink low impact development may be simulated by combining an infiltration element with a superjunction and an appropriate hydraulic control structure the coupling of hydraulics and infiltration distinguishes pipedream from existing models like swmm in which infiltration and hydraulics are treated as separate processes that are computed sequentially 3 6 software architecture fig 1 illustrates the complete architecture of the pipedream toolkit including the hydraulic solver the hydrologic module and the kalman filter the schematic indicates i the major components of the toolkit ii the ways in which these components exchange data and iii the inputs and outputs to each component here we briefly describe the software architecture with a focus on model inputs and outputs and how they interact with the hydraulic hydrologic and data assimilation components of the pipedream toolkit respectively inputs to a pipedream simulation include hydraulic and hydrologic forcings hydraulic control actions and sensor data for assimilation these inputs may be provided in either streaming or bulk format inputs to the hydrologic module include rainfall forcings which are used to compute runoff and infiltration inputs to the hydraulic model include direct inflows such as those contributed by wastewater treatment facilities boundary heads such as those imposed by tidal boundary conditions and real time controls which include weir and orifice positions as well as pumping actions as discussed earlier runoff from the hydrologic model is an input to the hydraulic model while ponding computed by the hydraulic model is an input to the hydrologic model the kalman filter accepts inputs in the form of depth or pressure sensor data at superjunctions and fuses these measurements into the hydraulic solver outputs of a simulation include hydraulic and hydrologic states at each timestep hydraulic states include hydraulic heads at superjunctions discharges in internal links and depths at internal junctions hydrologic states include infiltration and runoff rates for each soil element all model states may be programmatically accessed and modified by the user at any time this design allows for the pipedream toolkit to easily interface with external software such as visualization tools weather forecast apis and even scada software in addition to providing programmatic access to system states bulk simulation results at sampled time intervals may also be written to static files for subsequent analysis 3 7 implementation the pipedream toolkit is implemented in the python programming language which provides a powerful interpreter environment for interactive use van rossum 1995 acceleration of numerical code is realized using the numba just in time compiler which compiles native python code to machine code that achieves speeds comparable to code written in c or fortran lam et al 2015 we accelerate the solution of the system matrix equation by using a banded matrix solver upon model initialization the system matrix is automatically permuted into a banded form using the reverse cuthill mckee algorithm cuthill and mckee 1969 all code and data for this study are available at github com mdbartos pipedream 3 8 model validation we assess the pipedream toolkit by applying our state estimation methodology to a real world stormwater network and then evaluating the extent to which the kalman filter improves the accuracy of interpolated and forecasted system states first a real world catchment is selected and real time depth data is collected at four sites we then construct a pipedream model of the catchment and force the model with a real world storm event we then use a holdout cross validation approach to measure the extent to which fusing sensor data at selected sites reduces error at the holdout sites we also evaluate the ability of the kalman filter to forecast system states by fusing sensor measurements at 1 h intervals and quantifying the reduction in error over the remainder of each hour 3 3 a 1 h interval is chosen for the forecasting assessment because this sampling frequency is typical for telemetered stream gage data from a wireless sensor network these two applications interpolation and forecasting represent important use cases for real time state estimation with continuous monitoring our case study focuses on a 5 85 km2 urban watershed located in the midwestern united states fig 3 this watershed is the subject of a long term monitoring project led by the authors and thus features roughly two years of continuous sensor data bartos et al 2018 roughly 47 of the catchment is impervious with a majority of the impervious area located towards the downstream half of the catchment the representation of the channel network is derived from survey data and engineering drawings that describe the network topology hydraulic geometries storage curves and various hydraulic and hydrologic parameters needed to properly model catchment dynamics to characterize the response of the catchment wireless ultrasonic depth sensors are installed at four locations in series along the mainstem of the watershed numbered in increasing order from 1 to 4 in the downstream direction these sensors continuously report the distance to the water surface at an adaptive sampling rate that ranges from roughly 2 min to 1 h with a manufacturer specified reading to reading error of approximately 1 mm sensors at sites 1 and 2 measure the water level in two relatively large retention basins the sensor at site 3 measures the water level in an outlet box directly downstream of a third retention basin overflow from this retention basin drains into the outlet box through a rectangular weir that is approximately 5 m wide finally the sensor at site 4 measures the water level at the downstream end of a rectangular flume the rectangular flume is roughly 5 m wide and is connected at the upstream end to a fourth retention basin raw sensor data is preprocessed using a combination of manual and automatic filtering techniques invalid readings and sensor spikes are flagged and removed using an automated filtering routine consisting of range checks and threshold tests on the second derivative of the sensor signal after the initial quality control distance measurements are converted to water depth estimates using field measurements of the sensor offset to the channel bottom we apply the new hydraulic model to a real world storm event occurring on august 8 2017 this rain event is selected because i all sensor sites were active and reporting during this storm and ii the peak depth of the generated hydrograph is close to the median peak depth for the period of available data meaning that the storm event is representative of a typical storm event in this location precipitation intensity data are collected from two weather stations operated by weather underground near the catchment of interest weather underground 2019 runoff is generated from the rainfall using the green ampt formulation and then fed into the hydraulic model for the purposes of this analysis we assume uniform rainfall intensity over the catchment and use the average intensity between the two gages as input to the model the kalman filter is then applied to fuse depth sensor data into the hydraulic model we evaluate the extent to which kalman filtering improves interpolation and forecasting of depths and flows throughout the network we also perform a sensitivity analysis to gage the effect of uncertain parameters on the results namely we assess the sensitivity of the results to the assumed measurement and process covariances which reflect the relative confidence in the sensor data and model output respectively see section s16 4 results 4 1 interpolation using a holdout cross validation approach we find that the kalman filter is effective at interpolating system states at ungaged locations fig 4 shows the result of the holdout cross validation assessment for this experiment the filter is applied to sensor sites 1 and 3 and the output of the updated model is compared with sensor measurements at sites 2 and 4 from table 1 it can be seen that the filter reduces error at both holdout sites while the model performs well on its own the kalman filter reduces the mean absolute error mae at site 2 by 55 and at site 4 by 44 for site 2 a majority of the error is reduced at the peak of the hydrograph while at site 4 a majority of the error is reduced at the falling limb because the filter improves model accuracy even at locations where it is not directly applied the holdout assessment suggests that the kalman filter pushes the system closer to its actual state rather than simply overfitting individual sites to measured data the kalman filter is capable of correcting error introduced by uncertain dynamical inputs distinguishing it from a calibration only approach from fig 4 it can be seen that the model without filtering over predicts the discharge at sites 1 and 2 but under predicts at sites 3 and 4 this result suggests that the spatial heterogeneity of runoff is a major source of error this type of error is difficult to counter with model calibration alone given that calibration tends to target static parameters of the system such as channel roughness coefficients and impervious area percentages however spatial variability in runoff may also result from spatial variability in the originating rainfall field in contrast to continuous calibration the kalman filter handles this contingency by correcting system states in real time adding and removing mass from the system to match field observations this approach is robust to both parameter and input uncertainty making it especially suitable for real time applications in which the driving input is often uncertain or unknown 4 2 forecasting by adaptively correcting system dynamics kalman filtering pushes the hydraulic system closer to its measured state and thus enables improved forecasting of system behavior however not all sites offer the same forecasting benefit and sensor sites must be selected judiciously to maximize the accuracy of the forecast fig 5 shows the result of using the kalman filter to forecast system states at 1 h intervals in this application sensor data for each site is fused at the first minute of each hour indicated by circular markers and the model is then propagated forward in time to forecast system behavior for the remainder of the hour the effectiveness of the forecast is then gaged based on the reduction in mae and the length of time that the correction persists in general the most effective sites for forecasting are those for which the volume of storage is large compared to the volume of water entering and exiting the control volume when the filter is applied to correct the hydraulic head at a large retention pond site 1 as shown in fig 5 left the effect of the correction persists for the remainder of the hourly interval in other words applying the correction significantly changes the trajectory of the hydraulic head compared to the trajectory produced by the model alone the correction imposed by the filter increases the accuracy of the forecasted states measured by mae compared to the model only forecast sites with larger storage capacity are most effective because small changes in hydraulic head result in relatively large changes to the mass and energy balance of the system essentially allowing greater control over the state space by contrast sites with smaller storage capacity offer less forecasting benefit given that the effect of the correction is quickly overpowered by dynamics originating from elsewhere in the system at the outlet flume site 4 for example the proportion of mass entering and leaving the control volume at each time step is relatively large compared to the mass within the control volume itself thus correcting the system state at this location does not significantly alter the amount of mass or energy in the system and the effect of the correction persists for only a short time after the filter is applied as shown in fig 5 right the trajectory of the hydraulic head quickly returns to the model only trajectory after each application of the filter thus sensor data from sites with little storage capacity must be fused at a rapid frequency on the order of the time step used by the hydraulic solver in order to produce noticeable forecasting benefit 4 3 computational performance the pipedream model offers significantly improved model run times compared to pyswmm an existing interactive solver for stormwater networks model run time comparisons are conducted on networks α β and ε from the pystorms benchmarking library with network sizes ranging from 26 to 210 nodes superjunctions rimer et al 2019 when compared against pyswmm pipedream ran between 15 and 190 times faster with performance gains increasing for larger numbers of computational elements for many networks this multiple order of magnitude run time improvement could mean the difference between offline usage and true real time execution when compared with swmm5 in batch execution pipedream is currently about 2 1 3 4 times slower for the benchmark scenarios chosen however pipedream was able to achieve more consistently stable results in particular for the β network pipedream was able to achieve stable results while swmm was not even when using a small timestep 0 1 s table 2 shows a detailed breakdown of model performance for all scenarios figs s4 s6 compare model outputs for pipedream and swmm under each scenario additional information about each test network and storm scenario may be found in section s15 5 discussion by enabling real time interpolation and forecasting of hydraulic states pipedream provides a powerful new tool for urban flash flood nowcasting the dynamics of urban flash flooding are complex and spatially heterogeneous to the effect that there is no existing model that is capable of reliably forecasting flash floods in urban catchments hapuarachchi et al 2011 while many cities use gage networks to help detect flooding and communicate flood alerts gages are generally restricted to larger streams leaving significant blind spots in the drainage network habibi et al 2019 to address this problem the data assimilation methodology presented in this paper will allow emergency managers to better estimate localized flooding at ungaged locations by interpolating hydraulic states from locations where sensor data is available these high resolution flood estimates will enable new forms of rapid flood response such as localized alerts for motorists and targeted dispatch of emergency services moreover by correcting system states in real time the pipedream toolkit will assist with flood forecasting enabling emergency managers to more accurately predict downstream flooding at longer lead times especially in cases where flooding is primarily driven by upstream transport in addition to detecting localized floods the pipedream toolkit will also assist in identifying maintenance emergencies timely and accurate diagnosis of maintenance issues is essential for effective stormwater management short term maintenance emergencies such as storm drain blockages can lead to localized flooding while long term maintenance issues such as sediment accumulation can degrade the overall performance of the stormwater system wright and marchese 2017 however distinguishing true maintenance emergencies from spurious sensor faults remains a persistent challenge especially when attempting to diagnose anomalies from sensor data alone ni et al 2009 pipedream handles this ambiguity by combining sensor data with a dynamical model of the stormwater system by offering two independent estimates of system states pipedream makes it easier to track the source of anomalies and differentiate true emergencies from false positives moreover pipedream natively encodes the relative confidence in sensor model outputs through the measurement process covariances respectively allowing users to incorporate prior information about sensor faults directly into the data assimilation procedure perhaps most importantly the pipedream toolkit provides a foundation for real time control of urban drainage systems many cities are now implementing or seeking to implement real time control systems for urban drainage systems in order to improve performance cut costs or mitigate stormwater related hazards in both modeling studies and real world deployments real time control has shown proven results in mitigating combined sewer overflows and improving urban water quality montestruque and lemmon 2015 gelormino and ricker 1994 van nooijen and kolechkina 2013 puig et al 2009 andersen et al 1997 gaborit et al 2013 middleton and barrett 2008 jacopin et al 2001 carpenter et al 2013 however effective real time control is predicated on an accurate representation of system states by fusing sensor data with an accurate physically based process model the pipedream toolkit provides a firm basis for control whether for the purposes of simulation or real world execution crucially pipedream integrates control structures into the internal state space model facilitating native use of classical control algorithms such as model predictive control and linear quadratic regulation 5 1 future work the pipedream toolkit provides practitioners and researchers with a powerful new tool for building digital twins of stormwater systems however important questions remain about how this tool should best be applied to real world use cases we recommend the following future work focused on characterizing and improving the scalability and reliability of the model on the issue of scalability future work should examine the performance of pipedream for very large systems e g tens of thousands of elements in this study we evaluate model performance for networks of up to 210 superjunctions while pipedream scales favorably in comparison with swmm and pyswmm for networks of this size larger networks may present problems for implicit solvers that must solve a large system of equations simultaneously for these networks specialized algorithms for solving sparse matrix equations may be required saad 2003 in the context of data assimilation extensions to the conventional kalman filter may be required for very large systems for systems with many states and small process noise covariance for instance the error covariance matrix may become ill conditioned under these conditions the square root form of the kalman filter is often preferred verhaegen and dooren 1986 for even larger systems the error covariance matrix may become impractical to compute and an ensemble kalman filter enkf may be required evensen 1994 future work should investigate these extensions to the kalman filter algorithm and how they might be integrated into the existing modeling framework on the subject of model accuracy and reliability future work should examine how alternative numerical schemes for the saint venant equations perform in the context of real time data assimilation for this study we employed a first order implicit scheme with an upwind formulation for the advection term this scheme is highly stable and is thus well suited for running continuous simulations with data assimilation however higher order schemes may offer greater accuracy and result in reduced mass balance error future work should investigate these tradeoffs in the context of real time simulations future work should also examine the boundary conditions for the hydraulic solver in greater detail the assumption of weir like flow at the superlink boundaries helps to accelerate the solution of the system matrix however this formulation ignores the inertial component of the momentum balance at the interface between the superlinks and superjunctions inertial forces may be significant at the downstream end of conduits under supercritical flow conditions future work should investigate alternative formulations of the superlink boundary conditions that mitigate this source of error 6 conclusions in this study we develop a new toolkit for building real time digital twins of urban and natural drainage systems this toolkit consists of a robust hydraulic solver based on the full one dimensional saint venant equations along with an implicit kalman filtering methodology that facilitates assimilation of real time sensor data drawing on sensor data from a real world stormwater network we find that the implicit kalman filter is effective at both interpolating system states within the network and forecasting future states based on current measurements by providing a physically based methodology for state estimation in stormwater networks this toolkit will enable system operators to pre emptively detect and repair blockages leaks and other maintenance emergencies moreover by improving interpolation and forecasting of system states our toolkit will provide a strong foundation for model based real time control schemes such as model predictive control and linear quadratic regulation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments support for this project was provided by the j robert beyster computational innovation graduate fellowship the national science foundation earthcube initiative grant 1639640 and the national science foundation smart and connected communities program grant 1737432 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105120 data availability code and data links are available at https github com mdbartos pipedream permanent doi 10 5281 zenodo 3950373 
