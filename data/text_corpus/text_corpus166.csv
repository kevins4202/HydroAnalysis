index,text
830,we construct and study efficient high order discontinuous galerkin methods for the shallow water flows in open channels with irregular geometry and a non flat bottom topography in this paper the proposed methods are well balanced for the still water steady state solution and can preserve the non negativity of wet cross section numerically the well balanced property is obtained via a novel source term separation and discretization a simple positivity preserving limiter is employed to provide efficient and robust simulations near the wetting and drying fronts numerical examples are performed to verify the well balanced property the non negativity of the wet cross section and good performance for both continuous and discontinuous solutions keywords shallow water flows cross section discontinuous galerkin well balanced positivity preserving method 1 introduction the shallow water equations are commonly used to model and simulate flows in rivers and coastal areas in this paper we consider one dimensional shallow water flows in open channels with irregular geometry and a non flat bottom topography hernández duenas and karni 2011 vázquez céndon 1999 taking the form of 1 h t q x 0 q t q 2 h 1 2 g σ h 2 x 1 2 g h 2 σ x g σ h b x where σ x represents the width of the channel b denotes the bottom topography h is the water height h σ h is the wet cross section q h u is the mass flow rate u is the velocity and g is the gravitational constant this model is characterized by the non dimensional froude number u c where c g h σ and reduces to the nonlinear shallow water equations when the cross section σ x is a constant the shallow water flows in channels 1 belong to the class of hyperbolic equations with source terms also referred as hyperbolic balance laws and admit the still water steady state solution given by 2 u 0 h b c o n s t in which the source term is exactly balanced by the flux gradient one main challenge in the numerical simulation of such balance laws including the nonlinear shallow water equations with a non flat bottom topography and euler equations under gravitation fields is that a standard numerical method may not satisfy the balance of flux gradient and source term at the steady state in the discrete level and may introduce spurious oscillations near the steady state well balanced methods are designed to overcome this challenge and performs well at or near the steady state with coarse meshes many well balanced methods have been designed for the nonlinear shallow water equations see audusse et al 2004 bermudez and vazquez 1994 greenberg and leroux 1996 leveque 1998 perthame and simeoni 2001 xu 2002 xing 2014 xing and shu 2005 xing and shu 2013 and the references therein another commonly encountered challenge in the simulations of the shallow water related models is the wetting and drying treatment for the region where there is little or no water typical applications include the dam break problem flood waves and run up phenomena numerically negative water may be produced during the computation which may pose additional difficulty there are many existing positivity preserving techniques to overcome this difficulty and we refer to bokhove 2005 bunya et al 2009 ern et al 2008 xing and zhang 2013 xing et al 2010 kurganov and levy 2002 and the references therein for some recent works for the shallow water flows in open channels 1 an early work on well balanced methods was carried out in 1999 by vázquez céndon 1999 where the model was first reformulated into the nonlinear shallow water equations with additional source terms representing the channel width the method proposed in bermudez and vazquez 1994 can then be applied to deliver well balanced methods for the shallow water flows in channels garcía navarro and vázquez céndon 2000 designed well balanced method using proper flux difference splitting recently balbas and karni 2009 designed second order well balanced positivity preserving numerical methods in rectangular channels using the central schemes the extension to shallow water flows with arbitrary cross section was studied in hernández duenas and karni 2011 and balbás and hernández duenas 2014 well balanced method based on energy balanced property is studied in murillo and garcía navarro 2014 hernández duenas and beljadid 2016 developed a new non oscillatory semi discrete central upwind scheme coupled with artificial viscosity xing 2016 designed high order well balanced finite volume weighted essentially non oscillatory schemes for shallow water flows in open channels with irregular geometry and a non flat bottom topography all of the works mentioned above for the shallow water flows in open channels are based on finite difference or finite volume schemes during the past few decades high order finite element discontinuous galerkin dg methods have gained great attention in solving partial differential equations including the hyperbolic conservation laws dg methods using discontinuous piecewise polynomial space as the solution and test function spaces see cockburn et al 2000 for a historic review combine advantages of both finite element and finite volume methods and can achieve high order of accuracy easily with the use of high order polynomials within each element several advantages of the dg methods including their accuracy high parallel efficiency flexibility for hp adaptivity and arbitrary geometry and meshes make them attractive for a wide range of applications including the shallow water simulations the main objective of this paper is to develop efficient high order dg methods for the shallow water flows in open channels with non flat bottom topography the proposed methods have two attractive features well balanced for the still water steady state solutions and positivity preserving near the wetting and drying front this will be the first paper on high order dg methods for the shallow water flows in open channels to achieve these properties to our best knowledge to achieve well balanced property we start by rewriting the source terms in an equivalent special form using the still water steady state solution 2 then we apply integration by parts on these source terms to derive a numerical approximation which is exactly well balanced with the flux gradient at the steady state and also high order accurate for general solutions the approach to achieve high order well balanced property is very different from the other high order method for the shallow water flows in channels presented in xing 2016 where an extrapolation on the source term approximation was introduced to increase the order from second order to fourth order here we could achieve any order of accuracy in the dg framework a simple positivity preserving limiter adopted from zhang and shu 2010 and later applied to dg methods for the shallow water equations in xing et al 2010 was used to ensure the resulting methods maintain the non negativity of the cross sectional wet area this paper is organized as follows in section 2 we first present the novel high order well balanced dg methods the positivity preserving technique is presented in section 3 in section 4 some numerical examples are presented to verify the well balanced property the non negativity of the wet cross section high order accuracy in smooth regions for general solutions and essentially non oscillatory for general solutions with discontinuities finally some conclusions are given in section 5 2 well balanced methods in this section we present high order well balanced dg methods for the shallow water flows 1 in open channels which can preserve the still water steady state 2 exactly 2 1 notations we divide the interval i a b into n subintervals and denote the cells by i j x j 1 2 x j 1 2 for j 1 n the center of each cell is x j 1 2 x j 1 2 x j 1 2 and the mesh size is denoted by τ j x j 1 2 x j 1 2 with τ max 1 j n τ j being the maximal mesh size the piecewise polynomial space v τ k is defined as the space of polynomials of degree up to k in each cell ij that is 3 v τ k v v i j p k i j j 1 2 n note that the functions in v τ k are allowed to have discontinuities across element interfaces for any unknown function v its numerical approximation in the dg methods is denoted by vτ which belongs to the finite element space v τ k we denote by v τ j 1 2 and v τ j 1 2 the limit values of vτ at x j 1 2 from the right cell i j 1 and from the left cell ij respectively in addition we use the usual notation v τ j 1 2 v τ j 1 2 v τ j 1 2 2 to represent the arithmetic mean of the function vτ at the element interface x j 1 2 2 2 reformulation of the system and standard dg methods the first step in designing well balanced methods is to rewrite the source terms in an equivalent form using the information of the steady state solution 2 here we reformulate the original governing equations as 4 h t q x 0 q t q 2 h 1 2 g σ h 2 x 1 2 g h b 2 σ x g h b σ b x 1 2 g σ b 2 x where we replace the source term 1 2 g h 2 σ x g σ h b x by the equivalent form of 1 2 g h b 2 σ x g h b σ b x 1 2 g σ b 2 x in the momentum equation note that when σ is constant this reformulation reduces to the one in xing and shu 2006 for the shallow water equations therefore this can be viewed as a generalization of the technique presented in xing and shu 2006 for the shallow water flows in channels for the sake of easy presentation we introduce the notation 5 u h q to denote the conservative variable and rewrite the shallow water flows in channels 4 as 6 u t f u σ x s u σ σ b σ b 2 where f u σ and s u σ σb σb 2 denote the flux and source term respectively the initial condition of u x 0 is projected into the piecewise polynomial space v τ k to obtain uτ x 0 via the standard l 2 projection similarly we project σ x and σb x to obtain the piecewise polynomial functions στ and σb τ the standard dg method for the class of hyperbolic equations 6 has the form 7 i j t u τ n v d x i j f u τ n σ τ x v d x f j 1 2 v j 1 2 f j 1 2 v j 1 2 i j s u τ n σ τ σ b τ σ b τ 2 σ τ v d x with the numerical fluxes given by 8 f j 1 2 f u τ j 1 2 n u τ j 1 2 n σ τ j 1 2 σ τ j 1 2 where f is a numerical flux herein we adopt the simple lax friedrichs flux 9 f u τ j 1 2 n u τ j 1 2 n σ τ j 1 2 σ τ j 1 2 1 2 f u τ j 1 2 n σ τ j 1 2 f u τ j 1 2 n σ τ j 1 2 α u τ j 1 2 n u τ j 1 2 n where α max u λ u with λ u being the eigenvalues of the jacobian f u and the maximum is taken over the whole region for this system we have α max x i u τ g h τ high order total variation diminishing tvd runge kutta methods shu 1988 are often used as the temporal discretization of the method 7 in the numerical examples of this paper the third order tvd runge kutta method 10 u 1 u n δ t f u n u 2 3 4 u n 1 4 u 1 δ t f u 1 u n 1 1 3 u n 2 3 u 2 δ t f u 2 with f u being the spatial operator is used 2 3 well balanced numerical fluxes note that the standard dg methods 7 presented in the preceding subsection do not have the well balanced property in this subsection we present the modification to the numerical fluxes with the purpose of preserving the steady state 2 exactly suppose the initial condition is given as the steady state 2 i e h σ b h b c and q h u 0 with c being a constant we want to recover this steady state information using the computational variables uτ piecewise polynomials at each time level the projection of the initial condition leads to the fact that h τ σ b τ σ τ c at each time step we introduce the functions hτ and bτ based on the numerical solutions as 11 h τ h τ σ τ b τ σ b τ σ τ and their sum h b τ defined by 12 h b τ h τ σ b τ σ τ note that inside each cell ij h b τ is a rational polynomial function and may not be a polynomial but at the steady state solution 2 we can easily obtain that h b τ c at the cell boundary x j 1 2 we can evaluate u τ j 1 2 using the polynomials from the left and right boundaries similarly we can obtain σ τ j 1 2 and σ b τ j 1 2 the cell average and cell boundary values of h b τ are defined as 13 h b τ j h τ σ b τ j σ τ j h b τ j 1 2 h τ j 1 2 σ b τ j 1 2 σ τ j 1 2 which also satisfy h b τ j h b τ j 1 2 c at the still water state 2 for the purpose of positivity preserving as explained in section 3 we also introduce the updated cell boundary values at the time step tn as in audusse et al 2004 14 h τ j 1 2 max 0 h τ j 1 2 n b τ j 1 2 max b τ j 1 2 b τ j 1 2 which satisfies h τ j 1 2 h τ j 1 2 at the steady state now let us discuss the well balanced numerical fluxes in the lax friedrichs numerical flux f j 1 2 defined in 9 the additional term α u τ j 1 2 n u τ j 1 2 n contributes to the numerical viscosity which is essential for this nonlinear conservation laws however they may destroy the well balanced property at the steady state therefore we propose to modify this flux 9 as 15 f u τ j 1 2 n u τ j 1 2 n σ τ j 1 2 σ τ j 1 2 1 2 σ h u q 2 h 1 2 g σ h 2 τ j 1 2 n σ h u q 2 h 1 2 g σ h 2 τ j 1 2 n α σ h q τ j 1 2 n σ h q τ j 1 2 n where 16 σ τ j 1 2 min σ τ j 1 2 σ τ j 1 2 here we modify the first component of the flux term from q τ to σ τ h τ u τ for the purpose of positivity preserving to be explained in section 3 2 4 source term approximation next we present the high order well balanced approximation to the source term integration in xing 2016 we designed high order well balanced finite volume methods for the shallow water flows in channels and the well balanced approximation to the source term integration is obtained via an extrapolation technique that approach can only provide even order approximation to the source term and it is not easy to extend it to finite element methods here a very different approach to approximate the source term is presented for our dg methods the source term of the momentum equation in 4 takes the form of 17 i j s 2 v d x 1 2 g i j h b 2 σ x v d x g i j h b σ b x v d x 1 2 g i j σ b 2 x v d x where s 2 denote the second component of the source term we can further decompose this integral as 18 i j s 2 v d x 1 2 g i j h b 2 h b j 2 σ x v d x g i j h b h b j σ b x v d x 1 2 g h b j 2 i j σ x v d x g h b j i j σ b x v d x 1 2 g i j σ b 2 x v d x 1 2 g i j h b 2 h b j 2 σ x v d x g i j h b h b j σ b x v d x 1 2 g h b j 2 σ v x j 1 2 σ v x j 1 2 i j σ v x d x g h b j σ b v x j 1 2 σ b v x j 1 2 i j σ b v x d x 1 2 g σ b 2 v x j 1 2 σ b 2 v x j 1 2 i j σ b 2 v x d x by adding and subtracting the constant terms h b j 2 h b j inside the integral and applying integration by parts note that the constant term h b j can be replaced by any other term that can recover constant c at the still water steady state 2 for example h b x j our numerical approximation to the source term 18 takes the following form 19 i j s τ 2 v d x 1 2 g i j h b τ 2 h b τ j 2 σ τ x v d x g i j h b τ h b τ j σ b τ x v d x 1 2 g h b τ j 2 σ τ j 1 2 v j 1 2 σ τ j 1 2 v j 1 2 i j σ τ v x d x g h b τ j σ b τ j 1 2 v j 1 2 σ b τ j 1 2 v j 1 2 i j σ b τ v x d x 1 2 g σ b τ 2 σ τ j 1 2 v j 1 2 σ b τ 2 σ τ j 1 2 v j 1 2 i j σ b τ 2 σ τ v x d x where the terms σ σb σb 2 h b h b j are replaced by their numerical approximations στ σb τ σ b τ 2 σ τ h b τ h b τ j respectively moreover the boundary values of στ σb τ σ b τ 2 σ τ are replaced by their averages at the cell interface denoted by στ σb τ σ b τ 2 σ τ to be consistent with the numerical flux 15 at the steady state 2 5 slope limiter for the system of hyperbolic conservation laws when the solution contains discontinuities dg methods are often coupled with a slope limiter procedure to compress the possible oscillation near the discontinuities many different choices of slope limiters are available in the literature and we consider the classical total variation bounded tvb limiter shu 1987 in this paper the standard tvb limiter on the unknown uτ involves two steps the detection of the troubled cell and the reconstruction of the polynomial solution in these troubled cells the first step replies on the troubled cell indicators which analyze the smoothness of the solution in the cell ij based on the cell averages u τ j u τ j 1 and cell boundary values u τ j 1 2 u τ j 1 2 using these quantities we can construct the forward and backward differences 20 δ u τ j u τ j 1 u τ j δ u τ j u τ j u τ j 1 u τ j u τ j 1 2 u τ j u τ j u τ j u τ j 1 2 the tvb limiter is applied to these differences to obtain u τ j m o d m u τ j δ u τ j δ u τ j u τ j m o d m u τ j δ u τ j δ u τ j where m is the minmod type tvb limiter m b 1 b 2 b n b 1 if b 1 m δ x 2 sign b 1 min 1 i n b i if b 1 m δ x 2 and sign b 1 sign b n 0 otherwise with m being the tvb parameter to be chosen adequately for each test and proportional to the second derivatives of the solution near smooth critical points the cell interface values are then updated as 21 u τ j 1 2 m o d u τ j u τ j m o d u τ j 1 2 m o d u τ j u τ j m o d and the cell ij is marked as a troubled cell if the update in 21 changes either of the two cell interface values i e if u τ j 1 2 m o d u τ j 1 2 or u τ j 1 2 m o d u τ j 1 2 if a cell is marked as a troubled cell the reconstruction step is to replace the polynomial in this cell by a limited linear or quadratic polynomial for example one can choose the limited function u τ j m o d as the unique quadratic polynomial satisfying 21 and also maintaining the original cell average u τ j for the system of conservation laws we often apply the tvb limiter with local characteristic decomposition to achieve better numerical results i e the variables are projected into the characteristic direction before evaluating the differences in 20 after we compute the updated cell interface values 21 we project them back to the physical spaces to detect the troubled cells the standard limiter may conflict with the well balanced property and we propose a well balanced limiter procedure following the idea presented in xing et al 2010 by modifying the troubled cell detection step in the proposed well balanced slope limiter procedure we will first check if the limiting is needed based on the cell averages h b τ j h b τ j 1 and h b τ j 1 2 h b τ j 1 2 if the cell ij is flagged as needing limiting the actual tvb limiter is applied on uτ as the above procedure note that h b τ becomes constant at the still water steady state 2 therefore no limiter is applied when the steady state is reached and the well balanced property is maintained 2 6 summary of the well balanced schemes our proposed well balanced dg methods for the shallow water flows 1 in channels are given by 7 where the numerical fluxes are defined in 8 and 15 and the source term approximation is provided in 19 the method is completed by a temporal tvd runge kutta discretization 10 with the well balanced slope limiter procedure applied in each inner stage of the runge kutta methods collecting the results of the previous subsections it is straightforward to prove the following result proposition 1 the dg methods for the shallow water flows in channels 1 as described above can maintain the well balanced property for the steady state solutions 2 proof at the steady state 2 we have h b τ c q τ 0 for the mass equation h t q x 0 numerical approximation of the source term is 0 and the approximation of the flux term is f j 1 2 1 v j 1 2 f j 1 2 1 v j 1 2 i j f 1 v x d x where f 1 q τ stands for the flux term in the mass equation since q τ 0 at the steady state the volume integral i j f 1 v x d x 0 note that we have h τ j 1 2 h τ j 1 2 from 14 therefore the numerical flux f 1 as defined in 15 becomes zero too and there is no numerical dissipation next we prove the well balanced property for the momentum equation which contains the source term denote the flux term in this equation by f 2 with the numerical flux given in 15 its numerical approximation on the left side of 7 takes the form of 22 f j 1 2 2 v j 1 2 f j 1 2 2 v j 1 2 i j f 2 v x d x 1 2 g σ τ h τ 2 j 1 2 v x j 1 2 1 2 g σ τ h τ 2 j 1 2 v x j 1 2 i j 1 2 g σ τ h τ 2 v x d x since h b τ h b τ j c the source term approximation 19 becomes i j s τ 2 v d x 1 2 g c 2 σ τ j 1 2 v j 1 2 σ τ j 1 2 v j 1 2 i j σ τ v x d x g c σ b τ j 1 2 v j 1 2 σ b τ j 1 2 v j 1 2 i j σ b τ v x d x 1 2 g σ τ b τ 2 j 1 2 v j 1 2 σ τ b τ 2 j 1 2 v j 1 2 i j σ τ b τ 2 v x d x notice the equality that 23 1 2 g σ τ h τ 2 1 2 g σ τ h b τ 2 g σ τ h b τ b τ 1 2 g σ τ b τ 2 1 2 g c 2 σ τ g c σ τ b τ 1 2 g σ τ b τ 2 therefore we can conclude that the flux and source term approximations balance each other at the still water steady state which leads to the desired well balanced property 3 positivity preserving methods zhang and shu 2010 proposed a framework to design high order maximum principle preserving methods for hyperbolic conservation laws since then the method has gained many attentions and has been applied to various applications including the shallow water equations in xing 2016 xing and zhang 2013 and xing et al 2010 it was shown to be able to maintain the non negativity of water height under suitable cfl condition without affecting the mass conservation and keep the high order accuracy for the general solutions here we will explore the application of this approach to the well balanced dg methods presented in section 2 for the shallow water flows in open channels as explained in zhang and shu 2010 the key components to achieve this goal are the following two items the positivity of the first order version of this method and a simple positivity preserving limiter to be coupled with the high order method following the setup we only consider the simple euler time discretization and the same results can be generalized to multi step and tvd high order runge kutta methods without loss of generality we ignore the subscript τ in this section consider the well balanced dg methods 7 with the numerical fluxes defined in 15 take the test function v 1 leads to the following update of the cell averages of the wet cross section 24 h j n 1 h j n λ f j 1 2 1 f j 1 2 1 where 25 f j 1 2 1 1 2 σ j 1 2 h j 1 2 u j 1 2 n σ j 1 2 h j 1 2 u j 1 2 n α σ j 1 2 h j 1 2 σ j 1 2 h j 1 2 and λ δ t δ x the first order version of this method takes the form of 26 h j n 1 h j n λ f j 1 2 1 f j 1 2 1 with the numerical flux defined as 27 f j 1 2 1 1 2 σ j 1 2 h j 1 2 u j n σ j 1 2 h j 1 2 u j 1 n α σ j 1 2 h j 1 2 σ j 1 2 h j 1 2 and the first order version of the starred terms h j 1 2 σ j 1 2 given by 28 h j 1 2 max 0 h j n b j max b j b j 1 h j 1 2 max 0 h j 1 n b j 1 max b j b j 1 σ j 1 2 min σ j σ j 1 we have the following lemma about the positivity preserving property of this first order method lemma 2 under the cfl condition λα 1 with α max u g h consider the first order scheme 26 with the numerical flux 27 and 28 if h j n h j 1 n are non negative then h j n 1 is also non negative proof the scheme 26 can be written as h j n 1 1 1 2 λ α u j n σ j 1 2 σ j h j 1 2 h j n 1 2 λ α u j n σ j 1 2 σ j h j 1 2 h j n σ j h j n 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 1 2 λ α u j n 1 2 λ α u j n σ j h j n 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 λ α σ j h j n 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 2 λ α u j 1 n σ j 1 2 h j 1 2 since 0 h j 1 2 h j 1 2 h j n and σ j 1 2 σ j this also justifies the reason that we pick the minimum value in the definition of σ in 16 therefore h j n 1 is a linear combination of h j 1 n h j 1 2 and h j 1 2 and all the coefficients are non negative which leads to h j n 1 0 next we move to discuss high order schemes we refer to zhang and shu 2010 and xing et al 2010 for the details and only present the main idea here we introduce the n point with 2 n 3 k legendre gauss lobatto quadrature rule on the interval ij and denote these quadrature points by s j x j 1 2 x j 1 x j 2 x j n 1 x j n x j 1 2 with the corresponding quadrature weights w r for the interval 1 2 1 2 satisfying r 1 n w r 1 we employ the following positivity preserving limiter xing et al 2010 zhang and shu 2010 on the dg polynomial u j n x h j n x q j n x t 29 u j n x θ u j n x u j n u j n θ min 1 h j n h j n m j with 30 m j min x s j h j n x min r 1 n h j n x j r with this choice of mj we can show that h j n x j r 0 r 1 n and this limiter maintains the local conservation of the variable u j n x we compute the modified polynomial u j n x and use u j n x instead of u j n x in the well balanced methods 7 following the proofs in xing et al 2010 and zhang and shu 2010 we can verify that the well balanced methods coupled with this positivity preserving limiter are high order accurate positivity preserving and mass conservation under the cfl condition 31 λ α w 1 to be efficient we could implement the time step restriction 31 only when a preliminary calculation to the next time step produces negative wet cross section 4 numerical examples in this section we carry out extensive numerical experiments to demonstrate the performance of the proposed positivity preserving well balanced dg methods for the shallow water flows in open channels the third order finite element dg methods i e k 2 coupled with third order tvd runge kutta methods 10 are implemented in these examples the cfl number is taken as 0 16 which satisfies the requirement 31 to achieve positivity preserving property the gravitation constant g is fixed as 9 812 m s2 channels with both continuous and discontinuous width functions have been tested 4 1 accuracy test we first test the third order accuracy of the resulting method on an example with smooth solutions the following periodic bottom topography and channel width function b x sin 2 π x σ x e sin 2 π x are considered in this example the initial conditions are given by h x 0 3 e cos 2 π x q x 0 sin cos 2 π x x 0 1 with periodic boundary conditions we run the test up to the stopping time t 0 1 when the solutions are still smooth as the exact solutions are not available for this nonlinear system we apply the same method with a much refined n 25 600 cells to obtain a reference solution and then treat it as the exact solution when computing the errors and convergence rates which are shown in table 1 we can clearly observe that the expected third order accuracy is achieved 4 2 well balanced test the purpose of the second test problem balbas and karni 2009 xing 2016 is to verify the well balanced property of our proposed dg methods we consider the bottom topography given by 32 b x 0 25 1 cos 10 π x 0 5 if 0 4 x 0 6 0 otherwise in the domain 0 1 the channel with varying width σ x takes the form of 33 σ x 1 σ 0 1 cos 2 π x x l x r 2 x r x l if x x l x r 1 otherwise where xl and xr are the left and right boundary of the contraction and 1 2 σ 0 represents the minimum width of the channel at the point x l x r 2 in this example we choose x l 0 25 x r 0 75 and σ 0 0 2 the initial condition is provided as the steady state solution h b 1 q σ h u 0 and the periodic boundary condition is considered we solve the problem with 200 uniform cells until the final time t 1 the numerical surface level h b and the bottom b are plotted in fig 1 the 3d plot of the bottom topography b and the channel shape σ is provided in fig 2 in order to demonstrate that the steady state solution is maintained up to round off error the l 1 l 2 and l errors of the wet cross section h and the mass flow rate q with single precision and double precision are shown in table 2 we can clearly see that the l 1 l 2 and l errors are all at the level of round off errors for these precisions which verify the expected well balanced property 4 3 small perturbation test in this subsection we simulate the propagation of small perturbations to a steady state solution to demonstrate the capability of the proposed dg methods for such challenging case this test was first proposed by balbas and karni 2009 we set the bottom topography as 32 and the initial condition as h b 1 0 01 if 0 1 x 0 2 1 otherwise q σ h u 0 in a computational domain 0 1 with simple transmissive boundary conditions two different sets of channel σ x defined in 33 are tested one with a left shifted contraction x l 0 15 x r 0 65 σ 0 0 2 and the other with a right shifted contraction x l 0 35 x r 0 85 σ 0 0 2 for these tests involving such small perturbation of steady state solutions non well balanced numerical methods usually have difficulty with the calculations and produce oscillatory results kurganov and levy 2002 the numerical results of the proposed well balanced dg methods at different times on 200 uniform computational cells compared with refined 2000 cells reference solutions are shown in fig 3 for the sake of comparison we also present the numerical results by the non well balanced dg methods with 200 cells we can clearly observe that the well balanced results are free of spurious numerical oscillations and our methods can numerically capture such small perturbation well on relatively coarse meshes 4 4 a converging diverging channel here we consider the classic transcritical steady flow in a converging diverging channel originally proposed by garcía navarro et al 1992 this test is related to many practical problems such as the flow between bridge piers the bottom is set as flat i e b 0 and the converging diverging channel is given by σ x 5 0 7065 1 cos 2 π x 250 300 if x 150 450 5 otherwise in the computational domain 0 500 the initial conditions are set as h 2 q σ h u 20 and the boundary conditions are given by q 20 at the upstream and h 1 85 at the downstream we compute this test up to a long time t 5000 until it reaches a steady state using 200 uniform cells and present the numerical result of the water height h in fig 4 which agrees well with those in the literature garcía navarro et al 1992 vázquez céndon 1999 the flow changes from subcritical flow to supercritical flow at the critical point x 250 and later becomes subcritical flow via a stationary hydraulic jump to connect to the subcritical downstream boundary condition 4 5 drain on a non flat bottom in this section we consider a drainage test first proposed by gallouët et al 2003 and later appeared in xing and shu 2011 for the shallow water equations the goal is to test both well balanced property and positivity preserving feature of our methods the bottom topography is given by b x 0 2 0 05 x 10 2 if 8 x 12 0 otherwise on the computational domain 0 25 and the channel width is set as σ x 1 0 2 1 cos 2 π x 10 12 5 if 3 75 x 10 1 otherwise which is discontinuous at the point x 10 and is shown in fig 5 we consider the initial conditions as follows h x 0 0 5 b x h x 0 σ x h x 0 q x 0 0 a free boundary condition on h and zero on q is imposed on the left boundary and an outlet condition on a dry bed refer to gallouët et al 2003 for the details is used on the right boundary a uniform mesh with 250 cells is taken we present the water surface level h b and the discharge q at t 10 20 100 and 500 in fig 6 respectively we can observe that the numerical solution reaches the steady state after a long time the converged steady state is a still water requiring well balanced feature of the numerical methods on the left of the bump and a dry state requiring positivity preserving feature of the numerical methods on the right of the bump the numerical results show that our proposed dg methods work well for this challenging test even with a discontinuous channel 4 6 moving water steady states over a hump in this last numerical example xing 2016 we consider the convergence of our methods towards steady transcritical and subcritical flows with different channel configurations in xing and shu 2006 the same tests are used to check the performance of well balanced dg methods for the shallow water equations i e constant channel width the computational domain is set as 0 25 and the bottom topography is defined by b x 0 2 0 05 x 10 2 if 8 x 12 0 otherwise we choose different sets of variable channel width to be defined in each case to demonstrate the effect of channel on the final solutions the initial conditions are given by h x 0 0 5 b x q x 0 0 we take 200 uniform computational cells and set the final time as t 200 when the flow reaches moving water steady states depending on different boundary conditions the flow can be subcritical or transcritical with or without a steady shock analytical solutions for these moving water steady states can be computed and will be provided for comparison 4 6 1 subcritical flow the boundary condition is set as h u 4 42 at the upstream and h 2 at the downstream two different sets of channel width σ x defined in 33 are considered in this case one with a left shifted contraction x l 3 75 x r 16 25 σ 0 0 05 and the other with a right shifted contraction x l 8 75 x r 21 25 σ 0 0 05 the flow will evolve to a moving water steady state at time t 200 the converged state is a subcritical flow we show the surface level h b and the mass flow rate q at the final time in fig 7 and also include the analytical solutions in them for comparison it is clear that the numerical solutions are in good agreement with the analytic ones we can also observe the effect of different σ x on the converged steady state solutions 4 6 2 transcritical flow without a shock the boundary condition is set as h u 1 53 at the upstream and h 0 66 at the downstream two different sets of channel σ x defined in 33 are considered one with a left shifted contraction x l 3 75 x r 16 25 σ 0 0 15 and the other with a right shifted contraction x l 8 75 x r 21 25 σ 0 0 15 the converged steady state solution is a transcritical flow without a shock the surface level h b and the mass flow rate q are plotted in fig 8 which show very good agreement with the analytical solutions 4 6 3 transcritical flow with a shock the boundary condition is set as h u 0 18 at the upstream and h 0 33 at the downstream two different sets of channel σ x defined in 33 are considered one with a left shifted contraction x l 3 75 x r 16 25 σ 0 0 15 and the other with a right shifted contraction x l 8 75 x r 21 25 σ 0 0 15 the converged steady state solution is a transcritical flow with a shock appearing in the middle of the domain we present the surface level h b and the mass flow rate q in fig 9 which agree well with the analytical solutions 5 conclusions efficient dg methods have been designed in this paper for the shallow water flows in open channels with a non flat bottom topography the proposed methods have two nice features well balanced for the still water steady state solutions and positivity preserving near the wetting and drying front the well balanced property is achieved via a novel source term splitting and appropriate well balanced approximation of each split source term a simple positivity preserving limiter adopted from zhang and shu 2010 was used to ensure the resulting methods maintain the non negativity of the cross sectional wet area we have carried out extensive numerical simulations which demonstrate that the proposed methods are well balanced efficient for the small perturbation test near the steady state solutions positivity preserving near the wetting and drying front high order accurate and also perform well for both continuous and discontinuous solutions future work include the extension to the generalized model with the channel width σ depending on both x and z acknowledgments the second author g li acknowledges the support of the national natural science foundation of china through grants 11771228 the third author f shao was supported by the national natural science foundation of china through grants 41476101 the work of the last author y xing was partially supported by the nsf grant dms 1753581 and onr grant n00014 16 1 2714 
830,we construct and study efficient high order discontinuous galerkin methods for the shallow water flows in open channels with irregular geometry and a non flat bottom topography in this paper the proposed methods are well balanced for the still water steady state solution and can preserve the non negativity of wet cross section numerically the well balanced property is obtained via a novel source term separation and discretization a simple positivity preserving limiter is employed to provide efficient and robust simulations near the wetting and drying fronts numerical examples are performed to verify the well balanced property the non negativity of the wet cross section and good performance for both continuous and discontinuous solutions keywords shallow water flows cross section discontinuous galerkin well balanced positivity preserving method 1 introduction the shallow water equations are commonly used to model and simulate flows in rivers and coastal areas in this paper we consider one dimensional shallow water flows in open channels with irregular geometry and a non flat bottom topography hernández duenas and karni 2011 vázquez céndon 1999 taking the form of 1 h t q x 0 q t q 2 h 1 2 g σ h 2 x 1 2 g h 2 σ x g σ h b x where σ x represents the width of the channel b denotes the bottom topography h is the water height h σ h is the wet cross section q h u is the mass flow rate u is the velocity and g is the gravitational constant this model is characterized by the non dimensional froude number u c where c g h σ and reduces to the nonlinear shallow water equations when the cross section σ x is a constant the shallow water flows in channels 1 belong to the class of hyperbolic equations with source terms also referred as hyperbolic balance laws and admit the still water steady state solution given by 2 u 0 h b c o n s t in which the source term is exactly balanced by the flux gradient one main challenge in the numerical simulation of such balance laws including the nonlinear shallow water equations with a non flat bottom topography and euler equations under gravitation fields is that a standard numerical method may not satisfy the balance of flux gradient and source term at the steady state in the discrete level and may introduce spurious oscillations near the steady state well balanced methods are designed to overcome this challenge and performs well at or near the steady state with coarse meshes many well balanced methods have been designed for the nonlinear shallow water equations see audusse et al 2004 bermudez and vazquez 1994 greenberg and leroux 1996 leveque 1998 perthame and simeoni 2001 xu 2002 xing 2014 xing and shu 2005 xing and shu 2013 and the references therein another commonly encountered challenge in the simulations of the shallow water related models is the wetting and drying treatment for the region where there is little or no water typical applications include the dam break problem flood waves and run up phenomena numerically negative water may be produced during the computation which may pose additional difficulty there are many existing positivity preserving techniques to overcome this difficulty and we refer to bokhove 2005 bunya et al 2009 ern et al 2008 xing and zhang 2013 xing et al 2010 kurganov and levy 2002 and the references therein for some recent works for the shallow water flows in open channels 1 an early work on well balanced methods was carried out in 1999 by vázquez céndon 1999 where the model was first reformulated into the nonlinear shallow water equations with additional source terms representing the channel width the method proposed in bermudez and vazquez 1994 can then be applied to deliver well balanced methods for the shallow water flows in channels garcía navarro and vázquez céndon 2000 designed well balanced method using proper flux difference splitting recently balbas and karni 2009 designed second order well balanced positivity preserving numerical methods in rectangular channels using the central schemes the extension to shallow water flows with arbitrary cross section was studied in hernández duenas and karni 2011 and balbás and hernández duenas 2014 well balanced method based on energy balanced property is studied in murillo and garcía navarro 2014 hernández duenas and beljadid 2016 developed a new non oscillatory semi discrete central upwind scheme coupled with artificial viscosity xing 2016 designed high order well balanced finite volume weighted essentially non oscillatory schemes for shallow water flows in open channels with irregular geometry and a non flat bottom topography all of the works mentioned above for the shallow water flows in open channels are based on finite difference or finite volume schemes during the past few decades high order finite element discontinuous galerkin dg methods have gained great attention in solving partial differential equations including the hyperbolic conservation laws dg methods using discontinuous piecewise polynomial space as the solution and test function spaces see cockburn et al 2000 for a historic review combine advantages of both finite element and finite volume methods and can achieve high order of accuracy easily with the use of high order polynomials within each element several advantages of the dg methods including their accuracy high parallel efficiency flexibility for hp adaptivity and arbitrary geometry and meshes make them attractive for a wide range of applications including the shallow water simulations the main objective of this paper is to develop efficient high order dg methods for the shallow water flows in open channels with non flat bottom topography the proposed methods have two attractive features well balanced for the still water steady state solutions and positivity preserving near the wetting and drying front this will be the first paper on high order dg methods for the shallow water flows in open channels to achieve these properties to our best knowledge to achieve well balanced property we start by rewriting the source terms in an equivalent special form using the still water steady state solution 2 then we apply integration by parts on these source terms to derive a numerical approximation which is exactly well balanced with the flux gradient at the steady state and also high order accurate for general solutions the approach to achieve high order well balanced property is very different from the other high order method for the shallow water flows in channels presented in xing 2016 where an extrapolation on the source term approximation was introduced to increase the order from second order to fourth order here we could achieve any order of accuracy in the dg framework a simple positivity preserving limiter adopted from zhang and shu 2010 and later applied to dg methods for the shallow water equations in xing et al 2010 was used to ensure the resulting methods maintain the non negativity of the cross sectional wet area this paper is organized as follows in section 2 we first present the novel high order well balanced dg methods the positivity preserving technique is presented in section 3 in section 4 some numerical examples are presented to verify the well balanced property the non negativity of the wet cross section high order accuracy in smooth regions for general solutions and essentially non oscillatory for general solutions with discontinuities finally some conclusions are given in section 5 2 well balanced methods in this section we present high order well balanced dg methods for the shallow water flows 1 in open channels which can preserve the still water steady state 2 exactly 2 1 notations we divide the interval i a b into n subintervals and denote the cells by i j x j 1 2 x j 1 2 for j 1 n the center of each cell is x j 1 2 x j 1 2 x j 1 2 and the mesh size is denoted by τ j x j 1 2 x j 1 2 with τ max 1 j n τ j being the maximal mesh size the piecewise polynomial space v τ k is defined as the space of polynomials of degree up to k in each cell ij that is 3 v τ k v v i j p k i j j 1 2 n note that the functions in v τ k are allowed to have discontinuities across element interfaces for any unknown function v its numerical approximation in the dg methods is denoted by vτ which belongs to the finite element space v τ k we denote by v τ j 1 2 and v τ j 1 2 the limit values of vτ at x j 1 2 from the right cell i j 1 and from the left cell ij respectively in addition we use the usual notation v τ j 1 2 v τ j 1 2 v τ j 1 2 2 to represent the arithmetic mean of the function vτ at the element interface x j 1 2 2 2 reformulation of the system and standard dg methods the first step in designing well balanced methods is to rewrite the source terms in an equivalent form using the information of the steady state solution 2 here we reformulate the original governing equations as 4 h t q x 0 q t q 2 h 1 2 g σ h 2 x 1 2 g h b 2 σ x g h b σ b x 1 2 g σ b 2 x where we replace the source term 1 2 g h 2 σ x g σ h b x by the equivalent form of 1 2 g h b 2 σ x g h b σ b x 1 2 g σ b 2 x in the momentum equation note that when σ is constant this reformulation reduces to the one in xing and shu 2006 for the shallow water equations therefore this can be viewed as a generalization of the technique presented in xing and shu 2006 for the shallow water flows in channels for the sake of easy presentation we introduce the notation 5 u h q to denote the conservative variable and rewrite the shallow water flows in channels 4 as 6 u t f u σ x s u σ σ b σ b 2 where f u σ and s u σ σb σb 2 denote the flux and source term respectively the initial condition of u x 0 is projected into the piecewise polynomial space v τ k to obtain uτ x 0 via the standard l 2 projection similarly we project σ x and σb x to obtain the piecewise polynomial functions στ and σb τ the standard dg method for the class of hyperbolic equations 6 has the form 7 i j t u τ n v d x i j f u τ n σ τ x v d x f j 1 2 v j 1 2 f j 1 2 v j 1 2 i j s u τ n σ τ σ b τ σ b τ 2 σ τ v d x with the numerical fluxes given by 8 f j 1 2 f u τ j 1 2 n u τ j 1 2 n σ τ j 1 2 σ τ j 1 2 where f is a numerical flux herein we adopt the simple lax friedrichs flux 9 f u τ j 1 2 n u τ j 1 2 n σ τ j 1 2 σ τ j 1 2 1 2 f u τ j 1 2 n σ τ j 1 2 f u τ j 1 2 n σ τ j 1 2 α u τ j 1 2 n u τ j 1 2 n where α max u λ u with λ u being the eigenvalues of the jacobian f u and the maximum is taken over the whole region for this system we have α max x i u τ g h τ high order total variation diminishing tvd runge kutta methods shu 1988 are often used as the temporal discretization of the method 7 in the numerical examples of this paper the third order tvd runge kutta method 10 u 1 u n δ t f u n u 2 3 4 u n 1 4 u 1 δ t f u 1 u n 1 1 3 u n 2 3 u 2 δ t f u 2 with f u being the spatial operator is used 2 3 well balanced numerical fluxes note that the standard dg methods 7 presented in the preceding subsection do not have the well balanced property in this subsection we present the modification to the numerical fluxes with the purpose of preserving the steady state 2 exactly suppose the initial condition is given as the steady state 2 i e h σ b h b c and q h u 0 with c being a constant we want to recover this steady state information using the computational variables uτ piecewise polynomials at each time level the projection of the initial condition leads to the fact that h τ σ b τ σ τ c at each time step we introduce the functions hτ and bτ based on the numerical solutions as 11 h τ h τ σ τ b τ σ b τ σ τ and their sum h b τ defined by 12 h b τ h τ σ b τ σ τ note that inside each cell ij h b τ is a rational polynomial function and may not be a polynomial but at the steady state solution 2 we can easily obtain that h b τ c at the cell boundary x j 1 2 we can evaluate u τ j 1 2 using the polynomials from the left and right boundaries similarly we can obtain σ τ j 1 2 and σ b τ j 1 2 the cell average and cell boundary values of h b τ are defined as 13 h b τ j h τ σ b τ j σ τ j h b τ j 1 2 h τ j 1 2 σ b τ j 1 2 σ τ j 1 2 which also satisfy h b τ j h b τ j 1 2 c at the still water state 2 for the purpose of positivity preserving as explained in section 3 we also introduce the updated cell boundary values at the time step tn as in audusse et al 2004 14 h τ j 1 2 max 0 h τ j 1 2 n b τ j 1 2 max b τ j 1 2 b τ j 1 2 which satisfies h τ j 1 2 h τ j 1 2 at the steady state now let us discuss the well balanced numerical fluxes in the lax friedrichs numerical flux f j 1 2 defined in 9 the additional term α u τ j 1 2 n u τ j 1 2 n contributes to the numerical viscosity which is essential for this nonlinear conservation laws however they may destroy the well balanced property at the steady state therefore we propose to modify this flux 9 as 15 f u τ j 1 2 n u τ j 1 2 n σ τ j 1 2 σ τ j 1 2 1 2 σ h u q 2 h 1 2 g σ h 2 τ j 1 2 n σ h u q 2 h 1 2 g σ h 2 τ j 1 2 n α σ h q τ j 1 2 n σ h q τ j 1 2 n where 16 σ τ j 1 2 min σ τ j 1 2 σ τ j 1 2 here we modify the first component of the flux term from q τ to σ τ h τ u τ for the purpose of positivity preserving to be explained in section 3 2 4 source term approximation next we present the high order well balanced approximation to the source term integration in xing 2016 we designed high order well balanced finite volume methods for the shallow water flows in channels and the well balanced approximation to the source term integration is obtained via an extrapolation technique that approach can only provide even order approximation to the source term and it is not easy to extend it to finite element methods here a very different approach to approximate the source term is presented for our dg methods the source term of the momentum equation in 4 takes the form of 17 i j s 2 v d x 1 2 g i j h b 2 σ x v d x g i j h b σ b x v d x 1 2 g i j σ b 2 x v d x where s 2 denote the second component of the source term we can further decompose this integral as 18 i j s 2 v d x 1 2 g i j h b 2 h b j 2 σ x v d x g i j h b h b j σ b x v d x 1 2 g h b j 2 i j σ x v d x g h b j i j σ b x v d x 1 2 g i j σ b 2 x v d x 1 2 g i j h b 2 h b j 2 σ x v d x g i j h b h b j σ b x v d x 1 2 g h b j 2 σ v x j 1 2 σ v x j 1 2 i j σ v x d x g h b j σ b v x j 1 2 σ b v x j 1 2 i j σ b v x d x 1 2 g σ b 2 v x j 1 2 σ b 2 v x j 1 2 i j σ b 2 v x d x by adding and subtracting the constant terms h b j 2 h b j inside the integral and applying integration by parts note that the constant term h b j can be replaced by any other term that can recover constant c at the still water steady state 2 for example h b x j our numerical approximation to the source term 18 takes the following form 19 i j s τ 2 v d x 1 2 g i j h b τ 2 h b τ j 2 σ τ x v d x g i j h b τ h b τ j σ b τ x v d x 1 2 g h b τ j 2 σ τ j 1 2 v j 1 2 σ τ j 1 2 v j 1 2 i j σ τ v x d x g h b τ j σ b τ j 1 2 v j 1 2 σ b τ j 1 2 v j 1 2 i j σ b τ v x d x 1 2 g σ b τ 2 σ τ j 1 2 v j 1 2 σ b τ 2 σ τ j 1 2 v j 1 2 i j σ b τ 2 σ τ v x d x where the terms σ σb σb 2 h b h b j are replaced by their numerical approximations στ σb τ σ b τ 2 σ τ h b τ h b τ j respectively moreover the boundary values of στ σb τ σ b τ 2 σ τ are replaced by their averages at the cell interface denoted by στ σb τ σ b τ 2 σ τ to be consistent with the numerical flux 15 at the steady state 2 5 slope limiter for the system of hyperbolic conservation laws when the solution contains discontinuities dg methods are often coupled with a slope limiter procedure to compress the possible oscillation near the discontinuities many different choices of slope limiters are available in the literature and we consider the classical total variation bounded tvb limiter shu 1987 in this paper the standard tvb limiter on the unknown uτ involves two steps the detection of the troubled cell and the reconstruction of the polynomial solution in these troubled cells the first step replies on the troubled cell indicators which analyze the smoothness of the solution in the cell ij based on the cell averages u τ j u τ j 1 and cell boundary values u τ j 1 2 u τ j 1 2 using these quantities we can construct the forward and backward differences 20 δ u τ j u τ j 1 u τ j δ u τ j u τ j u τ j 1 u τ j u τ j 1 2 u τ j u τ j u τ j u τ j 1 2 the tvb limiter is applied to these differences to obtain u τ j m o d m u τ j δ u τ j δ u τ j u τ j m o d m u τ j δ u τ j δ u τ j where m is the minmod type tvb limiter m b 1 b 2 b n b 1 if b 1 m δ x 2 sign b 1 min 1 i n b i if b 1 m δ x 2 and sign b 1 sign b n 0 otherwise with m being the tvb parameter to be chosen adequately for each test and proportional to the second derivatives of the solution near smooth critical points the cell interface values are then updated as 21 u τ j 1 2 m o d u τ j u τ j m o d u τ j 1 2 m o d u τ j u τ j m o d and the cell ij is marked as a troubled cell if the update in 21 changes either of the two cell interface values i e if u τ j 1 2 m o d u τ j 1 2 or u τ j 1 2 m o d u τ j 1 2 if a cell is marked as a troubled cell the reconstruction step is to replace the polynomial in this cell by a limited linear or quadratic polynomial for example one can choose the limited function u τ j m o d as the unique quadratic polynomial satisfying 21 and also maintaining the original cell average u τ j for the system of conservation laws we often apply the tvb limiter with local characteristic decomposition to achieve better numerical results i e the variables are projected into the characteristic direction before evaluating the differences in 20 after we compute the updated cell interface values 21 we project them back to the physical spaces to detect the troubled cells the standard limiter may conflict with the well balanced property and we propose a well balanced limiter procedure following the idea presented in xing et al 2010 by modifying the troubled cell detection step in the proposed well balanced slope limiter procedure we will first check if the limiting is needed based on the cell averages h b τ j h b τ j 1 and h b τ j 1 2 h b τ j 1 2 if the cell ij is flagged as needing limiting the actual tvb limiter is applied on uτ as the above procedure note that h b τ becomes constant at the still water steady state 2 therefore no limiter is applied when the steady state is reached and the well balanced property is maintained 2 6 summary of the well balanced schemes our proposed well balanced dg methods for the shallow water flows 1 in channels are given by 7 where the numerical fluxes are defined in 8 and 15 and the source term approximation is provided in 19 the method is completed by a temporal tvd runge kutta discretization 10 with the well balanced slope limiter procedure applied in each inner stage of the runge kutta methods collecting the results of the previous subsections it is straightforward to prove the following result proposition 1 the dg methods for the shallow water flows in channels 1 as described above can maintain the well balanced property for the steady state solutions 2 proof at the steady state 2 we have h b τ c q τ 0 for the mass equation h t q x 0 numerical approximation of the source term is 0 and the approximation of the flux term is f j 1 2 1 v j 1 2 f j 1 2 1 v j 1 2 i j f 1 v x d x where f 1 q τ stands for the flux term in the mass equation since q τ 0 at the steady state the volume integral i j f 1 v x d x 0 note that we have h τ j 1 2 h τ j 1 2 from 14 therefore the numerical flux f 1 as defined in 15 becomes zero too and there is no numerical dissipation next we prove the well balanced property for the momentum equation which contains the source term denote the flux term in this equation by f 2 with the numerical flux given in 15 its numerical approximation on the left side of 7 takes the form of 22 f j 1 2 2 v j 1 2 f j 1 2 2 v j 1 2 i j f 2 v x d x 1 2 g σ τ h τ 2 j 1 2 v x j 1 2 1 2 g σ τ h τ 2 j 1 2 v x j 1 2 i j 1 2 g σ τ h τ 2 v x d x since h b τ h b τ j c the source term approximation 19 becomes i j s τ 2 v d x 1 2 g c 2 σ τ j 1 2 v j 1 2 σ τ j 1 2 v j 1 2 i j σ τ v x d x g c σ b τ j 1 2 v j 1 2 σ b τ j 1 2 v j 1 2 i j σ b τ v x d x 1 2 g σ τ b τ 2 j 1 2 v j 1 2 σ τ b τ 2 j 1 2 v j 1 2 i j σ τ b τ 2 v x d x notice the equality that 23 1 2 g σ τ h τ 2 1 2 g σ τ h b τ 2 g σ τ h b τ b τ 1 2 g σ τ b τ 2 1 2 g c 2 σ τ g c σ τ b τ 1 2 g σ τ b τ 2 therefore we can conclude that the flux and source term approximations balance each other at the still water steady state which leads to the desired well balanced property 3 positivity preserving methods zhang and shu 2010 proposed a framework to design high order maximum principle preserving methods for hyperbolic conservation laws since then the method has gained many attentions and has been applied to various applications including the shallow water equations in xing 2016 xing and zhang 2013 and xing et al 2010 it was shown to be able to maintain the non negativity of water height under suitable cfl condition without affecting the mass conservation and keep the high order accuracy for the general solutions here we will explore the application of this approach to the well balanced dg methods presented in section 2 for the shallow water flows in open channels as explained in zhang and shu 2010 the key components to achieve this goal are the following two items the positivity of the first order version of this method and a simple positivity preserving limiter to be coupled with the high order method following the setup we only consider the simple euler time discretization and the same results can be generalized to multi step and tvd high order runge kutta methods without loss of generality we ignore the subscript τ in this section consider the well balanced dg methods 7 with the numerical fluxes defined in 15 take the test function v 1 leads to the following update of the cell averages of the wet cross section 24 h j n 1 h j n λ f j 1 2 1 f j 1 2 1 where 25 f j 1 2 1 1 2 σ j 1 2 h j 1 2 u j 1 2 n σ j 1 2 h j 1 2 u j 1 2 n α σ j 1 2 h j 1 2 σ j 1 2 h j 1 2 and λ δ t δ x the first order version of this method takes the form of 26 h j n 1 h j n λ f j 1 2 1 f j 1 2 1 with the numerical flux defined as 27 f j 1 2 1 1 2 σ j 1 2 h j 1 2 u j n σ j 1 2 h j 1 2 u j 1 n α σ j 1 2 h j 1 2 σ j 1 2 h j 1 2 and the first order version of the starred terms h j 1 2 σ j 1 2 given by 28 h j 1 2 max 0 h j n b j max b j b j 1 h j 1 2 max 0 h j 1 n b j 1 max b j b j 1 σ j 1 2 min σ j σ j 1 we have the following lemma about the positivity preserving property of this first order method lemma 2 under the cfl condition λα 1 with α max u g h consider the first order scheme 26 with the numerical flux 27 and 28 if h j n h j 1 n are non negative then h j n 1 is also non negative proof the scheme 26 can be written as h j n 1 1 1 2 λ α u j n σ j 1 2 σ j h j 1 2 h j n 1 2 λ α u j n σ j 1 2 σ j h j 1 2 h j n σ j h j n 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 1 2 λ α u j n 1 2 λ α u j n σ j h j n 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 λ α σ j h j n 1 2 λ α u j 1 n σ j 1 2 h j 1 2 1 2 λ α u j 1 n σ j 1 2 h j 1 2 since 0 h j 1 2 h j 1 2 h j n and σ j 1 2 σ j this also justifies the reason that we pick the minimum value in the definition of σ in 16 therefore h j n 1 is a linear combination of h j 1 n h j 1 2 and h j 1 2 and all the coefficients are non negative which leads to h j n 1 0 next we move to discuss high order schemes we refer to zhang and shu 2010 and xing et al 2010 for the details and only present the main idea here we introduce the n point with 2 n 3 k legendre gauss lobatto quadrature rule on the interval ij and denote these quadrature points by s j x j 1 2 x j 1 x j 2 x j n 1 x j n x j 1 2 with the corresponding quadrature weights w r for the interval 1 2 1 2 satisfying r 1 n w r 1 we employ the following positivity preserving limiter xing et al 2010 zhang and shu 2010 on the dg polynomial u j n x h j n x q j n x t 29 u j n x θ u j n x u j n u j n θ min 1 h j n h j n m j with 30 m j min x s j h j n x min r 1 n h j n x j r with this choice of mj we can show that h j n x j r 0 r 1 n and this limiter maintains the local conservation of the variable u j n x we compute the modified polynomial u j n x and use u j n x instead of u j n x in the well balanced methods 7 following the proofs in xing et al 2010 and zhang and shu 2010 we can verify that the well balanced methods coupled with this positivity preserving limiter are high order accurate positivity preserving and mass conservation under the cfl condition 31 λ α w 1 to be efficient we could implement the time step restriction 31 only when a preliminary calculation to the next time step produces negative wet cross section 4 numerical examples in this section we carry out extensive numerical experiments to demonstrate the performance of the proposed positivity preserving well balanced dg methods for the shallow water flows in open channels the third order finite element dg methods i e k 2 coupled with third order tvd runge kutta methods 10 are implemented in these examples the cfl number is taken as 0 16 which satisfies the requirement 31 to achieve positivity preserving property the gravitation constant g is fixed as 9 812 m s2 channels with both continuous and discontinuous width functions have been tested 4 1 accuracy test we first test the third order accuracy of the resulting method on an example with smooth solutions the following periodic bottom topography and channel width function b x sin 2 π x σ x e sin 2 π x are considered in this example the initial conditions are given by h x 0 3 e cos 2 π x q x 0 sin cos 2 π x x 0 1 with periodic boundary conditions we run the test up to the stopping time t 0 1 when the solutions are still smooth as the exact solutions are not available for this nonlinear system we apply the same method with a much refined n 25 600 cells to obtain a reference solution and then treat it as the exact solution when computing the errors and convergence rates which are shown in table 1 we can clearly observe that the expected third order accuracy is achieved 4 2 well balanced test the purpose of the second test problem balbas and karni 2009 xing 2016 is to verify the well balanced property of our proposed dg methods we consider the bottom topography given by 32 b x 0 25 1 cos 10 π x 0 5 if 0 4 x 0 6 0 otherwise in the domain 0 1 the channel with varying width σ x takes the form of 33 σ x 1 σ 0 1 cos 2 π x x l x r 2 x r x l if x x l x r 1 otherwise where xl and xr are the left and right boundary of the contraction and 1 2 σ 0 represents the minimum width of the channel at the point x l x r 2 in this example we choose x l 0 25 x r 0 75 and σ 0 0 2 the initial condition is provided as the steady state solution h b 1 q σ h u 0 and the periodic boundary condition is considered we solve the problem with 200 uniform cells until the final time t 1 the numerical surface level h b and the bottom b are plotted in fig 1 the 3d plot of the bottom topography b and the channel shape σ is provided in fig 2 in order to demonstrate that the steady state solution is maintained up to round off error the l 1 l 2 and l errors of the wet cross section h and the mass flow rate q with single precision and double precision are shown in table 2 we can clearly see that the l 1 l 2 and l errors are all at the level of round off errors for these precisions which verify the expected well balanced property 4 3 small perturbation test in this subsection we simulate the propagation of small perturbations to a steady state solution to demonstrate the capability of the proposed dg methods for such challenging case this test was first proposed by balbas and karni 2009 we set the bottom topography as 32 and the initial condition as h b 1 0 01 if 0 1 x 0 2 1 otherwise q σ h u 0 in a computational domain 0 1 with simple transmissive boundary conditions two different sets of channel σ x defined in 33 are tested one with a left shifted contraction x l 0 15 x r 0 65 σ 0 0 2 and the other with a right shifted contraction x l 0 35 x r 0 85 σ 0 0 2 for these tests involving such small perturbation of steady state solutions non well balanced numerical methods usually have difficulty with the calculations and produce oscillatory results kurganov and levy 2002 the numerical results of the proposed well balanced dg methods at different times on 200 uniform computational cells compared with refined 2000 cells reference solutions are shown in fig 3 for the sake of comparison we also present the numerical results by the non well balanced dg methods with 200 cells we can clearly observe that the well balanced results are free of spurious numerical oscillations and our methods can numerically capture such small perturbation well on relatively coarse meshes 4 4 a converging diverging channel here we consider the classic transcritical steady flow in a converging diverging channel originally proposed by garcía navarro et al 1992 this test is related to many practical problems such as the flow between bridge piers the bottom is set as flat i e b 0 and the converging diverging channel is given by σ x 5 0 7065 1 cos 2 π x 250 300 if x 150 450 5 otherwise in the computational domain 0 500 the initial conditions are set as h 2 q σ h u 20 and the boundary conditions are given by q 20 at the upstream and h 1 85 at the downstream we compute this test up to a long time t 5000 until it reaches a steady state using 200 uniform cells and present the numerical result of the water height h in fig 4 which agrees well with those in the literature garcía navarro et al 1992 vázquez céndon 1999 the flow changes from subcritical flow to supercritical flow at the critical point x 250 and later becomes subcritical flow via a stationary hydraulic jump to connect to the subcritical downstream boundary condition 4 5 drain on a non flat bottom in this section we consider a drainage test first proposed by gallouët et al 2003 and later appeared in xing and shu 2011 for the shallow water equations the goal is to test both well balanced property and positivity preserving feature of our methods the bottom topography is given by b x 0 2 0 05 x 10 2 if 8 x 12 0 otherwise on the computational domain 0 25 and the channel width is set as σ x 1 0 2 1 cos 2 π x 10 12 5 if 3 75 x 10 1 otherwise which is discontinuous at the point x 10 and is shown in fig 5 we consider the initial conditions as follows h x 0 0 5 b x h x 0 σ x h x 0 q x 0 0 a free boundary condition on h and zero on q is imposed on the left boundary and an outlet condition on a dry bed refer to gallouët et al 2003 for the details is used on the right boundary a uniform mesh with 250 cells is taken we present the water surface level h b and the discharge q at t 10 20 100 and 500 in fig 6 respectively we can observe that the numerical solution reaches the steady state after a long time the converged steady state is a still water requiring well balanced feature of the numerical methods on the left of the bump and a dry state requiring positivity preserving feature of the numerical methods on the right of the bump the numerical results show that our proposed dg methods work well for this challenging test even with a discontinuous channel 4 6 moving water steady states over a hump in this last numerical example xing 2016 we consider the convergence of our methods towards steady transcritical and subcritical flows with different channel configurations in xing and shu 2006 the same tests are used to check the performance of well balanced dg methods for the shallow water equations i e constant channel width the computational domain is set as 0 25 and the bottom topography is defined by b x 0 2 0 05 x 10 2 if 8 x 12 0 otherwise we choose different sets of variable channel width to be defined in each case to demonstrate the effect of channel on the final solutions the initial conditions are given by h x 0 0 5 b x q x 0 0 we take 200 uniform computational cells and set the final time as t 200 when the flow reaches moving water steady states depending on different boundary conditions the flow can be subcritical or transcritical with or without a steady shock analytical solutions for these moving water steady states can be computed and will be provided for comparison 4 6 1 subcritical flow the boundary condition is set as h u 4 42 at the upstream and h 2 at the downstream two different sets of channel width σ x defined in 33 are considered in this case one with a left shifted contraction x l 3 75 x r 16 25 σ 0 0 05 and the other with a right shifted contraction x l 8 75 x r 21 25 σ 0 0 05 the flow will evolve to a moving water steady state at time t 200 the converged state is a subcritical flow we show the surface level h b and the mass flow rate q at the final time in fig 7 and also include the analytical solutions in them for comparison it is clear that the numerical solutions are in good agreement with the analytic ones we can also observe the effect of different σ x on the converged steady state solutions 4 6 2 transcritical flow without a shock the boundary condition is set as h u 1 53 at the upstream and h 0 66 at the downstream two different sets of channel σ x defined in 33 are considered one with a left shifted contraction x l 3 75 x r 16 25 σ 0 0 15 and the other with a right shifted contraction x l 8 75 x r 21 25 σ 0 0 15 the converged steady state solution is a transcritical flow without a shock the surface level h b and the mass flow rate q are plotted in fig 8 which show very good agreement with the analytical solutions 4 6 3 transcritical flow with a shock the boundary condition is set as h u 0 18 at the upstream and h 0 33 at the downstream two different sets of channel σ x defined in 33 are considered one with a left shifted contraction x l 3 75 x r 16 25 σ 0 0 15 and the other with a right shifted contraction x l 8 75 x r 21 25 σ 0 0 15 the converged steady state solution is a transcritical flow with a shock appearing in the middle of the domain we present the surface level h b and the mass flow rate q in fig 9 which agree well with the analytical solutions 5 conclusions efficient dg methods have been designed in this paper for the shallow water flows in open channels with a non flat bottom topography the proposed methods have two nice features well balanced for the still water steady state solutions and positivity preserving near the wetting and drying front the well balanced property is achieved via a novel source term splitting and appropriate well balanced approximation of each split source term a simple positivity preserving limiter adopted from zhang and shu 2010 was used to ensure the resulting methods maintain the non negativity of the cross sectional wet area we have carried out extensive numerical simulations which demonstrate that the proposed methods are well balanced efficient for the small perturbation test near the steady state solutions positivity preserving near the wetting and drying front high order accurate and also perform well for both continuous and discontinuous solutions future work include the extension to the generalized model with the channel width σ depending on both x and z acknowledgments the second author g li acknowledges the support of the national natural science foundation of china through grants 11771228 the third author f shao was supported by the national natural science foundation of china through grants 41476101 the work of the last author y xing was partially supported by the nsf grant dms 1753581 and onr grant n00014 16 1 2714 
831,in this study a one and two dimensional 1d 2d coupled model is developed to solve the shallow water equations swes the solutions are obtained using a lagrangian meshless method called smoothed particle hydrodynamics sph to simulate shallow water flows in converging diverging and curved channels a buffer zone is introduced to exchange information between the 1d and 2d sph swe models interpolated water discharge values and water surface levels at the internal boundaries are prescribed as the inflow outflow boundary conditions in the two sph swe models in addition instead of using the sph summation operator we directly solve the continuity equation by introducing a diffusive term to suppress oscillations in the predicted water depth the performance of the two approaches in calculating the water depth is comprehensively compared through a case study of a straight channel additionally three benchmark cases involving converging diverging and curved channels are adopted to demonstrate the ability of the proposed 1d and 2d coupled sph swe model through comparisons with measured data and predicted mesh based numerical results the proposed model provides satisfactory accuracy and guaranteed convergence keywords smoothed particle hydrodynamics shallow water equations 1d 2d coupled open channel 1 introduction open channel flow is an essential topic in hydraulics and studies of this topic range from drainage in artificial channels and rivers ying et al 2004 burguete et al 2008 and the design of hydraulic structures such as spillways unami et al 1999 and bridges biglari and sturm 1998 to flood prevention measures hsu et al 2003 numerical simulation approaches are extensively used to study open channel flow by solving either 1d or 2d shallow water equations swes for the discharge wetted cross sectional area flow velocity and depth chaudhry 2008 the swes can be derived from the area integrated and depth integrated navier stokes equations in general 1d swe models are used to model large open channel systems because of their high efficiency in comparison with the 2d swe models for example 1d swe models approximated by the finite difference method fdm choi and molinas 1993 finite element method fem sen and garg 1998 and finite volume method fvm sanders et al 2001 have been successfully applied to model large open channel flows however the existing 1d swe models cannot resolve the 2d flow phenomena that can be captured by 2d swe models e g flows in converging diverging or curved channels various coupled approaches that combine computationally efficient 1d swe models with numerically accurate 2d swe models have been proposed in the past effectively exchanging data between the 1d and 2d models is the key to success in these 1d 2d coupled methods the processes of connecting the 1d and 2d domains in 1d 2d coupled swe models can be categorized into two types based on the overlapping area marin and monnier 2009 fernandez nieto et al 2010 arico et al 2016 or the buffer zone blade et al 2012 morales hernandez et al 2013 morales hernandez et al 2016 in coupled models of the first type the 1d model remains intact and the 2d model is locally employed so called local zoom model marin and monnier 2009 first proposed a superposition approach to convert the contribution of the 2d model into the source terms of the 1d governing equations to predict river overtopping flows in coupled models of the second type the 1d model is decomposed into the 2d model and a buffer zone e g control meshes is established that incorporates the 1d and 2d domains blade et al 2012 proposed a flux based connection that considers the effects of the mass and momentum exchanged between the 1d and 2d domains in natural channels morales hernandez et al 2013 developed an approach that accounts for the interactions at the boundaries based on separate mass conservation and mass and momentum conservation strategies in shallow water flow simulations meshless numerical methods such as the smoothed particle hydrodynamics method sph moving particle semi implicit method mps and reproducing kernel particle method rkpm have attracted increased attention because of their excellent ability to overcome large deformation difficulties that arise from fluid solid interactions sph the most popular of these methods is a lagrangian meshless particle method sph was first proposed by lucy 1977 and gingold and monaghan 1977 in their investigations of astrophysical problems particles in space move along pathlines in a lagrangian sense and sph has an advantage over mesh based methods when interfacial flow dynamics in free surface flows monaghan 1994 fluid structure flows shao and gotoh 2004 and mudflows shao and lo 2003 are considered some significant characteristics of sph are as follows 1 unlike the eulerian conservation equations there are no nonlinear convective terms in the lagrangian equations thereby ensuring that the discretization is galilean invariant 2 the interface between two phases can be naturally captured and 3 the interactions between a fluid and a solid structure can be easily modeled bouscasse et al 2013 gong et al 2016 recently sph has also been utilized to solve swes so called sph swe model in modeling dam break flows wang and shen 1999 rodriguez paz and bonet 2005 ata and soulaimani 2005 chang et al 2011 kao and chang 2012 xia et al 2013 gu et al 2017 open channel flows vacondio et al 2012 chang and chang 2013 chen et al 2015 chang et al 2016 chang et al 2017 and run off flows chang et al 2016 in the sph literature narayanaswamy et al 2010 addressed the advantage of coupling a 1d finite difference boussinesq model with 2d sph navier stokes model in coastal flow simulations altomare et al 2015 presented a hybridization technique to pass information between a 2d wave propagation model and a 3d sph navier stokes model in real coastal applications however no numerical study of coupled sph swe models has been performed therefore we propose a coupled 1d and 2d sph swe model to more efficiently simulate open channel flows with complicated boundaries in our proposed model a buffer zone associated with the inflow outflow boundaries of the 1d and 2d computational domains is defined in the channel interior this paper focuses on subcritical flows because they are the most common in rivers and open channels flows such as those that originate from dam breaking are not easily modeled with this approach based on the characteristics of the proposed method only one variable can be assessed at the inflow outflow boundaries in this study the water discharge and water surface level at the internal boundaries are interpolated to specify the required inflow and outflow boundary conditions respectively for the 1d and 2d models to the best of our knowledge all the sph swe models use an sph summation operator instead of solving the continuity equation to calculate water depth because this approach enhances the numerical stability in this study we add the density diffusion term proposed by molteni and colagrossi 2009 into the continuity equation for stability enhancement a comprehensive comparison concerning the accuracy of the water depth and discharge and the computational efficiency of two approaches applied to calculate the variable water depth is performed based on test cases this paper is organized as follows in section 2 the 1d swes for modeling the wetted cross sectional area and water discharge are presented additionally the 2d swes for the water depth and velocity are introduced section 3 gives the fundamental sph operators and details of how the proposed sph swe model can be implemented are presented section 4 is devoted to the treatment of the three boundaries the wall inflow outflow and internal boundaries between the 1d and 2d models finally in section 5 four flow cases are considered in straight converging diverging and curved channels each case is solved to verify and validate the proposed model based on the exact solution the mesh based numerical results and measured data 2 governing equations 2 1 one dimensional shallow water equations the 1d swes can be derived from the area integration of the navier stokes equations chaudhry 2008 and govern the wetted cross sectional area and water discharge in open channel flows the lagrangian form of the 1d swes can be written as follows 1 d a d t a u x 2 d q d t q u x g a d w z b x g a s f in the above equations d d t denotes the total derivative term d d t t u u u v is the water velocity vector where u and v are the x and y components of water velocity respectively q denotes the water discharge au a is the wetted cross sectional area dw is the water depth zb is the bed elevation sf is the friction slope n m a 2 q 2 a 2 r 4 3 nma is the manning roughness coefficient r is the hydraulic radius and g is gravitational acceleration 2 2 two dimensional shallow water equations by vertically the direction of water depth integrating the navier stokes equations the 2d swes for the water depth and velocity derived in chaudhry 2008 can be given in lagrangian form as follows 3 d d w d t d w u 4 d u d t g d w z b g s f where the friction slope s f used in the 2d model is defined as n m a 2 u u d w 4 3 3 sph swe model 3 1 sph operators any physical quantity of particle a such as ϕ a can be approximated as follows within the sph context 5 ϕ r a ϕ r ω r a r h d v b 1 b n m b ϕ b ρ b ω r a b h a in the above equation mb ρ b vb is the mass of particle b ρ b is the density of particle b which is defined as ρ0 ab in the 1d model and ρ0 d w b in the 2d model ρ0 is the bulk density of water 1000 kg m3 vb is the volume of particle b r is the position vector r r x in the 1d model and r r x y in the 2d model r a b r a r b is the distance between particles a and b and ω r a b h a is the kernel function of particle a and is denoted as ω a b a in this study the smoothing length of particle a which is denoted as ha is assigned an initial value of 1 4l 0 l 0 δx 0 in the 1d model and l 0 δx 0 δy 0 in the 2d model where δx 0 and δy 0 are the initial particle spacings in the x and y directions respectively and n is the number of particles in the support domain of particle a violeau 2012 in this study the sph gradient operator is expressed in symmetric form and the sph divergent operator is expressed in asymmetric form violeau 2012 these sph differential operators are shown in eqs 6 and 7 6 ϕ a ρ a b 1 b n m b ϕ a ρ a ϕ b ρ b a ω a b a 7 ϕ a 1 ρ a b 1 b n m b ϕ a ϕ b a ω a b a where a ω a b a denotes ω a b a x a e x ω a b a y a e y and e x and e y are the unit vectors in the directions of the x and y axes respectively in addition the following sph laplacian operator is adopted to consider the viscous acceleration proposed by monaghan monaghan 1992 8 μ u a 2 d m 2 b 1 b n v b μ a b u a b r a b r a b 2 a ω a b a where μ is the kinematic viscosity u a b u a u b and μ a b 0 5 μ a μ b 3 2 discretized shallow water equations based on the chosen sph gradient and the divergent operators shown in eqs 6 and 7 the source terms in eqs 1 to 4 can be approximated as follows 9 d a d t a b 1 b n m b u a b x ω a b 10 dq dt a u a b 1 b n m b u a u b ω ab x g a a 2 b 1 b n m b η w a a a 2 η w b a b 2 ω ab x g a a s f a 11 d d w d t a b 1 b n m b u a b ω a b 12 d u dt a g d w a b 1 b n m b η w a d w a 2 η w b d w b 2 ω ab g s f a in the above equations η w is the water surface level dw zb uab ua ub and the wendland function is used as the kernel function violeau 2012 in our discretized swes the averaged kernel function is considered due to the non constant smoothing length i e ω ab x 0 5 ω ab a x a ω ba b x b and ω ab 0 5 a ω ab a b ω ba b hernquist and katz 1989 given that the mass in the compact domain of a fluid particle is constant i e ρ h d m constant the temporal variation of the smoothing length can be derived as dh dt h dm ρ dρ dt where dm denotes the number of dimensions in the domain altomare et al 2015 furthermore the corrected divergence operator of velocity i e x in eq 9 and in eq 11 as proposed by bonet and lok 1999 is used to achieve first order consistency as shown in eqs 13 and 14 13 ω a b a x a ω a b a x a b 1 b n v b x b a ω a b a x a 14 a ω a b a l a 1 a ω a b a where l a b 1 b n v b x b a ω a b a x a b 1 b n v b y b a ω a b a x a b 1 b n v b x b a ω a b a y a b 1 b n v b y b a ω a b a y a x b a x b x a and y b a y b y a 3 3 stabilization terms 3 3 1 artificial viscosity in the 1d sph swe model an artificial viscous force proposed by monaghan 1992 1994 as expressed in eq 15 is applied to prevent the occurrence of a disordered particle distribution 15 f a art b 1 b n a a b v b ν a b art u a b x a b x a b 2 ɛ 2 ω a b x where ν a b art α h a b c a b h a b 0 5 h a h b c a b 0 5 c a c b a a b 0 5 a a a b ε 10 10 and α is a parameter employed to control the artificial viscous effect 0 3 in this study 3 3 2 eddy viscosity as 2d channel flows become turbulent eddy viscosity effects can no longer be negligibly small particularly in cases with solid boundaries nadaoka and yagi 1998 noted that the turbulence structure of a shallow water flow can be divided into bed friction generated 3d turbulence at length scales less than the water depth and horizontal 2d eddies at much larger length scales bed friction plays a key role in transmitting the dissipated energy from horizontal 2d eddies to 3d turbulent processes therefore the les shallow water model is ideal for describing the development of horizontal 2d large scale eddies hence the sph les model proposed by shao and gotoh 2004 who introduced an les model for an sph solver is adopted to model the eddy viscosity originating from subparticle scale eddies that act on fluid particles as given in eq 16 16 f a e d d y 4 b 1 b n v b d w a ν t a d w b ν t b d w a u a b r a b r a b 2 ɛ 2 ω a b in the above equation ν t is the turbulent kinematic viscosity calculated by c s l 0 2 2 s s where s is the local strain tensor and cs is the smagorinsky constant 0 5 in this study a spatial filter with a width determined by csl 0 is implicitly used in the les model as a result only the solutions of particle scale variables are obtained 3 3 3 density diffusion since the continuity equation is independently solved for each fluid particle oscillatory solutions are frequently found in the density field here based on molteni and colagrossi 2009 the density diffusion terms shown in eqs 17 and 18 are incorporated into the 1d and 2d sph swe models respectively with the aim of suppressing oscillations in the density field 17 s a 1 d b 1 b n d a b v b a a a b x a b x a b 2 ɛ 2 ω a b x 18 s a 2 d b 1 b n d a b v b d w a d w b r a b ω a b r a b 2 ɛ 2 where d a b β h a b c a b and β 0 2 in this study 3 4 time integration method a modified verlet scheme proposed by molteni and colagrossi 2009 is adopted in this study this approach enables the use of large courant friedrichs lewy cfl numbers in the process of updating the solutions at the next time step table 1 shows the algorithms of the modified verlet scheme for the 1d and 2d models the time step δt that satisfies the cfl stability condition is as follows 19 δ t c f l min a l 0 u a c a in this study we set cfl to 0 8 4 boundary treatment 4 1 wall boundary in the 2d model the ghost particle technique developed by colagrossi and landrini 2003 is adopted to specify the wall boundary condition in this study four layers of ghost particles are involved in the computations of the water depth pressure force and viscous force of each fluid particle as schematically shown in fig 1 these ghost particles are associated the velocity divergence operator the gradient operator of water depth and the velocity laplacian operator for the ghost particles influenced by the velocity divergence operator both the normal and tangent components of their velocities are equal to zero due to the stationary non slip walls bouscasse et al 2013 to estimate the viscous interaction between the fluid and ghost particles mirror points such as point c in fig 1 are generated using a mirroring technique along the plane of symmetry following the work of bouscasse et al 2013 in the presence of free slip boundary conditions the normal and tangential components of the velocity of each ghost particle are identical to those at its mirror point however in the case of non slip boundary conditions the tangential velocity of each ghost particle is specified with the opposing sign as that at the mirror point and the normal velocity of each ghost particle remains the same as that at the mirror point to calculate the gradient operator of water depth for ghost particles the impermeable boundary condition shown in eq 20 is applied bouscasse et al 2013 20 d u d t w n g d w z b g s f w n 0 rearranging eq 20 leads to the following formula 21 d w n w z b s f w n taking the water depth bed elevation and water velocity at mirror point c into consideration the water depth of ghost particle g can be determined as follows 22 d w g b 1 b f b n v b d w b l g c z b b s f b n g c ω c b c in the above equation n g c denotes the unit normal vector r c r g r c r g l g c r c r g additionally ω cb c is the corrected kernel function which can be represented as follows randles and libersky 1996 23 ω c b c ω c b c b 1 b f b n v b ω c b c where hc 1 4l 0 4 2 inflow outflow boundary in this study three types of fluid particles namely inflow particles rectangular points inner particles circle points and outflow particles triangular points are used to resolve the problems related to the inflow outflow boundaries as schematically shown in fig 2 since sph is an explicit method the inflow outflow boundary conditions must be determined prior to solving the governing equations at each time step therefore we adopt the specified time interval method chaudhry 2008 to calculate unknown variables at the inflow outflow boundaries fig 2 a depicts the directions of the characteristic lines in the presence of a subcritical flow at the inflow boundary the water discharge is prescribed and the water depth is obtained by solving eq 24 along the negative characteristic line lp schematically shown in fig 2 a via newton raphson iterations 24 q p a p u p a p u l g c l d w p d w l g z b x l s f l δ t for a subcritical flow specified at the outflow boundary the water depth is prescribed and the water discharge is determined from eq 25 along the positive characteristic line rs shown in fig 2 a 25 q s a s u s a s u r g c r d w s d w r g z b x r s f r δ t one can refer to vacondio et al 2012 chang and chang 2013 federico et al 2012 for additional details of this method in the treatment of 2d inflow outflow boundary conditions the 1d specified time interval method is applied in this case no characteristic lines are used when a subcritical flow occurs at the inflow boundary the projection point a of particle i on the inflow boundary as shown in fig 2b is identified then the water depth at point a obtained by solving eq 24 is specified as the water depth of inflow particle i similarly if a subcritical flow is considered at the outflow boundary the water discharge of outflow particle o is equal to the water discharge at projection point d on the outflow boundary obtained using eq 25 as shown in fig 2 b in addition virtual bed particles are introduced to describe the bed elevation and the roughness vacondio et al 2012 the volume of each virtual bed particle equals l 0 and the bed elevation and the roughness of each fluid particle a are computed from eq 26 26 z b a b 1 b v b b n v b z b b ω a b b n m a a b 1 b v b b n v b n m a b ω a b b in the above equation the subscript vb denotes the virtual bed particle and h b vb 1 4l 0 4 3 internal boundary between the 1d and 2d models in the proposed coupled model we use the concept introduced by narayanaswamy et al 2010 to choose a buffer zone that connects the 1d domain and the 2d domain there are two types of connections the first type includes a 1d model at the upstream side and a 2d model at the downstream side and the second includes a 2d model at the upstream side and a 1d model at the downstream side as shown in fig 3 for the first type of connection the locations of section e and section a b in the buffer zone are the same as the inflow boundary of the 2d model and the outflow boundary of the 1d model similarly for the second type of connection the locations of section cd and section f in the buffer zone are the same as the inflow boundary of the 1d model and the outflow boundary of the 2d model here we call the inflow outflow boundaries of the buffer zone the internal boundaries by exchanging the dependent variables in the 1d and 2d models at the internal boundaries one can obtain the required inflow outflow boundary conditions and perform computations at the next time step as subcritical flows form at the internal boundaries the model at the upstream side provides the inflow boundary condition of water discharge for the model at the downstream side i e q a b q e for the first connection and q f q c d for the second connection where q e b 1 b f l a e b n v b q b ω a b a and q c d l c d b 1 b f l a c d b n v b u b n c d ω a b a d l similarly the model at the downstream side provides the outflow boundary condition of the water surface level for the model at the upstream side i e η w e η w a b for the first connection and η w c d η w f for the second connection where η w a b 1 l l a b b 1 b f l a a b b n v b η w b ω a b a d l and η w f b 1 b fl a f b n v b η w b ω ab a in the above equations the subscript fl stands for fluid particles and the smoothing length of interpolation point a is 1 4l 0 in all interpolations thus we can obtain one boundary condition at the inflow outflow boundaries of the 1d and 2d models and solve the characteristic equations shown in section 4 2 to determine another required inflow outflow boundary condition in the current study the width of the buffer zone is chosen as 2 h where h is equal to 2l 0 this value ensures that the sph summation includes the complete support domain at the boundaries of the buffer zone moreover the convergence of the boundary conditions in the buffer zone will be guaranteed as the initial particle spacing approaches zero the buffer zone will shrink to a point thus all boundary conditions at the internal boundary are identical for the 1d and 2d models 5 validation in this section a straight channel is chosen in our simulation to assess the performance of the two approaches to calculate the water depth the first approach uses the sph summation operator 27 d w a b 1 b n m b ω r a b h a since the non constant smoothing length is calculated by h a h 0 a d w 0 a d w a 1 2 where dw 0 and h 0 are the water depth and smoothing length at the beginning of the simulation respectively eq 27 becomes nonlinear a newton raphson iteration is applied to solve eq 25 rodriguez paz and bonet 2005 vacondio et al 2012 the second approach algebraically solves the sph discretized continuity equation given in eq 11 hereafter we call the first and second approaches the summation and continuity approaches respectively next three cases involving converging diverging and curved channels are investigated to assess the performance of the proposed 1d and 2d coupled sph swe model all the channels are rectangular horizontal and frictional numerical simulations were performed on an intel r core tm i7 2600 cpu 3 4 ghz pc equipped with 4gb of ram in addition a non dimensional l2 norm for any physical quantity ϕ is applied to calculate the convergence rate 28 l 2 ϕ 1 n i 1 n ϕ sph ϕ ext ϕ ext 2 in the above equation n is the number of particles at fixed points in the domain and the superscripts sph and ext denote the numerical and exact solutions respectively 5 1 straight channel flow simulation the first simulation involves a straight channel of length 1000 m and width 400 m this channel was considered by vacondio et al 2012 in their simulation using an sph summation operator to calculate the water depth the manning roughness coefficient is 0 0308 s m1 3 the exact solutions of the unit water discharge 15 m2 s and water depth 5 m are prescribed at the inflow and outflow boundaries respectively three different initial particle spacings of 80 m 40 m and 20 m are used to analyze the accuracy and convergence the non dimensional l2 norms based on the water depth and the velocity are calculated as follows 29 l 2 d w 1 n i 1 n d w i sph d w ext d w ext 2 l 2 u 1 n i 1 n u i sph u ext g d w ext 2 l 2 v 1 n i 1 n v i sph v ext g d w ext 2 in the above dw is the water depth d w ext 5 m u is the x component velocity u ext 3 m s v is the y component velocity v ext 0 m s and n is the number of particles in the domain the accuracy and the convergence of the two approaches to calculate the water depth are given in tables 2a and 2b the continuity approach is more accurate than the summation approach in calculating the velocity and the water depth fig 4 presents the simulated water discharge profiles of the two approaches for the case of an initial particle spacing of 20 m as shown in fig 4 the continuity approach exhibits better mass conservation than the summation approach l2 q of the summation approach is 1 02e 04 and l2 q of the continuity approach is 3 81e 05 in addition the continuity approach achieves a faster convergence speed the convergence rates based on the water depth and the velocity approach unity for the summation approach the water depth errors mainly occur in the vicinity of the wall boundaries and are not rapidly reduced by decreasing the initial particle spacing this issue is reflected by the low convergence rates in table 2a table 3 lists the cpu times for each approach involving 1159 particles and the total simulation time is 6000 s the summation approach takes 1 1 times the cpu time consumed by the continuity approach we believe that the approach of calculating water depth via solving the continuity equations can efficiently yield a higher accuracy than the summation approach for the water depth and yields a higher degree of mass conservation therefore in the following case studies only the continuity approach is used in the two sph swe models 5 2 converging channel flow simulation a converging channel with a junction angle of 90 was experimentally studied by weber et al 2001 and is adopted in this study for comparison the converging channels in the 2d and 1d 2d coupled models are schematically shown in fig 5 a and b where the solid lines denote the 2d domain and the dash lines denote the 1d domain table 4 shows the locations of the inflow outflow boundaries in the 1d domain the channel width is 0 914 m and the manning s roughness coefficient of 0 011 s m1 3 accounts for bed friction a water discharge of 0 043 m3 s and water depth d w 0 of 0 296 m are prescribed as the inflow and outflow boundary conditions in the main channel respectively and a water discharge of 0 127 m3 s is prescribed as the inflow boundary condition in the branch channel the outflow discharge in the main channel is therefore equal to 0 17 m3 s the initial particle spacing is set to 0 02285 m in the 1d and 2d models and the maximum time step is 0 007 s the depth contours and the streamline configurations for the two sph swe models are plotted in fig 6 the water depths gradually decrease downstream of the channel convergence section and the flow convergence results in the formation of a vortex fig 7 shows the simulated profiles of water depths at three lateral distances along the main channel the results obtained by the investigated sph swe models are all consistent with the measured data presented by weber et al 2001 and the fem based simulation results of song et al 2012 a comparison of the simulated profiles of water discharge along the main channel based on the two sph swe models is presented in fig 8 when the flow from the upstream side of the main channel converges at the junction with the flow from the branch channel the discharge increases to 0 17 m3 s in the 1d 2d coupled model l2 q in the 1d domain is 1 05e 02 and l2 q in the 2d domain is 7 58e 03 as listed in table 5 the convergence rates based on the water discharge in the 1d and 2d domains are 0 99 and 0 88 respectively in addition table 3 shows that the 2d model takes 2 1 times the cpu time required by the 1d 2d coupled model both the 1d 2d coupled model and 2d model results can be efficiently obtained for the converging channel 5 3 diverging channel flow simulation in the third simulation case a diverging channel with a junction angle of 90 that was experimentally studied by shettar and murthy 1996 is investigated this channel has a width of 0 3 m and the channel bottom is rough with a manning s roughness coefficient of 0 011 s m1 3 fig 9 shows the schematic of the diverging channel investigated using the 2d and 1d 2d coupled models the locations of the inflow outflow boundaries in the 1d domain are given in table 4 as boundary conditions a water discharge of 0 005673 m3 s and water depth of 0 0541 m are prescribed at the inflow and outflow boundaries of the main channel respectively and a water depth of 0 0458 m is specified at the outflow boundary of the branch channel the initial particle spacing is 0 01 m in both the 1d and 2d models and the maximum time step is 0 007 s the depth and streamline contours are plotted in fig 10 which shows that the water depth at the downstream side of the main channel increases and a vortex forms at the left side of the inlet in the branch channel as the flow separates at the diverging junction as shown in fig 11 the simulated water depths were compared with the data collected by shettar and murthy 1996 the variations in water depth along the four walls predicted using the two proposed sph swe models exhibit good agreement with the measured data however the sph based simulation results are less accurate than those predicted by the fem based method because the sph based solutions at the walls are obtained based on an extrapolation from the inner fluid particles fig 12 displays the sph based simulated profiles of water discharge along the main channel in the two sph swe models notably the accuracy and convergence of the 1d 2d coupled model were investigated l2 q in the 1d domain is 1 34e 02 and l2 q in the 2d domain is 2 86e 03 the convergence rates based on water discharges of 1 40 and 1 49 in the 1d and 2d domains respectively are shown in table 5 additionally the cpu times are shown in table 3 the ratio of the cpu time of the 2d model to that of the 1d 2d coupled model is 2 0 thus it is clear that the 1d 2d coupled model is more efficient in shallow water simulations 5 4 curved channel flow simulation a channel with a 180 constant curvature bend is schematically shown in fig 13 for the 2d and 1d 2d coupled models the channel has a width of 0 8 m and was originally designed by rozovskii 1961 table 4 illustrates the locations of the inflow outflow boundaries in the 1d domain this curved channel consists of an inner radius with a curvature of 0 4 m and an outer radius with a curvature of 1 2 m manning s roughness coefficient is adopted as 0 011 s m1 3 in this study the water discharge and water depth are set to 0 0123 m3 s and 0 058 m at the inflow and outflow boundaries respectively the initial particle spacing is equal to 0 02 m in both the 1d and 2d models and the maximum time step is 0 015 s fig 14 gives the simulated contours of water depths and the profiles of water velocity vectors for the two sph swe models in fig 14 c and d the water velocity along the inner wall increases as flow passes through the front part of the bend then the water velocity decreases as flow exits the bend along the outer wall flow deacceleration occurs within the front part of the bend and flow acceleration then occurs as the flow exits the bend hence the water depth along the inner wall is lower than that along the outer wall as schematically illustrated in fig 14 a and b the magnitudes of water velocities along the radial axis at the angles of 0 35 65 100 143 and 186 are compared with the data collected by rozovskii 1961 and the fdm based simulation results of lien et al 1999 as shown in fig 15 the simulated water velocity magnitudes exhibit good agreement with the measured data except the results predicted in the vicinity of the inner wall due to the use of a non slip boundary condition the magnitudes of water velocities in the vicinity of the inner wall computed by the two sph swe models are lower than the measured values fig 16 presents the simulated water discharge for the two sph swe models the degree of satisfaction of the mass conservation constraint in the 1d 2d coupled model can be determined using eq 28 specifically l2 q in the 1d domain is 6 94e 03 and l2 q in the 2d domain is 1 75e 03 as listed in table 5 the convergence rates based on the water discharge in the 1d and 2d domains are 1 07 and 1 15 respectively thus the two sph swe models are both applicable to simulations of curved channel flows however as shown in table 3 the 1d 2d coupled model yields a higher efficiency than the 2d model 6 concluding remarks in this study the proposed 1d and 2d coupled sph swe model was validated based on three subcritical flows in converging diverging and curved channels the simulation results compare well with measured data and mesh based numerical results model convergence based on water discharge was investigated to verify that the proposed sph model yields first order convergent solutions in 1d and 2d domains additionally we confirmed that the proposed model produces results that are quantitively comparable to those based on 2d sph swes but the results of the couple model require a much smaller cpu time furthermore incorporating the continuity equation and an externally added density diffusion term into the model yielded more accurate and efficient predictions of water depth than those based on the sph summation operator the convergence rate of the continuity approach can reach approximately unity conversely the water depth errors close to the wall boundaries result in lower convergence rates for the summation approach in summary the proposed 1d 2d coupled sph swe model is a potential alternative for numerical simulations of open channel flows bounded by irregularly shaped walls acknowledgments we thank the ministry of science and technology of taiwan most 106 2917 i 564 045 for financially supporting this study 
831,in this study a one and two dimensional 1d 2d coupled model is developed to solve the shallow water equations swes the solutions are obtained using a lagrangian meshless method called smoothed particle hydrodynamics sph to simulate shallow water flows in converging diverging and curved channels a buffer zone is introduced to exchange information between the 1d and 2d sph swe models interpolated water discharge values and water surface levels at the internal boundaries are prescribed as the inflow outflow boundary conditions in the two sph swe models in addition instead of using the sph summation operator we directly solve the continuity equation by introducing a diffusive term to suppress oscillations in the predicted water depth the performance of the two approaches in calculating the water depth is comprehensively compared through a case study of a straight channel additionally three benchmark cases involving converging diverging and curved channels are adopted to demonstrate the ability of the proposed 1d and 2d coupled sph swe model through comparisons with measured data and predicted mesh based numerical results the proposed model provides satisfactory accuracy and guaranteed convergence keywords smoothed particle hydrodynamics shallow water equations 1d 2d coupled open channel 1 introduction open channel flow is an essential topic in hydraulics and studies of this topic range from drainage in artificial channels and rivers ying et al 2004 burguete et al 2008 and the design of hydraulic structures such as spillways unami et al 1999 and bridges biglari and sturm 1998 to flood prevention measures hsu et al 2003 numerical simulation approaches are extensively used to study open channel flow by solving either 1d or 2d shallow water equations swes for the discharge wetted cross sectional area flow velocity and depth chaudhry 2008 the swes can be derived from the area integrated and depth integrated navier stokes equations in general 1d swe models are used to model large open channel systems because of their high efficiency in comparison with the 2d swe models for example 1d swe models approximated by the finite difference method fdm choi and molinas 1993 finite element method fem sen and garg 1998 and finite volume method fvm sanders et al 2001 have been successfully applied to model large open channel flows however the existing 1d swe models cannot resolve the 2d flow phenomena that can be captured by 2d swe models e g flows in converging diverging or curved channels various coupled approaches that combine computationally efficient 1d swe models with numerically accurate 2d swe models have been proposed in the past effectively exchanging data between the 1d and 2d models is the key to success in these 1d 2d coupled methods the processes of connecting the 1d and 2d domains in 1d 2d coupled swe models can be categorized into two types based on the overlapping area marin and monnier 2009 fernandez nieto et al 2010 arico et al 2016 or the buffer zone blade et al 2012 morales hernandez et al 2013 morales hernandez et al 2016 in coupled models of the first type the 1d model remains intact and the 2d model is locally employed so called local zoom model marin and monnier 2009 first proposed a superposition approach to convert the contribution of the 2d model into the source terms of the 1d governing equations to predict river overtopping flows in coupled models of the second type the 1d model is decomposed into the 2d model and a buffer zone e g control meshes is established that incorporates the 1d and 2d domains blade et al 2012 proposed a flux based connection that considers the effects of the mass and momentum exchanged between the 1d and 2d domains in natural channels morales hernandez et al 2013 developed an approach that accounts for the interactions at the boundaries based on separate mass conservation and mass and momentum conservation strategies in shallow water flow simulations meshless numerical methods such as the smoothed particle hydrodynamics method sph moving particle semi implicit method mps and reproducing kernel particle method rkpm have attracted increased attention because of their excellent ability to overcome large deformation difficulties that arise from fluid solid interactions sph the most popular of these methods is a lagrangian meshless particle method sph was first proposed by lucy 1977 and gingold and monaghan 1977 in their investigations of astrophysical problems particles in space move along pathlines in a lagrangian sense and sph has an advantage over mesh based methods when interfacial flow dynamics in free surface flows monaghan 1994 fluid structure flows shao and gotoh 2004 and mudflows shao and lo 2003 are considered some significant characteristics of sph are as follows 1 unlike the eulerian conservation equations there are no nonlinear convective terms in the lagrangian equations thereby ensuring that the discretization is galilean invariant 2 the interface between two phases can be naturally captured and 3 the interactions between a fluid and a solid structure can be easily modeled bouscasse et al 2013 gong et al 2016 recently sph has also been utilized to solve swes so called sph swe model in modeling dam break flows wang and shen 1999 rodriguez paz and bonet 2005 ata and soulaimani 2005 chang et al 2011 kao and chang 2012 xia et al 2013 gu et al 2017 open channel flows vacondio et al 2012 chang and chang 2013 chen et al 2015 chang et al 2016 chang et al 2017 and run off flows chang et al 2016 in the sph literature narayanaswamy et al 2010 addressed the advantage of coupling a 1d finite difference boussinesq model with 2d sph navier stokes model in coastal flow simulations altomare et al 2015 presented a hybridization technique to pass information between a 2d wave propagation model and a 3d sph navier stokes model in real coastal applications however no numerical study of coupled sph swe models has been performed therefore we propose a coupled 1d and 2d sph swe model to more efficiently simulate open channel flows with complicated boundaries in our proposed model a buffer zone associated with the inflow outflow boundaries of the 1d and 2d computational domains is defined in the channel interior this paper focuses on subcritical flows because they are the most common in rivers and open channels flows such as those that originate from dam breaking are not easily modeled with this approach based on the characteristics of the proposed method only one variable can be assessed at the inflow outflow boundaries in this study the water discharge and water surface level at the internal boundaries are interpolated to specify the required inflow and outflow boundary conditions respectively for the 1d and 2d models to the best of our knowledge all the sph swe models use an sph summation operator instead of solving the continuity equation to calculate water depth because this approach enhances the numerical stability in this study we add the density diffusion term proposed by molteni and colagrossi 2009 into the continuity equation for stability enhancement a comprehensive comparison concerning the accuracy of the water depth and discharge and the computational efficiency of two approaches applied to calculate the variable water depth is performed based on test cases this paper is organized as follows in section 2 the 1d swes for modeling the wetted cross sectional area and water discharge are presented additionally the 2d swes for the water depth and velocity are introduced section 3 gives the fundamental sph operators and details of how the proposed sph swe model can be implemented are presented section 4 is devoted to the treatment of the three boundaries the wall inflow outflow and internal boundaries between the 1d and 2d models finally in section 5 four flow cases are considered in straight converging diverging and curved channels each case is solved to verify and validate the proposed model based on the exact solution the mesh based numerical results and measured data 2 governing equations 2 1 one dimensional shallow water equations the 1d swes can be derived from the area integration of the navier stokes equations chaudhry 2008 and govern the wetted cross sectional area and water discharge in open channel flows the lagrangian form of the 1d swes can be written as follows 1 d a d t a u x 2 d q d t q u x g a d w z b x g a s f in the above equations d d t denotes the total derivative term d d t t u u u v is the water velocity vector where u and v are the x and y components of water velocity respectively q denotes the water discharge au a is the wetted cross sectional area dw is the water depth zb is the bed elevation sf is the friction slope n m a 2 q 2 a 2 r 4 3 nma is the manning roughness coefficient r is the hydraulic radius and g is gravitational acceleration 2 2 two dimensional shallow water equations by vertically the direction of water depth integrating the navier stokes equations the 2d swes for the water depth and velocity derived in chaudhry 2008 can be given in lagrangian form as follows 3 d d w d t d w u 4 d u d t g d w z b g s f where the friction slope s f used in the 2d model is defined as n m a 2 u u d w 4 3 3 sph swe model 3 1 sph operators any physical quantity of particle a such as ϕ a can be approximated as follows within the sph context 5 ϕ r a ϕ r ω r a r h d v b 1 b n m b ϕ b ρ b ω r a b h a in the above equation mb ρ b vb is the mass of particle b ρ b is the density of particle b which is defined as ρ0 ab in the 1d model and ρ0 d w b in the 2d model ρ0 is the bulk density of water 1000 kg m3 vb is the volume of particle b r is the position vector r r x in the 1d model and r r x y in the 2d model r a b r a r b is the distance between particles a and b and ω r a b h a is the kernel function of particle a and is denoted as ω a b a in this study the smoothing length of particle a which is denoted as ha is assigned an initial value of 1 4l 0 l 0 δx 0 in the 1d model and l 0 δx 0 δy 0 in the 2d model where δx 0 and δy 0 are the initial particle spacings in the x and y directions respectively and n is the number of particles in the support domain of particle a violeau 2012 in this study the sph gradient operator is expressed in symmetric form and the sph divergent operator is expressed in asymmetric form violeau 2012 these sph differential operators are shown in eqs 6 and 7 6 ϕ a ρ a b 1 b n m b ϕ a ρ a ϕ b ρ b a ω a b a 7 ϕ a 1 ρ a b 1 b n m b ϕ a ϕ b a ω a b a where a ω a b a denotes ω a b a x a e x ω a b a y a e y and e x and e y are the unit vectors in the directions of the x and y axes respectively in addition the following sph laplacian operator is adopted to consider the viscous acceleration proposed by monaghan monaghan 1992 8 μ u a 2 d m 2 b 1 b n v b μ a b u a b r a b r a b 2 a ω a b a where μ is the kinematic viscosity u a b u a u b and μ a b 0 5 μ a μ b 3 2 discretized shallow water equations based on the chosen sph gradient and the divergent operators shown in eqs 6 and 7 the source terms in eqs 1 to 4 can be approximated as follows 9 d a d t a b 1 b n m b u a b x ω a b 10 dq dt a u a b 1 b n m b u a u b ω ab x g a a 2 b 1 b n m b η w a a a 2 η w b a b 2 ω ab x g a a s f a 11 d d w d t a b 1 b n m b u a b ω a b 12 d u dt a g d w a b 1 b n m b η w a d w a 2 η w b d w b 2 ω ab g s f a in the above equations η w is the water surface level dw zb uab ua ub and the wendland function is used as the kernel function violeau 2012 in our discretized swes the averaged kernel function is considered due to the non constant smoothing length i e ω ab x 0 5 ω ab a x a ω ba b x b and ω ab 0 5 a ω ab a b ω ba b hernquist and katz 1989 given that the mass in the compact domain of a fluid particle is constant i e ρ h d m constant the temporal variation of the smoothing length can be derived as dh dt h dm ρ dρ dt where dm denotes the number of dimensions in the domain altomare et al 2015 furthermore the corrected divergence operator of velocity i e x in eq 9 and in eq 11 as proposed by bonet and lok 1999 is used to achieve first order consistency as shown in eqs 13 and 14 13 ω a b a x a ω a b a x a b 1 b n v b x b a ω a b a x a 14 a ω a b a l a 1 a ω a b a where l a b 1 b n v b x b a ω a b a x a b 1 b n v b y b a ω a b a x a b 1 b n v b x b a ω a b a y a b 1 b n v b y b a ω a b a y a x b a x b x a and y b a y b y a 3 3 stabilization terms 3 3 1 artificial viscosity in the 1d sph swe model an artificial viscous force proposed by monaghan 1992 1994 as expressed in eq 15 is applied to prevent the occurrence of a disordered particle distribution 15 f a art b 1 b n a a b v b ν a b art u a b x a b x a b 2 ɛ 2 ω a b x where ν a b art α h a b c a b h a b 0 5 h a h b c a b 0 5 c a c b a a b 0 5 a a a b ε 10 10 and α is a parameter employed to control the artificial viscous effect 0 3 in this study 3 3 2 eddy viscosity as 2d channel flows become turbulent eddy viscosity effects can no longer be negligibly small particularly in cases with solid boundaries nadaoka and yagi 1998 noted that the turbulence structure of a shallow water flow can be divided into bed friction generated 3d turbulence at length scales less than the water depth and horizontal 2d eddies at much larger length scales bed friction plays a key role in transmitting the dissipated energy from horizontal 2d eddies to 3d turbulent processes therefore the les shallow water model is ideal for describing the development of horizontal 2d large scale eddies hence the sph les model proposed by shao and gotoh 2004 who introduced an les model for an sph solver is adopted to model the eddy viscosity originating from subparticle scale eddies that act on fluid particles as given in eq 16 16 f a e d d y 4 b 1 b n v b d w a ν t a d w b ν t b d w a u a b r a b r a b 2 ɛ 2 ω a b in the above equation ν t is the turbulent kinematic viscosity calculated by c s l 0 2 2 s s where s is the local strain tensor and cs is the smagorinsky constant 0 5 in this study a spatial filter with a width determined by csl 0 is implicitly used in the les model as a result only the solutions of particle scale variables are obtained 3 3 3 density diffusion since the continuity equation is independently solved for each fluid particle oscillatory solutions are frequently found in the density field here based on molteni and colagrossi 2009 the density diffusion terms shown in eqs 17 and 18 are incorporated into the 1d and 2d sph swe models respectively with the aim of suppressing oscillations in the density field 17 s a 1 d b 1 b n d a b v b a a a b x a b x a b 2 ɛ 2 ω a b x 18 s a 2 d b 1 b n d a b v b d w a d w b r a b ω a b r a b 2 ɛ 2 where d a b β h a b c a b and β 0 2 in this study 3 4 time integration method a modified verlet scheme proposed by molteni and colagrossi 2009 is adopted in this study this approach enables the use of large courant friedrichs lewy cfl numbers in the process of updating the solutions at the next time step table 1 shows the algorithms of the modified verlet scheme for the 1d and 2d models the time step δt that satisfies the cfl stability condition is as follows 19 δ t c f l min a l 0 u a c a in this study we set cfl to 0 8 4 boundary treatment 4 1 wall boundary in the 2d model the ghost particle technique developed by colagrossi and landrini 2003 is adopted to specify the wall boundary condition in this study four layers of ghost particles are involved in the computations of the water depth pressure force and viscous force of each fluid particle as schematically shown in fig 1 these ghost particles are associated the velocity divergence operator the gradient operator of water depth and the velocity laplacian operator for the ghost particles influenced by the velocity divergence operator both the normal and tangent components of their velocities are equal to zero due to the stationary non slip walls bouscasse et al 2013 to estimate the viscous interaction between the fluid and ghost particles mirror points such as point c in fig 1 are generated using a mirroring technique along the plane of symmetry following the work of bouscasse et al 2013 in the presence of free slip boundary conditions the normal and tangential components of the velocity of each ghost particle are identical to those at its mirror point however in the case of non slip boundary conditions the tangential velocity of each ghost particle is specified with the opposing sign as that at the mirror point and the normal velocity of each ghost particle remains the same as that at the mirror point to calculate the gradient operator of water depth for ghost particles the impermeable boundary condition shown in eq 20 is applied bouscasse et al 2013 20 d u d t w n g d w z b g s f w n 0 rearranging eq 20 leads to the following formula 21 d w n w z b s f w n taking the water depth bed elevation and water velocity at mirror point c into consideration the water depth of ghost particle g can be determined as follows 22 d w g b 1 b f b n v b d w b l g c z b b s f b n g c ω c b c in the above equation n g c denotes the unit normal vector r c r g r c r g l g c r c r g additionally ω cb c is the corrected kernel function which can be represented as follows randles and libersky 1996 23 ω c b c ω c b c b 1 b f b n v b ω c b c where hc 1 4l 0 4 2 inflow outflow boundary in this study three types of fluid particles namely inflow particles rectangular points inner particles circle points and outflow particles triangular points are used to resolve the problems related to the inflow outflow boundaries as schematically shown in fig 2 since sph is an explicit method the inflow outflow boundary conditions must be determined prior to solving the governing equations at each time step therefore we adopt the specified time interval method chaudhry 2008 to calculate unknown variables at the inflow outflow boundaries fig 2 a depicts the directions of the characteristic lines in the presence of a subcritical flow at the inflow boundary the water discharge is prescribed and the water depth is obtained by solving eq 24 along the negative characteristic line lp schematically shown in fig 2 a via newton raphson iterations 24 q p a p u p a p u l g c l d w p d w l g z b x l s f l δ t for a subcritical flow specified at the outflow boundary the water depth is prescribed and the water discharge is determined from eq 25 along the positive characteristic line rs shown in fig 2 a 25 q s a s u s a s u r g c r d w s d w r g z b x r s f r δ t one can refer to vacondio et al 2012 chang and chang 2013 federico et al 2012 for additional details of this method in the treatment of 2d inflow outflow boundary conditions the 1d specified time interval method is applied in this case no characteristic lines are used when a subcritical flow occurs at the inflow boundary the projection point a of particle i on the inflow boundary as shown in fig 2b is identified then the water depth at point a obtained by solving eq 24 is specified as the water depth of inflow particle i similarly if a subcritical flow is considered at the outflow boundary the water discharge of outflow particle o is equal to the water discharge at projection point d on the outflow boundary obtained using eq 25 as shown in fig 2 b in addition virtual bed particles are introduced to describe the bed elevation and the roughness vacondio et al 2012 the volume of each virtual bed particle equals l 0 and the bed elevation and the roughness of each fluid particle a are computed from eq 26 26 z b a b 1 b v b b n v b z b b ω a b b n m a a b 1 b v b b n v b n m a b ω a b b in the above equation the subscript vb denotes the virtual bed particle and h b vb 1 4l 0 4 3 internal boundary between the 1d and 2d models in the proposed coupled model we use the concept introduced by narayanaswamy et al 2010 to choose a buffer zone that connects the 1d domain and the 2d domain there are two types of connections the first type includes a 1d model at the upstream side and a 2d model at the downstream side and the second includes a 2d model at the upstream side and a 1d model at the downstream side as shown in fig 3 for the first type of connection the locations of section e and section a b in the buffer zone are the same as the inflow boundary of the 2d model and the outflow boundary of the 1d model similarly for the second type of connection the locations of section cd and section f in the buffer zone are the same as the inflow boundary of the 1d model and the outflow boundary of the 2d model here we call the inflow outflow boundaries of the buffer zone the internal boundaries by exchanging the dependent variables in the 1d and 2d models at the internal boundaries one can obtain the required inflow outflow boundary conditions and perform computations at the next time step as subcritical flows form at the internal boundaries the model at the upstream side provides the inflow boundary condition of water discharge for the model at the downstream side i e q a b q e for the first connection and q f q c d for the second connection where q e b 1 b f l a e b n v b q b ω a b a and q c d l c d b 1 b f l a c d b n v b u b n c d ω a b a d l similarly the model at the downstream side provides the outflow boundary condition of the water surface level for the model at the upstream side i e η w e η w a b for the first connection and η w c d η w f for the second connection where η w a b 1 l l a b b 1 b f l a a b b n v b η w b ω a b a d l and η w f b 1 b fl a f b n v b η w b ω ab a in the above equations the subscript fl stands for fluid particles and the smoothing length of interpolation point a is 1 4l 0 in all interpolations thus we can obtain one boundary condition at the inflow outflow boundaries of the 1d and 2d models and solve the characteristic equations shown in section 4 2 to determine another required inflow outflow boundary condition in the current study the width of the buffer zone is chosen as 2 h where h is equal to 2l 0 this value ensures that the sph summation includes the complete support domain at the boundaries of the buffer zone moreover the convergence of the boundary conditions in the buffer zone will be guaranteed as the initial particle spacing approaches zero the buffer zone will shrink to a point thus all boundary conditions at the internal boundary are identical for the 1d and 2d models 5 validation in this section a straight channel is chosen in our simulation to assess the performance of the two approaches to calculate the water depth the first approach uses the sph summation operator 27 d w a b 1 b n m b ω r a b h a since the non constant smoothing length is calculated by h a h 0 a d w 0 a d w a 1 2 where dw 0 and h 0 are the water depth and smoothing length at the beginning of the simulation respectively eq 27 becomes nonlinear a newton raphson iteration is applied to solve eq 25 rodriguez paz and bonet 2005 vacondio et al 2012 the second approach algebraically solves the sph discretized continuity equation given in eq 11 hereafter we call the first and second approaches the summation and continuity approaches respectively next three cases involving converging diverging and curved channels are investigated to assess the performance of the proposed 1d and 2d coupled sph swe model all the channels are rectangular horizontal and frictional numerical simulations were performed on an intel r core tm i7 2600 cpu 3 4 ghz pc equipped with 4gb of ram in addition a non dimensional l2 norm for any physical quantity ϕ is applied to calculate the convergence rate 28 l 2 ϕ 1 n i 1 n ϕ sph ϕ ext ϕ ext 2 in the above equation n is the number of particles at fixed points in the domain and the superscripts sph and ext denote the numerical and exact solutions respectively 5 1 straight channel flow simulation the first simulation involves a straight channel of length 1000 m and width 400 m this channel was considered by vacondio et al 2012 in their simulation using an sph summation operator to calculate the water depth the manning roughness coefficient is 0 0308 s m1 3 the exact solutions of the unit water discharge 15 m2 s and water depth 5 m are prescribed at the inflow and outflow boundaries respectively three different initial particle spacings of 80 m 40 m and 20 m are used to analyze the accuracy and convergence the non dimensional l2 norms based on the water depth and the velocity are calculated as follows 29 l 2 d w 1 n i 1 n d w i sph d w ext d w ext 2 l 2 u 1 n i 1 n u i sph u ext g d w ext 2 l 2 v 1 n i 1 n v i sph v ext g d w ext 2 in the above dw is the water depth d w ext 5 m u is the x component velocity u ext 3 m s v is the y component velocity v ext 0 m s and n is the number of particles in the domain the accuracy and the convergence of the two approaches to calculate the water depth are given in tables 2a and 2b the continuity approach is more accurate than the summation approach in calculating the velocity and the water depth fig 4 presents the simulated water discharge profiles of the two approaches for the case of an initial particle spacing of 20 m as shown in fig 4 the continuity approach exhibits better mass conservation than the summation approach l2 q of the summation approach is 1 02e 04 and l2 q of the continuity approach is 3 81e 05 in addition the continuity approach achieves a faster convergence speed the convergence rates based on the water depth and the velocity approach unity for the summation approach the water depth errors mainly occur in the vicinity of the wall boundaries and are not rapidly reduced by decreasing the initial particle spacing this issue is reflected by the low convergence rates in table 2a table 3 lists the cpu times for each approach involving 1159 particles and the total simulation time is 6000 s the summation approach takes 1 1 times the cpu time consumed by the continuity approach we believe that the approach of calculating water depth via solving the continuity equations can efficiently yield a higher accuracy than the summation approach for the water depth and yields a higher degree of mass conservation therefore in the following case studies only the continuity approach is used in the two sph swe models 5 2 converging channel flow simulation a converging channel with a junction angle of 90 was experimentally studied by weber et al 2001 and is adopted in this study for comparison the converging channels in the 2d and 1d 2d coupled models are schematically shown in fig 5 a and b where the solid lines denote the 2d domain and the dash lines denote the 1d domain table 4 shows the locations of the inflow outflow boundaries in the 1d domain the channel width is 0 914 m and the manning s roughness coefficient of 0 011 s m1 3 accounts for bed friction a water discharge of 0 043 m3 s and water depth d w 0 of 0 296 m are prescribed as the inflow and outflow boundary conditions in the main channel respectively and a water discharge of 0 127 m3 s is prescribed as the inflow boundary condition in the branch channel the outflow discharge in the main channel is therefore equal to 0 17 m3 s the initial particle spacing is set to 0 02285 m in the 1d and 2d models and the maximum time step is 0 007 s the depth contours and the streamline configurations for the two sph swe models are plotted in fig 6 the water depths gradually decrease downstream of the channel convergence section and the flow convergence results in the formation of a vortex fig 7 shows the simulated profiles of water depths at three lateral distances along the main channel the results obtained by the investigated sph swe models are all consistent with the measured data presented by weber et al 2001 and the fem based simulation results of song et al 2012 a comparison of the simulated profiles of water discharge along the main channel based on the two sph swe models is presented in fig 8 when the flow from the upstream side of the main channel converges at the junction with the flow from the branch channel the discharge increases to 0 17 m3 s in the 1d 2d coupled model l2 q in the 1d domain is 1 05e 02 and l2 q in the 2d domain is 7 58e 03 as listed in table 5 the convergence rates based on the water discharge in the 1d and 2d domains are 0 99 and 0 88 respectively in addition table 3 shows that the 2d model takes 2 1 times the cpu time required by the 1d 2d coupled model both the 1d 2d coupled model and 2d model results can be efficiently obtained for the converging channel 5 3 diverging channel flow simulation in the third simulation case a diverging channel with a junction angle of 90 that was experimentally studied by shettar and murthy 1996 is investigated this channel has a width of 0 3 m and the channel bottom is rough with a manning s roughness coefficient of 0 011 s m1 3 fig 9 shows the schematic of the diverging channel investigated using the 2d and 1d 2d coupled models the locations of the inflow outflow boundaries in the 1d domain are given in table 4 as boundary conditions a water discharge of 0 005673 m3 s and water depth of 0 0541 m are prescribed at the inflow and outflow boundaries of the main channel respectively and a water depth of 0 0458 m is specified at the outflow boundary of the branch channel the initial particle spacing is 0 01 m in both the 1d and 2d models and the maximum time step is 0 007 s the depth and streamline contours are plotted in fig 10 which shows that the water depth at the downstream side of the main channel increases and a vortex forms at the left side of the inlet in the branch channel as the flow separates at the diverging junction as shown in fig 11 the simulated water depths were compared with the data collected by shettar and murthy 1996 the variations in water depth along the four walls predicted using the two proposed sph swe models exhibit good agreement with the measured data however the sph based simulation results are less accurate than those predicted by the fem based method because the sph based solutions at the walls are obtained based on an extrapolation from the inner fluid particles fig 12 displays the sph based simulated profiles of water discharge along the main channel in the two sph swe models notably the accuracy and convergence of the 1d 2d coupled model were investigated l2 q in the 1d domain is 1 34e 02 and l2 q in the 2d domain is 2 86e 03 the convergence rates based on water discharges of 1 40 and 1 49 in the 1d and 2d domains respectively are shown in table 5 additionally the cpu times are shown in table 3 the ratio of the cpu time of the 2d model to that of the 1d 2d coupled model is 2 0 thus it is clear that the 1d 2d coupled model is more efficient in shallow water simulations 5 4 curved channel flow simulation a channel with a 180 constant curvature bend is schematically shown in fig 13 for the 2d and 1d 2d coupled models the channel has a width of 0 8 m and was originally designed by rozovskii 1961 table 4 illustrates the locations of the inflow outflow boundaries in the 1d domain this curved channel consists of an inner radius with a curvature of 0 4 m and an outer radius with a curvature of 1 2 m manning s roughness coefficient is adopted as 0 011 s m1 3 in this study the water discharge and water depth are set to 0 0123 m3 s and 0 058 m at the inflow and outflow boundaries respectively the initial particle spacing is equal to 0 02 m in both the 1d and 2d models and the maximum time step is 0 015 s fig 14 gives the simulated contours of water depths and the profiles of water velocity vectors for the two sph swe models in fig 14 c and d the water velocity along the inner wall increases as flow passes through the front part of the bend then the water velocity decreases as flow exits the bend along the outer wall flow deacceleration occurs within the front part of the bend and flow acceleration then occurs as the flow exits the bend hence the water depth along the inner wall is lower than that along the outer wall as schematically illustrated in fig 14 a and b the magnitudes of water velocities along the radial axis at the angles of 0 35 65 100 143 and 186 are compared with the data collected by rozovskii 1961 and the fdm based simulation results of lien et al 1999 as shown in fig 15 the simulated water velocity magnitudes exhibit good agreement with the measured data except the results predicted in the vicinity of the inner wall due to the use of a non slip boundary condition the magnitudes of water velocities in the vicinity of the inner wall computed by the two sph swe models are lower than the measured values fig 16 presents the simulated water discharge for the two sph swe models the degree of satisfaction of the mass conservation constraint in the 1d 2d coupled model can be determined using eq 28 specifically l2 q in the 1d domain is 6 94e 03 and l2 q in the 2d domain is 1 75e 03 as listed in table 5 the convergence rates based on the water discharge in the 1d and 2d domains are 1 07 and 1 15 respectively thus the two sph swe models are both applicable to simulations of curved channel flows however as shown in table 3 the 1d 2d coupled model yields a higher efficiency than the 2d model 6 concluding remarks in this study the proposed 1d and 2d coupled sph swe model was validated based on three subcritical flows in converging diverging and curved channels the simulation results compare well with measured data and mesh based numerical results model convergence based on water discharge was investigated to verify that the proposed sph model yields first order convergent solutions in 1d and 2d domains additionally we confirmed that the proposed model produces results that are quantitively comparable to those based on 2d sph swes but the results of the couple model require a much smaller cpu time furthermore incorporating the continuity equation and an externally added density diffusion term into the model yielded more accurate and efficient predictions of water depth than those based on the sph summation operator the convergence rate of the continuity approach can reach approximately unity conversely the water depth errors close to the wall boundaries result in lower convergence rates for the summation approach in summary the proposed 1d 2d coupled sph swe model is a potential alternative for numerical simulations of open channel flows bounded by irregularly shaped walls acknowledgments we thank the ministry of science and technology of taiwan most 106 2917 i 564 045 for financially supporting this study 
832,in this paper a new analytical solution for interpreting dipole tests in heterogeneous media is derived by associating the shape of the tracer breakthrough curve with the log conductivity variance it is presented how the solution can be used for interpretation of dipole field test in view of geostatistical aquifer characterization on three illustrative examples the analytical solution for the tracer breakthrough curve at the pumping well in a dipole tracer test is developed by considering a perfectly stratified formation the analysis is carried out making use of the travel time of a generic solute particle from the injection to the pumping well injection conditions are adapted to different possible field setting solutions are presented for resident and flux proportional injection mode as well as for an instantaneous pulse of solute and continuous solute injections the analytical form of the solution allows a detailed investigation on the impact of heterogeneity the tracer input conditions and ergodicity conditions at the well the impact of heterogeneity manifests in a significant spreading of solute particles that increases the natural tendency to spreading induced by the dipole setup furthermore with increasing heterogeneity the number of layers needed to reach ergodic conditions become larger thus dipole test in highly heterogeneous aquifers might take place under non ergodic conditions giving that the log conductivity variance is underestimated the method is a promising geostatistical analyzing tool being the first analytical solution for dipole tracer test analysis taking heterogeneity of hydraulic conductivity into account 1 introduction groundwater is an important natural resource for drinking water supply hence the investigation and predictive modeling of flow and transport in porous media are of broad relevance in particular for water quality aspects and methods for contaminated site treatment like remediation risk assessment and natural attenuation tracer tests are thereby the foremost used observation method for inferring hydrogeological structural and transport parameters of the subsurface most important the hydraulic conductivity k which determines the velocity of groundwater flow typically exhibits a large spatial heterogeneity with values varying over orders of magnitudes gelhar 1993 a bunch of different tracer test types exist which all have their merits but also limitations tracer tests under ambient flow conditions are either limited to short travel distances because groundwater flow is usually very slow or the test operation suffers high costs due to a long test duration and the need for a large observation network more efficient in time and size of observation network are tracer tests under forced flow conditions due to higher velocities and directed flow however the analysis of test types under well flow conditions are more complex due to the non uniform flow field especially taking aquifer heterogeneity into account requires sophisticated analyzing methods in dipole tracer tests also named two well test or doublet tests a tracer is introduced at a recharge well and the breakthrough curve btc is measured at a pumping well the pumped water can optionally be used for recharge in a recirculation the test setting has the advantage of circumventing the problem of removal of waste water and the need for an additional water source however the complex flow pattern causes the interpretation of the transport behavior to be more complicated the dipole shape of the flow field gives that the observed concentration at the pumping well to be a superposition of tracer transport along different streamlines with different tracer arrival times the foremost aim in this work is to present an alternative interpretation method for dipole tests in heterogeneous media it will be given a simple solution for interpretation of dipole test in view of geostatistical aquifer characterization the first analytical analysis of dipole test was performed by hoopes and harleman 1967 they presented a mathematical description of the flow field provided equations for streamlines and the travel time of a tracer particle along a streamline they gave analytical expressions for the temporal and spatial distribution of tracer in a homogeneous aquifer by analyzing an approximate solutions for the concentration in the presence of convection dispersion and diffusion hoopes and harleman 1967 analyzed the impact of these processes they found that dispersion impacts only the very early part of the btc afterwards the arrival of different streamlines is dominant for constant tracer injection the btc is almost insensitive to dispersion shortly after grove and beetem 1971 presented a method for analyzing btc of dipole tests in homogeneous aquifers in order to evaluate the porosity and the longitudinal dispersivity the method focuses on tests with pulse tracer injection and recirculation being constructed as superposition of 1d solutions for individual streamlines based on calculations of the streamline length and particle travel time the approach took the crucial assumption that velocity is constant along flow lines the work of grove and beetem 1971 also included an illustrative example of an analysis of a dipole test in the fractured carbonate aquifer near carlsbad new mexico gelhar 1982 derived a semi analytical solution and provided type curves for the btc measured at the pumping well in a dipole flow system the type curves where numerically derived based on the theoretical work of gelhar and collins 1971 about longitudinal dispersion along streamlines in nonuniform flow for the solution transverse dispersion is assumed to be negligible the work of gelhar 1982 included the analysis of a dipole test at the hanford site for illustrating the method welty and gelhar 1989 reanalyzed several field tests using the method of gelhar 1982 in order to estimate dispersivity the first approach to numerically interpret dipole tests was presented by huyakorn et al 1986a they developed a problem adapted simulation software making use of a curvilinear fem the method was applied to the dipole field test at the chalk river site pickens and grisak 1981 more recently bianchi et al 2011 presented a numerical model to interpret the dipole test at the made site with explicitly including aquifer heterogeneity the authors performed simulations with conditioned log normal hydraulic conductivity fields several examples for dipole field tests in consolidated media ca be found in literature mostly as illustrative examples along analytical method development e g in the previously mentioned papers of grove and beetem 1971 gelhar 1982 welty and gelhar 1989 some early works on tracer tests report large distance dipole tests examples are the test at the savannah river plant webster et al 1970 where wells are 538 m 1765 feet apart with a duration of 2 years or the test at amargosa claasen and cordes 1975 over a distance of 122 m 400 feet there are also several examples for tests in consolidated media mostly over shorter ranges with well distances of maximally a few tenth of meters for instance at tucson arizona wilson 1971 at the chalk river site canada pickens and grisak 1981 at the kesterson aquifer california hyndman and gorelick 1996 and at the rocky mountain arsenal colorado thorbjarnarson and mackay 1997 tests of particular interest within this work are the dipole tests at made bianchi et al 2011 at barstow robson 1974 and at mobile molz et al 1986 they will act as illustrative examples for the method developed herein and will be described in detail later dipole tests in the presence of significant spatial heterogeneity of hydraulic conductivity have rarely been studied though the actual streamline structure of dipole tests is strongly impacted by aquifer heterogeneity the analysis of dipole tests with an explicit representation of aquifer heterogeneity has been done only using numerical models e g bianchi et al 2011 analytical models for dipole analysis including heterogeneity parameters are not available the methods of hoopes and harleman 1967 grove and beetem 1971 gelhar 1982 welty and gelhar 1989 are conceptualized for homogeneous conductivity taking the effect of aquifer heterogeneity only implicitly onto account by the lumped parameter of macrodispersivity in this paper we present an alternative concept for dipole test analysis by taking heterogeneity of hydraulic conductivity explicitly into account a geostatistical approach is used due to the limited data availability in subsurface hydrology in combination with high uncertainty in values the characteristics of hydraulic conductivity k are captured by the one point statistical parameters of mean conductivity k g and the log conductivity variance σ y 2 with y ln k the log conductivity an analytical solution for the btc at the pumping well in a dipole tracer test is developed by considering a stratified heterogeneous hydraulic conductivity structure thus associating the shape of the btc with statistical properties of the conductivity field the stochastic framework and part of the analysis have some similarities with the model presented by pedretti and fiori 2013 for convergent flow the sensitivity of dipole tests to spatial heterogeneity might be used to determine statistical parameters especially the log conductivity variance σ y 2 offering an alternative test method for geostatistical aquifer analysis the plan of the paper is given as following the mathematical framework sets the background of the method providing the derivation of the analytical solutions for different test configurations it is followed by an illustration of results and then a discussion on the impact of parameters and test conditions on the btc the method is then used for conductivity characterization on illustrative examples with a re interpretation of three dipole field tests with the newly developed method the paper ends with a summary and conclusions 2 mathematical framework we consider a confined aquifer of thickness l the heterogeneous hydraulic conductivity k field is modeled by considering the aquifer as a perfectly stratified formation i e made up from n layers of vertical thickness 2i with i the vertical integral scale of hydraulic conductivity each layer is of random and independent conductivity ki i 1 n the justification of the stratified model is in the relative short distance between pumping and injecting wells in dipole tests which is often found in recent applications of the order of the horizontal integral scale of k or even less the perfectly stratified formation has been often used in the past for modeling flow and transport in heterogeneous porous formations often leading to useful analytical solutions dagan 1990 matheron and de marsily 1980 mercado 1967 a dipole is created by injecting and extracting a discharge q in two fully penetrating wells at relative distance 2a adopting head boundary conditions in the wells the piezometric head does not depend on the vertical coordinate thus the distribution of head is the same for all layers a sketch of the conceptual model is provided in fig 1 the discharge per unit thickness qi for each layer i is equal to 1 q i k i k q with k n 1 i 1 n k i the arithmetic mean of ki and q q l the aquifer discharge per unit depth at a given initial time a pulse of solute of initial concentration c 0 and duration δ is introduced in the injection well the solute travels in the porous medium and it is collected downstream in the pumping well resulting in a breakthrough curve btc at the same well the btc which corresponds to the temporal behavior of solute flux at the pumping well depends on both the dipole setup e g the distance 2a the discharge q the duration δ etc and the medium configuration i e the vertical distribution of hydraulic conductivities ki and their porosity which in the following is assumed as constant in the entire domain scope of the present analysis is to calculate the btc in dependence on both the dipole setting and the aquifer configuration the analysis is carried out by considering the travel time of a generic solute particle from the injection to the pumping well it is well known that the probability density function pdf of such travel time is identical to the btc of a solute instantaneous pulse in the following we focus on advection only which is the most significant source of spreading due to the non uniform flow configuration local dispersion mechanisms like hydrodynamic dispersion or molecular diffusion are neglected the present solution is based on the analysis of hoopes and harleman 1967 they analyzed the travel time t of a particle from the injection to the extraction well pertaining to the generic streamline departing from the injection well at an angle θ with respect to the line joining the wells see fig 1 they derived an analytical solution for travel time in homogeneous formations under the assumption that the aquifer is indefinite homogeneous isotropic and confined between two horizontal planes in absence of natural flow the flow and piezometric head were obtained from the superposition of the flow fields of a line source and a line sink assuming negligible well radii the flow fields are obtained by the solution of water continuity equations and darcy s law the travel time t was obtained by integration of the flow field along the streamlines originating form the injection well the resulting formula for the travel time in an arbitrary layer i is 2 t 4 π n a 2 q i sin 2 θ 1 θ cot θ π θ π where n is the constant porosity qi is the layer s unit discharge θ is the angle and 2a is the distance between injection and pumping well hereinafter we work with the dimensionless travel time τ defined as 3 τ q t 4 π n a 2 g θ k k i with 4 g θ 1 sin 2 θ 1 θ cot θ the form of the dimensionless travel time results from the definition τ v c t s c with v c q n being the characteristic velocity and s c 4 π a being a characteristic length scale τ is symmetrical with respect to θ 0 and the half space θ 0 π can be safely considered in the analysis the travel time 3 is a random variable that depends on the hydraulic conductivity ki of each layer and the angle of attack θ which is uniformly distributed in the interval 0 π from 3 one can calculate the travel time distribution for the entire aquifer formation considering the dependence of τ on the random variables k and θ whose distributions are fk k and f θ 1 π respectively from basic statistics see e g papoulis 1991 the cumulative density function cdf pτ of τ follows as 5 p τ τ d f k k f θ θ d k d θ 1 π 0 π g θ k τ f k k d k d θ where d is the region in the θ k space such that g θ k k τ assuming ergodicity gives k k a with ka being the arithmetic ensemble mean the integration of 5 over k yields 6 p τ τ 1 π 0 π 1 p k g θ k a τ d θ with pk being the cdf of k from the above expression the travel time pdf is calculated as 7 f τ r τ 1 π k a τ 2 0 π g θ f k g θ k a τ d θ resident injection mode if we assume a log normal distribution for k the above specializes as follows 8 f τ r τ 1 π τ 2 π σ y 2 0 π exp σ y 2 2 ln g θ ln τ 2 2 σ y 2 d θ expression 7 is the pdf of the dimensionless travel time τ in its calculation we have assumed that the mass of solute entering in each layer from the injection well is constant for all layers i e the injection condition is of resident concentration instead the typical injection condition in applications is of flux proportional i e the mass of solute entering each layer is proportional to the local velocity at the injection well which is variable in the vertical and is proportional to the hydraulic conductivity ki of each layer the different injection conditions and their impact on the btc is deeply discussed in jankovic and fiori 2010 for transport in mean uniform flow and in pedretti and fiori 2013 for transport in convergent flow the travel time pdf for flux proportional injection mode along the above lines can be calculated by weighting each solute particle by the layer conductivity ki this way the resulting travel time pdf is obtained by averaging the pdf 7 by the weight 9 k i k a g θ τ obtaining 10 f τ f τ 1 π k a τ 3 0 π g 2 θ f k g θ k a τ d θ flux proportional injection mode assuming a log normal distribution for k the latter becomes 11 f τ f τ 1 π τ 2 2 π σ y 2 0 π g θ exp σ y 2 2 ln g θ ln τ 2 2 σ y 2 d θ summarizing expression 10 represents the travel time pdf under flux proportional injection mode that is the one to be used in applications the above solutions correspond to the btc for an instantaneous pulse of solute the extensions for continuous solute injections of initial concentration c 0 and duration δ which are typically employed in the tracer tests are calculated by the convolution 12 c τ 0 t c 0 t f τ τ t d t where 13 t τ when τ δ δ when τ δ δ q δ 4 π n a 2 in the simple but relevant case when c 0 is constant and k is log normally distributed i e when fτ is equal to 11 12 becomes 14 c τ c 0 φ τ when τ δ φ τ φ τ δ when τ δ with 15 φ τ 1 2 π 0 π 1 erf σ y 2 2 ln g θ ln τ 2 σ y 2 d θ formula 14 is the final result of the present analysis and provides the solute btc for the dipole configuration examined here the formula can be conveniently applied to the interpretation of dipole tests we remind that the above solutions consider that the vertical distribution of conductivity is fully sampled i e ergodicity is assumed such condition is typically met when l i 1 the issue shall be further discussed in the next sections 3 illustration of results in this section we illustrate the main results related to the proposed analytical solutions for the dipole tracer test the pdf of the dimensionless travel time τ q t 4 π n a 2 for the flux proportional injection condition and a log normal distribution of k is given by 11 the latter depends only on the log conductivity variance σ y 2 which represents the degree of heterogeneity of the aquifer system figs 2 and 3 display the travel time pdf p τ f and cdf f τ f for a few values of σ y 2 respectively starting from an almost homogeneous formation σ y 2 0 1 it is seen that the distribution of τ is characterized by a rising limb which is determined by the first fast arrivals of solute moving along the most connected paths between the injection and pumping wells small angle θ in turn the tail of the distribution is typically long and persistent being determined by the slow arrivals of solute particles that move along the longer and slower path lines large θ thus the dipole setup always determines a wide variety of paths in the medium causing a similar variability of arrival times and hence dispersion this is a well known feature which has already been described in past work grove and beetem 1971 hoopes and harleman 1967 koplik 2001 the spatial distribution of conductivity present in the aquifer system further enhances the above dispersion of solute as visible in the curves of figs 2 and 3 for σ y 2 0 it is seen that for increasing degree of heterogeneity σ y 2 the travel time distributions depart from the solution for homogeneous formations in two ways i a stronger preferential flow and ii a more persistent tail hence the two main features of f τ f discussed before are further strengthened by the medium heterogeneity the increase of preferential flow and hence a faster rising limb of the pdf and a peak higher and closer to τ 0 is the results of the availability of highly conductive layers in the system whose relative number increases with heterogeneity σ y 2 the second feature observed in heterogeneous systems is a stronger and more persistent tail as compared to the homogeneous case such behavior is determined by the combination of long and slow solute paths in the low conductive layers that are present in heterogeneous systems again the number of such low k elements increases with σ y 2 altogether the impact of heterogeneity manifests in a significant spreading of solute particles that increases the natural tendency to spreading induced by the dipole setup we emphasize that the injection mode flux proportional or resident concentration has a strong impact on the travel time distribution especially for highly heterogeneous formations in fig 4 we show the pdf of τ for the two injection conditions for a log normally distributed k i e formulas 11 and 8 respectively for a low σ y 2 0 1 and mild heterogeneous formation σ y 2 1 clearly the effect of the injection condition is small to negligible when heterogeneity is small while it is important when heterogeneity increases in particular the resident concentration injection condition generally leads to a less pronounced preferential flow and a heavier tail we note that similar results were observed by pedretti and fiori 2013 for the convergent tracer test in particular regarding the emergence of fast preferential flows when in presence of strongly heterogeneous formations such feature mostly affects the early limb of the btc instead the btc tail is in the present case mostly affected by the particular flow configuration determined by the dipole which is very much different from the convergent flow considered by pedretti and fiori 2013 4 impact of aquifer thickness non ergodicity as previously mentioned the solutions derived in section 2 are formally valid for an ergodic system i e when the number of layers is large enough that the conductivity distribution fk is adequately sampled over the aquifer depth l such conditions require l i 1 since the vertical integral scale of conductivity i is usually of the order of 0 1 m rubin 2003 table 2 1 the latter condition may be met in applications still it is worth exploring the travel time pdf under non ergodic conditions i e for moderate or small l i and check the conditions for the departure of the travel time pdf from the ergodic solutions derived in section 2 once again we assume in the following f τ f for a log normal k as reference for the ergodic solution i e formula 11 the travel time pdf for a finite number of layers n l 2 i can be easily obtained by a simple monte carlo numerical procedure along the following lines i a vector of n random conductivities drawn from a given cdf for k fk log normal in the following examples is generated i e k i i 1 n ii for each ki a set of travel times is calculated from eq 2 after discretization of θ in the interval 0 π iii the whole set of travel times for all layers are sorted and a cumulative frequency distribution is obtained the entire procedure i iii is repeated nmc times generating nmc cumulative frequency distributions that are averaged in order to obtain f τ f for a formation of given thickness l 2 n i the procedure is very simple and can be easily coded although it can also provide the bands of uncertainty of f τ f in the following we shall focus for brevity on f τ f only i e the ensemble average the principal results of the above procedure are represented in fig 5 that display fτ for four degrees of heterogeneity σ y 2 four panels and a few values of n the limiting cases are the case n 1 corresponding to the solution for a homogeneous formations and the ergodic solution formula 11 which is represented as a red thick line starting from low heterogeneous formations σ y 2 0 1 panel a most of the curves collapse to the ergodic solution when n 10 and above hence a relatively small aquifer thickness is needed in order to reach ergodic conditions in the test and the ergodic solution can be safely applied in aquifers of low heterogeneity a similar situation happens for mild heterogeneity σ y 2 1 panel b for which however the number of layers needed to reach ergodic conditions are larger than the previous case such increase is more consistent for highly heterogeneous systems σ y 2 4 panel c and more so for σ y 2 8 panel d for which a relatively high number of layers n 1000 needs to be sampled to get to ergodic conditions the reasons for this behavior is simple when the variance σ y 2 grows the log conductivity distribution becomes broad and a larger sample size is needed in order to capture it we note again that a similar result was observed by pedretti and fiori 2013 for the convergent tracer test although such sampling problems always occurs when dealing with highly heterogeneous random fields thus under non ergodic conditions the ergodic solutions developed in this work may overestimate the btc spreading when applied to a tracer test in particular if such solutions are employed for the aquifer characterization as discussed in the next section the inferred σ y 2 may be underestimated when in presence of highly heterogeneous aquifers 5 conductivity characterization by the dipole test with application examples the solution developed in this work can be used for the characterization of hydraulic conductivity k in particular the results of a dipole test can be interpreted through the analytical solution for fτ developed in section 2 to determine the log conductivity variance σ y 2 which epitomizes the degree of heterogeneity in the aquifer for instance assuming a log normal k expressions 14 15 can be fitted to the experimental btc in order to obtain σ y 2 we emphasize however that the solutions 10 12 are for a generic distribution fk and in principle even a numerical one could be employed the elements of fτ that are mostly impacted by the conductivity heterogeneity is the first segment i e the rising limb and the peak and the fitting procedure is typically influenced by that part instead the effects of σ y 2 on the tail are less relevant because the tail is anyway influenced by the later arrivals of the particles characterized by the longest and slowest path lines in the following we show a few application examples in all cases a log normal fk is assumed and hence the solution 11 is employed for the fitting of the experimental data the fitting of tracer tests depends on several different factors like e g the choice of the objective function the weight given to data points the part of the btc of interest to mention some thus the fitting method typically depends on the experience and preference of the modeler for the sole purpose of illustration we have adopted here a simple least square fit method with an a posteriori visual check that the fitted curves have a reasonable behavior the parameters employed in the applications as well as the fitted log conductivity variance are reproduced in table 1 the results are represented in terms of either concentration c relative concentration c c 0 or the cdf of travel time depending on the presentation of the results in the source papers in order to support the novelty of our approach the equivalent homogeneous solution for advective dispersive flow to a pumping well in a dipole test was calculated for all three examples therefore the model by hoopes and harleman 1967 their eq 26 similar to the one developed by gelhar 1982 was implemented here the model accounts for mixing along streamlines but neglects mixing between streamlines and adopts a few analytical approximations such that the solution is limited to small values of α a with α the equivalent longitudinal dispersivity we anticipate that the homogeneous model can reasonably capture the experimental data only for small heterogeneity of the porous medium but fails for heterogeneous formations furthermore the fitted dispersivities α are relatively large compared to the well distance a beyond the range of validity of the solution as also visible by the non zero concentration for t 0 this shows that the application of the equivalent homogeneous solution can be problematic for mild high heterogeneity since it is formally only valid for small dispersivities α 5 1 made a dipole tracer experiment referred to as made 5 was performed in 2008 at the columbus air force base in columbus mississippi commonly known as the made macro dispersion experiment site the main aim of the test was to investigate the influence of small scale mass transfer and dispersion processes on well to well transport test settings and results are presented in detail in line with a numerical model of the experiment by bianchi et al 2011 details on aquifer characteristics and parameters of the dipole test are summarized in table 1 four wells have been installed for the dipole test the injection and extraction wells are located 6 m apart and two multi level sampling wells were installed in between at distances of 1 5 m and 3 75 m from the injection well the btcs were measured at the extraction well as well as at seven different depths in the two multi level sampling wells the test was performed in 3 phases initially clean water was injected for 48 h at a rate of 5 68 l min after a relative steady state flow field was established 2078 l of bromide solution with a concentration of 1000 mg l was introduced into the aquifer within 366 min clean water was injected again during the third phase until the experiment was finished after 32 days from injection multiple hydrogeological geophysical investigations as well as tracer tests have been performed since the made site was established with the motivation to gain new insights into transport in highly heterogeneous aquifers for details see e g zheng et al 2011 thus for this site detailed geostatistical information is available for comparison the following values for geometric mean of hydraulic conductivity and variance of log conductivity as the one point statistical parameters were reported interpretation of the flowmeter measurements resulted in k g 4 3 10 5 m s and σ y 2 4 4 1 whereas the dpil observations deliver values of k g 6 7 10 6 m s and σ y 2 5 9 1 5 bohling et al 2016 2012 a fit of the analytical curve provided in this work with the experimental data is given in fig 6 and the estimated log conductivity variance is about σ y 2 4 such value is lower than the recent dpil based estimate by bohling et al 2016 that is σ y 2 5 9 with a 95 confidence interval of 4 4 7 4 being close to the lower bound instead the inferred value is closer to the one obtained by flowmeter measurements the differences between the two estimates might be explained by either the different areas of the made site explored by the two methods where the dpil analysis covered a much larger domain than the one related to the dipole test or more likely by non ergodic effects as discussed at the end of section 4 the good fit of the analytical solution to the experimental data not only for the peak although with some temporal anticipation but also for the tailing behavior can be nicely seen in the insert semi log plot in fig 6 a direct comparison to fig 7 of bianchi et al 2011 shows that the analytical curve based on an ade approach can reproduce the concentration distribution similarly good as the dual domain model fitted by bianchi et al 2011 it can be seen that an equivalent homogeneous transport model solution of hoopes and harleman 1967 with optimal fit of dispersivitiy α 2 73 m can neither reproduce the heavy peak behavior nor the tailing which is obviously strongly impacted by the strong aquifer heterogeneity 5 2 mobile molz et al 1986 and huyakorn et al 1986b presented and analyzed the results of a two well tracer test with pulse input of bromide at a site near mobile alabama the sites formation is composed of a low terrace deposit of quaternary age consisting of interbedded sands and clays which have been deposited along the western edge of the mobile river the sandy confined aquifer section is about 20 m thick and located between 40 and 60 m depth the two well tracer test was performed making use of two fully penetrating wells in a distance of 38 3 m equal injection and withdrawal rate average to 0 946 m3 min a slug of bromide as tracer was added to the injection water during the first 76 6 h of the experiment which persisted in total 32 5 days since the withdrawn water was re injected the tracer recirculated details on aquifer characteristics and parameters of the dipole test are summarized in table 1 investigation of hydraulic conductivity distribution have been performed by molz et al 1990 based on impeller meter measurements and small scale pumping tests results lead to the conclusion that the study aquifer is fairly homogeneous they reported a mean value of hydraulic conductivity of 54 9 m day with a standard deviation of only 2 4 m day further geostatistical analysis for the hydraulic conductivity at that site are not known to the authors fig 7 shows a fit of the analytical curve provided in this work with the experimental data the very small value of σ y 2 0 24 inferred for the estimated log conductivity variance supports the findings that the mobile aquifer is weakly heterogeneous in addition fig 7 gives a simple sensitivity analysis with the analytical curve for two higher variances of σ y 2 0 5 and σ y 2 1 showing that the solution is typically quite sensitive towards the log conductivity variance but mostly for the early time behavior and the peak the tail is mostly dominated by the arrival times of the different flow paths giving a diminishing impact of the heterogeneity on late time btc behavior furthermore the best fit of the equivalent homogeneous solution of hoopes and harleman 1967 dispersivity α 4 06 m to the data indicates that although the aquifer is mildly heterogeneous a purely homogeneous solution is not able to adequately reproduce the concentration distribution 5 3 barstow robson 1974 reported the results of a small scale dipole tracer test in the barstow s aquifer which consist of very permeable younger alluvium of holocene age deposited by the mojave river and alluvial fans the injection and withdrawal wells are located in a distance of 6 4 m both were perforated through most of the 27 45 m aquifer thickness a recharge discharged rate of 55 gallons per minute was reported the tracer solution of sodium chloride was constantly injected during the 84 h span of the test since the withdrawn water was re injected the tracer recirculated concentration was measured at temporal intervals of more than 4 h giving a sparse database in particular for the early time of the btc the test was originally analyzed making use of the method of grove and beetem 1971 to estimate values of macrodispersivity as pointed out by robson 1974 the late time behavior of the concentration curve should be taken with caution due to the tracer recirculation that was carried out during the test details on aquifer characteristics and parameters of the dipole test are summarized in table 1 geostatistical analysis for the hydraulic conductivity at that site are not known to the authors a fit of the analytical curve with the experimental data is given in fig 8 with an estimated log conductivity variance of σ y 2 0 5 along with the best fit for the equivalent homogeneous solution of hoopes and harleman 1967 fitted dispersivity of α 0 98 since no geostatistical analysis of the hydraulic conductivity in the aquifer is known there is no reference for comparison however the fit of the analytical curves with the measured data shows nicely how the shape of the btc can be related to the aquifer heterogeneity by the log conductivity variance rather than dispersivity 6 summary and conclusions in this paper we derived a new analytical solution for interpreting dipole tests in heterogeneous media furthermore it was presented how the solution can be used for interpretation of dipole field tests in view of geostatistical aquifer characterization the work was motivated by the lack of methods for dipole tests taking the strong spatial heterogeneity of hydraulic conductivity into account although tracer tests are central tools for inferring hydrogeological structural and transport parameters of the subsurface in dipole tracer tests also two well test or doublet tests a tracer is introduced at a recharge well and the breakthrough curve btc is measured at a pumping well the analytical solution for the btc at the pumping well was developed by considering a stratified heterogeneous hydraulic conductivity structure the analysis of the btc in this kind of media was carried out by considering the travel time of a generic solute particle from the injection to the pumping well the derivation of the analytical solutions was performed for two different injection conditions i resident concentration assuming that the mass of solute entering each layer from the injection well is constant for all layers and ii flux proportional injection mode where the entering mass is assumed proportional to layer conductivity the solution was derived for an instantaneous pulse of solute and extended to a formula for continuous solute injection the illustration of results focused on different aspects of the solution i the impact of heterogeneity ii the impact of the injection condition and iii the impact of non ergodic conditions at the injection well the analysis lead to the following conclusions the impact of heterogeneity manifests in a significant spreading of solute particles that increases the natural tendency to spreading induced by the dipole setup for a log normal conductivity distribution an increasing degree of heterogeneity leads to a stronger preferential flow and a more persistent tail the injection mode has a strong impact on the travel time distribution especially for highly heterogeneous formations the resident concentration injection condition generally leads to a less pronounced preferential flow and a heavier tail with increasing heterogeneity the number of layers needed to reach ergodic conditions becomes larger under non ergodic conditions the solutions developed in this work may overestimate the btc spreading in particular if such solutions are employed for the aquifer characterization the inferred log conductivity variance σ y 2 may be underestimated when in presence of highly heterogeneous aquifers as final step the derived method was used for conductivity characterization at three dipole field tests as illustrative examples thereby the log conductivity variance war inferred from the shape of the observed btcs the analysis of dipole tests at two sites indicated a mild heterogeneity being in line with other observations at these sites the dipole test performed at the heterogeneous made site was analyzed resulting in a high value of variance being in the same range as an geostatistical interpretation of flowmeter measurements but smaller then the variance resulting from a geostatistical interpretation of dpil measurements this results can be associated to the assumption of ergodicity in the analytical solution with might not be present at the heterogeneous field site giving an underestimation of variance by the analytical solution the presented method is the first fully analytical tool for dipole tracer test analysis taking heterogeneity of hydraulic conductivity into account assumptions in the derivation of the analytical solutions have been taken to be in line with the conditions encountered in the field it could be shown that the method is easily applicable to measured btcs for inferring the degree of heterogeneity namely the log conductivity variance the method is a promising geostatistical analyzing tool as addition to other geostatistical investigations methods often being time and cost intensive acknowledgements we are grateful to marco bianchi for providing the data of the made 5 experiments furthermore we would like to thank fred molz for providing background information of the mobile test site and sebastian müller for support with computations supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2018 03 006 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
832,in this paper a new analytical solution for interpreting dipole tests in heterogeneous media is derived by associating the shape of the tracer breakthrough curve with the log conductivity variance it is presented how the solution can be used for interpretation of dipole field test in view of geostatistical aquifer characterization on three illustrative examples the analytical solution for the tracer breakthrough curve at the pumping well in a dipole tracer test is developed by considering a perfectly stratified formation the analysis is carried out making use of the travel time of a generic solute particle from the injection to the pumping well injection conditions are adapted to different possible field setting solutions are presented for resident and flux proportional injection mode as well as for an instantaneous pulse of solute and continuous solute injections the analytical form of the solution allows a detailed investigation on the impact of heterogeneity the tracer input conditions and ergodicity conditions at the well the impact of heterogeneity manifests in a significant spreading of solute particles that increases the natural tendency to spreading induced by the dipole setup furthermore with increasing heterogeneity the number of layers needed to reach ergodic conditions become larger thus dipole test in highly heterogeneous aquifers might take place under non ergodic conditions giving that the log conductivity variance is underestimated the method is a promising geostatistical analyzing tool being the first analytical solution for dipole tracer test analysis taking heterogeneity of hydraulic conductivity into account 1 introduction groundwater is an important natural resource for drinking water supply hence the investigation and predictive modeling of flow and transport in porous media are of broad relevance in particular for water quality aspects and methods for contaminated site treatment like remediation risk assessment and natural attenuation tracer tests are thereby the foremost used observation method for inferring hydrogeological structural and transport parameters of the subsurface most important the hydraulic conductivity k which determines the velocity of groundwater flow typically exhibits a large spatial heterogeneity with values varying over orders of magnitudes gelhar 1993 a bunch of different tracer test types exist which all have their merits but also limitations tracer tests under ambient flow conditions are either limited to short travel distances because groundwater flow is usually very slow or the test operation suffers high costs due to a long test duration and the need for a large observation network more efficient in time and size of observation network are tracer tests under forced flow conditions due to higher velocities and directed flow however the analysis of test types under well flow conditions are more complex due to the non uniform flow field especially taking aquifer heterogeneity into account requires sophisticated analyzing methods in dipole tracer tests also named two well test or doublet tests a tracer is introduced at a recharge well and the breakthrough curve btc is measured at a pumping well the pumped water can optionally be used for recharge in a recirculation the test setting has the advantage of circumventing the problem of removal of waste water and the need for an additional water source however the complex flow pattern causes the interpretation of the transport behavior to be more complicated the dipole shape of the flow field gives that the observed concentration at the pumping well to be a superposition of tracer transport along different streamlines with different tracer arrival times the foremost aim in this work is to present an alternative interpretation method for dipole tests in heterogeneous media it will be given a simple solution for interpretation of dipole test in view of geostatistical aquifer characterization the first analytical analysis of dipole test was performed by hoopes and harleman 1967 they presented a mathematical description of the flow field provided equations for streamlines and the travel time of a tracer particle along a streamline they gave analytical expressions for the temporal and spatial distribution of tracer in a homogeneous aquifer by analyzing an approximate solutions for the concentration in the presence of convection dispersion and diffusion hoopes and harleman 1967 analyzed the impact of these processes they found that dispersion impacts only the very early part of the btc afterwards the arrival of different streamlines is dominant for constant tracer injection the btc is almost insensitive to dispersion shortly after grove and beetem 1971 presented a method for analyzing btc of dipole tests in homogeneous aquifers in order to evaluate the porosity and the longitudinal dispersivity the method focuses on tests with pulse tracer injection and recirculation being constructed as superposition of 1d solutions for individual streamlines based on calculations of the streamline length and particle travel time the approach took the crucial assumption that velocity is constant along flow lines the work of grove and beetem 1971 also included an illustrative example of an analysis of a dipole test in the fractured carbonate aquifer near carlsbad new mexico gelhar 1982 derived a semi analytical solution and provided type curves for the btc measured at the pumping well in a dipole flow system the type curves where numerically derived based on the theoretical work of gelhar and collins 1971 about longitudinal dispersion along streamlines in nonuniform flow for the solution transverse dispersion is assumed to be negligible the work of gelhar 1982 included the analysis of a dipole test at the hanford site for illustrating the method welty and gelhar 1989 reanalyzed several field tests using the method of gelhar 1982 in order to estimate dispersivity the first approach to numerically interpret dipole tests was presented by huyakorn et al 1986a they developed a problem adapted simulation software making use of a curvilinear fem the method was applied to the dipole field test at the chalk river site pickens and grisak 1981 more recently bianchi et al 2011 presented a numerical model to interpret the dipole test at the made site with explicitly including aquifer heterogeneity the authors performed simulations with conditioned log normal hydraulic conductivity fields several examples for dipole field tests in consolidated media ca be found in literature mostly as illustrative examples along analytical method development e g in the previously mentioned papers of grove and beetem 1971 gelhar 1982 welty and gelhar 1989 some early works on tracer tests report large distance dipole tests examples are the test at the savannah river plant webster et al 1970 where wells are 538 m 1765 feet apart with a duration of 2 years or the test at amargosa claasen and cordes 1975 over a distance of 122 m 400 feet there are also several examples for tests in consolidated media mostly over shorter ranges with well distances of maximally a few tenth of meters for instance at tucson arizona wilson 1971 at the chalk river site canada pickens and grisak 1981 at the kesterson aquifer california hyndman and gorelick 1996 and at the rocky mountain arsenal colorado thorbjarnarson and mackay 1997 tests of particular interest within this work are the dipole tests at made bianchi et al 2011 at barstow robson 1974 and at mobile molz et al 1986 they will act as illustrative examples for the method developed herein and will be described in detail later dipole tests in the presence of significant spatial heterogeneity of hydraulic conductivity have rarely been studied though the actual streamline structure of dipole tests is strongly impacted by aquifer heterogeneity the analysis of dipole tests with an explicit representation of aquifer heterogeneity has been done only using numerical models e g bianchi et al 2011 analytical models for dipole analysis including heterogeneity parameters are not available the methods of hoopes and harleman 1967 grove and beetem 1971 gelhar 1982 welty and gelhar 1989 are conceptualized for homogeneous conductivity taking the effect of aquifer heterogeneity only implicitly onto account by the lumped parameter of macrodispersivity in this paper we present an alternative concept for dipole test analysis by taking heterogeneity of hydraulic conductivity explicitly into account a geostatistical approach is used due to the limited data availability in subsurface hydrology in combination with high uncertainty in values the characteristics of hydraulic conductivity k are captured by the one point statistical parameters of mean conductivity k g and the log conductivity variance σ y 2 with y ln k the log conductivity an analytical solution for the btc at the pumping well in a dipole tracer test is developed by considering a stratified heterogeneous hydraulic conductivity structure thus associating the shape of the btc with statistical properties of the conductivity field the stochastic framework and part of the analysis have some similarities with the model presented by pedretti and fiori 2013 for convergent flow the sensitivity of dipole tests to spatial heterogeneity might be used to determine statistical parameters especially the log conductivity variance σ y 2 offering an alternative test method for geostatistical aquifer analysis the plan of the paper is given as following the mathematical framework sets the background of the method providing the derivation of the analytical solutions for different test configurations it is followed by an illustration of results and then a discussion on the impact of parameters and test conditions on the btc the method is then used for conductivity characterization on illustrative examples with a re interpretation of three dipole field tests with the newly developed method the paper ends with a summary and conclusions 2 mathematical framework we consider a confined aquifer of thickness l the heterogeneous hydraulic conductivity k field is modeled by considering the aquifer as a perfectly stratified formation i e made up from n layers of vertical thickness 2i with i the vertical integral scale of hydraulic conductivity each layer is of random and independent conductivity ki i 1 n the justification of the stratified model is in the relative short distance between pumping and injecting wells in dipole tests which is often found in recent applications of the order of the horizontal integral scale of k or even less the perfectly stratified formation has been often used in the past for modeling flow and transport in heterogeneous porous formations often leading to useful analytical solutions dagan 1990 matheron and de marsily 1980 mercado 1967 a dipole is created by injecting and extracting a discharge q in two fully penetrating wells at relative distance 2a adopting head boundary conditions in the wells the piezometric head does not depend on the vertical coordinate thus the distribution of head is the same for all layers a sketch of the conceptual model is provided in fig 1 the discharge per unit thickness qi for each layer i is equal to 1 q i k i k q with k n 1 i 1 n k i the arithmetic mean of ki and q q l the aquifer discharge per unit depth at a given initial time a pulse of solute of initial concentration c 0 and duration δ is introduced in the injection well the solute travels in the porous medium and it is collected downstream in the pumping well resulting in a breakthrough curve btc at the same well the btc which corresponds to the temporal behavior of solute flux at the pumping well depends on both the dipole setup e g the distance 2a the discharge q the duration δ etc and the medium configuration i e the vertical distribution of hydraulic conductivities ki and their porosity which in the following is assumed as constant in the entire domain scope of the present analysis is to calculate the btc in dependence on both the dipole setting and the aquifer configuration the analysis is carried out by considering the travel time of a generic solute particle from the injection to the pumping well it is well known that the probability density function pdf of such travel time is identical to the btc of a solute instantaneous pulse in the following we focus on advection only which is the most significant source of spreading due to the non uniform flow configuration local dispersion mechanisms like hydrodynamic dispersion or molecular diffusion are neglected the present solution is based on the analysis of hoopes and harleman 1967 they analyzed the travel time t of a particle from the injection to the extraction well pertaining to the generic streamline departing from the injection well at an angle θ with respect to the line joining the wells see fig 1 they derived an analytical solution for travel time in homogeneous formations under the assumption that the aquifer is indefinite homogeneous isotropic and confined between two horizontal planes in absence of natural flow the flow and piezometric head were obtained from the superposition of the flow fields of a line source and a line sink assuming negligible well radii the flow fields are obtained by the solution of water continuity equations and darcy s law the travel time t was obtained by integration of the flow field along the streamlines originating form the injection well the resulting formula for the travel time in an arbitrary layer i is 2 t 4 π n a 2 q i sin 2 θ 1 θ cot θ π θ π where n is the constant porosity qi is the layer s unit discharge θ is the angle and 2a is the distance between injection and pumping well hereinafter we work with the dimensionless travel time τ defined as 3 τ q t 4 π n a 2 g θ k k i with 4 g θ 1 sin 2 θ 1 θ cot θ the form of the dimensionless travel time results from the definition τ v c t s c with v c q n being the characteristic velocity and s c 4 π a being a characteristic length scale τ is symmetrical with respect to θ 0 and the half space θ 0 π can be safely considered in the analysis the travel time 3 is a random variable that depends on the hydraulic conductivity ki of each layer and the angle of attack θ which is uniformly distributed in the interval 0 π from 3 one can calculate the travel time distribution for the entire aquifer formation considering the dependence of τ on the random variables k and θ whose distributions are fk k and f θ 1 π respectively from basic statistics see e g papoulis 1991 the cumulative density function cdf pτ of τ follows as 5 p τ τ d f k k f θ θ d k d θ 1 π 0 π g θ k τ f k k d k d θ where d is the region in the θ k space such that g θ k k τ assuming ergodicity gives k k a with ka being the arithmetic ensemble mean the integration of 5 over k yields 6 p τ τ 1 π 0 π 1 p k g θ k a τ d θ with pk being the cdf of k from the above expression the travel time pdf is calculated as 7 f τ r τ 1 π k a τ 2 0 π g θ f k g θ k a τ d θ resident injection mode if we assume a log normal distribution for k the above specializes as follows 8 f τ r τ 1 π τ 2 π σ y 2 0 π exp σ y 2 2 ln g θ ln τ 2 2 σ y 2 d θ expression 7 is the pdf of the dimensionless travel time τ in its calculation we have assumed that the mass of solute entering in each layer from the injection well is constant for all layers i e the injection condition is of resident concentration instead the typical injection condition in applications is of flux proportional i e the mass of solute entering each layer is proportional to the local velocity at the injection well which is variable in the vertical and is proportional to the hydraulic conductivity ki of each layer the different injection conditions and their impact on the btc is deeply discussed in jankovic and fiori 2010 for transport in mean uniform flow and in pedretti and fiori 2013 for transport in convergent flow the travel time pdf for flux proportional injection mode along the above lines can be calculated by weighting each solute particle by the layer conductivity ki this way the resulting travel time pdf is obtained by averaging the pdf 7 by the weight 9 k i k a g θ τ obtaining 10 f τ f τ 1 π k a τ 3 0 π g 2 θ f k g θ k a τ d θ flux proportional injection mode assuming a log normal distribution for k the latter becomes 11 f τ f τ 1 π τ 2 2 π σ y 2 0 π g θ exp σ y 2 2 ln g θ ln τ 2 2 σ y 2 d θ summarizing expression 10 represents the travel time pdf under flux proportional injection mode that is the one to be used in applications the above solutions correspond to the btc for an instantaneous pulse of solute the extensions for continuous solute injections of initial concentration c 0 and duration δ which are typically employed in the tracer tests are calculated by the convolution 12 c τ 0 t c 0 t f τ τ t d t where 13 t τ when τ δ δ when τ δ δ q δ 4 π n a 2 in the simple but relevant case when c 0 is constant and k is log normally distributed i e when fτ is equal to 11 12 becomes 14 c τ c 0 φ τ when τ δ φ τ φ τ δ when τ δ with 15 φ τ 1 2 π 0 π 1 erf σ y 2 2 ln g θ ln τ 2 σ y 2 d θ formula 14 is the final result of the present analysis and provides the solute btc for the dipole configuration examined here the formula can be conveniently applied to the interpretation of dipole tests we remind that the above solutions consider that the vertical distribution of conductivity is fully sampled i e ergodicity is assumed such condition is typically met when l i 1 the issue shall be further discussed in the next sections 3 illustration of results in this section we illustrate the main results related to the proposed analytical solutions for the dipole tracer test the pdf of the dimensionless travel time τ q t 4 π n a 2 for the flux proportional injection condition and a log normal distribution of k is given by 11 the latter depends only on the log conductivity variance σ y 2 which represents the degree of heterogeneity of the aquifer system figs 2 and 3 display the travel time pdf p τ f and cdf f τ f for a few values of σ y 2 respectively starting from an almost homogeneous formation σ y 2 0 1 it is seen that the distribution of τ is characterized by a rising limb which is determined by the first fast arrivals of solute moving along the most connected paths between the injection and pumping wells small angle θ in turn the tail of the distribution is typically long and persistent being determined by the slow arrivals of solute particles that move along the longer and slower path lines large θ thus the dipole setup always determines a wide variety of paths in the medium causing a similar variability of arrival times and hence dispersion this is a well known feature which has already been described in past work grove and beetem 1971 hoopes and harleman 1967 koplik 2001 the spatial distribution of conductivity present in the aquifer system further enhances the above dispersion of solute as visible in the curves of figs 2 and 3 for σ y 2 0 it is seen that for increasing degree of heterogeneity σ y 2 the travel time distributions depart from the solution for homogeneous formations in two ways i a stronger preferential flow and ii a more persistent tail hence the two main features of f τ f discussed before are further strengthened by the medium heterogeneity the increase of preferential flow and hence a faster rising limb of the pdf and a peak higher and closer to τ 0 is the results of the availability of highly conductive layers in the system whose relative number increases with heterogeneity σ y 2 the second feature observed in heterogeneous systems is a stronger and more persistent tail as compared to the homogeneous case such behavior is determined by the combination of long and slow solute paths in the low conductive layers that are present in heterogeneous systems again the number of such low k elements increases with σ y 2 altogether the impact of heterogeneity manifests in a significant spreading of solute particles that increases the natural tendency to spreading induced by the dipole setup we emphasize that the injection mode flux proportional or resident concentration has a strong impact on the travel time distribution especially for highly heterogeneous formations in fig 4 we show the pdf of τ for the two injection conditions for a log normally distributed k i e formulas 11 and 8 respectively for a low σ y 2 0 1 and mild heterogeneous formation σ y 2 1 clearly the effect of the injection condition is small to negligible when heterogeneity is small while it is important when heterogeneity increases in particular the resident concentration injection condition generally leads to a less pronounced preferential flow and a heavier tail we note that similar results were observed by pedretti and fiori 2013 for the convergent tracer test in particular regarding the emergence of fast preferential flows when in presence of strongly heterogeneous formations such feature mostly affects the early limb of the btc instead the btc tail is in the present case mostly affected by the particular flow configuration determined by the dipole which is very much different from the convergent flow considered by pedretti and fiori 2013 4 impact of aquifer thickness non ergodicity as previously mentioned the solutions derived in section 2 are formally valid for an ergodic system i e when the number of layers is large enough that the conductivity distribution fk is adequately sampled over the aquifer depth l such conditions require l i 1 since the vertical integral scale of conductivity i is usually of the order of 0 1 m rubin 2003 table 2 1 the latter condition may be met in applications still it is worth exploring the travel time pdf under non ergodic conditions i e for moderate or small l i and check the conditions for the departure of the travel time pdf from the ergodic solutions derived in section 2 once again we assume in the following f τ f for a log normal k as reference for the ergodic solution i e formula 11 the travel time pdf for a finite number of layers n l 2 i can be easily obtained by a simple monte carlo numerical procedure along the following lines i a vector of n random conductivities drawn from a given cdf for k fk log normal in the following examples is generated i e k i i 1 n ii for each ki a set of travel times is calculated from eq 2 after discretization of θ in the interval 0 π iii the whole set of travel times for all layers are sorted and a cumulative frequency distribution is obtained the entire procedure i iii is repeated nmc times generating nmc cumulative frequency distributions that are averaged in order to obtain f τ f for a formation of given thickness l 2 n i the procedure is very simple and can be easily coded although it can also provide the bands of uncertainty of f τ f in the following we shall focus for brevity on f τ f only i e the ensemble average the principal results of the above procedure are represented in fig 5 that display fτ for four degrees of heterogeneity σ y 2 four panels and a few values of n the limiting cases are the case n 1 corresponding to the solution for a homogeneous formations and the ergodic solution formula 11 which is represented as a red thick line starting from low heterogeneous formations σ y 2 0 1 panel a most of the curves collapse to the ergodic solution when n 10 and above hence a relatively small aquifer thickness is needed in order to reach ergodic conditions in the test and the ergodic solution can be safely applied in aquifers of low heterogeneity a similar situation happens for mild heterogeneity σ y 2 1 panel b for which however the number of layers needed to reach ergodic conditions are larger than the previous case such increase is more consistent for highly heterogeneous systems σ y 2 4 panel c and more so for σ y 2 8 panel d for which a relatively high number of layers n 1000 needs to be sampled to get to ergodic conditions the reasons for this behavior is simple when the variance σ y 2 grows the log conductivity distribution becomes broad and a larger sample size is needed in order to capture it we note again that a similar result was observed by pedretti and fiori 2013 for the convergent tracer test although such sampling problems always occurs when dealing with highly heterogeneous random fields thus under non ergodic conditions the ergodic solutions developed in this work may overestimate the btc spreading when applied to a tracer test in particular if such solutions are employed for the aquifer characterization as discussed in the next section the inferred σ y 2 may be underestimated when in presence of highly heterogeneous aquifers 5 conductivity characterization by the dipole test with application examples the solution developed in this work can be used for the characterization of hydraulic conductivity k in particular the results of a dipole test can be interpreted through the analytical solution for fτ developed in section 2 to determine the log conductivity variance σ y 2 which epitomizes the degree of heterogeneity in the aquifer for instance assuming a log normal k expressions 14 15 can be fitted to the experimental btc in order to obtain σ y 2 we emphasize however that the solutions 10 12 are for a generic distribution fk and in principle even a numerical one could be employed the elements of fτ that are mostly impacted by the conductivity heterogeneity is the first segment i e the rising limb and the peak and the fitting procedure is typically influenced by that part instead the effects of σ y 2 on the tail are less relevant because the tail is anyway influenced by the later arrivals of the particles characterized by the longest and slowest path lines in the following we show a few application examples in all cases a log normal fk is assumed and hence the solution 11 is employed for the fitting of the experimental data the fitting of tracer tests depends on several different factors like e g the choice of the objective function the weight given to data points the part of the btc of interest to mention some thus the fitting method typically depends on the experience and preference of the modeler for the sole purpose of illustration we have adopted here a simple least square fit method with an a posteriori visual check that the fitted curves have a reasonable behavior the parameters employed in the applications as well as the fitted log conductivity variance are reproduced in table 1 the results are represented in terms of either concentration c relative concentration c c 0 or the cdf of travel time depending on the presentation of the results in the source papers in order to support the novelty of our approach the equivalent homogeneous solution for advective dispersive flow to a pumping well in a dipole test was calculated for all three examples therefore the model by hoopes and harleman 1967 their eq 26 similar to the one developed by gelhar 1982 was implemented here the model accounts for mixing along streamlines but neglects mixing between streamlines and adopts a few analytical approximations such that the solution is limited to small values of α a with α the equivalent longitudinal dispersivity we anticipate that the homogeneous model can reasonably capture the experimental data only for small heterogeneity of the porous medium but fails for heterogeneous formations furthermore the fitted dispersivities α are relatively large compared to the well distance a beyond the range of validity of the solution as also visible by the non zero concentration for t 0 this shows that the application of the equivalent homogeneous solution can be problematic for mild high heterogeneity since it is formally only valid for small dispersivities α 5 1 made a dipole tracer experiment referred to as made 5 was performed in 2008 at the columbus air force base in columbus mississippi commonly known as the made macro dispersion experiment site the main aim of the test was to investigate the influence of small scale mass transfer and dispersion processes on well to well transport test settings and results are presented in detail in line with a numerical model of the experiment by bianchi et al 2011 details on aquifer characteristics and parameters of the dipole test are summarized in table 1 four wells have been installed for the dipole test the injection and extraction wells are located 6 m apart and two multi level sampling wells were installed in between at distances of 1 5 m and 3 75 m from the injection well the btcs were measured at the extraction well as well as at seven different depths in the two multi level sampling wells the test was performed in 3 phases initially clean water was injected for 48 h at a rate of 5 68 l min after a relative steady state flow field was established 2078 l of bromide solution with a concentration of 1000 mg l was introduced into the aquifer within 366 min clean water was injected again during the third phase until the experiment was finished after 32 days from injection multiple hydrogeological geophysical investigations as well as tracer tests have been performed since the made site was established with the motivation to gain new insights into transport in highly heterogeneous aquifers for details see e g zheng et al 2011 thus for this site detailed geostatistical information is available for comparison the following values for geometric mean of hydraulic conductivity and variance of log conductivity as the one point statistical parameters were reported interpretation of the flowmeter measurements resulted in k g 4 3 10 5 m s and σ y 2 4 4 1 whereas the dpil observations deliver values of k g 6 7 10 6 m s and σ y 2 5 9 1 5 bohling et al 2016 2012 a fit of the analytical curve provided in this work with the experimental data is given in fig 6 and the estimated log conductivity variance is about σ y 2 4 such value is lower than the recent dpil based estimate by bohling et al 2016 that is σ y 2 5 9 with a 95 confidence interval of 4 4 7 4 being close to the lower bound instead the inferred value is closer to the one obtained by flowmeter measurements the differences between the two estimates might be explained by either the different areas of the made site explored by the two methods where the dpil analysis covered a much larger domain than the one related to the dipole test or more likely by non ergodic effects as discussed at the end of section 4 the good fit of the analytical solution to the experimental data not only for the peak although with some temporal anticipation but also for the tailing behavior can be nicely seen in the insert semi log plot in fig 6 a direct comparison to fig 7 of bianchi et al 2011 shows that the analytical curve based on an ade approach can reproduce the concentration distribution similarly good as the dual domain model fitted by bianchi et al 2011 it can be seen that an equivalent homogeneous transport model solution of hoopes and harleman 1967 with optimal fit of dispersivitiy α 2 73 m can neither reproduce the heavy peak behavior nor the tailing which is obviously strongly impacted by the strong aquifer heterogeneity 5 2 mobile molz et al 1986 and huyakorn et al 1986b presented and analyzed the results of a two well tracer test with pulse input of bromide at a site near mobile alabama the sites formation is composed of a low terrace deposit of quaternary age consisting of interbedded sands and clays which have been deposited along the western edge of the mobile river the sandy confined aquifer section is about 20 m thick and located between 40 and 60 m depth the two well tracer test was performed making use of two fully penetrating wells in a distance of 38 3 m equal injection and withdrawal rate average to 0 946 m3 min a slug of bromide as tracer was added to the injection water during the first 76 6 h of the experiment which persisted in total 32 5 days since the withdrawn water was re injected the tracer recirculated details on aquifer characteristics and parameters of the dipole test are summarized in table 1 investigation of hydraulic conductivity distribution have been performed by molz et al 1990 based on impeller meter measurements and small scale pumping tests results lead to the conclusion that the study aquifer is fairly homogeneous they reported a mean value of hydraulic conductivity of 54 9 m day with a standard deviation of only 2 4 m day further geostatistical analysis for the hydraulic conductivity at that site are not known to the authors fig 7 shows a fit of the analytical curve provided in this work with the experimental data the very small value of σ y 2 0 24 inferred for the estimated log conductivity variance supports the findings that the mobile aquifer is weakly heterogeneous in addition fig 7 gives a simple sensitivity analysis with the analytical curve for two higher variances of σ y 2 0 5 and σ y 2 1 showing that the solution is typically quite sensitive towards the log conductivity variance but mostly for the early time behavior and the peak the tail is mostly dominated by the arrival times of the different flow paths giving a diminishing impact of the heterogeneity on late time btc behavior furthermore the best fit of the equivalent homogeneous solution of hoopes and harleman 1967 dispersivity α 4 06 m to the data indicates that although the aquifer is mildly heterogeneous a purely homogeneous solution is not able to adequately reproduce the concentration distribution 5 3 barstow robson 1974 reported the results of a small scale dipole tracer test in the barstow s aquifer which consist of very permeable younger alluvium of holocene age deposited by the mojave river and alluvial fans the injection and withdrawal wells are located in a distance of 6 4 m both were perforated through most of the 27 45 m aquifer thickness a recharge discharged rate of 55 gallons per minute was reported the tracer solution of sodium chloride was constantly injected during the 84 h span of the test since the withdrawn water was re injected the tracer recirculated concentration was measured at temporal intervals of more than 4 h giving a sparse database in particular for the early time of the btc the test was originally analyzed making use of the method of grove and beetem 1971 to estimate values of macrodispersivity as pointed out by robson 1974 the late time behavior of the concentration curve should be taken with caution due to the tracer recirculation that was carried out during the test details on aquifer characteristics and parameters of the dipole test are summarized in table 1 geostatistical analysis for the hydraulic conductivity at that site are not known to the authors a fit of the analytical curve with the experimental data is given in fig 8 with an estimated log conductivity variance of σ y 2 0 5 along with the best fit for the equivalent homogeneous solution of hoopes and harleman 1967 fitted dispersivity of α 0 98 since no geostatistical analysis of the hydraulic conductivity in the aquifer is known there is no reference for comparison however the fit of the analytical curves with the measured data shows nicely how the shape of the btc can be related to the aquifer heterogeneity by the log conductivity variance rather than dispersivity 6 summary and conclusions in this paper we derived a new analytical solution for interpreting dipole tests in heterogeneous media furthermore it was presented how the solution can be used for interpretation of dipole field tests in view of geostatistical aquifer characterization the work was motivated by the lack of methods for dipole tests taking the strong spatial heterogeneity of hydraulic conductivity into account although tracer tests are central tools for inferring hydrogeological structural and transport parameters of the subsurface in dipole tracer tests also two well test or doublet tests a tracer is introduced at a recharge well and the breakthrough curve btc is measured at a pumping well the analytical solution for the btc at the pumping well was developed by considering a stratified heterogeneous hydraulic conductivity structure the analysis of the btc in this kind of media was carried out by considering the travel time of a generic solute particle from the injection to the pumping well the derivation of the analytical solutions was performed for two different injection conditions i resident concentration assuming that the mass of solute entering each layer from the injection well is constant for all layers and ii flux proportional injection mode where the entering mass is assumed proportional to layer conductivity the solution was derived for an instantaneous pulse of solute and extended to a formula for continuous solute injection the illustration of results focused on different aspects of the solution i the impact of heterogeneity ii the impact of the injection condition and iii the impact of non ergodic conditions at the injection well the analysis lead to the following conclusions the impact of heterogeneity manifests in a significant spreading of solute particles that increases the natural tendency to spreading induced by the dipole setup for a log normal conductivity distribution an increasing degree of heterogeneity leads to a stronger preferential flow and a more persistent tail the injection mode has a strong impact on the travel time distribution especially for highly heterogeneous formations the resident concentration injection condition generally leads to a less pronounced preferential flow and a heavier tail with increasing heterogeneity the number of layers needed to reach ergodic conditions becomes larger under non ergodic conditions the solutions developed in this work may overestimate the btc spreading in particular if such solutions are employed for the aquifer characterization the inferred log conductivity variance σ y 2 may be underestimated when in presence of highly heterogeneous aquifers as final step the derived method was used for conductivity characterization at three dipole field tests as illustrative examples thereby the log conductivity variance war inferred from the shape of the observed btcs the analysis of dipole tests at two sites indicated a mild heterogeneity being in line with other observations at these sites the dipole test performed at the heterogeneous made site was analyzed resulting in a high value of variance being in the same range as an geostatistical interpretation of flowmeter measurements but smaller then the variance resulting from a geostatistical interpretation of dpil measurements this results can be associated to the assumption of ergodicity in the analytical solution with might not be present at the heterogeneous field site giving an underestimation of variance by the analytical solution the presented method is the first fully analytical tool for dipole tracer test analysis taking heterogeneity of hydraulic conductivity into account assumptions in the derivation of the analytical solutions have been taken to be in line with the conditions encountered in the field it could be shown that the method is easily applicable to measured btcs for inferring the degree of heterogeneity namely the log conductivity variance the method is a promising geostatistical analyzing tool as addition to other geostatistical investigations methods often being time and cost intensive acknowledgements we are grateful to marco bianchi for providing the data of the made 5 experiments furthermore we would like to thank fred molz for providing background information of the mobile test site and sebastian müller for support with computations supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2018 03 006 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
833,new data collection techniques offer numerical modelers the ability to gather and utilize high quality data sets with high spatial and temporal resolution such data sets are currently needed for calibration verification and to fuel future model development particularly morphological simulations this study explores the use of high quality spatial and temporal data sets of observed bed load transport in braided river flume experiments to evaluate the ability of a two dimensional model delft3d to predict bed load transport this study uses a fixed bed model configuration and examines the model s shear stress calculations which are the foundation to predict the sediment fluxes necessary for morphological simulations the evaluation is conducted for three flow rates and model setup used highly accurate structure from motion sfm topography and discharge boundary conditions the model was hydraulically calibrated using bed roughness and performance was evaluated based on depth and inundation agreement model bed load performance was evaluated in terms of critical shear stress exceedance area compared to maps of observed bed mobility in a flume following the standard hydraulic calibration bed load performance was tested for sensitivity to horizontal eddy viscosity parameterization and bed morphology updating simulations produced depth errors equal to the sfm inherent errors inundation agreement of 77 85 and critical shear stress exceedance in agreement with 49 68 of the observed active area this study provides insight into the ability of physically based two dimensional simulations to accurately predict bed load as well as the effects of horizontal eddy viscosity and bed updating further this study highlights how using high spatial and temporal data to capture the physical processes at work during flume experiments can help to improve morphological modeling keywords numerical modeling river morphodynamics structure from motion bed load mobility maps bed load transport symbols list δsf difference of saturation between time lapse imagery d 50 median sediment diameter dem digital elevation model dod dem of difference fitwe inundation effective width i e total inundation area match fitcongruent inundation congruent fit i e inundation local agreement gcp ground control points mae mean absolute error me mean error q8 q16 q24 discharges of 8 m3 s 16 m3 s and 24 m3 s 0 8 l s 1 6 l s and 2 4 l s unscaled respectively rmse root mean squared error sde standard deviation error sfm structure from motion smi sediment mobility images τcongruent shear active area congruency i e shear predicted mobility local agreement τcr critical shear stress τwe total effective shear active area i e total shear predicted mobility area match tls terrestrial lidar scanners topcat topographic point cloud analysis toolkit wse water surface elevation zmin minimum elevation for given grid cell derived from topcat 1 introduction numerical models have become a dominant tool for both science and applications in river management however like all tools numerical models have inherent inaccuracies that restrict their abilities models thus require continuous attention and refinement focusing on reach scale simulations the increase in two dimensional 2d numerical modeling has been enabled by increased computational processing power readily available affordable software with user friendly interfaces and the increased availability and ease of acquisition of affordable high resolution topography bates 2012 javernick et al 2016 rumsby et al 2008 specifically topographic data sets derived from terrestrial lidar scanners tls and close range structure from motion sfm can achieve point cloud resolution accuracy and precision on the millimeter scale james robson 2012 additional data collection of water depth velocity and bed mobility using point sampling techniques of stream gauging velocity meters echo sounders and acoustic doppler current profilers also provide valuable data sets for model testing e g rennie church 2010 williams et al 2015 such unprecedented data offers new opportunities to further evaluate numerical model performance as topographic uncertainty and or inaccuracy can greatly influence overall model performance legleiter et al 2011 braided rivers provide particularly challenging conditions for numerical modeling due to their multi channel flows subtle and complex relief partial inundation and high lateral mobility javernick et al 2016 williams et al 2013 numerous studies have shown that numerical models are able to simulate hydraulics and morphodynamics of braided rivers with topographic input or when allowed to self develop braided river networks e g mcardell faeh 2001 moron edmonds amos 2017 murray paola 1994 schuurman marra kleinhans 2013 achievements in numerical modeling over the last five years have benefited from such rich topographic data sets specifically studies have evaluated 2d model performance on braided river and produced good overall agreement in depth velocity and inundation extent javernick et al 2016 williams et al 2013 2016a morphodynamic models have produced comparable volumes of erosion and deposition when compared to observed data e g predicted volumes within a factor of two of observed data williams et al 2016a additional numerical model achievements include generation of meandering braided and anabranching river styles nicholas 2013a 2013b and insight on how various vegetation types and life stages influence river pattern and morphodynamics nardin et al 2016 oorschot et al 2016 despite these advances many challenges remain for morphological modeling examples include the description of physical processes mosselman 2012 computational costs transport formula accuracy and the inclusion of mixed grain size sediment hirano 1971 1972 williams et al 2016b riparian vegetation and wood effects bertoldi ruiz villanueva 2015 lauer et al 2016 bank erosion stecca et al 2017 and spatial and temporal discretization and the influences of small boundary and initial condition errors williams et al 2016a with the new opportunities to acquire low cost high spatial and temporal resolution topography modelers can properly calibrate instantaneous depth and inundation e g javernick et al 2016 nicholas et al 2012 williams et al 2013 spatial and temporal velocity data is less accessible as these surveys are very labor intensive and or cost prohibitive morphological data for model calibration remains very limited since repeated topographic data collection typically has weeks months or years between data sets e g lane et al 2010 lane westaway hicks 2003 moretto et al 2014 wheaton et al 2013 with this limited temporal data model calibration protocols rely on matching reach scale observed erosion and deposition patterns derived from digital elevation models dems and dems of difference dods that only capture the flood impacts rather than processes further these temporally poor data sets contain an accumulation of the complicated individual and interrelated process between hydraulics morphology vegetation and animals all of which are not well understood bertoldi et al 2015 javernick 2014 vargas luna et al 2015 due to a multitude of parameters to calibrate in both hydraulic and morphologic simulations modelers with limited data can reach a parameterization or several that produce a convincing match to observed data however it is nearly impossible to know if these results are numerical artifacts or represent physical phenomena mosselman le 2016 therefore it is necessary to acquire and use greater spatial and temporal resolution data sets that capture morphological processes rather than just post flood impacts to calibrate and verify numerical models further the use of data sets with greater spatial and temporal resolution will enable new opportunities in model testing and development which is currently lacking williams et al 2016b the intent of this paper is to explore new opportunities to calibrate evaluate and verify current model capabilities using high spatial and temporal resolution data sets specifically the main objectives were i to compare a 2d numerical model s predictions of bed load transport occurrence and location to observed data from laboratory scale gravel bed braided rivers and ii to evaluate the sensitivity of bed load prediction to varying horizontal eddy viscosity parameterization and bed updating 2 methods evaluating this model s ability to predict bed load transport was accomplished by comparing local instantaneous shear stress calculations with observed near instantaneous bed load activity that was derived from georeferenced time lapse imagery techniques redolfi et al 2017 numerical models based on the shallow water equations compute bed load transport as a function of the bed shear stress which results from resolving the flow hydrodynamics on the current bed configuration therefore prediction of the instantaneous bed load on a known topography does not require modeling of the bed evolution in time for this reason it was sufficient to use the numerical model in fixed bed mode without the need to select and calibrate a sediment transport equation observed and model predicted bed load transport was quantified at low medium and high flow rates the quantification of observed model performance required the creation of two new metrics that provided a measure of the total shear area of predicted mobility and the local accuracy of predicted mobility research methods used to observe model and compare instantaneous sediment transport are presented in the following sections 2 1 flume experiments flume experiments at the university of trento s hydraulic laboratory were conducted in a 3 m wide 24 m long flume the flume s adjustable gradient was set to 1 the sediment was uniform grain size of 1 mm and the channel width was restricted to 1 6 m to allow for data collection within the flume yet maintain a naturally developed braided planform bertoldi et al 2009 garcia lugo bertoldi henshaw gurnell 2015 in this research three flow rates were assessed a low medium and high flow of 0 8 l s q8 1 6 l s q16 and 2 4 l s q24 respectively each experiment started with a graded channel bed with a 0 01 m deep and 0 15 m wide center channel this configuration fully developed into a braided network over a period of 24 hours using a generic hydrograph with an initial discharge of 0 5 l s a rising limb duration of 8 hours a peak discharge of 3 4 l s with a duration of 1 h and a falling limb duration of 15 hours down to the initial discharge subsequently when measurements were acquired a constant discharge phase was run with the prescribed q8 q16 and q24 discharges 2 2 topographic acquisition and generation the flume s topography was generated using the sfm software agisoft photoscan version 1 2 2 2294 photographs were captured with a nikon d7100 camera that was fitted with a fixed 28 mm focal length lens vertical images were taken approximately 2 7 0 1 m above the floodplain with a rail system but with slight variations in orientation this acquisition resulted in an image object resolution of 0 6 mm nearly half the grain size each sfm data set required 150 images 3 images to cover width and 50 rows covering the length which produced approximately 70 side and forward overlap and took 12 14 minutes to capture ground control points gcps were placed at 1 m intervals along the flume s longitudinal axis with four gcps spaced across the transverse axis for a total of 100 gcps prior to the experiments the gcps were rigorously surveyed with a leica total station in reflectorless mode i e no prism used this approach allowed each gcp centroid to be measured directly this configuration has known errors of 2 mm however each gcp was surveyed at least three times from at least three different local network control points following the survey all gcp centroids were averaged to achieve the smallest errors possible average standard deviations between measurements of x y and z were 0 24 0 12 and 0 13 mm respectively following the initial gcp survey 423 ground truthing points were acquired in the flume using the total station in reflectorless mode the resulting gcp xyz data were used for the point cloud transformation and the ground truthing data was a validation of the sfm s elevation accuracy the equation set resolved by the numerical model is scale independent as long as the froude number is preserved i e providing that velocity discharge and time also are scaled accordingly so that scaling up or down the problem by any arbitrary factor does not affect the results unfortunately this is not the case in delft3d flow because the numerical scheme is not designed to reproduce small scale problems and it contains hidden thresholds and dimensional parameters that are hard coded in the algorithm erik mosselman personal communication 2018 therefore to ensure the numerical model stability and accuracy all physical data were geometrically upscaled to a dimension that is representative of natural gravel bed braided rivers specifically choosing a geometrical scale of 40 the physical model reproduces hydromorphological conditions of a river with median grain size d 50 of 40 mm and width of 64 m reasonable conditions of natural braided gravel bed rivers e g brasington et al 2003 javernick et al 2016 redolfi et al 2016a according to froude scaling principles e g young warburton 1996 if length is scaled by 40 discharge and time must be scaled by a factor 405 2 and 401 2 respectively throughout the rest of this paper all physical data will be presented in the scaled form for convenience in comparing to the numerical results e g scaled flow discharges are 8 1 m³ s for q8 16 2 m³ s for q16 and 24 3 m³ s for q24 sfm image acquisition of 12 minutes is scaled to 75 minutes flume width of 1 6 m is scaled to 64 m etc processing the sfm data in photoscan used the high settings for accuracy and quality once the scene reconstruction was completed gcps were automatically detected markers and xyz data were imported a linear transformation was achieved with the gcps and then further processed with the non linear camera alignment optimization following javernick et al 2014 2016 data were then exported as both orthoimagery and point clouds raw point clouds produced 85 100 million points with an average density of 600 points m² to intelligently reduce the point cloud resolution to a practical size for the numerical model the software topographic point cloud analysis toolkit topcat brasington et al 2012 rychkov et al 2012 was utilized and the elevation minimums zmin were used for topographic representation javernick et al 2016 based on visual inspections and confirmed with photoscan generated dems it was determined that a grid resolution of 0 6 m 0 015 m unscaled would appropriately represent the steepest banks therefore topcat processed all topography point clouds to a 0 6 m resolution as documented by javernick et al 2014 2016 sfm is not fully capable of modeling the elevation of inundated areas due to light attenuation reflection and dynamic surfaces however due to the shallow inundation of the flume relatively fast image acquisition and absences of glint due to the flumes full enclosure and oblique led light strips it was anticipated that sfm could model the inundated channels relatively accurately in order to test this accuracy the braided planform was acquired with sfm in dry conditions and then repeated following an inundation of the flume to reach a steady discharge of 24 3 m³ s comparing the elevations of dry conditions to the elevations of wet areas enabled a determination of how well the sfm could model the inundated river beds however due to the mobile sediment extreme caution was required to avoid areas of significant change caused during the inundation period given the known issues with sfm and inundation area detection the sfm bathymetric refraction correction method of dietrich 2017 was also evaluated with the same sfm data sets in preparation to calibrate the numerical model s hydraulics based on observed water depth and inundation data it was necessary to map the water surface elevation wse of each experiment however visual identification of a wet to dry boundary for water surface mapping was difficult due to the saturated sand i e uniform sand color and low relief therefore using a typical method of imagery based breakline placement could not produce a continuous wse to reduce possible local errors due to wse misrepresentation a protocol was established that mapped the wse with only points that were chosen to confidently represent the wet dry boundary and with relatively low relief this approach helped to avoid the repercussions of choosing a point on a steep bank that might exaggerate the wse this produced a less than ideal point density for wse mapping i e several dozen wse points instead of hundreds but added confidence in individual points the wse protocol then proceeded to i generate a wse raster from the wse points and ii subtract the sfm topcat generated raster from the wse raster data set which provided an inundation map with depth data this method worked well except in a few shallow riffle areas which caused disconnected channels therefore in areas that were obviously inundated the inundation map was manually edited to connect such mistakes further areas that were groundwater fed were manually removed from the inundation map since the numerical model would not be able to predict groundwater fed channels finally several hundred points were randomly selected from the depth raster to compare the observed and modeled depth data 384 600 and 481 points for the q8 q16 and q24 simulations respectively for model calibration 2 3 time lapse camera acquisition process and output data observations of bed load motion using time lapse images taken by fixed cameras were obtained following the methods of redolfi et al 2017 images were acquired with two different slr cameras equipped with 18 mm lenses and provided a pixel size of 0 04 m 1 mm unscaled the area captured in the images was the central most section of the flume that covered a 280 m long reach 7 m unscaled this central location provided adequate distance from the upstream and downstream flume boundaries 340 m i e an adequate run in length for numerical modeling the camera sensitivity was set to minimize image noise while exposure time and aperture were chosen to ensure a correct exposure and focusing of the image in addition time lapse images were captured at intervals of 6 3 minutes 1 minute unscaled and provided sufficient time resolution to capture all the relevant changes of the active area images were geometrically transformed to correct the radial distortion introduced by the lenses and colors were automatically adjusted to obtain balanced images using 10 gcps a fully automated orthorectification was produced and the rgb image was converted to hue saturation value color space and differentiated into subsequent saturation maps finally obtaining maps of filtered differences of saturation δsf that exceeded the minimum level of detection in this case we set a threshold level of 18 255 produced the bed load transport images hereafter these transport images will be referred to as sediment mobility images or smi methods had to be developed to compare the smi with the synchronized sfm data sets since the sfm took 75 minutes to capture the entire flume of which 19 minutes 3 minutes unscaled were spent covering the time lapse reach it was not practical to use just one observation of smi because the topography was captured over several observations that included bed load transport and slight morphology changes therefore the sfm acquisition time of 19 minutes was assumed to represent a time averaged topography and required a similar time averaging of observed sediment mobility this averaged mobility was achieved by taking three smis synchronous with the sfm acquisition i e one before during and after sfm passing of the time lapse camera which was assumed to best represent the varying topography and provided the same time averaging duration to produce one representative mobility the three smis were converted into one mobility image through the following process i each smi active spatial extent was mapped with polygons ii the active area polygons were extracted to grid cells and iii a final representative smi was generated by combining the active grid cells that had occurred at least twice fig 1 in examining the final smi results areas with shadows e g small areas along the flume boundaries indicated sediment mobility even in areas where motion was not feasible i e dry areas therefore a final manual editing was required to remove any shaded area with obviously erroneous data however these shaded areas were limited to narrow strips along the left and right flume boundaries the final active area map was regarded with high confidence since the identified active area was observed to be active in at least two smi and erroneous data was easily identified and limited to shaded areas 2 4 numerical model delft3d flow hereafter delft3d was the model chosen based on its hydraulic and morphological abilities to model braided rivers with highly accurate topographic data sets e g javernick et al 2016 williams et al 2016a delft3d offers both 2d and 3d hydrodynamic simulations based on non steady navier stokes equations with shallow water and boussinesq approximations deltares 2010 this research used 2d depth averaged flow simulations for the horizontal flow conditions and did not consider 3d simulations due to the shallow water and computational demands the model setup utilized the pre determined 0 6 m grid resolution the model s grid encompassed the entire flume length 936 m which included a single flow entrance from the upstream as present in the flume the downstream boundary condition was satisfied by extending the downstream length and applying the discharge to water surface elevation relationship for an equivalent trapezoidal channel due to relatively high froude number the downstream boundary conditions have negligible effects on the study reaches hydraulics 2 5 model evaluation evaluation of the numerical model s ability to match observed bed load transport data observed data discussed in section 2 3 followed the sequence of i establishing a hydraulic calibration and evaluating the model s abilities to match the observed inundation and depth ii quantifying the agreement between the modeled shear stress and the observed active area and iii evaluating the shear prediction accuracy with additional horizontal eddy viscosity parameterization and brief morphological bed updating 2 5 1 model hydraulic calibration initial delft3d calibration followed javernick et al 2016 through parameterization of the bed roughness using nikarduse roughness length values between 0 3 m and 0 8 m and compared the predicted versus observed water inundation and depth horizontal eddy viscosity was set to 0 1 m² s and taken as a general first approach e g similar to the reports of javernick et al 2016 williams et al 2016a simulation performance was measured by depth residual errors calculated with root mean squared error rmse mean absolute error mae mean error me and standard deviation of error sde the simulation s inundation and flow routing were compared to the sfm orthoimagery data using the effective width fitwe and the congruent fit fitcongruent following ashmore and sauks 2006 and williams et al 2013 1 f i t w e i a s i m i a o b s v 2 f i t c o n g r u e n t i a o b s v i a s i m i a o b s v i a s i m where iasim and iaobsv are the simulated and observed total wetted areas respectively effective width results 100 indicate under predicted inundation while values 100 indicate over predicted inundation congruent fit results are 100 where 100 indicates total agreement between predicted and observed data and decreasing values indicate progressively greater inaccuracy in pursuing the optimal model and ensuring a control to compare each simulation the calibrations targeted a 100 fitwe and performance was focused on fitcongruent results 2 5 2 quantification of shear and active area agreement to compare the modeled shear stress to the observed active area the modeled shear stress at cell centers were separated into two data sets of i shear that occurred within the observed active area and ii shear data that occurred outside the observed active area these two data sets were then plotted as two histograms of shear stress overlaying the histograms produced distinct plots with moderate overlap for the predicted active areas within and outside the observed active area and the intersection of the two histograms was interpreted as an indicator of the critical shear stress τcr this method provided a cell by cell distribution of local shear stress in the process of sediment transport the τcr value is a physical parameter however the modeled τcr was treated as a calibration parameter i e similar to the roughness parameter but this calibration required a metric to quantify the overall agreement between observed and predicted active areas therefore the authors created two metrics based on the inundation metrics of fitwe and fitcongruent to quantify the total effective shear active area 3 τ w e a a s i m a a o b s v and shear active area congruency 4 τ c o n g r u e n t a a o b s v a a s i m a a o b s v a a s i m where aasim and aaobsv are the simulated and observed active areas of τ τc r τwe results 100 indicate under predicted total shear stress active area and τcongruent of 100 indicates total agreement between predicted and observed data in pursuing model calibration and ensuring a control to compare each simulation an alternative to the histogram inferred τcr was to determine the τcr value that produced a near 100 τwe i e similar to roughness parameterization for fitwe optimization similar to the inundation calibration the performance was focused on τcongruent results this approach provided a critical shear stress estimated for a reach scale both the histogram intersection method and the τwe calibration methods were used to determine τcr however it was recognized that the τwe method had the advantage of holding each simulation to near 100 τwe which provided the opportunity to compare different simulation results and the effects of horizontal eddy viscosity parameterization and bed updating therefore τwe was the preferred method for the added opportunity to compare different model calibrations and techniques but the histogram method was also considered 2 5 3 horizontal eddy viscosity and bed updating effects model calibration thus far has been a generic parameterization of bed roughness however the effects of a bed updating technique and horizontal eddy viscosity parameterization on shear stress calculations was evaluated with the newly defined τwe and τcongruent metrics bed updating examining the delft3d shear results spatial shear values showed erratic spikes and lows in the wetted channels fig 2 this was assumed to be due to sudden elevation changes in the wetted channels possibly due to inherent sfm errors and sfm errors compounded by bathymetric correction factors causing deeper areas to combat this issue the model was allowed a brief period of the mobile bed configuration where the river bed was allowed to update i e bed updating for durations of 10 and 20 minutes this method has been previously discussed amongst modelers as a method to improve the topographic representation for fixed bed models personal communication with murray hicks and richard measures niwa 2013 but to our knowledge has not been quantitatively investigated allowing a brief period of bed mobility provided a physically based approach to remove the erratic shear spikes as opposed to applying a uniform smoothing or subjective manual approach while this brief bed updating did cause channel bed smoothing significant morphological changes e g channel migration did not occur given that this period was brief and focused on bed smoothing it was not necessary to pursue a specific morphological calibration instead the authors enabled the mobile bed configuration using the standard meyer peter and muller 1948 formula and used default morphologic parameter values 2 5 4 horizontal eddy viscosity calibration horizontal eddy viscosity parameterization was tested by changing the initial value of 0 1 m² s by a factor of 5 and tested the values of 0 02 and 0 5 m² s all simulations and data were reprocessed following the previous method sections and were compared by keeping the fitwe and τwe near 100 but allowed to vary by 1 to avoid exhaustive modeling efforts 3 results 3 1 sfm topography and bathymetry the best transformation accuracies were achieved with the non linear camera optimization transformation similar to javernick et al 2014 and achieved absolute gcp x y z errors between 0 039 0 065 m 0 018 0 023 m and 0 012 0 022 m 0 98 1 63 mm 0 45 0 58 mm 0 30 0 55 mm unscaled respectively results of the separate ground truthing points produced residual me of 0 077 m mae of 0 084 m rmse of 0 148 m and sde of 0 127 m unscaled errors of me 1 9 mm mae 2 1 mm rmse 3 7 mm and sde 3 2 mm sfm results for the inundated channels produced a residual me of 0 121 m 3 03 mm unscaled wet sfm elevations minus dry sfm with positive errors indicating elevations higher than the dry sfm and sde 0 077 m 1 93 mm unscaled results from the dietrich 2017 method produced an improved me of 0 015 m 0 38 mm unscaled with sde 0 097 m 2 43 mm unscaled therefore the dietrich 2017 method was applied to all inundated topographic data for simulations in assessing the inherent sfm error and the discussed challenges to identify the wet dry boundary from orthoimagery and sfm data it was determined that water surface elevations errors are to be treated with the mae of 0 084 m 2 10 mm unscaled for this reason water depths below 0 1 m were not considered reliable and thus were not assessed in inundation or depth results the implications of this threshold did restrict the model s evaluation in very shallow areas but is considered comparable to field data collection and modeling scenarios where instruments frequently have minimum operating depth limits e g echo sounders frequently requiring 0 1 m depth 3 2 model evaluation 3 2 1 initial calibration initial calibrations to achieve 100 fitwe required significantly different roughness lengths for the three discharge simulations as shown in table 1 each of the simulations have higher ks than previously reported simulations of shallow braided river systems and required higher ks values for the progressively lower flow rates however higher roughness values associated with lower discharges is not unprecedented e g kim et al 2010 inundation results produced agreements fitcongruent between 77 and 84 which compares well to the findings of javernick et al 2016 and williams et al 2016a who both had high resolution high accuracy topographic field data and thus indicating that the methods utilized are comparable depth errors all show the modeled depth to be on average higher than the observed depth but are comparable to the inherent topographic errors of 0 1 m 2 5 mm unscaled spatial inundation and agreement quantification of the three simulated flow rates are shown in fig 3 all simulations had an effective width fitwe between 99 0 and 99 7 while the q8 and q16 simulations had 77 agreements the q24 simulation reached an agreement of 84 inundation performance was impaired in areas of shallow depth these areas include point bars backwater alcoves small anabranches shallow depths surrounding bars and occasionally along the banks these mismodeled areas are highlighted by examining the depth errors simulated depth minus observed depth in fig 4 comparison between figs 3 and 4 shows several cases where over predicted depth in a channel was matched by under predicted depth in nearby parallel channels and vice versa these instances suggest upstream flow partitioning issues further several depth errors that exist in fig 4a b and c shows narrow bands along banks these depth errors are likely due to the observed inundation being mapped with higher resolution imagery while the model simulation is restricted to the 0 6 m resolution discrete grid 3 2 2 shear evaluation using two methods to estimate the τcr provided comparable results the histogram method for the three different simulations are shown in fig 5 and have significant overlap between 20 and 50 n m² this overlap precluded defining a sharp threshold while the intersection could be a reasonable option to identify τcr and with this choice a value of around 35 n m² shields number 0 056 can be obtained for each of the three simulations however the method of calibrating τwe to approximately 100 provided a clear τcr value which was always very close to the histogram indicated value therefore the calibrated τwe determined τcr is the only results presented hereafter shear evaluation results for the q8 simulation are shown in fig 6 the q8 calibrated τwe of 100 2 predicted a τcr value of 37 2 n m² shields number 0 058 which results in a τcongruent of 51 3 evaluation of the spatial modeled τcr exceedance i e predicted mobility in fig 6b and c compared to the observed in fig 6a showed the area matches well in the upstream half main channel but with some disagreements in the initial upper most section near the longitudinal boundary along banks fig 6c area a and point bars fig 6c area b after the bifurcation in the downstream half of the reach the predicted mobility occurs over the shallow area on the true right fig 6c area c but then fails to consistently match the observed fig 6c area d including a modeled shear disconnect in the riffle located in the center of the planform fig 6c area e shear evaluation results for the q16 simulation are shown in fig 7 the q16 calibrated τwe of 100 0 produced a τcr value of 31 9 n m² shields number 0 049 which results in a τcongruent of 63 4 examining the spatial shear results in fig 7b and c compared to the observed of fig 7a the predicted mobility was more frequent in the upper reach s left anabranch where little motion was observed fig 7c area f similar to the q8 model modeled shear had disconnected in riffles fig 7c areas g and h and was misrepresented along banks and point bars in the downstream half of the reach the flow is split between a dominant right channel and a narrow left anabranch fig 7c area i the main channel has good agreement but the anabranch is not connected through the observed bifurcation instead a modeled bifurcation exists further downstream from the observed fig 7c area i which later connects the observed and predicted mobility in the narrow anabranch fig 7c area j however predicted mobility extends outside the narrow anabranch s observed boundary and throughout the remaining downstream extents fig 7c area k finally the small true right anabranch that extends the entire reach length fig 7c area l shows several areas of predicted mobility yet only small patches of observed mobility exist overall the agreement is strong in areas of deeper channel flow with degraded performance along riffles and other smaller channels and shallow depths the q24 shear evaluation results are shown in fig 8 where the τwe of 100 1 provided a τcr value of 36 2 n m² shields number 0 056 which resulted in a τcongruent of 58 4 examining the spatial shear modeled in fig 8b and c compared to the observed in fig 8a and similar to the other experiments there is good agreement for the majority of channels and planform however predicted mobility continues to be misrepresented in shallow areas e g fig 8c area m upstream of bifurcations e g fig 8c area n and occasionally well outside the observed boundary e g fig 8c area o in contrast to the low and medium discharge simulations this simulation s riffle areas show predicted mobility 3 2 3 bed updating results for the inclusion of bed updating are shown in table 2 where the 10 minutes of updating improved the shear agreement in all the simulations compared to the original no bed updating albeit only a slight improvement for the q8 simulation examining the shear agreement for 10 to 20 minutes updating the q8 and the q16 results deteriorated while the q24 continued to slightly improve in general the q8 simulation showed minimal improvement for the updating while the q16 and q24 simulations improved by 3 4 due to bed updating the bed updating simulations also required larger calibrated values of τcr compared to no updating simulations except the q16 s 20 minutes of updating finally examining the modeled shear results for the bed updating showed that there was a noticeable improvement in shear prediction in the riffle areas for the q8 and q16 simulations 3 2 4 horizontal eddy viscosity calibration viscosity parameterization and the impacts on shear prediction with varying updating was simulated however only the 10 min of bed updating results are shown for the three values of discharge in table 3 which typically produced the best results albeit not significant improvements giving calibration priority of fitwe of 100 and holding the τwe near 100 the various model simulations show that higher viscosity values require lower roughness values to maintain model inundation performance however while the inundation agreements can be maintained by altering the roughness the effects on the shear stress predictions can be relatively significant this is caused by the changes in the velocity flow field with higher velocity and thus shear stress in the channel center for lower values of the viscosity comparing the 0 5 m2 s and 0 1 m2 s eddy viscosity simulations for all three discharges the 0 5 m² s simulations τcongruent decreased by 4 8 and required a much lower τcr approximately reduced by 20 conversely the 0 02 m² s eddy viscosity had very similar results to the 0 1 m² s but generally with slightly lower values of τcongruent 1 2 apart from the q24 runs that had the same values 4 discussion 4 1 topographic acquisition and generation in generating the flume topography with sfm an ambitious effort was undertaken to achieve an unscaled millimeter accuracy through redundant gcp placement and rigorous survey the millimeter accuracy was achieved geometrically scaling the flume topography by 40 produced average errors of 0 08 m which are well within the best of reported sfm acquisition and accuracy of river field surveyed dems e g fonstad et al 2013 javernick et al 2014 these results represent an improvement compared to recent findings by morgan et al 2017 who reported a significant parabolic distortion of the longitudinal profile measured along a 9 m long flume in this study uniformly distributed errors of frequently occurring parabolic distortion have not been observed likely due to the gcp location redundancy and accurate total station reflectorless mode survey evaluating the sfm topographic performance of the wet versus dry conditions the sfm without assistance represented the bed channel with average elevation errors of 0 12 m indicating an under predicted water depth such results were anticipated and dietrich 2017 method did prove to be valuable in correcting the water depth as noted in the inundation mapping areas that were incorrectly mapped were riffles this result could be from the generally shallow inundation a locally steep water surface slope not adequately mapped with the wse method and or the sfm bathymetry was not mapped correctly in these areas examining the raw sfm point clouds the riffles typically have elevation spikes considering the sfm techniques this is likely due to the riffles dynamic water surface which hinders an accurate sfm image matching technique e g overlapping images capturing varying water surface elevations and changes in the surface angles causing different reflection and refraction given greater noise in riffles the topcat zmin has less reliable data to process and an accurate representation of the inundated bed was reduced nevertheless the sfm data with bathymetric correction and topcat filtering method did produce a high resolution and high accuracy topographic data set for the numerical model simulation and proved to be an efficient and low cost method 4 2 model evaluation 4 2 1 hydraulic calibration evaluation initial model roughness parameterization with a near 100 fitwe produced fitcongruent results between 77 and 84 which are comparable to reports of braided river simulations with high resolution and accurate field collected topographic data sets 60 84 fitcongruent demonstrated by javernick et al 2016 and williams et al 2013 water depth data for each experiment was higher on average than observed me 0 04 to 0 06 m and mae 0 09 to 0 12 m but well within the water surface elevation uncertainty range of 0 1 m assessing inundation for the three discharge simulations showed that performance typically degraded in areas surrounding bars and misrepresentation of small anabranches point bars and back water alcoves however backwater areas may have had groundwater discharge sources the model does not incorporate as previously discussed in javernick et al 2016 simulation of shallow water inundation routing can be very sensitive to subtle topographic inaccuracies inherent topographic misrepresentation due to grid resolution and general model flow prediction accuracy nevertheless as indicated by the fitcongruent results the initial calibration performed well for the complex conditions examined in this study moreover as flow increases and depth rises very slight in a braided river the overall routing and subtle topographic errors become less influential however roughness calibration depends also on the value of the eddy viscosity with several combinations producing similar results in terms of fitcongruent eddy viscosity is a challenging parameter to calibrate as it mainly affects the lateral distribution of velocity with only minor effects on the flow depth unfortunately velocity data are rare even at laboratory scale making it difficult to independently calibrate the eddy viscosity 4 2 2 shear evaluation direct observations of bed load transport occurrence and spatial location offered the opportunity to evaluate the 2d numerical model s ability to correctly reproduce bed load transport patterns in a braided river this is particularly relevant considering that sediments in braided rivers are transported only over a small fraction of the channel width even at high values of discharge ashmore et al 2011 garcia lugo et al 2015 a correct reproduction of the spatial and temporal patterns of bed load activity is therefore a necessary condition to model their morphological evolution such an assessment of the instantaneous bed load transport is unprecedented and highlights several issues that need further investigation acoustic doppler current profiler adcp data sets of previous studies e g rennie church 2010 williams et al 2015 may have provided similar opportunities however the presented flume experiment s data provided a unique opportunity to evaluate a simplified and controlled environment i e uniform sediment constant flow rate clearly observed sediment mobility etc as well as higher flow rates which can be considered as analogous to peak flow conditions local accuracy of bed load occurrence evaluated through the parameter τcongruent was mediocre and ranged between 51 and 63 figs 6c 7c and 8c unlike the inundation fitcongruent the τcongruent did not increase with increased flow rate which indicated that the inundation agreement is not a proxy to indicating bed load agreement there are several issues that may explain these mediocre performances first most mathematical models including delft3d assume that bed load transport is only a function of the local conditions this hypothesis can be limiting in strongly spatially and temporally unsteady conditions typical of braided rivers where it may be worth accounting for non local effects by including an adaptation length parameter adaptation length parameters in numerical models attempt to include non equilibrium sediment transport which quantifies the travel distance required for a packet of sediment to reach a new equilibrium concentration when it moves into a region of higher and lower shear stress lai gaeuman 2013 p 1 gaeuman et al 2015 recently reported adaptation lengths of up to 100 particle diameters which could be relevant considering the high resolution of the topographic and bed load activity data sets future model developments should consider the inclusion of adaptation lengths for bed load transport second some physical property effects are not accounted in the model that may be relevant at the laboratory scale specifically water viscosity surface tension and low reynolds number may cause slight discrepancies however these effects will be limited to the areas of relatively small velocities and depths i e anabranches since these areas have limited bed load transport occurrences these discrepancies are likely restricted to small areas and do not significantly impact the model prediction performance third the imagery technique used to map bed load activity may introduce further errors the combined use of three smi images is an attempt to reduce possible random errors as well as to match the time interval needed to survey the reach with sfm while it is acknowledged that faster topographic acquisition and a single sediment mobility image would reduce the inherent method inaccuracies the sources or errors are anticipated to be minimal given the short intervals between data sets relative to bed activity demonstrated in the three smi images of fig 1 both the analysis of the shear stress frequency distributions fig 5 and the numerical model calibration of the critical threshold for transport initiation to match the observed averaged active width resulted in similar values of the critical shields number estimated in the range 0 05 0 065 for results in table 2 and range between 0 04 and 0 07 for table 3 possible causes to the high shields values could include low reynolds numbers implying the flume experiments not being fully turbulent and associated scaling issues and or the flume sediment being more rounded than typical gravel grains and thus implying that the same diameter can be related to a larger mass the use of uniform sediment excluded issues related to armoring grain imbrication and other sediment sorting patterns while the τcr is a physical value for bed load processes this value was treated as a calibration parameter in the numerical model this assumption was justified based on the same approach as bed roughness values which in the physical world are defined values yet are calibration parameters in numerical models further calibrating the τcr value to near 100 τwe offered an experiment control to compare each simulation s predictive performance and the slight parameterization was consideredworthwhile examining figs 6 8 the q8 and q16 simulations had areas where predicted mobility through riffles was disconnected this may be due to the shallow water areas and or the sfm issues associated with riffles outlined in the topography discussion and thus influence depth and velocity alternatively these disconnected areas could be due to the lack of adaptation length conversely the shear agreement performed best in the main channels of simpler planform and greater depth this is demonstrated in the upstream half of the q8 results and for the entire q16 simulation reasons for the q16 s higher τcongruent may be attributed to its planform which was dominated by a main channel in the upstream and downstream end and the midstream section s bifurcation occurs with relatively even flow partitioning this less complex system with greater flow concentration in defined channels likely has higher τcongruent agreement because the bed load transport is occurring and would continue to occur with less dependence on the additional factors that cause bed load transport e g adaptation length alternatively in shallow and slow flow areas near greater depth and velocity areas adaptation length may likely be more influential which could explain the bed load activity extending in and through areas of lower shear stress these issues highlight the challenges of shallow depth adaptation length and bifurcation calculations as many papers have discussed bifurcation is complex and strongly imbalanced water and sediment and a correct reproduction is crucial for numerical modeling advancement e g bertoldi 2012 bolla pittaluga repetto tubino 2003 kleinhans et al 2008 redolfi et al 2016b conversely predicted mobility occurs periodically outside the observed active area in all simulations this may be due to having τwe of 100 which assumed that τcr was the sole predictor to sediment mobility and thus included larger areas than actuality such an increase in area would contain lower shear stress values and thereby reduce the τcr estimate consequently if the predicted τcr is lower than reality the already present high shields values would increase while many of the uncertainties e g hydraulic model equations accuracies smi representation bed load prediction based solely on shear stress and inaccuracies e g sfm inherent errors have been discussed and are likely to have minimal effects these issues collectively degrade the model s predictive capabilities however within the confines of what the fixed bed model can predict for inundation agreement of a complicated braided river and with shear stress values alone these seemingly mediocre τcongruent results are encouraging while the areas of inaccuracies are yet to be understood viewing these data and figures may also provide an initial analysis of where τcr is not the sole factor for bed load transport and the necessity of correct partitioning initial incipient motion and adaptation length 4 2 3 bed updating evaluation despite the accurate topography and high resolution model grid the bathymetric representation produced exaggerated shear stress values that were assumed to be due to subtle bed elevation spikes and troughs from inherent sfm and bathymetric modifications and grid resolution misrepresentation using the model s morphological updating offered an unbiased physically based approach to reducing these issues and proved to be an efficient method to improve τcongruent results for every simulation in table 2 albeit very minimal for q8 no updating to 10 minutes of updating simulations however the optimization of τcongruent was shown to be sensitive to the updating duration and could become counterproductive at a faster rate for the lower discharges computationally this is likely explained by the lower roughness values of the higher discharge simulations requiring longer updating to have the same positive effects additional reasoning may be due to the higher flow rate simulations having greater inundation during topographic acquisition and thus the larger area of uncertain bathymetric errors required greater time to resolve therefore modelers who apply this method should consider the quality of their data as well as i the hydraulic model parameterization of roughness ii morphological parameterization of sediment mobility iii the simulated discharge and iv the level of uncertainty in the topography i e higher discharges and greater areas of inundation resulting in higher uncertainty of river channel elevations 4 2 4 horizontal eddy viscosity evaluation calibrating the horizontal eddy viscosity altered the lateral distribution of velocity in the channel with higher horizontal eddy viscosity values producing lower velocity toward the center of the channels and thus higher predicted shear and transport rates during bed updating the initial horizontal eddy viscosity value of 0 1 m² s proved to be a good first attempt at the model parameterization however as shown in the results of table 3 an incorrect horizontal eddy viscosity value can have a significant impact on the shear agreement results despite having a high inundation agreement examining the τcongruent for the q16 shows a slightly improved τcongruent with the lowest horizontal eddy viscosity despite having a slightly worse inundation agreement this implies that the optimization of shear stress accuracy cannot be accomplished purely on optimized inundation agreement reached by concurrently calibrating roughness and horizontal eddy viscosity which is a method that many modelers might attempt or assume 4 3 implications for modelers and model development in preparation for a model simulation a modeler must consider the model s purpose desired accuracy and what calibration is necessary and or possible with available data sets in considering the horizontal eddy viscosity and bed updating presented in this paper most modelers will not have the supporting data to achieve such rigorous calibrations however while the improved τcongruent results may be small in comparison to the models overall agreements the potential to increase the τcongruent e g demonstrated to be up to 7 with horizontal eddy viscosity parameterization and up to 4 6 with bed updating are relatively significant when compared to improvements obtained in other lengthy parameterization sensitivity analysis e g grid size wetting and drying secondary flow etc perhaps what modelers with typical data sets can take from this paper is the value of horizontal eddy viscosity calibration on a shear prediction and selecting a lower horizontal eddy viscosity value would be erring on the side of caution further in considering bed updating allowing a brief period of bed updating was typically shown to improve the model s shear prediction but erring on the side of caution would be a brief updating period however further investigation of these applications would be best suited by research investigating how bed updating and horizontal eddy viscosity affects inundation and shear prediction in simulations with topographic data sets of varying uncertainty e g similar to legleiter et al 2011 recent studies have highlighted the dominant mechanisms for braided river evolution and planform maintenance and sediment supply specifically wheaton et al 2013 quantified several controlling mechanisms of which chute cutoff lobe dissection and bank erosion were the three most dominant in terms of changes to sediment storage as presented in this paper the prediction of braided river bed load produced mediocre results and other papers have highlighted the limits and current challenges of bank erosion e g stecca et al 2017 williams et al 2013 thinking forward to future model development bed load transport must be further assessed and developed to ensure that simulations are representing the dominant mechanisms and not entirely focused on bank erosion further in considering the importance of bank erosion for braided river mechanisms and sediment exchanges and the current limited model representation of bank erosion modelers seeking to optimize bank erosion must properly calibrate horizontal eddy viscosity parameterization specifically if a modeler assumes a higher than actual horizontal eddy viscosity the consequences will likely be lower velocities near the bank and thus reduced simulated bank erosion extending this work further investigation should use high spatial and temporal data sets to assess how the observed spatial inaccuracy of bed load occurrence translates into different erosion and deposition patterns and to what extent this may be addressed by model parametrization and or inclusion of an adaptation length 5 conclusion this paper presented the use of low cost data sets of high spatial and temporal resolution and a preview of new opportunities to calibrate and verify numerical models specifically this paper has presented techniques to capture bed load transport observations from braided flume experiments and their application for one of the first attempts to predict local bed load transport of the same topography and boundary conditions this technique has provided an evaluation of the predicted shear stress which is the foundation to accurately predict the sediment fluxes necessary for morphological simulations to limit the model input sources of uncertainty and error this study used a fixed bed configuration highly accurate topography and a high model resolution of 0 6 m 0 015 m unscaled which produced inundation accuracies amongst the highest reported for braided rivers 77 84 inundation agreement evaluating the ability to accurately predict mobile areas of a low medium and high discharge with the models total inundation and total shear areas at 100 1 as control variables model performance produced results between 51 and 63 active area agreement with only standard hydraulic model calibration included in these experiments and presented in this paper are i new techniques and methods to enable the comparison of quasi instantaneous bed load transport and topographic acquisition ii the quantification of brief morphological bed updating to potentially reduce inherent errors in the topographic data set used for fixed bed numerical modeling and iii the effects of horizontal eddy viscosity parameterization on bed load prediction while many considerations must be given in the use and application of a short duration bed updating the inclusion of such updating has been shown to have improvements up to 4 6 on the shear predicted bed load transport horizontal eddy viscosity parameterization demonstrated that various roughness and eddy viscosity value combinations could produce highly similar inundation performance yet could impact shear prediction agreement up to 7 with the inclusion of bed updating and horizontal eddy viscosity parameterization the shear prediction agreement ranged between 49 and 68 for the three discharge simulations while these results are seemingly mediocre these results are encouraging when considering the remaining assumptions uncertainties and inaccuracies that still exist in predicting this bed load activity most specifically is the uncertainty within the assumption of matching observed bed load to predicted shear stress without considering adaptation length effects within these confines perhaps more encouraging are the demonstrated data sets and new opportunities to use low cost yet highly accurate data sets that can reduce the temporal gaps that currently exist in morphological studies and thus approach the physical processes instead of cause and effect studies these data sets make possible the investigation of quasi instantaneous local patterns of sediment transport whereas most previous studies focus on spatially averaged and temporally integrated measures i e pre and post flood differenced elevation models in the context of how modelers typically use numerical models this paper illustrates the limits of inundation and depth based calibration the importance of horizontal eddy viscosity calibration and the challenges and limitations faced for accurately representing local physical processes in morphological models with limited access to the data necessary to adequately calibrate and verify a model e g instantaneous sediment transport matched with high resolution and temporal spatial topographic data sets modelers need to be aware of the assumptions and uncertainty involved how these are associated with model results and the inabilities for models to accurately predict shear stress in fixed bed configurations and morphology in mobile bed configuration acknowldgements this project has received funding from the european union s horizon 2020 research and innovation programme under the marie sklodowska curie grant agreement no 656917 special thanks to alfonso vitti for helping and advising the flume survey luca adami for his initial effort in using sfm in the flume and lorenzo forti and andrea bampi for their constant help in building and modifying the sfm acquisition platforms and for the flume technical support finally thank you to the anonymous journal referees who helped revise and improve the quality of this paper 
833,new data collection techniques offer numerical modelers the ability to gather and utilize high quality data sets with high spatial and temporal resolution such data sets are currently needed for calibration verification and to fuel future model development particularly morphological simulations this study explores the use of high quality spatial and temporal data sets of observed bed load transport in braided river flume experiments to evaluate the ability of a two dimensional model delft3d to predict bed load transport this study uses a fixed bed model configuration and examines the model s shear stress calculations which are the foundation to predict the sediment fluxes necessary for morphological simulations the evaluation is conducted for three flow rates and model setup used highly accurate structure from motion sfm topography and discharge boundary conditions the model was hydraulically calibrated using bed roughness and performance was evaluated based on depth and inundation agreement model bed load performance was evaluated in terms of critical shear stress exceedance area compared to maps of observed bed mobility in a flume following the standard hydraulic calibration bed load performance was tested for sensitivity to horizontal eddy viscosity parameterization and bed morphology updating simulations produced depth errors equal to the sfm inherent errors inundation agreement of 77 85 and critical shear stress exceedance in agreement with 49 68 of the observed active area this study provides insight into the ability of physically based two dimensional simulations to accurately predict bed load as well as the effects of horizontal eddy viscosity and bed updating further this study highlights how using high spatial and temporal data to capture the physical processes at work during flume experiments can help to improve morphological modeling keywords numerical modeling river morphodynamics structure from motion bed load mobility maps bed load transport symbols list δsf difference of saturation between time lapse imagery d 50 median sediment diameter dem digital elevation model dod dem of difference fitwe inundation effective width i e total inundation area match fitcongruent inundation congruent fit i e inundation local agreement gcp ground control points mae mean absolute error me mean error q8 q16 q24 discharges of 8 m3 s 16 m3 s and 24 m3 s 0 8 l s 1 6 l s and 2 4 l s unscaled respectively rmse root mean squared error sde standard deviation error sfm structure from motion smi sediment mobility images τcongruent shear active area congruency i e shear predicted mobility local agreement τcr critical shear stress τwe total effective shear active area i e total shear predicted mobility area match tls terrestrial lidar scanners topcat topographic point cloud analysis toolkit wse water surface elevation zmin minimum elevation for given grid cell derived from topcat 1 introduction numerical models have become a dominant tool for both science and applications in river management however like all tools numerical models have inherent inaccuracies that restrict their abilities models thus require continuous attention and refinement focusing on reach scale simulations the increase in two dimensional 2d numerical modeling has been enabled by increased computational processing power readily available affordable software with user friendly interfaces and the increased availability and ease of acquisition of affordable high resolution topography bates 2012 javernick et al 2016 rumsby et al 2008 specifically topographic data sets derived from terrestrial lidar scanners tls and close range structure from motion sfm can achieve point cloud resolution accuracy and precision on the millimeter scale james robson 2012 additional data collection of water depth velocity and bed mobility using point sampling techniques of stream gauging velocity meters echo sounders and acoustic doppler current profilers also provide valuable data sets for model testing e g rennie church 2010 williams et al 2015 such unprecedented data offers new opportunities to further evaluate numerical model performance as topographic uncertainty and or inaccuracy can greatly influence overall model performance legleiter et al 2011 braided rivers provide particularly challenging conditions for numerical modeling due to their multi channel flows subtle and complex relief partial inundation and high lateral mobility javernick et al 2016 williams et al 2013 numerous studies have shown that numerical models are able to simulate hydraulics and morphodynamics of braided rivers with topographic input or when allowed to self develop braided river networks e g mcardell faeh 2001 moron edmonds amos 2017 murray paola 1994 schuurman marra kleinhans 2013 achievements in numerical modeling over the last five years have benefited from such rich topographic data sets specifically studies have evaluated 2d model performance on braided river and produced good overall agreement in depth velocity and inundation extent javernick et al 2016 williams et al 2013 2016a morphodynamic models have produced comparable volumes of erosion and deposition when compared to observed data e g predicted volumes within a factor of two of observed data williams et al 2016a additional numerical model achievements include generation of meandering braided and anabranching river styles nicholas 2013a 2013b and insight on how various vegetation types and life stages influence river pattern and morphodynamics nardin et al 2016 oorschot et al 2016 despite these advances many challenges remain for morphological modeling examples include the description of physical processes mosselman 2012 computational costs transport formula accuracy and the inclusion of mixed grain size sediment hirano 1971 1972 williams et al 2016b riparian vegetation and wood effects bertoldi ruiz villanueva 2015 lauer et al 2016 bank erosion stecca et al 2017 and spatial and temporal discretization and the influences of small boundary and initial condition errors williams et al 2016a with the new opportunities to acquire low cost high spatial and temporal resolution topography modelers can properly calibrate instantaneous depth and inundation e g javernick et al 2016 nicholas et al 2012 williams et al 2013 spatial and temporal velocity data is less accessible as these surveys are very labor intensive and or cost prohibitive morphological data for model calibration remains very limited since repeated topographic data collection typically has weeks months or years between data sets e g lane et al 2010 lane westaway hicks 2003 moretto et al 2014 wheaton et al 2013 with this limited temporal data model calibration protocols rely on matching reach scale observed erosion and deposition patterns derived from digital elevation models dems and dems of difference dods that only capture the flood impacts rather than processes further these temporally poor data sets contain an accumulation of the complicated individual and interrelated process between hydraulics morphology vegetation and animals all of which are not well understood bertoldi et al 2015 javernick 2014 vargas luna et al 2015 due to a multitude of parameters to calibrate in both hydraulic and morphologic simulations modelers with limited data can reach a parameterization or several that produce a convincing match to observed data however it is nearly impossible to know if these results are numerical artifacts or represent physical phenomena mosselman le 2016 therefore it is necessary to acquire and use greater spatial and temporal resolution data sets that capture morphological processes rather than just post flood impacts to calibrate and verify numerical models further the use of data sets with greater spatial and temporal resolution will enable new opportunities in model testing and development which is currently lacking williams et al 2016b the intent of this paper is to explore new opportunities to calibrate evaluate and verify current model capabilities using high spatial and temporal resolution data sets specifically the main objectives were i to compare a 2d numerical model s predictions of bed load transport occurrence and location to observed data from laboratory scale gravel bed braided rivers and ii to evaluate the sensitivity of bed load prediction to varying horizontal eddy viscosity parameterization and bed updating 2 methods evaluating this model s ability to predict bed load transport was accomplished by comparing local instantaneous shear stress calculations with observed near instantaneous bed load activity that was derived from georeferenced time lapse imagery techniques redolfi et al 2017 numerical models based on the shallow water equations compute bed load transport as a function of the bed shear stress which results from resolving the flow hydrodynamics on the current bed configuration therefore prediction of the instantaneous bed load on a known topography does not require modeling of the bed evolution in time for this reason it was sufficient to use the numerical model in fixed bed mode without the need to select and calibrate a sediment transport equation observed and model predicted bed load transport was quantified at low medium and high flow rates the quantification of observed model performance required the creation of two new metrics that provided a measure of the total shear area of predicted mobility and the local accuracy of predicted mobility research methods used to observe model and compare instantaneous sediment transport are presented in the following sections 2 1 flume experiments flume experiments at the university of trento s hydraulic laboratory were conducted in a 3 m wide 24 m long flume the flume s adjustable gradient was set to 1 the sediment was uniform grain size of 1 mm and the channel width was restricted to 1 6 m to allow for data collection within the flume yet maintain a naturally developed braided planform bertoldi et al 2009 garcia lugo bertoldi henshaw gurnell 2015 in this research three flow rates were assessed a low medium and high flow of 0 8 l s q8 1 6 l s q16 and 2 4 l s q24 respectively each experiment started with a graded channel bed with a 0 01 m deep and 0 15 m wide center channel this configuration fully developed into a braided network over a period of 24 hours using a generic hydrograph with an initial discharge of 0 5 l s a rising limb duration of 8 hours a peak discharge of 3 4 l s with a duration of 1 h and a falling limb duration of 15 hours down to the initial discharge subsequently when measurements were acquired a constant discharge phase was run with the prescribed q8 q16 and q24 discharges 2 2 topographic acquisition and generation the flume s topography was generated using the sfm software agisoft photoscan version 1 2 2 2294 photographs were captured with a nikon d7100 camera that was fitted with a fixed 28 mm focal length lens vertical images were taken approximately 2 7 0 1 m above the floodplain with a rail system but with slight variations in orientation this acquisition resulted in an image object resolution of 0 6 mm nearly half the grain size each sfm data set required 150 images 3 images to cover width and 50 rows covering the length which produced approximately 70 side and forward overlap and took 12 14 minutes to capture ground control points gcps were placed at 1 m intervals along the flume s longitudinal axis with four gcps spaced across the transverse axis for a total of 100 gcps prior to the experiments the gcps were rigorously surveyed with a leica total station in reflectorless mode i e no prism used this approach allowed each gcp centroid to be measured directly this configuration has known errors of 2 mm however each gcp was surveyed at least three times from at least three different local network control points following the survey all gcp centroids were averaged to achieve the smallest errors possible average standard deviations between measurements of x y and z were 0 24 0 12 and 0 13 mm respectively following the initial gcp survey 423 ground truthing points were acquired in the flume using the total station in reflectorless mode the resulting gcp xyz data were used for the point cloud transformation and the ground truthing data was a validation of the sfm s elevation accuracy the equation set resolved by the numerical model is scale independent as long as the froude number is preserved i e providing that velocity discharge and time also are scaled accordingly so that scaling up or down the problem by any arbitrary factor does not affect the results unfortunately this is not the case in delft3d flow because the numerical scheme is not designed to reproduce small scale problems and it contains hidden thresholds and dimensional parameters that are hard coded in the algorithm erik mosselman personal communication 2018 therefore to ensure the numerical model stability and accuracy all physical data were geometrically upscaled to a dimension that is representative of natural gravel bed braided rivers specifically choosing a geometrical scale of 40 the physical model reproduces hydromorphological conditions of a river with median grain size d 50 of 40 mm and width of 64 m reasonable conditions of natural braided gravel bed rivers e g brasington et al 2003 javernick et al 2016 redolfi et al 2016a according to froude scaling principles e g young warburton 1996 if length is scaled by 40 discharge and time must be scaled by a factor 405 2 and 401 2 respectively throughout the rest of this paper all physical data will be presented in the scaled form for convenience in comparing to the numerical results e g scaled flow discharges are 8 1 m³ s for q8 16 2 m³ s for q16 and 24 3 m³ s for q24 sfm image acquisition of 12 minutes is scaled to 75 minutes flume width of 1 6 m is scaled to 64 m etc processing the sfm data in photoscan used the high settings for accuracy and quality once the scene reconstruction was completed gcps were automatically detected markers and xyz data were imported a linear transformation was achieved with the gcps and then further processed with the non linear camera alignment optimization following javernick et al 2014 2016 data were then exported as both orthoimagery and point clouds raw point clouds produced 85 100 million points with an average density of 600 points m² to intelligently reduce the point cloud resolution to a practical size for the numerical model the software topographic point cloud analysis toolkit topcat brasington et al 2012 rychkov et al 2012 was utilized and the elevation minimums zmin were used for topographic representation javernick et al 2016 based on visual inspections and confirmed with photoscan generated dems it was determined that a grid resolution of 0 6 m 0 015 m unscaled would appropriately represent the steepest banks therefore topcat processed all topography point clouds to a 0 6 m resolution as documented by javernick et al 2014 2016 sfm is not fully capable of modeling the elevation of inundated areas due to light attenuation reflection and dynamic surfaces however due to the shallow inundation of the flume relatively fast image acquisition and absences of glint due to the flumes full enclosure and oblique led light strips it was anticipated that sfm could model the inundated channels relatively accurately in order to test this accuracy the braided planform was acquired with sfm in dry conditions and then repeated following an inundation of the flume to reach a steady discharge of 24 3 m³ s comparing the elevations of dry conditions to the elevations of wet areas enabled a determination of how well the sfm could model the inundated river beds however due to the mobile sediment extreme caution was required to avoid areas of significant change caused during the inundation period given the known issues with sfm and inundation area detection the sfm bathymetric refraction correction method of dietrich 2017 was also evaluated with the same sfm data sets in preparation to calibrate the numerical model s hydraulics based on observed water depth and inundation data it was necessary to map the water surface elevation wse of each experiment however visual identification of a wet to dry boundary for water surface mapping was difficult due to the saturated sand i e uniform sand color and low relief therefore using a typical method of imagery based breakline placement could not produce a continuous wse to reduce possible local errors due to wse misrepresentation a protocol was established that mapped the wse with only points that were chosen to confidently represent the wet dry boundary and with relatively low relief this approach helped to avoid the repercussions of choosing a point on a steep bank that might exaggerate the wse this produced a less than ideal point density for wse mapping i e several dozen wse points instead of hundreds but added confidence in individual points the wse protocol then proceeded to i generate a wse raster from the wse points and ii subtract the sfm topcat generated raster from the wse raster data set which provided an inundation map with depth data this method worked well except in a few shallow riffle areas which caused disconnected channels therefore in areas that were obviously inundated the inundation map was manually edited to connect such mistakes further areas that were groundwater fed were manually removed from the inundation map since the numerical model would not be able to predict groundwater fed channels finally several hundred points were randomly selected from the depth raster to compare the observed and modeled depth data 384 600 and 481 points for the q8 q16 and q24 simulations respectively for model calibration 2 3 time lapse camera acquisition process and output data observations of bed load motion using time lapse images taken by fixed cameras were obtained following the methods of redolfi et al 2017 images were acquired with two different slr cameras equipped with 18 mm lenses and provided a pixel size of 0 04 m 1 mm unscaled the area captured in the images was the central most section of the flume that covered a 280 m long reach 7 m unscaled this central location provided adequate distance from the upstream and downstream flume boundaries 340 m i e an adequate run in length for numerical modeling the camera sensitivity was set to minimize image noise while exposure time and aperture were chosen to ensure a correct exposure and focusing of the image in addition time lapse images were captured at intervals of 6 3 minutes 1 minute unscaled and provided sufficient time resolution to capture all the relevant changes of the active area images were geometrically transformed to correct the radial distortion introduced by the lenses and colors were automatically adjusted to obtain balanced images using 10 gcps a fully automated orthorectification was produced and the rgb image was converted to hue saturation value color space and differentiated into subsequent saturation maps finally obtaining maps of filtered differences of saturation δsf that exceeded the minimum level of detection in this case we set a threshold level of 18 255 produced the bed load transport images hereafter these transport images will be referred to as sediment mobility images or smi methods had to be developed to compare the smi with the synchronized sfm data sets since the sfm took 75 minutes to capture the entire flume of which 19 minutes 3 minutes unscaled were spent covering the time lapse reach it was not practical to use just one observation of smi because the topography was captured over several observations that included bed load transport and slight morphology changes therefore the sfm acquisition time of 19 minutes was assumed to represent a time averaged topography and required a similar time averaging of observed sediment mobility this averaged mobility was achieved by taking three smis synchronous with the sfm acquisition i e one before during and after sfm passing of the time lapse camera which was assumed to best represent the varying topography and provided the same time averaging duration to produce one representative mobility the three smis were converted into one mobility image through the following process i each smi active spatial extent was mapped with polygons ii the active area polygons were extracted to grid cells and iii a final representative smi was generated by combining the active grid cells that had occurred at least twice fig 1 in examining the final smi results areas with shadows e g small areas along the flume boundaries indicated sediment mobility even in areas where motion was not feasible i e dry areas therefore a final manual editing was required to remove any shaded area with obviously erroneous data however these shaded areas were limited to narrow strips along the left and right flume boundaries the final active area map was regarded with high confidence since the identified active area was observed to be active in at least two smi and erroneous data was easily identified and limited to shaded areas 2 4 numerical model delft3d flow hereafter delft3d was the model chosen based on its hydraulic and morphological abilities to model braided rivers with highly accurate topographic data sets e g javernick et al 2016 williams et al 2016a delft3d offers both 2d and 3d hydrodynamic simulations based on non steady navier stokes equations with shallow water and boussinesq approximations deltares 2010 this research used 2d depth averaged flow simulations for the horizontal flow conditions and did not consider 3d simulations due to the shallow water and computational demands the model setup utilized the pre determined 0 6 m grid resolution the model s grid encompassed the entire flume length 936 m which included a single flow entrance from the upstream as present in the flume the downstream boundary condition was satisfied by extending the downstream length and applying the discharge to water surface elevation relationship for an equivalent trapezoidal channel due to relatively high froude number the downstream boundary conditions have negligible effects on the study reaches hydraulics 2 5 model evaluation evaluation of the numerical model s ability to match observed bed load transport data observed data discussed in section 2 3 followed the sequence of i establishing a hydraulic calibration and evaluating the model s abilities to match the observed inundation and depth ii quantifying the agreement between the modeled shear stress and the observed active area and iii evaluating the shear prediction accuracy with additional horizontal eddy viscosity parameterization and brief morphological bed updating 2 5 1 model hydraulic calibration initial delft3d calibration followed javernick et al 2016 through parameterization of the bed roughness using nikarduse roughness length values between 0 3 m and 0 8 m and compared the predicted versus observed water inundation and depth horizontal eddy viscosity was set to 0 1 m² s and taken as a general first approach e g similar to the reports of javernick et al 2016 williams et al 2016a simulation performance was measured by depth residual errors calculated with root mean squared error rmse mean absolute error mae mean error me and standard deviation of error sde the simulation s inundation and flow routing were compared to the sfm orthoimagery data using the effective width fitwe and the congruent fit fitcongruent following ashmore and sauks 2006 and williams et al 2013 1 f i t w e i a s i m i a o b s v 2 f i t c o n g r u e n t i a o b s v i a s i m i a o b s v i a s i m where iasim and iaobsv are the simulated and observed total wetted areas respectively effective width results 100 indicate under predicted inundation while values 100 indicate over predicted inundation congruent fit results are 100 where 100 indicates total agreement between predicted and observed data and decreasing values indicate progressively greater inaccuracy in pursuing the optimal model and ensuring a control to compare each simulation the calibrations targeted a 100 fitwe and performance was focused on fitcongruent results 2 5 2 quantification of shear and active area agreement to compare the modeled shear stress to the observed active area the modeled shear stress at cell centers were separated into two data sets of i shear that occurred within the observed active area and ii shear data that occurred outside the observed active area these two data sets were then plotted as two histograms of shear stress overlaying the histograms produced distinct plots with moderate overlap for the predicted active areas within and outside the observed active area and the intersection of the two histograms was interpreted as an indicator of the critical shear stress τcr this method provided a cell by cell distribution of local shear stress in the process of sediment transport the τcr value is a physical parameter however the modeled τcr was treated as a calibration parameter i e similar to the roughness parameter but this calibration required a metric to quantify the overall agreement between observed and predicted active areas therefore the authors created two metrics based on the inundation metrics of fitwe and fitcongruent to quantify the total effective shear active area 3 τ w e a a s i m a a o b s v and shear active area congruency 4 τ c o n g r u e n t a a o b s v a a s i m a a o b s v a a s i m where aasim and aaobsv are the simulated and observed active areas of τ τc r τwe results 100 indicate under predicted total shear stress active area and τcongruent of 100 indicates total agreement between predicted and observed data in pursuing model calibration and ensuring a control to compare each simulation an alternative to the histogram inferred τcr was to determine the τcr value that produced a near 100 τwe i e similar to roughness parameterization for fitwe optimization similar to the inundation calibration the performance was focused on τcongruent results this approach provided a critical shear stress estimated for a reach scale both the histogram intersection method and the τwe calibration methods were used to determine τcr however it was recognized that the τwe method had the advantage of holding each simulation to near 100 τwe which provided the opportunity to compare different simulation results and the effects of horizontal eddy viscosity parameterization and bed updating therefore τwe was the preferred method for the added opportunity to compare different model calibrations and techniques but the histogram method was also considered 2 5 3 horizontal eddy viscosity and bed updating effects model calibration thus far has been a generic parameterization of bed roughness however the effects of a bed updating technique and horizontal eddy viscosity parameterization on shear stress calculations was evaluated with the newly defined τwe and τcongruent metrics bed updating examining the delft3d shear results spatial shear values showed erratic spikes and lows in the wetted channels fig 2 this was assumed to be due to sudden elevation changes in the wetted channels possibly due to inherent sfm errors and sfm errors compounded by bathymetric correction factors causing deeper areas to combat this issue the model was allowed a brief period of the mobile bed configuration where the river bed was allowed to update i e bed updating for durations of 10 and 20 minutes this method has been previously discussed amongst modelers as a method to improve the topographic representation for fixed bed models personal communication with murray hicks and richard measures niwa 2013 but to our knowledge has not been quantitatively investigated allowing a brief period of bed mobility provided a physically based approach to remove the erratic shear spikes as opposed to applying a uniform smoothing or subjective manual approach while this brief bed updating did cause channel bed smoothing significant morphological changes e g channel migration did not occur given that this period was brief and focused on bed smoothing it was not necessary to pursue a specific morphological calibration instead the authors enabled the mobile bed configuration using the standard meyer peter and muller 1948 formula and used default morphologic parameter values 2 5 4 horizontal eddy viscosity calibration horizontal eddy viscosity parameterization was tested by changing the initial value of 0 1 m² s by a factor of 5 and tested the values of 0 02 and 0 5 m² s all simulations and data were reprocessed following the previous method sections and were compared by keeping the fitwe and τwe near 100 but allowed to vary by 1 to avoid exhaustive modeling efforts 3 results 3 1 sfm topography and bathymetry the best transformation accuracies were achieved with the non linear camera optimization transformation similar to javernick et al 2014 and achieved absolute gcp x y z errors between 0 039 0 065 m 0 018 0 023 m and 0 012 0 022 m 0 98 1 63 mm 0 45 0 58 mm 0 30 0 55 mm unscaled respectively results of the separate ground truthing points produced residual me of 0 077 m mae of 0 084 m rmse of 0 148 m and sde of 0 127 m unscaled errors of me 1 9 mm mae 2 1 mm rmse 3 7 mm and sde 3 2 mm sfm results for the inundated channels produced a residual me of 0 121 m 3 03 mm unscaled wet sfm elevations minus dry sfm with positive errors indicating elevations higher than the dry sfm and sde 0 077 m 1 93 mm unscaled results from the dietrich 2017 method produced an improved me of 0 015 m 0 38 mm unscaled with sde 0 097 m 2 43 mm unscaled therefore the dietrich 2017 method was applied to all inundated topographic data for simulations in assessing the inherent sfm error and the discussed challenges to identify the wet dry boundary from orthoimagery and sfm data it was determined that water surface elevations errors are to be treated with the mae of 0 084 m 2 10 mm unscaled for this reason water depths below 0 1 m were not considered reliable and thus were not assessed in inundation or depth results the implications of this threshold did restrict the model s evaluation in very shallow areas but is considered comparable to field data collection and modeling scenarios where instruments frequently have minimum operating depth limits e g echo sounders frequently requiring 0 1 m depth 3 2 model evaluation 3 2 1 initial calibration initial calibrations to achieve 100 fitwe required significantly different roughness lengths for the three discharge simulations as shown in table 1 each of the simulations have higher ks than previously reported simulations of shallow braided river systems and required higher ks values for the progressively lower flow rates however higher roughness values associated with lower discharges is not unprecedented e g kim et al 2010 inundation results produced agreements fitcongruent between 77 and 84 which compares well to the findings of javernick et al 2016 and williams et al 2016a who both had high resolution high accuracy topographic field data and thus indicating that the methods utilized are comparable depth errors all show the modeled depth to be on average higher than the observed depth but are comparable to the inherent topographic errors of 0 1 m 2 5 mm unscaled spatial inundation and agreement quantification of the three simulated flow rates are shown in fig 3 all simulations had an effective width fitwe between 99 0 and 99 7 while the q8 and q16 simulations had 77 agreements the q24 simulation reached an agreement of 84 inundation performance was impaired in areas of shallow depth these areas include point bars backwater alcoves small anabranches shallow depths surrounding bars and occasionally along the banks these mismodeled areas are highlighted by examining the depth errors simulated depth minus observed depth in fig 4 comparison between figs 3 and 4 shows several cases where over predicted depth in a channel was matched by under predicted depth in nearby parallel channels and vice versa these instances suggest upstream flow partitioning issues further several depth errors that exist in fig 4a b and c shows narrow bands along banks these depth errors are likely due to the observed inundation being mapped with higher resolution imagery while the model simulation is restricted to the 0 6 m resolution discrete grid 3 2 2 shear evaluation using two methods to estimate the τcr provided comparable results the histogram method for the three different simulations are shown in fig 5 and have significant overlap between 20 and 50 n m² this overlap precluded defining a sharp threshold while the intersection could be a reasonable option to identify τcr and with this choice a value of around 35 n m² shields number 0 056 can be obtained for each of the three simulations however the method of calibrating τwe to approximately 100 provided a clear τcr value which was always very close to the histogram indicated value therefore the calibrated τwe determined τcr is the only results presented hereafter shear evaluation results for the q8 simulation are shown in fig 6 the q8 calibrated τwe of 100 2 predicted a τcr value of 37 2 n m² shields number 0 058 which results in a τcongruent of 51 3 evaluation of the spatial modeled τcr exceedance i e predicted mobility in fig 6b and c compared to the observed in fig 6a showed the area matches well in the upstream half main channel but with some disagreements in the initial upper most section near the longitudinal boundary along banks fig 6c area a and point bars fig 6c area b after the bifurcation in the downstream half of the reach the predicted mobility occurs over the shallow area on the true right fig 6c area c but then fails to consistently match the observed fig 6c area d including a modeled shear disconnect in the riffle located in the center of the planform fig 6c area e shear evaluation results for the q16 simulation are shown in fig 7 the q16 calibrated τwe of 100 0 produced a τcr value of 31 9 n m² shields number 0 049 which results in a τcongruent of 63 4 examining the spatial shear results in fig 7b and c compared to the observed of fig 7a the predicted mobility was more frequent in the upper reach s left anabranch where little motion was observed fig 7c area f similar to the q8 model modeled shear had disconnected in riffles fig 7c areas g and h and was misrepresented along banks and point bars in the downstream half of the reach the flow is split between a dominant right channel and a narrow left anabranch fig 7c area i the main channel has good agreement but the anabranch is not connected through the observed bifurcation instead a modeled bifurcation exists further downstream from the observed fig 7c area i which later connects the observed and predicted mobility in the narrow anabranch fig 7c area j however predicted mobility extends outside the narrow anabranch s observed boundary and throughout the remaining downstream extents fig 7c area k finally the small true right anabranch that extends the entire reach length fig 7c area l shows several areas of predicted mobility yet only small patches of observed mobility exist overall the agreement is strong in areas of deeper channel flow with degraded performance along riffles and other smaller channels and shallow depths the q24 shear evaluation results are shown in fig 8 where the τwe of 100 1 provided a τcr value of 36 2 n m² shields number 0 056 which resulted in a τcongruent of 58 4 examining the spatial shear modeled in fig 8b and c compared to the observed in fig 8a and similar to the other experiments there is good agreement for the majority of channels and planform however predicted mobility continues to be misrepresented in shallow areas e g fig 8c area m upstream of bifurcations e g fig 8c area n and occasionally well outside the observed boundary e g fig 8c area o in contrast to the low and medium discharge simulations this simulation s riffle areas show predicted mobility 3 2 3 bed updating results for the inclusion of bed updating are shown in table 2 where the 10 minutes of updating improved the shear agreement in all the simulations compared to the original no bed updating albeit only a slight improvement for the q8 simulation examining the shear agreement for 10 to 20 minutes updating the q8 and the q16 results deteriorated while the q24 continued to slightly improve in general the q8 simulation showed minimal improvement for the updating while the q16 and q24 simulations improved by 3 4 due to bed updating the bed updating simulations also required larger calibrated values of τcr compared to no updating simulations except the q16 s 20 minutes of updating finally examining the modeled shear results for the bed updating showed that there was a noticeable improvement in shear prediction in the riffle areas for the q8 and q16 simulations 3 2 4 horizontal eddy viscosity calibration viscosity parameterization and the impacts on shear prediction with varying updating was simulated however only the 10 min of bed updating results are shown for the three values of discharge in table 3 which typically produced the best results albeit not significant improvements giving calibration priority of fitwe of 100 and holding the τwe near 100 the various model simulations show that higher viscosity values require lower roughness values to maintain model inundation performance however while the inundation agreements can be maintained by altering the roughness the effects on the shear stress predictions can be relatively significant this is caused by the changes in the velocity flow field with higher velocity and thus shear stress in the channel center for lower values of the viscosity comparing the 0 5 m2 s and 0 1 m2 s eddy viscosity simulations for all three discharges the 0 5 m² s simulations τcongruent decreased by 4 8 and required a much lower τcr approximately reduced by 20 conversely the 0 02 m² s eddy viscosity had very similar results to the 0 1 m² s but generally with slightly lower values of τcongruent 1 2 apart from the q24 runs that had the same values 4 discussion 4 1 topographic acquisition and generation in generating the flume topography with sfm an ambitious effort was undertaken to achieve an unscaled millimeter accuracy through redundant gcp placement and rigorous survey the millimeter accuracy was achieved geometrically scaling the flume topography by 40 produced average errors of 0 08 m which are well within the best of reported sfm acquisition and accuracy of river field surveyed dems e g fonstad et al 2013 javernick et al 2014 these results represent an improvement compared to recent findings by morgan et al 2017 who reported a significant parabolic distortion of the longitudinal profile measured along a 9 m long flume in this study uniformly distributed errors of frequently occurring parabolic distortion have not been observed likely due to the gcp location redundancy and accurate total station reflectorless mode survey evaluating the sfm topographic performance of the wet versus dry conditions the sfm without assistance represented the bed channel with average elevation errors of 0 12 m indicating an under predicted water depth such results were anticipated and dietrich 2017 method did prove to be valuable in correcting the water depth as noted in the inundation mapping areas that were incorrectly mapped were riffles this result could be from the generally shallow inundation a locally steep water surface slope not adequately mapped with the wse method and or the sfm bathymetry was not mapped correctly in these areas examining the raw sfm point clouds the riffles typically have elevation spikes considering the sfm techniques this is likely due to the riffles dynamic water surface which hinders an accurate sfm image matching technique e g overlapping images capturing varying water surface elevations and changes in the surface angles causing different reflection and refraction given greater noise in riffles the topcat zmin has less reliable data to process and an accurate representation of the inundated bed was reduced nevertheless the sfm data with bathymetric correction and topcat filtering method did produce a high resolution and high accuracy topographic data set for the numerical model simulation and proved to be an efficient and low cost method 4 2 model evaluation 4 2 1 hydraulic calibration evaluation initial model roughness parameterization with a near 100 fitwe produced fitcongruent results between 77 and 84 which are comparable to reports of braided river simulations with high resolution and accurate field collected topographic data sets 60 84 fitcongruent demonstrated by javernick et al 2016 and williams et al 2013 water depth data for each experiment was higher on average than observed me 0 04 to 0 06 m and mae 0 09 to 0 12 m but well within the water surface elevation uncertainty range of 0 1 m assessing inundation for the three discharge simulations showed that performance typically degraded in areas surrounding bars and misrepresentation of small anabranches point bars and back water alcoves however backwater areas may have had groundwater discharge sources the model does not incorporate as previously discussed in javernick et al 2016 simulation of shallow water inundation routing can be very sensitive to subtle topographic inaccuracies inherent topographic misrepresentation due to grid resolution and general model flow prediction accuracy nevertheless as indicated by the fitcongruent results the initial calibration performed well for the complex conditions examined in this study moreover as flow increases and depth rises very slight in a braided river the overall routing and subtle topographic errors become less influential however roughness calibration depends also on the value of the eddy viscosity with several combinations producing similar results in terms of fitcongruent eddy viscosity is a challenging parameter to calibrate as it mainly affects the lateral distribution of velocity with only minor effects on the flow depth unfortunately velocity data are rare even at laboratory scale making it difficult to independently calibrate the eddy viscosity 4 2 2 shear evaluation direct observations of bed load transport occurrence and spatial location offered the opportunity to evaluate the 2d numerical model s ability to correctly reproduce bed load transport patterns in a braided river this is particularly relevant considering that sediments in braided rivers are transported only over a small fraction of the channel width even at high values of discharge ashmore et al 2011 garcia lugo et al 2015 a correct reproduction of the spatial and temporal patterns of bed load activity is therefore a necessary condition to model their morphological evolution such an assessment of the instantaneous bed load transport is unprecedented and highlights several issues that need further investigation acoustic doppler current profiler adcp data sets of previous studies e g rennie church 2010 williams et al 2015 may have provided similar opportunities however the presented flume experiment s data provided a unique opportunity to evaluate a simplified and controlled environment i e uniform sediment constant flow rate clearly observed sediment mobility etc as well as higher flow rates which can be considered as analogous to peak flow conditions local accuracy of bed load occurrence evaluated through the parameter τcongruent was mediocre and ranged between 51 and 63 figs 6c 7c and 8c unlike the inundation fitcongruent the τcongruent did not increase with increased flow rate which indicated that the inundation agreement is not a proxy to indicating bed load agreement there are several issues that may explain these mediocre performances first most mathematical models including delft3d assume that bed load transport is only a function of the local conditions this hypothesis can be limiting in strongly spatially and temporally unsteady conditions typical of braided rivers where it may be worth accounting for non local effects by including an adaptation length parameter adaptation length parameters in numerical models attempt to include non equilibrium sediment transport which quantifies the travel distance required for a packet of sediment to reach a new equilibrium concentration when it moves into a region of higher and lower shear stress lai gaeuman 2013 p 1 gaeuman et al 2015 recently reported adaptation lengths of up to 100 particle diameters which could be relevant considering the high resolution of the topographic and bed load activity data sets future model developments should consider the inclusion of adaptation lengths for bed load transport second some physical property effects are not accounted in the model that may be relevant at the laboratory scale specifically water viscosity surface tension and low reynolds number may cause slight discrepancies however these effects will be limited to the areas of relatively small velocities and depths i e anabranches since these areas have limited bed load transport occurrences these discrepancies are likely restricted to small areas and do not significantly impact the model prediction performance third the imagery technique used to map bed load activity may introduce further errors the combined use of three smi images is an attempt to reduce possible random errors as well as to match the time interval needed to survey the reach with sfm while it is acknowledged that faster topographic acquisition and a single sediment mobility image would reduce the inherent method inaccuracies the sources or errors are anticipated to be minimal given the short intervals between data sets relative to bed activity demonstrated in the three smi images of fig 1 both the analysis of the shear stress frequency distributions fig 5 and the numerical model calibration of the critical threshold for transport initiation to match the observed averaged active width resulted in similar values of the critical shields number estimated in the range 0 05 0 065 for results in table 2 and range between 0 04 and 0 07 for table 3 possible causes to the high shields values could include low reynolds numbers implying the flume experiments not being fully turbulent and associated scaling issues and or the flume sediment being more rounded than typical gravel grains and thus implying that the same diameter can be related to a larger mass the use of uniform sediment excluded issues related to armoring grain imbrication and other sediment sorting patterns while the τcr is a physical value for bed load processes this value was treated as a calibration parameter in the numerical model this assumption was justified based on the same approach as bed roughness values which in the physical world are defined values yet are calibration parameters in numerical models further calibrating the τcr value to near 100 τwe offered an experiment control to compare each simulation s predictive performance and the slight parameterization was consideredworthwhile examining figs 6 8 the q8 and q16 simulations had areas where predicted mobility through riffles was disconnected this may be due to the shallow water areas and or the sfm issues associated with riffles outlined in the topography discussion and thus influence depth and velocity alternatively these disconnected areas could be due to the lack of adaptation length conversely the shear agreement performed best in the main channels of simpler planform and greater depth this is demonstrated in the upstream half of the q8 results and for the entire q16 simulation reasons for the q16 s higher τcongruent may be attributed to its planform which was dominated by a main channel in the upstream and downstream end and the midstream section s bifurcation occurs with relatively even flow partitioning this less complex system with greater flow concentration in defined channels likely has higher τcongruent agreement because the bed load transport is occurring and would continue to occur with less dependence on the additional factors that cause bed load transport e g adaptation length alternatively in shallow and slow flow areas near greater depth and velocity areas adaptation length may likely be more influential which could explain the bed load activity extending in and through areas of lower shear stress these issues highlight the challenges of shallow depth adaptation length and bifurcation calculations as many papers have discussed bifurcation is complex and strongly imbalanced water and sediment and a correct reproduction is crucial for numerical modeling advancement e g bertoldi 2012 bolla pittaluga repetto tubino 2003 kleinhans et al 2008 redolfi et al 2016b conversely predicted mobility occurs periodically outside the observed active area in all simulations this may be due to having τwe of 100 which assumed that τcr was the sole predictor to sediment mobility and thus included larger areas than actuality such an increase in area would contain lower shear stress values and thereby reduce the τcr estimate consequently if the predicted τcr is lower than reality the already present high shields values would increase while many of the uncertainties e g hydraulic model equations accuracies smi representation bed load prediction based solely on shear stress and inaccuracies e g sfm inherent errors have been discussed and are likely to have minimal effects these issues collectively degrade the model s predictive capabilities however within the confines of what the fixed bed model can predict for inundation agreement of a complicated braided river and with shear stress values alone these seemingly mediocre τcongruent results are encouraging while the areas of inaccuracies are yet to be understood viewing these data and figures may also provide an initial analysis of where τcr is not the sole factor for bed load transport and the necessity of correct partitioning initial incipient motion and adaptation length 4 2 3 bed updating evaluation despite the accurate topography and high resolution model grid the bathymetric representation produced exaggerated shear stress values that were assumed to be due to subtle bed elevation spikes and troughs from inherent sfm and bathymetric modifications and grid resolution misrepresentation using the model s morphological updating offered an unbiased physically based approach to reducing these issues and proved to be an efficient method to improve τcongruent results for every simulation in table 2 albeit very minimal for q8 no updating to 10 minutes of updating simulations however the optimization of τcongruent was shown to be sensitive to the updating duration and could become counterproductive at a faster rate for the lower discharges computationally this is likely explained by the lower roughness values of the higher discharge simulations requiring longer updating to have the same positive effects additional reasoning may be due to the higher flow rate simulations having greater inundation during topographic acquisition and thus the larger area of uncertain bathymetric errors required greater time to resolve therefore modelers who apply this method should consider the quality of their data as well as i the hydraulic model parameterization of roughness ii morphological parameterization of sediment mobility iii the simulated discharge and iv the level of uncertainty in the topography i e higher discharges and greater areas of inundation resulting in higher uncertainty of river channel elevations 4 2 4 horizontal eddy viscosity evaluation calibrating the horizontal eddy viscosity altered the lateral distribution of velocity in the channel with higher horizontal eddy viscosity values producing lower velocity toward the center of the channels and thus higher predicted shear and transport rates during bed updating the initial horizontal eddy viscosity value of 0 1 m² s proved to be a good first attempt at the model parameterization however as shown in the results of table 3 an incorrect horizontal eddy viscosity value can have a significant impact on the shear agreement results despite having a high inundation agreement examining the τcongruent for the q16 shows a slightly improved τcongruent with the lowest horizontal eddy viscosity despite having a slightly worse inundation agreement this implies that the optimization of shear stress accuracy cannot be accomplished purely on optimized inundation agreement reached by concurrently calibrating roughness and horizontal eddy viscosity which is a method that many modelers might attempt or assume 4 3 implications for modelers and model development in preparation for a model simulation a modeler must consider the model s purpose desired accuracy and what calibration is necessary and or possible with available data sets in considering the horizontal eddy viscosity and bed updating presented in this paper most modelers will not have the supporting data to achieve such rigorous calibrations however while the improved τcongruent results may be small in comparison to the models overall agreements the potential to increase the τcongruent e g demonstrated to be up to 7 with horizontal eddy viscosity parameterization and up to 4 6 with bed updating are relatively significant when compared to improvements obtained in other lengthy parameterization sensitivity analysis e g grid size wetting and drying secondary flow etc perhaps what modelers with typical data sets can take from this paper is the value of horizontal eddy viscosity calibration on a shear prediction and selecting a lower horizontal eddy viscosity value would be erring on the side of caution further in considering bed updating allowing a brief period of bed updating was typically shown to improve the model s shear prediction but erring on the side of caution would be a brief updating period however further investigation of these applications would be best suited by research investigating how bed updating and horizontal eddy viscosity affects inundation and shear prediction in simulations with topographic data sets of varying uncertainty e g similar to legleiter et al 2011 recent studies have highlighted the dominant mechanisms for braided river evolution and planform maintenance and sediment supply specifically wheaton et al 2013 quantified several controlling mechanisms of which chute cutoff lobe dissection and bank erosion were the three most dominant in terms of changes to sediment storage as presented in this paper the prediction of braided river bed load produced mediocre results and other papers have highlighted the limits and current challenges of bank erosion e g stecca et al 2017 williams et al 2013 thinking forward to future model development bed load transport must be further assessed and developed to ensure that simulations are representing the dominant mechanisms and not entirely focused on bank erosion further in considering the importance of bank erosion for braided river mechanisms and sediment exchanges and the current limited model representation of bank erosion modelers seeking to optimize bank erosion must properly calibrate horizontal eddy viscosity parameterization specifically if a modeler assumes a higher than actual horizontal eddy viscosity the consequences will likely be lower velocities near the bank and thus reduced simulated bank erosion extending this work further investigation should use high spatial and temporal data sets to assess how the observed spatial inaccuracy of bed load occurrence translates into different erosion and deposition patterns and to what extent this may be addressed by model parametrization and or inclusion of an adaptation length 5 conclusion this paper presented the use of low cost data sets of high spatial and temporal resolution and a preview of new opportunities to calibrate and verify numerical models specifically this paper has presented techniques to capture bed load transport observations from braided flume experiments and their application for one of the first attempts to predict local bed load transport of the same topography and boundary conditions this technique has provided an evaluation of the predicted shear stress which is the foundation to accurately predict the sediment fluxes necessary for morphological simulations to limit the model input sources of uncertainty and error this study used a fixed bed configuration highly accurate topography and a high model resolution of 0 6 m 0 015 m unscaled which produced inundation accuracies amongst the highest reported for braided rivers 77 84 inundation agreement evaluating the ability to accurately predict mobile areas of a low medium and high discharge with the models total inundation and total shear areas at 100 1 as control variables model performance produced results between 51 and 63 active area agreement with only standard hydraulic model calibration included in these experiments and presented in this paper are i new techniques and methods to enable the comparison of quasi instantaneous bed load transport and topographic acquisition ii the quantification of brief morphological bed updating to potentially reduce inherent errors in the topographic data set used for fixed bed numerical modeling and iii the effects of horizontal eddy viscosity parameterization on bed load prediction while many considerations must be given in the use and application of a short duration bed updating the inclusion of such updating has been shown to have improvements up to 4 6 on the shear predicted bed load transport horizontal eddy viscosity parameterization demonstrated that various roughness and eddy viscosity value combinations could produce highly similar inundation performance yet could impact shear prediction agreement up to 7 with the inclusion of bed updating and horizontal eddy viscosity parameterization the shear prediction agreement ranged between 49 and 68 for the three discharge simulations while these results are seemingly mediocre these results are encouraging when considering the remaining assumptions uncertainties and inaccuracies that still exist in predicting this bed load activity most specifically is the uncertainty within the assumption of matching observed bed load to predicted shear stress without considering adaptation length effects within these confines perhaps more encouraging are the demonstrated data sets and new opportunities to use low cost yet highly accurate data sets that can reduce the temporal gaps that currently exist in morphological studies and thus approach the physical processes instead of cause and effect studies these data sets make possible the investigation of quasi instantaneous local patterns of sediment transport whereas most previous studies focus on spatially averaged and temporally integrated measures i e pre and post flood differenced elevation models in the context of how modelers typically use numerical models this paper illustrates the limits of inundation and depth based calibration the importance of horizontal eddy viscosity calibration and the challenges and limitations faced for accurately representing local physical processes in morphological models with limited access to the data necessary to adequately calibrate and verify a model e g instantaneous sediment transport matched with high resolution and temporal spatial topographic data sets modelers need to be aware of the assumptions and uncertainty involved how these are associated with model results and the inabilities for models to accurately predict shear stress in fixed bed configurations and morphology in mobile bed configuration acknowldgements this project has received funding from the european union s horizon 2020 research and innovation programme under the marie sklodowska curie grant agreement no 656917 special thanks to alfonso vitti for helping and advising the flume survey luca adami for his initial effort in using sfm in the flume and lorenzo forti and andrea bampi for their constant help in building and modifying the sfm acquisition platforms and for the flume technical support finally thank you to the anonymous journal referees who helped revise and improve the quality of this paper 
834,we propose a new pilot points method for conditioning discrete multiple point statistical mps facies simulation on dynamic flow data while conditioning mps simulation on static hard data is straightforward their calibration against nonlinear flow data is nontrivial the proposed method generates conditional models from a conceptual model of geologic connectivity known as a training image ti by strategically placing and estimating pilot points to place pilot points a score map is generated based on three sources of information i the uncertainty in facies distribution ii the model response sensitivity information and iii the observed flow data once the pilot points are placed the facies values at these points are inferred from production data and then are used along with available hard data at well locations to simulate a new set of conditional facies realizations while facies estimation at the pilot points can be performed using different inversion algorithms in this study the ensemble smoother es is adopted to update permeability maps from production data which are then used to statistically infer facies types at the pilot point locations the developed method combines the information in the flow data and the ti by using the former to infer facies values at selected locations away from the wells and the latter to ensure consistent facies structure and connectivity where away from measurement locations several numerical experiments are used to evaluate the performance of the developed method and to discuss its important properties keywords pilot points multiple point statistics mps model calibration geologic facies ensemble kalman filter ensemble smoother 1 introduction calibration of subsurface flow models against scattered non linear well response data typically leads to an inverse problem to infer hydrological properties of the formation and to enable more accurate prediction of future flow and transport behavior a difficulty in formulating and solving model calibration inverse problems is related to the large number of parameters that need to be estimated from limited scattered measurements making the problem ill posed carrera and neuman 1986 carrera et al 2005 kitanidis 1986 mclaughlin and townley 1996 to overcome problem ill posedness additional assumptions about the solution are introduced into the problem formulation one approach is to constrain the solution through regularization functions to impose a pre specified spatial structure e g smoothness on the distributed formation properties carrera et al 2005 yeh 1986 an alternative approach is to reduce the number of model parameters while maintaining the most salient features of the original parameters i e parameterization several parameterization techniques have been developed and applied to subsurface flow model calibration problems a classical parameterization technique is zonation in which the spatial distribution of parameters is approximated by a finite number of zones representing lithofacies with identical properties berre et al 2009 sun et al 1998 tsai et al 2003 another class of parameterization techniques involves subspace methods in which an appropriate low dimensional subspace is used to describe the main spatial variability of the parameters the most notable example in this category is the principal component analysis gavalas et al 1976 jafarpour and mclaughlin 2009 karhunen 1947 li and cirpka 2006 loeve 1978 ma et al 2008 reynolds et al 1996 in some cases in addition to reducing the number of parameters parameterization serves to improve solution plausibility by constraining model parameters to honor the expected geologic continuity a specific example of these techniques is the pilot points method which was originally proposed by de marsily 1978 similar methods including variation and extended version of the original pilot points method for parametrization of subsurface model calibration techniques that have been studied in the literature alcolea et al 2006 alcolea et al 2008 certes and de marsily 1991 doherty 2003 finsterle and kowalsky 2008 gómez hernánez and sahuquillo 1997 jiménez et al 2016 jiménez et al 2013 kowalsky et al 2004 lavenue and pickens 1992 ramarao et al 1995 riva et al 2010 rubin et al 2010 in the pilot points method a subset of grid cells is selected as pilot points and the unknown property values in those cells are considered as the parameters to be estimated using available measurements geostatistical interpolation schemes mainly kriging methods deutsch and journel 1992 are then used to populate the unknown parameters in the remaining cells of the model to generate the full map of the property distributions using this approach the pilot points reflect the information in the dynamic flow data while geostatistical methods are used to incorporate a pre specified prior geologic continuity model e g a variogram or covariance function through interpolation as a model calibration approach the pilot points method has received broad attention within the subsurface modeling community as it offers two main benefits while attempting to match flow related measurements i it generates model parameters that honor a pre specified variogram model and are conditioned on hard data and ii it alleviates the computational burden by reducing the number of parameters that need to be estimated implementation of the pilot points method entails several issues that must be addressed the first one is related to the number of pilot points that should be used since pilot points reflect the information in the flow data the number of pilot points represents a weight given to flow data relative to the prior variogram model using too many pilot points with small distances can lead to over parameterization which can result in computational overhead non unique parameter estimation results and overly constrain the interpolation step unless proper measures are taken to address such issues on the other hand having very few pilot points can limit the effect of flow data and reduce the efficiency of the model calibration process beyond these general insights there is no fixed rule on the optimal number of pilot points for a given model scenario in addition to the number of pilot points placement of pilot points in the model domain presents another challenge several approaches have been proposed for placing pilot points in the literature the simplest approach is to assume that all grid blocks have the same potential to be selected as pilot points de marsily located the pilot points more or less uniformly but also attempted to follow zones of large sensitivities and large hydraulic property contrasts de marsily 1984 for gaussian distributed fields one recommendation is to place pilot points on a pseudo regular grids for example a few studies have reported good results when the spacing between pilot points is in the order of 1 to 3 points per correlation lengths gómez hernánez and sahuquillo 1997 kowalsky et al 2004 wen and deutsch 2002 alcolea et al 2006 reported that increasing the number of such regularly distributed pilot points may lead to spurious numerical effects and instabilities various aspects of the inverse modeling problem including numerical and computational considerations optimization algorithms and joint inversion using co kriging techniques are discussed in franssen et al 1999 while these observations provide useful rule of thumb they cannot be applied universally in general the field configuration including well locations interwell connectivity and sensitivity information play important roles in placement of pilot points therefore an alternative method to uniform placement of pilot points is to place them strategically for example pilot points can be placed in regions with high sensitivity in order to increase their contribution to reducing the data mismatch objective function lavenue and marsily 2001 lavenue and pickens 1992 lavenue et al 1995 ramarao et al 1995 pilot point placement has also been carried out based on both sensitivity and covariance information bissell 1994 bissell et al 1997 where the leading singular vectors of the sensitivity and covariance matrix are used to identify the locations of pilot points tonkin and doherty 2005 suggested constructing a highly parameterized base model for calculating base parameter sensitivities and then decomposing the base parameter normal matrix into eigenvectors that represent principal orthogonal directions in the parameter space these eigenvectors define a parameter subspace in which calibration data are used to find the coordinates of the solution based on the idea of parameter space reduction via svd yang et al 2012 used multiple leading singular vectors of the sensitivity matrix of model outputs with respect to the model parameters to define the intensity scores and then placed pilot points at locations with high intensity scores however in their framework an initial static set of uniformly distributed candidates potential pilot points must be predefined from which the pilot points are selected most recently jiménez et al 2016 proposed an approach based on learning not only the pilot point values but also their number and suitable locations during the reversible jump markov chain monte carlo rj mcmc procedure which requires a large number of iterations to converge they defined an initial set of pilot points that are uniformly distributed and then perturbed the pilot point values as well as their number and location from the initial state the above studies suggest that strategic placement of pilot points can be more beneficial than simply placing them uniformly in space however different strategies or criteria can be used for placement of pilot points and each strategy can lead to a different solution yang et al 2012 considered strategic locations to be those that could increase the information yield that is the information that can be extracted from the state variables for improving parameter estimation hence they use sensitivity information in their study to identify strategic locations in a probabilistic formulation an important criterion for placing pilot points may be uncertainty reduction in this case to increase the confidence in the calibrated models pilot point locations with higher uncertainty are preferable in general the locations with high uncertainty and high sensitivity have greater potential for improving the quality of solutions another source of information that can be used in placing pilot points is flow data that is flow data can be used to both help with finding promising locations of the pilot points and estimating the parameter values at those points while pilot points method has been successfully applied to model calibration problems it employs kriging or co kriging techniques for joint inversion based on variogram covariance functions which are suitable for continuous and multi gaussian random fields this assumption is in no way unique to the pilot points method and has been widely used in many other conventional model calibration techniques the multi gaussian assumption offers simplicity and mathematical convenience by requiring only the first two statistical moments of the distribution to fully characterize its probability density function pdf however in geologic formations that are characterized by sharp transitions between extreme low and high values of subsurface properties typically due to changing facies types the multi gaussian assumption fails to capture such behavior and to represent the expected variability and distribution of the underlying features for example in fluvial deposits e g river beds braided or meandering channels multi gaussian models cannot characterize the distinct geometry and shape of the relevant geobodies especially when complex geometric features are involved alcolea et al 2006 carle et al 1998 de marsily et al 2005 gómez hernández and wen 1998 guardiano and srivastava 1993 kerrou et al 2008 strebelle 2002 western et al 2001 zinn and harvey 2003 this limitation is primarily due to the use of a covariance model to represent the spatial relations as it only represents point to point correlations without accounting for multiple point data patterns to better represent discrete facies and their distribution several geostatistical simulation techniques are available two traditional approaches for simulating lithofacies distribution that honor a prior statistical representation and various types of measured and interpreted data are pixel based approaches such as sequential indicator simulation chiles and delfiner 2009 goovaerts 1997 isaaks 1991 journel 1983 and object based boolean methods e g marked point process that are capable of describing the continuity in geobodies with well defined shapes chiu et al 2013 deutsch and wang 1996 haldorsen and lake 1984 holden et al 1998 object based methods however lack the flexibility of grid based simulation techniques rendering the data integration aspect particularly difficult more recently multiple point statistics mps caers and zhang 2004 guardiano and srivastava 1993 hu and chugunova 2008 strebelle 2002 has been developed as a flexible grid based alternative simulation approach to object based methods for generating complex geologic patterns that are not amenable to variogram based modeling techniques instead of a covariance model mps relies on a conceptual model of geologic connectivity patterns known as a training image ti to represent the statistical information about the expected patterns in the model a particular implementation of the mps that is used for simulation of the discrete facies patterns is the single normal equation simulation snesim strebelle 2002 the grid based implementation of the snesim algorithm provides simple conditioning of the results to hard data e g facies measurement at well locations and soft data e g seismic in particular integration of hard data is conveniently accomplished by simply assigning the hard data to the corresponding cells and excluding them from the simulation random path journel 2002 remy et al 2009 strebelle 2002 conditioning the output of mps simulation to dynamic flow data is however nontrivial this is primarily due to the nonlinear and indirect relation between hydraulic properties and flow data a number of approaches have been studied for calibration of mps based models against flow data while preserving the complex connectivity in these models these techniques are either based on continuous approximation of the discrete geologic facies in which the complex connectivity features are updated by adjusting a few parameters that control their distribution alcolea and renard 2010 jafarpour 2011 jafarpour and mclaughlin 2008 2009 khaninezhad et al 2012a b zhou et al 2011 or they are based on manipulation of the simulation algorithm to reproduce the dynamic response data caers and hoffman 2006 jafarpour and khodabakhshi 2011 khodabakhshi and jafarpour 2013 mariethoz et al 2010 the latter approach has the advantage of automatically preserving the exact connectivity patterns in the ti which is critical when complex patterns are involved while the former is simpler and amenable to continuous gradient based inversion techniques in this paper we present an adapted pilot points method that can be used for conditioning the simulations of complex facies patterns to nonlinear flow data the motivation is to combine the convenient hard data conditioning i e pilot point values in mps simulation for complex models with integration of flow data by using pilot points method with strategic placement for strategically placing pilot points we use three sources of information sensitivity information variance reduction and flow data to apply the pilot points to mps based facies simulation the original implementation has been adapted in two major ways first the method is modified to include estimation of discrete values facies types second the use of stochastic simulation necessitates sampling and updating multiple realizations of the facies maps and pilot points to implement the modifications first we use mps simulation to generate multiple realizations of facies maps and the corresponding permeability maps that are conditioned on the hard data at wells next we update permeability values at the pilot points using the ensemble smoother es skjervheim and evensen 2011 van leeuwen and evensen 1996 based on the updated ensemble of permeability maps facies types are inferred at the pilot points the resulting updated facies values at the pilot points are in turn used as hard data in mps simulation to generate an updated ensemble of facies maps in our proposed method the mps simulation ensures that the simulated facies patterns are consistent with the ti while the updated pilot points incorporate the information in the dynamic flow data hence both high order statistical information from the ti and nonlinear flow data are incorporated in the calibration process in the next section we introduce the proposed pilot points method followed by a series of synthetic examples and their discussion 2 methodology 2 1 conditional mps facies simulation with pilot points we consider the stochastic simulation of a random function m x m x 1 m x 2 m x n g defined on a grid system with ng cells where the notation m xi refers to the parameter value m at the location xi in sequential simulation instead of characterizing and sampling from the multivariate distribution of the random function the multiplication rule of probability is invoked to sequentially visit each un sampled cell and compute and sample from the corresponding univariate conditional probability i e 1 f c z p m x n 1 z m x 1 m x 2 m x n where m x 1 m x 2 m xn are conditioning or previously sampled data and m x n 1 is the un sampled value at the current simulation cell in practice the conditioning data m x 1 m x 2 m xn are taken from a local neighborhood of x n 1 when the random function m x is multi gaussian all the conditional probabilities are gaussian and can be computed by estimating the corresponding conditional mean and variance of m x n 1 e g using kriging however when the multi gaussian assumption does not hold computing the conditional probability for m x n 1 becomes nontrivial unlike traditional geostatistical simulation techniques that are based on random function models mps simulation utilizes empirical multivariate distributions that are inferred from a training image guardiano and srivastava 1993 strebelle 2002 if m x represents a discrete random process e g facies distribution the corresponding conditional probabilities are simple to describe and can be inferred from a ti that contains the mps information patterns hu and chugunova 2008 strebelle 2002 when using pilot points for conditioning the facies distribution the conditioning data includes the observed facies types at the well locations when they exist and the facies types at the pilot points the facies types at the pilot points parameterize the discrete stochastic random function if np pilot points are chosen the facies types at the np locations are estimated from integration of flow data whereas the facies types at the well locations are considered known and fixed when applied to calibration of discrete facies distribution the discrete facies types at the pilot points must be provided to the snesim algorithm for conditioning however directly estimating these discrete values is complex since the hydraulic properties estimated during data integration are continuous instead the facies types may be inferred using the estimated from the flow data hydraulic properties at the pilot points the details about inferring the facies types from the estimated hydraulic properties will be discussed in section 2 3 2 2 strategic placement of pilot points when placing pilot points it is desirable to select locations that exhibit significant contribution to explaining the parameter variability from observed data e g heads flow rates etc and high potential to reduce the uncertainty in the facies distribution candidates for such points are those that have high potential to be estimated accurately i e flow responses show high sensitivity to those locations and that can maximally reduce the uncertainty in the facies distribution typically these two criteria tend to compete as high sensitivity locations are near well locations which often have low uncertainty in this work we seek to identify locations with high sensitivity and uncertainty let us consider m m 1 m 2 m n e n s r n g n e n s to be the matrix that represents an initial ensemble of the model parameters where ng is the number of cells in the model and nens is the ensemble size for each realization of the model parameters m i let j i r n g n d be the sensitivity matrix of the model output flow simulation output e g rates heads saturation etc with respect to model parameters e g permeability hydraulic conductivity etc where nd is the dimension of measurements since the sensitivity matrix j i is different from realization to realization a global sensitivity matrix is defined as j a u g j 1 j 2 j n e n s r n g n a where na nd nens the first step for placing pilot points is to generate score maps following a similar approach discussed in yang et al 2012 for matrix z j aug the singular value decomposition svd is written as 2 z u σ v t where u u i j r n g n g σ s i j r n g n a v v i j r n a n a note that in σ the off diagonal elements are zeros and the diagonal element sii λ i refers to the ith largest singular value to identify the preferred locations for pilot points the singular vectors corresponding to the first ns singular values such that k 1 n s λ k 2 95 are used using the ns singular vectors the following score map is defined 3 s j k 1 n s λ k 2 u k j l 1 n g u k l j 1 2 n g with this definition regions with high score values in the calculated maps indicate locations for which the model responses show high sensitivity to the corresponding parameters at those locations to identify the high sensitivity regions a s percentile ps is used as the threshold to truncate the original score map defined by eq 3 such that the scores below ps become zeros thus non zero regions in the truncated map represent the high score regions in the original map and the pilot points will be placed using a procedure described in the next section in these non zero regions only to account for the uncertainty in model parameters the parameter sample variance map c d i a g 1 n e n s 1 m m m m t r n g is used where m r n g n e n s is a matrix that has the sample mean in all of its columns if we normalize the variance map onto range 0 1 by a linear transformation s j c j c m i n c m a x c m i n j 1 2 n g and cmin cmax are the minimum and maximum values of the elements in c notice that the normalized variance map is the only information entering the score hence areas that assume low values in the corresponding score map correspond to positions with lower uncertainty therefore to reduce the uncertainty in the parameter space pilot points are placed in the locations with higher score values for estimation of the parameters at the pilot points it is desirable to place the pilot points in locations with high sensitivity and high variance therefore we define a new score map by combining the truncated score maps after normalizing into 0 1 range based on the sensitivity and variance information the combined map is generated through the schur point wise product of the two truncated score maps the schur product of the two score maps represents a grid block based operation to find the intersection of the locations in the domain that have high sensitivity and variance the regions with significant scores in the resulting map correspond to target locations for placing pilot points these resulting regions are covered by randomly placing pilot points in them we note that the use of variance map in obtaining uncertainty score maps only captures the second order statistics of the parameters and does not include higher statistics while variance is not an optimal measure to use for parameters with higher order variability its simplicity and inclusion in the update step of the filter motivate its use for quantifying the prior model parameter uncertainty in our formulation additionally computationally demanding optimization methods can be used with some performance metrics to find optimal locations for pilot points the simple approach used in this work has resulted in satisfactory performance an additional source of information in finding promising locations for pilot points is the flow data we incorporate this information through an iterative data assimilation scheme as discussed in section 3 to place the pilot points on the combined map we consider a region of influence around each pilot point to establish a minimum distance criterion when a pilot point is placed on the map its region of influence is removed from consideration for additional pilot points the shape of the region of influence can be decided based on prior knowledge about the continuity model or based on sensitivity analysis in our study no preference is given to any direction and a symmetric region with radius r from the center of a pilot point is used where r is chosen using sensitivity analysis we note that incorporation of reliable information about the correlation structures in the field is expected to improve the results for example one could use an ellipsoid to define the region of influence based on available prior knowledge this region of influence as well as the truncated map mentioned earlier provides a means to avoid over parameterization in section 3 two approaches used for placing the pilot points are investigated the first approach is deterministic in which the ranking in the score map is used to place the pilot points the second approach randomly places the pilot points in non zero regions on the score map without following any order in both cases no overlap is allowed between the regions of influence of different pilot points the numerical experiments suggest that the random placement is more robust compared with the deterministic one therefore in the proposed method the random placement is used to distribute the pilot points on the score map one advantage of the proposed approaches is that by defining a threshold for the score map and a radius of influence the number and locations of the pilot points are determined systematically notice that although the proposed placement of pilot points is sub optimal and can be further improved it is easy to implement with a little extra computational cost whereas formal optimization of the pilot points placement is not trivial and expected to be computationally demanding 2 3 facies estimation at pilot points after placing the pilot points an inversion method is needed to estimate the parameters at the pilot points using the flow data we note that in our implementation the facies types are not updated directly by integrating the flow data instead the permeability values at the pilot points are estimated first the estimated permeability values are then used to infer the conditional to flow data facies probability at the pilot points following the same approach linear scaling proposed by jafarpour and khodabakhshi jafarpour and khodabakhshi 2011 where the facies probabilities at each pilot point are estimated by linear scaling the ensemble mean of permeability at the same point we denote the two facies types as channel ch and non channel nc and their corresponding permeability values as kch and knc respectively if the ensemble mean of log permeability at a grid cell x is log k x then the probability of channel facies at the grid cell is estimated as p a x c h c p m i n p m a x p m i n log k x l o g k n c l o g k c h l o g k n c where pmin pmax 0 10 0 90 are the bounds of the probability maps in the original pcm to only allow for at most 90 likelihood on the updated results for k x kch and k x knc pmax and pmin values are used respectively as the facies probabilities samples from the facies probability are then drawn to generate an ensemble of facies realizations at the pilot point locations which are subsequently included along with hard data from existing well locations in the snesim algorithm to regenerate an updated ensemble of facies models for data assimilation we use a variant of the ensemble kalman filter enkf known as ensemble kalman smoother or simply ensemble smoother es the enkf is a monte carlo based approximation of the sequential kalman filtering method that allows for nonlinear propagation of uncertainty from model parameters e g hydraulic conductivity permeability into the outputs e g heads flow rates of a dynamical system to approximately represent through samples the prediction uncertainty distribution when measurements become available they are used in a linear analysis scheme to update the parameters states of the system evensen 1994 evensen 2004 following the notation in evensen 2004 the augmented state matrix containing the ensemble members ψ i r n a u g is denoted as 4 a ψ 1 ψ 2 ψ n r n a u g n e n s in this study the ensemble members ψ i are the permeability at the pilot point locations the ensemble mean is stored in each column of a a 1 n where 1 n r n e n s n e n s is a matrix in which all the elements are 1 nens and the perturbation matrix is defined as a a a given a vector of measurements d r n m with nm being the number of measurements the perturbed observations d j d ε j is stored in columns of a matrix d d 1 d 2 d n e n s r n m n e n s the ensemble of perturbed observations is defined as e ϵ 1 ϵ 2 ϵ n e n s r n m n e n s define the sample states covariance and measurement error covariance matrices respectively as 5 p e a a t n 1 r e e e t n 1 the analysis step of the enkf can be expressed as follows 6 a a a k d ha a p e h t h p e h t r e 1 d ha where a a is the updated ensemble k is kalman gain and h is the measurement matrix letting d d ha s ha r n m n e n s and c e ss t ee t eq 6 can be rewritten as 7 a a a i s t c e 1 d ax w h e r e x i s t c e 1 d r n e n s n e n s evensen proposed a robust algorithm to perform the analysis step by calculating a a ax in eq 7 to alleviate the rapid reduction in variance after the update which is a well known issue of the enkf update a damping factor is applied to the kalman gain matrix of the enkf update the resulting update takes the form 8 a d a m p a a α k d ha α ax 1 α a α a a 1 α a where α 0 1 is a damping factor we note that a reduction in the kalman gain can be viewed as inflating the observation error variance and leads to a more modest update the choice of α as a tuning parameter is not immediately obvious and may require trial and error to specify different from enkf es van leeuwen and evensen 1996 is an alternative data assimilation method which avoids the sequential updating of the realizations and the associated restart which is performed after each update step to generate forecasts for the next data assimilation step in es one global update in the space time domain is performed instead of using recursive updates in time as in enkf as a result compared with enkf es provides a significant reduction in simulation time which makes it more efficient and much simpler to implement and apply especially in complex subsurface applications however implementation of es requires multiple iterations which introduce additional computational overhead as mentioned earlier both enkf and es can be used in our proposed method however due to its lower computational cost es is used as the data assimilation of choice in this work 2 4 implementation steps in this section we summarize the implementation steps starting with the initial ensemble of the facies models which are conditioned on hard data from wells the proposed pilot points conditioning with mps simulation is performed using the following steps 1 generate an initial ensemble using the snesim algorithm 2 construct the sample variance map from the initial ensemble 3 perform forward simulation to predict the states and measurements model outputs and the sensitivity information using the adjoint method for each of the ensemble members 4 generate the score maps based on sensitivity and variance information and combine them to obtain a combined truncated score map as discussed in section 2 2 5 using an appropriate radius of influence determine the pilot point locations based on the placement procedure discussed in section 2 2 6 estimate the permeability for each realization at the pilot points using the es data assimilation 7 infer the facies probabilities using the approach proposed in khodabakhshi and jafarpour 2013 and discussed above at the pilot points from the corresponding updated permeability and use them to generate an ensemble of facies types at the pilot points by sampling 8 use the snesim algorithm to generate an ensemble of facies realizations that are conditioned on the hard data and the sampled facies types at the pilot points a major difference between the above implementation with standard enkf and es approach is that the smoother in our approach is used to only update the permeability values at the pilot points locations it s important to note that in the above scheme the pilot points are placed based on the prior information only and no dynamic flow data is involved in their placement an iterative scheme of the proposed method can be formed by repeating step 2 8 that is in each iteration the updated ensemble of facies types from the previous iteration is considered as the initial ensemble at the current iteration consequently the score map as well as the locations and number of pilot points are updated in step 3 5 thus performing these iterations leads to integration of the flow data into the updated facies realizations resulting in updated sensitivity and variance information and hence updated score map unlike the non iterative scheme the iterative form of the updates incorporates the information in the measurements flow data in identifying the pilot point locations hence the iterative scheme provides a mechanism to include flow data in addition to sensitivity and initial uncertainty in placing the pilot points 3 numerical experiments we present two examples in this section example 1 includes a series of numerical experiments to discuss various aspects of the proposed approach example 2 involves a larger and more complex subsurface model in which we show the application of the proposed approach we note that in example 2 we have also performed similar experiments and comparisons as in example 1 with similar conclusion however for brevity we only present the final results from applying the proposed method to example 2 3 1 uniform pilot points placement in example 1 we present a series of numerical experiments as listed in table 1 to evaluate the performance of the proposed workflow and discuss its properties we use two dimensional two phase wetting and non wetting phase flow in subsurface to demonstrate the performance of our method the model in our first experiment consists of a 65 65 grid system with five injection wells on the left and five extraction wells on the right the reference facies distribution is shown in the top plot of fig 1 and consists of channel facies black and background mudstone white it is assumed that the values of permeability and porosity for each facies type are known e g from analysis of core samples and well logs and the only uncertainty is related to the distribution of facies in space away from well locations the pressure at the injection wells and flow rates of both phases at the extraction wells are measured every month over a two year period and used for data assimilation the initial ensemble of facies realizations is conditioned on the hard data the facies types at 10 well locations the second row in fig 1 shows the mean variance and three out of 100 sample realizations of the initial ensemble of the spatial distribution of the log permeability generated using the snesim algorithm strebelle 2002 the initial samples share statistically similar connectivity patterns with the reference model however the exact shape and location of the features are different from those in the reference model since the initial ensemble is conditioned on the measurement at the well locations the samples show less uncertainty about the facies distribution at the vicinity of the wells this can be verified by examining the initial ensemble mean and variance maps in fig 1 second row the main uncertainty is therefore related to the channel locations and connectivity in the middle of the domain in experiment a 1 the standard es is used to assimilate the measurements and update the distribution of log permeability field in the initial ensemble the third row of fig 1 plots the mean variance and three samples from the updated ensemble using the es analysis a comparison with the reference model reveals that the updated maps are closer to the reference model but the exact shape and discrete nature of the channel features are lost this is a general issue that is observed in the application of enkf or es methods to characterization of facies specifically these methods apply second order continuous updates that are not suitable for estimation of discrete facies with higher order statistical patterns unless indirect approaches such as continuous parameterization of facies are used to avoid such inconsistency we propose to use the observed flow data to infer facies types at pilot points and use them to generate conditional samples to infer the facies type following the approach proposed in jafarpour and khodabakhshi 2011 we first find facies probabilities at the pilot point locations as discussed in the description of the method section 2 3 and use them to generate realizations of facies types at the pilot point locations a comparison between the variance maps second column in the second and third rows of fig 1 shows that the low variance regions in the initial ensemble are updated minimally and remain largely unchanged after the update this observation suggests that the locations with low variance in the initial ensemble are not good candidates for placing the pilot points in experiment a 2 the pilot points are uniformly distributed across the entire domain instead of applying a strategic placement the facies types at the pilot points are inferred from the ensemble of updated permeability models in experiment a 1 the last row in fig 1 shows the updated results for the ensemble of log permeability generated after conditioning on uniformly placed pilot points the results demonstrate that uniform placement of pilot points is not a good strategy and may not capture the correct connectivity in facies when the pilot points are placed uniformly some of them may be assigned to low sensitivity regions hence they may not be updated correctly since the estimated facies types at the pilot points are treated as hard data during facies simulation they play a significant role in describing the connectivity of the final facies maps hence it is important to avoid arbitrary updates due to the placement of pilot points in low sensitivity regions this is the primary motivation of our work on strategic placement of pilot points which will be introduced in the next section 3 2 strategic placement of pilot points fig 2 shows the score maps calculated following the approach discussed in section 2 fig 2a and b display the normalized score maps obtained from the sensitivity matrix and its truncated version using a threshold of 75 percentile i e p 75 respectively fig 2c and d depict similar score maps that are computed using the variance map the final score map is obtained by taking the schur product of the two truncated maps as discussed in section 2 2 these score maps and hence the resulting pilot point locations are generated based on prior information the initial ensemble of facies and the corresponding sensitivity information based on predicted model responses that is dynamic flow measurements are not included in placing the pilot points fig 3 uses a simple schematic to illustrate the deterministic placement of pilot points at the beginning the first pilot point is placed at the location with the highest score the next pilot point is then placed to satisfy two requirements i its region of influence does not overlap with those of all previously placed pilot points ii it is at the location with the highest score among all possible locations in the rest of the non zero regions this procedure is repeated until there are no positions in the truncated score map that can accommodate a pilot point without an overlap with previously placed pilot points we note that the region of influence can take a different shape than a circle and may require sensitivity analysis to identify in this example the radius of the circular region r i e the region of influence is assumed to be 7 grid cells in practice sensitivity analysis and field correlation length can be used to specify this radius in the deterministic approach all ensemble members share the same pilot point configuration as discussed earlier in section 2 2 an alternative approach for finding pilot point locations is randomly placing them on the score map instead of using a descending order of the scores the importance of random placement is better appreciated by noting that pilot points that intersect with the channel facies provide more important conditioning information than those that do not intersect the channels hence a random placement approach increases the chance of pilot points hitting the channel facies if the dynamic data supports it in placing the pilot points in the random case overlap between the regions of influence is not permitted in our experiments the ensemble of nens 100 realizations is divided into ng 10 groups and each group is randomly assigned a pilot point configuration to highlight the effect of pilot point placement in the next experiment it is assumed that at the pilot points the facies types are known perfectly that is instead of using model calibration to infer the facies types the exact facies types from the reference model are assigned to the pilot points this is done to ensure that the pilot point placement analysis is not affected by errors in the data integration step fig 4 a top shows the truncated score map and the corresponding pilot points which are placed using the deterministic approach the second row of fig 4a shows the mean variance and three sample realizations of the resulting facies models fig 4b displays three pilot point configurations out of ten that are placed randomly the second row in fig 4b shows the ensemble mean and variance of the facies distribution along with three sample realizations each from a different group in comparison with the results from fig 4a the conditional facies represent the reference map more accurately this and several other examples that were performed over the course of this work indicate that the random placement of pilot points is more robust than the deterministic approach therefore we adopt the random placement approach in our proposed method 3 3 incorporating flow data in pilot point placement in experiment a 3 the non iterative scheme of the proposed method is used in this experiment the facies type at the pilot points is inferred using the method described in section 2 3 after estimating the log permeability maps by applying the standard es the resulting facies types are then used to regenerate conditional realizations from the ti using the snesim algorithm in experiment a 4 we apply multiple updates to the permeability map in an iterative form using the ensemble smoother with multiple data assimilation es mda approach before performing any resampling from the ti that is es mda iterations are first completed to estimate the permeability values at pilot points and then the results are used to resample from the ti experiment a 4 differs from a 3 in that it involves full iterations of es mda prior to resampling from the ti emerick and reynolds 2013 it is important to note that the iterations of es mda are only applied to improve the estimation of the permeability values at the pilot points therefore the pilot point locations are still identified prior to data integration fig 5 a1 and b1 show the ensemble mean of log permeability obtained from the standard es and esmda respectively the corresponding mean variance and three samples from the final ensemble of facies maps after conditioning on the pilot points are shown in fig 5 a2 and a3 for es and in fig 5 b2 and b3 for es mda two additional points are important to stress first although the entire map of updated log permeability is shown only the values at pilot point locations are used in our analysis second the facies types at the pilot points are determined by first identifying the facies probability at each pilot point and then using the resulting facies probabilities to draw samples for facies types at those points prior to conditional resampling with the snesim algorithm to quantify the performance of the two approaches the root mean squared error rmse corresponding to a variable x is calculated for all grid cells rmse x i 1 n e n s j 1 n e n s x i j x i r e f 2 i 1 2 3 n g the total rmse of an ensemble is defined as rms e t o t a l x i 1 n g rmse x i the relative reduction of the total rmse of the updated ensemble xupdate compared to that of the initial ensemble xinitial can be obtained as r rmse x rms e t o t a l x u p d a t e rms e t o t a l x i n i t i a l rms e t o t a l x i n i t i a l the relative reduction of the total rmse of the model parameters is 3 24 and 6 43 for experiment a 3 and a 4 respectively table 2 compared to the standard es the updated mean of facies maps with the es mda is slightly improved however considering the computational overhead associated with multiple iterations used in the es mda algorithm a trade off exists between the incremental improvements versus the computational cost in these two experiments prior information was used for placing the pilot points on the score map an additional and important source of information for identifying promising locations for pilot points is the dynamic measurements e g flow and pressure head data in the next set of experiments a 5 and a 6 the iterative scheme of our proposed method is used in this case in each iteration first the pilot points are placed based on the current ensemble of facies permeability maps from previous iteration the es algorithm is then used to update the log permeability values at the pilot points the facies type at each pilot point is inferred and used with the ti to regenerate i e resample the new ensemble of facies and permeability maps that will be passed to implement the next iteration since the new ensemble of facies maps incorporate the dynamic data this scheme ensures that the flow data is also used in determining the pilot point locations fig 6 a and b show the final ensemble of facies maps after two and four full iterations respectively the results from these two experiments show noticeable improvements over those in fig 5 highlighting the importance of flow data in placing the pilot points in these two experiments the relative reductions of the total rmse of the model parameters are 8 46 and 15 55 respectively table 2 fig 7 a and b display the predicted and observed flow rates for the wetting and non wetting phases at the extraction wells for the initial and updated models from experiment a 5 and a 6 respectively the initial predictions are relatively far from the observed values and show a larger spread the predictions after two and four full iterations show increasingly improved forecasts in experiment a 6 compared to the initial ensemble the relative reductions in the total rmse for the heads at the injection wells and flow rates for both non wetting and wetting phases at the extraction wells for the updated ensemble of facies maps are 26 0 30 1 and 37 6 respectively together the results from the updated facies maps and production forecasts demonstrate noticeable improvements when the iterative scheme is performed to incorporate the flow data both in pilot point placement and facies identification 3 4 example 2 complex facies connectivity patterns we now consider a larger example with more complex connectivity patterns the example also involves two phase flow in a two dimensional model the top panel in fig 8 shows the reference model which consists of a 201 61 grid system with five injection wells diamonds and five extraction wells squares the reference facies distribution consists of channel facies black and background mudstone white the pressure at the injection wells and flow rates of both phases at the extraction wells are measured every month over a two year period and used as conditioning data the initial ensemble of facies realizations is conditioned on the hard data the facies types at 10 well locations the first column in fig 8 shows three sample out of 100 realizations of the initial ensemble of log permeability distributions along with the ensemble mean variance maps while the initial samples share similar connectivity patterns statistically with the reference model they do not represent the exact shape and location of the features in the reference model since the initial ensemble is conditioned on the hard data at the well locations the initial ensemble members show less uncertainty about the facies distribution near the wells using this example with complex connectivity patterns we performed a series of the experiments that are similar to those in example 1 the results in all the experiments were consistent with those of example 1 and show that the proposed method iterative scheme with four full iterations and random pilot point placement outperforms other approaches used in example 1 both in estimating facies distribution and in matching the flow data here for brevity we only show the results of our proposed final approach the second column in fig 8 contains the results after the implementation of the proposed method qualitatively the updated sample better represents the connectivity in the reference model to quantify this performance the corresponding relative reduction of the total rmse in model parameter estimation is calculated to be 19 95 fig 9 a and b display the predicted and observed flow rates of the wetting and non wetting phases at the extraction wells for the initial and updated after four full iterations ensemble respectively the initial predictions are relatively far from the observed values and show a larger spread the predictions after four full iterations show noticeable improvements with the updated models the relative reductions in the total rmse in matching the pressure heads at the injection wells and flow rates for both non wetting and wetting phases at the extraction wells in experiment b 6 are 26 0 30 1 and 37 6 respectively finally unlike iterative inverse modeling formulations that minimize a data mismatch cost function the updates performed using different flavors of the enkf or es by construction do not guarantee an improvement in data match unless such a mechanism is explicitly enforced in addition to this inherent property the stochastic nature of the snesim conditional simulation step which is used to incorporate the facies values at the pilot points introduces additional variability randomness in the facies distribution which can affect the quality of data match one approach to improve the predictive performance of the updated ensemble is to introduce an acceptance rejection sampling step in the last stage of the workflow when the final ensemble of conditional facies is generated in our experiments we observed good data match after our final updates and did not implement an acceptance rejection criterion 4 conclusions we present a novel approach based on the pilot points parameterization for conditioning multiple point statistical simulation of complex geologic facies on nonlinear flow data the pilot points method is originally developed for geostatistical parameterization of model calibration problems using variogram based kriging techniques here we develop a pilot point scheme for conditioning stochastic discrete facies simulation on flow data using ensemble based data assimilation techniques the main motivation behind the proposed approach is to address the difficulty in parameterization of discrete facies for model calibration at least two challenges are faced in calibrating complex geologic facies models i honoring the discrete and non gaussian nature of the model parameters facies distribution and ii preserving their expected complex connectivity patterns the developed approach takes advantage of the convenient hard data conditioning in mps simulation to incorporate the production data this is achieved by first identifying the facies types at a set of pilot point locations from flow data and then using them along with available hard data for conditional facies simulation by drawing conditional samples from a ti the proposed method ensures that the discrete nature of facies and the prior model connectivity in the ti are honored during model calibration to implement the pilot point conditioning approach the first step is a strategic identification of pilot point locations placement of pilot points is carried out by combining the information related to the sample variance to maximize reduction in uncertainty and the sensitivity of the flow responses of the prior models to increase the effectiveness of the updates these criteria are combined to place pilot points in locations that have the potential to make a significant improvement to the prior models and to avoid locations where little variability is present once a score map is constructed a predefined threshold is used to truncate regions with low values of sensitivity and variance and only retain promising regions for placing the pilot points we investigated two techniques for locating pilot points in the truncated score map a deterministic placement method that covers the effective regions based on the ranking of the scores and a random placement strategy where for each subgroup of the ensemble the pilot points are randomly located within the truncated score maps in both cases the pilot points that are introduced into the score map have a minimum distance constraint that is they cover an area with a radius of influence that may not have any overlap with the region of influence of other pilot points this strategy results in automatic identification of the number of pilot points but it also requires a tuning parameter i e the radius of influence for each pilot point the results from our numerical experiments show that the random placement of pilot points is more robust as it covers more points on the score map and increases the chance of intersecting with important facies types e g channels we also show the importance of dynamic data in both determining good candidate locations for pilot points and for identifying the values of the pilot points via data assimilation the dynamic data can be easily incorporated by repeating the workflow with the conditional facies maps that are generated from previous steps by the iterative approach on the workflow the updated facies that contain the flow data are used for placing new pilot point locations the facies estimation results are clearly improved when dynamic data is included in selecting the pilot point locations the developed method presents an important extension to the original pilot points method for stochastic calibration of complex facies distributions against flow data a key advantage of the presented approach is that it automatically reproduces discrete facies models with complex connectivity patterns that are specified in the prior model properties that are difficult to achieve using conventional model calibration techniques acknowledgment the data and simulation results for this paper can be obtained by contacting the corresponding author at jafarpou usc edu 
834,we propose a new pilot points method for conditioning discrete multiple point statistical mps facies simulation on dynamic flow data while conditioning mps simulation on static hard data is straightforward their calibration against nonlinear flow data is nontrivial the proposed method generates conditional models from a conceptual model of geologic connectivity known as a training image ti by strategically placing and estimating pilot points to place pilot points a score map is generated based on three sources of information i the uncertainty in facies distribution ii the model response sensitivity information and iii the observed flow data once the pilot points are placed the facies values at these points are inferred from production data and then are used along with available hard data at well locations to simulate a new set of conditional facies realizations while facies estimation at the pilot points can be performed using different inversion algorithms in this study the ensemble smoother es is adopted to update permeability maps from production data which are then used to statistically infer facies types at the pilot point locations the developed method combines the information in the flow data and the ti by using the former to infer facies values at selected locations away from the wells and the latter to ensure consistent facies structure and connectivity where away from measurement locations several numerical experiments are used to evaluate the performance of the developed method and to discuss its important properties keywords pilot points multiple point statistics mps model calibration geologic facies ensemble kalman filter ensemble smoother 1 introduction calibration of subsurface flow models against scattered non linear well response data typically leads to an inverse problem to infer hydrological properties of the formation and to enable more accurate prediction of future flow and transport behavior a difficulty in formulating and solving model calibration inverse problems is related to the large number of parameters that need to be estimated from limited scattered measurements making the problem ill posed carrera and neuman 1986 carrera et al 2005 kitanidis 1986 mclaughlin and townley 1996 to overcome problem ill posedness additional assumptions about the solution are introduced into the problem formulation one approach is to constrain the solution through regularization functions to impose a pre specified spatial structure e g smoothness on the distributed formation properties carrera et al 2005 yeh 1986 an alternative approach is to reduce the number of model parameters while maintaining the most salient features of the original parameters i e parameterization several parameterization techniques have been developed and applied to subsurface flow model calibration problems a classical parameterization technique is zonation in which the spatial distribution of parameters is approximated by a finite number of zones representing lithofacies with identical properties berre et al 2009 sun et al 1998 tsai et al 2003 another class of parameterization techniques involves subspace methods in which an appropriate low dimensional subspace is used to describe the main spatial variability of the parameters the most notable example in this category is the principal component analysis gavalas et al 1976 jafarpour and mclaughlin 2009 karhunen 1947 li and cirpka 2006 loeve 1978 ma et al 2008 reynolds et al 1996 in some cases in addition to reducing the number of parameters parameterization serves to improve solution plausibility by constraining model parameters to honor the expected geologic continuity a specific example of these techniques is the pilot points method which was originally proposed by de marsily 1978 similar methods including variation and extended version of the original pilot points method for parametrization of subsurface model calibration techniques that have been studied in the literature alcolea et al 2006 alcolea et al 2008 certes and de marsily 1991 doherty 2003 finsterle and kowalsky 2008 gómez hernánez and sahuquillo 1997 jiménez et al 2016 jiménez et al 2013 kowalsky et al 2004 lavenue and pickens 1992 ramarao et al 1995 riva et al 2010 rubin et al 2010 in the pilot points method a subset of grid cells is selected as pilot points and the unknown property values in those cells are considered as the parameters to be estimated using available measurements geostatistical interpolation schemes mainly kriging methods deutsch and journel 1992 are then used to populate the unknown parameters in the remaining cells of the model to generate the full map of the property distributions using this approach the pilot points reflect the information in the dynamic flow data while geostatistical methods are used to incorporate a pre specified prior geologic continuity model e g a variogram or covariance function through interpolation as a model calibration approach the pilot points method has received broad attention within the subsurface modeling community as it offers two main benefits while attempting to match flow related measurements i it generates model parameters that honor a pre specified variogram model and are conditioned on hard data and ii it alleviates the computational burden by reducing the number of parameters that need to be estimated implementation of the pilot points method entails several issues that must be addressed the first one is related to the number of pilot points that should be used since pilot points reflect the information in the flow data the number of pilot points represents a weight given to flow data relative to the prior variogram model using too many pilot points with small distances can lead to over parameterization which can result in computational overhead non unique parameter estimation results and overly constrain the interpolation step unless proper measures are taken to address such issues on the other hand having very few pilot points can limit the effect of flow data and reduce the efficiency of the model calibration process beyond these general insights there is no fixed rule on the optimal number of pilot points for a given model scenario in addition to the number of pilot points placement of pilot points in the model domain presents another challenge several approaches have been proposed for placing pilot points in the literature the simplest approach is to assume that all grid blocks have the same potential to be selected as pilot points de marsily located the pilot points more or less uniformly but also attempted to follow zones of large sensitivities and large hydraulic property contrasts de marsily 1984 for gaussian distributed fields one recommendation is to place pilot points on a pseudo regular grids for example a few studies have reported good results when the spacing between pilot points is in the order of 1 to 3 points per correlation lengths gómez hernánez and sahuquillo 1997 kowalsky et al 2004 wen and deutsch 2002 alcolea et al 2006 reported that increasing the number of such regularly distributed pilot points may lead to spurious numerical effects and instabilities various aspects of the inverse modeling problem including numerical and computational considerations optimization algorithms and joint inversion using co kriging techniques are discussed in franssen et al 1999 while these observations provide useful rule of thumb they cannot be applied universally in general the field configuration including well locations interwell connectivity and sensitivity information play important roles in placement of pilot points therefore an alternative method to uniform placement of pilot points is to place them strategically for example pilot points can be placed in regions with high sensitivity in order to increase their contribution to reducing the data mismatch objective function lavenue and marsily 2001 lavenue and pickens 1992 lavenue et al 1995 ramarao et al 1995 pilot point placement has also been carried out based on both sensitivity and covariance information bissell 1994 bissell et al 1997 where the leading singular vectors of the sensitivity and covariance matrix are used to identify the locations of pilot points tonkin and doherty 2005 suggested constructing a highly parameterized base model for calculating base parameter sensitivities and then decomposing the base parameter normal matrix into eigenvectors that represent principal orthogonal directions in the parameter space these eigenvectors define a parameter subspace in which calibration data are used to find the coordinates of the solution based on the idea of parameter space reduction via svd yang et al 2012 used multiple leading singular vectors of the sensitivity matrix of model outputs with respect to the model parameters to define the intensity scores and then placed pilot points at locations with high intensity scores however in their framework an initial static set of uniformly distributed candidates potential pilot points must be predefined from which the pilot points are selected most recently jiménez et al 2016 proposed an approach based on learning not only the pilot point values but also their number and suitable locations during the reversible jump markov chain monte carlo rj mcmc procedure which requires a large number of iterations to converge they defined an initial set of pilot points that are uniformly distributed and then perturbed the pilot point values as well as their number and location from the initial state the above studies suggest that strategic placement of pilot points can be more beneficial than simply placing them uniformly in space however different strategies or criteria can be used for placement of pilot points and each strategy can lead to a different solution yang et al 2012 considered strategic locations to be those that could increase the information yield that is the information that can be extracted from the state variables for improving parameter estimation hence they use sensitivity information in their study to identify strategic locations in a probabilistic formulation an important criterion for placing pilot points may be uncertainty reduction in this case to increase the confidence in the calibrated models pilot point locations with higher uncertainty are preferable in general the locations with high uncertainty and high sensitivity have greater potential for improving the quality of solutions another source of information that can be used in placing pilot points is flow data that is flow data can be used to both help with finding promising locations of the pilot points and estimating the parameter values at those points while pilot points method has been successfully applied to model calibration problems it employs kriging or co kriging techniques for joint inversion based on variogram covariance functions which are suitable for continuous and multi gaussian random fields this assumption is in no way unique to the pilot points method and has been widely used in many other conventional model calibration techniques the multi gaussian assumption offers simplicity and mathematical convenience by requiring only the first two statistical moments of the distribution to fully characterize its probability density function pdf however in geologic formations that are characterized by sharp transitions between extreme low and high values of subsurface properties typically due to changing facies types the multi gaussian assumption fails to capture such behavior and to represent the expected variability and distribution of the underlying features for example in fluvial deposits e g river beds braided or meandering channels multi gaussian models cannot characterize the distinct geometry and shape of the relevant geobodies especially when complex geometric features are involved alcolea et al 2006 carle et al 1998 de marsily et al 2005 gómez hernández and wen 1998 guardiano and srivastava 1993 kerrou et al 2008 strebelle 2002 western et al 2001 zinn and harvey 2003 this limitation is primarily due to the use of a covariance model to represent the spatial relations as it only represents point to point correlations without accounting for multiple point data patterns to better represent discrete facies and their distribution several geostatistical simulation techniques are available two traditional approaches for simulating lithofacies distribution that honor a prior statistical representation and various types of measured and interpreted data are pixel based approaches such as sequential indicator simulation chiles and delfiner 2009 goovaerts 1997 isaaks 1991 journel 1983 and object based boolean methods e g marked point process that are capable of describing the continuity in geobodies with well defined shapes chiu et al 2013 deutsch and wang 1996 haldorsen and lake 1984 holden et al 1998 object based methods however lack the flexibility of grid based simulation techniques rendering the data integration aspect particularly difficult more recently multiple point statistics mps caers and zhang 2004 guardiano and srivastava 1993 hu and chugunova 2008 strebelle 2002 has been developed as a flexible grid based alternative simulation approach to object based methods for generating complex geologic patterns that are not amenable to variogram based modeling techniques instead of a covariance model mps relies on a conceptual model of geologic connectivity patterns known as a training image ti to represent the statistical information about the expected patterns in the model a particular implementation of the mps that is used for simulation of the discrete facies patterns is the single normal equation simulation snesim strebelle 2002 the grid based implementation of the snesim algorithm provides simple conditioning of the results to hard data e g facies measurement at well locations and soft data e g seismic in particular integration of hard data is conveniently accomplished by simply assigning the hard data to the corresponding cells and excluding them from the simulation random path journel 2002 remy et al 2009 strebelle 2002 conditioning the output of mps simulation to dynamic flow data is however nontrivial this is primarily due to the nonlinear and indirect relation between hydraulic properties and flow data a number of approaches have been studied for calibration of mps based models against flow data while preserving the complex connectivity in these models these techniques are either based on continuous approximation of the discrete geologic facies in which the complex connectivity features are updated by adjusting a few parameters that control their distribution alcolea and renard 2010 jafarpour 2011 jafarpour and mclaughlin 2008 2009 khaninezhad et al 2012a b zhou et al 2011 or they are based on manipulation of the simulation algorithm to reproduce the dynamic response data caers and hoffman 2006 jafarpour and khodabakhshi 2011 khodabakhshi and jafarpour 2013 mariethoz et al 2010 the latter approach has the advantage of automatically preserving the exact connectivity patterns in the ti which is critical when complex patterns are involved while the former is simpler and amenable to continuous gradient based inversion techniques in this paper we present an adapted pilot points method that can be used for conditioning the simulations of complex facies patterns to nonlinear flow data the motivation is to combine the convenient hard data conditioning i e pilot point values in mps simulation for complex models with integration of flow data by using pilot points method with strategic placement for strategically placing pilot points we use three sources of information sensitivity information variance reduction and flow data to apply the pilot points to mps based facies simulation the original implementation has been adapted in two major ways first the method is modified to include estimation of discrete values facies types second the use of stochastic simulation necessitates sampling and updating multiple realizations of the facies maps and pilot points to implement the modifications first we use mps simulation to generate multiple realizations of facies maps and the corresponding permeability maps that are conditioned on the hard data at wells next we update permeability values at the pilot points using the ensemble smoother es skjervheim and evensen 2011 van leeuwen and evensen 1996 based on the updated ensemble of permeability maps facies types are inferred at the pilot points the resulting updated facies values at the pilot points are in turn used as hard data in mps simulation to generate an updated ensemble of facies maps in our proposed method the mps simulation ensures that the simulated facies patterns are consistent with the ti while the updated pilot points incorporate the information in the dynamic flow data hence both high order statistical information from the ti and nonlinear flow data are incorporated in the calibration process in the next section we introduce the proposed pilot points method followed by a series of synthetic examples and their discussion 2 methodology 2 1 conditional mps facies simulation with pilot points we consider the stochastic simulation of a random function m x m x 1 m x 2 m x n g defined on a grid system with ng cells where the notation m xi refers to the parameter value m at the location xi in sequential simulation instead of characterizing and sampling from the multivariate distribution of the random function the multiplication rule of probability is invoked to sequentially visit each un sampled cell and compute and sample from the corresponding univariate conditional probability i e 1 f c z p m x n 1 z m x 1 m x 2 m x n where m x 1 m x 2 m xn are conditioning or previously sampled data and m x n 1 is the un sampled value at the current simulation cell in practice the conditioning data m x 1 m x 2 m xn are taken from a local neighborhood of x n 1 when the random function m x is multi gaussian all the conditional probabilities are gaussian and can be computed by estimating the corresponding conditional mean and variance of m x n 1 e g using kriging however when the multi gaussian assumption does not hold computing the conditional probability for m x n 1 becomes nontrivial unlike traditional geostatistical simulation techniques that are based on random function models mps simulation utilizes empirical multivariate distributions that are inferred from a training image guardiano and srivastava 1993 strebelle 2002 if m x represents a discrete random process e g facies distribution the corresponding conditional probabilities are simple to describe and can be inferred from a ti that contains the mps information patterns hu and chugunova 2008 strebelle 2002 when using pilot points for conditioning the facies distribution the conditioning data includes the observed facies types at the well locations when they exist and the facies types at the pilot points the facies types at the pilot points parameterize the discrete stochastic random function if np pilot points are chosen the facies types at the np locations are estimated from integration of flow data whereas the facies types at the well locations are considered known and fixed when applied to calibration of discrete facies distribution the discrete facies types at the pilot points must be provided to the snesim algorithm for conditioning however directly estimating these discrete values is complex since the hydraulic properties estimated during data integration are continuous instead the facies types may be inferred using the estimated from the flow data hydraulic properties at the pilot points the details about inferring the facies types from the estimated hydraulic properties will be discussed in section 2 3 2 2 strategic placement of pilot points when placing pilot points it is desirable to select locations that exhibit significant contribution to explaining the parameter variability from observed data e g heads flow rates etc and high potential to reduce the uncertainty in the facies distribution candidates for such points are those that have high potential to be estimated accurately i e flow responses show high sensitivity to those locations and that can maximally reduce the uncertainty in the facies distribution typically these two criteria tend to compete as high sensitivity locations are near well locations which often have low uncertainty in this work we seek to identify locations with high sensitivity and uncertainty let us consider m m 1 m 2 m n e n s r n g n e n s to be the matrix that represents an initial ensemble of the model parameters where ng is the number of cells in the model and nens is the ensemble size for each realization of the model parameters m i let j i r n g n d be the sensitivity matrix of the model output flow simulation output e g rates heads saturation etc with respect to model parameters e g permeability hydraulic conductivity etc where nd is the dimension of measurements since the sensitivity matrix j i is different from realization to realization a global sensitivity matrix is defined as j a u g j 1 j 2 j n e n s r n g n a where na nd nens the first step for placing pilot points is to generate score maps following a similar approach discussed in yang et al 2012 for matrix z j aug the singular value decomposition svd is written as 2 z u σ v t where u u i j r n g n g σ s i j r n g n a v v i j r n a n a note that in σ the off diagonal elements are zeros and the diagonal element sii λ i refers to the ith largest singular value to identify the preferred locations for pilot points the singular vectors corresponding to the first ns singular values such that k 1 n s λ k 2 95 are used using the ns singular vectors the following score map is defined 3 s j k 1 n s λ k 2 u k j l 1 n g u k l j 1 2 n g with this definition regions with high score values in the calculated maps indicate locations for which the model responses show high sensitivity to the corresponding parameters at those locations to identify the high sensitivity regions a s percentile ps is used as the threshold to truncate the original score map defined by eq 3 such that the scores below ps become zeros thus non zero regions in the truncated map represent the high score regions in the original map and the pilot points will be placed using a procedure described in the next section in these non zero regions only to account for the uncertainty in model parameters the parameter sample variance map c d i a g 1 n e n s 1 m m m m t r n g is used where m r n g n e n s is a matrix that has the sample mean in all of its columns if we normalize the variance map onto range 0 1 by a linear transformation s j c j c m i n c m a x c m i n j 1 2 n g and cmin cmax are the minimum and maximum values of the elements in c notice that the normalized variance map is the only information entering the score hence areas that assume low values in the corresponding score map correspond to positions with lower uncertainty therefore to reduce the uncertainty in the parameter space pilot points are placed in the locations with higher score values for estimation of the parameters at the pilot points it is desirable to place the pilot points in locations with high sensitivity and high variance therefore we define a new score map by combining the truncated score maps after normalizing into 0 1 range based on the sensitivity and variance information the combined map is generated through the schur point wise product of the two truncated score maps the schur product of the two score maps represents a grid block based operation to find the intersection of the locations in the domain that have high sensitivity and variance the regions with significant scores in the resulting map correspond to target locations for placing pilot points these resulting regions are covered by randomly placing pilot points in them we note that the use of variance map in obtaining uncertainty score maps only captures the second order statistics of the parameters and does not include higher statistics while variance is not an optimal measure to use for parameters with higher order variability its simplicity and inclusion in the update step of the filter motivate its use for quantifying the prior model parameter uncertainty in our formulation additionally computationally demanding optimization methods can be used with some performance metrics to find optimal locations for pilot points the simple approach used in this work has resulted in satisfactory performance an additional source of information in finding promising locations for pilot points is the flow data we incorporate this information through an iterative data assimilation scheme as discussed in section 3 to place the pilot points on the combined map we consider a region of influence around each pilot point to establish a minimum distance criterion when a pilot point is placed on the map its region of influence is removed from consideration for additional pilot points the shape of the region of influence can be decided based on prior knowledge about the continuity model or based on sensitivity analysis in our study no preference is given to any direction and a symmetric region with radius r from the center of a pilot point is used where r is chosen using sensitivity analysis we note that incorporation of reliable information about the correlation structures in the field is expected to improve the results for example one could use an ellipsoid to define the region of influence based on available prior knowledge this region of influence as well as the truncated map mentioned earlier provides a means to avoid over parameterization in section 3 two approaches used for placing the pilot points are investigated the first approach is deterministic in which the ranking in the score map is used to place the pilot points the second approach randomly places the pilot points in non zero regions on the score map without following any order in both cases no overlap is allowed between the regions of influence of different pilot points the numerical experiments suggest that the random placement is more robust compared with the deterministic one therefore in the proposed method the random placement is used to distribute the pilot points on the score map one advantage of the proposed approaches is that by defining a threshold for the score map and a radius of influence the number and locations of the pilot points are determined systematically notice that although the proposed placement of pilot points is sub optimal and can be further improved it is easy to implement with a little extra computational cost whereas formal optimization of the pilot points placement is not trivial and expected to be computationally demanding 2 3 facies estimation at pilot points after placing the pilot points an inversion method is needed to estimate the parameters at the pilot points using the flow data we note that in our implementation the facies types are not updated directly by integrating the flow data instead the permeability values at the pilot points are estimated first the estimated permeability values are then used to infer the conditional to flow data facies probability at the pilot points following the same approach linear scaling proposed by jafarpour and khodabakhshi jafarpour and khodabakhshi 2011 where the facies probabilities at each pilot point are estimated by linear scaling the ensemble mean of permeability at the same point we denote the two facies types as channel ch and non channel nc and their corresponding permeability values as kch and knc respectively if the ensemble mean of log permeability at a grid cell x is log k x then the probability of channel facies at the grid cell is estimated as p a x c h c p m i n p m a x p m i n log k x l o g k n c l o g k c h l o g k n c where pmin pmax 0 10 0 90 are the bounds of the probability maps in the original pcm to only allow for at most 90 likelihood on the updated results for k x kch and k x knc pmax and pmin values are used respectively as the facies probabilities samples from the facies probability are then drawn to generate an ensemble of facies realizations at the pilot point locations which are subsequently included along with hard data from existing well locations in the snesim algorithm to regenerate an updated ensemble of facies models for data assimilation we use a variant of the ensemble kalman filter enkf known as ensemble kalman smoother or simply ensemble smoother es the enkf is a monte carlo based approximation of the sequential kalman filtering method that allows for nonlinear propagation of uncertainty from model parameters e g hydraulic conductivity permeability into the outputs e g heads flow rates of a dynamical system to approximately represent through samples the prediction uncertainty distribution when measurements become available they are used in a linear analysis scheme to update the parameters states of the system evensen 1994 evensen 2004 following the notation in evensen 2004 the augmented state matrix containing the ensemble members ψ i r n a u g is denoted as 4 a ψ 1 ψ 2 ψ n r n a u g n e n s in this study the ensemble members ψ i are the permeability at the pilot point locations the ensemble mean is stored in each column of a a 1 n where 1 n r n e n s n e n s is a matrix in which all the elements are 1 nens and the perturbation matrix is defined as a a a given a vector of measurements d r n m with nm being the number of measurements the perturbed observations d j d ε j is stored in columns of a matrix d d 1 d 2 d n e n s r n m n e n s the ensemble of perturbed observations is defined as e ϵ 1 ϵ 2 ϵ n e n s r n m n e n s define the sample states covariance and measurement error covariance matrices respectively as 5 p e a a t n 1 r e e e t n 1 the analysis step of the enkf can be expressed as follows 6 a a a k d ha a p e h t h p e h t r e 1 d ha where a a is the updated ensemble k is kalman gain and h is the measurement matrix letting d d ha s ha r n m n e n s and c e ss t ee t eq 6 can be rewritten as 7 a a a i s t c e 1 d ax w h e r e x i s t c e 1 d r n e n s n e n s evensen proposed a robust algorithm to perform the analysis step by calculating a a ax in eq 7 to alleviate the rapid reduction in variance after the update which is a well known issue of the enkf update a damping factor is applied to the kalman gain matrix of the enkf update the resulting update takes the form 8 a d a m p a a α k d ha α ax 1 α a α a a 1 α a where α 0 1 is a damping factor we note that a reduction in the kalman gain can be viewed as inflating the observation error variance and leads to a more modest update the choice of α as a tuning parameter is not immediately obvious and may require trial and error to specify different from enkf es van leeuwen and evensen 1996 is an alternative data assimilation method which avoids the sequential updating of the realizations and the associated restart which is performed after each update step to generate forecasts for the next data assimilation step in es one global update in the space time domain is performed instead of using recursive updates in time as in enkf as a result compared with enkf es provides a significant reduction in simulation time which makes it more efficient and much simpler to implement and apply especially in complex subsurface applications however implementation of es requires multiple iterations which introduce additional computational overhead as mentioned earlier both enkf and es can be used in our proposed method however due to its lower computational cost es is used as the data assimilation of choice in this work 2 4 implementation steps in this section we summarize the implementation steps starting with the initial ensemble of the facies models which are conditioned on hard data from wells the proposed pilot points conditioning with mps simulation is performed using the following steps 1 generate an initial ensemble using the snesim algorithm 2 construct the sample variance map from the initial ensemble 3 perform forward simulation to predict the states and measurements model outputs and the sensitivity information using the adjoint method for each of the ensemble members 4 generate the score maps based on sensitivity and variance information and combine them to obtain a combined truncated score map as discussed in section 2 2 5 using an appropriate radius of influence determine the pilot point locations based on the placement procedure discussed in section 2 2 6 estimate the permeability for each realization at the pilot points using the es data assimilation 7 infer the facies probabilities using the approach proposed in khodabakhshi and jafarpour 2013 and discussed above at the pilot points from the corresponding updated permeability and use them to generate an ensemble of facies types at the pilot points by sampling 8 use the snesim algorithm to generate an ensemble of facies realizations that are conditioned on the hard data and the sampled facies types at the pilot points a major difference between the above implementation with standard enkf and es approach is that the smoother in our approach is used to only update the permeability values at the pilot points locations it s important to note that in the above scheme the pilot points are placed based on the prior information only and no dynamic flow data is involved in their placement an iterative scheme of the proposed method can be formed by repeating step 2 8 that is in each iteration the updated ensemble of facies types from the previous iteration is considered as the initial ensemble at the current iteration consequently the score map as well as the locations and number of pilot points are updated in step 3 5 thus performing these iterations leads to integration of the flow data into the updated facies realizations resulting in updated sensitivity and variance information and hence updated score map unlike the non iterative scheme the iterative form of the updates incorporates the information in the measurements flow data in identifying the pilot point locations hence the iterative scheme provides a mechanism to include flow data in addition to sensitivity and initial uncertainty in placing the pilot points 3 numerical experiments we present two examples in this section example 1 includes a series of numerical experiments to discuss various aspects of the proposed approach example 2 involves a larger and more complex subsurface model in which we show the application of the proposed approach we note that in example 2 we have also performed similar experiments and comparisons as in example 1 with similar conclusion however for brevity we only present the final results from applying the proposed method to example 2 3 1 uniform pilot points placement in example 1 we present a series of numerical experiments as listed in table 1 to evaluate the performance of the proposed workflow and discuss its properties we use two dimensional two phase wetting and non wetting phase flow in subsurface to demonstrate the performance of our method the model in our first experiment consists of a 65 65 grid system with five injection wells on the left and five extraction wells on the right the reference facies distribution is shown in the top plot of fig 1 and consists of channel facies black and background mudstone white it is assumed that the values of permeability and porosity for each facies type are known e g from analysis of core samples and well logs and the only uncertainty is related to the distribution of facies in space away from well locations the pressure at the injection wells and flow rates of both phases at the extraction wells are measured every month over a two year period and used for data assimilation the initial ensemble of facies realizations is conditioned on the hard data the facies types at 10 well locations the second row in fig 1 shows the mean variance and three out of 100 sample realizations of the initial ensemble of the spatial distribution of the log permeability generated using the snesim algorithm strebelle 2002 the initial samples share statistically similar connectivity patterns with the reference model however the exact shape and location of the features are different from those in the reference model since the initial ensemble is conditioned on the measurement at the well locations the samples show less uncertainty about the facies distribution at the vicinity of the wells this can be verified by examining the initial ensemble mean and variance maps in fig 1 second row the main uncertainty is therefore related to the channel locations and connectivity in the middle of the domain in experiment a 1 the standard es is used to assimilate the measurements and update the distribution of log permeability field in the initial ensemble the third row of fig 1 plots the mean variance and three samples from the updated ensemble using the es analysis a comparison with the reference model reveals that the updated maps are closer to the reference model but the exact shape and discrete nature of the channel features are lost this is a general issue that is observed in the application of enkf or es methods to characterization of facies specifically these methods apply second order continuous updates that are not suitable for estimation of discrete facies with higher order statistical patterns unless indirect approaches such as continuous parameterization of facies are used to avoid such inconsistency we propose to use the observed flow data to infer facies types at pilot points and use them to generate conditional samples to infer the facies type following the approach proposed in jafarpour and khodabakhshi 2011 we first find facies probabilities at the pilot point locations as discussed in the description of the method section 2 3 and use them to generate realizations of facies types at the pilot point locations a comparison between the variance maps second column in the second and third rows of fig 1 shows that the low variance regions in the initial ensemble are updated minimally and remain largely unchanged after the update this observation suggests that the locations with low variance in the initial ensemble are not good candidates for placing the pilot points in experiment a 2 the pilot points are uniformly distributed across the entire domain instead of applying a strategic placement the facies types at the pilot points are inferred from the ensemble of updated permeability models in experiment a 1 the last row in fig 1 shows the updated results for the ensemble of log permeability generated after conditioning on uniformly placed pilot points the results demonstrate that uniform placement of pilot points is not a good strategy and may not capture the correct connectivity in facies when the pilot points are placed uniformly some of them may be assigned to low sensitivity regions hence they may not be updated correctly since the estimated facies types at the pilot points are treated as hard data during facies simulation they play a significant role in describing the connectivity of the final facies maps hence it is important to avoid arbitrary updates due to the placement of pilot points in low sensitivity regions this is the primary motivation of our work on strategic placement of pilot points which will be introduced in the next section 3 2 strategic placement of pilot points fig 2 shows the score maps calculated following the approach discussed in section 2 fig 2a and b display the normalized score maps obtained from the sensitivity matrix and its truncated version using a threshold of 75 percentile i e p 75 respectively fig 2c and d depict similar score maps that are computed using the variance map the final score map is obtained by taking the schur product of the two truncated maps as discussed in section 2 2 these score maps and hence the resulting pilot point locations are generated based on prior information the initial ensemble of facies and the corresponding sensitivity information based on predicted model responses that is dynamic flow measurements are not included in placing the pilot points fig 3 uses a simple schematic to illustrate the deterministic placement of pilot points at the beginning the first pilot point is placed at the location with the highest score the next pilot point is then placed to satisfy two requirements i its region of influence does not overlap with those of all previously placed pilot points ii it is at the location with the highest score among all possible locations in the rest of the non zero regions this procedure is repeated until there are no positions in the truncated score map that can accommodate a pilot point without an overlap with previously placed pilot points we note that the region of influence can take a different shape than a circle and may require sensitivity analysis to identify in this example the radius of the circular region r i e the region of influence is assumed to be 7 grid cells in practice sensitivity analysis and field correlation length can be used to specify this radius in the deterministic approach all ensemble members share the same pilot point configuration as discussed earlier in section 2 2 an alternative approach for finding pilot point locations is randomly placing them on the score map instead of using a descending order of the scores the importance of random placement is better appreciated by noting that pilot points that intersect with the channel facies provide more important conditioning information than those that do not intersect the channels hence a random placement approach increases the chance of pilot points hitting the channel facies if the dynamic data supports it in placing the pilot points in the random case overlap between the regions of influence is not permitted in our experiments the ensemble of nens 100 realizations is divided into ng 10 groups and each group is randomly assigned a pilot point configuration to highlight the effect of pilot point placement in the next experiment it is assumed that at the pilot points the facies types are known perfectly that is instead of using model calibration to infer the facies types the exact facies types from the reference model are assigned to the pilot points this is done to ensure that the pilot point placement analysis is not affected by errors in the data integration step fig 4 a top shows the truncated score map and the corresponding pilot points which are placed using the deterministic approach the second row of fig 4a shows the mean variance and three sample realizations of the resulting facies models fig 4b displays three pilot point configurations out of ten that are placed randomly the second row in fig 4b shows the ensemble mean and variance of the facies distribution along with three sample realizations each from a different group in comparison with the results from fig 4a the conditional facies represent the reference map more accurately this and several other examples that were performed over the course of this work indicate that the random placement of pilot points is more robust than the deterministic approach therefore we adopt the random placement approach in our proposed method 3 3 incorporating flow data in pilot point placement in experiment a 3 the non iterative scheme of the proposed method is used in this experiment the facies type at the pilot points is inferred using the method described in section 2 3 after estimating the log permeability maps by applying the standard es the resulting facies types are then used to regenerate conditional realizations from the ti using the snesim algorithm in experiment a 4 we apply multiple updates to the permeability map in an iterative form using the ensemble smoother with multiple data assimilation es mda approach before performing any resampling from the ti that is es mda iterations are first completed to estimate the permeability values at pilot points and then the results are used to resample from the ti experiment a 4 differs from a 3 in that it involves full iterations of es mda prior to resampling from the ti emerick and reynolds 2013 it is important to note that the iterations of es mda are only applied to improve the estimation of the permeability values at the pilot points therefore the pilot point locations are still identified prior to data integration fig 5 a1 and b1 show the ensemble mean of log permeability obtained from the standard es and esmda respectively the corresponding mean variance and three samples from the final ensemble of facies maps after conditioning on the pilot points are shown in fig 5 a2 and a3 for es and in fig 5 b2 and b3 for es mda two additional points are important to stress first although the entire map of updated log permeability is shown only the values at pilot point locations are used in our analysis second the facies types at the pilot points are determined by first identifying the facies probability at each pilot point and then using the resulting facies probabilities to draw samples for facies types at those points prior to conditional resampling with the snesim algorithm to quantify the performance of the two approaches the root mean squared error rmse corresponding to a variable x is calculated for all grid cells rmse x i 1 n e n s j 1 n e n s x i j x i r e f 2 i 1 2 3 n g the total rmse of an ensemble is defined as rms e t o t a l x i 1 n g rmse x i the relative reduction of the total rmse of the updated ensemble xupdate compared to that of the initial ensemble xinitial can be obtained as r rmse x rms e t o t a l x u p d a t e rms e t o t a l x i n i t i a l rms e t o t a l x i n i t i a l the relative reduction of the total rmse of the model parameters is 3 24 and 6 43 for experiment a 3 and a 4 respectively table 2 compared to the standard es the updated mean of facies maps with the es mda is slightly improved however considering the computational overhead associated with multiple iterations used in the es mda algorithm a trade off exists between the incremental improvements versus the computational cost in these two experiments prior information was used for placing the pilot points on the score map an additional and important source of information for identifying promising locations for pilot points is the dynamic measurements e g flow and pressure head data in the next set of experiments a 5 and a 6 the iterative scheme of our proposed method is used in this case in each iteration first the pilot points are placed based on the current ensemble of facies permeability maps from previous iteration the es algorithm is then used to update the log permeability values at the pilot points the facies type at each pilot point is inferred and used with the ti to regenerate i e resample the new ensemble of facies and permeability maps that will be passed to implement the next iteration since the new ensemble of facies maps incorporate the dynamic data this scheme ensures that the flow data is also used in determining the pilot point locations fig 6 a and b show the final ensemble of facies maps after two and four full iterations respectively the results from these two experiments show noticeable improvements over those in fig 5 highlighting the importance of flow data in placing the pilot points in these two experiments the relative reductions of the total rmse of the model parameters are 8 46 and 15 55 respectively table 2 fig 7 a and b display the predicted and observed flow rates for the wetting and non wetting phases at the extraction wells for the initial and updated models from experiment a 5 and a 6 respectively the initial predictions are relatively far from the observed values and show a larger spread the predictions after two and four full iterations show increasingly improved forecasts in experiment a 6 compared to the initial ensemble the relative reductions in the total rmse for the heads at the injection wells and flow rates for both non wetting and wetting phases at the extraction wells for the updated ensemble of facies maps are 26 0 30 1 and 37 6 respectively together the results from the updated facies maps and production forecasts demonstrate noticeable improvements when the iterative scheme is performed to incorporate the flow data both in pilot point placement and facies identification 3 4 example 2 complex facies connectivity patterns we now consider a larger example with more complex connectivity patterns the example also involves two phase flow in a two dimensional model the top panel in fig 8 shows the reference model which consists of a 201 61 grid system with five injection wells diamonds and five extraction wells squares the reference facies distribution consists of channel facies black and background mudstone white the pressure at the injection wells and flow rates of both phases at the extraction wells are measured every month over a two year period and used as conditioning data the initial ensemble of facies realizations is conditioned on the hard data the facies types at 10 well locations the first column in fig 8 shows three sample out of 100 realizations of the initial ensemble of log permeability distributions along with the ensemble mean variance maps while the initial samples share similar connectivity patterns statistically with the reference model they do not represent the exact shape and location of the features in the reference model since the initial ensemble is conditioned on the hard data at the well locations the initial ensemble members show less uncertainty about the facies distribution near the wells using this example with complex connectivity patterns we performed a series of the experiments that are similar to those in example 1 the results in all the experiments were consistent with those of example 1 and show that the proposed method iterative scheme with four full iterations and random pilot point placement outperforms other approaches used in example 1 both in estimating facies distribution and in matching the flow data here for brevity we only show the results of our proposed final approach the second column in fig 8 contains the results after the implementation of the proposed method qualitatively the updated sample better represents the connectivity in the reference model to quantify this performance the corresponding relative reduction of the total rmse in model parameter estimation is calculated to be 19 95 fig 9 a and b display the predicted and observed flow rates of the wetting and non wetting phases at the extraction wells for the initial and updated after four full iterations ensemble respectively the initial predictions are relatively far from the observed values and show a larger spread the predictions after four full iterations show noticeable improvements with the updated models the relative reductions in the total rmse in matching the pressure heads at the injection wells and flow rates for both non wetting and wetting phases at the extraction wells in experiment b 6 are 26 0 30 1 and 37 6 respectively finally unlike iterative inverse modeling formulations that minimize a data mismatch cost function the updates performed using different flavors of the enkf or es by construction do not guarantee an improvement in data match unless such a mechanism is explicitly enforced in addition to this inherent property the stochastic nature of the snesim conditional simulation step which is used to incorporate the facies values at the pilot points introduces additional variability randomness in the facies distribution which can affect the quality of data match one approach to improve the predictive performance of the updated ensemble is to introduce an acceptance rejection sampling step in the last stage of the workflow when the final ensemble of conditional facies is generated in our experiments we observed good data match after our final updates and did not implement an acceptance rejection criterion 4 conclusions we present a novel approach based on the pilot points parameterization for conditioning multiple point statistical simulation of complex geologic facies on nonlinear flow data the pilot points method is originally developed for geostatistical parameterization of model calibration problems using variogram based kriging techniques here we develop a pilot point scheme for conditioning stochastic discrete facies simulation on flow data using ensemble based data assimilation techniques the main motivation behind the proposed approach is to address the difficulty in parameterization of discrete facies for model calibration at least two challenges are faced in calibrating complex geologic facies models i honoring the discrete and non gaussian nature of the model parameters facies distribution and ii preserving their expected complex connectivity patterns the developed approach takes advantage of the convenient hard data conditioning in mps simulation to incorporate the production data this is achieved by first identifying the facies types at a set of pilot point locations from flow data and then using them along with available hard data for conditional facies simulation by drawing conditional samples from a ti the proposed method ensures that the discrete nature of facies and the prior model connectivity in the ti are honored during model calibration to implement the pilot point conditioning approach the first step is a strategic identification of pilot point locations placement of pilot points is carried out by combining the information related to the sample variance to maximize reduction in uncertainty and the sensitivity of the flow responses of the prior models to increase the effectiveness of the updates these criteria are combined to place pilot points in locations that have the potential to make a significant improvement to the prior models and to avoid locations where little variability is present once a score map is constructed a predefined threshold is used to truncate regions with low values of sensitivity and variance and only retain promising regions for placing the pilot points we investigated two techniques for locating pilot points in the truncated score map a deterministic placement method that covers the effective regions based on the ranking of the scores and a random placement strategy where for each subgroup of the ensemble the pilot points are randomly located within the truncated score maps in both cases the pilot points that are introduced into the score map have a minimum distance constraint that is they cover an area with a radius of influence that may not have any overlap with the region of influence of other pilot points this strategy results in automatic identification of the number of pilot points but it also requires a tuning parameter i e the radius of influence for each pilot point the results from our numerical experiments show that the random placement of pilot points is more robust as it covers more points on the score map and increases the chance of intersecting with important facies types e g channels we also show the importance of dynamic data in both determining good candidate locations for pilot points and for identifying the values of the pilot points via data assimilation the dynamic data can be easily incorporated by repeating the workflow with the conditional facies maps that are generated from previous steps by the iterative approach on the workflow the updated facies that contain the flow data are used for placing new pilot point locations the facies estimation results are clearly improved when dynamic data is included in selecting the pilot point locations the developed method presents an important extension to the original pilot points method for stochastic calibration of complex facies distributions against flow data a key advantage of the presented approach is that it automatically reproduces discrete facies models with complex connectivity patterns that are specified in the prior model properties that are difficult to achieve using conventional model calibration techniques acknowledgment the data and simulation results for this paper can be obtained by contacting the corresponding author at jafarpou usc edu 
