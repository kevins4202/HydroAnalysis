index,text
26155,active stormwater control will play an increasingly important role in mitigating urban flooding which is becoming more common with climate change and sea level rise in this paper we describe and demonstrate swmm mpc software developed for simulating model predictive control mpc for urban drainage systems using open source software python and the epa stormwater management model version 5 swmm5 swmm mpc uses an evolutionary algorithm as an optimizer and supports parallel processing in the demonstration case for a hypothetical tidally influenced urban drainage system the swmm mpc control policies for two storage units achieved its objectives of 1 practically eliminating flooding and 2 maintaining the water level at the storage units close to a target level although the current swmm mpc workflow was feasible for a simple model using a desktop pc a high performance computer or cloud based computer with more computational cores would likely be needed for most real world models keywords urban flooding real time control parallel processing model predictive control stormwater urban drainage modeling 1 introduction researchers have predicted that storm intensity will increase on average due to climate change berggren et al 2012 neumann et al 2015 coastal cities have an additional challenge as sea levels rise which makes it more difficult to drain storm runoff from streets coastal cities have already experienced increased flooding from high tidal events alone sweet and park 2014 more intense storms and rising sea levels will put greater stress on urban drainage systems necessitating changes for urban drainage systems to perform at current levels one possible adjustment is to make capital improvements such as increasing pipe size or constructing new storage units another option is to convert drainage systems from passive gravity driven systems to active or smart systems kerkez et al 2016 active systems can increase performance of a urban drainage system at a lower cost than traditional capital improvements meneses et al 2018 actively controlling an urban drainage system does not increase the actual capacity of urban drainage infrastructure but rather more efficiently uses the existing infrastructure increasing its effective capacity for example one part of an active urban drainage system could be a valve at the outlet of a retention basin which can be automatically opened or closed based on system conditions and forecasts with this setup the valve could be closed more during a storm which would utilize the available storage better than would have been possible without the valve for an active urban drainage system to achieve its objective e g minimize flooding reduce combined sewer overflows an effective management strategy is required management decisions for an urban drainage system include which actuators e g valves and pumps in the system should change when to change them and to what setting they should be changed these decisions are referred to as a control policy vrabie et al 2009 mayne et al 2005 langson et al 2004 an effective control policy for an active urban drainage system may depend on a number of factors such as antecedent moisture conditions expected intensity and duration of oncoming rainfall current water levels in the system the condition of the drainage infrastructure and other factors e g tide levels in tidally influenced urban drainage systems a common approach for determining an effective control policy is model predictive control mpc camacho and bordons 2007 mpc has been used effectively in many control applications including automotive controls del re et al 2010 hvac heating ventilation and air conditioning afram and janabi sharifi 2014 and other industrial applications qin and badgwell 2003 mpc has also been used effectively in urban drainage applications lund et al 2018 puig et al 2009 cembrano et al 2004 sch√ºtze et al 2004 gelormino and ricker 1994 in mpc a process model is used to simulate the physical system and evaluate alternative control policies forecast data can be used as input for the simulation during the control period on line optimization is performed meaning that an optimal control policy is found and implemented at each time step camacho and bordons 2007 although capable of finding effective control policies implementing mpc for a urban drainage system is non trivial due to the dynamics within the system the fundamental governing equations for modeling urban drainage systems are the st venant equations which when considered fully are non linear tayfur et al 1993 this makes finding an optimal control policy for urban drainage systems challenging using mpc darsono and labadie 2007 to address this dilemma two alternative approaches are typically employeed the first is to simplify the governing equations of the process model to a linear system this makes the optimization problem solvable using well established procedures such as simplex nelder and mead 1965 gelormino and ricker 1994 took the approach of linearizing their system converting their process model into a linear time invariant model to perform mpc for a large combined sewer system in seattle washington usa the second approach for implementing mpc for urban drainage systems is to retain the non linear st venant equations and use a metaheuristic to find the best control policy at each time step in this approach a true optimization procedure is not possible because the system remains non linear instead a metaheuristic e g an evolutionary algorithm ea can be used gandomi et al 2013 the use of a metaheuristic precludes the possibility of determining a guaranteed optimal control policy and is typically computationally expensive the advantage of this approach however is that the non linear governing equations in the process model are retained this approach was taken by heusch and ostrowski 2011 who used a dynamically dimensioned search for finding the best control policy for their process model they used the united states environmental protection agency s stormwater management model version 5 epa swmm5 which numerically solves the st venant equations huber et al 2005 similar to heusch and ostrowski 2011 we have selected to follow the second approach so that the nonlinearities in the process model can be maintained and to leverage epa swmm5 as the process model epa swmm5 is an attractive choice as a process model for urban drainage systems for several reasons epa swmm5 is in the public domain making it free of charge and its source code is open source making it customizable the model simulates a wide variety of urban drainage structures including active controls such as orifices with variable openings and pumps epa swmm5 has been used in many research applications as well as in engineering practice to model urban drainage systems burger et al 2014 notwithstanding the wide use and utility of epa swmm5 for modeling urban drainage systems and the established utility of mpc as a successful approach for determining effective control policies to our knowledge there is currently no software available for performing mpc using epa swmm5 although heusch and ostrowski 2011 developed software that implements mpc with epa swmm5 that software was closed source and is no longer available this study advances the work done by heusch and ostrowski 2011 by creating open source software for simulating mpc for epa swmm5 swmm mpc and by demonstrating swmm mpc s parallel computing capabilities by making swmm mpc open source other researchers will be able to use improve and build from the source code although the software written by heusch and ostrowski 2011 supported the use of parallel computing this capability which is critical to the usability of such software given its associated computational costs was never demonstrated or tested in the literature the swmm mpc software was written in the python programming language several third party python packages were necessary for the success of this project including pyswmm https github com openwateranalytics pyswmm and the distributed evolutionary algorithms for python deap https github com deap deap to evaluate swmm mpc it was applied to a demonstration model with two simulated active control devices the demonstration model was a hypothetical urban drainage system with a tidally influenced tailwater condition this was chosen since coastal cities are becoming more at risk of flooding and thus may have larger benefits from active controls the swmm mpc results were compared to the results from a rules based approach and a scenario with no active control the software was run on a desktop personal computer pc a high performance computer hpc and a rented cloud based machine to demonstrate and test the parallel processing capability of the software the remainder of this paper describes the methods used to implement swmm mpc including a description of the mpc workflow and the interaction and role of the third party python libraries the use case model is then described and the results of the evaluation are presented and discussed as part of the results and discussion the benefits of parallelization and the use of a high performance and cloud based computing for running swmm mpc are quantified and discussed 2 methods 2 1 overview of mpc for urban drainage systems mpc for an urban drainage system consists of three main components as shown in fig 1 the first component is the physical system including the system states and system controls the system states include hydraulic states such as water levels at nodes and flow rates in pipes and hydrologic states such as watershed soil moisture and runoff in a real system these states would ideally come from real time sensors the system controls are actuators that accept and implement the settings resulting from the mpc process at each time step the second component in mpc is a process model used to simulate the future states of the urban drainage system the process model uses the states read from the urban drainage system as its initial states the process model also takes forecast model inputs such as rainfall or tide level given the current state of the system and future disturbances the process model is used to evaluate the effectiveness of control policy candidates a control policy consists of one setting for each actuator for each control time step for the duration of the control horizon the length of time over which control settings are found an individual setting can be a number as would be the case for a valve where the number would correspond to the percent open of the valve an individual setting can also be a binary setting as would be the case for a pump that can either be on or off as an example consider a system with a variably opening valve and an on off pump with a control horizon of one hour and a control time step of 15 min a control policy for this system would consist of two arrays an array of numbers between 0 and 1 to specify the percent open that the valve should be and an array of on or off to specify the setting of the pump both arrays would have four settings one for each time step in the control horizon of one hour to evaluate the effectiveness of a given control policy the settings in the policy are applied to simulated actuators in the process model and the process model simulation is executed at the end of the simulation a cost is determined for the policy the cost is based on a user defined cost function in this study we consider mainly the cost resulting from flooding but other costs could be considered within this general framework including the costs of combined sewer overflows cso and water quality the cost may also be a factor of other process model outputs such as deviation from target water levels at certain points sch√ºtze et al 2004 the third component of mpc for a urban drainage system is an optimization routine to determine the best control policy for the system using the process model to assign a cost to a given control policy the optimization procedure seeks to find the control policy that incurs the smallest cost if the process model is linear a true optimum can be found using traditional optimization procedures like simplex nelder and mead 1965 if the process model is non linear other approaches must be taken such as using a metaheuristic to find an effective control policy gandomi et al 2013 in summary the chronological workflow for mpc for a urban drainage system is 1 system states are read from the physical system 2 using the system states as initial conditions and future disturbances as input a process model is used to evaluate control policies 3 the best control policy is selected through an optimization procedure and 4 the best control policy is implemented in the real system although the best control policy is obtained for the entire control horizon only the first step in the control policy is used since the procedure re optimizes at every control time step 2 2 mpc for swmm5 swmm mpc implementation of the parts of mpc using python and swmm5 was done in the swmm mpc python package the software simulates online mpc for an urban drainage system using swmm5 as the process model and as the simulated physical system the current system could also be used in an offline mode to find a control policy for a forecast storm event beforehand 2 2 1 simulated urban drainage system owa swmm5 and pyswmm in swmm mpc an enhanced version of swmm5 owa swmm5 https github com openwateranalytics stormwater management model was used via an accompanying python library pyswmm https github com openwateranalytics pyswmm to simulate the physical urban drainage system both owa swmm5 and pyswmm were developed and are distributed by open water analytics owa swmm5 and pyswmm provide three key functionalities needed to simulate the online optimization procedure required by mpc first unlike when a simulation is run via epa swmm5 when using pyswmm custom python routines can be executed between each time step of the simulation this is critical to swmm mpc because at each time step in the mpc simulation workflow three processes occur 1 the states from the simulated urban drainage system are read and transferred to the process model 2 the metaheuristic is run and 3 the best policy found by the metaheuristic needs is implemented in the simulated urban drainage system using pyswmm python code can be run to perform each of these processes at each control time step second pyswmm enables the transfer of system states at each time step from the simulated urban drainage system to the process model this is accomplished through a hotstart file a swmm5 hotstart file contains all of the hydraulic and hydrologic states of the model at the time in the simulation when the hotstart file is saved when a hotstart file is read into a simulation that simulation s initial hydraulic and hydrologic states are the states represented in the hotstart file this functionality is well suited to transfer the states of the simulated urban drainage system to the process model in the swmm mpc workflow using epa swmm5 a hotstart file can be saved only at the end of a simulation this is a critical limitation because in mpc the system states need to be transferred at every time step to address this limitation we added new functionality to owa swmm5 and pyswmm to enable hotstart files to be saved at any point in a swmm5 simulation executed using pyswmm this functionality allowed the system states of the simulated urban drainage system to be transferred to the process model at each time step third through pyswmm the best control policy found by the metaheuristic can be implemented at each time step this is done using pyswmm to change the settings of the actuators in the model during the simulation when a simulation is initialized in pyswmm each object in a swmm5 model every node link subcatchment etc can be read into a python object via its element id as defined in the swmm5 input file each of these python objects has attributes that can be read e g depth at a node and flow in a link actuators in the model read into python objects also have the target setting attribute that can be assigned to implement a control setting for an actuator via pyswmm its target setting is set to the first setting in the best control policy 2 2 2 process model epa swmm5 in addition to representing a real urban drainage system swmm5 was used as the process model however in contrast to using owa swmm5 to simulate the physical urban drainage system the standard epa swmm5 was used as process models this was necessary because the current version of pyswmm is not thread safe this is a functionality needed in swmm mpc because at each time step during the simulation of the urban drainage system at least one process model simulation is run in a predictive fashion to evaluate control policy candidates epa swmm5 unlike pyswmm is thread safe 2 2 3 active controls in epa swmm5 epa swmm5 simulates the active control of certain hydraulic structures including pumps orifices and gates each of these structures has a setting that can be assigned for example the setting for an orifice is a decimal number between 0 and 1 which corresponds to the percent open of the orifice e g a 0 5 setting would mean the orifice was 50 open the user can also define an amount of time for a structure to implement a change in setting this time to change parameter in epa swmm5 represents the delay seen in reality for changing an actuator s setting changing controls during an epa swmm5 simulation is done using one or more control rules see example in fig 2 a control rule is specified in the swmm5 input file before the simulation begins and consists of four parts the first two parts are the rule name and the condition in the example the rule name is r1 the condition is if node j1 depth 2 meaning that the program will check if the depth at the node with the id of j1 is less than 2 the units being defined globally in the model input file as feet or meters in epa swmm5 the condition can be the state at any link or node and can also be related to global simulation states such as the model simulation time the third part of a control rule defines which structure s should change if the specified condition is met in the example the structure that will change is orifice r1 finally the fourth part of the rule defines the setting to which the structure should change in the example this is 0 6 meaning that if the condition is met the orifice should be set to 60 open in swmm mpc a control policy is a time series of control settings one control setting per control time step for the control duration this is implemented in epa swmm5 as a set of control rules since a control policy in mpc is a time series each control rule s condition is based solely on the model s simulation time in decimal hours for example fig 3 shows a control policy of four settings 0 2 0 4 0 5 and 0 2 for orifice r1 at a 15 min control time step implemented as control rules this text would be written to the epa swmm5 process model input file under the controls heading 2 2 4 metaheuristic evolutionary algorithm because we used epa swmm5 as a black box process model a metaheuristic was used in place of a true optimization procedure to find an effective control policy at each time step in the mpc run we chose an evolutionary algorithm ea for the metaheuristic since it has been shown to be successful in other urban drainage control applications zimmer et al 2015 2018 and it s inherent propensity for parallelization maier et al 2014 an ea begins with an initial population of individuals where in our case each individual is a control policy a fitness score or conversely a cost is assigned to each individual in the population and certain individuals are selected to survive into the next generation based on their fitness score mechanisms for improving the fitness of the individuals from one generation to the next mimic natural processes including cross over and mutation maier et al 2014 the process of selection and improvement is repeated from generation to generation until a stopping criteria is met common stopping criteria include a user defined number of generations or an acceptably low rate of improvement from one generation to the next the use of an ea requires several user defined parameters including the number of individuals in the initial population the cross over rate the mutation rate and the stopping criteria in swmm mpc policies are transcribed to a string of bits in the genetic algorithm the settings for orifices and weirs are represented by three bits with eight total possibilities 0 8 open 1 8 open 8 8 open we limited the settings of orifices and weirs to three bits to limit the solution space explored by the algorithm the setting of a pump is represented by only one binary digit 1 or on 0 or off since the ea searches for the policy that incurs the minimum cost the way in which a cost is assigned to each individual control policy has a large impact on the ea s effectiveness in swmm mpc the cost of a control policy is determined using the process model and a cost function first each individual control policy is implemented in the process model input file as a set of control rules as described above once the control policy is implemented the epa swmm5 model is executed elements of the model output resulting from the process model execution become input for the cost function eqn 1 the cost function used in swmm mpc is 1 cost Œ± a v u x Œ≤ b d u x where a v b d are each 1 dimensional vectors u and x are 2 dimensional vectors and Œ± and Œ≤ are scalers the members of a are user defined weight values for flooding at any node in the system and the members of v are the flood volumes at each node over the entire simulation as calculated by the process model the members of b are user defined weights for deviation from user defined target water levels at each node in the system and the members of d are the average absolute deviations from target water levels again as calculated by the process model over the entire simulation u is represents the control policies for all controls for each time step x represents the system states Œ± and Œ≤ are user defined constants used to scale and give overall weights to flooding costs compared to deviation costs typically weights for the components of the cost or objective function sum to 1 and can include a scaling factor to account for variables in different units or scales kim and de weck 2005 in this formulation Œ± and Œ≤ include both the weight and the scaling factors for the objectives we intentionally made this cost function flexible so that users can customize it to meet their objectives which may vary between use cases a cost for flooding is obviously important as that is a major concern for many communities and the prevention of which is one of the main purposes for urban drainage systems we also included a cost from deviations for target water levels because in certain cases it is desirable to maintain water levels close to a target depth for example it may be important to keep a certain amount of water in a retention pond for aesthetic and or ecological purposes more components to the cost function could be added by users according to their needs such as water quality parameters although the cost function is flexible when implemented in swmm mpc the user need only define what is important to the specific application for example default for a is a vector of all 1 s when one node is specified the weight of any unspecified node becomes zero the default for b is all zeros since the user has to specify a target depth for a given node to execute eas we used the distributed evolutionary algorithms for python deap https github com deap deap library an advantage of eas is that they can easily be run in parallel since they perform many independent evaluations maier et al 2014 in deap parallel processing is supported through integration with the built in multiprocessing python library through the multiprocessing library users can specify how many computational cores should be used to distribute the evaluation of the individual control policies in each ga generation 2 2 5 swmm mpc workflow the mpc workflow in swmm mpc was implemented using three main python functions see fig 4 the function in the workflow called by the user is run swmm mpc this function runs the mpc workflow and calls the two other main functions run swmm mpc takes 13 user inputs as shown in table 1 through these inputs the user specifies the model input file that represents the urban drainage system the control inputs i e which controls to find a policy for the control time step and the control horizon and ea parameters e g number of generations cost function parameters the most complex of the user supplied arguments are target depth dict and node flood wgt dict see fig 5 for examples these two arguments define the a and b variables in the cost function additionally the target depth dict argument is used to determine d these arguments map from python data structures to the mathematical variables in the cost function the target depth dict argument is a dictionary whose keys are node ids and whose values are dictionaries the inner dictionary has two keys the target depth of the node and the weight of the cost for deviations from the weight at the node in fig 5 the target depth dict specifies that the target depths of nodes st1 and st2 are 4 0 and 3 5 respectively the weights are also specified deviation from the target depth at node st1 will be twice as costly as deviation from node st2 the node flood wgt dict is a simpler dictionary the keys of which are node ids and the values are weights in fig 5 the node flood weight dict specifies that flooding at node j1 is five times costlier than flooding at node st1 note that if one or more node is included in the target depth dict or the node flood wgt dict other nodes are not included in the cost calculation in terms of the cost function the corresponding weights in a and b are zero this is shown in fig 5 the weight of deviations from a water level at node j1 and the weight of flooding at node st2 would both be zero since they are not included in the dictionaries in the run swmm mpc function the swmm5 model simulating the urban drainage system is run step by step via pyswmm at the beginning of the simulation the swmm5 input file representing the urban drainage system is copied this copy serves as the input file used for the process model to ensure that the states and simulation periods of process model remain in sync with the simulated urban drainage system at each time step a hotstart file from the urban drainage system simulation is saved and then used as the initial states for the process model the process model s simulation start date and time are also updated to match the urban drainage system simulation s current date and time once the process model s simulation start date and time is same as the simulated urban drainage system and the hotstart file of the simulated urban drainage system is saved the run ea function is called the run ea function initiates the ea which starts by creating an initial population of individual control policies in our case an individual is a 1 dimensional vector each member of which is a setting for an individual actuator for one control time step the initial population for the first time step is a group of random individuals for subsequent time steps elitism is used where the best policy found in the previous time step is used to seed the initial population of the current time step for example consider a control policy for i control time steps for j controls represented by a i x j vector u for the first time step the settings for time 0 through time i are found however only the first setting is implemented because settings that minimize cost for times 1 2 i have also been found by the optimizer in swmm mpc these settings are used as the basis for seeding the population for the next time step this base individual is then varied randomly to create several similar individuals some randomly created individuals are also added to the population for the next generation to create variety in the policies and to ensure the number of policies is equal to the initial population size specified by the user the control policies initiated in the run ea function are input into the third main function evaluate the evaluate function makes a copy of the process model input file and the input hotstart file to avoid file naming conflicts a random string is appended to the hotstart and input file names the control policy is then implemented in the newly created input file by adding corresponding control rules once the control policy is implemented in the input file the simulation is executed with epa swmm5 when the simulation run is completed the evaluate function parses the output file to determine v and d in the cost function the policy s cost can then be determined since the remaining cost function parameters Œ± a Œ≤ and b are user defined the evaluation of an individual control policy is independent of all others therefore the evaluate function is the part of the workflow that is parallelized through python s multiprocessing module using the cost that has been assigned to each policy the run ea function selects the best individuals to retain in the population of policies for the next generation after the user defined number of generations are complete the best policy found by the ea is implemented in the simulated urban drainage system in the run swmm mpc function the policy is also saved and used to seed the population of the next time step finally the setting for that time step is recorded so that at the end of the simulation the best control policy for the entire simulation time is saved if the cost at a time step is ever zero the algorithm stops since the performance cannot improve further 2 3 system demonstration to demonstrate the utility and functionality of swmm mpc three cases a b and c with increasing complexity were implemented in a simple swmm5 model three control scenarios passive rules based and mpc were applied to each of the cases the computational costs of running swmm mpc were also quantified 2 3 1 demonstration model and rainfall event a schema of the model used to demonstrate swmm mpc is shown in fig 6 and the model properties are given in table 2 since the intent of this portion of the paper is to offer a simple example application the software a simple demonstration model was used however any swmm5 model with controllable features can be used with swmm mpc the demonstration model has two subcatchments s1 and s2 for the example use case we used a 1 year 12 h design storm for norfolk virginia a coastal city that experiences frequent flooding mitchell et al 2013 the design storm see fig 7 had 78 2 mm of total rainfall bonnin et al 2018 with an scs type ii temporal distribution mockus 2012 the use of design storms is typical when designing and modeling stormwater infrastructure for example villarreal et al 2004 and hatt et al 2009 to introduce some spatial variation in the model the rain for s2 started 18 min after the start of the rain event for s1 s2 is smaller but flashier with a smaller width and a larger percent of impervious surfaces this is manifested in a higher peak flow shown in fig 7 the subcatchment s1 and s2 drain directly to two storage units in parallel st1 and st2 respectively in swmm5 models storage units are generic and are used to represent both natural storage facilities such as a pond as well as man made facilities such as an underground tank or retention pond two orifices r1 and r2 control the flow out of two storage units the two orifices from the storage units meet and flow through a junction j1 before leaving the system through the outfall the model simulation time was 24 h and the routing time step was ten seconds in the mpc and passive scenarios and five seconds in the rules based scenario 2 3 2 demonstration cases we implemented and compared the three control scenarios for three use cases with different objectives and model constraints increasing in complexity see table 3 in case a the simplest of the three cases the only objective was to minimize flooding in case b an additional objective of maintaining a target water depth of 0 5 m at the storage units was added in case c the objective was the same as case b but a tidal boundary condition at the outfall was added see fig 8 since cases b and c are multi objective Œ± and Œ≤ had to be selected based on the relative importance and the relative scale of each the units of v were gallons in the swmm5 demonstration model much smaller in magnitude in our case than the deviations from target water levels which were in units of feet to scale the two variables and emphasize minimizing flooding at node j1 over minimizing deviations from the target water depths at the storage units we set the cost of flooding Œ± 2000 times larger than the cost of deviations Œ≤ 1000 compared to 0 5 for cases b and c 2 3 3 control scenarios 2 3 3 1 scenario 1 passive in this scenario there is no active control the outlets are 100 open at all times the system drains by gravity alone there are no orifices or pumps in the system 2 3 3 2 scenario 2 rule based control for this scenario simple logical rules controlled the orifice openings and therefore the discharge from the storage units in practice such rules can be based on experience and knowledge gained by local stormwater personnel over time although heuristic based rules alter a dynamic actuated system the rules themselves are static meaning that they do not change for the duration of an event furthermore the rules do not adjust based on forecast conditions there were two sets of rules used in the demonstration cases for case a where the only objective was to reduce flooding the rules see fig 9 specified to open the orifice completely as long as the depth in the downstream node j1 was below 0 49 m 80 of the maximum depth additionally to avoid overtopping if the storage units reached 1 49 m 98 of the maximum depth the orifices were to open to avoid overtopping of the storage units the rules for cases b and c were the same these rules see fig 10 were similar to the case a rules however in addition to reducing flooding the rules for cases b and c also took into account the target depth of 0 52 m in these rules the orifices were open when the depth at node j1 was below 0 49 m and when the depth in the storage units was above the target water depth the orifices were closed when the depth at node j1 was above 0 49 m or when the depth in the storage units was below the target depth the control rules scenarios for each case were run via pyswmm with a control time step of 5 s 2 3 3 3 scenario 3 mpc the mpc control policy was found using swmm mpc as described in section 2 2 5 above one advantage of mpc over the rule based control is the ability to adjust the actuators based on forecast conditions for the use cases we used a control time step of 15 min and a control horizon of one hour therefore with two controls a single control policy consisted of a vector of eight values 2 controls x 4 control steps per hour x 1 h see table 4 the number of individuals for the initial population of policies in the genetic algorithm was 120 and the number of generations was 8 2 3 4 use of parallel high performance and cloud computing the ea used for selecting the best control policy is computationally expensive and therefore some analysis of computational costs for executing the swmm mpc workflow was performed the wall clock times for case a the more complex of the two cases were compared when using a typical personal computer pc and the university of virginia s high performance computing hpc system rivanna recognizing that many likely most municipalities will not have hpc resources available to them we also explored the use of a commercial cloud computing service for running swmm mpc these services such as amazon web services google cloud platform or microsoft azure allow users to rent large powerful computers charging only for the time that the computers are being used to explore the option of renting a cloud based machine we also executed case a through google cloud platform gcp the number of cores available ram and processor speeds of the pc hpc and gcp machines are listed in table 5 case a was run with a varying number of cores on each platform 3 results and discussion 3 1 results from demonstration cases figs 11 13 show the results of the three control scenarios applied to the three demonstration cases in all cases cases a b and c there are times where flooding occurred at one or more nodes flooding occurs in swmm5 when the depth of water at a given node exceeds the maximum depth of that node when this occurs the depth in swmm5 at that node is recorded as the maximum depth which is why some of the depth values in figs 11 13 appear capped fig 11 shows the results from the three control scenarios for case a in the rules based and mpc scenarios the control policies kept the valves closed more on average thus retaining more water in the storage units and preventing flooding at node j1 see fig 11 a and b the water level at st2 reaches much higher values in the rules based and swmm mpc scenarios peak of 1 16 m 3 80 ft and 1 14 m 3 73 ft compared to the passive peak of 1 05 m 3 45 ft in case a the rules based control and the swmm mpc scenario eliminated flooding altogether this was the simplest control case and the algorithm in swmm mpc was able to find a policy that eliminated flooding by hour 3 this policy resulted in a cost function equaling zero which stopped the algorithm from running therefore after hour 4 the orifice settings were unchanged in the swmm mpc scenario the rules based scenario in fig 11 causes oscillation around the 80 full depth at j1 0 49 as a result of the control rules for case b a target water depth at st1 and st2 of 0 52 m was introduced and the initial depth at st1 and st2 was 0 52 m with more water in the system to begin with much more flooding occurred in general compared to case a and neither the rules based nor the swmm mpc control was not able eliminate flooding the swmm mpc scenario however reduced flooding by 90 compared to the passive scenario where the rules based reduced flooding by only 54 the main reason for the better performance by the swmm mpc scenario was its ability to take action proactively and prioritize reducing flooding over maintaining the target depth the swmm mpc algorithm allowed the water level at st2 to go well below the target depth at the beginning of the simulation to create more storage for when the peak of the runoff arrived additionally the swmm mpc policy held more water back in st1 than the rules based scenario more fully utilizing the storage available in the larger of the two storage units because the rules in the rules based scenario maintained the water at the target depth in st2 there was not enough storage available to handle the peak of the runoff and the storage unit was overtopped the addition of the tidal boundary condition in case c caused increased flooding for all three control scenarios again the swmm mpc was able to significantly reduce flooding resulting in a 74 reduction compared to the passive scenario the swmm mpc policy held even more water back and held it back for longer in st1 in case c compared to case b this reduced flooding in j1 during the second high tide which occurred around 19 h the swmm mpc policy did not outperform the rules based scenario by as much as case b the rules based scenario reduced flooding by 58 in case c a possible explanation is that with the additional water volume from the tidal condition the system may be reaching its physical limits in terms of total available storage 3 2 computational cost of swmm mpc fig 14 shows the wall clock times for executing swmm mpc for case c on a pc an hpc and gcp machines with a varying number of processing cores the simulation had 96 control time steps 15 min resolution for 24 h if used for online mpc in a real case the wall clock time required for one time step would need to be less than the time step itself otherwise the setting for the next time step would not be determined before it would need to be implemented the fastest wall clock time using the pc was 214 7 min using eight computational cores the maximum available therefore the time required to find the best control policy at each control time step was 2 2 min in this case the pc s computing power was sufficient 2 2 min per time step compared to 15 min time step for the hpc the best case scenario was a wall clock time of 34 8 min 0 36 min per time step using the maximum of 28 computational cores although the minimum wall clock time was achieved using all 28 cores on the hpc the improvement in wall clock time when increasing the number of cores past 16 was minimal this is a relevant consideration when using a shared hpc resource where requesting more computational cores likely corresponds to a longer wait in the job queue the wall clock times using gcp were much lower than the pc or hpc in the best case 32 vcpus and 120 gb of ram were used for a wall clock time of 25 6 min 0 27 min per time step this is a 1 3 speed up compared to the fastest run using the hpc and almost a 8 speed up compared to the fastest pc run the financial cost of renting this machine was 1 71 per hour the gcp hardware is newer than the hpc hardware which may explain why the wall clock time is lower even when the same number of computational cores was used 3 3 practical considerations 3 3 1 computational costs the execution times for our example use case were viable however a more complex model a smaller control time step or different ea parameters more generations or individuals per generation would increase the execution time for example the demonstration swmm5 model required only one second or less to execute the swmm mpc workflow executed that model thousands of times in our cases the model was run more than 70 000 times 24 h simulation x 4 time steps per hour x 8 generations per time step x approx 80 individuals per generation therefore requiring approximately 70 000 s of computation time if each model takes approx 1 s to run if a more sophisticated swmm5 model instance were used the execution time would be much higher for example in related research we are using a more complex model of the stormwater infrastructure for a neighborhood in norfolk virginia that requires close to 60 s to execute for a 24 h simulation time period the wall clock time for swmm mpc for this more complex model would increase by around a factor of 60 compared to the simple cases demonstrated here assuming a linear increase the wall clock time would be 132 min time step using the pc thus rendering it unfeasible for running on a pc again assuming linear scaling using 32 cores on gcp the same simulation would execute at a rate of 16 7 min time step just over the 15 min times step cutoff to be feasible with this setup fewer generations or fewer individuals per generation would have to be used to reduce the number of model runs per time step alternatively a reduced complexity model could be used to reduce the swmm5 runtime as further discussed below another factor to consider for practical use of swmm mpc is the control horizon and the number of control structures whose policies will be found using swmm mpc these two parameters determine the size of the overall control policy and therefore the solution space that the ea will be searching in our example use case the control policy was a vector of 24 bits therefore there were possible solutions this solution space already large would double if the control time step were 7 5 min instead of 15 or if the control horizon were two hours instead of one a larger solution space would result in a larger computation time to reach an effective solution given the computational cost of the current swmm mpc approach the required complexity and thus the wall clock time of the process model is an important consideration a scenario with a simple process model approx 1 s wall clock time can be feasibly executed with just a pc as shown in the system demonstration however a simple model may not represent complex urban drainage systems well enough to produce an effective control policy defining what level of detail is sufficient in the process model may be difficult however as it may depend on the objective of the modeling a certain storm event or the system itself there is a trade off among 1 model complexity 2 model run time and 3 the model s ability to effectively simulate the relevant parts of the system to achieve a stated objective and support decision makers this trade off is very significant to the use of process models in a receding control horizon approach such as mpc and needs further research if a more complex model is needed municipalities or others needing cloud based resources to run swmm mpc must consider the financial cost of renting a machine using the 32 core machine on gcp the cost of finding the control policy for the 24 h time span in case c in our system demonstration was very low 0 72 this was however for one simple case the cost would be higher with more complex scenarios such as a more complex model a shorter time step or more controls if we assume the use of a more complex model which takes 60 s to run would increase running time by a factor of 60 that would also increase the cost by a factor of 60 to 43 78 however the system would only run if there were a storm in the forecast large enough to require active control to mitigate given the computational cost of running the evolutionary algorithm other more efficient alternatives should be explored in future research one possible alternative that is reinforcement learning kaelbling et al 1996 this approach may be able to converge to a solution more quickly than an evolutionary algorithm and thus reduce run times and computational another future improvement could be adding a penalty to changing actuator states and or using another dynamic optimizer to have a less erratic behavior in the actuators 3 3 2 data and modeling uncertainty the current design of swmm mpc does not take into account the uncertainties in the system states the forecast data and the process model because the swmm5 engine is used to simulate both the urban drainage system and the process model the process model assumes 1 perfect knowledge of the urban drainage system states 2 perfect knowledge of future disturbances and 3 perfect modeling of the urban drainage system in a real implementation there would be significant uncertainties in each of these aspects in a real implementation knowledge of the system states is available only from a limited number of sensors in the system this data limited in spatial and temporal resolution would need to be estimated using a state estimator to set all the states in the system as is done in power systems abur and exp√≥sito 2004 more work will need to be done to investigate ways of incorporating sensor values to set the process model s initial conditions additionally in the current case the future disturbances i e primarily rainfall are known perfectly when in reality there is a large amount of uncertainty involved with forecasting such disturbances see for example hong and pai 2007 valverde ram√≠rez et al 2005 and bellon and austin 1984 regarding uncertainty in forecasting rainfall in addition to data uncertainties seen in reality swmm mpc does not currently consider gaps between the simulated behavior through the swmm5 process model and the actual behavior of the urban drainage system but assumes that simulation and reality are the same in actuality the gap between simulation and reality in urban drainage systems can be significant see for example mark et al 2004 on a related note the ability for swmm mpc to find a control policy that is effective for the urban drainage system is directly related to how well the process model represents the system given the simulation and data gaps seen in reality the simulated results through policies found by swmm mpc should be considered as the best case scenario and if the same policies were used in practice any effects should be expected to be seen to a lesser extent further research is needed to determine the degree to which the results from the policies implemented in reality will differ compared to the simulation results swmm mpc finds only the one best control policy which minimizes a single cost value defined by a set of weights for various objectives this is also known as the weighted sum method for multi objective optimization marler and arora 2010 this method has some disadvantages including the inability to explore the solutions that have similar performance along the pareto front das and dennis 1997 future work could be done in swmm mpc to support the exploration of pareto optimal solutions and the trade offs between objectives 4 conclusions a free and open source software package swmm mpc was developed which computes a control policy for controls within an urban drainage system model the widely used united states environmental protection agency stormwater management model version 5 swmm5 is used to simulate the urban drainage system and as the process model a third party python library pyswmm is a critical component of the swmm mpc workflow allowing a swmm5 model to be run step by step in a python environment an evolutionary algorithm was used to find an effective control policy at each time step when tested using a simple swmm5 model the swmm mpc software was able to produce control policies that met objectives including minimizing flooding and minimizing deviation from target water levels at certain nodes in the system swmm mpc leverages parallel computing to run the computationally expensive evolutionary algorithm more quickly the wall clock time for a simple swmm5 model for a 24 h simulation was reduced from 307 min to 214 min when the computational cores on a desktop pc were increased from two to eight the wall clock time was reduced even further to 34 8 min on a 28 core high performance computer and to 25 6 min on a 32 core machine rented through the google cloud platform parallel computing will be necessary to make swmm mpc feasible for use in real time control of a real world drainage system with complex process models as average storm intensity is projected to increase and sea levels are expected to continue to rise cities globally and especially on the coasts can expect more intense and frequent flood conditions active control of urban drainage systems will be part of a portfolio of approaches used when confronting these challenges the swmm mpc software we have developed can be used built from and improved upon as a tool to assist decision makers and researchers in finding effective control policies for active control of urban drainage systems software availability the swmm mpc software is open source and available for use and improvement on github at https github com uvadmist swmm mpc a docker image of swmm mpc is also available at https hub docker com r jsadler2 swmm mpc the demonstration model is published on hydroshare sadler 2018 acknowledgments we acknowledge the support of awards 1735587 critical and resilient infrastructure systems crisp and 1737432 smart and connected communities s cc from the united states national science foundation we thank bryant mcdonnell one of the main developers of pyswmm for his assistance in adding the hotstart file functionality to owa swmm5 and pyswmm we also thank the consultants of the university of virginia graduate writing lab including kelly cunningham its director for their helpful feedback in preparing the manuscript appendix a supplementary data the following is are the supplementary data to this article multimedia component multimedia component appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 009 
26155,active stormwater control will play an increasingly important role in mitigating urban flooding which is becoming more common with climate change and sea level rise in this paper we describe and demonstrate swmm mpc software developed for simulating model predictive control mpc for urban drainage systems using open source software python and the epa stormwater management model version 5 swmm5 swmm mpc uses an evolutionary algorithm as an optimizer and supports parallel processing in the demonstration case for a hypothetical tidally influenced urban drainage system the swmm mpc control policies for two storage units achieved its objectives of 1 practically eliminating flooding and 2 maintaining the water level at the storage units close to a target level although the current swmm mpc workflow was feasible for a simple model using a desktop pc a high performance computer or cloud based computer with more computational cores would likely be needed for most real world models keywords urban flooding real time control parallel processing model predictive control stormwater urban drainage modeling 1 introduction researchers have predicted that storm intensity will increase on average due to climate change berggren et al 2012 neumann et al 2015 coastal cities have an additional challenge as sea levels rise which makes it more difficult to drain storm runoff from streets coastal cities have already experienced increased flooding from high tidal events alone sweet and park 2014 more intense storms and rising sea levels will put greater stress on urban drainage systems necessitating changes for urban drainage systems to perform at current levels one possible adjustment is to make capital improvements such as increasing pipe size or constructing new storage units another option is to convert drainage systems from passive gravity driven systems to active or smart systems kerkez et al 2016 active systems can increase performance of a urban drainage system at a lower cost than traditional capital improvements meneses et al 2018 actively controlling an urban drainage system does not increase the actual capacity of urban drainage infrastructure but rather more efficiently uses the existing infrastructure increasing its effective capacity for example one part of an active urban drainage system could be a valve at the outlet of a retention basin which can be automatically opened or closed based on system conditions and forecasts with this setup the valve could be closed more during a storm which would utilize the available storage better than would have been possible without the valve for an active urban drainage system to achieve its objective e g minimize flooding reduce combined sewer overflows an effective management strategy is required management decisions for an urban drainage system include which actuators e g valves and pumps in the system should change when to change them and to what setting they should be changed these decisions are referred to as a control policy vrabie et al 2009 mayne et al 2005 langson et al 2004 an effective control policy for an active urban drainage system may depend on a number of factors such as antecedent moisture conditions expected intensity and duration of oncoming rainfall current water levels in the system the condition of the drainage infrastructure and other factors e g tide levels in tidally influenced urban drainage systems a common approach for determining an effective control policy is model predictive control mpc camacho and bordons 2007 mpc has been used effectively in many control applications including automotive controls del re et al 2010 hvac heating ventilation and air conditioning afram and janabi sharifi 2014 and other industrial applications qin and badgwell 2003 mpc has also been used effectively in urban drainage applications lund et al 2018 puig et al 2009 cembrano et al 2004 sch√ºtze et al 2004 gelormino and ricker 1994 in mpc a process model is used to simulate the physical system and evaluate alternative control policies forecast data can be used as input for the simulation during the control period on line optimization is performed meaning that an optimal control policy is found and implemented at each time step camacho and bordons 2007 although capable of finding effective control policies implementing mpc for a urban drainage system is non trivial due to the dynamics within the system the fundamental governing equations for modeling urban drainage systems are the st venant equations which when considered fully are non linear tayfur et al 1993 this makes finding an optimal control policy for urban drainage systems challenging using mpc darsono and labadie 2007 to address this dilemma two alternative approaches are typically employeed the first is to simplify the governing equations of the process model to a linear system this makes the optimization problem solvable using well established procedures such as simplex nelder and mead 1965 gelormino and ricker 1994 took the approach of linearizing their system converting their process model into a linear time invariant model to perform mpc for a large combined sewer system in seattle washington usa the second approach for implementing mpc for urban drainage systems is to retain the non linear st venant equations and use a metaheuristic to find the best control policy at each time step in this approach a true optimization procedure is not possible because the system remains non linear instead a metaheuristic e g an evolutionary algorithm ea can be used gandomi et al 2013 the use of a metaheuristic precludes the possibility of determining a guaranteed optimal control policy and is typically computationally expensive the advantage of this approach however is that the non linear governing equations in the process model are retained this approach was taken by heusch and ostrowski 2011 who used a dynamically dimensioned search for finding the best control policy for their process model they used the united states environmental protection agency s stormwater management model version 5 epa swmm5 which numerically solves the st venant equations huber et al 2005 similar to heusch and ostrowski 2011 we have selected to follow the second approach so that the nonlinearities in the process model can be maintained and to leverage epa swmm5 as the process model epa swmm5 is an attractive choice as a process model for urban drainage systems for several reasons epa swmm5 is in the public domain making it free of charge and its source code is open source making it customizable the model simulates a wide variety of urban drainage structures including active controls such as orifices with variable openings and pumps epa swmm5 has been used in many research applications as well as in engineering practice to model urban drainage systems burger et al 2014 notwithstanding the wide use and utility of epa swmm5 for modeling urban drainage systems and the established utility of mpc as a successful approach for determining effective control policies to our knowledge there is currently no software available for performing mpc using epa swmm5 although heusch and ostrowski 2011 developed software that implements mpc with epa swmm5 that software was closed source and is no longer available this study advances the work done by heusch and ostrowski 2011 by creating open source software for simulating mpc for epa swmm5 swmm mpc and by demonstrating swmm mpc s parallel computing capabilities by making swmm mpc open source other researchers will be able to use improve and build from the source code although the software written by heusch and ostrowski 2011 supported the use of parallel computing this capability which is critical to the usability of such software given its associated computational costs was never demonstrated or tested in the literature the swmm mpc software was written in the python programming language several third party python packages were necessary for the success of this project including pyswmm https github com openwateranalytics pyswmm and the distributed evolutionary algorithms for python deap https github com deap deap to evaluate swmm mpc it was applied to a demonstration model with two simulated active control devices the demonstration model was a hypothetical urban drainage system with a tidally influenced tailwater condition this was chosen since coastal cities are becoming more at risk of flooding and thus may have larger benefits from active controls the swmm mpc results were compared to the results from a rules based approach and a scenario with no active control the software was run on a desktop personal computer pc a high performance computer hpc and a rented cloud based machine to demonstrate and test the parallel processing capability of the software the remainder of this paper describes the methods used to implement swmm mpc including a description of the mpc workflow and the interaction and role of the third party python libraries the use case model is then described and the results of the evaluation are presented and discussed as part of the results and discussion the benefits of parallelization and the use of a high performance and cloud based computing for running swmm mpc are quantified and discussed 2 methods 2 1 overview of mpc for urban drainage systems mpc for an urban drainage system consists of three main components as shown in fig 1 the first component is the physical system including the system states and system controls the system states include hydraulic states such as water levels at nodes and flow rates in pipes and hydrologic states such as watershed soil moisture and runoff in a real system these states would ideally come from real time sensors the system controls are actuators that accept and implement the settings resulting from the mpc process at each time step the second component in mpc is a process model used to simulate the future states of the urban drainage system the process model uses the states read from the urban drainage system as its initial states the process model also takes forecast model inputs such as rainfall or tide level given the current state of the system and future disturbances the process model is used to evaluate the effectiveness of control policy candidates a control policy consists of one setting for each actuator for each control time step for the duration of the control horizon the length of time over which control settings are found an individual setting can be a number as would be the case for a valve where the number would correspond to the percent open of the valve an individual setting can also be a binary setting as would be the case for a pump that can either be on or off as an example consider a system with a variably opening valve and an on off pump with a control horizon of one hour and a control time step of 15 min a control policy for this system would consist of two arrays an array of numbers between 0 and 1 to specify the percent open that the valve should be and an array of on or off to specify the setting of the pump both arrays would have four settings one for each time step in the control horizon of one hour to evaluate the effectiveness of a given control policy the settings in the policy are applied to simulated actuators in the process model and the process model simulation is executed at the end of the simulation a cost is determined for the policy the cost is based on a user defined cost function in this study we consider mainly the cost resulting from flooding but other costs could be considered within this general framework including the costs of combined sewer overflows cso and water quality the cost may also be a factor of other process model outputs such as deviation from target water levels at certain points sch√ºtze et al 2004 the third component of mpc for a urban drainage system is an optimization routine to determine the best control policy for the system using the process model to assign a cost to a given control policy the optimization procedure seeks to find the control policy that incurs the smallest cost if the process model is linear a true optimum can be found using traditional optimization procedures like simplex nelder and mead 1965 if the process model is non linear other approaches must be taken such as using a metaheuristic to find an effective control policy gandomi et al 2013 in summary the chronological workflow for mpc for a urban drainage system is 1 system states are read from the physical system 2 using the system states as initial conditions and future disturbances as input a process model is used to evaluate control policies 3 the best control policy is selected through an optimization procedure and 4 the best control policy is implemented in the real system although the best control policy is obtained for the entire control horizon only the first step in the control policy is used since the procedure re optimizes at every control time step 2 2 mpc for swmm5 swmm mpc implementation of the parts of mpc using python and swmm5 was done in the swmm mpc python package the software simulates online mpc for an urban drainage system using swmm5 as the process model and as the simulated physical system the current system could also be used in an offline mode to find a control policy for a forecast storm event beforehand 2 2 1 simulated urban drainage system owa swmm5 and pyswmm in swmm mpc an enhanced version of swmm5 owa swmm5 https github com openwateranalytics stormwater management model was used via an accompanying python library pyswmm https github com openwateranalytics pyswmm to simulate the physical urban drainage system both owa swmm5 and pyswmm were developed and are distributed by open water analytics owa swmm5 and pyswmm provide three key functionalities needed to simulate the online optimization procedure required by mpc first unlike when a simulation is run via epa swmm5 when using pyswmm custom python routines can be executed between each time step of the simulation this is critical to swmm mpc because at each time step in the mpc simulation workflow three processes occur 1 the states from the simulated urban drainage system are read and transferred to the process model 2 the metaheuristic is run and 3 the best policy found by the metaheuristic needs is implemented in the simulated urban drainage system using pyswmm python code can be run to perform each of these processes at each control time step second pyswmm enables the transfer of system states at each time step from the simulated urban drainage system to the process model this is accomplished through a hotstart file a swmm5 hotstart file contains all of the hydraulic and hydrologic states of the model at the time in the simulation when the hotstart file is saved when a hotstart file is read into a simulation that simulation s initial hydraulic and hydrologic states are the states represented in the hotstart file this functionality is well suited to transfer the states of the simulated urban drainage system to the process model in the swmm mpc workflow using epa swmm5 a hotstart file can be saved only at the end of a simulation this is a critical limitation because in mpc the system states need to be transferred at every time step to address this limitation we added new functionality to owa swmm5 and pyswmm to enable hotstart files to be saved at any point in a swmm5 simulation executed using pyswmm this functionality allowed the system states of the simulated urban drainage system to be transferred to the process model at each time step third through pyswmm the best control policy found by the metaheuristic can be implemented at each time step this is done using pyswmm to change the settings of the actuators in the model during the simulation when a simulation is initialized in pyswmm each object in a swmm5 model every node link subcatchment etc can be read into a python object via its element id as defined in the swmm5 input file each of these python objects has attributes that can be read e g depth at a node and flow in a link actuators in the model read into python objects also have the target setting attribute that can be assigned to implement a control setting for an actuator via pyswmm its target setting is set to the first setting in the best control policy 2 2 2 process model epa swmm5 in addition to representing a real urban drainage system swmm5 was used as the process model however in contrast to using owa swmm5 to simulate the physical urban drainage system the standard epa swmm5 was used as process models this was necessary because the current version of pyswmm is not thread safe this is a functionality needed in swmm mpc because at each time step during the simulation of the urban drainage system at least one process model simulation is run in a predictive fashion to evaluate control policy candidates epa swmm5 unlike pyswmm is thread safe 2 2 3 active controls in epa swmm5 epa swmm5 simulates the active control of certain hydraulic structures including pumps orifices and gates each of these structures has a setting that can be assigned for example the setting for an orifice is a decimal number between 0 and 1 which corresponds to the percent open of the orifice e g a 0 5 setting would mean the orifice was 50 open the user can also define an amount of time for a structure to implement a change in setting this time to change parameter in epa swmm5 represents the delay seen in reality for changing an actuator s setting changing controls during an epa swmm5 simulation is done using one or more control rules see example in fig 2 a control rule is specified in the swmm5 input file before the simulation begins and consists of four parts the first two parts are the rule name and the condition in the example the rule name is r1 the condition is if node j1 depth 2 meaning that the program will check if the depth at the node with the id of j1 is less than 2 the units being defined globally in the model input file as feet or meters in epa swmm5 the condition can be the state at any link or node and can also be related to global simulation states such as the model simulation time the third part of a control rule defines which structure s should change if the specified condition is met in the example the structure that will change is orifice r1 finally the fourth part of the rule defines the setting to which the structure should change in the example this is 0 6 meaning that if the condition is met the orifice should be set to 60 open in swmm mpc a control policy is a time series of control settings one control setting per control time step for the control duration this is implemented in epa swmm5 as a set of control rules since a control policy in mpc is a time series each control rule s condition is based solely on the model s simulation time in decimal hours for example fig 3 shows a control policy of four settings 0 2 0 4 0 5 and 0 2 for orifice r1 at a 15 min control time step implemented as control rules this text would be written to the epa swmm5 process model input file under the controls heading 2 2 4 metaheuristic evolutionary algorithm because we used epa swmm5 as a black box process model a metaheuristic was used in place of a true optimization procedure to find an effective control policy at each time step in the mpc run we chose an evolutionary algorithm ea for the metaheuristic since it has been shown to be successful in other urban drainage control applications zimmer et al 2015 2018 and it s inherent propensity for parallelization maier et al 2014 an ea begins with an initial population of individuals where in our case each individual is a control policy a fitness score or conversely a cost is assigned to each individual in the population and certain individuals are selected to survive into the next generation based on their fitness score mechanisms for improving the fitness of the individuals from one generation to the next mimic natural processes including cross over and mutation maier et al 2014 the process of selection and improvement is repeated from generation to generation until a stopping criteria is met common stopping criteria include a user defined number of generations or an acceptably low rate of improvement from one generation to the next the use of an ea requires several user defined parameters including the number of individuals in the initial population the cross over rate the mutation rate and the stopping criteria in swmm mpc policies are transcribed to a string of bits in the genetic algorithm the settings for orifices and weirs are represented by three bits with eight total possibilities 0 8 open 1 8 open 8 8 open we limited the settings of orifices and weirs to three bits to limit the solution space explored by the algorithm the setting of a pump is represented by only one binary digit 1 or on 0 or off since the ea searches for the policy that incurs the minimum cost the way in which a cost is assigned to each individual control policy has a large impact on the ea s effectiveness in swmm mpc the cost of a control policy is determined using the process model and a cost function first each individual control policy is implemented in the process model input file as a set of control rules as described above once the control policy is implemented the epa swmm5 model is executed elements of the model output resulting from the process model execution become input for the cost function eqn 1 the cost function used in swmm mpc is 1 cost Œ± a v u x Œ≤ b d u x where a v b d are each 1 dimensional vectors u and x are 2 dimensional vectors and Œ± and Œ≤ are scalers the members of a are user defined weight values for flooding at any node in the system and the members of v are the flood volumes at each node over the entire simulation as calculated by the process model the members of b are user defined weights for deviation from user defined target water levels at each node in the system and the members of d are the average absolute deviations from target water levels again as calculated by the process model over the entire simulation u is represents the control policies for all controls for each time step x represents the system states Œ± and Œ≤ are user defined constants used to scale and give overall weights to flooding costs compared to deviation costs typically weights for the components of the cost or objective function sum to 1 and can include a scaling factor to account for variables in different units or scales kim and de weck 2005 in this formulation Œ± and Œ≤ include both the weight and the scaling factors for the objectives we intentionally made this cost function flexible so that users can customize it to meet their objectives which may vary between use cases a cost for flooding is obviously important as that is a major concern for many communities and the prevention of which is one of the main purposes for urban drainage systems we also included a cost from deviations for target water levels because in certain cases it is desirable to maintain water levels close to a target depth for example it may be important to keep a certain amount of water in a retention pond for aesthetic and or ecological purposes more components to the cost function could be added by users according to their needs such as water quality parameters although the cost function is flexible when implemented in swmm mpc the user need only define what is important to the specific application for example default for a is a vector of all 1 s when one node is specified the weight of any unspecified node becomes zero the default for b is all zeros since the user has to specify a target depth for a given node to execute eas we used the distributed evolutionary algorithms for python deap https github com deap deap library an advantage of eas is that they can easily be run in parallel since they perform many independent evaluations maier et al 2014 in deap parallel processing is supported through integration with the built in multiprocessing python library through the multiprocessing library users can specify how many computational cores should be used to distribute the evaluation of the individual control policies in each ga generation 2 2 5 swmm mpc workflow the mpc workflow in swmm mpc was implemented using three main python functions see fig 4 the function in the workflow called by the user is run swmm mpc this function runs the mpc workflow and calls the two other main functions run swmm mpc takes 13 user inputs as shown in table 1 through these inputs the user specifies the model input file that represents the urban drainage system the control inputs i e which controls to find a policy for the control time step and the control horizon and ea parameters e g number of generations cost function parameters the most complex of the user supplied arguments are target depth dict and node flood wgt dict see fig 5 for examples these two arguments define the a and b variables in the cost function additionally the target depth dict argument is used to determine d these arguments map from python data structures to the mathematical variables in the cost function the target depth dict argument is a dictionary whose keys are node ids and whose values are dictionaries the inner dictionary has two keys the target depth of the node and the weight of the cost for deviations from the weight at the node in fig 5 the target depth dict specifies that the target depths of nodes st1 and st2 are 4 0 and 3 5 respectively the weights are also specified deviation from the target depth at node st1 will be twice as costly as deviation from node st2 the node flood wgt dict is a simpler dictionary the keys of which are node ids and the values are weights in fig 5 the node flood weight dict specifies that flooding at node j1 is five times costlier than flooding at node st1 note that if one or more node is included in the target depth dict or the node flood wgt dict other nodes are not included in the cost calculation in terms of the cost function the corresponding weights in a and b are zero this is shown in fig 5 the weight of deviations from a water level at node j1 and the weight of flooding at node st2 would both be zero since they are not included in the dictionaries in the run swmm mpc function the swmm5 model simulating the urban drainage system is run step by step via pyswmm at the beginning of the simulation the swmm5 input file representing the urban drainage system is copied this copy serves as the input file used for the process model to ensure that the states and simulation periods of process model remain in sync with the simulated urban drainage system at each time step a hotstart file from the urban drainage system simulation is saved and then used as the initial states for the process model the process model s simulation start date and time are also updated to match the urban drainage system simulation s current date and time once the process model s simulation start date and time is same as the simulated urban drainage system and the hotstart file of the simulated urban drainage system is saved the run ea function is called the run ea function initiates the ea which starts by creating an initial population of individual control policies in our case an individual is a 1 dimensional vector each member of which is a setting for an individual actuator for one control time step the initial population for the first time step is a group of random individuals for subsequent time steps elitism is used where the best policy found in the previous time step is used to seed the initial population of the current time step for example consider a control policy for i control time steps for j controls represented by a i x j vector u for the first time step the settings for time 0 through time i are found however only the first setting is implemented because settings that minimize cost for times 1 2 i have also been found by the optimizer in swmm mpc these settings are used as the basis for seeding the population for the next time step this base individual is then varied randomly to create several similar individuals some randomly created individuals are also added to the population for the next generation to create variety in the policies and to ensure the number of policies is equal to the initial population size specified by the user the control policies initiated in the run ea function are input into the third main function evaluate the evaluate function makes a copy of the process model input file and the input hotstart file to avoid file naming conflicts a random string is appended to the hotstart and input file names the control policy is then implemented in the newly created input file by adding corresponding control rules once the control policy is implemented in the input file the simulation is executed with epa swmm5 when the simulation run is completed the evaluate function parses the output file to determine v and d in the cost function the policy s cost can then be determined since the remaining cost function parameters Œ± a Œ≤ and b are user defined the evaluation of an individual control policy is independent of all others therefore the evaluate function is the part of the workflow that is parallelized through python s multiprocessing module using the cost that has been assigned to each policy the run ea function selects the best individuals to retain in the population of policies for the next generation after the user defined number of generations are complete the best policy found by the ea is implemented in the simulated urban drainage system in the run swmm mpc function the policy is also saved and used to seed the population of the next time step finally the setting for that time step is recorded so that at the end of the simulation the best control policy for the entire simulation time is saved if the cost at a time step is ever zero the algorithm stops since the performance cannot improve further 2 3 system demonstration to demonstrate the utility and functionality of swmm mpc three cases a b and c with increasing complexity were implemented in a simple swmm5 model three control scenarios passive rules based and mpc were applied to each of the cases the computational costs of running swmm mpc were also quantified 2 3 1 demonstration model and rainfall event a schema of the model used to demonstrate swmm mpc is shown in fig 6 and the model properties are given in table 2 since the intent of this portion of the paper is to offer a simple example application the software a simple demonstration model was used however any swmm5 model with controllable features can be used with swmm mpc the demonstration model has two subcatchments s1 and s2 for the example use case we used a 1 year 12 h design storm for norfolk virginia a coastal city that experiences frequent flooding mitchell et al 2013 the design storm see fig 7 had 78 2 mm of total rainfall bonnin et al 2018 with an scs type ii temporal distribution mockus 2012 the use of design storms is typical when designing and modeling stormwater infrastructure for example villarreal et al 2004 and hatt et al 2009 to introduce some spatial variation in the model the rain for s2 started 18 min after the start of the rain event for s1 s2 is smaller but flashier with a smaller width and a larger percent of impervious surfaces this is manifested in a higher peak flow shown in fig 7 the subcatchment s1 and s2 drain directly to two storage units in parallel st1 and st2 respectively in swmm5 models storage units are generic and are used to represent both natural storage facilities such as a pond as well as man made facilities such as an underground tank or retention pond two orifices r1 and r2 control the flow out of two storage units the two orifices from the storage units meet and flow through a junction j1 before leaving the system through the outfall the model simulation time was 24 h and the routing time step was ten seconds in the mpc and passive scenarios and five seconds in the rules based scenario 2 3 2 demonstration cases we implemented and compared the three control scenarios for three use cases with different objectives and model constraints increasing in complexity see table 3 in case a the simplest of the three cases the only objective was to minimize flooding in case b an additional objective of maintaining a target water depth of 0 5 m at the storage units was added in case c the objective was the same as case b but a tidal boundary condition at the outfall was added see fig 8 since cases b and c are multi objective Œ± and Œ≤ had to be selected based on the relative importance and the relative scale of each the units of v were gallons in the swmm5 demonstration model much smaller in magnitude in our case than the deviations from target water levels which were in units of feet to scale the two variables and emphasize minimizing flooding at node j1 over minimizing deviations from the target water depths at the storage units we set the cost of flooding Œ± 2000 times larger than the cost of deviations Œ≤ 1000 compared to 0 5 for cases b and c 2 3 3 control scenarios 2 3 3 1 scenario 1 passive in this scenario there is no active control the outlets are 100 open at all times the system drains by gravity alone there are no orifices or pumps in the system 2 3 3 2 scenario 2 rule based control for this scenario simple logical rules controlled the orifice openings and therefore the discharge from the storage units in practice such rules can be based on experience and knowledge gained by local stormwater personnel over time although heuristic based rules alter a dynamic actuated system the rules themselves are static meaning that they do not change for the duration of an event furthermore the rules do not adjust based on forecast conditions there were two sets of rules used in the demonstration cases for case a where the only objective was to reduce flooding the rules see fig 9 specified to open the orifice completely as long as the depth in the downstream node j1 was below 0 49 m 80 of the maximum depth additionally to avoid overtopping if the storage units reached 1 49 m 98 of the maximum depth the orifices were to open to avoid overtopping of the storage units the rules for cases b and c were the same these rules see fig 10 were similar to the case a rules however in addition to reducing flooding the rules for cases b and c also took into account the target depth of 0 52 m in these rules the orifices were open when the depth at node j1 was below 0 49 m and when the depth in the storage units was above the target water depth the orifices were closed when the depth at node j1 was above 0 49 m or when the depth in the storage units was below the target depth the control rules scenarios for each case were run via pyswmm with a control time step of 5 s 2 3 3 3 scenario 3 mpc the mpc control policy was found using swmm mpc as described in section 2 2 5 above one advantage of mpc over the rule based control is the ability to adjust the actuators based on forecast conditions for the use cases we used a control time step of 15 min and a control horizon of one hour therefore with two controls a single control policy consisted of a vector of eight values 2 controls x 4 control steps per hour x 1 h see table 4 the number of individuals for the initial population of policies in the genetic algorithm was 120 and the number of generations was 8 2 3 4 use of parallel high performance and cloud computing the ea used for selecting the best control policy is computationally expensive and therefore some analysis of computational costs for executing the swmm mpc workflow was performed the wall clock times for case a the more complex of the two cases were compared when using a typical personal computer pc and the university of virginia s high performance computing hpc system rivanna recognizing that many likely most municipalities will not have hpc resources available to them we also explored the use of a commercial cloud computing service for running swmm mpc these services such as amazon web services google cloud platform or microsoft azure allow users to rent large powerful computers charging only for the time that the computers are being used to explore the option of renting a cloud based machine we also executed case a through google cloud platform gcp the number of cores available ram and processor speeds of the pc hpc and gcp machines are listed in table 5 case a was run with a varying number of cores on each platform 3 results and discussion 3 1 results from demonstration cases figs 11 13 show the results of the three control scenarios applied to the three demonstration cases in all cases cases a b and c there are times where flooding occurred at one or more nodes flooding occurs in swmm5 when the depth of water at a given node exceeds the maximum depth of that node when this occurs the depth in swmm5 at that node is recorded as the maximum depth which is why some of the depth values in figs 11 13 appear capped fig 11 shows the results from the three control scenarios for case a in the rules based and mpc scenarios the control policies kept the valves closed more on average thus retaining more water in the storage units and preventing flooding at node j1 see fig 11 a and b the water level at st2 reaches much higher values in the rules based and swmm mpc scenarios peak of 1 16 m 3 80 ft and 1 14 m 3 73 ft compared to the passive peak of 1 05 m 3 45 ft in case a the rules based control and the swmm mpc scenario eliminated flooding altogether this was the simplest control case and the algorithm in swmm mpc was able to find a policy that eliminated flooding by hour 3 this policy resulted in a cost function equaling zero which stopped the algorithm from running therefore after hour 4 the orifice settings were unchanged in the swmm mpc scenario the rules based scenario in fig 11 causes oscillation around the 80 full depth at j1 0 49 as a result of the control rules for case b a target water depth at st1 and st2 of 0 52 m was introduced and the initial depth at st1 and st2 was 0 52 m with more water in the system to begin with much more flooding occurred in general compared to case a and neither the rules based nor the swmm mpc control was not able eliminate flooding the swmm mpc scenario however reduced flooding by 90 compared to the passive scenario where the rules based reduced flooding by only 54 the main reason for the better performance by the swmm mpc scenario was its ability to take action proactively and prioritize reducing flooding over maintaining the target depth the swmm mpc algorithm allowed the water level at st2 to go well below the target depth at the beginning of the simulation to create more storage for when the peak of the runoff arrived additionally the swmm mpc policy held more water back in st1 than the rules based scenario more fully utilizing the storage available in the larger of the two storage units because the rules in the rules based scenario maintained the water at the target depth in st2 there was not enough storage available to handle the peak of the runoff and the storage unit was overtopped the addition of the tidal boundary condition in case c caused increased flooding for all three control scenarios again the swmm mpc was able to significantly reduce flooding resulting in a 74 reduction compared to the passive scenario the swmm mpc policy held even more water back and held it back for longer in st1 in case c compared to case b this reduced flooding in j1 during the second high tide which occurred around 19 h the swmm mpc policy did not outperform the rules based scenario by as much as case b the rules based scenario reduced flooding by 58 in case c a possible explanation is that with the additional water volume from the tidal condition the system may be reaching its physical limits in terms of total available storage 3 2 computational cost of swmm mpc fig 14 shows the wall clock times for executing swmm mpc for case c on a pc an hpc and gcp machines with a varying number of processing cores the simulation had 96 control time steps 15 min resolution for 24 h if used for online mpc in a real case the wall clock time required for one time step would need to be less than the time step itself otherwise the setting for the next time step would not be determined before it would need to be implemented the fastest wall clock time using the pc was 214 7 min using eight computational cores the maximum available therefore the time required to find the best control policy at each control time step was 2 2 min in this case the pc s computing power was sufficient 2 2 min per time step compared to 15 min time step for the hpc the best case scenario was a wall clock time of 34 8 min 0 36 min per time step using the maximum of 28 computational cores although the minimum wall clock time was achieved using all 28 cores on the hpc the improvement in wall clock time when increasing the number of cores past 16 was minimal this is a relevant consideration when using a shared hpc resource where requesting more computational cores likely corresponds to a longer wait in the job queue the wall clock times using gcp were much lower than the pc or hpc in the best case 32 vcpus and 120 gb of ram were used for a wall clock time of 25 6 min 0 27 min per time step this is a 1 3 speed up compared to the fastest run using the hpc and almost a 8 speed up compared to the fastest pc run the financial cost of renting this machine was 1 71 per hour the gcp hardware is newer than the hpc hardware which may explain why the wall clock time is lower even when the same number of computational cores was used 3 3 practical considerations 3 3 1 computational costs the execution times for our example use case were viable however a more complex model a smaller control time step or different ea parameters more generations or individuals per generation would increase the execution time for example the demonstration swmm5 model required only one second or less to execute the swmm mpc workflow executed that model thousands of times in our cases the model was run more than 70 000 times 24 h simulation x 4 time steps per hour x 8 generations per time step x approx 80 individuals per generation therefore requiring approximately 70 000 s of computation time if each model takes approx 1 s to run if a more sophisticated swmm5 model instance were used the execution time would be much higher for example in related research we are using a more complex model of the stormwater infrastructure for a neighborhood in norfolk virginia that requires close to 60 s to execute for a 24 h simulation time period the wall clock time for swmm mpc for this more complex model would increase by around a factor of 60 compared to the simple cases demonstrated here assuming a linear increase the wall clock time would be 132 min time step using the pc thus rendering it unfeasible for running on a pc again assuming linear scaling using 32 cores on gcp the same simulation would execute at a rate of 16 7 min time step just over the 15 min times step cutoff to be feasible with this setup fewer generations or fewer individuals per generation would have to be used to reduce the number of model runs per time step alternatively a reduced complexity model could be used to reduce the swmm5 runtime as further discussed below another factor to consider for practical use of swmm mpc is the control horizon and the number of control structures whose policies will be found using swmm mpc these two parameters determine the size of the overall control policy and therefore the solution space that the ea will be searching in our example use case the control policy was a vector of 24 bits therefore there were possible solutions this solution space already large would double if the control time step were 7 5 min instead of 15 or if the control horizon were two hours instead of one a larger solution space would result in a larger computation time to reach an effective solution given the computational cost of the current swmm mpc approach the required complexity and thus the wall clock time of the process model is an important consideration a scenario with a simple process model approx 1 s wall clock time can be feasibly executed with just a pc as shown in the system demonstration however a simple model may not represent complex urban drainage systems well enough to produce an effective control policy defining what level of detail is sufficient in the process model may be difficult however as it may depend on the objective of the modeling a certain storm event or the system itself there is a trade off among 1 model complexity 2 model run time and 3 the model s ability to effectively simulate the relevant parts of the system to achieve a stated objective and support decision makers this trade off is very significant to the use of process models in a receding control horizon approach such as mpc and needs further research if a more complex model is needed municipalities or others needing cloud based resources to run swmm mpc must consider the financial cost of renting a machine using the 32 core machine on gcp the cost of finding the control policy for the 24 h time span in case c in our system demonstration was very low 0 72 this was however for one simple case the cost would be higher with more complex scenarios such as a more complex model a shorter time step or more controls if we assume the use of a more complex model which takes 60 s to run would increase running time by a factor of 60 that would also increase the cost by a factor of 60 to 43 78 however the system would only run if there were a storm in the forecast large enough to require active control to mitigate given the computational cost of running the evolutionary algorithm other more efficient alternatives should be explored in future research one possible alternative that is reinforcement learning kaelbling et al 1996 this approach may be able to converge to a solution more quickly than an evolutionary algorithm and thus reduce run times and computational another future improvement could be adding a penalty to changing actuator states and or using another dynamic optimizer to have a less erratic behavior in the actuators 3 3 2 data and modeling uncertainty the current design of swmm mpc does not take into account the uncertainties in the system states the forecast data and the process model because the swmm5 engine is used to simulate both the urban drainage system and the process model the process model assumes 1 perfect knowledge of the urban drainage system states 2 perfect knowledge of future disturbances and 3 perfect modeling of the urban drainage system in a real implementation there would be significant uncertainties in each of these aspects in a real implementation knowledge of the system states is available only from a limited number of sensors in the system this data limited in spatial and temporal resolution would need to be estimated using a state estimator to set all the states in the system as is done in power systems abur and exp√≥sito 2004 more work will need to be done to investigate ways of incorporating sensor values to set the process model s initial conditions additionally in the current case the future disturbances i e primarily rainfall are known perfectly when in reality there is a large amount of uncertainty involved with forecasting such disturbances see for example hong and pai 2007 valverde ram√≠rez et al 2005 and bellon and austin 1984 regarding uncertainty in forecasting rainfall in addition to data uncertainties seen in reality swmm mpc does not currently consider gaps between the simulated behavior through the swmm5 process model and the actual behavior of the urban drainage system but assumes that simulation and reality are the same in actuality the gap between simulation and reality in urban drainage systems can be significant see for example mark et al 2004 on a related note the ability for swmm mpc to find a control policy that is effective for the urban drainage system is directly related to how well the process model represents the system given the simulation and data gaps seen in reality the simulated results through policies found by swmm mpc should be considered as the best case scenario and if the same policies were used in practice any effects should be expected to be seen to a lesser extent further research is needed to determine the degree to which the results from the policies implemented in reality will differ compared to the simulation results swmm mpc finds only the one best control policy which minimizes a single cost value defined by a set of weights for various objectives this is also known as the weighted sum method for multi objective optimization marler and arora 2010 this method has some disadvantages including the inability to explore the solutions that have similar performance along the pareto front das and dennis 1997 future work could be done in swmm mpc to support the exploration of pareto optimal solutions and the trade offs between objectives 4 conclusions a free and open source software package swmm mpc was developed which computes a control policy for controls within an urban drainage system model the widely used united states environmental protection agency stormwater management model version 5 swmm5 is used to simulate the urban drainage system and as the process model a third party python library pyswmm is a critical component of the swmm mpc workflow allowing a swmm5 model to be run step by step in a python environment an evolutionary algorithm was used to find an effective control policy at each time step when tested using a simple swmm5 model the swmm mpc software was able to produce control policies that met objectives including minimizing flooding and minimizing deviation from target water levels at certain nodes in the system swmm mpc leverages parallel computing to run the computationally expensive evolutionary algorithm more quickly the wall clock time for a simple swmm5 model for a 24 h simulation was reduced from 307 min to 214 min when the computational cores on a desktop pc were increased from two to eight the wall clock time was reduced even further to 34 8 min on a 28 core high performance computer and to 25 6 min on a 32 core machine rented through the google cloud platform parallel computing will be necessary to make swmm mpc feasible for use in real time control of a real world drainage system with complex process models as average storm intensity is projected to increase and sea levels are expected to continue to rise cities globally and especially on the coasts can expect more intense and frequent flood conditions active control of urban drainage systems will be part of a portfolio of approaches used when confronting these challenges the swmm mpc software we have developed can be used built from and improved upon as a tool to assist decision makers and researchers in finding effective control policies for active control of urban drainage systems software availability the swmm mpc software is open source and available for use and improvement on github at https github com uvadmist swmm mpc a docker image of swmm mpc is also available at https hub docker com r jsadler2 swmm mpc the demonstration model is published on hydroshare sadler 2018 acknowledgments we acknowledge the support of awards 1735587 critical and resilient infrastructure systems crisp and 1737432 smart and connected communities s cc from the united states national science foundation we thank bryant mcdonnell one of the main developers of pyswmm for his assistance in adding the hotstart file functionality to owa swmm5 and pyswmm we also thank the consultants of the university of virginia graduate writing lab including kelly cunningham its director for their helpful feedback in preparing the manuscript appendix a supplementary data the following is are the supplementary data to this article multimedia component multimedia component appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 009 
26156,connectivity is a state of the art concept that improves the current understanding of hydrological processes at multiple scales two point connectivity statistics provide a promising approach to quantify hydrologic connectivity which measures the probability of any two nodes in a hydrologically relevant pattern that are connected based on their separation distances however limited computational capacity has been the main constraint for implementations in large gridded patterns 1 million nodes here we propose a new algorithm based on array vectorization and convolutional neural network operators convolution and pooling that leverages parallel computational capacity of a gpu test results suggested that the new algorithm significantly increases the computational efficiency and is also sensitive to the variability of connectivity states and robust to complex topography we envision that our algorithm can pave the way for investigating behaviors in large scale e g watershed processes based on quantifying the connectivity of gridded hydrological patterns and digital elevation models keywords connectivity statistics gpu accelerated computing gridded hydrological patterns vectorization convolutional neural network 1 introduction hydrologic connectivity is generally viewed as pathways of water and water mediated substances such as pollutants and sediments between spatial locations in a watershed during a finite temporal interval bracken and croke 2007 research to date has recognized connectivity analysis as a promising approach to improve our understanding of watershed scale hydrologic behavior e g tockner et al 1999 heathwaite et al 2005 kn√∂sche 2006 recent works on connectivity have focused on landscape features and topographic gradients interpreted using pattern analysis spatial statistics percolation theory or graph theory aurousseau et al 2009 gascuel odoux et al 2009 janzen and mcdonnell 2015 knudby and carrera 2005 connectivity has been considered a potential alternative for interpreting watershed runoff generation mechanisms watershed models would better describe the physical process controls and achieve higher accuracy if model conceptualization could be based on the connectedness of linear or nonlinear temporal and spatial networks for a large number of small scale reservoirs and constrained by landscape and environmental factors such as topography soil moisture content and vegetation phillips et al 2011 one widely used approach is two point connectivity statistics i e connectivity functions that quantify and characterize the probability of connected direction and magnitude for spatial patterns of hydrological conditions such as soil moisture conditions and flow processes on the terrain surface ali and roy 2010 connectivity functions measure the probability of two locations that are connected western et al 2001 and have also been applied to watersheds in different hydroclimatic regimes e g meerkerk et al 2009 ali and roy 2010 the concept of connectivity functions in spatial pattern analysis has a close counterpart in traditional geostatistical techniques i e the experimental variogram cressie 1993 which examines the correlation strength of relevant spatial data between any two locations based on search radiuses or separation distances regardless of continuity connectivity functions are also separation distance based metrics but with a specific emphasis on connectedness unlike traditional variogram analysis connectivity functions explicitly describe the spatial variability of connectivity by measuring the probability of two paired locations that are connected by a continuous pathway in a specific hydrological condition e g high soil moisture western et al 2001 theoretically connectivity functions can be scaled up from the plot or field scale to the watershed scale simply by extending the spatial coverage of the hydrological patterns being evaluated the impedance effects physical constraints for the connected fluxes pathways on surface or subsurface topography e g in porous media for geometrically determined trajectories can also be captured as directional connectivity functions in 3d in addition to omnidirectional trajectories in 2d western et al 2001 renard and allard 2013 the temporal variability of connectivity states during single storm events can also be quantified by computing the integral of connectivity functions on all separation distances i e the integral connectivity scale lengths icsls for a temporal snapshot of the hydrologic patterns e g drainage pathways or inundation patterns within the event duration ali and roy 2010 meerkerk et al 2009 the charm of the icsls is simplicity they explicitly describe the spatial connectivity states of the hydrological patterns across different separation distances in a single index the icsls provide a direct and intuitive way of measuring and comparing connectivity states within or between hydrologic systems e g watersheds however studies of hydrologic connectivity using the icsls have almost exclusively focused on low resolution spatial patterns of a limited total number of nodes e g western et al 2001 ali and roy 2010 few attempts have been made to scale up the computation of the icsls to high resolution gridded patterns across spatial scales meerkerk et al 2009 high resolution connectivity analysis across spatial scales can provide critical information on driving factors for hydrologic processes and rainfall runoff modeling that may have been missed in work using much smaller and coarser resolutions ali et al 2018 the two most likely reasons for the lack of work to date on connectivity analysis of high resolution hydrologic gridded patterns include the following 1 sample collection for such hydrologic patterns requires considerable time and resources and 2 the analysis of large amounts of spatial data is highly computationally intensive advances in high performance computing and remote sensing techniques are providing promising ways of reducing or eliminating several limitations of implementing icsls and providing high resolution data across spatial scales correia and rego 1998 devantier and feldman 1993 siart et al 2009 vieux 2016 these advances allow the quantification of connectivity states for high resolution hydrological patterns covering a large watershed e g 10 10 000 km2 using a series of temporal snapshots in areas of different physical dynamics such as hillslopes floodplains and stream channels however the original algorithm for icsls proposed by western et al 2001 is implemented using single cell loops and recursions and is not suitable for implementation on high resolution gridded patterns because of the long computation time low memory usage efficiency and susceptibility to errors such as stack overflow a preliminary study based on the original algorithm suggested that the computational time for grid patterns of more than 1 million cells equivalent to a 1 km2 domain at 1 m resolution or 10 000 km2 domain at 100 m resolution becomes unreasonable i e 72 h 3 days even when the maximum recursion limit has been manually increased in addition the topographic determined i e directional icsls lack the capacity of processing a digital elevation model dem with complex topographic features e g depressions and flats that are commonly included this ineptitude occurs because the downslope flow directions out of depressions cannot be determined and are thus not assumed to be connected with flows in downslope regions however depressions and downslope cells in a dem may actually be hydraulically connected according to field observations or physically based modeling results because of the fill and spill process the only work we are aware of that has improved the original algorithm is from ali and roy 2010 which used cpu parallel computational techniques based on the imagine processing toolbox in matlab mathworks inc 2019 but only for part of the programming workflow to replace the recursive approach the rest of their algorithm remains e g using single cell based for loops unchanged compared with the original algorithm the main objective of this work is to further increase the computational speed for the icsls such that it is suitable for applying to large grids by creating a new algorithm that is fully optimized and parallel note that the core concept of the icsls is still based on allard 1994 and western et al 2001 in our new algorithm we introduced array vectorization and operators of a convolutional neural network cnn array vectorization eliminates the needs of any single cell based loops recursions and boosts the computational speed and memory use efficiency cnn operators including convolution and pooling can efficiently identify directional connectivity on complex topography based on high resolution dem the new algorithm also leverages gpu parallel computing capacity more efficient than cpu parallel and was implemented in mxnet chen et al 2015 mxnet is a widely used machine learning deep learning framework and has been applied in ecological pattern analysis geological features classification and spatial object detection chen et al 2017 d√≠ez hermano 2017 li et al 2018 we hypothesize that the new algorithm for connectivity function both omnidirectional and directional can significantly increase the computational speed for existing algorithms e g at least at a scale of 10 100 times our algorithm is generic imperative in style so it can be easily adapted to different programming languages in addition we also made other improvements to incorporate additional directional indicators for hydrological patterns to better describe the variability of connectivity states these improvements include the following 1 incorporate a cardinal direction indicator that allows the connectivity functions to represent the 2d directional trend for the connected pathways between paired locations and 2 improve the topographically determined connectivity by allowing a multiple flow direction routing method to process topography with difficult morphologic features including flat areas and topographic depressions thus the effects of impedance constraint for connected hydrological pathways due to these flat areas and depressions can be explicitly quantified this paper starts with a detailed description of the mathematical concept and current applications of the connectivity functions followed by a detailed programming workflow for the new algorithm we proposed in this work including 1 input variables that are needed from users with recommended initial values based on commonly available hardware configurations 2 detailed steps of processing and manipulating data of hydrological patterns in gis raster format using techniques such as vectorization and cnn operators i e convolution and pooling for quantifying their omnidirectional and directional connectivity in 2d and 3d euclidean space 3 an algorithm test on a hypothetical and real world landscape with flow patterns of different spatial orientations connected lengths and topographic features i e flat areas and depressions for the sensitivity and robustness of our algorithm and 4 the comparison of correctness and computational speed with the existing algorithms 2 methods 2 1 detailed review of the connectivity function concept the connectivity function œÑ h allard 1994 western et al 2001 represents the probability that any pair of locations in a gridded hydrological pattern with separation distance h at cell centers such as soil moisture conditions or surface flow are connected by contiguous pathways of neighboring cells fig 1 a note that in this study h is calculated based on the distance between two cell centers in a connected pair then the pattern is binarized by labeling each cell as high or low based on a predefined threshold k fig 1b c the threshold can either be time variant based on specific percentiles of multivariate distributions at a certain time point or time invariant based on a fixed physical indicator such as a percentage of volumetric soil moisture ali and roy 2010 anderson et al 2006 here we use a time invariant threshold as a demonstration our example is an overland flow depth pattern and we apply a 0 1 m depth threshold to differentiate between very shallow sheet flow and concentrated channelized flow nrcs 1986 cells labeled as high are connected if any of their eight neighboring cells in horizontal vertical and diagonal directions are also labeled high which indicates a physically active state of flow the algorithm then randomly scans and pairs any two cells labeled high in the spatial pattern to test for connectivity until all possible pairs are identified a pair of cells is connected if there is a linked pathway of contiguous high cells between them e g cell a and b in fig 1b œÑ h calculates the probability that any randomly paired cells x and x h separated by a lag distance h are connected if the pathways connecting between x to x h are always valid regardless of their trajectories and or impedance factors e g terrain morphological barriers œÑ h is omnidirectional based on a similar conceptualization i e experimental variogram used in describing the random percolation process allard 1994 and western et al 2001 proposed that the basic form of connectivity function is 1 œÑ h p x x h x z x h g where g represents all the cells in the spatial pattern and z represents the regions of high cells here the connectivity states are quantified by explicitly incorporating the connected pathways separated by h the integral of the connectivity function curve across different separation distances is the omnidirectional integral connectivity scale length omni in the algorithm implementation the connectivity function œÑ h i e probability of connectedness is plotted against the average lag distance for each range bin and connected as a line graph fig 1d omni is the total area under the curve which is computed as the total area of a series of trapezoids with their parallel sides touching each other omni represents the accumulated connectivity states for the flow patterns across spatial scales at a particular time point topo is a directional connectivity metric and is computed in the same manner as omni but adds a physical constraint to the connected trajectories between the paired locations to generate a subset of pathways based on topography in a 3d space i e trajectories of linked flow pathways connecting two paired locations x and x h must follow a one way continuously downslope upslope gradient estimated by the flow routing algorithm on the dem otherwise they are not valid fig 2 a in works to date two methods for computing the downslope flow pathways simplified flow routing are available 1 assign the two steepest gradients from the eight neighboring cells as the downslope directions western et al 2001 and 2 treat all lower elevation cells among the eight neighboring cells as downslope ali and roy 2010 meerkerk et al 2009 however method 2 is likely to overestimate flow directions by creating overly dispersed flow paths seibert and mcglynn 2007 in addition neither of these methods are sufficient to measure the directional connectivity function on difficult surface topography including flat areas and topographic depressions these morphological features may still connect flow with lower connectivity probability depending on the large scale topographic gradient and overflow but the connected flow pathways over these areas are completely excluded in the current topo algorithm fig 2b our new algorithm can differentiate flow on flat areas and topographic depressions and reflect their impact on the magnitude of the icsls therefore a more robust flow routing algorithm is needed for topo we first use the priority flood method to fill depressions to the elevations of pour point s barnes et al 2014a flow directions in the flat areas are also assigned including those newly created after the depression removal filling this functionality ensures that the connected flow trajectory on the topography is continuous at a larger spatial gradient then we route flow using the triangular multiple flow direction md method seibert and mcglynn 2007 note that we do not ensure the accuracy of routing pathways within surface depressions and flat areas in this paper since our goal is to validate the linkage between pairs of cells and compute the connectivity function rather than physically model flow mass and momentum transfer along the pathways in addition a spatial direction indicator is also needed to describe the effects of anisotropy on overland flow similar to a directional experimental variogram i e directionally biased spatial trends of flow connectivity directional trends of connectivity patterns can be incorporated into omni and topo based on the cardinal directions card for the connected trajectories between paired locations note that card captures and quantifies the spatial anisotropy of the topography reflected by flow patterns representing the connectivity probability at different cardinal directions four directions are used in this paper ns we ne sw nw se the detailed programming procedure for cstat is included in the following sections 2 2 cstat omni 2 2 1 step 1 initializing variables and settings the algorithm requires user input for several variables as predefined parameters table 1 the computational depth is broadcdp and determines the number of cells extracted from the gridded patterns that are included in one iteration of computation for the separation distances between paired cells the reason for limiting the total number of cells in one iteration is to avoid gpu memory overload the recommended value of broadcdp is between 1700 and 7000 based on the memory capacity of the gpu card table 2 the spatial resolution is resolution i e cell size of the gridded patterns and determined automatically from the input files the parameter threshold is used for binarizing the value of each cell in the patterns into high and low the lag bin width is h fig 1d the values of h and threshold depend on the objective of the connectivity computation and the hydrologically relevant patterns that are being evaluated in this study we used 2 hypothetical flow patterns i e one includes areas of high and low and one only include areas of high for comparison at 1 m 2 m 5 m and 10 m resolutions a total of 8 patterns fig 3 and 3 modeled flow patterns at 2 m resolution on a dem as an example to demonstrate the sensitivity and correctness of cstat in each of the hypothetical patterns 16 flow pathways of different lengths and cardinal directions are included table 3 the total number of cells is 28 1 million 7 0 million 1 1 million and 0 28 million at 1 m 2 m 5 m and 10 m resolutions respectively for the hypothetical patterns h is set to 300 m and determined to uniquely include the total length of one flow pathway in one lag bin except the first lag bin of distance 0 table 3 we aim to demonstrate that our new algorithm for omni can identify the separation distances and cardinal directions that are unique to each connected flow pathway note that the hypothetical patterns have already been binarized into high and low regions as cells of 1s and 0s the modeled patterns at 2 m resolution represents several time snapshots of overland flow for the interested domain in a single storm event and the total number of cells is 70 k the parameter threshold is set to 0 1m nrcs 1986 we aim to demonstrate that our new algorithm for topo is able to quantify the impact of topographic impedance i e resistance on the connected flow pathways for the modeled patterns h is determined by variable i max and the number of lag bins n bin set by the users variable i max is the maximum possible lag distance and is estimated automatically based on the spatial dimension of the gridded pattern as the hypotenuse length of the domain s enclosing shape rectangle which is automatically extracted from the metadata of the input gis raster data then l max is divided by the number of lag bins of equal interval such that 2 h m i n l m a x n b i n note that n bin must remain the same for all the flow patterns computed because omni and topo based on different n bin or lag bin assignment are not directly comparable 2 2 2 step 2 reading the gridded patterns into cpu memory as numpy arrays and data preprocessing the gridded patterns of flow pathways in gis raster data format are read as 2d numpy arrays and the original spatial arrangement of cells is preserved as the array index locations cartesian coordinates as row and column note that the origin of the x and y axis in a numpy array index is at the upper left i e approximately the most nw location corner the threshold is used to binarize the cells of the numpy array into the value of 1s high for concentrated and channel flow and the value of 0s low for very shallow sheetflow the cells of missing value or nodata are assigned the value of 1s in the original algorithm proposed by western et al 2001 the search for the paired high cells and computation of their separation distances requires single cell based loops and recursions this requirement may be intuitive but significantly reduces the efficiency of the algorithm and is not memory efficient for grids with a large number of cells in this study we assumed that any connected pair of cells can only be found within a spatial clustered region i e each cell is connected with at least one of its eight neighboring cells therefore we reduced the computational intensity of the pairing process by limiting the searching boundary to each spatial clustered region with a unique region index the binary labeling and clustering procedure is similar to the image processing toolbox of matlab used by ali and roy 2010 in this study the clustered regions within the cells of 1s are identified and labeled using the python scipy module function imagine label the kernel shape of the neighboring cells was firstly defined in terms of which one s are included as connected for each center cell all eight neighboring cells in this paper then the imagine label function was used to segment the cells of 1 to clustered regions and labeling each region using sequential index integers starting from r 1 to rn thus the cells of 1s are replaced by the region index number e g fig 1c the preprocessing of the numpy array is completed and we name the processed array array 0 2 2 3 step 3 extracting coordinates of eligible locations from the numpy array and converting to the mxnet gpu array to compute the pairwise separation distances coordinates for the cells with label 1 or 0 of array 0 are extracted using the numpy module where the array vectorization technique was used to compute the separation distances 1 between high cells and low cells 2 between high cells and high cells and 3 between the connected pair of cells in each clustered region without single cell based loops the most efficient way of vectorizing numpy arrays is broadcasting i e a scalar based matrix operator between arrays of different dimensions shapes broadcasting between two arrays is equivalent to generating full permutations between the elements of each array i e each element in one array is paired with each element in another array regardless of their redundancy in order in this paper we reduce the full permutations to full combinations by using python array slicing operator for computational efficiency next cartesian coordinates of array 0 are extracted into two numpy arrays with dimension of n 2 based on their values excluding areas of missing data one array represents all high cells and named as array 1 and the other represents cells with value 0 i e all low cells and named as array 2 the cartesian coordinates of each clustered connected region of high are also extracted named as array r n where n is the total number of clustered regions the array 1 array 2 and array r n are then divided into small segments and assigned unique segment indices the length of these segments i e the number of the array elements sliced in each iteration is equal to the broadcdp we name the mth segment of array 1 as sma1 the mth segment of array 2 as sma2 and the mth segment of array r n as smarn as we mentioned earlier the reason for dividing these arrays into smaller segments is that gpu memory capacity is usually more limited compared with cpu system memory thus the issue of gpu memory overload can be eliminated if the broadcdp is set appropriately sma1 sma2 and smarn are then sequentially converted into mxnet gpu arrays and the vectorization is implemented using broadcasting between two segments in the following substeps 1 sma1 and sma1 all high cells 2 sma1 and sma2 high and low cells and 3 smarn and smarn high cells that are connected within each clustered region this broadcasting operator generates two element full permutations here the benefit of broadcasting gpu arrays instead of cpu arrays is the much higher parallel computation capacity of a gpu card than that of a cpu this broadcasting procedure achieves identical results to recursively searching the paired locations in array 1 between array 1 and array 2 and array rn without explicit looping through each cell or using any recursion function note that broadcasting and computation of the euclidean distances between any paired coordinates in each segment are implemented simultaneously then we include a logic to determine if the two segments being broadcasted for substep 1 and 3 are identical i e of the same segment index if this condition is met the redundant elementwise pairing between the same pair of cells but with switched order e g x y and y x and the cell pairing with itself e g x x or y y is excluded from the computation to increase the computation efficiency this integrated logic is equivalent to only including the two element combinations instead of permutations for substep 1 and 3 note that we only use the relative scale and distance in the pairing and computation process based on the 2d array cartesian coordinates this step completes the collection of all the possible separation distances and the corresponding counts of pairs in each lag bin the detailed illustration for the entire array manipulation process including the pairing between two segments and between elements of paired segments in this step can be found in fig 4 2 2 4 step 4 computing the connectivity functions and icsls in each lag bin we assign the aggregated values for the total number of pairs from section 2 2 3 step 3 substep 1 and 2 as tc g similarly we assign the total number of connected pairs and the sum of lag distances from section 2 2 3 substep 3 as tc z and ld g the probability of the connectedness i e connectivity œÑ h in each lag bin is calculated as p tc z tc g and the average separation distance for each lag bin is calculated as l mean ld g tc z note that in this paper we only consider the average separation distance for the high cells instead of including both high and low cells as proposed by western et al 2001 in addition if the low cells are present the result of œÑ h that are calculated by cstat is slightly larger than ¬Ω of the previous algorithm i e the total number of two element combinations is ¬Ω of two element permutations for substep 3 thus tc z is ¬Ω of previous algorithm and tc g is smaller than previous algorithm because the result of substep 1 is slightly reduced compared to previous algorithm 2 3 cstat card cstat card is computed by appending a cardinal direction indicator i e ns we ne sw or nw se for the connected pairs of cells the direction is computed by the slope of a line connecting the paired locations fig 5 and then converted to degrees 90 90 based on the inverse tangent function to remove negative values we added 90 so the range becomes 0 180 therefore the degree values should be interpreted as the counterclockwise deviation from the ns direction half plane and 0 is equivalent to 180 we compute the histogram of connected pairs in each of the 4 cardinal directions for each lag bin that can be presented in the results addition to œÑ h similarly we also included the computation of the cardinal direction specific œÑ h and corresponding omni which describes the œÑ h for all the connected pairs in a cardinal direction all the tabular results for œÑ h including average separation distances probability of connectivity the histogram in each cardinal direction for each lag bin omni and the computational time are written to text files 2 4 cstat topo cstat topo follows the same procedures as cstat omni except that before proceeding to step 3 a flow pathway verification step is included in advance to ensure that the connected pathways are topographically valid i e the trajectories on the pathways between any pair of locations are counted as connected only if they follow overlap downslope flow routing directions on the topography without interruption then we define these pairs as being topographically connected since this step is between step 2 and step 3 in computing omni it is named step 2 5 2 4 1 step 2 5 identifying the topographic determined flow pathways the dem in raster grid format is converted to an mxnet gpu array which is named array dem the downslope flow directions for each cell in the array dem are generated based on the md routing method proposed by seibert and mcglynn 2007 and stored in an indicator grid array d to eliminate the unnecessary search we limit the downslope flow accumulation areas within the location of each spatial clustered region in array r n only the separation distances are computed similar to omni except that one cell of a pair is always fixed and is the cell where the downslope flow accumulation starts whose elevation is equal or higher than other cells in the downslope accumulated area the cells in flat areas i e with 8 neighboring cells of the same elevations are identified simultaneously and labeled in another indicator grid array flat in this study we created a new algorithm to compute the flow direction using convolution and pooling operators instead of single cell based loops and recursions the details of our new implementation are described below first the dem grid of the same spatial domain as the flow pattern is read into memory as a numpy array using the geospatial data abstraction library gdal warmerdam 2008 and converted to an mxnet gpu array array dem two additional gpu arrays are also generated to store the value of cartesian coordinates of the array dem as array x for rows and array y for columns then for each cell m eight triangular facets based on m s eight neighboring cells are defined starting from east 0 and rotating counterclockwise back to e 360 see fig 6 a the same as in d and md seibert and mcglynn 2007 tarboton 1997 the eight facets and neighboring cells are indexed by eight integer numbers in a counterclockwise sequence as facet 0 to facet 7 and neighbor 0 to neighbor 7 fig 6b three empty python lists of length 8 are initiated to store the arrays of differences between the values of centered cells m and two contiguous neighboring cells for each of the eight triangular facets e g p1 and p2 in facet 1 based on the array x array y and array dem we name the arrays dxi dyi and dzi where i is the neighboring cell index number from 0 to 7 for example dx2 indicates the difference of the x axis for the neighboring cell to the north dx2 xm xp2 instead of looping through each cell of m in array x array y and array z to compute the dxi dyi and dzi we use the convolution and pooling operators based on kernel weights in mxnet convolution and pooling are often used as an efficient and optimized approach in machine learning for image processing convolutions are based on a 3 3 kernel window i e a square of 9 cells of specific configuration as the weight kernel which slides from the top left to the lower right for each cell of array x array y and array dem by rows fig 7 we designed a series of 8 kernel structures and each one corresponds to only one neighboring cell table 4 for example for weighted kernel 0 it computes the convolution for dx0 the adjacent cell on the east similar convolution and pooling operators are implemented on array y and array dem and the results are stored at each cell in array dx i array dy i and array dz i then for each triangular facet we compute the normal vector between cell m and the two contiguous neighboring cells as defined in seibert and mcglynn 2007 3 n j n x j n y j n z j d z i d y i 1 d z i 1 d y i d z i d x i 1 d z i 1 d x i d y i d x i 1 d y i 1 d z i where j represents the facet index number from 0 to 7 the normal vectors nxj nyj and nzj are stored separately in three gpu arrays next the flow directions and the slopes for each triangular fact are computed based on nxj nyj and nzj as in seibert and mcglynn 2007 and stored in gpu array dj at the location of each cell m 4 d j 0 n x j 0 a n d n y j 0 œÄ n x j 0 a n d n y j 0 œÄ 2 arctan n y j n x j n x j 0 3 œÄ 2 arctan n y j n x j n x j 0 note that the convolution operators cannot apply to the edge of array x array y and array dem i e the arrays dj have the dimension of only that of array x array y and array dem minus 2 we extend the shape of array x array y and array dem by appending zeros to the edges so the dimensions of dj are consistent with array x array y and array dem this implicitly assumes that the flow directions on the edge of the dem are always out of the domain the edge appending can be achieved by using a pad parameter in the convolution operator in mxnet similar to seibert and mcglynn 2007 we made further modifications for these cases when the flow direction of the facet dj is not within 45 degrees of the corresponding facet or both elevations of neighboring cells on the facet e g p1 and p2 are higher than that of cell m thus the last step of updating dj is to ensure that for the downslope directions directly pointing toward the neighboring cells of m i e œÄ 4 or j 1 œÄ 4 only the adjacent facets with the same directions i e dj dj 1 and dj dj 1 are maintained otherwise these values in dj are eliminated and converted to 999 as nodata note that dj includes the flow directions at each of the 8 facets i e a total of 8 arrays to simplify the further process and save the memory space we collapsed the directional information for the 8 facets in dj into array flowdir by using the directional kernel values based on the 2n start from the east 1 represents e 2 represents ne 4 represents n and so on so that the value of each cell in array flowdir represents a unique flow direction combinations a total of 255 unique combinations 8c1 8c2 8c8 from that cell in the later process the exact directions at a particular cell can be efficiently retrieved from a python dictionary e g unpack the value of array flowdir based on the 1 to 1 correlations 2 4 2 flow direction assignment in topographic depressions and flat areas of the dem topo is designed to compute the connectivity states based on existing flow patterns either from observations or modeling instead of physically simulating water mass momentum and energy transfer therefore the spatial extent of the flow pathways from the observations or modeling is used as the mask to indicate the regions in which the topo algorithm is implemented we used the priority flood barnes et al 2014a to fill the topographic depressions and then subtract the original dem from the depression filled dem as Œ¥dem to delineate the boundary cells of each depression and pour point s the boundary delineation is achieved by using two functions from the scipy library 1 a binary labeling on the Œ¥dem to generate the internal areas for each depression and 2 a dilution operator to generate the edges based on the internal areas for each depression then the pour points cells are identified as the cells with the lowest elevation s on the depression boundary the priority flood depression filling approach also ensures that the downslope flow pathways are not being blocked at the highest level of depressions or nested depressions in topology that are no lower than the nested pour points thus this approach allows a flexible determination of the aggregated effects i e the magnitude of resistance of depressions on connected pathway s based on a spatial scale for the topographic gradient that is larger than the scales of individual depressions note that some flat areas may appear in the depression filled dem to calculate the flow directions for the flat areas we used a customized version integrated with the md routing method of the algorithm proposed by barnes et al 2014b which generates the artificial gradient between the cells located at the edges of each flat region that are 1 adjacent to the neighbor of higher elevations i e high edge and 2 adjacent to the neighbor of low elevations i e low edge the flow direction grid that is generated based on md in the previous step is amended as array flowdir amend to include the flow direction for the cells in flat areas and depressions that have not been previously assigned 2 4 3 downslope search for connected pairs and the computation of topo after the flow direction grids are generated downslope flow accumulation areas are delineated within each of the spatial clustered regions r n of array 1 one cell within the spatially clustered region is assigned as the starting location kn and the downslope accumulation area cn is delineated based on the flow directions indicated by array flowdir amend the search loops through each cell as the starting point kn in the region r n by tracing the chain of its downslope cells controlled by the direction kernel values in array flowdir amend and looks up the exact flow direction s in the dictionary built in the previous step until the chain of downslope cells is empty the cells in cn are marked as 1 in an indicator array array flowdown anywhere else are 0s to delineate cn using a memory and stack efficient way in topo we assume that the traverse of the flow cascading downslope in cn on the topography is equivalent to the tree branches rooted at the starting upslope location kn this assumption allows us to use a parallel nonrecursive strategy i e the breadth first search bfs method bfs searches all the neighboring cells at the current depth the closest neighbors at one cell distance and then the neighbors at a two cell distance and so on until all the tree branches downslope cells are visited fig 8 this search is implemented by creating a stack control list i e the chain and a marker list the stack control list is used to append new locations that need to be processed to search downslope cells i e in the current eight neighbors and to indicate the termination of the iteration when no cell location remains the marker list is used to exclude the locations already visited in the previous iterations and avoid repeated search once this downslope search procedure is completed the starting point kn is paired with every cell in array flowdown where the values are 1s and the separation distances are computed based on the coordinates of the cells in array flowdown the starting point kn is always one location of a pair thus no combinations between two segments of r n are searched but we simply match the starting cells and all the cells in one segment of r n i e one to many broadcasting this process repeats for all the cells in the same clustered region r n in cn the rationale for this step is that because any pair of cells within the same spatial clustered region r n are connected any other locations within cn paired with the starting cell kn and within the same spatial clustered region r n are also topographically connected therefore the algorithm we designed can efficiently count the pairs and compute the separation distances between kn and any other locations within cn by limiting the pairing process within the spatial clustered regions r n connectivity functions and icsls are then computed in a similar manner as described in step 3 note that we do not apply any weight on the flow directions in this study because md and d are identical in most cases with mostly a proportion between two adjacent neighboring cells based on the triangular facets 3 test results and discussions we use the decrease rates of connectivity drc i e the decrease rate of the total connected pairs of cells between any two contiguous lag bins from a lower to a higher separation distance to test the sensitivity for the flowlines based on their corresponding maximum connected lengths and cardinal directions in the hypothetical flow pattern see fig 3 the drc can be taken as an indicator of individual connected objects e g flowlines based on the local maximum for connectivity functions the results indicate that the drc is a robust indicator for detecting individual components in hydrological patterns and is consistent with different spatial resolutions fig 9 the plots of the connectivity function and omni values for the flow pattern see fig 10 using 1 m resolution as an example plots for other resolutions can be found in the supporting information show that omni at each cardinal direction can be used to identify changes in separation distances on connected flow pathways for example the total lengths of connected flowlines are ranked from high to low as we ns ne sw and nw se which can be reflected on the omni at each of the corresponding cardinal directions however for flow patterns with a large portion of low cells the sensitivity of omni at each cardinal direction is significantly reduced such that the difference between omni values becomes negligible note that the difference between the omni at each cardinal direction and the aggregated omni for the entire pattern is also negligible for the pattern with significant low cells because the area under the connectivity function curve between 0 and the first lag bin 1 300 m takes more than 98 of the total area in addition the impact of grid resolution on the connectivity function and omni is lower compared with that of the number of low cells the aggregated omni values increase approximately 20 from 1 m to 10 m resolution for the patterns with a significant number of low cells and omni at each cardinal direction only increases approximately 8 conversely both the aggregated and the directional omni values for the patterns with only high cells show negligible changes because the total number of high cells have a much smaller degree of decrease with the decrease of spatial resolution i e 1 10 for the test patterns compared with that of low cells i e 1 100 for the test patterns thus if no low cells exist changes in connectivity functions and omni are negligible regardless of the grid resolution we further tested cstat including omni and topo on a local region 360 m 845 m of a meandering river course including an oxbow lake and its surrounding floodplain the oxbow lake does not connect to the stream in low intensity rainfall events and can be considered a depression the overland flow and streamflow patterns i e time snapshots are derived from the map output of a fully distributed 2d hydraulic model for a single storm event which is calibrated to the hydrograph at the watershed outlet three snapshots are included in this study 18 h 24 h and 31 h representing the initial rising and peak of the storm event the drc graphs show that at the initial stage of the event 18 h maximum separation distances on the connected flow pathways i e local maximums at the four cardinal directions are readily identifiable fig 11 a the highest magnitude of the drc is at the separation distance of 236 m and then a significant decrease appears at 300 m at the nw se direction fig 11a marked in purple these changes in the drc indicate three segments of the flow pathways that are connected mainly segments on the streamflow fig 12 at the nw se direction are all smaller than 236 m marked by orange arrows but the euclidean distances between any two segments marked by green arrows are larger than 300 m as the storm event proceeds i e at 24 h when the overland flow at the floodplain is prevalent the magnitudes of the local maximum decrease and drc graphs becomes smoother such change is even more obvious for the snapshot at 31 h when the local peaks in the drc graphs are not readily visible the drc graphs at 31 h indicate that the total number of connected flow pathways at each lag bin increases significantly and the difference of their lengths and cardinal directions become minor omni at 18 h is higher than at 24 h because the large amount of low cells has not yet appeared in the flow pattern table 5 the soil has relatively high infiltration capacity at 18 h but lower capacity at 24 h causing the occurrence of more infiltration excess overland flow represented by wet cells but in low because of the shallow flow depth at 31 h peak of the storm event deeper overland flow with mostly high cells concentrates in this region and the connected flow pathways have the largest spatial extent among these snapshots therefore omni at 31 h is the highest in contrast topo for all of the three snapshots are significantly lower than omnis with the maximum separation distances of 40 50 m on the connected flow pathways note that the aggregated topo does not change between the snapshots although a slight increase for the topo at each cardinal direction was found from 18 h to 31 h in addition topo for the flow patterns generated from the depression removed filled dem in the same region shows identical results these results mean that the md flow direction algorithm adapted in this study is robust and consistent to identify the directional flow connectivity on topography with or without dem preprocessing therefore cstat provides a realistic and robust way of measuring the hydrological connectivity for overland flow and streamflow on topography including the complex features such as topographic depressions and flat areas the trade off is that topo takes significantly more computing time than omni the results also show that cstat generates similar results to the existing algorithms the detailed comparison between cstat and existing algorithms is included in the supporting information as an ms excel file the computing time in seconds of omni for each of the hypothetical patterns is presented in fig 13 the algorithm of western et al 2001 using single cell based loops and recursions and is significantly slower than the other two i e the algorithm in this paper and the one proposed by ali and roy 2010 using matlab and the speed difference increases as the total number of high and low cells increases e g from 400 times to 3500 times faster for the test pattern at 2 m resolution with a total number of high and low cells of more than 3 million western s algorithm takes more than 0 61 million seconds 7 days and 2 h to complete the computation while ali and roy s algorithm and cstat generate the results within 967 and 161 s respectively note that the speed difference between cstat and ali and roy s algorithm is much smaller but also increases as the total number of high and low cells increases e g from 18 times slower at 10 m resolution to 7 2 times faster at 1 m resolution to further provide the reader with a detailed estimation of the computing time based on the number of grid cells for cstat we used a series of hypothetical flow patterns 48 in total with the number of high and low cells between 4 and 4 million stepwise increments the results indicate that cstat is more suitable for flow patterns high cells only with the total number of high cells larger than 1000 or high and low cells coexist larger than 200 and 3 9 million respectively note that we did not estimate the computing time t for topo because of the different complexities of topography the computing time t in seconds for cstat omni can be estimated by the equation below 5 t h 0 04646 h l 1 0045 10 8 i f l 0 h 2 1 10 8 h 0 00153 i f l 0 where h and l are the total number of high and low cells respectively this equation indicates that the total number of high cells has significantly more impact on the computing time than the low cells similar to the current algorithms for the icsls cstat also requires the observations of hydrological patterns or outputs of a physically based model for the hydrological processes in the study area as a priori for quantifying the characteristics of connectivity states the advantage of cstat is that it can be applied to hydrological patterns based on large grids the improved computational efficiency of cstat will open the doors to a deeper understanding of system dynamics e g the external drivers and help to build the linkage between physical processes across spatial scales at high resolution 4 conclusions connectivity characteristics of hydrologic processes depend on the variability of complex connectedness of the flow pathways across multiple scales landscape features e g local scale topographic depressions and flat areas may also be contributing to the shifts in connectivity these small scale features are often considered the source of high uncertainty in the hydrologic models e g antoine et al 2009 bracken et al 2013 connectivity at the hydrosystem level reflects the complex aggregation of feedback interaction dynamics from these small scale factors but it has been difficult to find a unified indicator to inspect the relationships they have with the physical processes the explicit description of changes in connectivity states provides a novel venue to identify how the hydrosystem states is transformed which reflects such feedback dynamics to date although connectivity statistics are among the very few promising options available low computational efficiency is severely limiting the applications of the current algorithm for analyzing hydrological processes at the hydrosystem level uisng high resolutio data in this paper we introduce a new algorithm cstat for quantifying connectivity characteristics this algorithm is capable of processing large datasets in a reasonable timeframe the workflow of this algorithm is fully automated and requires minimum user interference additionally the general style code can be implemented on various platforms and hardware configurations we envision that cstat can pave the way for investigating connectivity characteristics based on high resolution hydrological pattern across multiple scales connectivity characteristics can be useful for the hydrological model community to improve model conceptualization and parameterization e g by designing a better sampling scheme and reducing the model complexity as discussed in keesstra et al 2018 as a case study we implemented cstat omni in a real world watershed and discussed the effect of small scale topographic features on the overland flow and streamflow response by the changes in connectivity states across multiple scales up to the watershed scale yu and harbor 2019 a major limitation of cstat is that it depends on the fully distributed model or high resolution in situ or remotely sensed observations to generate the hydrological pattern i e does not natively account for the water mass and momentum transfer between the paired locations ali et al 2018 in addition the computing time is not a simple linear function of the number of high and low cells in the grid which can reduce computational speed when a complex pattern is being analyzed future work to improve the algorithm will focus on the following 1 incorporate a simplified water and water mediated substances transfer scheme between any pair of cells to determine the material connectedness without relying on fully distributed models 2 parallelize the algorithm across multiple gpus to obtain a higher computational speed increase and 3 incorporate a network distance computation scheme for the separation distances so the subsurface flow connectivity can also be estimated where the trajectories of the flow pathways cannot be measured based on euclidean distance 5 software and data availability the cstat including omni and topo algorithm was developed by the first author of this paper and implemented in mxnet version 1 2 0 a machine learning framework with python binding cstat requires numpy scipy and gdal modules installed in the python version 3 x environment a workstation with at least one gpu device of memory size 4 gb is required source code is under the apache 2 0 license and can be accessed from the author s github repository at the link below https github com codefortheplanet cstatplus instructions regarding the use of the code and the accompanying test data can also be found in this repository acknowledgments purdue university provided fellowship funding support for this research appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104496 
26156,connectivity is a state of the art concept that improves the current understanding of hydrological processes at multiple scales two point connectivity statistics provide a promising approach to quantify hydrologic connectivity which measures the probability of any two nodes in a hydrologically relevant pattern that are connected based on their separation distances however limited computational capacity has been the main constraint for implementations in large gridded patterns 1 million nodes here we propose a new algorithm based on array vectorization and convolutional neural network operators convolution and pooling that leverages parallel computational capacity of a gpu test results suggested that the new algorithm significantly increases the computational efficiency and is also sensitive to the variability of connectivity states and robust to complex topography we envision that our algorithm can pave the way for investigating behaviors in large scale e g watershed processes based on quantifying the connectivity of gridded hydrological patterns and digital elevation models keywords connectivity statistics gpu accelerated computing gridded hydrological patterns vectorization convolutional neural network 1 introduction hydrologic connectivity is generally viewed as pathways of water and water mediated substances such as pollutants and sediments between spatial locations in a watershed during a finite temporal interval bracken and croke 2007 research to date has recognized connectivity analysis as a promising approach to improve our understanding of watershed scale hydrologic behavior e g tockner et al 1999 heathwaite et al 2005 kn√∂sche 2006 recent works on connectivity have focused on landscape features and topographic gradients interpreted using pattern analysis spatial statistics percolation theory or graph theory aurousseau et al 2009 gascuel odoux et al 2009 janzen and mcdonnell 2015 knudby and carrera 2005 connectivity has been considered a potential alternative for interpreting watershed runoff generation mechanisms watershed models would better describe the physical process controls and achieve higher accuracy if model conceptualization could be based on the connectedness of linear or nonlinear temporal and spatial networks for a large number of small scale reservoirs and constrained by landscape and environmental factors such as topography soil moisture content and vegetation phillips et al 2011 one widely used approach is two point connectivity statistics i e connectivity functions that quantify and characterize the probability of connected direction and magnitude for spatial patterns of hydrological conditions such as soil moisture conditions and flow processes on the terrain surface ali and roy 2010 connectivity functions measure the probability of two locations that are connected western et al 2001 and have also been applied to watersheds in different hydroclimatic regimes e g meerkerk et al 2009 ali and roy 2010 the concept of connectivity functions in spatial pattern analysis has a close counterpart in traditional geostatistical techniques i e the experimental variogram cressie 1993 which examines the correlation strength of relevant spatial data between any two locations based on search radiuses or separation distances regardless of continuity connectivity functions are also separation distance based metrics but with a specific emphasis on connectedness unlike traditional variogram analysis connectivity functions explicitly describe the spatial variability of connectivity by measuring the probability of two paired locations that are connected by a continuous pathway in a specific hydrological condition e g high soil moisture western et al 2001 theoretically connectivity functions can be scaled up from the plot or field scale to the watershed scale simply by extending the spatial coverage of the hydrological patterns being evaluated the impedance effects physical constraints for the connected fluxes pathways on surface or subsurface topography e g in porous media for geometrically determined trajectories can also be captured as directional connectivity functions in 3d in addition to omnidirectional trajectories in 2d western et al 2001 renard and allard 2013 the temporal variability of connectivity states during single storm events can also be quantified by computing the integral of connectivity functions on all separation distances i e the integral connectivity scale lengths icsls for a temporal snapshot of the hydrologic patterns e g drainage pathways or inundation patterns within the event duration ali and roy 2010 meerkerk et al 2009 the charm of the icsls is simplicity they explicitly describe the spatial connectivity states of the hydrological patterns across different separation distances in a single index the icsls provide a direct and intuitive way of measuring and comparing connectivity states within or between hydrologic systems e g watersheds however studies of hydrologic connectivity using the icsls have almost exclusively focused on low resolution spatial patterns of a limited total number of nodes e g western et al 2001 ali and roy 2010 few attempts have been made to scale up the computation of the icsls to high resolution gridded patterns across spatial scales meerkerk et al 2009 high resolution connectivity analysis across spatial scales can provide critical information on driving factors for hydrologic processes and rainfall runoff modeling that may have been missed in work using much smaller and coarser resolutions ali et al 2018 the two most likely reasons for the lack of work to date on connectivity analysis of high resolution hydrologic gridded patterns include the following 1 sample collection for such hydrologic patterns requires considerable time and resources and 2 the analysis of large amounts of spatial data is highly computationally intensive advances in high performance computing and remote sensing techniques are providing promising ways of reducing or eliminating several limitations of implementing icsls and providing high resolution data across spatial scales correia and rego 1998 devantier and feldman 1993 siart et al 2009 vieux 2016 these advances allow the quantification of connectivity states for high resolution hydrological patterns covering a large watershed e g 10 10 000 km2 using a series of temporal snapshots in areas of different physical dynamics such as hillslopes floodplains and stream channels however the original algorithm for icsls proposed by western et al 2001 is implemented using single cell loops and recursions and is not suitable for implementation on high resolution gridded patterns because of the long computation time low memory usage efficiency and susceptibility to errors such as stack overflow a preliminary study based on the original algorithm suggested that the computational time for grid patterns of more than 1 million cells equivalent to a 1 km2 domain at 1 m resolution or 10 000 km2 domain at 100 m resolution becomes unreasonable i e 72 h 3 days even when the maximum recursion limit has been manually increased in addition the topographic determined i e directional icsls lack the capacity of processing a digital elevation model dem with complex topographic features e g depressions and flats that are commonly included this ineptitude occurs because the downslope flow directions out of depressions cannot be determined and are thus not assumed to be connected with flows in downslope regions however depressions and downslope cells in a dem may actually be hydraulically connected according to field observations or physically based modeling results because of the fill and spill process the only work we are aware of that has improved the original algorithm is from ali and roy 2010 which used cpu parallel computational techniques based on the imagine processing toolbox in matlab mathworks inc 2019 but only for part of the programming workflow to replace the recursive approach the rest of their algorithm remains e g using single cell based for loops unchanged compared with the original algorithm the main objective of this work is to further increase the computational speed for the icsls such that it is suitable for applying to large grids by creating a new algorithm that is fully optimized and parallel note that the core concept of the icsls is still based on allard 1994 and western et al 2001 in our new algorithm we introduced array vectorization and operators of a convolutional neural network cnn array vectorization eliminates the needs of any single cell based loops recursions and boosts the computational speed and memory use efficiency cnn operators including convolution and pooling can efficiently identify directional connectivity on complex topography based on high resolution dem the new algorithm also leverages gpu parallel computing capacity more efficient than cpu parallel and was implemented in mxnet chen et al 2015 mxnet is a widely used machine learning deep learning framework and has been applied in ecological pattern analysis geological features classification and spatial object detection chen et al 2017 d√≠ez hermano 2017 li et al 2018 we hypothesize that the new algorithm for connectivity function both omnidirectional and directional can significantly increase the computational speed for existing algorithms e g at least at a scale of 10 100 times our algorithm is generic imperative in style so it can be easily adapted to different programming languages in addition we also made other improvements to incorporate additional directional indicators for hydrological patterns to better describe the variability of connectivity states these improvements include the following 1 incorporate a cardinal direction indicator that allows the connectivity functions to represent the 2d directional trend for the connected pathways between paired locations and 2 improve the topographically determined connectivity by allowing a multiple flow direction routing method to process topography with difficult morphologic features including flat areas and topographic depressions thus the effects of impedance constraint for connected hydrological pathways due to these flat areas and depressions can be explicitly quantified this paper starts with a detailed description of the mathematical concept and current applications of the connectivity functions followed by a detailed programming workflow for the new algorithm we proposed in this work including 1 input variables that are needed from users with recommended initial values based on commonly available hardware configurations 2 detailed steps of processing and manipulating data of hydrological patterns in gis raster format using techniques such as vectorization and cnn operators i e convolution and pooling for quantifying their omnidirectional and directional connectivity in 2d and 3d euclidean space 3 an algorithm test on a hypothetical and real world landscape with flow patterns of different spatial orientations connected lengths and topographic features i e flat areas and depressions for the sensitivity and robustness of our algorithm and 4 the comparison of correctness and computational speed with the existing algorithms 2 methods 2 1 detailed review of the connectivity function concept the connectivity function œÑ h allard 1994 western et al 2001 represents the probability that any pair of locations in a gridded hydrological pattern with separation distance h at cell centers such as soil moisture conditions or surface flow are connected by contiguous pathways of neighboring cells fig 1 a note that in this study h is calculated based on the distance between two cell centers in a connected pair then the pattern is binarized by labeling each cell as high or low based on a predefined threshold k fig 1b c the threshold can either be time variant based on specific percentiles of multivariate distributions at a certain time point or time invariant based on a fixed physical indicator such as a percentage of volumetric soil moisture ali and roy 2010 anderson et al 2006 here we use a time invariant threshold as a demonstration our example is an overland flow depth pattern and we apply a 0 1 m depth threshold to differentiate between very shallow sheet flow and concentrated channelized flow nrcs 1986 cells labeled as high are connected if any of their eight neighboring cells in horizontal vertical and diagonal directions are also labeled high which indicates a physically active state of flow the algorithm then randomly scans and pairs any two cells labeled high in the spatial pattern to test for connectivity until all possible pairs are identified a pair of cells is connected if there is a linked pathway of contiguous high cells between them e g cell a and b in fig 1b œÑ h calculates the probability that any randomly paired cells x and x h separated by a lag distance h are connected if the pathways connecting between x to x h are always valid regardless of their trajectories and or impedance factors e g terrain morphological barriers œÑ h is omnidirectional based on a similar conceptualization i e experimental variogram used in describing the random percolation process allard 1994 and western et al 2001 proposed that the basic form of connectivity function is 1 œÑ h p x x h x z x h g where g represents all the cells in the spatial pattern and z represents the regions of high cells here the connectivity states are quantified by explicitly incorporating the connected pathways separated by h the integral of the connectivity function curve across different separation distances is the omnidirectional integral connectivity scale length omni in the algorithm implementation the connectivity function œÑ h i e probability of connectedness is plotted against the average lag distance for each range bin and connected as a line graph fig 1d omni is the total area under the curve which is computed as the total area of a series of trapezoids with their parallel sides touching each other omni represents the accumulated connectivity states for the flow patterns across spatial scales at a particular time point topo is a directional connectivity metric and is computed in the same manner as omni but adds a physical constraint to the connected trajectories between the paired locations to generate a subset of pathways based on topography in a 3d space i e trajectories of linked flow pathways connecting two paired locations x and x h must follow a one way continuously downslope upslope gradient estimated by the flow routing algorithm on the dem otherwise they are not valid fig 2 a in works to date two methods for computing the downslope flow pathways simplified flow routing are available 1 assign the two steepest gradients from the eight neighboring cells as the downslope directions western et al 2001 and 2 treat all lower elevation cells among the eight neighboring cells as downslope ali and roy 2010 meerkerk et al 2009 however method 2 is likely to overestimate flow directions by creating overly dispersed flow paths seibert and mcglynn 2007 in addition neither of these methods are sufficient to measure the directional connectivity function on difficult surface topography including flat areas and topographic depressions these morphological features may still connect flow with lower connectivity probability depending on the large scale topographic gradient and overflow but the connected flow pathways over these areas are completely excluded in the current topo algorithm fig 2b our new algorithm can differentiate flow on flat areas and topographic depressions and reflect their impact on the magnitude of the icsls therefore a more robust flow routing algorithm is needed for topo we first use the priority flood method to fill depressions to the elevations of pour point s barnes et al 2014a flow directions in the flat areas are also assigned including those newly created after the depression removal filling this functionality ensures that the connected flow trajectory on the topography is continuous at a larger spatial gradient then we route flow using the triangular multiple flow direction md method seibert and mcglynn 2007 note that we do not ensure the accuracy of routing pathways within surface depressions and flat areas in this paper since our goal is to validate the linkage between pairs of cells and compute the connectivity function rather than physically model flow mass and momentum transfer along the pathways in addition a spatial direction indicator is also needed to describe the effects of anisotropy on overland flow similar to a directional experimental variogram i e directionally biased spatial trends of flow connectivity directional trends of connectivity patterns can be incorporated into omni and topo based on the cardinal directions card for the connected trajectories between paired locations note that card captures and quantifies the spatial anisotropy of the topography reflected by flow patterns representing the connectivity probability at different cardinal directions four directions are used in this paper ns we ne sw nw se the detailed programming procedure for cstat is included in the following sections 2 2 cstat omni 2 2 1 step 1 initializing variables and settings the algorithm requires user input for several variables as predefined parameters table 1 the computational depth is broadcdp and determines the number of cells extracted from the gridded patterns that are included in one iteration of computation for the separation distances between paired cells the reason for limiting the total number of cells in one iteration is to avoid gpu memory overload the recommended value of broadcdp is between 1700 and 7000 based on the memory capacity of the gpu card table 2 the spatial resolution is resolution i e cell size of the gridded patterns and determined automatically from the input files the parameter threshold is used for binarizing the value of each cell in the patterns into high and low the lag bin width is h fig 1d the values of h and threshold depend on the objective of the connectivity computation and the hydrologically relevant patterns that are being evaluated in this study we used 2 hypothetical flow patterns i e one includes areas of high and low and one only include areas of high for comparison at 1 m 2 m 5 m and 10 m resolutions a total of 8 patterns fig 3 and 3 modeled flow patterns at 2 m resolution on a dem as an example to demonstrate the sensitivity and correctness of cstat in each of the hypothetical patterns 16 flow pathways of different lengths and cardinal directions are included table 3 the total number of cells is 28 1 million 7 0 million 1 1 million and 0 28 million at 1 m 2 m 5 m and 10 m resolutions respectively for the hypothetical patterns h is set to 300 m and determined to uniquely include the total length of one flow pathway in one lag bin except the first lag bin of distance 0 table 3 we aim to demonstrate that our new algorithm for omni can identify the separation distances and cardinal directions that are unique to each connected flow pathway note that the hypothetical patterns have already been binarized into high and low regions as cells of 1s and 0s the modeled patterns at 2 m resolution represents several time snapshots of overland flow for the interested domain in a single storm event and the total number of cells is 70 k the parameter threshold is set to 0 1m nrcs 1986 we aim to demonstrate that our new algorithm for topo is able to quantify the impact of topographic impedance i e resistance on the connected flow pathways for the modeled patterns h is determined by variable i max and the number of lag bins n bin set by the users variable i max is the maximum possible lag distance and is estimated automatically based on the spatial dimension of the gridded pattern as the hypotenuse length of the domain s enclosing shape rectangle which is automatically extracted from the metadata of the input gis raster data then l max is divided by the number of lag bins of equal interval such that 2 h m i n l m a x n b i n note that n bin must remain the same for all the flow patterns computed because omni and topo based on different n bin or lag bin assignment are not directly comparable 2 2 2 step 2 reading the gridded patterns into cpu memory as numpy arrays and data preprocessing the gridded patterns of flow pathways in gis raster data format are read as 2d numpy arrays and the original spatial arrangement of cells is preserved as the array index locations cartesian coordinates as row and column note that the origin of the x and y axis in a numpy array index is at the upper left i e approximately the most nw location corner the threshold is used to binarize the cells of the numpy array into the value of 1s high for concentrated and channel flow and the value of 0s low for very shallow sheetflow the cells of missing value or nodata are assigned the value of 1s in the original algorithm proposed by western et al 2001 the search for the paired high cells and computation of their separation distances requires single cell based loops and recursions this requirement may be intuitive but significantly reduces the efficiency of the algorithm and is not memory efficient for grids with a large number of cells in this study we assumed that any connected pair of cells can only be found within a spatial clustered region i e each cell is connected with at least one of its eight neighboring cells therefore we reduced the computational intensity of the pairing process by limiting the searching boundary to each spatial clustered region with a unique region index the binary labeling and clustering procedure is similar to the image processing toolbox of matlab used by ali and roy 2010 in this study the clustered regions within the cells of 1s are identified and labeled using the python scipy module function imagine label the kernel shape of the neighboring cells was firstly defined in terms of which one s are included as connected for each center cell all eight neighboring cells in this paper then the imagine label function was used to segment the cells of 1 to clustered regions and labeling each region using sequential index integers starting from r 1 to rn thus the cells of 1s are replaced by the region index number e g fig 1c the preprocessing of the numpy array is completed and we name the processed array array 0 2 2 3 step 3 extracting coordinates of eligible locations from the numpy array and converting to the mxnet gpu array to compute the pairwise separation distances coordinates for the cells with label 1 or 0 of array 0 are extracted using the numpy module where the array vectorization technique was used to compute the separation distances 1 between high cells and low cells 2 between high cells and high cells and 3 between the connected pair of cells in each clustered region without single cell based loops the most efficient way of vectorizing numpy arrays is broadcasting i e a scalar based matrix operator between arrays of different dimensions shapes broadcasting between two arrays is equivalent to generating full permutations between the elements of each array i e each element in one array is paired with each element in another array regardless of their redundancy in order in this paper we reduce the full permutations to full combinations by using python array slicing operator for computational efficiency next cartesian coordinates of array 0 are extracted into two numpy arrays with dimension of n 2 based on their values excluding areas of missing data one array represents all high cells and named as array 1 and the other represents cells with value 0 i e all low cells and named as array 2 the cartesian coordinates of each clustered connected region of high are also extracted named as array r n where n is the total number of clustered regions the array 1 array 2 and array r n are then divided into small segments and assigned unique segment indices the length of these segments i e the number of the array elements sliced in each iteration is equal to the broadcdp we name the mth segment of array 1 as sma1 the mth segment of array 2 as sma2 and the mth segment of array r n as smarn as we mentioned earlier the reason for dividing these arrays into smaller segments is that gpu memory capacity is usually more limited compared with cpu system memory thus the issue of gpu memory overload can be eliminated if the broadcdp is set appropriately sma1 sma2 and smarn are then sequentially converted into mxnet gpu arrays and the vectorization is implemented using broadcasting between two segments in the following substeps 1 sma1 and sma1 all high cells 2 sma1 and sma2 high and low cells and 3 smarn and smarn high cells that are connected within each clustered region this broadcasting operator generates two element full permutations here the benefit of broadcasting gpu arrays instead of cpu arrays is the much higher parallel computation capacity of a gpu card than that of a cpu this broadcasting procedure achieves identical results to recursively searching the paired locations in array 1 between array 1 and array 2 and array rn without explicit looping through each cell or using any recursion function note that broadcasting and computation of the euclidean distances between any paired coordinates in each segment are implemented simultaneously then we include a logic to determine if the two segments being broadcasted for substep 1 and 3 are identical i e of the same segment index if this condition is met the redundant elementwise pairing between the same pair of cells but with switched order e g x y and y x and the cell pairing with itself e g x x or y y is excluded from the computation to increase the computation efficiency this integrated logic is equivalent to only including the two element combinations instead of permutations for substep 1 and 3 note that we only use the relative scale and distance in the pairing and computation process based on the 2d array cartesian coordinates this step completes the collection of all the possible separation distances and the corresponding counts of pairs in each lag bin the detailed illustration for the entire array manipulation process including the pairing between two segments and between elements of paired segments in this step can be found in fig 4 2 2 4 step 4 computing the connectivity functions and icsls in each lag bin we assign the aggregated values for the total number of pairs from section 2 2 3 step 3 substep 1 and 2 as tc g similarly we assign the total number of connected pairs and the sum of lag distances from section 2 2 3 substep 3 as tc z and ld g the probability of the connectedness i e connectivity œÑ h in each lag bin is calculated as p tc z tc g and the average separation distance for each lag bin is calculated as l mean ld g tc z note that in this paper we only consider the average separation distance for the high cells instead of including both high and low cells as proposed by western et al 2001 in addition if the low cells are present the result of œÑ h that are calculated by cstat is slightly larger than ¬Ω of the previous algorithm i e the total number of two element combinations is ¬Ω of two element permutations for substep 3 thus tc z is ¬Ω of previous algorithm and tc g is smaller than previous algorithm because the result of substep 1 is slightly reduced compared to previous algorithm 2 3 cstat card cstat card is computed by appending a cardinal direction indicator i e ns we ne sw or nw se for the connected pairs of cells the direction is computed by the slope of a line connecting the paired locations fig 5 and then converted to degrees 90 90 based on the inverse tangent function to remove negative values we added 90 so the range becomes 0 180 therefore the degree values should be interpreted as the counterclockwise deviation from the ns direction half plane and 0 is equivalent to 180 we compute the histogram of connected pairs in each of the 4 cardinal directions for each lag bin that can be presented in the results addition to œÑ h similarly we also included the computation of the cardinal direction specific œÑ h and corresponding omni which describes the œÑ h for all the connected pairs in a cardinal direction all the tabular results for œÑ h including average separation distances probability of connectivity the histogram in each cardinal direction for each lag bin omni and the computational time are written to text files 2 4 cstat topo cstat topo follows the same procedures as cstat omni except that before proceeding to step 3 a flow pathway verification step is included in advance to ensure that the connected pathways are topographically valid i e the trajectories on the pathways between any pair of locations are counted as connected only if they follow overlap downslope flow routing directions on the topography without interruption then we define these pairs as being topographically connected since this step is between step 2 and step 3 in computing omni it is named step 2 5 2 4 1 step 2 5 identifying the topographic determined flow pathways the dem in raster grid format is converted to an mxnet gpu array which is named array dem the downslope flow directions for each cell in the array dem are generated based on the md routing method proposed by seibert and mcglynn 2007 and stored in an indicator grid array d to eliminate the unnecessary search we limit the downslope flow accumulation areas within the location of each spatial clustered region in array r n only the separation distances are computed similar to omni except that one cell of a pair is always fixed and is the cell where the downslope flow accumulation starts whose elevation is equal or higher than other cells in the downslope accumulated area the cells in flat areas i e with 8 neighboring cells of the same elevations are identified simultaneously and labeled in another indicator grid array flat in this study we created a new algorithm to compute the flow direction using convolution and pooling operators instead of single cell based loops and recursions the details of our new implementation are described below first the dem grid of the same spatial domain as the flow pattern is read into memory as a numpy array using the geospatial data abstraction library gdal warmerdam 2008 and converted to an mxnet gpu array array dem two additional gpu arrays are also generated to store the value of cartesian coordinates of the array dem as array x for rows and array y for columns then for each cell m eight triangular facets based on m s eight neighboring cells are defined starting from east 0 and rotating counterclockwise back to e 360 see fig 6 a the same as in d and md seibert and mcglynn 2007 tarboton 1997 the eight facets and neighboring cells are indexed by eight integer numbers in a counterclockwise sequence as facet 0 to facet 7 and neighbor 0 to neighbor 7 fig 6b three empty python lists of length 8 are initiated to store the arrays of differences between the values of centered cells m and two contiguous neighboring cells for each of the eight triangular facets e g p1 and p2 in facet 1 based on the array x array y and array dem we name the arrays dxi dyi and dzi where i is the neighboring cell index number from 0 to 7 for example dx2 indicates the difference of the x axis for the neighboring cell to the north dx2 xm xp2 instead of looping through each cell of m in array x array y and array z to compute the dxi dyi and dzi we use the convolution and pooling operators based on kernel weights in mxnet convolution and pooling are often used as an efficient and optimized approach in machine learning for image processing convolutions are based on a 3 3 kernel window i e a square of 9 cells of specific configuration as the weight kernel which slides from the top left to the lower right for each cell of array x array y and array dem by rows fig 7 we designed a series of 8 kernel structures and each one corresponds to only one neighboring cell table 4 for example for weighted kernel 0 it computes the convolution for dx0 the adjacent cell on the east similar convolution and pooling operators are implemented on array y and array dem and the results are stored at each cell in array dx i array dy i and array dz i then for each triangular facet we compute the normal vector between cell m and the two contiguous neighboring cells as defined in seibert and mcglynn 2007 3 n j n x j n y j n z j d z i d y i 1 d z i 1 d y i d z i d x i 1 d z i 1 d x i d y i d x i 1 d y i 1 d z i where j represents the facet index number from 0 to 7 the normal vectors nxj nyj and nzj are stored separately in three gpu arrays next the flow directions and the slopes for each triangular fact are computed based on nxj nyj and nzj as in seibert and mcglynn 2007 and stored in gpu array dj at the location of each cell m 4 d j 0 n x j 0 a n d n y j 0 œÄ n x j 0 a n d n y j 0 œÄ 2 arctan n y j n x j n x j 0 3 œÄ 2 arctan n y j n x j n x j 0 note that the convolution operators cannot apply to the edge of array x array y and array dem i e the arrays dj have the dimension of only that of array x array y and array dem minus 2 we extend the shape of array x array y and array dem by appending zeros to the edges so the dimensions of dj are consistent with array x array y and array dem this implicitly assumes that the flow directions on the edge of the dem are always out of the domain the edge appending can be achieved by using a pad parameter in the convolution operator in mxnet similar to seibert and mcglynn 2007 we made further modifications for these cases when the flow direction of the facet dj is not within 45 degrees of the corresponding facet or both elevations of neighboring cells on the facet e g p1 and p2 are higher than that of cell m thus the last step of updating dj is to ensure that for the downslope directions directly pointing toward the neighboring cells of m i e œÄ 4 or j 1 œÄ 4 only the adjacent facets with the same directions i e dj dj 1 and dj dj 1 are maintained otherwise these values in dj are eliminated and converted to 999 as nodata note that dj includes the flow directions at each of the 8 facets i e a total of 8 arrays to simplify the further process and save the memory space we collapsed the directional information for the 8 facets in dj into array flowdir by using the directional kernel values based on the 2n start from the east 1 represents e 2 represents ne 4 represents n and so on so that the value of each cell in array flowdir represents a unique flow direction combinations a total of 255 unique combinations 8c1 8c2 8c8 from that cell in the later process the exact directions at a particular cell can be efficiently retrieved from a python dictionary e g unpack the value of array flowdir based on the 1 to 1 correlations 2 4 2 flow direction assignment in topographic depressions and flat areas of the dem topo is designed to compute the connectivity states based on existing flow patterns either from observations or modeling instead of physically simulating water mass momentum and energy transfer therefore the spatial extent of the flow pathways from the observations or modeling is used as the mask to indicate the regions in which the topo algorithm is implemented we used the priority flood barnes et al 2014a to fill the topographic depressions and then subtract the original dem from the depression filled dem as Œ¥dem to delineate the boundary cells of each depression and pour point s the boundary delineation is achieved by using two functions from the scipy library 1 a binary labeling on the Œ¥dem to generate the internal areas for each depression and 2 a dilution operator to generate the edges based on the internal areas for each depression then the pour points cells are identified as the cells with the lowest elevation s on the depression boundary the priority flood depression filling approach also ensures that the downslope flow pathways are not being blocked at the highest level of depressions or nested depressions in topology that are no lower than the nested pour points thus this approach allows a flexible determination of the aggregated effects i e the magnitude of resistance of depressions on connected pathway s based on a spatial scale for the topographic gradient that is larger than the scales of individual depressions note that some flat areas may appear in the depression filled dem to calculate the flow directions for the flat areas we used a customized version integrated with the md routing method of the algorithm proposed by barnes et al 2014b which generates the artificial gradient between the cells located at the edges of each flat region that are 1 adjacent to the neighbor of higher elevations i e high edge and 2 adjacent to the neighbor of low elevations i e low edge the flow direction grid that is generated based on md in the previous step is amended as array flowdir amend to include the flow direction for the cells in flat areas and depressions that have not been previously assigned 2 4 3 downslope search for connected pairs and the computation of topo after the flow direction grids are generated downslope flow accumulation areas are delineated within each of the spatial clustered regions r n of array 1 one cell within the spatially clustered region is assigned as the starting location kn and the downslope accumulation area cn is delineated based on the flow directions indicated by array flowdir amend the search loops through each cell as the starting point kn in the region r n by tracing the chain of its downslope cells controlled by the direction kernel values in array flowdir amend and looks up the exact flow direction s in the dictionary built in the previous step until the chain of downslope cells is empty the cells in cn are marked as 1 in an indicator array array flowdown anywhere else are 0s to delineate cn using a memory and stack efficient way in topo we assume that the traverse of the flow cascading downslope in cn on the topography is equivalent to the tree branches rooted at the starting upslope location kn this assumption allows us to use a parallel nonrecursive strategy i e the breadth first search bfs method bfs searches all the neighboring cells at the current depth the closest neighbors at one cell distance and then the neighbors at a two cell distance and so on until all the tree branches downslope cells are visited fig 8 this search is implemented by creating a stack control list i e the chain and a marker list the stack control list is used to append new locations that need to be processed to search downslope cells i e in the current eight neighbors and to indicate the termination of the iteration when no cell location remains the marker list is used to exclude the locations already visited in the previous iterations and avoid repeated search once this downslope search procedure is completed the starting point kn is paired with every cell in array flowdown where the values are 1s and the separation distances are computed based on the coordinates of the cells in array flowdown the starting point kn is always one location of a pair thus no combinations between two segments of r n are searched but we simply match the starting cells and all the cells in one segment of r n i e one to many broadcasting this process repeats for all the cells in the same clustered region r n in cn the rationale for this step is that because any pair of cells within the same spatial clustered region r n are connected any other locations within cn paired with the starting cell kn and within the same spatial clustered region r n are also topographically connected therefore the algorithm we designed can efficiently count the pairs and compute the separation distances between kn and any other locations within cn by limiting the pairing process within the spatial clustered regions r n connectivity functions and icsls are then computed in a similar manner as described in step 3 note that we do not apply any weight on the flow directions in this study because md and d are identical in most cases with mostly a proportion between two adjacent neighboring cells based on the triangular facets 3 test results and discussions we use the decrease rates of connectivity drc i e the decrease rate of the total connected pairs of cells between any two contiguous lag bins from a lower to a higher separation distance to test the sensitivity for the flowlines based on their corresponding maximum connected lengths and cardinal directions in the hypothetical flow pattern see fig 3 the drc can be taken as an indicator of individual connected objects e g flowlines based on the local maximum for connectivity functions the results indicate that the drc is a robust indicator for detecting individual components in hydrological patterns and is consistent with different spatial resolutions fig 9 the plots of the connectivity function and omni values for the flow pattern see fig 10 using 1 m resolution as an example plots for other resolutions can be found in the supporting information show that omni at each cardinal direction can be used to identify changes in separation distances on connected flow pathways for example the total lengths of connected flowlines are ranked from high to low as we ns ne sw and nw se which can be reflected on the omni at each of the corresponding cardinal directions however for flow patterns with a large portion of low cells the sensitivity of omni at each cardinal direction is significantly reduced such that the difference between omni values becomes negligible note that the difference between the omni at each cardinal direction and the aggregated omni for the entire pattern is also negligible for the pattern with significant low cells because the area under the connectivity function curve between 0 and the first lag bin 1 300 m takes more than 98 of the total area in addition the impact of grid resolution on the connectivity function and omni is lower compared with that of the number of low cells the aggregated omni values increase approximately 20 from 1 m to 10 m resolution for the patterns with a significant number of low cells and omni at each cardinal direction only increases approximately 8 conversely both the aggregated and the directional omni values for the patterns with only high cells show negligible changes because the total number of high cells have a much smaller degree of decrease with the decrease of spatial resolution i e 1 10 for the test patterns compared with that of low cells i e 1 100 for the test patterns thus if no low cells exist changes in connectivity functions and omni are negligible regardless of the grid resolution we further tested cstat including omni and topo on a local region 360 m 845 m of a meandering river course including an oxbow lake and its surrounding floodplain the oxbow lake does not connect to the stream in low intensity rainfall events and can be considered a depression the overland flow and streamflow patterns i e time snapshots are derived from the map output of a fully distributed 2d hydraulic model for a single storm event which is calibrated to the hydrograph at the watershed outlet three snapshots are included in this study 18 h 24 h and 31 h representing the initial rising and peak of the storm event the drc graphs show that at the initial stage of the event 18 h maximum separation distances on the connected flow pathways i e local maximums at the four cardinal directions are readily identifiable fig 11 a the highest magnitude of the drc is at the separation distance of 236 m and then a significant decrease appears at 300 m at the nw se direction fig 11a marked in purple these changes in the drc indicate three segments of the flow pathways that are connected mainly segments on the streamflow fig 12 at the nw se direction are all smaller than 236 m marked by orange arrows but the euclidean distances between any two segments marked by green arrows are larger than 300 m as the storm event proceeds i e at 24 h when the overland flow at the floodplain is prevalent the magnitudes of the local maximum decrease and drc graphs becomes smoother such change is even more obvious for the snapshot at 31 h when the local peaks in the drc graphs are not readily visible the drc graphs at 31 h indicate that the total number of connected flow pathways at each lag bin increases significantly and the difference of their lengths and cardinal directions become minor omni at 18 h is higher than at 24 h because the large amount of low cells has not yet appeared in the flow pattern table 5 the soil has relatively high infiltration capacity at 18 h but lower capacity at 24 h causing the occurrence of more infiltration excess overland flow represented by wet cells but in low because of the shallow flow depth at 31 h peak of the storm event deeper overland flow with mostly high cells concentrates in this region and the connected flow pathways have the largest spatial extent among these snapshots therefore omni at 31 h is the highest in contrast topo for all of the three snapshots are significantly lower than omnis with the maximum separation distances of 40 50 m on the connected flow pathways note that the aggregated topo does not change between the snapshots although a slight increase for the topo at each cardinal direction was found from 18 h to 31 h in addition topo for the flow patterns generated from the depression removed filled dem in the same region shows identical results these results mean that the md flow direction algorithm adapted in this study is robust and consistent to identify the directional flow connectivity on topography with or without dem preprocessing therefore cstat provides a realistic and robust way of measuring the hydrological connectivity for overland flow and streamflow on topography including the complex features such as topographic depressions and flat areas the trade off is that topo takes significantly more computing time than omni the results also show that cstat generates similar results to the existing algorithms the detailed comparison between cstat and existing algorithms is included in the supporting information as an ms excel file the computing time in seconds of omni for each of the hypothetical patterns is presented in fig 13 the algorithm of western et al 2001 using single cell based loops and recursions and is significantly slower than the other two i e the algorithm in this paper and the one proposed by ali and roy 2010 using matlab and the speed difference increases as the total number of high and low cells increases e g from 400 times to 3500 times faster for the test pattern at 2 m resolution with a total number of high and low cells of more than 3 million western s algorithm takes more than 0 61 million seconds 7 days and 2 h to complete the computation while ali and roy s algorithm and cstat generate the results within 967 and 161 s respectively note that the speed difference between cstat and ali and roy s algorithm is much smaller but also increases as the total number of high and low cells increases e g from 18 times slower at 10 m resolution to 7 2 times faster at 1 m resolution to further provide the reader with a detailed estimation of the computing time based on the number of grid cells for cstat we used a series of hypothetical flow patterns 48 in total with the number of high and low cells between 4 and 4 million stepwise increments the results indicate that cstat is more suitable for flow patterns high cells only with the total number of high cells larger than 1000 or high and low cells coexist larger than 200 and 3 9 million respectively note that we did not estimate the computing time t for topo because of the different complexities of topography the computing time t in seconds for cstat omni can be estimated by the equation below 5 t h 0 04646 h l 1 0045 10 8 i f l 0 h 2 1 10 8 h 0 00153 i f l 0 where h and l are the total number of high and low cells respectively this equation indicates that the total number of high cells has significantly more impact on the computing time than the low cells similar to the current algorithms for the icsls cstat also requires the observations of hydrological patterns or outputs of a physically based model for the hydrological processes in the study area as a priori for quantifying the characteristics of connectivity states the advantage of cstat is that it can be applied to hydrological patterns based on large grids the improved computational efficiency of cstat will open the doors to a deeper understanding of system dynamics e g the external drivers and help to build the linkage between physical processes across spatial scales at high resolution 4 conclusions connectivity characteristics of hydrologic processes depend on the variability of complex connectedness of the flow pathways across multiple scales landscape features e g local scale topographic depressions and flat areas may also be contributing to the shifts in connectivity these small scale features are often considered the source of high uncertainty in the hydrologic models e g antoine et al 2009 bracken et al 2013 connectivity at the hydrosystem level reflects the complex aggregation of feedback interaction dynamics from these small scale factors but it has been difficult to find a unified indicator to inspect the relationships they have with the physical processes the explicit description of changes in connectivity states provides a novel venue to identify how the hydrosystem states is transformed which reflects such feedback dynamics to date although connectivity statistics are among the very few promising options available low computational efficiency is severely limiting the applications of the current algorithm for analyzing hydrological processes at the hydrosystem level uisng high resolutio data in this paper we introduce a new algorithm cstat for quantifying connectivity characteristics this algorithm is capable of processing large datasets in a reasonable timeframe the workflow of this algorithm is fully automated and requires minimum user interference additionally the general style code can be implemented on various platforms and hardware configurations we envision that cstat can pave the way for investigating connectivity characteristics based on high resolution hydrological pattern across multiple scales connectivity characteristics can be useful for the hydrological model community to improve model conceptualization and parameterization e g by designing a better sampling scheme and reducing the model complexity as discussed in keesstra et al 2018 as a case study we implemented cstat omni in a real world watershed and discussed the effect of small scale topographic features on the overland flow and streamflow response by the changes in connectivity states across multiple scales up to the watershed scale yu and harbor 2019 a major limitation of cstat is that it depends on the fully distributed model or high resolution in situ or remotely sensed observations to generate the hydrological pattern i e does not natively account for the water mass and momentum transfer between the paired locations ali et al 2018 in addition the computing time is not a simple linear function of the number of high and low cells in the grid which can reduce computational speed when a complex pattern is being analyzed future work to improve the algorithm will focus on the following 1 incorporate a simplified water and water mediated substances transfer scheme between any pair of cells to determine the material connectedness without relying on fully distributed models 2 parallelize the algorithm across multiple gpus to obtain a higher computational speed increase and 3 incorporate a network distance computation scheme for the separation distances so the subsurface flow connectivity can also be estimated where the trajectories of the flow pathways cannot be measured based on euclidean distance 5 software and data availability the cstat including omni and topo algorithm was developed by the first author of this paper and implemented in mxnet version 1 2 0 a machine learning framework with python binding cstat requires numpy scipy and gdal modules installed in the python version 3 x environment a workstation with at least one gpu device of memory size 4 gb is required source code is under the apache 2 0 license and can be accessed from the author s github repository at the link below https github com codefortheplanet cstatplus instructions regarding the use of the code and the accompanying test data can also be found in this repository acknowledgments purdue university provided fellowship funding support for this research appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104496 
26157,sea surface temperature sst is a vitally important parameter of the global ocean which can profoundly affect the climate and marine ecosystems to achieve an accurate and holistic prediction of the short and mid term sst field a spatiotemporal deep learning model is proposed which can capture the correlations of sst across both space and time the model uses the convolutional long short term memory convlstm as the building block and is trained in an end to end manner experiments using 36 year satellite derived sst data in a subarea of the east china sea demonstrate that the proposed model outperforms the persistence model the linear support vector regression svr model and two lstm models with different settings when judged using multiple statistics and from different perspectives the results suggest that the proposed model is highly promising for short and mid term daily sst field prediction accurately and conveniently keywords big data modelling spatiotemporal deep learning convlstm sea surface temperature sst field prediction time series satellite data 1 introduction and background sea surface temperature is one of the most important parameters of the world s ocean and plays a fundamental role in the exchange of energy momentum and moisture between the oceans and the atmosphere gentemann et al 2000 changes of sst can have profound effects on the global climate marine ecosystem and even vegetation bouali et al 2017 cane et al 1997 castro et al 2016 chaidez et al 2017 friedel 2012 herbert et al 2010 rauscher et al 2015 yao et al 2017 it can affect the precipitation distribution and further lead to droughts and floods rauscher et al 2015 salles et al 2016 daily sst which shows surface thermal front and intensity can be utilized to help detect marine ecosystems and assess the variability of such ecosystems and to improve the understanding and qualification of the vertical structure of the water mass and the internal wave propagation besides daily sst has been widely incorporated into hurricane models for anticipating tropical cyclone intensity esa therefore it s vitally important and necessary to predict the future dynamics of sst especially daily sst in order to help identify potential extreme weather events in advance sst prediction is an active research topic methods for sst prediction can be summarized into three categories including numerical methods data driven methods and the combination of these two the numeric methods are established based on physical chemical and biological parameters as well as the complex interactions among them aparna et al 2018 liu and fu 2018 stockdale et al 2006 it uses mathematical models to describe the variations of sst the numerical methods are usually quite sophisticated and predict sst at the ocean and even global scales with relatively low resolutions recent work has improved the sst and sst related variables forecast potential of the physical model namely nordic version of the nucleus for european modelling of the ocean nemo nordic in the baltic sea by assimilating high resolution sea surface temperature data into the model liu and fu 2018 the data driven methods attempt to solve the sst prediction problem from a data centric perspective they approximate the processes that generate the observed sst from patterns and relationships found in the observations the learned patterns are then used to infer future ssts the data driven methods require less ocean and atmospheric domain knowledge and are less sophisticated than the numerical methods besides they are suitable for high resolution sst predictions at relatively small scales methods in this category include traditional statistical methods such as the markov model yan and ants 2000 the popular machine learning approaches and artificial intelligent ones support vector machine svm and neural network are widely used machine learning methods for sst prediction aparna et al 2018 lins et al 2013 patil and deo 2017 tanggang et al 1998 wu et al 2006 zhang et al 2017 artificial intelligence methods for sst prediction include genetic algorithm ga and particle swarm optimization pso lins et al 2013 neetu et al 2011 to improve the prediction performance the numeric methods are combined with the machine learning methods patil et al 2016 in this paper we focus on the high resolution local sst field prediction in the east china sea as the numerical methods are too complex and involve the sophisticated interactions of a lot of variables the much simpler data driven methods are chosen however for traditional data driven methods most of them treat the observations in each location of an area independently without considering the influences of its surrounding locations which may reduce the prediction performance besides the prediction process is usually performed at each location individually which is inconvenient zhang et al 2017 therefore a prediction model that considers the spatiotemporal context and can predict the areal sst field holistically in an end to end manner is needed to this end the recently proposed recurrent neural network unit called convolutional long short term memory convlstm can be used due to its successful usage in similar applications such as precipitation prediction kim et al 2017 shi et al 2015 and video frame prediction sautermeister 2016 in these similar tasks deep learning models based on the convlstm unit which captures spatiotemporal correlations have achieved the state of the art prediction performance in this study we adopt the convlstm unit as the building block of our deep learning model to model the complex spatiotemporal dependencies of sst and further utilize the captured spatiotemporal dependencies for the sst field prediction task the goals of this paper are twofold 1 developing a spatiotemporal deep learning model that can model and capture both the spatial and temporal dependencies of sst and predict sst field accurately and holistically in an end to end manner 2 investigating the applicability effectiveness and advantages of the developed model in predicting short and mid term daily sst fields through experiments in a selected area in the east china sea using 36 year time series satellite data the remainder of the paper is organized as follows section 2 describes the study area and satellite derived time series sst data used in this study section 3 presents the proposed spatiotemporal deep learning model for sst field prediction the experimental results and thorough discussion are given in section 4 finally section 5 concludes the paper 2 study area and data 2 1 study area the study area locates in the east china sea the east china sea is part of the famous maritime silk road and one of the busiest seaways in the world understanding the patterns of sst and predicting future sst in this area is vitally important for safe seaway transportation as well as the production and lives of people in china and its surrounding countries as the east china sea is located in the subtropical zone the average annual water temperature is between 20 c and 24 c with the annual temperature difference between 7 c and 9 c the fluctuating characteristics of the sst in the east china sea makes it suitable to be used for testing the prediction performance of the proposed spatiotemporal deep learning model in this study we choose a subarea of the east china sea as our study area covering the spatial extent of 27 5 n 33 n 123 5 e 127 5 e as is shown in fig 1 the reason why we choose this subarea is that it is the rectangular area that covers majority of the east china sea without land pixels in it 2 2 data the data used is the daily optimum interpolation sea surface temperature daily oisst version 2 produced by the national oceanic and atmospheric administration noaa with a spatial resolution of 1 4 latitude by 1 4 longitude it s an analysis data using observations from multiple platforms including satellites ships and buoys on a regular global grid reynolds et al 2007 at each grid it contains a long time series of daily mean sst the gaps are filled through interpolation making it a spatially complete sst dataset the satellite sst derived from the advanced very high resolution radiometer avhrr is used to produce the adopted oisst which covers the global ocean from 89 975 s to 89 875 n 0 125 e to 359 875 e and covers temporal range from 1982 09 01 to present continuously the subset of the oisst for the study area contains 13 421 daily sst images from 1982 09 01 to 2018 05 30 each image is with the size of 22 16 pixels the original data are pre processed and transformed into a 5 dimensional 5 d tensor which is about 1 9 gigabytes gb before feeding the tensor for training and testing the proposed spatiotemporal deep learning model and the lstm models for comparison purpose the tensor is normalized to the range 0 1 for the linear svr model the tensor is normalized to the range 1 1 the normalization can make the data more centered which helps to train the models more easily and quickly and to further improve the performance of the models 3 formulation of the sst field prediction problem in our research the goal of sst field prediction is to forecast the next 10 days sst field using the past few days sst field fixed prediction window to achieve this goal we adopt a rolling prediction scheme in this scheme we first use the sst field in the prediction window to forecast the next 1 day s sst field then we move the time window 1 day ahead which incorporates the newly forecasted sst field as its latest element to predict the sst field one more day ahead by repeating this process 10 days ahead sst field prediction can be achieved therefore the key to achieving the goal is to predict the next 1 day s sst field suppose we observe the dynamic sst field over a spatial region which is represented by m n grid with m being the number of rows and n being the number of columns inside each cell of the m n grid there are v measurements as we only use sst as the predicting variable the value of v is 1 varying over time thus the observation at any time slice t can be represented by a 3 d tensor x t r m n v where r represents the domain of the observed sst in a period of time n we can obtain a spatiotemporal sequence of observations x t n 1 x t n 2 x t the 1 day ahead sst prediction problem is thus to forecast the most likely tensor in the future given the previous n days observations which can be formulated in equation 1 and illustrated in fig 2 1 x t 1 argmax x t 1 p x t 1 x t n 1 x t n 2 x t where p is a conditional probability and x t 1 is the predicted sst field at time t 1 4 the spatiotemporal deep learning model for sst field modelling and prediction 4 1 building block of the model from section 3 we can see that the sst field prediction involves the domains of both space and time to deal with it we adopt the convlstm as the building block of our model the convlstm was proposed to overcome the drawbacks of fully connected lstm fc lstm in handling spatiotemporal data during which the spatial correlations are lost shi et al 2015 different from fc lstm the convlstm determines the future state of a cell in the grid by the inputs and past states of its local neighbours during which both the spatial and temporal correlations are captured and utilized this is achieved by replacing the matrix multiplication operation used in fc lstm with convolution operation in the state to state and input to state transitions as shown in fig 3 the working mechanism of the convlstm including the gates input gate forget gate and output gate and information flow can be expressed using equation 2 7 with all the variables defined and explained in table 1 in the equations denotes the convolution operation and denotes the hadamard product œÉ is a sigmoid function used as the activation function applied to the weighted sum of the inputs of each gate the weighted sum of the inputs of each gate is a 3 d tensor and the œÉ is applied to each element of the 3 d tensor 2 f t œÉ w x f x t w h f h t 1 w c f c t 1 b f 3 i t œÉ w x i x t w hi h t 1 w c i c t 1 b i 4 c t tanh w x c x t w hc h t 1 b c 5 c t f t c t 1 i t c t 6 o t œÉ w x o x t w ho h t 1 w c o c t b o 7 h t o t tanh c t 4 2 structure of the model to better capture the spatial and temporal correlations among sst fields at different time we build a spatiotemporal deep learning model by stacking the convlstm layers as shown in fig 4 the model consists of four layers including an input layer two convlstm layers and a fully connected layer as the output layer our experiments reveal that this neural network design can achieve the best prediction accuracy among a couple of designs the input of the whole network is in the 5 d form expressed as samples timesteps width height features samples is the batch size for training empirically the batch size is set to a value around 128 by trials and errors around 128 we set it to 150 which can achieve the best prediction accuracy and relatively high training efficiency timesteps is the length of the time window of the input sst fields to predict the future sst field in our method we set it to 50 which is about 4 times of the prediction length 10 days according to the periodical changes of temperature data zhang et al 2017 meaning that we use the previous 50 days spatiotemporal sst sequence to predict the 51st day s sst field width and height are 22 and 16 respectively which are the spatial size of the input sst images as we only use one observation variable namely sst as the predictor features is set to 1 as there are no theoretical rules for determining the best number of convolutional kernels of a convolutional layer the usually adopted method is to test the performance of the neural network model by varying the number of kernels in a given range using this scheme we set the number of the convolutional kernels of both the two convlstm layers to 12 similarly we vary the size of convolutional kernels in a given range and finally find that the model can achieve best performance when the sizes of all convolutional kernels are set to 3 3 zero paddings are used to ensure that the states have the same spatial dimensions rows and columns as the inputs we use mean square error mse as the loss function and adam kingma and ba 2014 as the optimizer for training the model the whole model is conveniently trained in an end to end manner 5 experiments and discussion 5 1 experiment setup in this study we compare the proposed spatiotemporal deep learning model with three different models including lstm linear svr and persistence model only for the comparison of 1 day ahead predictions for the lstm model two different kinds of settings are compared one is that we treat each pixel in an sst field as an individual sample and it contains only one feature which is called lstm 1 feature model the other setting is that we treat all the pixels in an sst field as a sample and it contains 352 22 16 features which is called lstm n features model both these two settings consider only the temporal correlations except the second one is an end to end trainable model the linear svr model adopts the same setting as the lstm 1 feature model the persistence model simply uses the current sst field as the predictions of the next day to build the dataset for training and testing the models we first create the 5 d tensor as stated in section 2 2 the tensor is arranged in the order of time along the samples dimension then along the samples dimension of the 5 d tensor described in section 2 2 we select the first 80 as the training samples and the remaining 20 as held out testing samples further 5 of the training samples are split out for validation purpose during training namely the training and testing samples are not randomly chosen but chosen in the order of time the testing samples are samples in the future relative to the training samples the actual number of samples for training validation and testing are shown in table 2 all of the samples are with the dimension of 51 22 16 1 the proposed spatiotemporal deep learning model and two lstm models are implemented with keras using tensorflow 1 5 0 gpu version as the backend and trained and tested on a computer with a single nvidia titan xp gpu abadi et al 2016 chollet 2015 the linear svr is implemented using scikit learn 0 19 1 pedregosa et al 2011 during training of the proposed spatiotemporal deep learning model the loss function namely mse decreases quickly and converges to a small value on both the training dataset and validation dataset and the difference between the two converged values are small indicating that the model is trained to a proper degree besides the training results of the proposed model are relatively stable with respect to repeated training though they are different in the first few epochs for repeated training due to the random initialization of the model they then converge and become stable the difference among the converged values for repeated training are small as well three indexes are selected to measure the performance of the different models for sst field prediction including the root mean square error rmse the mean absolute percentage error mape and the pearson correlation coefficient r which are defined as follows 8 r m s e 1 n d i 2 n 9 m a p e 1 n 1 n y i y i y i 100 10 r 1 n y i y y i y 1 n y i y 2 1 n y i y 2 where d i is the error vector calculated by the difference between the reference sst value y i and the predicted sst value y i y and y are the mean value of the reference sst values and the predicted sst values respectively n is the total number of testing samples 5 2 experiment results and discussion we conduct experiments for 1 10 days ahead predictions and compare the prediction performance of different models using different statistics and from different perspectives specifically for the 1 day ahead prediction persistence model is incorporated for comparison fig 5 shows the spatial distribution of rmses over the study area for 1 day ahead sst field prediction using different models it can be seen that the proposed model performs better than the linear svr lstm 1 feature model and the persistence model and much better than the lstm n features model it can also be seen that all the five models have their own largest prediction errors at nearly the same locations this may be due to the abrupt changes of sst at these locations as shown in fig 6 however the prediction capability of the proposed model is less affected by this condition fig 7 shows the rmse of predictions for the lead time of 1 10 days by different models for each prediction lead the rmse is calculated over the whole study area on all the testing samples the figure shows that the proposed model consistently outperforms all the other three models which means that it can better capture the patterns hidden in historical sst fields and make more accurate predictions based on the captured patterns the improvement of the rmse of sst field predictions by the proposed model relative to the three models is given in table 3 for linear svr and lstm 1 feature model the improvement increases along with the lead time of prediction while for the lstm n features model the improvement decreases from the lead time of 1 7 days and then increases one interesting finding from fig 6 is that the linear svr model outperforms the lstm 1 feature model for the lead time of 1 4 days but it performs worse than the lstm 1 feature model from the lead time of 5 days a similar phenomenon exists between the linear svr model and the lstm n features model the reason may be that lstm based models are good at modelling long time dependencies of sst with their recurrent network structure and gating mechanisms fig 8 and table 4 show the comparison of mape of 1 10 days ahead prediction of sst field by different models and the percentage improvement of mape by the proposed model relative to the other three models respectively they show similar patterns as those of the rmse in fig 7 and table 3 in fig 9 the pearson correlation coefficients r between the predicted sst field and the reference sst field for each prediction lead on all the testing samples are compared the corresponding p values for the calculated r are all 0 showing the statistical significance of the calculated r it shows that the r values of the proposed model are quite close to 1 and consistently bigger than those of the other three models showing a stronger positive linear correlation between the predicted sst and the reference sst besides the r values of the proposed model decrease more slowly than those of the other three models as the prediction lead increases from 1 to 10 days the r values of the linear svr model shows almost a straight downward trend as the prediction lead increases fig 10 compares the prediction performance of different models from the perspective of the spatial distribution of rmses of prediction over the study area it shows that for each prediction lead the rmses of the proposed model are lower than those of the other three models and are more evenly distributed over the study area the rmses of the linear svr model lstm 1 feature model and lstm n features model increase quickly at locations where sst changes quickly as shown in fig 6 as the prediction lead increases in contrast the proposed model has consistently and stably better performance at these locations to investigate the prediction performance from a holistic perspective we compare the distribution of the prediction errors of 1 10 days ahead as a whole it is achieved by conducting the kernel density estimation kde of the prediction errors of different models using the gaussian kernel terrell and scott 1992 the kde curves of prediction errors of different models are shown in fig 11 the figure reveals that the proposed model has a much denser kde curve with its mean almost equivalent to 0 showing that the errors of the proposed model spread more densely around 0 than the other three models do table 5 is also from a holistic perspective and shows the performance statistics calculated on all the 1 10 days ahead sst field prediction of each model which demonstrates the outperforming prediction capability of the proposed model as well besides the lstm 1 feature model predicts better than the linear svr model considering long prediction leads holistically which is in line with what has been previously observed from fig 7 from the above results using different statistics and from different perspectives we can see that the proposed model outperforms the liner svr model the lstm 1 feature model and the lstm n features model for the sst field prediction task the reason is that the proposed model has both spatial modelling capability by using convolutional operation and temporal modelling capability by using recurrent network structure and gating mechanisms it determines the future state of a cell in the grid by the inputs and past states of its local neighbours during which both the spatial and temporal correlations of sst are captured and utilized which however is not inherently implemented in other models as we aim to predict the short and mid term daily sst field the experiments only investigate the prediction performance of different models for 1 10 days ahead however the model can also be utilized for longer term sst field prediction which may be explored in the future based on current results longer trends can be inferred as well 1 the lstm 1 feature model and lstm n features model will outperform the linear svr model due to that the lstm based models are good at modelling long time dependencies with their recurrent network structure and gating mechanisms 2 the proposed spatiotemporal deep learning model will consistently outperform the linear svr model the lstm 1 feature model and the lstm n features model due to its strong abilities of modelling spatiotemporal dependencies of sst 6 conclusions sst is a key physical parameter of the world ocean and plays an important role in the air sea interactions changes of sst can have profound effects on the marine ecosystem climate change and may even lead to extreme weather events such as tropical storms floods and droughts to achieve short and mid term sst field prediction accurately and conveniently a spatiotemporal deep learning model is proposed the model can capture both the spatial and temporal correlations of time series sst fields and make predictions in an end to end manner the 36 year daily sst time series data derived from the avhrr satellite sensors in a subarea of the east china sea are used to train and test the model thorough comparisons with the persistence model for the comparison of 1 day ahead prediction only the linear svr model the lstm 1 feature model and the lstm n features model using different statistics including rmse mape pearson correlation coefficient and kde from both the lead and holistic perspective demonstrates that the proposed spatiotemporal deep learning model consistently outperforms the other models for 1 10 days ahead prediction of sst fields besides the proposed model can directly predict the sst fields while the linear svr model and the lstm 1 feature model have to make predictions pixel by pixel which is inconvenient the lstm n features model can directly predict the sst fields but it cannot capture the spatial correlations of sst which makes it perform poorly in the sst field prediction task the results indicate that the proposed spatiotemporal deep learning model is promising in modelling the complex patterns of sst from historical observations based on which accurate predictions of short and mid term daily sst fields can be made by accurately predicting the daily resolution sst field using the proposed spatiotemporal deep learning model the short and mid term dynamics of fast changing sst can be caught it can then be used to detect and assess the variability of the marine ecosystems based on the surface thermal front and intensity shown on the sst field maps besides the predicted daily sst field can be incorporated into the hurricane models for anticipating tropical cyclone intensity esa and utilized to track fast events such as storms kuwano yoshida and minobe 2017 rss zhou et al 2015 though future sst fields are predicted based on the discovered spatiotemporal patterns of sst from historical sst time series data and good results have been achieved in the subarea of the east china sea some physical limitations still exist as we know the sst is closely coupled with the atmosphere ocean exchange of heat and momentum emery et al 2001 the future sst is affected not only by the past sst but also by other hydrological and meteorological conditions therefore the prediction performance is limited by just using the sst itself as the input prediction variable however the spatiotemporal deep learning model itself supports any number of input variables in the future if sufficient continuous and long time series of these hydrological and meteorological data can be obtained they can be easily incorporated into the model and better prediction performance may be expected although the spatiotemporal deep learning model is proposed to address the sst field prediction problem it can also be applied to the prediction of other meteorological environmental and atmospheric parameter fields such as wind soil moisture atmospheric pollutants and so on ma et al 2019 wang et al 2019 zang et al 2018 in the future high resolution prediction of large extents of sst fields e g ocean level can be investigated based on the proposed spatiotemporal deep learning model and with the help of cybergis and supercomputing wang and goodchild 2019 wright and wang 2011 declarations of interest none acknowledgment this research was supported by the national key research and development program no 2018yfb2100500 the national natural science foundation of china nsfc program no 41890822 and the creative research groups of natural science foundation of hubei province of china no 2016cfa003 the authors would like to thank the following data and tool providers noaa oar esrl psd for providing oisst v2 avhrr data https www esrl noaa gov psd google for providing the open source machine learning framework tensorflow https www tensorflow org keras https keras io for making it easier to use tensorflow to build the spatiotemporal deep learning model and scikit learn http scikit learn org stable for providing the machine learning library based on which the experiments are conducted 
26157,sea surface temperature sst is a vitally important parameter of the global ocean which can profoundly affect the climate and marine ecosystems to achieve an accurate and holistic prediction of the short and mid term sst field a spatiotemporal deep learning model is proposed which can capture the correlations of sst across both space and time the model uses the convolutional long short term memory convlstm as the building block and is trained in an end to end manner experiments using 36 year satellite derived sst data in a subarea of the east china sea demonstrate that the proposed model outperforms the persistence model the linear support vector regression svr model and two lstm models with different settings when judged using multiple statistics and from different perspectives the results suggest that the proposed model is highly promising for short and mid term daily sst field prediction accurately and conveniently keywords big data modelling spatiotemporal deep learning convlstm sea surface temperature sst field prediction time series satellite data 1 introduction and background sea surface temperature is one of the most important parameters of the world s ocean and plays a fundamental role in the exchange of energy momentum and moisture between the oceans and the atmosphere gentemann et al 2000 changes of sst can have profound effects on the global climate marine ecosystem and even vegetation bouali et al 2017 cane et al 1997 castro et al 2016 chaidez et al 2017 friedel 2012 herbert et al 2010 rauscher et al 2015 yao et al 2017 it can affect the precipitation distribution and further lead to droughts and floods rauscher et al 2015 salles et al 2016 daily sst which shows surface thermal front and intensity can be utilized to help detect marine ecosystems and assess the variability of such ecosystems and to improve the understanding and qualification of the vertical structure of the water mass and the internal wave propagation besides daily sst has been widely incorporated into hurricane models for anticipating tropical cyclone intensity esa therefore it s vitally important and necessary to predict the future dynamics of sst especially daily sst in order to help identify potential extreme weather events in advance sst prediction is an active research topic methods for sst prediction can be summarized into three categories including numerical methods data driven methods and the combination of these two the numeric methods are established based on physical chemical and biological parameters as well as the complex interactions among them aparna et al 2018 liu and fu 2018 stockdale et al 2006 it uses mathematical models to describe the variations of sst the numerical methods are usually quite sophisticated and predict sst at the ocean and even global scales with relatively low resolutions recent work has improved the sst and sst related variables forecast potential of the physical model namely nordic version of the nucleus for european modelling of the ocean nemo nordic in the baltic sea by assimilating high resolution sea surface temperature data into the model liu and fu 2018 the data driven methods attempt to solve the sst prediction problem from a data centric perspective they approximate the processes that generate the observed sst from patterns and relationships found in the observations the learned patterns are then used to infer future ssts the data driven methods require less ocean and atmospheric domain knowledge and are less sophisticated than the numerical methods besides they are suitable for high resolution sst predictions at relatively small scales methods in this category include traditional statistical methods such as the markov model yan and ants 2000 the popular machine learning approaches and artificial intelligent ones support vector machine svm and neural network are widely used machine learning methods for sst prediction aparna et al 2018 lins et al 2013 patil and deo 2017 tanggang et al 1998 wu et al 2006 zhang et al 2017 artificial intelligence methods for sst prediction include genetic algorithm ga and particle swarm optimization pso lins et al 2013 neetu et al 2011 to improve the prediction performance the numeric methods are combined with the machine learning methods patil et al 2016 in this paper we focus on the high resolution local sst field prediction in the east china sea as the numerical methods are too complex and involve the sophisticated interactions of a lot of variables the much simpler data driven methods are chosen however for traditional data driven methods most of them treat the observations in each location of an area independently without considering the influences of its surrounding locations which may reduce the prediction performance besides the prediction process is usually performed at each location individually which is inconvenient zhang et al 2017 therefore a prediction model that considers the spatiotemporal context and can predict the areal sst field holistically in an end to end manner is needed to this end the recently proposed recurrent neural network unit called convolutional long short term memory convlstm can be used due to its successful usage in similar applications such as precipitation prediction kim et al 2017 shi et al 2015 and video frame prediction sautermeister 2016 in these similar tasks deep learning models based on the convlstm unit which captures spatiotemporal correlations have achieved the state of the art prediction performance in this study we adopt the convlstm unit as the building block of our deep learning model to model the complex spatiotemporal dependencies of sst and further utilize the captured spatiotemporal dependencies for the sst field prediction task the goals of this paper are twofold 1 developing a spatiotemporal deep learning model that can model and capture both the spatial and temporal dependencies of sst and predict sst field accurately and holistically in an end to end manner 2 investigating the applicability effectiveness and advantages of the developed model in predicting short and mid term daily sst fields through experiments in a selected area in the east china sea using 36 year time series satellite data the remainder of the paper is organized as follows section 2 describes the study area and satellite derived time series sst data used in this study section 3 presents the proposed spatiotemporal deep learning model for sst field prediction the experimental results and thorough discussion are given in section 4 finally section 5 concludes the paper 2 study area and data 2 1 study area the study area locates in the east china sea the east china sea is part of the famous maritime silk road and one of the busiest seaways in the world understanding the patterns of sst and predicting future sst in this area is vitally important for safe seaway transportation as well as the production and lives of people in china and its surrounding countries as the east china sea is located in the subtropical zone the average annual water temperature is between 20 c and 24 c with the annual temperature difference between 7 c and 9 c the fluctuating characteristics of the sst in the east china sea makes it suitable to be used for testing the prediction performance of the proposed spatiotemporal deep learning model in this study we choose a subarea of the east china sea as our study area covering the spatial extent of 27 5 n 33 n 123 5 e 127 5 e as is shown in fig 1 the reason why we choose this subarea is that it is the rectangular area that covers majority of the east china sea without land pixels in it 2 2 data the data used is the daily optimum interpolation sea surface temperature daily oisst version 2 produced by the national oceanic and atmospheric administration noaa with a spatial resolution of 1 4 latitude by 1 4 longitude it s an analysis data using observations from multiple platforms including satellites ships and buoys on a regular global grid reynolds et al 2007 at each grid it contains a long time series of daily mean sst the gaps are filled through interpolation making it a spatially complete sst dataset the satellite sst derived from the advanced very high resolution radiometer avhrr is used to produce the adopted oisst which covers the global ocean from 89 975 s to 89 875 n 0 125 e to 359 875 e and covers temporal range from 1982 09 01 to present continuously the subset of the oisst for the study area contains 13 421 daily sst images from 1982 09 01 to 2018 05 30 each image is with the size of 22 16 pixels the original data are pre processed and transformed into a 5 dimensional 5 d tensor which is about 1 9 gigabytes gb before feeding the tensor for training and testing the proposed spatiotemporal deep learning model and the lstm models for comparison purpose the tensor is normalized to the range 0 1 for the linear svr model the tensor is normalized to the range 1 1 the normalization can make the data more centered which helps to train the models more easily and quickly and to further improve the performance of the models 3 formulation of the sst field prediction problem in our research the goal of sst field prediction is to forecast the next 10 days sst field using the past few days sst field fixed prediction window to achieve this goal we adopt a rolling prediction scheme in this scheme we first use the sst field in the prediction window to forecast the next 1 day s sst field then we move the time window 1 day ahead which incorporates the newly forecasted sst field as its latest element to predict the sst field one more day ahead by repeating this process 10 days ahead sst field prediction can be achieved therefore the key to achieving the goal is to predict the next 1 day s sst field suppose we observe the dynamic sst field over a spatial region which is represented by m n grid with m being the number of rows and n being the number of columns inside each cell of the m n grid there are v measurements as we only use sst as the predicting variable the value of v is 1 varying over time thus the observation at any time slice t can be represented by a 3 d tensor x t r m n v where r represents the domain of the observed sst in a period of time n we can obtain a spatiotemporal sequence of observations x t n 1 x t n 2 x t the 1 day ahead sst prediction problem is thus to forecast the most likely tensor in the future given the previous n days observations which can be formulated in equation 1 and illustrated in fig 2 1 x t 1 argmax x t 1 p x t 1 x t n 1 x t n 2 x t where p is a conditional probability and x t 1 is the predicted sst field at time t 1 4 the spatiotemporal deep learning model for sst field modelling and prediction 4 1 building block of the model from section 3 we can see that the sst field prediction involves the domains of both space and time to deal with it we adopt the convlstm as the building block of our model the convlstm was proposed to overcome the drawbacks of fully connected lstm fc lstm in handling spatiotemporal data during which the spatial correlations are lost shi et al 2015 different from fc lstm the convlstm determines the future state of a cell in the grid by the inputs and past states of its local neighbours during which both the spatial and temporal correlations are captured and utilized this is achieved by replacing the matrix multiplication operation used in fc lstm with convolution operation in the state to state and input to state transitions as shown in fig 3 the working mechanism of the convlstm including the gates input gate forget gate and output gate and information flow can be expressed using equation 2 7 with all the variables defined and explained in table 1 in the equations denotes the convolution operation and denotes the hadamard product œÉ is a sigmoid function used as the activation function applied to the weighted sum of the inputs of each gate the weighted sum of the inputs of each gate is a 3 d tensor and the œÉ is applied to each element of the 3 d tensor 2 f t œÉ w x f x t w h f h t 1 w c f c t 1 b f 3 i t œÉ w x i x t w hi h t 1 w c i c t 1 b i 4 c t tanh w x c x t w hc h t 1 b c 5 c t f t c t 1 i t c t 6 o t œÉ w x o x t w ho h t 1 w c o c t b o 7 h t o t tanh c t 4 2 structure of the model to better capture the spatial and temporal correlations among sst fields at different time we build a spatiotemporal deep learning model by stacking the convlstm layers as shown in fig 4 the model consists of four layers including an input layer two convlstm layers and a fully connected layer as the output layer our experiments reveal that this neural network design can achieve the best prediction accuracy among a couple of designs the input of the whole network is in the 5 d form expressed as samples timesteps width height features samples is the batch size for training empirically the batch size is set to a value around 128 by trials and errors around 128 we set it to 150 which can achieve the best prediction accuracy and relatively high training efficiency timesteps is the length of the time window of the input sst fields to predict the future sst field in our method we set it to 50 which is about 4 times of the prediction length 10 days according to the periodical changes of temperature data zhang et al 2017 meaning that we use the previous 50 days spatiotemporal sst sequence to predict the 51st day s sst field width and height are 22 and 16 respectively which are the spatial size of the input sst images as we only use one observation variable namely sst as the predictor features is set to 1 as there are no theoretical rules for determining the best number of convolutional kernels of a convolutional layer the usually adopted method is to test the performance of the neural network model by varying the number of kernels in a given range using this scheme we set the number of the convolutional kernels of both the two convlstm layers to 12 similarly we vary the size of convolutional kernels in a given range and finally find that the model can achieve best performance when the sizes of all convolutional kernels are set to 3 3 zero paddings are used to ensure that the states have the same spatial dimensions rows and columns as the inputs we use mean square error mse as the loss function and adam kingma and ba 2014 as the optimizer for training the model the whole model is conveniently trained in an end to end manner 5 experiments and discussion 5 1 experiment setup in this study we compare the proposed spatiotemporal deep learning model with three different models including lstm linear svr and persistence model only for the comparison of 1 day ahead predictions for the lstm model two different kinds of settings are compared one is that we treat each pixel in an sst field as an individual sample and it contains only one feature which is called lstm 1 feature model the other setting is that we treat all the pixels in an sst field as a sample and it contains 352 22 16 features which is called lstm n features model both these two settings consider only the temporal correlations except the second one is an end to end trainable model the linear svr model adopts the same setting as the lstm 1 feature model the persistence model simply uses the current sst field as the predictions of the next day to build the dataset for training and testing the models we first create the 5 d tensor as stated in section 2 2 the tensor is arranged in the order of time along the samples dimension then along the samples dimension of the 5 d tensor described in section 2 2 we select the first 80 as the training samples and the remaining 20 as held out testing samples further 5 of the training samples are split out for validation purpose during training namely the training and testing samples are not randomly chosen but chosen in the order of time the testing samples are samples in the future relative to the training samples the actual number of samples for training validation and testing are shown in table 2 all of the samples are with the dimension of 51 22 16 1 the proposed spatiotemporal deep learning model and two lstm models are implemented with keras using tensorflow 1 5 0 gpu version as the backend and trained and tested on a computer with a single nvidia titan xp gpu abadi et al 2016 chollet 2015 the linear svr is implemented using scikit learn 0 19 1 pedregosa et al 2011 during training of the proposed spatiotemporal deep learning model the loss function namely mse decreases quickly and converges to a small value on both the training dataset and validation dataset and the difference between the two converged values are small indicating that the model is trained to a proper degree besides the training results of the proposed model are relatively stable with respect to repeated training though they are different in the first few epochs for repeated training due to the random initialization of the model they then converge and become stable the difference among the converged values for repeated training are small as well three indexes are selected to measure the performance of the different models for sst field prediction including the root mean square error rmse the mean absolute percentage error mape and the pearson correlation coefficient r which are defined as follows 8 r m s e 1 n d i 2 n 9 m a p e 1 n 1 n y i y i y i 100 10 r 1 n y i y y i y 1 n y i y 2 1 n y i y 2 where d i is the error vector calculated by the difference between the reference sst value y i and the predicted sst value y i y and y are the mean value of the reference sst values and the predicted sst values respectively n is the total number of testing samples 5 2 experiment results and discussion we conduct experiments for 1 10 days ahead predictions and compare the prediction performance of different models using different statistics and from different perspectives specifically for the 1 day ahead prediction persistence model is incorporated for comparison fig 5 shows the spatial distribution of rmses over the study area for 1 day ahead sst field prediction using different models it can be seen that the proposed model performs better than the linear svr lstm 1 feature model and the persistence model and much better than the lstm n features model it can also be seen that all the five models have their own largest prediction errors at nearly the same locations this may be due to the abrupt changes of sst at these locations as shown in fig 6 however the prediction capability of the proposed model is less affected by this condition fig 7 shows the rmse of predictions for the lead time of 1 10 days by different models for each prediction lead the rmse is calculated over the whole study area on all the testing samples the figure shows that the proposed model consistently outperforms all the other three models which means that it can better capture the patterns hidden in historical sst fields and make more accurate predictions based on the captured patterns the improvement of the rmse of sst field predictions by the proposed model relative to the three models is given in table 3 for linear svr and lstm 1 feature model the improvement increases along with the lead time of prediction while for the lstm n features model the improvement decreases from the lead time of 1 7 days and then increases one interesting finding from fig 6 is that the linear svr model outperforms the lstm 1 feature model for the lead time of 1 4 days but it performs worse than the lstm 1 feature model from the lead time of 5 days a similar phenomenon exists between the linear svr model and the lstm n features model the reason may be that lstm based models are good at modelling long time dependencies of sst with their recurrent network structure and gating mechanisms fig 8 and table 4 show the comparison of mape of 1 10 days ahead prediction of sst field by different models and the percentage improvement of mape by the proposed model relative to the other three models respectively they show similar patterns as those of the rmse in fig 7 and table 3 in fig 9 the pearson correlation coefficients r between the predicted sst field and the reference sst field for each prediction lead on all the testing samples are compared the corresponding p values for the calculated r are all 0 showing the statistical significance of the calculated r it shows that the r values of the proposed model are quite close to 1 and consistently bigger than those of the other three models showing a stronger positive linear correlation between the predicted sst and the reference sst besides the r values of the proposed model decrease more slowly than those of the other three models as the prediction lead increases from 1 to 10 days the r values of the linear svr model shows almost a straight downward trend as the prediction lead increases fig 10 compares the prediction performance of different models from the perspective of the spatial distribution of rmses of prediction over the study area it shows that for each prediction lead the rmses of the proposed model are lower than those of the other three models and are more evenly distributed over the study area the rmses of the linear svr model lstm 1 feature model and lstm n features model increase quickly at locations where sst changes quickly as shown in fig 6 as the prediction lead increases in contrast the proposed model has consistently and stably better performance at these locations to investigate the prediction performance from a holistic perspective we compare the distribution of the prediction errors of 1 10 days ahead as a whole it is achieved by conducting the kernel density estimation kde of the prediction errors of different models using the gaussian kernel terrell and scott 1992 the kde curves of prediction errors of different models are shown in fig 11 the figure reveals that the proposed model has a much denser kde curve with its mean almost equivalent to 0 showing that the errors of the proposed model spread more densely around 0 than the other three models do table 5 is also from a holistic perspective and shows the performance statistics calculated on all the 1 10 days ahead sst field prediction of each model which demonstrates the outperforming prediction capability of the proposed model as well besides the lstm 1 feature model predicts better than the linear svr model considering long prediction leads holistically which is in line with what has been previously observed from fig 7 from the above results using different statistics and from different perspectives we can see that the proposed model outperforms the liner svr model the lstm 1 feature model and the lstm n features model for the sst field prediction task the reason is that the proposed model has both spatial modelling capability by using convolutional operation and temporal modelling capability by using recurrent network structure and gating mechanisms it determines the future state of a cell in the grid by the inputs and past states of its local neighbours during which both the spatial and temporal correlations of sst are captured and utilized which however is not inherently implemented in other models as we aim to predict the short and mid term daily sst field the experiments only investigate the prediction performance of different models for 1 10 days ahead however the model can also be utilized for longer term sst field prediction which may be explored in the future based on current results longer trends can be inferred as well 1 the lstm 1 feature model and lstm n features model will outperform the linear svr model due to that the lstm based models are good at modelling long time dependencies with their recurrent network structure and gating mechanisms 2 the proposed spatiotemporal deep learning model will consistently outperform the linear svr model the lstm 1 feature model and the lstm n features model due to its strong abilities of modelling spatiotemporal dependencies of sst 6 conclusions sst is a key physical parameter of the world ocean and plays an important role in the air sea interactions changes of sst can have profound effects on the marine ecosystem climate change and may even lead to extreme weather events such as tropical storms floods and droughts to achieve short and mid term sst field prediction accurately and conveniently a spatiotemporal deep learning model is proposed the model can capture both the spatial and temporal correlations of time series sst fields and make predictions in an end to end manner the 36 year daily sst time series data derived from the avhrr satellite sensors in a subarea of the east china sea are used to train and test the model thorough comparisons with the persistence model for the comparison of 1 day ahead prediction only the linear svr model the lstm 1 feature model and the lstm n features model using different statistics including rmse mape pearson correlation coefficient and kde from both the lead and holistic perspective demonstrates that the proposed spatiotemporal deep learning model consistently outperforms the other models for 1 10 days ahead prediction of sst fields besides the proposed model can directly predict the sst fields while the linear svr model and the lstm 1 feature model have to make predictions pixel by pixel which is inconvenient the lstm n features model can directly predict the sst fields but it cannot capture the spatial correlations of sst which makes it perform poorly in the sst field prediction task the results indicate that the proposed spatiotemporal deep learning model is promising in modelling the complex patterns of sst from historical observations based on which accurate predictions of short and mid term daily sst fields can be made by accurately predicting the daily resolution sst field using the proposed spatiotemporal deep learning model the short and mid term dynamics of fast changing sst can be caught it can then be used to detect and assess the variability of the marine ecosystems based on the surface thermal front and intensity shown on the sst field maps besides the predicted daily sst field can be incorporated into the hurricane models for anticipating tropical cyclone intensity esa and utilized to track fast events such as storms kuwano yoshida and minobe 2017 rss zhou et al 2015 though future sst fields are predicted based on the discovered spatiotemporal patterns of sst from historical sst time series data and good results have been achieved in the subarea of the east china sea some physical limitations still exist as we know the sst is closely coupled with the atmosphere ocean exchange of heat and momentum emery et al 2001 the future sst is affected not only by the past sst but also by other hydrological and meteorological conditions therefore the prediction performance is limited by just using the sst itself as the input prediction variable however the spatiotemporal deep learning model itself supports any number of input variables in the future if sufficient continuous and long time series of these hydrological and meteorological data can be obtained they can be easily incorporated into the model and better prediction performance may be expected although the spatiotemporal deep learning model is proposed to address the sst field prediction problem it can also be applied to the prediction of other meteorological environmental and atmospheric parameter fields such as wind soil moisture atmospheric pollutants and so on ma et al 2019 wang et al 2019 zang et al 2018 in the future high resolution prediction of large extents of sst fields e g ocean level can be investigated based on the proposed spatiotemporal deep learning model and with the help of cybergis and supercomputing wang and goodchild 2019 wright and wang 2011 declarations of interest none acknowledgment this research was supported by the national key research and development program no 2018yfb2100500 the national natural science foundation of china nsfc program no 41890822 and the creative research groups of natural science foundation of hubei province of china no 2016cfa003 the authors would like to thank the following data and tool providers noaa oar esrl psd for providing oisst v2 avhrr data https www esrl noaa gov psd google for providing the open source machine learning framework tensorflow https www tensorflow org keras https keras io for making it easier to use tensorflow to build the spatiotemporal deep learning model and scikit learn http scikit learn org stable for providing the machine learning library based on which the experiments are conducted 
26158,available water capacity awc is a fundamental factor in energy water nexus particularly awc influences climate dynamics hydrological processes and water management measuring awc is often laborious time consuming and sometimes impossible pedotransfer functions ptfs are therefore used as alternatives to model awc classical statistical based pedotransfer functions are widely used to model awc however these classical statistical models do not incorporate prior knowledge of the existing awc data therefore the objective of this study is two fold firstly to develop awc ptfs for topsoil using five physical soil properties and secondly to evaluate relative importance of the physical soil properties in the bayesian based awc model results show that bayesian ptfs plausibly simulate awc with percent bias pbias of 1 root mean square error rmse of 0 021 to 0 027 cm cm and degree of agreement d1 of 0 6 0 79 silt had the greatest influence on the awc both inside and outside bayesian framework keywords available water capacity awc bayesian based pedotransfer functions bptfs physical soil properties relative importance score ogallala aquifer 1 software and or data availability the following is a summary of the source code for developing pedotransfer functions ptfs of soil available water capacity awc in a bayesian framework the code can be freely downloaded from https github com kachiengz bptfs name of codebayesian based pedotransfer functions bptfs developerkevin o achieng contact addressuniversity of wyoming department of civil and architectural engineering 1000 e university ave laramie wy 82071 united states telephone number 1 3o7 761 3oo7 e mail kachieng uwyo edu year first available2019 hardwarelaptop windows mac linux softwarer programming language version 3 0 1 bms biomod2 program languager program size 103 mb 2 introduction knowledge of soil awc is important in carrying out robust water management hydrological processes like surface runoff and infiltration of rain and or irrigation water depend on soil awc surface runoff occurs if the soil is saturated beyond its field capacity conversely infiltration occurs as long as the soil water content has not reached saturated level since the soil awc represents the amount of soil water that can be extracted by plants the awc allows for selection of suitable irrigation scheduling and management practices allen et al 1998 and forecasting of yields of crops de paepe bono et al 2018 soil awc also dictates exchange of water and energy between the root zone and the atmosphere particularly with respect to evapotranspiration and storage of rain irrigation water based on soil awc extreme events like drought risk can be forecasted and mitigated in a timely manner jones and montanarella 2000 knowledge of soil awc also allows for monitoring of plant and animal response to stress imposed by extreme climate conditions such as droughts and floods soil water is held between soil particles at different suction depending on soil texture the suction a negative pressure also known as matric potential at which thoroughly saturated soil stops to drain freely is the measure of soil water content which is called field capacity fc on the other hand the soil water content associated with the largest suction beyond which soil water is not extractable by plant roots and thus causing withering of the plant is called permanent wilting point pwp fc and pwp are usually pegged at 30kpa and 1500kpa respectively assouline and or 2014 veihmeyer and hendrickson 1931 awc is widely computed as a function of soil depth root zone thickness and the difference between soil water content at the fc and the pwp both fc and pwp are dependent on soil texture and so is awc to measure fc soil is thoroughly saturated and allowed to drain under gravity for 48 hrs until the drainage stops before measurement of volumetric water content is taken similarly permanent wilting point is measured by measuring volumetric water content of a dry soil from which root water uptake is impossible usually at the onset of wilting in a plant based on measurement of the fc and pwp awc can be determined there are a number of bottlenecks that derail the process of measuring awc 1 awc often requires direct measurement of soil water potential and the corresponding soil water content a task that is often tedious costly and sometimes impossible a haghverdi et al 2014 w√∂sten et al 2001 in order to establish a soil water retention curve from which fc and pwp are extracted 2 the awc depends on the fc and pwp values which in turn depend on soil texture land cover and climate factors that are mainly uncertain silva et al 2014 and 3 precise measurement of soil water content at low suction in order to get fc has been particularly found to be a tall order khlosi et al 2008 vereecken et al 2010 in general the process of taking measurements of both fc and pwp can be time consuming laborious costly and sometimes impossible jones et al 2000 rom√°n dobarco et al 2019 as a result not many studies have used field measurements of soil fc and pwp to map soil awc w√∂sten et al 2001 in the light of above challenges awc is often determined from soil physical properties that are easy to measure and whose data are often readily available from most global national and or state soil survey databases the mapping function that maps the easily measurable soil physical properties e g soil texture organic matter content and bulk density to the more difficult to measure soil physical property awc in this case is called a pedotransfer function saxton and rawls 2006 schaap and bouten 1996 a pedotransfer function can simply be interpreted as without loss of generality a classical statistical regression model that maps easily measurable soil hydraulic variables e g soil texture onto useful but difficult to measure soil hydraulic variable like awc soil physical properties are the primary variables that are used in most awc pedotransfer models these pedotransfer models are often developed based on classical statistics specifically the awc is regressed on the explanatory variables like the soil physical properties in a multi variate regression or machine learning paradigm rom√°n dobarco et al 2019 rubio et al 2008 saxton et al 2006 examples of widely used classical geostatistical pedotransfer functions include geographically weighted regression regression kriging kriging co kriging and artificial neural networks amir haghverdi et al 2015 these classical geostatistical pedotransfer functions have been used to map soil awc at various spatial scales not only in the usa kern 1995 mikhailova et al 2018 zheng et al 1996 but also in other regions like scotland poggio et al 2010 new south wales australia malone et al 2009 and france cazemier et al 2001 and at global scales reynolds et al 2000 studies conducted in the h√©rault orb libron valley region languedoc france determined awc as a function of soil texture and bulk density lagacherie et al 2000 soil awc of australian soils were modeled by regressing measured awc onto soil texture bulk density and organic matter minasny et al 1999 in south india soil awc has been estimated as a function of soil moisture and bulk density sreelash et al 2017 in argentina soil awc was simulated using linear regression and artificial neural network by using field measured input variables soil texture bulk density organic matter and soil water content de paepe angel bono et al 2018 a bayesian based ensemble kalman filter method has been used to develop relatively high performing ptf for saturated soil hydraulic conductivity pan et al 2011 other studies have coupled bayesian frameworks with artificial neural network to develop ptf to better simulate soil water retention across a wide range of spatial scales jana et al 2007 reasonable prediction of soil water content has been obtained when predictions from 19 ptfs are averaged within a bayesian framework guber et al 2009 bayesian analysis has also been found to be robust in estimating soil hydraulic and transport properties in a study that involved measuring pressure head and solute concentration based on a unsaturated soil column experimental setup moreira et al 2016 in a vadose zone study conducted in new zealand forecasting of soil pressure head from a tensiometer based measured pressure head data and bayesian based multiobjective optimization was found to produce better predictions w√∂hling and vrugt 2008 than use of a single model a bayesian based empirical model chiu et al 2012 that was used to estimate the hydraulic parameters of the van genuchten based water retention curve based on 90 soil samples that were extracted from unsaturated soil hydraulic database unsoda leij 1996 was found to produce better parameter prediction for sandy soil than for finer soils in a study conducted in an agricultural field near julich germany that involved measuring soil water content using time domain reflectometer tdr incorporating prior knowledge of the probability distribution of the soil hydraulic properties was found to improve predictive capability of the bayesian inverse modelling of in the situ soil water dynamics scharnagl et al 2011 in a soil erosion study conducted to account for uncertainty in the soil sediment prediction wang et al 2015 bayesian analysis was used in averaging soil sediments produced by four soil erosion models that are built in the agricultural policy environmental extender apex software results of this study showed that averaging the sediments predicted from the four erosion models in a bayesian framework resulted in a more robust prediction of the sediment and the associated model uncertainty even though there have been many vadose zone based hydrological studies that have used bayesian analysis to simulate hydrological processes there are limited studies if any that have developed bayesian pedotransfer function in a simple to use software language such as r to simulate the soil awc classical statistical regression models have difficulty in capturing the relative importance of the individual explanatory variables in the awc model besides prior knowledge of probability distribution of soil awc is not incorporated in the pedotransfer functions when classical statistics are used to model soil awc therefore the objective of this study is to 1 develop a new pedotransfer model in a bayesian framework based on measurements of awc and five physical soil properties 2 present the relative importance of the explanatory variables in the awc pedotransfer models 3 methods 3 1 study area and data this study focuses on topsoils in the ogallala aquifer this aquifer transverses eight states of the us and it is the largest aquifer in north america covering an area of 451000 km2 the aquifer supports about 30 of us irrigated agriculture 97 of irrigation water supply within the aquifer brown and pervez 2014 and fresh drinking water to about 80 of people living within its vicinity dennehy 2000 since planning of irrigation across this area requires soil awc reliable measurement and plausible modelling of awc are both important the study area shown in fig 1 was subdivided into 6 regions two regions in the northern part two in the central part and two regions in the southern portion of the study area the model dataset i e actual awc and physical soil properties for ogallala aquifer topsoils was obtained from the us natural resources conservation service nrcs s soil survey geographic ssurgo database soil survey staff 2016 this database is hosted by and freely available at the geospatial data gateway http datagateway nrcs usda gov the topsoil data of the physical soil characteristics obtained from ssurgo database include clay sand silt bulk density g cm3 and organic matter a total of 6000 samples were retrieved from the database each of the six regions had about 1000 topsoil samples for model calibration an additional 333 samples from outside the six regions but within the study area was used for testing the awc pedotransfer models 3 2 calibration of the bayesian based ptfs the bayesian based pedotransfer model of the topsoil s awc is developed by regressing observed awc onto the five regressors physical soil properties the total number of regression models which consists of all possible combinations of regressors that awc can be regressed onto is 32 i e 2 5 32 these regression models are assumed to be normally distributed within the regression model space any regression model that contains kj regressors 0 kj k is expressed as 1 y o i n Œº z j Œ≤ j œÉ Œµ where Œº is the intercept i n is an n dimensional vector of 1 s œÉ is the residual standard deviation hence œÉ 2 is the residual variance Œµ is the residual error that results from fitting all regressors to the independent variable which is assumed to be independent and normally distributed with mean of zero and a variance of œÉ Œ≤ j is a kj dimensional vector of relevant regression coefficients such that Œ≤ j 0 for k kj regressors that are not included in regression model mj z j is an n x kj matrix that consists of columns of kj regressors x 1 x 2 x k j with each regressor having n observations and y o is the awc the bayesian framework unlike classical statistics incorporates the prior knowledge of the data i e the likelihood of the measured awc prior probabilities of regression models mpriors and the prior probabilities of the regression parameters Œº œÉ Œ≤ j otherwise known as the g priors the priors and the likelihood are used to compute the posterior model probability pmp of the regression model parameters based on bayes theorem as shown 2 p m j y o p y o m j p m j p y o l m j y o p m j p y o l m j y o p m j h 1 2 k p y o m h p m h where l m j y o is the likelihood of regression model m j p y o or h 1 2 k p y o m h p m h is the normalizing constant of the posterior model probability p m j is the prior model probability of regression model m j notice that the three priors that are specified are the prior probabilities of regression models mpriors p m j the joint prior probabilities of the intercept and residual variance of the regression models p Œº œÉ 2 and the prior probabilities of the regressors coefficients Œ≤ j of regression models g priors Œ≤ j p the mprior and the g prior used in this study are discrete uniform distribution ley and steel 2012 liang et al 2008 and empirical bayes g local ebl cui and george 2004 george and foster 2000 g prior respectively the uniform mprior is formulated by assuming that all the regressors are independent ley et al 2012 liang et al 2008 and is expressed by the following equations 3 p m j 1 2 j 1 2 2 k 14 and the ebl g prior zellner 1986 is determined based on the following equation 4 g z j t z j 1 Œ≤ j 0 œÉ 2 p Œ≤ j Œº œÉ 2 m j f n k j where f n q m v is the density function of a q dimensional normal distribution with mean m and covariance matrix v g is the scalar quantity that ranges from 0 to 1 the ebl estimates the value of g as a maximum likelihood that only comprises non negative values and is expressed as 5 g m a x f j 1 0 the f j is the standard f statistics for testing whether the regression coefficient of regression model mj is zero or not i e the null hypothesis ho Œ≤ j 0 verses alternative hypothesis ha Œ≤ j 0 6 f j r j 2 k j 1 r j 2 n 1 k j where r j 2 is the coefficient of determination of regression model mj k j is the number of regressors contained in regression model mj n is the number of observations of the dependent variable awc before computing the likelihood the joint prior probabilities of the intercept and residual variance of the regression models p Œº œÉ 2 is determined based on the widely used improper non informative formulation fern√°ndez et al 2002 raftery 1996 7 p Œº œÉ 2 m j 1 œÉ 2 the likelihood of regression model m j is expressed as shown fern√°ndez et al 2001b 2001a 8 l m j y o p y o 1 y o 2 y o n m j p y o Œº Œ≤ j œÉ 2 m j p Œº œÉ 2 p Œ≤ j Œº œÉ 2 m j d Œº d œÉ 2 d Œ≤ j where p y o Œº Œ≤ j œÉ 2 m j is the probability of the data given the regression model parameters Œº Œ≤ j œÉ 2 of model m j the regression coefficients and intercept are weighted with their corresponding pmp to get average weighted values of these regression model parameters for example the pmp weighted regression coefficient of a given regressor e Œ≤ m j z over the regression model space is computed as 9 e Œ≤ m j z j 1 2 k e Œ≤ j y o m j z p m j y o where e Œ≤ j y o m j z is the estimate of regression coefficient Œ≤ j from classical statistical ordinary least square regression the regression model that contains all the regressors whose coefficients and intercept have been weighted with pmp is the bayesian based model that is used to simulate awc once the awc bayesian model is obtained the next task is to figure out the relative importance of the regressors to do this we determine a posterior inclusion pip of each regressor the pip is a measure of the relative importance of the regressor within the bayesian framework for any given regressor the pip is calculated by summing up pmp of all regression models within the regression model space that contain that regressor since the total pmp of all the regression models is 1 the maximum pip value of any regressor is 1 10 p i p p Œ≤ j 0 y o Œ≤ j 0 p m j y o 3 3 model performance analysis based on statistical performance indices in order to determine how well the bayesian based pedotransfer awc model simulates awc statistical performance indices were used these indices used included the widely used model performance indices such as the root mean square error rmse legates and mccabe 1999 the percent bias error pbias gupta et al 1998 and willmott s index of agreement d 1 willmott 1982 willmott et al 1985 the rmse describes the goodness of fit between the actual and simulated data and it is expressed as 11 r m s e 1 p i 1 p y i o i 2 where y i is the modelled awc o i is the measured awc and p is the number of samples the pbias represents the bias between the modeled and the measured awc h v gupta et al 1998 negative and positive pbia represent underestimation and overestimation respectively of the awc pbias values near zero signify that the ptf has less bias mathematically pbias is expressed as 12 p b i a s i 1 p y i o i i 1 p o i 100 index of agreement d 1 willmott 1984 willmott et al 2012 describes the degree with which simulated awc agrees with observed awc the absolute error form of d1 ranges between 0 no fit agreement and 1 perfect fit agreement and is expressed as absolute deviation based form of index of agreement is computed as 13 d 1 1 i 1 p o i y i i 1 p o i Œº o y i Œº o where Œº o is the mean of the measured awc 4 results and discussion 4 1 awc and its explanatory variables the study area from which the soil data was obtained and for which the bayesian based ptfs was developed is shown in fig 1 a summary of the physical soil properties can be found in fig 2 the violin plots shown in fig 2 portray both distribution and the statistics with respect to percentiles i e 25th 50th 75th minimum and maximum of the physical soil properties across the six regions within the study area accordingly the awc seems to be decreasing southwards from region1 to region6 as shown in fig 2 a the topsoil sand and silt increase southwards and northwards respectively as shown in fig 2 b and c respectively clay bulk density and organic matter across the six regions have a mean of 15 20 1 25 1 5 g cm3 and 1 2 respectively as shown in fig 2 d e and f respectively the two regions in the north of the study area region1 and region2 have some spots with organic matter of as high as 80 this can be justified by the relatively greater agricultural activities mainly irrigated corn and soybean in the north than in the south of the study area fig 3 represents the correlation between the modelled and the measured awc under the calibration phase 4 2 bayesian based ptfs in this study eight widely used classical ptfs aina and periaswamy 1985 contreras and bonilla 2018 s c gupta and larson 1979 rawls et al 1983 1982 are developed in a bayesian framework using five physical soil properties for the topsoil awc the bayesian based ptfs are tabulated as shown in table 1 moist bulk density has the largest negative regression coefficient of 0 02 0 1 whereas silt has the strongest positive regression slope of 0 001 0 005 across all the ptfs as shown in table 1 fig 4 provides the validation phase based correlation between modelled and measured awc the performance of calibrated bayesian based ptfs which was evaluated using the three widely used performance indices root mean square error rmse willmott s index of agreement d1 and percent bias error pbias are shown in fig 5 and table 2 the calibrated ptfs have pbias values of less than 1 across the six regions as shown in fig 5 with exception of cla om ptf the d1 varied between acceptable values of 0 6 and 0 79 and rmse of 0 021 0 036 cm cm across the regions ptfs as shown in table 2 the relatively low d1 of 0 39 0 61 of clay om ptf can be attributed to the fact that the distribution of clay and om have weak correlation with that of the awc as shown the violinplot in fig 2 but these d1 values still show moderate agreement of the clay om ptfs with the measured awc validation of the trained ptfs was conducted using an independent dataset outside the regions selected but within the study area ogallala aquifer validation results are summarized in table 3 and in fig 6 similar to the calibration phase the ptfs in the validation phase have reasonable performance with d1 of 0 6 0 67 as summarized in table 3 even though the validation ptfs have relatively larger errors compared to their counterpart calibrated ptfs as evident in the validation pbias of 10 to 10 which is shown in fig 6 and rmse of 0 028 0 062 cm cm they still have reasonable simulation of the awc since these errors are within tolerable limits the northern regions ptfs seem to underestimate awc while the southern regions ptfs overestimate awc especially in the validation phase as shown in fig 6 on the other hand the central regions ptfs neither consistently overestimate nor underestimate awc sand exhibited the weakest regression slope among the physical soil properties therefore soil bulk density and silt seem to be the most dominating factors that affect awc ptfs in both the north and south areas of the ogallala aquifer 4 3 sensitivity of bayesian based ptf to physical soil properties within a bayesian framework the sensitivity of the explanatory variables was evaluated based on leave one out procedure note that only the ptf model with all the five physical properties was used in order to evaluate sensitivity of awc to these properties within the bayesian framework leaving out bulk density from the ptf model resulted in the greatest change in modelled awc nearly across all the six regions as shown in fig 7 a the overall change of modelled awc when bulk density was left out was 20 to 100 as shown as shown in fig 7 a however this change was not uniform throughout the study area the change in modelled awc increased southwards with region1 having the least increase 20 and region6 having the greatest increase in modelled awc 100 leaving out organic matter from the ptf model led to the least change in modelled awc however the change in modelled awc was negative with the northern regions having the least decline of modelled awc 1 and the southern regions having the greatest decline in modelled awc of up to 5 as shown in fig 7 b texture wise among the three textural classes leaving out silt led to the greatest change in modelled awc this change was mostly negative with decline of as high as 80 in region3 as shown in fig 7 c therefore within bayesian framework the topsoil awc ptfs in the ogallala aquifer are mainly sensitive to bulk density and silt 4 4 relative importance of physical soil properties to bayesian based ptf outside the bayesian framework the relative importance of the physical soil properties on the awc ptf were analyzed based on their correlation with the awc the classical pearson s correlation coefficient between individual explanatory variables and the soil awc was computed this was done to analyze the strength of correlation between the physical soil properties and the soil awc results are summarized as shown in table 4 the soil properties with the greatest positive and negative correlation coefficients are silt and sand respectively as shown in table 4 this further emphasizes the role of soil texture and particularly silt in the awc ptf model in the ogallala aquifer however the strength of correlation is slightly weaker in the southern region than northern region of the study area organic matter and bulk density have positive and negative correlation coefficients respectively as shown in table 4 the second method used to analyze correlation of the explanatory variables involved shuffling a single explanatory variable and then predicting the awc using the shuffled variable once the model prediction is obtained a pearson s correlation coefficient is computed between the shuffled based prediction and the measured awc the relative importance score of the explanatory variable in question is computed as 1 minus the correlation coefficient the relative importance ranges from 0 zero influence of the explanatory variable on awc to 1 the strongest influence of the explanatory variable on awc this analysis was implemented using biomod2 package thuiller et al 2016 in r programming language version 3 0 1 the results suggest that sand has the strongest influence on awc with a score of 1 in the southern region of the study area and silt is mostly strongly influential in the north and central regions with a score of 1 as shown in table 5 clay has a strong influence with a score of 1 in the central region moderate influence on awc with a score of 0 35 in the southern and very weak influence with a score of 0 01 in the north both organic matter and bulk density seem to have very weak influence on awc with a score of 0 0 09 across the study area within the bayesian framework the relative importance of the physical soil properties on awc ptf was analyzed using the posterior inclusion probability pip of these explanatory variables pip has been successfully used in other studies to evaluate relative importance of explanatory variables in the bayesian framework achieng and zhu 2019 eicher et al 2007 fern√°ndez et al 2001b 2001a rodriguez iturbe et al 2001 overall the most influential physical soil property on the awc ptf across all the regions is soil bulk density with a pip of 0 94 1 as shown in fig 8 soil texture wise silt seem to have the greatest influence on awc with a pip of 0 86 0 92 as shown in fig 8 with exception of the south region i e region6 which has a pip value of 0 46 both clay and sand have pip of 0 92 and 0 90 respectively as shown in fig 8 therefore even though clay and silt have weak influence on awc in most regions they play a significant role in fitting ptfs in the south region within the bayesian framework organic matter doesn t seem to have a clear spatial trend with respect to its relative importance in bayesian based ptf this can be seen by its very strong influence with pip of 0 91 1 0 in regions1 2 4 and 5 and its weak influence as seen in regions3 pip value of 0 11 and region6 pip value of 0 05 the main advantages of the newly developed bptfs include 1 it allows for evaluation of the relative importance of each of the explanatory variable physical soil properties to the awc model based on the posterior inclusion probability pip 2 bptfs allows for incorporation of the prior knowledge of the physical soil properties this is achieved by use of prior probability of the regression parameters i e the mean the variance and the regression coefficient and the prior probability of the bayesian regression models 3 bptfs considers all the possible regression models 2 5 32 while developing the ptf whereas the classical regression analysis only considers one regression model use of multiple regression models in bayesian model averaging has been found to lead to superior prediction than use of a single model as is in the case of classical statistical regression madigan and raftery 1994 use of a single model as is the case in classical statistical regression often leads to statistical bias w√∂hling et al 2008 and 4 bptfs is computationally fast e g for this study compute time is less than 5 min with the 6000 data samples and it is easy to use since it only requires basic knowledge of the r language the only disadvantage is that unlike the classical regression which can be easily analyzed on e g ms excel the btfs program require use of r software therefore basic knowledge of r is recommended 5 conclusions bayesian based awc pedotransfer functions were developed for the ogallala aquifer topsoil which supports most irrigated agriculture especially in the western usa this study is an improvement over the commonly used classical statistical regression analysis to develop ptfs this is because of the fact that the bayesian framework allows for incorporation of prior knowledge of the distribution and statistics of the physical soil parameters into the ptfs since the study area lies within nebraska s sandhills the majority of the ogallala aquifer is dominated with silt especially in the northern region and sand southwards the following is a summary of the key findings of this study out of the three textural classes silt has been found to play the most influential awc ptfs within and outside bayesian framework between the two non textural soil properties the bulk density had the greatest influence on awc especially within the bayesian framework sand has a relatively weaker influence on awc ptfs even though the majority of the study area lies in the nebraska sandhill region the bayesian based ptfs are region dependent the influence of sand and organic matter on awc ptfs are also are region dependent acknowledgements special thank you goes to the paul a rechard fellowship of the university of wyoming for financial support the anonymous reviewers and the editor have provided valuable comments throughout the review process of the original manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104500 
26158,available water capacity awc is a fundamental factor in energy water nexus particularly awc influences climate dynamics hydrological processes and water management measuring awc is often laborious time consuming and sometimes impossible pedotransfer functions ptfs are therefore used as alternatives to model awc classical statistical based pedotransfer functions are widely used to model awc however these classical statistical models do not incorporate prior knowledge of the existing awc data therefore the objective of this study is two fold firstly to develop awc ptfs for topsoil using five physical soil properties and secondly to evaluate relative importance of the physical soil properties in the bayesian based awc model results show that bayesian ptfs plausibly simulate awc with percent bias pbias of 1 root mean square error rmse of 0 021 to 0 027 cm cm and degree of agreement d1 of 0 6 0 79 silt had the greatest influence on the awc both inside and outside bayesian framework keywords available water capacity awc bayesian based pedotransfer functions bptfs physical soil properties relative importance score ogallala aquifer 1 software and or data availability the following is a summary of the source code for developing pedotransfer functions ptfs of soil available water capacity awc in a bayesian framework the code can be freely downloaded from https github com kachiengz bptfs name of codebayesian based pedotransfer functions bptfs developerkevin o achieng contact addressuniversity of wyoming department of civil and architectural engineering 1000 e university ave laramie wy 82071 united states telephone number 1 3o7 761 3oo7 e mail kachieng uwyo edu year first available2019 hardwarelaptop windows mac linux softwarer programming language version 3 0 1 bms biomod2 program languager program size 103 mb 2 introduction knowledge of soil awc is important in carrying out robust water management hydrological processes like surface runoff and infiltration of rain and or irrigation water depend on soil awc surface runoff occurs if the soil is saturated beyond its field capacity conversely infiltration occurs as long as the soil water content has not reached saturated level since the soil awc represents the amount of soil water that can be extracted by plants the awc allows for selection of suitable irrigation scheduling and management practices allen et al 1998 and forecasting of yields of crops de paepe bono et al 2018 soil awc also dictates exchange of water and energy between the root zone and the atmosphere particularly with respect to evapotranspiration and storage of rain irrigation water based on soil awc extreme events like drought risk can be forecasted and mitigated in a timely manner jones and montanarella 2000 knowledge of soil awc also allows for monitoring of plant and animal response to stress imposed by extreme climate conditions such as droughts and floods soil water is held between soil particles at different suction depending on soil texture the suction a negative pressure also known as matric potential at which thoroughly saturated soil stops to drain freely is the measure of soil water content which is called field capacity fc on the other hand the soil water content associated with the largest suction beyond which soil water is not extractable by plant roots and thus causing withering of the plant is called permanent wilting point pwp fc and pwp are usually pegged at 30kpa and 1500kpa respectively assouline and or 2014 veihmeyer and hendrickson 1931 awc is widely computed as a function of soil depth root zone thickness and the difference between soil water content at the fc and the pwp both fc and pwp are dependent on soil texture and so is awc to measure fc soil is thoroughly saturated and allowed to drain under gravity for 48 hrs until the drainage stops before measurement of volumetric water content is taken similarly permanent wilting point is measured by measuring volumetric water content of a dry soil from which root water uptake is impossible usually at the onset of wilting in a plant based on measurement of the fc and pwp awc can be determined there are a number of bottlenecks that derail the process of measuring awc 1 awc often requires direct measurement of soil water potential and the corresponding soil water content a task that is often tedious costly and sometimes impossible a haghverdi et al 2014 w√∂sten et al 2001 in order to establish a soil water retention curve from which fc and pwp are extracted 2 the awc depends on the fc and pwp values which in turn depend on soil texture land cover and climate factors that are mainly uncertain silva et al 2014 and 3 precise measurement of soil water content at low suction in order to get fc has been particularly found to be a tall order khlosi et al 2008 vereecken et al 2010 in general the process of taking measurements of both fc and pwp can be time consuming laborious costly and sometimes impossible jones et al 2000 rom√°n dobarco et al 2019 as a result not many studies have used field measurements of soil fc and pwp to map soil awc w√∂sten et al 2001 in the light of above challenges awc is often determined from soil physical properties that are easy to measure and whose data are often readily available from most global national and or state soil survey databases the mapping function that maps the easily measurable soil physical properties e g soil texture organic matter content and bulk density to the more difficult to measure soil physical property awc in this case is called a pedotransfer function saxton and rawls 2006 schaap and bouten 1996 a pedotransfer function can simply be interpreted as without loss of generality a classical statistical regression model that maps easily measurable soil hydraulic variables e g soil texture onto useful but difficult to measure soil hydraulic variable like awc soil physical properties are the primary variables that are used in most awc pedotransfer models these pedotransfer models are often developed based on classical statistics specifically the awc is regressed on the explanatory variables like the soil physical properties in a multi variate regression or machine learning paradigm rom√°n dobarco et al 2019 rubio et al 2008 saxton et al 2006 examples of widely used classical geostatistical pedotransfer functions include geographically weighted regression regression kriging kriging co kriging and artificial neural networks amir haghverdi et al 2015 these classical geostatistical pedotransfer functions have been used to map soil awc at various spatial scales not only in the usa kern 1995 mikhailova et al 2018 zheng et al 1996 but also in other regions like scotland poggio et al 2010 new south wales australia malone et al 2009 and france cazemier et al 2001 and at global scales reynolds et al 2000 studies conducted in the h√©rault orb libron valley region languedoc france determined awc as a function of soil texture and bulk density lagacherie et al 2000 soil awc of australian soils were modeled by regressing measured awc onto soil texture bulk density and organic matter minasny et al 1999 in south india soil awc has been estimated as a function of soil moisture and bulk density sreelash et al 2017 in argentina soil awc was simulated using linear regression and artificial neural network by using field measured input variables soil texture bulk density organic matter and soil water content de paepe angel bono et al 2018 a bayesian based ensemble kalman filter method has been used to develop relatively high performing ptf for saturated soil hydraulic conductivity pan et al 2011 other studies have coupled bayesian frameworks with artificial neural network to develop ptf to better simulate soil water retention across a wide range of spatial scales jana et al 2007 reasonable prediction of soil water content has been obtained when predictions from 19 ptfs are averaged within a bayesian framework guber et al 2009 bayesian analysis has also been found to be robust in estimating soil hydraulic and transport properties in a study that involved measuring pressure head and solute concentration based on a unsaturated soil column experimental setup moreira et al 2016 in a vadose zone study conducted in new zealand forecasting of soil pressure head from a tensiometer based measured pressure head data and bayesian based multiobjective optimization was found to produce better predictions w√∂hling and vrugt 2008 than use of a single model a bayesian based empirical model chiu et al 2012 that was used to estimate the hydraulic parameters of the van genuchten based water retention curve based on 90 soil samples that were extracted from unsaturated soil hydraulic database unsoda leij 1996 was found to produce better parameter prediction for sandy soil than for finer soils in a study conducted in an agricultural field near julich germany that involved measuring soil water content using time domain reflectometer tdr incorporating prior knowledge of the probability distribution of the soil hydraulic properties was found to improve predictive capability of the bayesian inverse modelling of in the situ soil water dynamics scharnagl et al 2011 in a soil erosion study conducted to account for uncertainty in the soil sediment prediction wang et al 2015 bayesian analysis was used in averaging soil sediments produced by four soil erosion models that are built in the agricultural policy environmental extender apex software results of this study showed that averaging the sediments predicted from the four erosion models in a bayesian framework resulted in a more robust prediction of the sediment and the associated model uncertainty even though there have been many vadose zone based hydrological studies that have used bayesian analysis to simulate hydrological processes there are limited studies if any that have developed bayesian pedotransfer function in a simple to use software language such as r to simulate the soil awc classical statistical regression models have difficulty in capturing the relative importance of the individual explanatory variables in the awc model besides prior knowledge of probability distribution of soil awc is not incorporated in the pedotransfer functions when classical statistics are used to model soil awc therefore the objective of this study is to 1 develop a new pedotransfer model in a bayesian framework based on measurements of awc and five physical soil properties 2 present the relative importance of the explanatory variables in the awc pedotransfer models 3 methods 3 1 study area and data this study focuses on topsoils in the ogallala aquifer this aquifer transverses eight states of the us and it is the largest aquifer in north america covering an area of 451000 km2 the aquifer supports about 30 of us irrigated agriculture 97 of irrigation water supply within the aquifer brown and pervez 2014 and fresh drinking water to about 80 of people living within its vicinity dennehy 2000 since planning of irrigation across this area requires soil awc reliable measurement and plausible modelling of awc are both important the study area shown in fig 1 was subdivided into 6 regions two regions in the northern part two in the central part and two regions in the southern portion of the study area the model dataset i e actual awc and physical soil properties for ogallala aquifer topsoils was obtained from the us natural resources conservation service nrcs s soil survey geographic ssurgo database soil survey staff 2016 this database is hosted by and freely available at the geospatial data gateway http datagateway nrcs usda gov the topsoil data of the physical soil characteristics obtained from ssurgo database include clay sand silt bulk density g cm3 and organic matter a total of 6000 samples were retrieved from the database each of the six regions had about 1000 topsoil samples for model calibration an additional 333 samples from outside the six regions but within the study area was used for testing the awc pedotransfer models 3 2 calibration of the bayesian based ptfs the bayesian based pedotransfer model of the topsoil s awc is developed by regressing observed awc onto the five regressors physical soil properties the total number of regression models which consists of all possible combinations of regressors that awc can be regressed onto is 32 i e 2 5 32 these regression models are assumed to be normally distributed within the regression model space any regression model that contains kj regressors 0 kj k is expressed as 1 y o i n Œº z j Œ≤ j œÉ Œµ where Œº is the intercept i n is an n dimensional vector of 1 s œÉ is the residual standard deviation hence œÉ 2 is the residual variance Œµ is the residual error that results from fitting all regressors to the independent variable which is assumed to be independent and normally distributed with mean of zero and a variance of œÉ Œ≤ j is a kj dimensional vector of relevant regression coefficients such that Œ≤ j 0 for k kj regressors that are not included in regression model mj z j is an n x kj matrix that consists of columns of kj regressors x 1 x 2 x k j with each regressor having n observations and y o is the awc the bayesian framework unlike classical statistics incorporates the prior knowledge of the data i e the likelihood of the measured awc prior probabilities of regression models mpriors and the prior probabilities of the regression parameters Œº œÉ Œ≤ j otherwise known as the g priors the priors and the likelihood are used to compute the posterior model probability pmp of the regression model parameters based on bayes theorem as shown 2 p m j y o p y o m j p m j p y o l m j y o p m j p y o l m j y o p m j h 1 2 k p y o m h p m h where l m j y o is the likelihood of regression model m j p y o or h 1 2 k p y o m h p m h is the normalizing constant of the posterior model probability p m j is the prior model probability of regression model m j notice that the three priors that are specified are the prior probabilities of regression models mpriors p m j the joint prior probabilities of the intercept and residual variance of the regression models p Œº œÉ 2 and the prior probabilities of the regressors coefficients Œ≤ j of regression models g priors Œ≤ j p the mprior and the g prior used in this study are discrete uniform distribution ley and steel 2012 liang et al 2008 and empirical bayes g local ebl cui and george 2004 george and foster 2000 g prior respectively the uniform mprior is formulated by assuming that all the regressors are independent ley et al 2012 liang et al 2008 and is expressed by the following equations 3 p m j 1 2 j 1 2 2 k 14 and the ebl g prior zellner 1986 is determined based on the following equation 4 g z j t z j 1 Œ≤ j 0 œÉ 2 p Œ≤ j Œº œÉ 2 m j f n k j where f n q m v is the density function of a q dimensional normal distribution with mean m and covariance matrix v g is the scalar quantity that ranges from 0 to 1 the ebl estimates the value of g as a maximum likelihood that only comprises non negative values and is expressed as 5 g m a x f j 1 0 the f j is the standard f statistics for testing whether the regression coefficient of regression model mj is zero or not i e the null hypothesis ho Œ≤ j 0 verses alternative hypothesis ha Œ≤ j 0 6 f j r j 2 k j 1 r j 2 n 1 k j where r j 2 is the coefficient of determination of regression model mj k j is the number of regressors contained in regression model mj n is the number of observations of the dependent variable awc before computing the likelihood the joint prior probabilities of the intercept and residual variance of the regression models p Œº œÉ 2 is determined based on the widely used improper non informative formulation fern√°ndez et al 2002 raftery 1996 7 p Œº œÉ 2 m j 1 œÉ 2 the likelihood of regression model m j is expressed as shown fern√°ndez et al 2001b 2001a 8 l m j y o p y o 1 y o 2 y o n m j p y o Œº Œ≤ j œÉ 2 m j p Œº œÉ 2 p Œ≤ j Œº œÉ 2 m j d Œº d œÉ 2 d Œ≤ j where p y o Œº Œ≤ j œÉ 2 m j is the probability of the data given the regression model parameters Œº Œ≤ j œÉ 2 of model m j the regression coefficients and intercept are weighted with their corresponding pmp to get average weighted values of these regression model parameters for example the pmp weighted regression coefficient of a given regressor e Œ≤ m j z over the regression model space is computed as 9 e Œ≤ m j z j 1 2 k e Œ≤ j y o m j z p m j y o where e Œ≤ j y o m j z is the estimate of regression coefficient Œ≤ j from classical statistical ordinary least square regression the regression model that contains all the regressors whose coefficients and intercept have been weighted with pmp is the bayesian based model that is used to simulate awc once the awc bayesian model is obtained the next task is to figure out the relative importance of the regressors to do this we determine a posterior inclusion pip of each regressor the pip is a measure of the relative importance of the regressor within the bayesian framework for any given regressor the pip is calculated by summing up pmp of all regression models within the regression model space that contain that regressor since the total pmp of all the regression models is 1 the maximum pip value of any regressor is 1 10 p i p p Œ≤ j 0 y o Œ≤ j 0 p m j y o 3 3 model performance analysis based on statistical performance indices in order to determine how well the bayesian based pedotransfer awc model simulates awc statistical performance indices were used these indices used included the widely used model performance indices such as the root mean square error rmse legates and mccabe 1999 the percent bias error pbias gupta et al 1998 and willmott s index of agreement d 1 willmott 1982 willmott et al 1985 the rmse describes the goodness of fit between the actual and simulated data and it is expressed as 11 r m s e 1 p i 1 p y i o i 2 where y i is the modelled awc o i is the measured awc and p is the number of samples the pbias represents the bias between the modeled and the measured awc h v gupta et al 1998 negative and positive pbia represent underestimation and overestimation respectively of the awc pbias values near zero signify that the ptf has less bias mathematically pbias is expressed as 12 p b i a s i 1 p y i o i i 1 p o i 100 index of agreement d 1 willmott 1984 willmott et al 2012 describes the degree with which simulated awc agrees with observed awc the absolute error form of d1 ranges between 0 no fit agreement and 1 perfect fit agreement and is expressed as absolute deviation based form of index of agreement is computed as 13 d 1 1 i 1 p o i y i i 1 p o i Œº o y i Œº o where Œº o is the mean of the measured awc 4 results and discussion 4 1 awc and its explanatory variables the study area from which the soil data was obtained and for which the bayesian based ptfs was developed is shown in fig 1 a summary of the physical soil properties can be found in fig 2 the violin plots shown in fig 2 portray both distribution and the statistics with respect to percentiles i e 25th 50th 75th minimum and maximum of the physical soil properties across the six regions within the study area accordingly the awc seems to be decreasing southwards from region1 to region6 as shown in fig 2 a the topsoil sand and silt increase southwards and northwards respectively as shown in fig 2 b and c respectively clay bulk density and organic matter across the six regions have a mean of 15 20 1 25 1 5 g cm3 and 1 2 respectively as shown in fig 2 d e and f respectively the two regions in the north of the study area region1 and region2 have some spots with organic matter of as high as 80 this can be justified by the relatively greater agricultural activities mainly irrigated corn and soybean in the north than in the south of the study area fig 3 represents the correlation between the modelled and the measured awc under the calibration phase 4 2 bayesian based ptfs in this study eight widely used classical ptfs aina and periaswamy 1985 contreras and bonilla 2018 s c gupta and larson 1979 rawls et al 1983 1982 are developed in a bayesian framework using five physical soil properties for the topsoil awc the bayesian based ptfs are tabulated as shown in table 1 moist bulk density has the largest negative regression coefficient of 0 02 0 1 whereas silt has the strongest positive regression slope of 0 001 0 005 across all the ptfs as shown in table 1 fig 4 provides the validation phase based correlation between modelled and measured awc the performance of calibrated bayesian based ptfs which was evaluated using the three widely used performance indices root mean square error rmse willmott s index of agreement d1 and percent bias error pbias are shown in fig 5 and table 2 the calibrated ptfs have pbias values of less than 1 across the six regions as shown in fig 5 with exception of cla om ptf the d1 varied between acceptable values of 0 6 and 0 79 and rmse of 0 021 0 036 cm cm across the regions ptfs as shown in table 2 the relatively low d1 of 0 39 0 61 of clay om ptf can be attributed to the fact that the distribution of clay and om have weak correlation with that of the awc as shown the violinplot in fig 2 but these d1 values still show moderate agreement of the clay om ptfs with the measured awc validation of the trained ptfs was conducted using an independent dataset outside the regions selected but within the study area ogallala aquifer validation results are summarized in table 3 and in fig 6 similar to the calibration phase the ptfs in the validation phase have reasonable performance with d1 of 0 6 0 67 as summarized in table 3 even though the validation ptfs have relatively larger errors compared to their counterpart calibrated ptfs as evident in the validation pbias of 10 to 10 which is shown in fig 6 and rmse of 0 028 0 062 cm cm they still have reasonable simulation of the awc since these errors are within tolerable limits the northern regions ptfs seem to underestimate awc while the southern regions ptfs overestimate awc especially in the validation phase as shown in fig 6 on the other hand the central regions ptfs neither consistently overestimate nor underestimate awc sand exhibited the weakest regression slope among the physical soil properties therefore soil bulk density and silt seem to be the most dominating factors that affect awc ptfs in both the north and south areas of the ogallala aquifer 4 3 sensitivity of bayesian based ptf to physical soil properties within a bayesian framework the sensitivity of the explanatory variables was evaluated based on leave one out procedure note that only the ptf model with all the five physical properties was used in order to evaluate sensitivity of awc to these properties within the bayesian framework leaving out bulk density from the ptf model resulted in the greatest change in modelled awc nearly across all the six regions as shown in fig 7 a the overall change of modelled awc when bulk density was left out was 20 to 100 as shown as shown in fig 7 a however this change was not uniform throughout the study area the change in modelled awc increased southwards with region1 having the least increase 20 and region6 having the greatest increase in modelled awc 100 leaving out organic matter from the ptf model led to the least change in modelled awc however the change in modelled awc was negative with the northern regions having the least decline of modelled awc 1 and the southern regions having the greatest decline in modelled awc of up to 5 as shown in fig 7 b texture wise among the three textural classes leaving out silt led to the greatest change in modelled awc this change was mostly negative with decline of as high as 80 in region3 as shown in fig 7 c therefore within bayesian framework the topsoil awc ptfs in the ogallala aquifer are mainly sensitive to bulk density and silt 4 4 relative importance of physical soil properties to bayesian based ptf outside the bayesian framework the relative importance of the physical soil properties on the awc ptf were analyzed based on their correlation with the awc the classical pearson s correlation coefficient between individual explanatory variables and the soil awc was computed this was done to analyze the strength of correlation between the physical soil properties and the soil awc results are summarized as shown in table 4 the soil properties with the greatest positive and negative correlation coefficients are silt and sand respectively as shown in table 4 this further emphasizes the role of soil texture and particularly silt in the awc ptf model in the ogallala aquifer however the strength of correlation is slightly weaker in the southern region than northern region of the study area organic matter and bulk density have positive and negative correlation coefficients respectively as shown in table 4 the second method used to analyze correlation of the explanatory variables involved shuffling a single explanatory variable and then predicting the awc using the shuffled variable once the model prediction is obtained a pearson s correlation coefficient is computed between the shuffled based prediction and the measured awc the relative importance score of the explanatory variable in question is computed as 1 minus the correlation coefficient the relative importance ranges from 0 zero influence of the explanatory variable on awc to 1 the strongest influence of the explanatory variable on awc this analysis was implemented using biomod2 package thuiller et al 2016 in r programming language version 3 0 1 the results suggest that sand has the strongest influence on awc with a score of 1 in the southern region of the study area and silt is mostly strongly influential in the north and central regions with a score of 1 as shown in table 5 clay has a strong influence with a score of 1 in the central region moderate influence on awc with a score of 0 35 in the southern and very weak influence with a score of 0 01 in the north both organic matter and bulk density seem to have very weak influence on awc with a score of 0 0 09 across the study area within the bayesian framework the relative importance of the physical soil properties on awc ptf was analyzed using the posterior inclusion probability pip of these explanatory variables pip has been successfully used in other studies to evaluate relative importance of explanatory variables in the bayesian framework achieng and zhu 2019 eicher et al 2007 fern√°ndez et al 2001b 2001a rodriguez iturbe et al 2001 overall the most influential physical soil property on the awc ptf across all the regions is soil bulk density with a pip of 0 94 1 as shown in fig 8 soil texture wise silt seem to have the greatest influence on awc with a pip of 0 86 0 92 as shown in fig 8 with exception of the south region i e region6 which has a pip value of 0 46 both clay and sand have pip of 0 92 and 0 90 respectively as shown in fig 8 therefore even though clay and silt have weak influence on awc in most regions they play a significant role in fitting ptfs in the south region within the bayesian framework organic matter doesn t seem to have a clear spatial trend with respect to its relative importance in bayesian based ptf this can be seen by its very strong influence with pip of 0 91 1 0 in regions1 2 4 and 5 and its weak influence as seen in regions3 pip value of 0 11 and region6 pip value of 0 05 the main advantages of the newly developed bptfs include 1 it allows for evaluation of the relative importance of each of the explanatory variable physical soil properties to the awc model based on the posterior inclusion probability pip 2 bptfs allows for incorporation of the prior knowledge of the physical soil properties this is achieved by use of prior probability of the regression parameters i e the mean the variance and the regression coefficient and the prior probability of the bayesian regression models 3 bptfs considers all the possible regression models 2 5 32 while developing the ptf whereas the classical regression analysis only considers one regression model use of multiple regression models in bayesian model averaging has been found to lead to superior prediction than use of a single model as is in the case of classical statistical regression madigan and raftery 1994 use of a single model as is the case in classical statistical regression often leads to statistical bias w√∂hling et al 2008 and 4 bptfs is computationally fast e g for this study compute time is less than 5 min with the 6000 data samples and it is easy to use since it only requires basic knowledge of the r language the only disadvantage is that unlike the classical regression which can be easily analyzed on e g ms excel the btfs program require use of r software therefore basic knowledge of r is recommended 5 conclusions bayesian based awc pedotransfer functions were developed for the ogallala aquifer topsoil which supports most irrigated agriculture especially in the western usa this study is an improvement over the commonly used classical statistical regression analysis to develop ptfs this is because of the fact that the bayesian framework allows for incorporation of prior knowledge of the distribution and statistics of the physical soil parameters into the ptfs since the study area lies within nebraska s sandhills the majority of the ogallala aquifer is dominated with silt especially in the northern region and sand southwards the following is a summary of the key findings of this study out of the three textural classes silt has been found to play the most influential awc ptfs within and outside bayesian framework between the two non textural soil properties the bulk density had the greatest influence on awc especially within the bayesian framework sand has a relatively weaker influence on awc ptfs even though the majority of the study area lies in the nebraska sandhill region the bayesian based ptfs are region dependent the influence of sand and organic matter on awc ptfs are also are region dependent acknowledgements special thank you goes to the paul a rechard fellowship of the university of wyoming for financial support the anonymous reviewers and the editor have provided valuable comments throughout the review process of the original manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104500 
26159,global or national scale flood early warning systems fews can benefit developing countries and ungauged regions that lack observational data computational infrastructure and or the human capacity for streamflow modelling existing land surface models lsm typically generate forecasts using coarse resolution grid cells which at least for streamflow have little value when used for flood warning at local scales we present the design and development of a new automated computational system using existing well established open source software tools that quickly downscales or maps the runoff generated from such coarse grid based lsms onto high resolution vector based stream networks then routes the results using a vector based river routing model we conducted experiments using the era interim land reanalysis data a 35 year retrospective gridded runoff data product from the european center for medium range weather forecasts ecmwf to assess our fast downscaling system the accuracy of our approach is comparable to the global flood awareness system glofas a well established gridded routing model using the same forcings but our method provides streamflow predictions on significantly higher resolution stream networks we found that the river network resolution has negligible effect on the simulated streamflow with our model routing in other words we can forecast streamflow for very small stream segments and potentially improve local flood awareness and response much more successfully than previously possible using readily available climate forcings from lsms keywords grid to vector mapping rapid vector based river routing streamflow prediction flood early warning system 1 introduction flood early warning systems fews are important tools for flood management since they can provide greater prediction and timing of forthcoming hazards so that populations are more prepared before and more resilient after flood events effective fews that provide flood warnings with a reasonable lead time are mostly established at catchment scale usually in areas where sufficient data technology and human resources are available basha and rus 2007 guy j p schumann 2018 latt and wittenberg 2014 due to limited resources it is costly or even impossible to build effective and sustainable fews in data scarce areas cools et al 2016 emerton et al 2016 in recent decades considerable advances in satellite technology and high performance computing have led to significant improvements in modelling and predicting the hydrologic cycle at the global scale guy j p schumann 2018 sood and smakhtin 2015 ward et al 2015 the hydrologic processes are usually land surface models lsms that simulate the water and energy interactions between the atmosphere and earth surface the lsms take the meteorological forcing from weather prediction models or observations as input and provide output as water balance parameters from the earth s surface including evapotranspiration soil moisture snow and runoff pitman 2003 among these land surface results runoff has received most attention because the importance of streamflow to society bai et al 2016 many researchers have performed river routing using runoff forecasts from lsms to compute river flow and estimate floods alfieri et al 2013 hirpa et al 2018 wu et al 2012 2014 these studies demonstrate the potential to develop global or large scale fews by leveraging global runoff predictions if these approaches could be automated they could provide data support for developing countries and ungauged regions that lack sufficient resources to develop the cyber infrastructure and human capacity to implement advanced flood prediction systems however the relatively low resolutions of these lsms are not practical for flood prediction at local scales developing practical flood prediction systems requires higher resolution stream networks where managers can select predictions at relevant locations the development of such systems presents a number of challenges first river routing models have traditionally used a gird based scheme in which river segments calculation units are discretized as grids balazs m fekete 2011 yamazaki et al 2013 significant computational resources are required to perform global or large scale grid based routing at resolutions fine enough to provide local scale solutions second to better simulate the hydrodynamic process in river channels river routing models have been improved increasingly to include more and more complex physical processes shaad 2018 yamazaki et al 2013 it is still challenging to implement complex physically based river routing models at the global scale because they have more parameters and higher requirement for observation data and computing power to run and calibrate younis and de roo 2010 which is especially difficult in data scarce areas last the result of grid based routing models is river discharge at each grid cell it is a challenge to convert from gridded outflow volumes to estimated river discharge at a local scale so that the benefits of the global models can be maximized for these systems to be useful for flood prediction it is essential to provide streamflow at a fine resolution or at a river reach level scale vector based river routing modelling is receiving more attention as it facilitates large scale hydrological predictions at finer resolutions and some large scale models have started to shift from a grid based environment towards to a vector based environment for predictions david et al 2011 2013 lin et al 2018 2019 sikder et al 2019 the difference between vector based river routing and grid based river routing is shown in fig 1 the grid based river routing defines the river network as a set of connected grid cells and performs the streamflow routing simulation in each river grid cell in the vector based river routing the river network is represented as lines and the calculations are implemented in each river segment in both cases grid based and vector based river routing calculations require parameterization of the computational elements including estimates of cross sectional geometry surface roughness and slope for grid based routing these parameters are estimated for each grid cell whereas for vector based routing each stream segment is parameterized separately several recent studies have implemented vector based river routing with large scale lsms and demonstrated its feasibility and flexibility david et al 2011a replaced the grid river routing scheme in the sim france model with routing application for parallel computation of discharge rapid and obtained comparably accurate simulations but higher model efficiency lehner and grill 2013 developed a vector river routing model hydrorout and coupled it with a global river network database hydrosheds to support global scale eco hydrological modelling mizukami et al 2016 developed a river network routing tool mizuroute that post processes the runoff outputs from lsms and performs continental scale streamflow simulations this tool can use both grid based and vector based river networks the authors demonstrated its capability to produce streamflow on a vector based river network over the contiguous united states conus snow et al 2016 implemented rapid on the runoff generated by the european centre for medium range weather forecasts ecmwf model molteni et al 1996 to develop a us national scale streamflow prediction web application tavakoly et al 2017 performed river flow modelling in mississippi river basin using rapid and high resolution nhdplus river data their validation results showed that rapid has a satisfactory performance in continental scale river routing lin et al 2018 integrated rapid with the community wrf hydro framework for continental scale flood discharge modelling and demonstrated its computational efficiency and reasonable accuracy in predicting flood discharge during hurricane ike in 2008 in general the advantages of vector based routing models compared with grid based routing models especially at large scales include computational efficiency higher resolution representations of the hydrological features and more precise locations for predictions research has shown that routing through a vector based representation of a river network has more advantages and flexibilities for large spatial domains than routing through a grid based representation of river networks at coarse resolutions lehner and grill 2013 lin et al 2018 mizukami et al 2016 singh et al 2015 to date global lsms have typically been generated at such coarse resolutions that at least for streamflow they provide little value at local scales when used for flood prediction and warning at the same time developing regions of the world lack observational data computational infrastructure and the human capacity to create actionable streamflow and by extension flood forecasts global gridded runoff models could fill the need for flood prediction in developing regions if the large scale gridded results could be made useful at local scales this requires the development and integration of a system that can take output from the lsms accurately distribute flows to vector based stream segments and route the resulting flows for streamflow and flood prediction we have designed and developed a computational system using existing well established open source tools that routes runoff generated from large scale grid based modelling into high resolution vector based river networks ultimately providing fews support for flood management in data scarce regions our contribution to this field is in developing the tools and data structures to create and develop the automated system to distribute grid flow to vector stream segments this study presents both the system we developed to provide automated flood forecasts and our method for distributing grid based flows to vector stream segments we present experimental results that demonstrate the feasibility of the system and its flexibility in providing useful flood prediction over large regions these predictions are useful as they can be computed at more precise locations than predictions from grid based systems in section 2 we present our method for grid to vector flow mapping and describe the experimental design we used to validate this method section 3 presents the results of our integrated system and validation experiments section 4 provides a detailed discussion on the results and describes the benefits of this integrated system for flood prediction in data scarce regions section 4 also provides conclusions and potential areas for future work this paper describes an integrated system for using lsm data to create useful flood predictions at a local scale a software availability section is provided in the end to describe a global fews web app we have developed using our approach and the source code of all the tools and software used in this paper our contributions are developing the integrated system and creating a new method for distributing course resolution grid cell flows to high resolution vector stream segments as part of developing this system we extended the rapidpy vector routing model snow et al 2017 to accept input from additional lsm models 2 methods 2 1 grid to vector mapping to develop a system that couples course resolution lsms with high resolution vector based river routing models is to determine how to distribute the grid generated runoff data to the correct river segment this is relatively straightforward when coupling grid based run off with grid based river routing models where runoff from the lsm grid is distributed as river inflow to the nearest downstream river grid cell for vector based routing unlike grid based river routing methods no such one to one relationship exists to distribute the gridded runoff and with the correct vectorized river segments for example one grid cell might contribute to multiple river segments or one river segment might accept runoff from multiple grid cells these complex relationships make runoff distribution complicated one solution based on physical processes is to determine the contributing catchment for each river segment and use the runoff from those grid cells to calculate the water inflow of each river segment studies have adopted different methods for coupling gridded lsms with vector based routing models david et al 2013 mapped nldas2 gridded runoff to the united states nhdplus river network using a catchment centroid based method the inflow of a catchment was calculated using the runoff value of the grid cell where the catchment centroid was located snow et al 2016 adopted an area weighted method to convert global runoff depths generated by the ecmwf model to the runoff volume of each nhdplus catchment lin et al 2018 compared the area weighted method with the catchment centroid based method and concluded that the model is more sensitive to the grid to vector coupling interface than the grid resolution the area weighted coupling exhibits better results for high resolution gridded forcing data especially when the grid cells of the forcing are much smaller than the catchments we used an area weighted grid to vector method adapted from snow et al 2016 shown as fig 2 this method employs a weight table to convert the calculated gridded runoff depths to the inflow of each catchment the weight table contains the area ratio of each lsm grid cell to the intersected catchments we refer to this area ratio as weight in this context fig 3 show the workflow for calculating a weight table for gridded lsm runoff data first we compute a thiessen polygon feature based on the coordinates latitude and longitude attributes stored with the runoff data in the netcdf file fig 2 shows example thiessen polygons as grids squares each polygon is a square with the given coordinates as centroids next the algorithm intersects the polygon with data from a drainage file that contains an identifier for each river segment the river identifier is referred to as comid in this research in this way we obtain a polygon feature that has the attributes of geodesic area and the intersecting catchment comid finally we generate the weight table from the intersected polygon features we compute the inflow of a catchment as the sum of all the runoff products from each intersected lsm grid cell runoff i j the area of the lsm grid cell l 2 and l refers to the grid size and the weight k of each intersected grid cell shown as eq 1 k refers to the number of grid cells intersected with the catchment 1 i n f l o w k 1 n r u n o f f i j l 2 w e i g h t k 2 2 rapid river routing model rapid is a vector based river routing model developed by david et al 2011a that uses a matrix based version of the muskingum method to simulate the water flow through a vector based river network that can range from watershed scale to global scale david et al 2011b the muskingum routing method has been widely used in vector based river routing because of its simplicity and low computational cost compared with other methods gill 1978 tung 1985 since its first formal release rapid has been used and verified in a number of studies including continental scale high resolution flow modelling and operational flood forecasting computation of river height at the regional scale and other hydrological topics david et al 2016 a set of assisting tools exists to lower the barrier to operating rapid by users from different disciplines including an arcgis toolset ding 2016 developed by esri for rapid input data preprocessing and an open source software tool rapidpy developed by snow et al 2017 to assist in preparing inputs in the required formats and running rapid rapidpy supports several large scale lsms including ecmwf era interim dee et al 2011 nldas xia et al 2012 and gldas lorenz et al 2015 we have extended rapidpy to support more models including era5 karl hennermann 2018 hiwat gatlin et al 2018 and cosmo rockel et al 2008 rapid is developed in fortran and compiled as a dynamic link library dll whose methods can be called or executed from external software the rapid required inputs shown as the gray boxes in fig 4 include 1 a set of forcing files of lsm surface and subsurface runoff 2 a model initialization file describing the streamflow of each river at the start time of the simulation 3 a file describing the water inflow from surface and subsurface runoff into the upstream point of each river reach 4 a file documenting the river network s topological connectivity 5 two files defining the muskingum routing parameters k and x and other basic information such as the internal time step and duration of the simulation all the information of required inputs and simulation settings are documented in a namelist text file for rapid to read at runtime river connectivity and muskingum parameters can be calculated by third party gis software such as arcgis proprietary taudem open source and others the arcgis extension arc hydro has a tool called dendritic terrain with unknown stream location that can delineate watersheds and generate a stream network shapefile from a flow direction file and a flow accumulation file these files are directly generated from a dem file through the flow direction tool and flow accumulation tool in arcgis arc hydro has a tool called calculate the muskingum parameters which can estimate the muskingum k and x values for each stream segment the value of k is associated with the flow travel time through a stream and calculated using eq 2 by multiplying the user given factor Œªk by the length of the stream segment over the flow wave velocity the value of x for each stream segment is assigned based on the user given factor Œªx david et al 2013 2 k j Œª k l j v x j Œª x 0 1 where kj and xj are the muskingum parameters for reach j lj is the length of a river reach and v is flow wave celerity default value in this tool is 1 km h or 0 28 m s Œªk and Œªx are two multiplying factors later determined by the rapid optimization procedure default value of Œªk is 0 35 default value of Œªx is 3 the files of river connectivity and muskingum parameters files only need to be generated once for a river network then the weight table described in section 2 1 is generated through the arcgis rapid preprocessing toolset all the above steps only need to be performed once for a river network and an incorporated lsm once these data have been generated rapidpy is used to write the muskingum and river connectivity files in the required formats from the arcgis processed results create catchment inflow files with the weight table populate the namelist file and run rapid when rapidpy is used in a forecasting mode after the simulation is complete rapidpy generates an initialization file from the streamflow results for the next forecast model run this offers users the flexibility to extract different data from the initialization file including the result of a specific day or seasonal averages 2 3 experimental design 2 3 1 comparison with glofas the global flood awareness system glofas developed by ecmwf and the joint research centre of the european commission is a coupled hydro meteorological model that generates global streamflow predictions for large scale river basins alfieri et al 2013 glofas provides daily streamflow forecasts of up to 30 days by using the lisflood hydrological model forced by the surface and subsurface runoff from the ecmwf integrated forecast system ifs meteorological forecasts lisflood is a grid based routing model that can separately simulate different hydrological processes that occur in large river basins younis and de roo 2010 the lisflood processes activated in glofas include the simulation of groundwater storage groundwater flow and flow routing in river channels hirpa et al 2018 glofas also provides a long term reanalysis dataset 1980 01 to 2017 12 with daily streamflow at the global scale with a gridded spatial resolution of 0 1 this reanalysis dataset uses the same hydrological model but is forced by the runoff from era interim land era interim land is a global land surface reanalysis dataset with parameterization improved htessel land surface model balsamo et al 2009 e l wipfler et al 2011 driven by meteorological forcing from the era interim atmospheric reanalysis and observed precipitation adjustments g balsamo et al 2015 as shown in fig 5 the htessel model is implemented to simulate the water and energy fluxes between land surface and atmosphere and estimate the surface and subsurface runoff required for river routing wang et al 2019 albergel et al 2018 dee et al 2011 the resolution of era interim land surface and subsurface runoff are both 80 km in glofas the lisflood model is set up on global coverage with horizontal grid resolution of 0 1 to better represent the hydrological process at large river basin scale shaw et al 2005 so the gridded surface and subsurface runoff from era interim land is resampled from 80 km to 0 1 to be used as input of the lisflood model it has been demonstrated that glofas can skillfully detect hazardous events in large river basins and also provide a reasonable streamflow forecast in most parts of the world alfieri et al 2013 hirpa et al 2018 to evaluate the performance of our system using rapid for global scale streamflow prediction we routed the surface and subsurface runoff of 35 year 1980 01 2014 12 era interim land data and compared the resulting routed streamflow with glofas reanalysis data shown as fig 5 the primary benefit of using our rapid based system with vector based stream routing instead of relying only on glofas is that the gridded surface and subsurface runoff is mapped to a high resolution river network that includes streams in much smaller basins than glofas this means that flood predictions can be computed at more precise locations section 3 1 provides the results of this experiment 2 3 2 sensitivity of watershed resolution watershed boundaries and river networks are the basis for vector based river routing for many countries and regions in the world it is difficult and expensive to implement in situ measurements to collect hydrographic data especially at a large spatial scale for large scale or global hydrological modelling the watershed and river network maps are normally delineated from digital elevation model dem files for this method the resolution and accuracy of the river network entirely depends on the resolution of the dem file in recent years tremendous improvements have been made in the availability quality and resolution of large scale hydrographic datasets due to the availability of large scale or global earth data obtained from satellite remote sensing technologies this includes high resolution dem data the most well known versions of large scale hydrographic datasets include the us national hydrography dataset plus version 2 mckay et al 2012 the australian hydrological geospatial fabric atkinson et al 2008 the european catchments and rivers network system ecrins agency 2012 and the global scale hydrographic dataset hydrosheds gong et al 2011 in summary several hydrographic datasets exist at different resolutions allowing users to choose the appropriate dataset based on data availability and study purposes to satisfy river continuum e g mass balance streamflow at each location of the river network is influenced by upstream processes vannote et al 1980 in rapid each reach segment or catchment is a modelling unit each reach segment gets lateral inflow determined from the catchment runoff and routes these flows to downstream segment how these catchment runoff flows are distributed to the stream segments depends on the algorithm used the size of the catchment and the resolution of the river segments as shown in fig 10 the same outlet of a watershed can have a different numbers of upstream segments under different hydrographic data resolutions we performed an experiment to evaluate whether the discharge at the same outlet is affected by the number of upstream segments it has in other words the sensitivity of a vector based river routing model to the resolution of the hydrographic data we selected several watersheds in continental united states conus with the following criteria 1 cover an area of several hundred square miles 2 have an outlet that corresponds with a stream gauge and 3 a be a relatively unregulated area i e with no major reservoirs or diversions we delineated each watershed at low medium and high resolutions we then routed the 35 year era interim land reanalysis data using our rapid based system for each of the watersheds defined at different resolutions we compare and present the simulated results in section 3 2 3 results 3 1 comparison with glofas we have implemented our system for south asia africa south america and conus to validate our system we randomly selected 100 glofas reporting stations across these areas from glofas website http www globalfloods eu glofas forecasting fig 6 shows the selected stations the reason for using glofas reporting stations is that those stations provide the upstream area used in the glofas routing model one challenge we faced comparing glofas reanalysis data with our era rapid simulated results is that glofas uses grid based river routing and our system uses vector based river routing the challenge is to correctly match glofas grid cells with the vector representation of the streams used in the rapid routing scheme our method for matching the outlet points was to select from our model the largest stream among the streams that interact with the glofas grid cell then we calculated the upstream area of the selected stream using watershed delineation procedures using the same dem data that generated the streams we compared the calculated upstream areas we computed using this method with the upstream areas of these stations provided by glofas we assume that stations with an upstream area difference less than 10 between these two models are matched we based this threshold by considering that the resolution of glofas is 0 1 and the streams used in this experiment are delineated from 3 arc sec hydrosheds dem data we initially identified 100 glofas stations for comparison all the points in fig 6 of these 20 points did not meet the selection criteria red points in fig 6 based on our criteria we selected and evaluated 80 stations shown as grey points in fig 6 for each station we calculated 35 year cumulative volume per unit area cvpua from glofas reanalysis data and era rapid simulation results we compared the 35 year cvpua of 80 stations from the two models shown in fig 7 the cvpua values of two models are highly correlated with a high r2 of 0 9818 this statistic provides additional evidence that the glofas and rapid locations are good matches at these 80 stations to compare the two streamflow simulations we calculated two statistical skill scores including the kling gupta efficiency kge2012 and the pearson correlation coefficient r for each station both for the glofas reanalysis and the era rapid these two metrics are computed using the hydrostats library of the hydroerr package jackson et al 2019 roberts et al 2018 a python package that provides statistical analysis for hydrological prediction models for these results the median of kge is 0 73 and the median of r is 0 92 the two simulations are highly correlated for 89 of the stations with r 0 7 and well fitted for 70 of the stations with kge 0 5 fig 8 shows the distribution of these two skill scores in the research regions the simulations show consistency between the two methods in south asia most of south america and africa and the west coast and east coast of conus there are a number of stations in the middle part of conus with negative kge and low r values we found these stations all have relatively low cvpua values therefore the poor fit in these stations might because small volume of water that was routed with different models it also might be due to the fact that the comparison node for the grid based system is at the center of the cell and the node for the vector based system is on the stream line even though we used contributing area comparisons to limit these discrepancies at low flows differences in gauge locations could create higher variance the distributions indicate that era rapid and glofas results are generally consistent however the degree of consistency varies across regions fig 9 shows the hydrographs that compare era rapid with glofas at two stations with different level of skill scores the first station dipayal station in nepal shows very good match between two simulations the second station rocky reach dam station in the united states shows a significant difference between the two datasets the vector based era rapid outputs a lower peak flow than grid based glofas the disparity might due to the difference between these two routing models glofas considers all processes in the grid based routing including overland surface routing subsurface storage and routing while rapid simplifies the explicit representation of these processes and only focuses on the reach scale streamflow response the disparity could alsobecause the variations in the location of the comparison points change the reported flow overall era rapid results are comparable to glofas results in other words the higher resolution vector based routing model generated streamflow estimates that are generally consistent with the results from the glofas provides using a gridded data routing model we can assume with some confidence that our high resolution stream segment forecasts are at least as reliable as the well established low resolution glofas results 3 2 sensitivity of watershed resolution for sensitivity studies we selected five watersheds across the conus including meramec river near sullivan mo east branch delaware river at margaretville ny alsea river near tidewater or white river near fort apache az and north fork clearwater river near canyon ranger station id see fig 10 these watersheds are summarized in table 1 we selected these watersheds because of the availability of ground truth flow data the us geological survey usgs provides sufficient historical streamflow observations at these locations more than 35 years that we can use to evaluate whether the different stream densities resulting from different dem resolutions affects the simulation performance all the selected watersheds have a usgs stream gauge located at an outlet for comparison each watershed was delineated into 3 7 and around 20 catchment sub basins for the low medium and high watershed resolutions respectively shown as fig 10 detailed information of the sub basins see table 1 for each watershed we ran the 35 year era interim land reanalysis data 1980 2014 80 km resolution see section 2 3 1 for more dataset information as forcing at each watershed resolution and the generated flowrates at the basin mouth we compared the results for the entire watershed based on sub basins delineated at different resolutions with each other table 2 table 2 shows the statistical comparison of the 35 year model simulated streamflow at the same watershed outlet generated with different stream densities we adopted the following statistical metrics to comprehensively and quantitatively assess the sensitivity of watershed resolution kge r and mean absolute average mae table 2 shows that all the comparisons resulted in a high r average of 0 9849 and kge average of 0 9616 and a low mae average of 1 605 m3 this small variation among the results from the different resolution sub basins demonstrates that the model generates similar streamflow forecasts using different watershed resolutions we found that the comparisons between medium resolution and high resolution watersheds resulted in a higher r and kge and lower mae than the comparisons of low resolution and medium resolution this suggests that the simulated streamflow at the basin mouth changes less as the watershed resolution increases essentially the watershed resolution has a negligible effect on the simulated streamflow since slight differences exist in the results simulated from different watershed resolutions we can conclude that the prediction performance of the model while negligible is slightly affected by the watershed resolution 4 discussion and conclusions the motivation of this study was to create and test an enhanced method and develop a system to route coarse gridded runoff generated from global or national lsms onto a high resolution vector based river network and provide flood prediction at local scales there are many regions in the world that are hydrologic data poor and have little or no observed data or working models to provide the backbone of a regional or national streamflow forecasting system or fews leveraging global models can be an efficient way to provide baseline hydrologic information and supplement whatever other resources are available but there remains the question of whether or not the information is useful enough at local scales to be able to make informed decisions managers need flood predictions at specific locations on local stream networks we demonstrated the ability to distribute grid based runoff data for routing on high resolution vector based stream networks that meet this needs this represents a significant step forward because international development organizations like the world bank can reinvest dollars focused on leveraging such hydrologic information into applications and have more informed decisions by using these tools we presented and tested a system including an innovative downscaling method for mapping large scale lsm grid cells to high resolution stream networks by leveraging the rapid vector based river routing model we compared routing 35 year era interim land reanalysis data in rapid and the results from glofas reanalysis data and demonstrated that our rapid based system has comparable performance with glofas however using results from our vector based stream network we can provide streamflow predictions at much higher resolutions this is done with much less computational cost than creating high resolution gridded data as it inherits all the benefits of vector based routing models by comparing routing results using the same forcing data with different river network resolutions of the same watershed we found that the river network resolution has a negligible effect on the simulated streamflow using our system this means that using our system including the downscaling method and using rapid model for routing can produce consistent results regardless of the resolution of hydrographic datasets the user chooses this is in major contrast to the performance of grid based routing models which is significantly affected by the grid resolution the system is developed with open source tools and is available for others to use and extend we have implemented this system in a streamflow prediction tool web app detailed information see software availability that aims to provide global streamflow prediction by mapping gridded ecmwf runoff forecasts to the global vector based high resolution river network we have made available the implementation for the south asia africa south america and conus regions we are working to expand this application it to cover the globe the primary benefit of this work is to streamline the process of mapping and routing gridded runoff into high resolution river network and provide the possibility to simultaneously produce streamflow forecasts from current existing lsm prediction models it can provide flood management support to data scarce regions by enabling the streamflow estimates at specific locations using various global or national climate forecasting systems this work presents a new method for distributing grid based runoff to vector stream networks and serves as a method and guidance for coupling lsms with other vector based routing models we demonstrated that our mapping process can efficiently convert gridded runoff data to intuitive and useful river discharge at local scales provide an easy way to access various global or national runoff forecasts simultaneously and make informed decisions we expect that our work and outcomes can motivate contribution on streamlining different disciplinary models to maximize current outcomes and facilitate environmental modelling research the next steps of our work include introducing approaches to optimize rapid parameters such as flow travel time estimation approach to calculate muskingum parameter k incorporating data assimilation methods to improve initial states by leveraging observations or some large scale lsm modelling systems like gldas incorporating web processing services to facilitate process interoperability and data processing qiao et al 2019 software availability the global streamflow prediction tool web app an implementation of this system can be found at https tethys byu edu apps streamflow prediction tool username demo password demo the web app was implemented on tethys platform home page https www tethysplatform org source code https github com tethysplatform tethys git which is an open source software framework for environmental web app development swain et al 2016 the tools included in this system are all open source the source code of the arcgis rapid preprocessing toolbox developed by esri can be found at https github com esri python toolbox for rapid git the original version of rapidpy developed by u s army engineer research and development center is available at https github com erdc rapidpy git the extended version of rapidpy that accepts forcings from era5 karl hennermann 2018 hiwat gatlin et al 2018 and cosmo rockel et al 2008 is available at https github com byu hydroinformatics rapidpy git acknowledgments this work was supported by the nasa roses servir applied sciences team research grant number nnx16an45g c√©dric h david was supported by the jet propulsion laboratory california institute of technology under a contract with nasa including a grant from the servir applied sciences team author contributions to this article are as follows xiaohui qiao conducted the bulk of the work and is the primary contributor e james nelson provided the funding and coordinated the research daniel p ames assisted with the research and contributed to the text zhiyu li provided computational and technical support c√©dric h david is the original author of rapid gustavious p williams provided statistical support wade roberts jorge luis s√°nchez lozano chris edwards michael souffront and mir a matin all contributed to the experimental studies the authors express their appreciation to esri and u s army engineer research and development center for their input on the arcgis rapid preprocessing toolset and rapidpy which greatly helped this work 
26159,global or national scale flood early warning systems fews can benefit developing countries and ungauged regions that lack observational data computational infrastructure and or the human capacity for streamflow modelling existing land surface models lsm typically generate forecasts using coarse resolution grid cells which at least for streamflow have little value when used for flood warning at local scales we present the design and development of a new automated computational system using existing well established open source software tools that quickly downscales or maps the runoff generated from such coarse grid based lsms onto high resolution vector based stream networks then routes the results using a vector based river routing model we conducted experiments using the era interim land reanalysis data a 35 year retrospective gridded runoff data product from the european center for medium range weather forecasts ecmwf to assess our fast downscaling system the accuracy of our approach is comparable to the global flood awareness system glofas a well established gridded routing model using the same forcings but our method provides streamflow predictions on significantly higher resolution stream networks we found that the river network resolution has negligible effect on the simulated streamflow with our model routing in other words we can forecast streamflow for very small stream segments and potentially improve local flood awareness and response much more successfully than previously possible using readily available climate forcings from lsms keywords grid to vector mapping rapid vector based river routing streamflow prediction flood early warning system 1 introduction flood early warning systems fews are important tools for flood management since they can provide greater prediction and timing of forthcoming hazards so that populations are more prepared before and more resilient after flood events effective fews that provide flood warnings with a reasonable lead time are mostly established at catchment scale usually in areas where sufficient data technology and human resources are available basha and rus 2007 guy j p schumann 2018 latt and wittenberg 2014 due to limited resources it is costly or even impossible to build effective and sustainable fews in data scarce areas cools et al 2016 emerton et al 2016 in recent decades considerable advances in satellite technology and high performance computing have led to significant improvements in modelling and predicting the hydrologic cycle at the global scale guy j p schumann 2018 sood and smakhtin 2015 ward et al 2015 the hydrologic processes are usually land surface models lsms that simulate the water and energy interactions between the atmosphere and earth surface the lsms take the meteorological forcing from weather prediction models or observations as input and provide output as water balance parameters from the earth s surface including evapotranspiration soil moisture snow and runoff pitman 2003 among these land surface results runoff has received most attention because the importance of streamflow to society bai et al 2016 many researchers have performed river routing using runoff forecasts from lsms to compute river flow and estimate floods alfieri et al 2013 hirpa et al 2018 wu et al 2012 2014 these studies demonstrate the potential to develop global or large scale fews by leveraging global runoff predictions if these approaches could be automated they could provide data support for developing countries and ungauged regions that lack sufficient resources to develop the cyber infrastructure and human capacity to implement advanced flood prediction systems however the relatively low resolutions of these lsms are not practical for flood prediction at local scales developing practical flood prediction systems requires higher resolution stream networks where managers can select predictions at relevant locations the development of such systems presents a number of challenges first river routing models have traditionally used a gird based scheme in which river segments calculation units are discretized as grids balazs m fekete 2011 yamazaki et al 2013 significant computational resources are required to perform global or large scale grid based routing at resolutions fine enough to provide local scale solutions second to better simulate the hydrodynamic process in river channels river routing models have been improved increasingly to include more and more complex physical processes shaad 2018 yamazaki et al 2013 it is still challenging to implement complex physically based river routing models at the global scale because they have more parameters and higher requirement for observation data and computing power to run and calibrate younis and de roo 2010 which is especially difficult in data scarce areas last the result of grid based routing models is river discharge at each grid cell it is a challenge to convert from gridded outflow volumes to estimated river discharge at a local scale so that the benefits of the global models can be maximized for these systems to be useful for flood prediction it is essential to provide streamflow at a fine resolution or at a river reach level scale vector based river routing modelling is receiving more attention as it facilitates large scale hydrological predictions at finer resolutions and some large scale models have started to shift from a grid based environment towards to a vector based environment for predictions david et al 2011 2013 lin et al 2018 2019 sikder et al 2019 the difference between vector based river routing and grid based river routing is shown in fig 1 the grid based river routing defines the river network as a set of connected grid cells and performs the streamflow routing simulation in each river grid cell in the vector based river routing the river network is represented as lines and the calculations are implemented in each river segment in both cases grid based and vector based river routing calculations require parameterization of the computational elements including estimates of cross sectional geometry surface roughness and slope for grid based routing these parameters are estimated for each grid cell whereas for vector based routing each stream segment is parameterized separately several recent studies have implemented vector based river routing with large scale lsms and demonstrated its feasibility and flexibility david et al 2011a replaced the grid river routing scheme in the sim france model with routing application for parallel computation of discharge rapid and obtained comparably accurate simulations but higher model efficiency lehner and grill 2013 developed a vector river routing model hydrorout and coupled it with a global river network database hydrosheds to support global scale eco hydrological modelling mizukami et al 2016 developed a river network routing tool mizuroute that post processes the runoff outputs from lsms and performs continental scale streamflow simulations this tool can use both grid based and vector based river networks the authors demonstrated its capability to produce streamflow on a vector based river network over the contiguous united states conus snow et al 2016 implemented rapid on the runoff generated by the european centre for medium range weather forecasts ecmwf model molteni et al 1996 to develop a us national scale streamflow prediction web application tavakoly et al 2017 performed river flow modelling in mississippi river basin using rapid and high resolution nhdplus river data their validation results showed that rapid has a satisfactory performance in continental scale river routing lin et al 2018 integrated rapid with the community wrf hydro framework for continental scale flood discharge modelling and demonstrated its computational efficiency and reasonable accuracy in predicting flood discharge during hurricane ike in 2008 in general the advantages of vector based routing models compared with grid based routing models especially at large scales include computational efficiency higher resolution representations of the hydrological features and more precise locations for predictions research has shown that routing through a vector based representation of a river network has more advantages and flexibilities for large spatial domains than routing through a grid based representation of river networks at coarse resolutions lehner and grill 2013 lin et al 2018 mizukami et al 2016 singh et al 2015 to date global lsms have typically been generated at such coarse resolutions that at least for streamflow they provide little value at local scales when used for flood prediction and warning at the same time developing regions of the world lack observational data computational infrastructure and the human capacity to create actionable streamflow and by extension flood forecasts global gridded runoff models could fill the need for flood prediction in developing regions if the large scale gridded results could be made useful at local scales this requires the development and integration of a system that can take output from the lsms accurately distribute flows to vector based stream segments and route the resulting flows for streamflow and flood prediction we have designed and developed a computational system using existing well established open source tools that routes runoff generated from large scale grid based modelling into high resolution vector based river networks ultimately providing fews support for flood management in data scarce regions our contribution to this field is in developing the tools and data structures to create and develop the automated system to distribute grid flow to vector stream segments this study presents both the system we developed to provide automated flood forecasts and our method for distributing grid based flows to vector stream segments we present experimental results that demonstrate the feasibility of the system and its flexibility in providing useful flood prediction over large regions these predictions are useful as they can be computed at more precise locations than predictions from grid based systems in section 2 we present our method for grid to vector flow mapping and describe the experimental design we used to validate this method section 3 presents the results of our integrated system and validation experiments section 4 provides a detailed discussion on the results and describes the benefits of this integrated system for flood prediction in data scarce regions section 4 also provides conclusions and potential areas for future work this paper describes an integrated system for using lsm data to create useful flood predictions at a local scale a software availability section is provided in the end to describe a global fews web app we have developed using our approach and the source code of all the tools and software used in this paper our contributions are developing the integrated system and creating a new method for distributing course resolution grid cell flows to high resolution vector stream segments as part of developing this system we extended the rapidpy vector routing model snow et al 2017 to accept input from additional lsm models 2 methods 2 1 grid to vector mapping to develop a system that couples course resolution lsms with high resolution vector based river routing models is to determine how to distribute the grid generated runoff data to the correct river segment this is relatively straightforward when coupling grid based run off with grid based river routing models where runoff from the lsm grid is distributed as river inflow to the nearest downstream river grid cell for vector based routing unlike grid based river routing methods no such one to one relationship exists to distribute the gridded runoff and with the correct vectorized river segments for example one grid cell might contribute to multiple river segments or one river segment might accept runoff from multiple grid cells these complex relationships make runoff distribution complicated one solution based on physical processes is to determine the contributing catchment for each river segment and use the runoff from those grid cells to calculate the water inflow of each river segment studies have adopted different methods for coupling gridded lsms with vector based routing models david et al 2013 mapped nldas2 gridded runoff to the united states nhdplus river network using a catchment centroid based method the inflow of a catchment was calculated using the runoff value of the grid cell where the catchment centroid was located snow et al 2016 adopted an area weighted method to convert global runoff depths generated by the ecmwf model to the runoff volume of each nhdplus catchment lin et al 2018 compared the area weighted method with the catchment centroid based method and concluded that the model is more sensitive to the grid to vector coupling interface than the grid resolution the area weighted coupling exhibits better results for high resolution gridded forcing data especially when the grid cells of the forcing are much smaller than the catchments we used an area weighted grid to vector method adapted from snow et al 2016 shown as fig 2 this method employs a weight table to convert the calculated gridded runoff depths to the inflow of each catchment the weight table contains the area ratio of each lsm grid cell to the intersected catchments we refer to this area ratio as weight in this context fig 3 show the workflow for calculating a weight table for gridded lsm runoff data first we compute a thiessen polygon feature based on the coordinates latitude and longitude attributes stored with the runoff data in the netcdf file fig 2 shows example thiessen polygons as grids squares each polygon is a square with the given coordinates as centroids next the algorithm intersects the polygon with data from a drainage file that contains an identifier for each river segment the river identifier is referred to as comid in this research in this way we obtain a polygon feature that has the attributes of geodesic area and the intersecting catchment comid finally we generate the weight table from the intersected polygon features we compute the inflow of a catchment as the sum of all the runoff products from each intersected lsm grid cell runoff i j the area of the lsm grid cell l 2 and l refers to the grid size and the weight k of each intersected grid cell shown as eq 1 k refers to the number of grid cells intersected with the catchment 1 i n f l o w k 1 n r u n o f f i j l 2 w e i g h t k 2 2 rapid river routing model rapid is a vector based river routing model developed by david et al 2011a that uses a matrix based version of the muskingum method to simulate the water flow through a vector based river network that can range from watershed scale to global scale david et al 2011b the muskingum routing method has been widely used in vector based river routing because of its simplicity and low computational cost compared with other methods gill 1978 tung 1985 since its first formal release rapid has been used and verified in a number of studies including continental scale high resolution flow modelling and operational flood forecasting computation of river height at the regional scale and other hydrological topics david et al 2016 a set of assisting tools exists to lower the barrier to operating rapid by users from different disciplines including an arcgis toolset ding 2016 developed by esri for rapid input data preprocessing and an open source software tool rapidpy developed by snow et al 2017 to assist in preparing inputs in the required formats and running rapid rapidpy supports several large scale lsms including ecmwf era interim dee et al 2011 nldas xia et al 2012 and gldas lorenz et al 2015 we have extended rapidpy to support more models including era5 karl hennermann 2018 hiwat gatlin et al 2018 and cosmo rockel et al 2008 rapid is developed in fortran and compiled as a dynamic link library dll whose methods can be called or executed from external software the rapid required inputs shown as the gray boxes in fig 4 include 1 a set of forcing files of lsm surface and subsurface runoff 2 a model initialization file describing the streamflow of each river at the start time of the simulation 3 a file describing the water inflow from surface and subsurface runoff into the upstream point of each river reach 4 a file documenting the river network s topological connectivity 5 two files defining the muskingum routing parameters k and x and other basic information such as the internal time step and duration of the simulation all the information of required inputs and simulation settings are documented in a namelist text file for rapid to read at runtime river connectivity and muskingum parameters can be calculated by third party gis software such as arcgis proprietary taudem open source and others the arcgis extension arc hydro has a tool called dendritic terrain with unknown stream location that can delineate watersheds and generate a stream network shapefile from a flow direction file and a flow accumulation file these files are directly generated from a dem file through the flow direction tool and flow accumulation tool in arcgis arc hydro has a tool called calculate the muskingum parameters which can estimate the muskingum k and x values for each stream segment the value of k is associated with the flow travel time through a stream and calculated using eq 2 by multiplying the user given factor Œªk by the length of the stream segment over the flow wave velocity the value of x for each stream segment is assigned based on the user given factor Œªx david et al 2013 2 k j Œª k l j v x j Œª x 0 1 where kj and xj are the muskingum parameters for reach j lj is the length of a river reach and v is flow wave celerity default value in this tool is 1 km h or 0 28 m s Œªk and Œªx are two multiplying factors later determined by the rapid optimization procedure default value of Œªk is 0 35 default value of Œªx is 3 the files of river connectivity and muskingum parameters files only need to be generated once for a river network then the weight table described in section 2 1 is generated through the arcgis rapid preprocessing toolset all the above steps only need to be performed once for a river network and an incorporated lsm once these data have been generated rapidpy is used to write the muskingum and river connectivity files in the required formats from the arcgis processed results create catchment inflow files with the weight table populate the namelist file and run rapid when rapidpy is used in a forecasting mode after the simulation is complete rapidpy generates an initialization file from the streamflow results for the next forecast model run this offers users the flexibility to extract different data from the initialization file including the result of a specific day or seasonal averages 2 3 experimental design 2 3 1 comparison with glofas the global flood awareness system glofas developed by ecmwf and the joint research centre of the european commission is a coupled hydro meteorological model that generates global streamflow predictions for large scale river basins alfieri et al 2013 glofas provides daily streamflow forecasts of up to 30 days by using the lisflood hydrological model forced by the surface and subsurface runoff from the ecmwf integrated forecast system ifs meteorological forecasts lisflood is a grid based routing model that can separately simulate different hydrological processes that occur in large river basins younis and de roo 2010 the lisflood processes activated in glofas include the simulation of groundwater storage groundwater flow and flow routing in river channels hirpa et al 2018 glofas also provides a long term reanalysis dataset 1980 01 to 2017 12 with daily streamflow at the global scale with a gridded spatial resolution of 0 1 this reanalysis dataset uses the same hydrological model but is forced by the runoff from era interim land era interim land is a global land surface reanalysis dataset with parameterization improved htessel land surface model balsamo et al 2009 e l wipfler et al 2011 driven by meteorological forcing from the era interim atmospheric reanalysis and observed precipitation adjustments g balsamo et al 2015 as shown in fig 5 the htessel model is implemented to simulate the water and energy fluxes between land surface and atmosphere and estimate the surface and subsurface runoff required for river routing wang et al 2019 albergel et al 2018 dee et al 2011 the resolution of era interim land surface and subsurface runoff are both 80 km in glofas the lisflood model is set up on global coverage with horizontal grid resolution of 0 1 to better represent the hydrological process at large river basin scale shaw et al 2005 so the gridded surface and subsurface runoff from era interim land is resampled from 80 km to 0 1 to be used as input of the lisflood model it has been demonstrated that glofas can skillfully detect hazardous events in large river basins and also provide a reasonable streamflow forecast in most parts of the world alfieri et al 2013 hirpa et al 2018 to evaluate the performance of our system using rapid for global scale streamflow prediction we routed the surface and subsurface runoff of 35 year 1980 01 2014 12 era interim land data and compared the resulting routed streamflow with glofas reanalysis data shown as fig 5 the primary benefit of using our rapid based system with vector based stream routing instead of relying only on glofas is that the gridded surface and subsurface runoff is mapped to a high resolution river network that includes streams in much smaller basins than glofas this means that flood predictions can be computed at more precise locations section 3 1 provides the results of this experiment 2 3 2 sensitivity of watershed resolution watershed boundaries and river networks are the basis for vector based river routing for many countries and regions in the world it is difficult and expensive to implement in situ measurements to collect hydrographic data especially at a large spatial scale for large scale or global hydrological modelling the watershed and river network maps are normally delineated from digital elevation model dem files for this method the resolution and accuracy of the river network entirely depends on the resolution of the dem file in recent years tremendous improvements have been made in the availability quality and resolution of large scale hydrographic datasets due to the availability of large scale or global earth data obtained from satellite remote sensing technologies this includes high resolution dem data the most well known versions of large scale hydrographic datasets include the us national hydrography dataset plus version 2 mckay et al 2012 the australian hydrological geospatial fabric atkinson et al 2008 the european catchments and rivers network system ecrins agency 2012 and the global scale hydrographic dataset hydrosheds gong et al 2011 in summary several hydrographic datasets exist at different resolutions allowing users to choose the appropriate dataset based on data availability and study purposes to satisfy river continuum e g mass balance streamflow at each location of the river network is influenced by upstream processes vannote et al 1980 in rapid each reach segment or catchment is a modelling unit each reach segment gets lateral inflow determined from the catchment runoff and routes these flows to downstream segment how these catchment runoff flows are distributed to the stream segments depends on the algorithm used the size of the catchment and the resolution of the river segments as shown in fig 10 the same outlet of a watershed can have a different numbers of upstream segments under different hydrographic data resolutions we performed an experiment to evaluate whether the discharge at the same outlet is affected by the number of upstream segments it has in other words the sensitivity of a vector based river routing model to the resolution of the hydrographic data we selected several watersheds in continental united states conus with the following criteria 1 cover an area of several hundred square miles 2 have an outlet that corresponds with a stream gauge and 3 a be a relatively unregulated area i e with no major reservoirs or diversions we delineated each watershed at low medium and high resolutions we then routed the 35 year era interim land reanalysis data using our rapid based system for each of the watersheds defined at different resolutions we compare and present the simulated results in section 3 2 3 results 3 1 comparison with glofas we have implemented our system for south asia africa south america and conus to validate our system we randomly selected 100 glofas reporting stations across these areas from glofas website http www globalfloods eu glofas forecasting fig 6 shows the selected stations the reason for using glofas reporting stations is that those stations provide the upstream area used in the glofas routing model one challenge we faced comparing glofas reanalysis data with our era rapid simulated results is that glofas uses grid based river routing and our system uses vector based river routing the challenge is to correctly match glofas grid cells with the vector representation of the streams used in the rapid routing scheme our method for matching the outlet points was to select from our model the largest stream among the streams that interact with the glofas grid cell then we calculated the upstream area of the selected stream using watershed delineation procedures using the same dem data that generated the streams we compared the calculated upstream areas we computed using this method with the upstream areas of these stations provided by glofas we assume that stations with an upstream area difference less than 10 between these two models are matched we based this threshold by considering that the resolution of glofas is 0 1 and the streams used in this experiment are delineated from 3 arc sec hydrosheds dem data we initially identified 100 glofas stations for comparison all the points in fig 6 of these 20 points did not meet the selection criteria red points in fig 6 based on our criteria we selected and evaluated 80 stations shown as grey points in fig 6 for each station we calculated 35 year cumulative volume per unit area cvpua from glofas reanalysis data and era rapid simulation results we compared the 35 year cvpua of 80 stations from the two models shown in fig 7 the cvpua values of two models are highly correlated with a high r2 of 0 9818 this statistic provides additional evidence that the glofas and rapid locations are good matches at these 80 stations to compare the two streamflow simulations we calculated two statistical skill scores including the kling gupta efficiency kge2012 and the pearson correlation coefficient r for each station both for the glofas reanalysis and the era rapid these two metrics are computed using the hydrostats library of the hydroerr package jackson et al 2019 roberts et al 2018 a python package that provides statistical analysis for hydrological prediction models for these results the median of kge is 0 73 and the median of r is 0 92 the two simulations are highly correlated for 89 of the stations with r 0 7 and well fitted for 70 of the stations with kge 0 5 fig 8 shows the distribution of these two skill scores in the research regions the simulations show consistency between the two methods in south asia most of south america and africa and the west coast and east coast of conus there are a number of stations in the middle part of conus with negative kge and low r values we found these stations all have relatively low cvpua values therefore the poor fit in these stations might because small volume of water that was routed with different models it also might be due to the fact that the comparison node for the grid based system is at the center of the cell and the node for the vector based system is on the stream line even though we used contributing area comparisons to limit these discrepancies at low flows differences in gauge locations could create higher variance the distributions indicate that era rapid and glofas results are generally consistent however the degree of consistency varies across regions fig 9 shows the hydrographs that compare era rapid with glofas at two stations with different level of skill scores the first station dipayal station in nepal shows very good match between two simulations the second station rocky reach dam station in the united states shows a significant difference between the two datasets the vector based era rapid outputs a lower peak flow than grid based glofas the disparity might due to the difference between these two routing models glofas considers all processes in the grid based routing including overland surface routing subsurface storage and routing while rapid simplifies the explicit representation of these processes and only focuses on the reach scale streamflow response the disparity could alsobecause the variations in the location of the comparison points change the reported flow overall era rapid results are comparable to glofas results in other words the higher resolution vector based routing model generated streamflow estimates that are generally consistent with the results from the glofas provides using a gridded data routing model we can assume with some confidence that our high resolution stream segment forecasts are at least as reliable as the well established low resolution glofas results 3 2 sensitivity of watershed resolution for sensitivity studies we selected five watersheds across the conus including meramec river near sullivan mo east branch delaware river at margaretville ny alsea river near tidewater or white river near fort apache az and north fork clearwater river near canyon ranger station id see fig 10 these watersheds are summarized in table 1 we selected these watersheds because of the availability of ground truth flow data the us geological survey usgs provides sufficient historical streamflow observations at these locations more than 35 years that we can use to evaluate whether the different stream densities resulting from different dem resolutions affects the simulation performance all the selected watersheds have a usgs stream gauge located at an outlet for comparison each watershed was delineated into 3 7 and around 20 catchment sub basins for the low medium and high watershed resolutions respectively shown as fig 10 detailed information of the sub basins see table 1 for each watershed we ran the 35 year era interim land reanalysis data 1980 2014 80 km resolution see section 2 3 1 for more dataset information as forcing at each watershed resolution and the generated flowrates at the basin mouth we compared the results for the entire watershed based on sub basins delineated at different resolutions with each other table 2 table 2 shows the statistical comparison of the 35 year model simulated streamflow at the same watershed outlet generated with different stream densities we adopted the following statistical metrics to comprehensively and quantitatively assess the sensitivity of watershed resolution kge r and mean absolute average mae table 2 shows that all the comparisons resulted in a high r average of 0 9849 and kge average of 0 9616 and a low mae average of 1 605 m3 this small variation among the results from the different resolution sub basins demonstrates that the model generates similar streamflow forecasts using different watershed resolutions we found that the comparisons between medium resolution and high resolution watersheds resulted in a higher r and kge and lower mae than the comparisons of low resolution and medium resolution this suggests that the simulated streamflow at the basin mouth changes less as the watershed resolution increases essentially the watershed resolution has a negligible effect on the simulated streamflow since slight differences exist in the results simulated from different watershed resolutions we can conclude that the prediction performance of the model while negligible is slightly affected by the watershed resolution 4 discussion and conclusions the motivation of this study was to create and test an enhanced method and develop a system to route coarse gridded runoff generated from global or national lsms onto a high resolution vector based river network and provide flood prediction at local scales there are many regions in the world that are hydrologic data poor and have little or no observed data or working models to provide the backbone of a regional or national streamflow forecasting system or fews leveraging global models can be an efficient way to provide baseline hydrologic information and supplement whatever other resources are available but there remains the question of whether or not the information is useful enough at local scales to be able to make informed decisions managers need flood predictions at specific locations on local stream networks we demonstrated the ability to distribute grid based runoff data for routing on high resolution vector based stream networks that meet this needs this represents a significant step forward because international development organizations like the world bank can reinvest dollars focused on leveraging such hydrologic information into applications and have more informed decisions by using these tools we presented and tested a system including an innovative downscaling method for mapping large scale lsm grid cells to high resolution stream networks by leveraging the rapid vector based river routing model we compared routing 35 year era interim land reanalysis data in rapid and the results from glofas reanalysis data and demonstrated that our rapid based system has comparable performance with glofas however using results from our vector based stream network we can provide streamflow predictions at much higher resolutions this is done with much less computational cost than creating high resolution gridded data as it inherits all the benefits of vector based routing models by comparing routing results using the same forcing data with different river network resolutions of the same watershed we found that the river network resolution has a negligible effect on the simulated streamflow using our system this means that using our system including the downscaling method and using rapid model for routing can produce consistent results regardless of the resolution of hydrographic datasets the user chooses this is in major contrast to the performance of grid based routing models which is significantly affected by the grid resolution the system is developed with open source tools and is available for others to use and extend we have implemented this system in a streamflow prediction tool web app detailed information see software availability that aims to provide global streamflow prediction by mapping gridded ecmwf runoff forecasts to the global vector based high resolution river network we have made available the implementation for the south asia africa south america and conus regions we are working to expand this application it to cover the globe the primary benefit of this work is to streamline the process of mapping and routing gridded runoff into high resolution river network and provide the possibility to simultaneously produce streamflow forecasts from current existing lsm prediction models it can provide flood management support to data scarce regions by enabling the streamflow estimates at specific locations using various global or national climate forecasting systems this work presents a new method for distributing grid based runoff to vector stream networks and serves as a method and guidance for coupling lsms with other vector based routing models we demonstrated that our mapping process can efficiently convert gridded runoff data to intuitive and useful river discharge at local scales provide an easy way to access various global or national runoff forecasts simultaneously and make informed decisions we expect that our work and outcomes can motivate contribution on streamlining different disciplinary models to maximize current outcomes and facilitate environmental modelling research the next steps of our work include introducing approaches to optimize rapid parameters such as flow travel time estimation approach to calculate muskingum parameter k incorporating data assimilation methods to improve initial states by leveraging observations or some large scale lsm modelling systems like gldas incorporating web processing services to facilitate process interoperability and data processing qiao et al 2019 software availability the global streamflow prediction tool web app an implementation of this system can be found at https tethys byu edu apps streamflow prediction tool username demo password demo the web app was implemented on tethys platform home page https www tethysplatform org source code https github com tethysplatform tethys git which is an open source software framework for environmental web app development swain et al 2016 the tools included in this system are all open source the source code of the arcgis rapid preprocessing toolbox developed by esri can be found at https github com esri python toolbox for rapid git the original version of rapidpy developed by u s army engineer research and development center is available at https github com erdc rapidpy git the extended version of rapidpy that accepts forcings from era5 karl hennermann 2018 hiwat gatlin et al 2018 and cosmo rockel et al 2008 is available at https github com byu hydroinformatics rapidpy git acknowledgments this work was supported by the nasa roses servir applied sciences team research grant number nnx16an45g c√©dric h david was supported by the jet propulsion laboratory california institute of technology under a contract with nasa including a grant from the servir applied sciences team author contributions to this article are as follows xiaohui qiao conducted the bulk of the work and is the primary contributor e james nelson provided the funding and coordinated the research daniel p ames assisted with the research and contributed to the text zhiyu li provided computational and technical support c√©dric h david is the original author of rapid gustavious p williams provided statistical support wade roberts jorge luis s√°nchez lozano chris edwards michael souffront and mir a matin all contributed to the experimental studies the authors express their appreciation to esri and u s army engineer research and development center for their input on the arcgis rapid preprocessing toolset and rapidpy which greatly helped this work 
