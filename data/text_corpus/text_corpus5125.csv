index,text
25625,process based forest growth models have increased applications in forest ecology carbon sequestration and climate change the forest growth model physiological principles for predicting growth 3 pg has been widely used in these fields due to its moderate complexity and explicit physiologic background as a further step in this continued effort we developed 3 pg2py a python version of 3 pg2 a modified version of 3 pg with respect to water balance prediction to facilitate its extension and application to broader communities in this study the basic structure of 3 pg2py is explained and its simulation against observations is demonstrated to facilitate sensitivity and uncertainty analyses two global sensitivity analysis algorithms and an ensemble kalman filter algorithm are integrated into 3 pg2py and examples are presented additionally an interface for spatial simulation is implemented 3 pg2py is compatible with python 2 7 and open source with 3 pg2py the users can adapt the model easily to more diversified applications especially the computationally intensive ones keywords 3 pg2 forest growth model python global sensitivity analysis variance based sensitivity analysis fourier amplitude sensitivity test ensemble kalman filter 1 introduction forest is one of the most important terrestrial ecosystems forests cover about 31 percent global land area and have prominent impacts in wood production biodiversity conservation hydrological cycle social services bioenergy and climate change brown et al 2005 jandl et al 2007 díaz et al 2009 crossman et al 2011 buchholz et al 2016 fao 2020 the total carbon stock in forests was about 662 gigatons in 2020 fao 2020 it s estimated that terrestrial ecosystems provide a gigaton net carbon sink per year for atmospheric co2 and this sink mainly originates from carbon accumulation in forests although environmental disturbances e g fire drought insect outbreaks and management and dynamics could drive particular forests from a carbon sink to a source canadell and raupach 2008 forests therefore play a crucial role in regulating atmospheric co2 and could be regarded as a natural brake on climate change gibbs et al 2007 fahey et al 2010 the reforestation and increasing carbon stock at stand or landscape scales are two ways to increase carbon sinks via forestry activities canadell and raupach 2008 according to fao 2020 plantation forests composed of one or two species and intensively managed with regular spacing even aged and mainly for productive purposes cover about 3 percent of the global forest area i e 45 percent of the total area of the planted forests and the other planted forests 55 percent of all planted forests are not intensively managed and mainly provide ecosystem restoration and soil and water protection functions despite this relatively small ratio of the plantation forests to global forest area its influence on climate mitigation is not dispensable for example china alone had a net annual forest increase of 1 94 million ha in 2010 2020 fao 2020 and it s estimated that this offset about one fifth of chinese fossil fuel emissions in 2000 fang et al 2007 wang et al 2007 canadell and raupach 2008 compared with forest area estimation which could be performed by investigating forest inventory data or using remote sensing data the assessment of carbon stocks or biomass of forests under various environmental conditions and management strategies is more difficult and full of uncertainties landsberg and waring 1997 here we distinguish the static and dynamic forest carbon stock estimations and focus on the later one in the remaining part of this paper as summarized by gibbs et al 2007 the static forest carbon stock estimation can be classified into three broad approaches according to the adapted methods or suitable spatial scales i e biome average approach ground based forest inventory measurement and satellite based approach the main drawbacks of the static forest carbon stocks estimation is that it s completely empirical and has no mechanisms of forest growth under the premise of changes in management and climate its reliability may decrease landsberg and waring 1997 gupta and sharma 2019 the dynamic forest carbon estimation could be performed by either process based forest growth model or empirical model landsberg and sands 2011 coops et al 1998 neumann et al 2016 the advantage of the process based forest model lies in that it might accommodate future environmental changes more readily and explicable the process based forest growth model refers to a set of physiological procedures expressed by mathematical equations about the workings of forest ecosystem i e the interactions of different processes among varying levels and responses to environmental stimuli and its generality compared with empirical models lies in that the processes or mechanisms involved generally could be applied to all similar forest ecosystems landsberg and sands 2011 landsberg and waring 1997 gupta and sharma 2019 as pointed out by landsberg and sands 2011 the process based models could be used either as scientific tools to identify knowledge gaps and assess different processes or as practical tools however the complexity of specific models must be controlled and a pragmatic principle should be preferred if the model outputs are expected to be of practical values the physiological principles for predicting growth 3 pg first published by landsberg and waring 1997 is a relatively simple process based and stand level deterministic carbon balance model of forest growth 3 pg was originally developed to simulate the growth of even aged monospecific evergreen forests or plantations sands and landsberg 2002 paul et al 2007 potithep and yasuoka 2011 coops et al 2012 meyer et al 2017 3 pg has been applied worldwide for a range of species and regions gupta and sharma 2019 more recently it has been developed further for mixed evergreen and deciduous tree species in even or uneven aged forests forrester and tang 2016 forrester et al 2021b the original 3 pg and 3 pg2 a modified version of 3 pg with respect to water balance prediction are written in the visual basic for applications vba landsberg and sands 2011 almeida et al 2007b almeida and sands 2016a however the excel version of 3 pg 3 pg2 albeit convenient for practical users due to its comprehensible user interface as shown by excel spreadsheets has been a great obstacle for applications such as data assimilation uncertainty and sensitivity analysis parameter optimization and parallel computing song et al 2012 trotsiuk et al 2020 the common features of these specific applications usually involve 1 intensive computation e g monte carlo simulation and parameter shuffling 2 high performance computing hpc and 3 spatial simulation to resolve these technical and scientific challenges in forest growth modelling when applying this model a series versions of 3 pg have been developed to fit different applications and scientific assumptions see https 3pg forestry ubc ca software for example 3 pg spatial a spatial version of 3 pg coops et al 1998 tickle et al 2001 almeida et al 2016 3 pgmix for mixed and uneven aged evergreen and deciduous forest forrester and tang 2016 forrester et al 2021b 3 pgn 3 pg nitrogen xenakis et al 2008 and r3pg an r package wrapped fortran implementation of 3 pg to simulate monospecific as well as mixtures of evergreen and deciduous tree species in even or uneven aged stands and to calculate wood δ 13 c as well wei et al 2014 trotsiuk et al 2020 forrester et al 2021b in this study we present a new version of 3 pg2 in python programming language namely 3 pg2py which has been successfully applied in global sensitivity analysis of 3 pg2 at stand level song et al 2012 2013 it s noted as above that there are already some other available 3 pg implementations in other programming languages e g fortran and r thomas et al 2017 trotsiuk et al 2020 the 3 pg2py is intended to provide an extensible workbench in pure python for forest modelers preferred this programming language and the object oriented model structure makes it readily extensible to fit specific algorithms to address the challenges mentioned above the uncertainty analysis is an essential part of development calibration and application of the generally rich parameterized process based models including 3 pg van oijen et al 2005 thomas et al 2017 forrester et al 2021a for example trotsiuk et al 2020 integrated the morris sensitivity analysis and a bayesian calibration method into r3pg to facilitate model calibration in this study we further refined the sensitivity analysis capability of 3 pg by integrating two effective global sensitivity analysis gsa algorithms into 3 pg2py uncertainty analysis including parameter and state variable estimation and data assimilation are actually indispensable features of modelling practice nowadays we implemented a state parameter estimation algorithm by using the ensemble kalman filter enkf to meet these requirements in 3 pg2py moradkhani et al 2005b for users interested in spatial applications we also implemented an interface suitable for spatial simulation and it can be readily extended to support spatial applications by revising the corresponding initialization and output modules in 3 pg2py we developed 3 pg2py based on 3 pg2 using python programming language python is object oriented cross platform and open source with rich numerical and scientific packages 3 pg2py is free and open source and could be easily adapted to add new modules for experienced users in present study the structure and key features of 3 pg2py is explained and its applications in single plot simulation gsa analysis and state parameter estimation are demonstrated respectively the remaining part of this paper is organized as follows the brief physiological principles input output data requirements and code structure of 3 pg2py are presented in section 2 section 3 demonstrates single plot simulation by using data of two plots and schematically shows the simulation and data collection processes the new modules in 3 pg2py i e the gsa state parameter estimation and spatial simulation interface are introduced in section 4 and we conclude in section 5 2 model description and structure 2 1 model description the 3 pg2py 3 pg2 model is a simplified process based and stand level deterministic forest growth model and can be used to predict forest gross primary production water balance evapotranspiration rate stem number and partitioning of biomass to stem foliage and root in the model the canopy photosynthesis determines biomass production process to be more specific the canopy quantum efficiency qe and absorbed photosynthetically active radiation apar are used to estimate gross primary production gpp the qe is affected by multiple factors e g stand age air temperature vapor pressure deficit vpd frost available soil water soil nutritional status and canopy conductance the ratio between net primary production npp and gpp i e carbon use efficiency is assumed to be conservative for simplicity in 3 pg landsberg and sands 2011 waring et al 1998 the biomass partitioning between above ground and root is tuned by growth modifiers e g temperature soil water and stand age and soil fertility the biomass partitioning among foliage stem and root is determined by a series of allometric equations related with stem diameter at breast height dbh almeida et al 2007 landsberg and waring 1997 the outputs of 3 pg are state variables e g the foliage stem and root biomass pools available soil water and stocking and variables such as stand evapotranspiration specific leaf area sla canopy leaf area index lai net primary production npp and mean dbh the model runs in a monthly time step so monthly average meteorological data i e mean maximum and minimum air temperature vpd frost days shortwave solar radiation and rainfall should be provided the site specific parameters e g latitude soil texture maximum available soil water fertility rating and initial number of stems per hectare and species specific parameters should also be provided sands and landsberg 2002 2 2 model structure 3 pg2py is developed in an object oriented manner in python the interfaces for all the parameter settings including parameters 3 pg parameters site series under simulation siteseries site specific parameters excel sheets named by site names as listed in the siteseries sheet respectively and meteorological data silo metdata in 3 pg2py are kept the same as 3 pg for excel the xlrd module is used to read excel file in 3 pg2py so the microsoft excel is not necessary during model running the simulation results are exported to a sqlite database one table per site the sqlite database is cross platform and integrated with python by sqlite3 module but users can also revise the code to use other preferable databases the code structure of 3 pg2py could be generally classified as two parts i e the model parameter initialization part and core part fig 1 the entrance of 3 pg2py is in the file main py where the global variables soil type data standard 3 pg parameters site series list and site specific parameters are initialized sequentially in the core part of 3 pg2py i e file main module py the simulation of each site is executed in turn and the values of each output variable are stored in the database the simulated forest growth is forced by meteorological data water availability soil water and site and stand specific parameters e g soil fertility and texture stand stocking and stand age the environmental factors formulate growth stress modifiers and allometric equations and then determine the accumulation of biomass and biomass partitioning in stem foliage and root as a model mainly for the managed forest the influence of silvicultural events is also considered 3 single plot simulation in this section we demonstrate the basic usage of 3 pg2py by simulating two forest sites named as site1 and site2 with corymbia maculata and eucalyptus cladocalyx in a monthly time step respectively for simplicity the default parameter values for the species are applied the two sites are located in australia and had different number of observations and thinning events before the age of 10 years the simulation periods of site1 and site2 are 50 and 8 years respectively fig 2 same as 3 pg for excel the user can simulate any number of sites by editing the siteseries sheet in file 3pgxl parameters xls as shown in fig 2 see also fig 1 3 pg2py first initializes two sites with corresponding global and site specific parameters then executes the growth simulation for each site sequentially the values of the output variables at each step are written to a sqlite database and user could edit variable list or database table in main module py to include only the interested variables if necessary in this example 10 variables as well as stand age table 1 are selected for each site the outputs are organized as a database table with fields variables as listed in the code in fig 2 the simulated stem and foliage biomass and the mean dbh as well as the observed data and thinning events stems per hectare are plotted 4 new modules 4 1 global sensitivity analysis compared with the so called local sensitivity analysis lsa the gsa is able to explore the full parameter space and quantify the contribution of each parameter to the variations of model state variables by averaging over the variations of all target parameters saltelli et al 2008 the fourier amplitude sensitivity analysis test fast and variance based method are effective gsa methods saltelli et al 2010 xu and gertner 2011 although relatively simple 3 pg2 still contains more than 50 standard parameters as well as tens of site stand and soil specific parameters for sites with different forest species and environmental conditions the sensitivities of model state variables to parameters are very likely different so gsa is usually a necessary step in model calibration and parameter estimation practices the fast and variance based gsa algorithms are implemented in 3 pg2py to facilitate model calibration this is based on the fact that the implementation complexity of gsa algorithms has actually been a great obstacle to many modelers and this difficulty forces many users turn to simple but biased sensitivity analysis methods for example lsa the lsas estimate the sensitivity of one specific parameter by changing its value while holding the other parameters constant and it s only suitable for simple linear or additive models saltelli et al 2008 the process based models are usually non linear and have high dimensional parameter space and gsa algorithms are specifically designed to explore its full parameter space for the variance based method two sensitivity indices could be derived i e the main effect and total effect sensitivity indices respectively the main effect sensitivity index represents the partial variance contributed by a certain parameter to the total variance of a specific state variable while the total effect sensitivity index equals the main effect plus interactions with other parameters saltelli et al 2010 song et al 2012 the drawback of the variance based method is that it s computationally intensive compared with the fast method actually the results of fast are comparable to the main effect sensitivity index calculated by the variance based method song et al 2013 two examples using the variance based fig 3 and fast fig 4 methods are presented in this study respectively in fig 4 the time series sensitivity indices following the simulation process are calculated by fast it s worth to emphasize that the sensitivity analysis algorithms are not merely methods to quantify the relative sensitivities of specific state variable to parameters but also an effective means to verify whether the interactions among different modules are under the correct trajectories constrained by basic biophysiological assumptions in ecological models the fast and variance based gsa algorithms are both integrated in the file main py in 3 pg2py the user can change the targeted model parameter list and the corresponding state variables and then run the gsa the end age of simulation controls to which stand age the gsa explains and it can be changed either manually in the excel spreadsheet of 3pgxl parameters xls before running or by changing the variable plot endage value in the code during model running dynamically 4 2 state parameter estimation using ensemble kalman filter the sequential data assimilation methods are effective means to improve forecast constrained by the imprecise parameters and assumptions in the process based models by assimilating observed data moradkhani et al 2005b the enkf is a well known sequential data assimilation method suitable for nonlinear models by propagating an ensemble of updated model states from previous time step to approximate the forecast state error covariance matrix and it s capable to explicitly take all sources of uncertainty from input output and model structure into account evensen 2003 generally the enkf is used to estimate the time evolution of state variables with prespecified parameters however it might be unreliable to assume that model parameters do not change over time for example it has been found that carbon use efficiency may vary with forest type and age mäkelä and valentine 2001 time dependent parameter sensitivity analysis of 3 pg2 also implies that the influence of model parameters to state variable may evolve with time song et al 2013 so it s desirable to simultaneously assemble the temporal evolutions of parameters and state variables in the sequential data assimilation system moradkhani et al 2005a 2005b we implemented a state parameter estimation module using enkf in 3 pg2py following the algorithm proposed by moradkhani et al 2005b as a case study we demonstrate the state parameter estimation of 3 pg2py for about 15 years using pseudo observed data every half a year in site1 three parameters table 2 and fig 5 two state variables fig 6 and one prediction variable fig 7 are selected respectively the parameters are sampled from uniform distributions and 500 starting points are randomly sampled from parameter space with ensemble size of 30 it s noted that the ensemble size and hyper parameters i e the variance of noises added to parameters and output variables are crucial to the performance of the ensemble forecast and should be carefully calibrated before running of the state parameter enkf moradkhani et al 2005b the time evolution of the three parameters is shown in fig 5 it shows that although the convergences of the confidence intervals are not significant the means of ensemble members of the parameters smoothly converge to certain values despite the randomly selected start points in the parameter spaces at each step the updated parameters are used to update model states and fig 6 displays the time evolution of two state variables fig 7 shows the estimation of avdbh by assimilating the pseudo observed avdbh every half a year and the ensemble mean is very close to the observations 4 3 spatial simulation interface 3 pg2py is a stand level model from the perspective of spatialization the running of 3 pg2py can be viewed as a simulation at the scale of a single plot or pixel it means that only model initialization and output modules need to be revised if spatial simulation is preferred an interface suitable for spatial simulation is implemented in current version of 3 pg2py fig 8 it fully utilizes the advantage of object oriented model structure and can assemble any number of plots or pixels this architecture makes the spatial simulation quite flexible additionally it can be used as the basis for ensemble algorithms for example the enkf presented in section 4 2 is based on this frame in which an ensemble of state or parameter vectors can be propagated in parallel and the ensemble members can exchange information at each step the spatial scale of a single pixel is up to the user and can be determined by both of application requirement and data availability the parameters values and forcing data for each pixel can be set individually by using the observer design pattern a typical solution to propagate events to all members e g pixels in an ensemble 5 summary and conclusions 3 pg2py is a new open source python version of 3 pg2 and it s compatible with python 2 7 3 pg2py is aimed at the applications algorithms such as data assimilation uncertainty and sensitivity analysis parameter optimization and parallel computing in forest growth simulation related studies these applications usually need a model to run on cross platform and involve intensive computations in parameter shuffling but these tasks can hardly be performed by 3 pg2 we acknowledge that some other versions of this model e g r3pg trotsiuk et al 2020 can fulfill part of these requirements but 3 pg2py could find its place in providing an extensible workbench for forest modelers and the object oriented model structure makes it readily extensible to implement specific algorithms to address the challenges mentioned above we added three new modules i e the spatial simulation interface global sensitivity analysis and state parameter estimation by using enkf into 3 pg2py this serves two purposes first it demonstrates that the structure of 3 pg2py is very flexible and extensible and second it provides effective tools to assist users to perform global sensitivity and uncertainty analysis two crucial tasks in modelling practices and this will make 3 pg2py more useful with 3 pg2py the users can make full use of the readily available numerical computation resources in python community and naturally has the cross platform capability we demonstrated that this improvement in computing efficiency is crucial to solve some specific problems e g the gsa song et al 2012 2013 and we believe that 3 pg2py could ease the applications in data assimilation and model optimization for users interested in spatial simulation 3 pg2py provides a concise interface and the users can implement the procedures in parameter including forcing data initialization and state output data collections under current framework to get the full functionality we strictly followed the model structure of 3 pg2 during the development of 3 pg2py that s to say some deficiencies in 3 pg2 can still be found in 3 pg2py for example the constant npp gpp ratio in 3 pg2 might be an oversimplification assumption because the carbon use efficiency might decline with the increased maintenance respiration as trees grow taller or age mäkelä and valentine 2001 landsberg and waring 1997 a literature survey also reported that the carbon use efficiency varies with forest type and correlates with age as well as the ratio of leaf mass to total mass delucia et al 2007 additionally a more recent study about a boreal forest showed that the increment of aboveground tree biomass is only a minor fraction 9 of gross ecosystem production and is decoupled from ecosystem c input pappas et al 2020 in addition the static allometric relationship between biomass and dbh might also be in doubt if the spatial scale increases weiskittel et al 2015 and it means that parameters in the allometric equations should be carefully calibrated to suit local stand characteristics temesgen et al 2015 in view that data assimilation and the ensemble related methods are more commonly used in ecological modelling practices nowadays we would like to suggest that specific interface should be designed during model development to facilitate users to run an ensemble of model instances sequentially or parallelly and the information of the parameters or state variables could be retrieved or modified explicitly during the running of the ensemble a general framework to serve this for ecological modelling might be of great help software availability name of software 3 pg2py developers xiaodong song yu song contact address college of geomatics municipal engineering zhejiang university of water resources and electric power hangzhou 310018 china email xdsongy gmail com availability https github com xdsongy 3 pg2py software license gnu general public license v3 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was partially funded by the national natural science foundation of china no 41301484 31270588 the china scholarship council and csiro s sustainable agriculture flagship the natural science foundation of zhejiang province lq13c030007 the entrepreneurship and innovation project for high level overseas returnees in hangzhou city in 2019 and the science and technology project of the water resources department of zhejiang province rc1810 we would like to thank the anonymous reviewers for their valuable suggestions and comments 
25625,process based forest growth models have increased applications in forest ecology carbon sequestration and climate change the forest growth model physiological principles for predicting growth 3 pg has been widely used in these fields due to its moderate complexity and explicit physiologic background as a further step in this continued effort we developed 3 pg2py a python version of 3 pg2 a modified version of 3 pg with respect to water balance prediction to facilitate its extension and application to broader communities in this study the basic structure of 3 pg2py is explained and its simulation against observations is demonstrated to facilitate sensitivity and uncertainty analyses two global sensitivity analysis algorithms and an ensemble kalman filter algorithm are integrated into 3 pg2py and examples are presented additionally an interface for spatial simulation is implemented 3 pg2py is compatible with python 2 7 and open source with 3 pg2py the users can adapt the model easily to more diversified applications especially the computationally intensive ones keywords 3 pg2 forest growth model python global sensitivity analysis variance based sensitivity analysis fourier amplitude sensitivity test ensemble kalman filter 1 introduction forest is one of the most important terrestrial ecosystems forests cover about 31 percent global land area and have prominent impacts in wood production biodiversity conservation hydrological cycle social services bioenergy and climate change brown et al 2005 jandl et al 2007 díaz et al 2009 crossman et al 2011 buchholz et al 2016 fao 2020 the total carbon stock in forests was about 662 gigatons in 2020 fao 2020 it s estimated that terrestrial ecosystems provide a gigaton net carbon sink per year for atmospheric co2 and this sink mainly originates from carbon accumulation in forests although environmental disturbances e g fire drought insect outbreaks and management and dynamics could drive particular forests from a carbon sink to a source canadell and raupach 2008 forests therefore play a crucial role in regulating atmospheric co2 and could be regarded as a natural brake on climate change gibbs et al 2007 fahey et al 2010 the reforestation and increasing carbon stock at stand or landscape scales are two ways to increase carbon sinks via forestry activities canadell and raupach 2008 according to fao 2020 plantation forests composed of one or two species and intensively managed with regular spacing even aged and mainly for productive purposes cover about 3 percent of the global forest area i e 45 percent of the total area of the planted forests and the other planted forests 55 percent of all planted forests are not intensively managed and mainly provide ecosystem restoration and soil and water protection functions despite this relatively small ratio of the plantation forests to global forest area its influence on climate mitigation is not dispensable for example china alone had a net annual forest increase of 1 94 million ha in 2010 2020 fao 2020 and it s estimated that this offset about one fifth of chinese fossil fuel emissions in 2000 fang et al 2007 wang et al 2007 canadell and raupach 2008 compared with forest area estimation which could be performed by investigating forest inventory data or using remote sensing data the assessment of carbon stocks or biomass of forests under various environmental conditions and management strategies is more difficult and full of uncertainties landsberg and waring 1997 here we distinguish the static and dynamic forest carbon stock estimations and focus on the later one in the remaining part of this paper as summarized by gibbs et al 2007 the static forest carbon stock estimation can be classified into three broad approaches according to the adapted methods or suitable spatial scales i e biome average approach ground based forest inventory measurement and satellite based approach the main drawbacks of the static forest carbon stocks estimation is that it s completely empirical and has no mechanisms of forest growth under the premise of changes in management and climate its reliability may decrease landsberg and waring 1997 gupta and sharma 2019 the dynamic forest carbon estimation could be performed by either process based forest growth model or empirical model landsberg and sands 2011 coops et al 1998 neumann et al 2016 the advantage of the process based forest model lies in that it might accommodate future environmental changes more readily and explicable the process based forest growth model refers to a set of physiological procedures expressed by mathematical equations about the workings of forest ecosystem i e the interactions of different processes among varying levels and responses to environmental stimuli and its generality compared with empirical models lies in that the processes or mechanisms involved generally could be applied to all similar forest ecosystems landsberg and sands 2011 landsberg and waring 1997 gupta and sharma 2019 as pointed out by landsberg and sands 2011 the process based models could be used either as scientific tools to identify knowledge gaps and assess different processes or as practical tools however the complexity of specific models must be controlled and a pragmatic principle should be preferred if the model outputs are expected to be of practical values the physiological principles for predicting growth 3 pg first published by landsberg and waring 1997 is a relatively simple process based and stand level deterministic carbon balance model of forest growth 3 pg was originally developed to simulate the growth of even aged monospecific evergreen forests or plantations sands and landsberg 2002 paul et al 2007 potithep and yasuoka 2011 coops et al 2012 meyer et al 2017 3 pg has been applied worldwide for a range of species and regions gupta and sharma 2019 more recently it has been developed further for mixed evergreen and deciduous tree species in even or uneven aged forests forrester and tang 2016 forrester et al 2021b the original 3 pg and 3 pg2 a modified version of 3 pg with respect to water balance prediction are written in the visual basic for applications vba landsberg and sands 2011 almeida et al 2007b almeida and sands 2016a however the excel version of 3 pg 3 pg2 albeit convenient for practical users due to its comprehensible user interface as shown by excel spreadsheets has been a great obstacle for applications such as data assimilation uncertainty and sensitivity analysis parameter optimization and parallel computing song et al 2012 trotsiuk et al 2020 the common features of these specific applications usually involve 1 intensive computation e g monte carlo simulation and parameter shuffling 2 high performance computing hpc and 3 spatial simulation to resolve these technical and scientific challenges in forest growth modelling when applying this model a series versions of 3 pg have been developed to fit different applications and scientific assumptions see https 3pg forestry ubc ca software for example 3 pg spatial a spatial version of 3 pg coops et al 1998 tickle et al 2001 almeida et al 2016 3 pgmix for mixed and uneven aged evergreen and deciduous forest forrester and tang 2016 forrester et al 2021b 3 pgn 3 pg nitrogen xenakis et al 2008 and r3pg an r package wrapped fortran implementation of 3 pg to simulate monospecific as well as mixtures of evergreen and deciduous tree species in even or uneven aged stands and to calculate wood δ 13 c as well wei et al 2014 trotsiuk et al 2020 forrester et al 2021b in this study we present a new version of 3 pg2 in python programming language namely 3 pg2py which has been successfully applied in global sensitivity analysis of 3 pg2 at stand level song et al 2012 2013 it s noted as above that there are already some other available 3 pg implementations in other programming languages e g fortran and r thomas et al 2017 trotsiuk et al 2020 the 3 pg2py is intended to provide an extensible workbench in pure python for forest modelers preferred this programming language and the object oriented model structure makes it readily extensible to fit specific algorithms to address the challenges mentioned above the uncertainty analysis is an essential part of development calibration and application of the generally rich parameterized process based models including 3 pg van oijen et al 2005 thomas et al 2017 forrester et al 2021a for example trotsiuk et al 2020 integrated the morris sensitivity analysis and a bayesian calibration method into r3pg to facilitate model calibration in this study we further refined the sensitivity analysis capability of 3 pg by integrating two effective global sensitivity analysis gsa algorithms into 3 pg2py uncertainty analysis including parameter and state variable estimation and data assimilation are actually indispensable features of modelling practice nowadays we implemented a state parameter estimation algorithm by using the ensemble kalman filter enkf to meet these requirements in 3 pg2py moradkhani et al 2005b for users interested in spatial applications we also implemented an interface suitable for spatial simulation and it can be readily extended to support spatial applications by revising the corresponding initialization and output modules in 3 pg2py we developed 3 pg2py based on 3 pg2 using python programming language python is object oriented cross platform and open source with rich numerical and scientific packages 3 pg2py is free and open source and could be easily adapted to add new modules for experienced users in present study the structure and key features of 3 pg2py is explained and its applications in single plot simulation gsa analysis and state parameter estimation are demonstrated respectively the remaining part of this paper is organized as follows the brief physiological principles input output data requirements and code structure of 3 pg2py are presented in section 2 section 3 demonstrates single plot simulation by using data of two plots and schematically shows the simulation and data collection processes the new modules in 3 pg2py i e the gsa state parameter estimation and spatial simulation interface are introduced in section 4 and we conclude in section 5 2 model description and structure 2 1 model description the 3 pg2py 3 pg2 model is a simplified process based and stand level deterministic forest growth model and can be used to predict forest gross primary production water balance evapotranspiration rate stem number and partitioning of biomass to stem foliage and root in the model the canopy photosynthesis determines biomass production process to be more specific the canopy quantum efficiency qe and absorbed photosynthetically active radiation apar are used to estimate gross primary production gpp the qe is affected by multiple factors e g stand age air temperature vapor pressure deficit vpd frost available soil water soil nutritional status and canopy conductance the ratio between net primary production npp and gpp i e carbon use efficiency is assumed to be conservative for simplicity in 3 pg landsberg and sands 2011 waring et al 1998 the biomass partitioning between above ground and root is tuned by growth modifiers e g temperature soil water and stand age and soil fertility the biomass partitioning among foliage stem and root is determined by a series of allometric equations related with stem diameter at breast height dbh almeida et al 2007 landsberg and waring 1997 the outputs of 3 pg are state variables e g the foliage stem and root biomass pools available soil water and stocking and variables such as stand evapotranspiration specific leaf area sla canopy leaf area index lai net primary production npp and mean dbh the model runs in a monthly time step so monthly average meteorological data i e mean maximum and minimum air temperature vpd frost days shortwave solar radiation and rainfall should be provided the site specific parameters e g latitude soil texture maximum available soil water fertility rating and initial number of stems per hectare and species specific parameters should also be provided sands and landsberg 2002 2 2 model structure 3 pg2py is developed in an object oriented manner in python the interfaces for all the parameter settings including parameters 3 pg parameters site series under simulation siteseries site specific parameters excel sheets named by site names as listed in the siteseries sheet respectively and meteorological data silo metdata in 3 pg2py are kept the same as 3 pg for excel the xlrd module is used to read excel file in 3 pg2py so the microsoft excel is not necessary during model running the simulation results are exported to a sqlite database one table per site the sqlite database is cross platform and integrated with python by sqlite3 module but users can also revise the code to use other preferable databases the code structure of 3 pg2py could be generally classified as two parts i e the model parameter initialization part and core part fig 1 the entrance of 3 pg2py is in the file main py where the global variables soil type data standard 3 pg parameters site series list and site specific parameters are initialized sequentially in the core part of 3 pg2py i e file main module py the simulation of each site is executed in turn and the values of each output variable are stored in the database the simulated forest growth is forced by meteorological data water availability soil water and site and stand specific parameters e g soil fertility and texture stand stocking and stand age the environmental factors formulate growth stress modifiers and allometric equations and then determine the accumulation of biomass and biomass partitioning in stem foliage and root as a model mainly for the managed forest the influence of silvicultural events is also considered 3 single plot simulation in this section we demonstrate the basic usage of 3 pg2py by simulating two forest sites named as site1 and site2 with corymbia maculata and eucalyptus cladocalyx in a monthly time step respectively for simplicity the default parameter values for the species are applied the two sites are located in australia and had different number of observations and thinning events before the age of 10 years the simulation periods of site1 and site2 are 50 and 8 years respectively fig 2 same as 3 pg for excel the user can simulate any number of sites by editing the siteseries sheet in file 3pgxl parameters xls as shown in fig 2 see also fig 1 3 pg2py first initializes two sites with corresponding global and site specific parameters then executes the growth simulation for each site sequentially the values of the output variables at each step are written to a sqlite database and user could edit variable list or database table in main module py to include only the interested variables if necessary in this example 10 variables as well as stand age table 1 are selected for each site the outputs are organized as a database table with fields variables as listed in the code in fig 2 the simulated stem and foliage biomass and the mean dbh as well as the observed data and thinning events stems per hectare are plotted 4 new modules 4 1 global sensitivity analysis compared with the so called local sensitivity analysis lsa the gsa is able to explore the full parameter space and quantify the contribution of each parameter to the variations of model state variables by averaging over the variations of all target parameters saltelli et al 2008 the fourier amplitude sensitivity analysis test fast and variance based method are effective gsa methods saltelli et al 2010 xu and gertner 2011 although relatively simple 3 pg2 still contains more than 50 standard parameters as well as tens of site stand and soil specific parameters for sites with different forest species and environmental conditions the sensitivities of model state variables to parameters are very likely different so gsa is usually a necessary step in model calibration and parameter estimation practices the fast and variance based gsa algorithms are implemented in 3 pg2py to facilitate model calibration this is based on the fact that the implementation complexity of gsa algorithms has actually been a great obstacle to many modelers and this difficulty forces many users turn to simple but biased sensitivity analysis methods for example lsa the lsas estimate the sensitivity of one specific parameter by changing its value while holding the other parameters constant and it s only suitable for simple linear or additive models saltelli et al 2008 the process based models are usually non linear and have high dimensional parameter space and gsa algorithms are specifically designed to explore its full parameter space for the variance based method two sensitivity indices could be derived i e the main effect and total effect sensitivity indices respectively the main effect sensitivity index represents the partial variance contributed by a certain parameter to the total variance of a specific state variable while the total effect sensitivity index equals the main effect plus interactions with other parameters saltelli et al 2010 song et al 2012 the drawback of the variance based method is that it s computationally intensive compared with the fast method actually the results of fast are comparable to the main effect sensitivity index calculated by the variance based method song et al 2013 two examples using the variance based fig 3 and fast fig 4 methods are presented in this study respectively in fig 4 the time series sensitivity indices following the simulation process are calculated by fast it s worth to emphasize that the sensitivity analysis algorithms are not merely methods to quantify the relative sensitivities of specific state variable to parameters but also an effective means to verify whether the interactions among different modules are under the correct trajectories constrained by basic biophysiological assumptions in ecological models the fast and variance based gsa algorithms are both integrated in the file main py in 3 pg2py the user can change the targeted model parameter list and the corresponding state variables and then run the gsa the end age of simulation controls to which stand age the gsa explains and it can be changed either manually in the excel spreadsheet of 3pgxl parameters xls before running or by changing the variable plot endage value in the code during model running dynamically 4 2 state parameter estimation using ensemble kalman filter the sequential data assimilation methods are effective means to improve forecast constrained by the imprecise parameters and assumptions in the process based models by assimilating observed data moradkhani et al 2005b the enkf is a well known sequential data assimilation method suitable for nonlinear models by propagating an ensemble of updated model states from previous time step to approximate the forecast state error covariance matrix and it s capable to explicitly take all sources of uncertainty from input output and model structure into account evensen 2003 generally the enkf is used to estimate the time evolution of state variables with prespecified parameters however it might be unreliable to assume that model parameters do not change over time for example it has been found that carbon use efficiency may vary with forest type and age mäkelä and valentine 2001 time dependent parameter sensitivity analysis of 3 pg2 also implies that the influence of model parameters to state variable may evolve with time song et al 2013 so it s desirable to simultaneously assemble the temporal evolutions of parameters and state variables in the sequential data assimilation system moradkhani et al 2005a 2005b we implemented a state parameter estimation module using enkf in 3 pg2py following the algorithm proposed by moradkhani et al 2005b as a case study we demonstrate the state parameter estimation of 3 pg2py for about 15 years using pseudo observed data every half a year in site1 three parameters table 2 and fig 5 two state variables fig 6 and one prediction variable fig 7 are selected respectively the parameters are sampled from uniform distributions and 500 starting points are randomly sampled from parameter space with ensemble size of 30 it s noted that the ensemble size and hyper parameters i e the variance of noises added to parameters and output variables are crucial to the performance of the ensemble forecast and should be carefully calibrated before running of the state parameter enkf moradkhani et al 2005b the time evolution of the three parameters is shown in fig 5 it shows that although the convergences of the confidence intervals are not significant the means of ensemble members of the parameters smoothly converge to certain values despite the randomly selected start points in the parameter spaces at each step the updated parameters are used to update model states and fig 6 displays the time evolution of two state variables fig 7 shows the estimation of avdbh by assimilating the pseudo observed avdbh every half a year and the ensemble mean is very close to the observations 4 3 spatial simulation interface 3 pg2py is a stand level model from the perspective of spatialization the running of 3 pg2py can be viewed as a simulation at the scale of a single plot or pixel it means that only model initialization and output modules need to be revised if spatial simulation is preferred an interface suitable for spatial simulation is implemented in current version of 3 pg2py fig 8 it fully utilizes the advantage of object oriented model structure and can assemble any number of plots or pixels this architecture makes the spatial simulation quite flexible additionally it can be used as the basis for ensemble algorithms for example the enkf presented in section 4 2 is based on this frame in which an ensemble of state or parameter vectors can be propagated in parallel and the ensemble members can exchange information at each step the spatial scale of a single pixel is up to the user and can be determined by both of application requirement and data availability the parameters values and forcing data for each pixel can be set individually by using the observer design pattern a typical solution to propagate events to all members e g pixels in an ensemble 5 summary and conclusions 3 pg2py is a new open source python version of 3 pg2 and it s compatible with python 2 7 3 pg2py is aimed at the applications algorithms such as data assimilation uncertainty and sensitivity analysis parameter optimization and parallel computing in forest growth simulation related studies these applications usually need a model to run on cross platform and involve intensive computations in parameter shuffling but these tasks can hardly be performed by 3 pg2 we acknowledge that some other versions of this model e g r3pg trotsiuk et al 2020 can fulfill part of these requirements but 3 pg2py could find its place in providing an extensible workbench for forest modelers and the object oriented model structure makes it readily extensible to implement specific algorithms to address the challenges mentioned above we added three new modules i e the spatial simulation interface global sensitivity analysis and state parameter estimation by using enkf into 3 pg2py this serves two purposes first it demonstrates that the structure of 3 pg2py is very flexible and extensible and second it provides effective tools to assist users to perform global sensitivity and uncertainty analysis two crucial tasks in modelling practices and this will make 3 pg2py more useful with 3 pg2py the users can make full use of the readily available numerical computation resources in python community and naturally has the cross platform capability we demonstrated that this improvement in computing efficiency is crucial to solve some specific problems e g the gsa song et al 2012 2013 and we believe that 3 pg2py could ease the applications in data assimilation and model optimization for users interested in spatial simulation 3 pg2py provides a concise interface and the users can implement the procedures in parameter including forcing data initialization and state output data collections under current framework to get the full functionality we strictly followed the model structure of 3 pg2 during the development of 3 pg2py that s to say some deficiencies in 3 pg2 can still be found in 3 pg2py for example the constant npp gpp ratio in 3 pg2 might be an oversimplification assumption because the carbon use efficiency might decline with the increased maintenance respiration as trees grow taller or age mäkelä and valentine 2001 landsberg and waring 1997 a literature survey also reported that the carbon use efficiency varies with forest type and correlates with age as well as the ratio of leaf mass to total mass delucia et al 2007 additionally a more recent study about a boreal forest showed that the increment of aboveground tree biomass is only a minor fraction 9 of gross ecosystem production and is decoupled from ecosystem c input pappas et al 2020 in addition the static allometric relationship between biomass and dbh might also be in doubt if the spatial scale increases weiskittel et al 2015 and it means that parameters in the allometric equations should be carefully calibrated to suit local stand characteristics temesgen et al 2015 in view that data assimilation and the ensemble related methods are more commonly used in ecological modelling practices nowadays we would like to suggest that specific interface should be designed during model development to facilitate users to run an ensemble of model instances sequentially or parallelly and the information of the parameters or state variables could be retrieved or modified explicitly during the running of the ensemble a general framework to serve this for ecological modelling might be of great help software availability name of software 3 pg2py developers xiaodong song yu song contact address college of geomatics municipal engineering zhejiang university of water resources and electric power hangzhou 310018 china email xdsongy gmail com availability https github com xdsongy 3 pg2py software license gnu general public license v3 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was partially funded by the national natural science foundation of china no 41301484 31270588 the china scholarship council and csiro s sustainable agriculture flagship the natural science foundation of zhejiang province lq13c030007 the entrepreneurship and innovation project for high level overseas returnees in hangzhou city in 2019 and the science and technology project of the water resources department of zhejiang province rc1810 we would like to thank the anonymous reviewers for their valuable suggestions and comments 
25626,the corals of the great barrier reef gbr in australia are under pressure from contaminants including nitrogen entering the sea to provide decision support in reaching target water quality outcomes development of a nitrogen forecasting model may be useful here we propose a new technique that considers the whole gbr as a frame and treats forecasting of nitrogen as a next frame prediction task to produce spatial maps of nitrogen over the whole gbr at forecast time steps to achieve this we design an innovative deep neural network dnn inspired by the finite element fe analysis concept in our proposed method the gbr area is meshed into small elements with pre calculated stiffness matrices first next both the stiffness matrices and the nitrogen values of each element are fed into the designed dnn for element wise nitrogen prediction the final result is then gained by attaching separate outputs of each element unlike other next frame prediction models our fe dnn model generates accurate forecasts with unblurred prediction frames we demonstrate that our model is the first to provide nitrogen forecasts for the entire gbr with low mean square error mse while generating a high resolution prediction frame the proposed model is applicable to other environmental modelling applications that are governed by partial differential equations pde e g sea temperature prediction and sediment distribution forecasting nonetheless no knowledge of the underlying pdes is required to use our dnn based model our method can produce accurate forecasting predictions by leveraging existing hindcasting simulation models keywords machine learning deep neural networks finite element analysis partial differential equation total nitrogen forecasting next frame prediction great barrier reef physics informed neural network ereefs modelling suite 1 introduction the great barrier reef gbr is the world s largest coral reef system located off the east coast of queensland australia this world heritage site is facing severe threats that challenge its resilience including extreme weather events and climate change agricultural pollutants coastal activities surface runoff associated with the catchment areas etc among these threats land and agricultural activities are the main sources of pollutants from gbr catchments steven et al 2019 nutrients fine sediments and pesticides are considered to be the primary land based pollutants that significantly reduce ocean water quality waterhouse et al 2020 according to the australian and queensland government s long term sustainability plan for the gbr reef 2050 plan reef 2050 2021 excess nitrogen is particularly challenging in the gbr high rainfall flash floods numerous short river basins and the close proximity of the reef to the wet tropics of queensland mean nutrients are flushed to the reef lagoon quickly accordingly the total nitrogen is amongst the most commonly measured and monitored water quality variables worldwide in coastal and marine waters nitrogen is usually considered the primary limiting nutrient in other words there is a strong consensus that it is the limited supply of nitrogen that limits marine ecosystem productivity in most cases although phosphorus silica and iron may co limit productivity in some situations howarth and marino 2006 when the total nitrogen increases the growth and productivity of marine algae and other photosynthesising organisms increases often to the detriment of marine ecosystems this process is known as eutrophication and there is an extensive literature assessing its prevalence causes and management smith et al 1999 there is extensive evidence that the coastal waters of the gbr have been subject to some degree of eutrophication due to changes in its catchment land use since european settlement kroon et al 2012 bell et al 2014 mccloskey et al 2021 and that this has had a negative effect on gbr ecosystems de ath and fabricius 2010 macneil et al 2019 though the offshore gbr and much of the midshelf remain oligotrophic i e has low nitrogen and phosphorus concentrations in absolute terms mckinnon et al 2017 management of nitrogen loads to the gbr in order to improve gbr water quality has been the focus of major investments by state and federal governments not for profit organizations and farmers for many years kroon et al 2016 coggan et al 2021 waltham et al 2021 towards this end a greater focus on experimentation evaluation and modelling to understand future nitrogen scenarios could further support water quality programs najafzadeh et al 2019 in particular predictive models can be used to forecast and manage the high risk areas in the coral reef ecosystems waterhouse et al 2020 however implementing an accurate nitrogen predictor for the vast areas of the gbr is a challenging task nitrogen values in the gbr form a big frame matrix that vary with both spatial coordinate x y and the time one technique to handle this giant time varying frame is to transform it into a timeseries by averaging all nitrogen values on each day this technique has been employed by many predictive models for a variety of target parameters e g physical chemical and biochemical characteristics of water najafzadeh and niazmardi 2021 water quality index najafzadeh et al 2021 nitrogen uptake in crops sharifi 2020 marine environment salinity o2 no3 phosphorus silicon chlorophyll and alkalinity wen et al 2021 etc the employed timeseries forecasting models in these published works range from decision tree and multivariate regression in statistical models to support vector regression in shallow neural networks and further to the long short term memory lstm in deep neural networks for example one of the most recent models that has used this averaging technique is the fuzzy partitioning lstm model introduced by wen et al 2021 in this model the data attributes are partitioned by fuzzy c means before feeding to an lstm network for supervised learning this architecture makes the model ready for high speed distributed learning as well as inference as opposed to the above technique there is a second approach to design a next frame predictor in this approach nitrogen values of each day across the gbr form a frame the goal is to forecast future frames from the historical frames this approach is referred to as next frame prediction in parlance zhou et al 2020 while time series forecasting could be applied to predict a value for each pixel separately next frame forecasting has the great advantage of incorporating both spatial and time series information rather than considering the history of each pixel in isolation this provides a much richer source of information for each prediction it is worth mentioning that next frame prediction is a type of forecasting problem which is different from simulation problems widely carried out by hydrodynamic models huang et al 2021 standalone hydrodynamic models cannot forecast unless future boundary conditions can be reliably predicted except by coupling with a data driven surrogate model for this reason hydrodynamic models for water quality forecasting are rarely reported in the literature are mainly timeseries forecasting models and typically have high errors khan et al 2020 to the best of our knowledge all existing data driven next frame predictors in the literature treat each frame as a whole in other words they simply stack up historical 2d frames making a 3d matrix and then feed the resulting 3d matrix to their deep neural network dnn models to output a 2d prediction frame some of the commonly used dnns are recurrent neural networks wang et al 2019 3d convolutional neural network conv3d mathieu et al 2016 convolutional long short term memory convlstm hong et al 2018 guen and thome 2020 etc one of the most successful next frame predicting models in the literature is phydnet proposed by guen and thome 2020 phydnet disentangles physical knowledge described by partial differential equations from data before feeding it to the convlstm model the experiments with sea surface temperature data showed the ability of phydnet to outperform state of the art methods in ensuing sections we will apply phydnet to our nitrogen distribution dataset for comparison we will show that the main disadvantage of these next frame predictors is their low coefficient of determination r 2 in other words frames predicted by these models are blurred i e reduced r 2 to reduce their overall prediction error as measured by the mean squared error mse to address this problem we propose a new dnn inspired by the finite element analysis fe dnn by dividing the gbr study area into small elements and by introducing the so called stiffness matrices concept from the finite element analysis into the proposed fe dnn model prediction accuracy is increased while the details of data variations are preserved to investigate the performance of the proposed fe dnn model we employ it to forecast nitrogen distribution frames in the gbr from hindcast distributions provided by an existing partial differential equations pde based simulation model this distribution follows a complicated set of pdes baird et al 2020 the ereefs modelling suite steven et al 2019 provides plenty of simulated nitrogen distribution data based on biogeochemical transformations and the spatial distribution of total nitrogen across the gbr but does not forecast future values in addition there are some sparsely collected nitrogen measurements across the gbr which are useful in understanding and predicting nitrogen distribution in the gbr these criteria make nitrogen prediction a good case study for fe dnn implementation the rest of this article is organized as follows in section 2 nitrogen in the gbr will be defined and challenges in high resolution nitrogen prediction will be discussed section 3 will describe fe dnn as our proposed solution to the problem of next frame nitrogen prediction in the gbr this data is introduced in subsection 4 1 we will then evaluate the accuracy of the fe dnn model for nitrogen distribution forecasting in the rest of section 4 where a detailed investigation of both the computational complexity and the ablation properties of our model is also provided the paper is concluded in section 5 2 background and problem definition the gbr is recognized by unesco as a world heritage area of outstanding universal value due to its great cultural and natural significance and unmatched biodiversity as stated in the previous section reduced water quality since european settlement has been identified as a key threat to the health and resilience of gbr ecosystems de ath and fabricius 2010 while climate change is the single greatest threat to the world heritage status of the gbr water quality adds cumulative pressure reduces the resilience of reef ecosystems to climate change macneil et al 2019 and may be more readily subject to improvement through local management actions to support such actions it is important to be able to monitor and predict water quality on the scale of the whole gbr in this regard land sourced pollutants from farms and runoff in gbr catchments cause major damage to coral reefs among all the primary pollutants the greatest water quality risks to the gbr are from nitrogen discharges reef 2050 2021 accordingly the key component of the australias reef 2050 reef 2021 plan is to manage all nitrogen forms in gbr waters including nitrogen oxides nox ammonia ions nh 4 detrital particulates dissolved organic nitrogen living biological forms of nitrogen etc baird et al 2020 there is continual recycling between all the above forms of nitrogen therefore the total nitrogen tn is defined as the sum of all the nitrogen forms and employed in this paper as the parameter of interest tn distribution in the gbr can be simulated through a complicated set of pdes which are solved by the ereefs modelling suite the ereefs marine models are process based simulation models that predict in hindcast the past spatial and temporal distribution of physical and water quality variables in space and time as a function of environmental drivers including river discharges meteorological conditions and global ocean currents this suite of models includes components that simulate hydrodynamics sediment dynamics biogeochemical transformations of water quality variables and optical conditions in the water by numerical integration of a set of pdes steven et al 2019 the models are used to supplement sparse in situ water quality observations to support monitoring of the great barrier reef and have also been used to project how water quality might change under alternative land management scenarios to support policy decisions for gbr catchments hence they provide two types of predictions hindcasting i e prediction of past conditions and projection i e counter factual scenario analysis however ereefs does not currently provide forecasting predictions i e prediction of conditions at a specific point in future time prediction of water quality in lakes rivers and marine ecosystems has long been a focus of research and pragmatic modelling efforts this began with simple empirical and physics based models of phosphorus and nitrogen dynamics in aquatic systems and has steadily progressed over time to more and more complex coupled hydrodynamic biogeochemical ecosystem modelling systems the range and development of these models has been reviewed by ren and yang 2000 robson 2014 and lindemann et al 2017 hydrodynamic water quality models in current use are generally complex computationally intensive and have limited application in forecasting contexts more recently machine learning approaches have been adopted in water quality modelling for example ahmed et al 2019 applied the adaptive neuro fuzzy inference system radial basis function neural network and multi layer perceptron neural network to forecast time series of three water quality parameters in a river basin as a function of a range of other water quality observations while haghiabi et al 2018 compared the performance of an artificial neural network a support vector machine and a group method of data handling in a similar context most machine learning applications to date have been limited to forecasting time series of water quality at one or a few discrete locations other works have focused on hybrid approaches such as using machine learning models as surrogate or meta models for physics based water quality models or the use of machine learning to support data assimilation to imrpove the performance of physics based models one example of this approach is the work of margvelashvili et al 2013 who used error subspace emulators to assimilate remote sensing ocean color data into the ereefs marine sediment dynamics models in contrast to all previous works we employ the dnn to forecast tn distribution in the wide gbr based on the ereefs findings the tn in nearshore regions of the gbr is due mostly to river discharges on the other hand in the midshelf we can also see marine sources for tn these various nitrogen sources in the wide coverage of the gbr make the tn distribution a challenge task to predict in the current study we show how hindcast results from a process based environmental model can be used to train an fe dnn model to provide forecast predictions measured tn values in the gbr are scarce making data hungry dnn training unfeasible to address this a physics informed neural network will be designed 3 proposed model as discussed in section 1 there is no high resolution model in the literature that is able to forecast tn distribution over the gbr in our proposed model shown in fig 1 a we solve the tn forecasting problem by meshing the gbr study area into small overlapping elements to elaborate each day in n days of the input frames consists of a frame of tn values of all the meshes the tn value for each mesh is termed a pixel which represents the average tn in a 16 km2 mesh area several pixels are then grouped into a matrix to create a square element the historical element wise tn values until the present day are used to predict a pixel value for f days later where f is known as the forecasting horizon as illustrated in fig 1b the element is slid across the entire frame so that all possible pixel values can be predicted the element size shown in fig 1 is 5 5 however this size is a hyperparameter in our model that needs to be optimized as shown there is a narrow margin of pixels where tn values cannot be predicted the size of this unpredictable margin is equal to half of the element size e g 2 pixels for 5 5 elements and 3 pixels for 7 7 elements it is worth noting that classic image processing techniques for filling the marginal pixels e g padding flipping are not suitable for our tn distribution prediction problem this is mainly because every pixel in the gbr is highly dependent on its neighbors historical tn values which are chaotic asymmetric and highly dynamic skerratt et al 2019 therefore we cannot simply fill these pixels by techniques such as constant padding or symmetrical flipping alternative solutions are needed to predict these marginal pixels e g linear regression convolutional neural network lstm etc 3 1 fe dnn model to accurately predict the output tn frame from the timed input frames we design a novel dnn this network is inspired by the concept of finite element analysis fea and thus is dubbed fe dnn fea is a well known numerical method for solving boundary value problems in engineering this method is extensively used in mathematical physics simulations i e current transient response to the current transient inputs however fea cannot be readily applied to forecasting scenarios for obtaining future response to the historical inputs unless future inputs boundary conditions can be accurately estimated to address this problem our proposed fe dnn integrates the concept of fea with modern machine learning techniques to predict unknown future values table 1 provides a conceptual comparison between fe dnn and fea by using the well known linear spring problem fea starts by dividing a study area into small elements huang et al 2020 in each element the governing equation in fea i e x f f k takes both the stiffness matrix k and the excitations as input and yields target values similarly our dnn can be expressed as a system of linear equations i e p w f e k where p represents the unknown output pixel values in the predicted frame w is the known weights of the neurons e is the known input frames and k is the known stiffness matrix 1 1 during the training phase in our supervised learning p is known and w is unknown additionally the recursive approach to solve the fea is similar to the recursive gradient descent approach to solve the system of linear equations in fe dnn the stiffness matrix k in fea for the spring problem represents the elastic behavior of the underlying material we will discuss k in more detail in section 3 2 however in contrast to fea that multiplies the inversed stiffness matrix into the excitation we feed k as a separate input to our dnn i e f e k this way the fe dnn will learn the behavior of the stiffness matrix in conjunction with input tn values variations across the historical frames fig 2 illustrates the architecture of the proposed fe dnn model it takes n elements along with n stiffness matrices as input these inputs are fed into their n corresponding 2d convolutional layers conv2d the conv2d represents f e k in table 1 which merges the stiffness matrices with their relevant tn elements the resulting merged matrices then form a 3d matrix and fed to a conv3d layer finally the outputs of the conv3d layer are flatten to enter a multilayer perceptron mlp with four dense layers except for the last dense layer that uses the rectified linear unit relu activation function the rest of the mlp dense layers use sigmoid our experiments show that using relu in all dense layers enforces the lower tn bound to be 0 this makes the model lazy in truly learning the complex transformations in gbr nitrogen distribution as a result the model cannot capture tn variations which results in a lower r 2 in other words a model with relu activation functions cannot capture tn variations in the vast gbr as good as it could do with sigmoid the total number of trainable weight parameters in the proposed fe dnn model is 154 136 to avoid overfitting while training these weights five dropout points with a 20 dropping ratio are placed in layer intervals of fig 2 all the layers are equipped with the ridge regularization of l2 0 01 meanwhile the learning rate is set to 0 001 in an effort to both improve convergence of weight learning and avoid overfitting to better understand how the proposed fe dnn model works a flowchart is presented in fig 3 this flowchart covers both the training and inference phases the workflow starts by meshing the study area into pixels and completes by saving the trained model or yielding the forecasted results data flow in this diagram has a main loop to sweep the input elements locations to predict the tn values in every pixel of the output frame this loop is marked by a dashed line inside the figure 3 2 stiffness matrices in fea stiffness matrix calculation is a pre processing step of numerical modelling the stiffness matrix can be defined as an approximate solution to the underlying pdes which represent the elastic deformation of matters in accordance with both their own properties and the constant external perturbations huang et al 2020 accordingly calculating the stiffness matrix requires obtaining a solution to the complex underlying pdes of the system under consideration however in our proposed fe dnn method instead of finding an approximate solution to these complex pdes to achieve the required stiffness matrices we use existing training data to extract the variation of the output pixel in response to the changes in the input frames while the resulting matrix resembles the definition of the stiffness matrix in fea its calculation requires no knowledge of the underlying pdes to elaborate consider the linear spring problem in table 1 with two external forces f 1 f 2 and one displacement value x 1 the spring constant is k 2 0 which results in the stiffness matrix k 2 0 2 0 we simulate this problem for f 1 f 2 and x 1 in fig 4 a where f 1 and f 2 are sine functions in the presence of random gaussian noise and x 1 is the target displacement the simulation is conducted for 2 s with 20 sample points which form our training dataset by dividing this time into 4 segments with 5 samples per segment one can numerically calculate the stiffness matrix of each segment r k r as 1 k r x f k r f x matrix algebra in the form of k r f x t x x t 1 where f is a 2 5 matrix of five f 1 and f 2 samples and x is a 1 5 matrix of five x 1 samples the final stiffness matrix k of the linear spring problem can then be calculated by averaging k r as follows 2 k 1 4 r 1 4 k r the true versus calculated values of k are presented in fig 4a as can be seen from the figure the result has about 97 accuracy across the entire dynamic range considering the tn stiffness matrix in the gbr applying 1 and 2 to our tn prediction problem requires the following adjustments 1 each pixel in the targeted output frame is influenced by the past n days so we will have n stiffness matrices for each pixel 2 we can split the wide gbr tn values in time by calculating the stiffness matrices for each month of the year in this way the high dynamics of the gbr will be better captured 3 as illustrated in fig 2 in our proposed method k is calculated and provided to the model as a separate input this is unlike fea where the stiffness matrix k is mathematically multiplied by its relevant element 4 by assigning a 1 1 kernel and 1 filter to all the conv2d layers in fig 2 these layers implement e k however given the negative values in k the output of the conv2d layers are e k therefore in our proposed model unlike the original fea implementation where the stiffness matrix is calculated as k f x it is calculated by subtracting the known outputs of the model p from element e i e k e p given the above adjustments we reformulate 1 and 2 to better address the requirements of our gbr application as also shown in fig 1 consider the situation where n input elements e around the target pixel at coordinate x y are used to calculate its value p in f days after today p d f x y in month m the stiffness matrix k will be 3 k n m x y 1 n y 1 n d y d e d n x y p d f x y j n 0 n 1 where y sweeps the years of the training dataset n y is the number of training years d sweeps the days of month m n d is the total number of days in month m and j is an all one matrix of the same size as our elements as can be seen from 3 we will have n stiffness matrices for a given coordinate x y in a given month m of the year with a given forecasting horizon f in other words k n 1 m x y k n 2 m x y k 0 m x y represent the average variations of a pixel in month m in response to the element wise tn variations in the last n days it is worth mentioning that the stiffness matrix k in 3 depends on the month but is independent of both the day and year to better illustrate these calculations the normalized stiffness matrices for three random geolocations x y in the gbr are plotted in fig 4b these plots are made for the months of january may and september the element size is set to be 7 7 and the forecasting horizon f is one day the number of input frames n is equal to 3 resulting in three stiffness matrices per month these stiffness matrices are labeled as two days ago yesterday and today denoted by k 2 m x y k 1 m x y and k 0 m x y respectively 3 3 physics informed neural network to train the proposed fe dnn model one would require a large quantity of observational tn data in the gbr however the existing sparse tn measurements in gbr are insufficient for our data hungry dnn to overcome this problem a novel method termed physics informed neural network pinn zhu et al 2021 is employed the use of pinn enables us to merge scarce observational data with readily available ereefs simulation results and use both types of data to train our neural network despite its name the pinn is not a new neural network on its own right but a technique in defining a physics informed loss function which mixes pde solutions with measured values i e the ground truth therefore it can be applied to almost any neural network corresponding to a physical model that can be described by underlying pdes the pinn inspired loss function that we develop for our fe dnn model is illustrated in fig 5 using the mean squared error mse metric the output of the model in this figure is the output of our fe dnn network in fig 2 we use this output to calculate two loss functions as follows 4 l measured 1 n data t n output t n measured 2 5 l pde 1 n data t n output t n pde 2 where n data is the total number of data points tn measured is the observational tn values and tn pde is the simulated tn values obtained from ereefs the loss functions in 4 and 5 are then combined together to create the following overall loss metric 6 l 1 λ l measured λ l pde where λ is an adjustable hyperparameter we then use the loss function in 6 to train our model and to recursively optimize the unknown weights of the fe dnn network to summarize we overcome the observational data sparsity problem by integrating the pinn technique with the process of dnn training 4 results and discussions in this section we will start by introducing the measured tn data along with the pde simulation results for tn in the gbr we will then optimize the element size before proceeding to the accuracy analysis computational complexity and ablation studies 4 1 data sources the proposed fe dnn network is used to predict the tn distribution in the gbr the observational tn values i e tn measured in 4 are gathered from the gbr marine park authority marine monitoring program mmp which is led by australian institute of marine science aims aims mmp 2021 these measurements are sparsely gathered and thus are insufficient for training the proposed fe dnn model hence by integrating the pinn technique described in section 3 3 the simulated data from solving pdes are used to compensate for the scarcity of the measured data as discussed earlier in section 2 to obtain the pde solutions for the tn distribution in the gbr i e tn pde in 5 the ereefs modelling suite is employed ereefs has a regional model on a 4 km grid gbr4 which extends into the coral sea and covers the entire gbr area nci 2021 however this raw gbr4 biogeochemical model has another version which is interpolated onto a regular grid this version of the ereefs simulation data is downloadable from the aims website aims ereefs 2021 and we have therefore used it in our study the ereefs simulation data from the aims website aims ereefs 2021 is provided on a daily basis from 2011 to 2018 we divide this time span into 2011 to 2017 for the training and validation dataset and 2018 for the testing dataset to increase the model training speed we have spatially downsampled the dataset by a factor of 4 which has resulted in 16 km wide pixels as mentioned in section 3 the tn concentrations within river deltas in the gbr are so high that using the full range of the data values for processing makes variations in other areas appear insignificant even though there are significant and ecologically important variations in water quality including sediment and nitrogen concentrations throughout the nearshore regions and out to the midshelf waters to capture these variations we use the logarithmic scale for model training as follows 7 t n log log 10 t n 1 the added 1 in 7 is to avoid log 0 this equation is used both for tn scaling and for the stiffness matrix calculation it is worth mentioning that the logarithmic scaling is not required in other next frame prediction applications if the data is linearly distributed between its boundaries to better understand the nature of these ereefs simulation outputs the statistics of tn values are presented in table 2 all the data in this table are in linear scale 4 2 prediction accuracy as in the ereefs modelling suite the physical unit of tn in this paper is mgn m 3 which is the same as μgn l by contrast tn measurements in the mmp are made in μmol l so we need to convert the mmp values by multiplying them by 14 01 given that the molar mass of nitrogen is 14 01 g as stated earlier in section 3 1 the element size is a hyperparameter that needs to be optimized the effect of the element size on prediction accuracy is investigated in fig 6 the mse values in this figure are averaged per quarter of the training year of 2018 in all cases of the 3 day 5 day and 7 day forecasting horizons increasing the element size generally improves the performance by contrast a greater element size leads to more unpredictable marginal pixels as shown in fig 1b the horizontal dash lines in this figure indicate the average mse values for the 7 7 element size in the reported results of our fe dnn we have used the element size of 7 7 fig 7 demonstrates our fe dnn tn prediction results for a typical day of the first 8 months in 2018 i e for the test dataset the true values and the absolute differences between the true and predicted values are also plotted all the predictions are made using 7 7 elements with n 3 and all the absolute differences are multiplied by 1000 the results are in logarithmic scale and they show 1 day 3 day 5 day and 7 day forecasting horizons this demonstrates the ability of our model to very closely predict tn values across the entire gbr area due to the adoption of the element size of 7 7 and based on the illustration in fig 1 three marginal pixels are left unpredicted by the proposed fe dnn model these pixels are predicted in fig 7 by employing a simple linear regression model the higher error values of the regression model are obvious in the surrounding margins of this figure especially in the longer forecasting horizons for all f 1 in fig 7a f 3 in fig 7b f 5 in fig 7c and f 7 in fig 7d the 1st 2nd and 3rd days of each month are fed to the model s input the 4th 6th 8th or 10th days of the months are forecast in those forecasting horizons respectively the absolute differences i e the prediction errors spread geographically when increasing the forecasting horizon f resulting in a larger mse fig 7 shows that not only the proposed fe dnn can result in very accurate prediction it also generates unblurred output frames i e a high coefficient of determination r 2 which are not achievable by conventional next frame prediction methods both of these advantages are demonstrated in more detail in table 3 where the mse is used to measure the prediction accuracy of our proposed model while r 2 indicates the high resolution and unblurred prediction frames here the mse is as low as 3 of the test data on average and it is almost constant for all studied f values in addition the r 2 values imply that in all prediction cases we have accurately captured around 98 99 of the predicted tn variations throughout the gbr we always use a direct forecasting approach in the reported scenarios of this paper this means that we separately train the model for each forecasting horizon however it is also possible to employ the direct recursive forecasting approach in other words we can use a previously predicted frame as input to predict the next frame and so forth to investigate how well our model predicts future values compared with simple propagation of the historical input values a study is conducted in fig 8 where the mse is employed as a mathematical distance metric to measure the pairwise distances between our prediction true future values and historical input values to better comprehend this figure a new distance index di metric is defined as 8 d i dist true future values historical input values dist prediction historical input values where dist stands for the mse distance as expected the distance between future values in f days later and historical values in the current day increases with f besides by increasing the forecasting horizon our prediction broadens its distance from the input values also the prediction keeps its constant distance with the true values even for the case of a week ahead prediction i e f 7 all these desirable distancing behaviours keep the di value close to 1 0 for all forecasting horizons finally in table 4 the performance of the proposed fe dnn model with 7 7 elements is compared with both the conv3d model by mathieu et al 2016 and the convlstm phydnet model by guen and thome 2020 at the time of writing phydnet guen and thome 2020 is ranked as the best video predicting model in multiple categories papers with code all the comparisons in table 4 are conducted for the 2018 test dataset as can be seen in this table neither of conv3d nor phydnet can accurately predict the tn dynamics in the wide gbr the mse is greater than 10 for larger f values and the mean absolute error mae is always greater than one which results in r 2 90 and di 1 the fe dnn error bias values in table 4 are close to zero indicating unbiased predictions another performance metric in table 4 is the scatter index which is calculated in percentage by dividing the root mse by the mean of the true values in each day and expressing the result as a percentage the near zero scatter index of fe dnn indicates a low relative error with respect to the mean tn the reliability analysis in table 4 calculates the percentage of relative absolute errors that are less than 0 2 according to the chinese standards saberi movahed et al 2020 high reliability values show that this approach is consistently accurate u95 is another performance metric in table 4 which is a type of uncertainty metric saberi movahed et al 2020 this metric considers the 95 confidence interval and calculates the uncertainty range of models predictions the proposed fe dnn model offers the lowest uncertainty values of the models evaluated with 95 confidence we also conduct an f test in table 4 to analyze the variance of the forecasted tn the f test is a statistical test to find out whether the predictions and the true values have the same variance the null hypothesis h0 is that the variances are equal p values greater than 0 05 reject the h0 indicating that variances are not equal based on this test our model perfectly catches the variations in tn while other models cannot adapt to the rapid tn changes in the wide gbr it is worth reminding from section 1 that both the conv3d and phydnet next frame predictors treat each frame as a whole consequently they fall short against the proposed fe dnn model that borrows the finite element concept from fea and uses the modified stiffness matrices to produce accurate predictions 4 3 computational complexity the proposed fe dnn model was implemented using keras apis of tensorflow in python the model was trained on a windows machine with intel core i7 7700hq cpu nvidia geforce 1050 gpu and 16 gb ram the computational complexity is analyzed in terms of computational resource requirements as well as the running times vaz et al 2017 in this regard we evaluate our model s demand when varying the input element size this analysis for one day tn prediction in the entire gbr is illustrated in fig 9 the results are obtained during inference where the weights and biases are fixed the memory in this figure refers to local ram consumption not the gpu memory in use it also excludes the tn input and output data and only includes the model variables the small reduction in resource demand when increasing the element size is due to an increase in the number of unpredictable marginal pixels this results in a need for predicting fewer pixels by the model overall the plots reveal almost a constant demand for both the simulation time and the memory which suggest our model is efficient the time and memory demands of the proposed fe dnn model with 7 7 elements are compared with those of the tensorflow implementation of conv3d mathieu et al 2016 and the pytorch implementation of phydnet guen and thome 2020 in table 5 fe dnn has a similar memory footprint to the contribution of mathieu et al 2016 because both of them are based on the conv3d neural networks by contrast the phydnet model demands larger ram as it is based on the convlstm neural networks one day tn prediction in the entire gbr takes longer in the fe dnn model this is due to the fact that our model sweeps the study area pixel by pixel while the other two models digest the whole input tn frame at once 4 4 ablation study in this subsection an ablation study is conducted to better understand the impacts of different blocks of our model shown in fig 2 the ablation study calculates the overall accuracy of the model when leaving a target block out of the structure du 2020 to elaborate we quantify the importance of any desired block simply by omitting it from the ensemble of the proposed fe dnn the results of the ablation study are shown in table 6 using 7 7 elements over the test dataset in 2018 the mse in the first row is calculated in presence of all the blocks and it is averaged over the 12 months as expected the mse increases in subsequent rows by removing functional parts from the structural body the difference between the mse values in the first row and any other row is an indicator of the significance of the excluded block in the prediction performance of our proposed model comparing all the mse values in this table reveals the importance of the stiffness matrix in the next frame analysis removing this fea inspired parameter reduces the model accuracy by 18 moreover the overall effect of the four dense layers of mlp in fig 2 seems to be more significant than removing the conv3d layer 4 5 limitations similar to other dnn based models the proposed fe dnn is subject to some limitations these limits are applicable when using the fe dnn in other geolocations or employing it to forecast other environmental parameters some of these limitations are listed bellow our fe dnn technique does not rely on pdes or their solutions as a dnn fe dnn learns the behavior of the underlying system only by looking at the training data so the main limitation of the developed model is the availability of training data it is worth noting that access to suitable training data is one of the main limitations of any dnn model the current fe dnn model is trained to forecast the tn distribution in the gbr applying the developed model to another study area requires re training the model with local data which must be available for each pixel on a daily time step to achieve comparable results the proposed fe dnn is only suitable for any spatio temporal data that are gathered or interpolated in regular spatial nodes in regular time intervals for example the model can be applied to remotely sensed observation forecasting only if its data are regularly interpolated in both the time and spatial domains to fill gaps due to sun glint clouds or other observational quality issues though this limitation will be present for other similar forecasting models that require constant spatiotemporal training data computational time and memory resources are two important limitations of this model to be able to train the model for the entire gbr in a decent time we have downsampled the dataset into spatially 16 km wide pixels in the absence of downsampling the lack of computation resources would impose a significant problem however this problem can be addressed by using a more powerful computing unit 4 6 future directions future research can involve actions either to address the limitations discussed in section 4 5 or to enhance the capabilities of the proposed model some of these actions are discussed below fea is a numerical technique to solve large scale pdes arising in engineering and mathematical physics fea can also deal with arbitrarily shaped regions as long as a discrete representation of the region i e the meshing exists huang et al 2020 relying on the fea concept the proposed fe dnn is applicable to almost any physical or environmental next frame forecasting problem with ruling pdes while the next frame prediction of tn is carried out in this contribution and the next frame prediction of sea surface temperature is conducted by guen and thome 2020 the proposed fe dnn model can be applied in future research to many other environmental parameter such as heat transfer water flow small particle movements etc remote sensing data could be extremely useful in training our data driven dnn based model while remote sensing tn data for the wide gbr is not available ocean color algorithms have been developed to provide remote sensing observations of other water quality variables including chlorophyll a total suspended sediments secchi depth and benthic photosynthetically active radiation magno canto et al 2019 petus et al 2019 there are some published works in the literature that have tried to estimate nitrogen distribution over wide areas using correlations with other remotely sensed environmental parameters for example sarangi 2012 estimated nitrogen in southern indian waters using remotely sensed sea surface temperature and wang et al 2018 estimated nitrogen in the coastal regions of east china sea using remotely sensed sea surface salinity and sea surface reflectance having said that an accurate algorithm for retrieving tn levels at oceanic scales in optically diverse waters from remote sensing observations has yet to be developed finally training of computationally expensive dnn solutions like the fe dnn requires a variety of hardware resources this demand for computational resources can be handled by existing parallel processing techniques e g the shared memory multiprocessors or the distributed computing systems dcs jahanbakht et al 2021 gpus fpgas and multi core cpus are few examples of the shared memory parallelization techniques dcs on the other hand consist of a network of cooperating computers that offer high performance data processing using an on premise dcs or a cloud based distributed computing service like amazon aws microsoft azure etc can be the next step for fe dnn implementation research 5 conclusion inspired by the well known fea we proposed the fe dnn model for next frame prediction of physical parameters in wide spatial coordinates our model is applicable to any environmental modelling scenarios which are governed by underlying pdes we applied our novel model to the problem of tn distribution prediction in the gbr to the best of our knowledge our study is the first to use a data driven machine learning approach for nitrogen prediction in the gbr one challenge in training our dnn based model is the scarcity of observational tn data in the gbr to address this problem we employed the pinn technique to merge the large amounts of simulated data with the sparse measurement data this enabled us to successfully train our proposed fe dnn model for tn forecasting the performed analyses revealed that our next frame predictor model achieves a very high accuracy with a low prediction mse while yielding high resolution prediction frames with very high r 2 values the calculated r 2 metric was more than 98 resulting in unblurred tn prediction frames in the entire gbr we believe that our model and this study can be beneficial and support internally significant water quality programs like the australias reef 2050 reef 2021 plan this can help improve ecosystem recovery and resilience by informed decision making based on accurate prediction modelling furthermore it can be adopted by existing hindcasting simulators to provide accurate forecasting predictions software and data availability the observational tn values are gathered from the gbr marine park authority mmp which is led by australian institute of marine science aims mmp 2021 the pde solutions for the tn distribution in the gbr are obtained from the ereefs modelling suite to elaborate the ereefs regional gbr4 biogeochemical simulation data are downloaded from the aims website aims ereefs 2021 the proposed fe dnn model is implemented by keras apis of tensorflow 2 5 0 in python 3 8 funding this work is funded by the australian government research training program scholarship this research was supported partially by the australian government through the australian research council s discovery projects funding scheme project dp220101634 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the ereefs model simulations were produced as part of the ereefs project ereefs info a collaboration between the science industry endowment fund sief the commonwealth scientific industrial research organisation csiro the australian institute of marine science aims the bureau of meteorology bom and the great barrier reef foundation gbrf with support from bhp billinton mitsubishi alliance the australian and queensland governments and with observations obtained through the integrated marine observing system imos the marine monitoring program is funded by the gbr marine park authority and data provided by aims 
25626,the corals of the great barrier reef gbr in australia are under pressure from contaminants including nitrogen entering the sea to provide decision support in reaching target water quality outcomes development of a nitrogen forecasting model may be useful here we propose a new technique that considers the whole gbr as a frame and treats forecasting of nitrogen as a next frame prediction task to produce spatial maps of nitrogen over the whole gbr at forecast time steps to achieve this we design an innovative deep neural network dnn inspired by the finite element fe analysis concept in our proposed method the gbr area is meshed into small elements with pre calculated stiffness matrices first next both the stiffness matrices and the nitrogen values of each element are fed into the designed dnn for element wise nitrogen prediction the final result is then gained by attaching separate outputs of each element unlike other next frame prediction models our fe dnn model generates accurate forecasts with unblurred prediction frames we demonstrate that our model is the first to provide nitrogen forecasts for the entire gbr with low mean square error mse while generating a high resolution prediction frame the proposed model is applicable to other environmental modelling applications that are governed by partial differential equations pde e g sea temperature prediction and sediment distribution forecasting nonetheless no knowledge of the underlying pdes is required to use our dnn based model our method can produce accurate forecasting predictions by leveraging existing hindcasting simulation models keywords machine learning deep neural networks finite element analysis partial differential equation total nitrogen forecasting next frame prediction great barrier reef physics informed neural network ereefs modelling suite 1 introduction the great barrier reef gbr is the world s largest coral reef system located off the east coast of queensland australia this world heritage site is facing severe threats that challenge its resilience including extreme weather events and climate change agricultural pollutants coastal activities surface runoff associated with the catchment areas etc among these threats land and agricultural activities are the main sources of pollutants from gbr catchments steven et al 2019 nutrients fine sediments and pesticides are considered to be the primary land based pollutants that significantly reduce ocean water quality waterhouse et al 2020 according to the australian and queensland government s long term sustainability plan for the gbr reef 2050 plan reef 2050 2021 excess nitrogen is particularly challenging in the gbr high rainfall flash floods numerous short river basins and the close proximity of the reef to the wet tropics of queensland mean nutrients are flushed to the reef lagoon quickly accordingly the total nitrogen is amongst the most commonly measured and monitored water quality variables worldwide in coastal and marine waters nitrogen is usually considered the primary limiting nutrient in other words there is a strong consensus that it is the limited supply of nitrogen that limits marine ecosystem productivity in most cases although phosphorus silica and iron may co limit productivity in some situations howarth and marino 2006 when the total nitrogen increases the growth and productivity of marine algae and other photosynthesising organisms increases often to the detriment of marine ecosystems this process is known as eutrophication and there is an extensive literature assessing its prevalence causes and management smith et al 1999 there is extensive evidence that the coastal waters of the gbr have been subject to some degree of eutrophication due to changes in its catchment land use since european settlement kroon et al 2012 bell et al 2014 mccloskey et al 2021 and that this has had a negative effect on gbr ecosystems de ath and fabricius 2010 macneil et al 2019 though the offshore gbr and much of the midshelf remain oligotrophic i e has low nitrogen and phosphorus concentrations in absolute terms mckinnon et al 2017 management of nitrogen loads to the gbr in order to improve gbr water quality has been the focus of major investments by state and federal governments not for profit organizations and farmers for many years kroon et al 2016 coggan et al 2021 waltham et al 2021 towards this end a greater focus on experimentation evaluation and modelling to understand future nitrogen scenarios could further support water quality programs najafzadeh et al 2019 in particular predictive models can be used to forecast and manage the high risk areas in the coral reef ecosystems waterhouse et al 2020 however implementing an accurate nitrogen predictor for the vast areas of the gbr is a challenging task nitrogen values in the gbr form a big frame matrix that vary with both spatial coordinate x y and the time one technique to handle this giant time varying frame is to transform it into a timeseries by averaging all nitrogen values on each day this technique has been employed by many predictive models for a variety of target parameters e g physical chemical and biochemical characteristics of water najafzadeh and niazmardi 2021 water quality index najafzadeh et al 2021 nitrogen uptake in crops sharifi 2020 marine environment salinity o2 no3 phosphorus silicon chlorophyll and alkalinity wen et al 2021 etc the employed timeseries forecasting models in these published works range from decision tree and multivariate regression in statistical models to support vector regression in shallow neural networks and further to the long short term memory lstm in deep neural networks for example one of the most recent models that has used this averaging technique is the fuzzy partitioning lstm model introduced by wen et al 2021 in this model the data attributes are partitioned by fuzzy c means before feeding to an lstm network for supervised learning this architecture makes the model ready for high speed distributed learning as well as inference as opposed to the above technique there is a second approach to design a next frame predictor in this approach nitrogen values of each day across the gbr form a frame the goal is to forecast future frames from the historical frames this approach is referred to as next frame prediction in parlance zhou et al 2020 while time series forecasting could be applied to predict a value for each pixel separately next frame forecasting has the great advantage of incorporating both spatial and time series information rather than considering the history of each pixel in isolation this provides a much richer source of information for each prediction it is worth mentioning that next frame prediction is a type of forecasting problem which is different from simulation problems widely carried out by hydrodynamic models huang et al 2021 standalone hydrodynamic models cannot forecast unless future boundary conditions can be reliably predicted except by coupling with a data driven surrogate model for this reason hydrodynamic models for water quality forecasting are rarely reported in the literature are mainly timeseries forecasting models and typically have high errors khan et al 2020 to the best of our knowledge all existing data driven next frame predictors in the literature treat each frame as a whole in other words they simply stack up historical 2d frames making a 3d matrix and then feed the resulting 3d matrix to their deep neural network dnn models to output a 2d prediction frame some of the commonly used dnns are recurrent neural networks wang et al 2019 3d convolutional neural network conv3d mathieu et al 2016 convolutional long short term memory convlstm hong et al 2018 guen and thome 2020 etc one of the most successful next frame predicting models in the literature is phydnet proposed by guen and thome 2020 phydnet disentangles physical knowledge described by partial differential equations from data before feeding it to the convlstm model the experiments with sea surface temperature data showed the ability of phydnet to outperform state of the art methods in ensuing sections we will apply phydnet to our nitrogen distribution dataset for comparison we will show that the main disadvantage of these next frame predictors is their low coefficient of determination r 2 in other words frames predicted by these models are blurred i e reduced r 2 to reduce their overall prediction error as measured by the mean squared error mse to address this problem we propose a new dnn inspired by the finite element analysis fe dnn by dividing the gbr study area into small elements and by introducing the so called stiffness matrices concept from the finite element analysis into the proposed fe dnn model prediction accuracy is increased while the details of data variations are preserved to investigate the performance of the proposed fe dnn model we employ it to forecast nitrogen distribution frames in the gbr from hindcast distributions provided by an existing partial differential equations pde based simulation model this distribution follows a complicated set of pdes baird et al 2020 the ereefs modelling suite steven et al 2019 provides plenty of simulated nitrogen distribution data based on biogeochemical transformations and the spatial distribution of total nitrogen across the gbr but does not forecast future values in addition there are some sparsely collected nitrogen measurements across the gbr which are useful in understanding and predicting nitrogen distribution in the gbr these criteria make nitrogen prediction a good case study for fe dnn implementation the rest of this article is organized as follows in section 2 nitrogen in the gbr will be defined and challenges in high resolution nitrogen prediction will be discussed section 3 will describe fe dnn as our proposed solution to the problem of next frame nitrogen prediction in the gbr this data is introduced in subsection 4 1 we will then evaluate the accuracy of the fe dnn model for nitrogen distribution forecasting in the rest of section 4 where a detailed investigation of both the computational complexity and the ablation properties of our model is also provided the paper is concluded in section 5 2 background and problem definition the gbr is recognized by unesco as a world heritage area of outstanding universal value due to its great cultural and natural significance and unmatched biodiversity as stated in the previous section reduced water quality since european settlement has been identified as a key threat to the health and resilience of gbr ecosystems de ath and fabricius 2010 while climate change is the single greatest threat to the world heritage status of the gbr water quality adds cumulative pressure reduces the resilience of reef ecosystems to climate change macneil et al 2019 and may be more readily subject to improvement through local management actions to support such actions it is important to be able to monitor and predict water quality on the scale of the whole gbr in this regard land sourced pollutants from farms and runoff in gbr catchments cause major damage to coral reefs among all the primary pollutants the greatest water quality risks to the gbr are from nitrogen discharges reef 2050 2021 accordingly the key component of the australias reef 2050 reef 2021 plan is to manage all nitrogen forms in gbr waters including nitrogen oxides nox ammonia ions nh 4 detrital particulates dissolved organic nitrogen living biological forms of nitrogen etc baird et al 2020 there is continual recycling between all the above forms of nitrogen therefore the total nitrogen tn is defined as the sum of all the nitrogen forms and employed in this paper as the parameter of interest tn distribution in the gbr can be simulated through a complicated set of pdes which are solved by the ereefs modelling suite the ereefs marine models are process based simulation models that predict in hindcast the past spatial and temporal distribution of physical and water quality variables in space and time as a function of environmental drivers including river discharges meteorological conditions and global ocean currents this suite of models includes components that simulate hydrodynamics sediment dynamics biogeochemical transformations of water quality variables and optical conditions in the water by numerical integration of a set of pdes steven et al 2019 the models are used to supplement sparse in situ water quality observations to support monitoring of the great barrier reef and have also been used to project how water quality might change under alternative land management scenarios to support policy decisions for gbr catchments hence they provide two types of predictions hindcasting i e prediction of past conditions and projection i e counter factual scenario analysis however ereefs does not currently provide forecasting predictions i e prediction of conditions at a specific point in future time prediction of water quality in lakes rivers and marine ecosystems has long been a focus of research and pragmatic modelling efforts this began with simple empirical and physics based models of phosphorus and nitrogen dynamics in aquatic systems and has steadily progressed over time to more and more complex coupled hydrodynamic biogeochemical ecosystem modelling systems the range and development of these models has been reviewed by ren and yang 2000 robson 2014 and lindemann et al 2017 hydrodynamic water quality models in current use are generally complex computationally intensive and have limited application in forecasting contexts more recently machine learning approaches have been adopted in water quality modelling for example ahmed et al 2019 applied the adaptive neuro fuzzy inference system radial basis function neural network and multi layer perceptron neural network to forecast time series of three water quality parameters in a river basin as a function of a range of other water quality observations while haghiabi et al 2018 compared the performance of an artificial neural network a support vector machine and a group method of data handling in a similar context most machine learning applications to date have been limited to forecasting time series of water quality at one or a few discrete locations other works have focused on hybrid approaches such as using machine learning models as surrogate or meta models for physics based water quality models or the use of machine learning to support data assimilation to imrpove the performance of physics based models one example of this approach is the work of margvelashvili et al 2013 who used error subspace emulators to assimilate remote sensing ocean color data into the ereefs marine sediment dynamics models in contrast to all previous works we employ the dnn to forecast tn distribution in the wide gbr based on the ereefs findings the tn in nearshore regions of the gbr is due mostly to river discharges on the other hand in the midshelf we can also see marine sources for tn these various nitrogen sources in the wide coverage of the gbr make the tn distribution a challenge task to predict in the current study we show how hindcast results from a process based environmental model can be used to train an fe dnn model to provide forecast predictions measured tn values in the gbr are scarce making data hungry dnn training unfeasible to address this a physics informed neural network will be designed 3 proposed model as discussed in section 1 there is no high resolution model in the literature that is able to forecast tn distribution over the gbr in our proposed model shown in fig 1 a we solve the tn forecasting problem by meshing the gbr study area into small overlapping elements to elaborate each day in n days of the input frames consists of a frame of tn values of all the meshes the tn value for each mesh is termed a pixel which represents the average tn in a 16 km2 mesh area several pixels are then grouped into a matrix to create a square element the historical element wise tn values until the present day are used to predict a pixel value for f days later where f is known as the forecasting horizon as illustrated in fig 1b the element is slid across the entire frame so that all possible pixel values can be predicted the element size shown in fig 1 is 5 5 however this size is a hyperparameter in our model that needs to be optimized as shown there is a narrow margin of pixels where tn values cannot be predicted the size of this unpredictable margin is equal to half of the element size e g 2 pixels for 5 5 elements and 3 pixels for 7 7 elements it is worth noting that classic image processing techniques for filling the marginal pixels e g padding flipping are not suitable for our tn distribution prediction problem this is mainly because every pixel in the gbr is highly dependent on its neighbors historical tn values which are chaotic asymmetric and highly dynamic skerratt et al 2019 therefore we cannot simply fill these pixels by techniques such as constant padding or symmetrical flipping alternative solutions are needed to predict these marginal pixels e g linear regression convolutional neural network lstm etc 3 1 fe dnn model to accurately predict the output tn frame from the timed input frames we design a novel dnn this network is inspired by the concept of finite element analysis fea and thus is dubbed fe dnn fea is a well known numerical method for solving boundary value problems in engineering this method is extensively used in mathematical physics simulations i e current transient response to the current transient inputs however fea cannot be readily applied to forecasting scenarios for obtaining future response to the historical inputs unless future inputs boundary conditions can be accurately estimated to address this problem our proposed fe dnn integrates the concept of fea with modern machine learning techniques to predict unknown future values table 1 provides a conceptual comparison between fe dnn and fea by using the well known linear spring problem fea starts by dividing a study area into small elements huang et al 2020 in each element the governing equation in fea i e x f f k takes both the stiffness matrix k and the excitations as input and yields target values similarly our dnn can be expressed as a system of linear equations i e p w f e k where p represents the unknown output pixel values in the predicted frame w is the known weights of the neurons e is the known input frames and k is the known stiffness matrix 1 1 during the training phase in our supervised learning p is known and w is unknown additionally the recursive approach to solve the fea is similar to the recursive gradient descent approach to solve the system of linear equations in fe dnn the stiffness matrix k in fea for the spring problem represents the elastic behavior of the underlying material we will discuss k in more detail in section 3 2 however in contrast to fea that multiplies the inversed stiffness matrix into the excitation we feed k as a separate input to our dnn i e f e k this way the fe dnn will learn the behavior of the stiffness matrix in conjunction with input tn values variations across the historical frames fig 2 illustrates the architecture of the proposed fe dnn model it takes n elements along with n stiffness matrices as input these inputs are fed into their n corresponding 2d convolutional layers conv2d the conv2d represents f e k in table 1 which merges the stiffness matrices with their relevant tn elements the resulting merged matrices then form a 3d matrix and fed to a conv3d layer finally the outputs of the conv3d layer are flatten to enter a multilayer perceptron mlp with four dense layers except for the last dense layer that uses the rectified linear unit relu activation function the rest of the mlp dense layers use sigmoid our experiments show that using relu in all dense layers enforces the lower tn bound to be 0 this makes the model lazy in truly learning the complex transformations in gbr nitrogen distribution as a result the model cannot capture tn variations which results in a lower r 2 in other words a model with relu activation functions cannot capture tn variations in the vast gbr as good as it could do with sigmoid the total number of trainable weight parameters in the proposed fe dnn model is 154 136 to avoid overfitting while training these weights five dropout points with a 20 dropping ratio are placed in layer intervals of fig 2 all the layers are equipped with the ridge regularization of l2 0 01 meanwhile the learning rate is set to 0 001 in an effort to both improve convergence of weight learning and avoid overfitting to better understand how the proposed fe dnn model works a flowchart is presented in fig 3 this flowchart covers both the training and inference phases the workflow starts by meshing the study area into pixels and completes by saving the trained model or yielding the forecasted results data flow in this diagram has a main loop to sweep the input elements locations to predict the tn values in every pixel of the output frame this loop is marked by a dashed line inside the figure 3 2 stiffness matrices in fea stiffness matrix calculation is a pre processing step of numerical modelling the stiffness matrix can be defined as an approximate solution to the underlying pdes which represent the elastic deformation of matters in accordance with both their own properties and the constant external perturbations huang et al 2020 accordingly calculating the stiffness matrix requires obtaining a solution to the complex underlying pdes of the system under consideration however in our proposed fe dnn method instead of finding an approximate solution to these complex pdes to achieve the required stiffness matrices we use existing training data to extract the variation of the output pixel in response to the changes in the input frames while the resulting matrix resembles the definition of the stiffness matrix in fea its calculation requires no knowledge of the underlying pdes to elaborate consider the linear spring problem in table 1 with two external forces f 1 f 2 and one displacement value x 1 the spring constant is k 2 0 which results in the stiffness matrix k 2 0 2 0 we simulate this problem for f 1 f 2 and x 1 in fig 4 a where f 1 and f 2 are sine functions in the presence of random gaussian noise and x 1 is the target displacement the simulation is conducted for 2 s with 20 sample points which form our training dataset by dividing this time into 4 segments with 5 samples per segment one can numerically calculate the stiffness matrix of each segment r k r as 1 k r x f k r f x matrix algebra in the form of k r f x t x x t 1 where f is a 2 5 matrix of five f 1 and f 2 samples and x is a 1 5 matrix of five x 1 samples the final stiffness matrix k of the linear spring problem can then be calculated by averaging k r as follows 2 k 1 4 r 1 4 k r the true versus calculated values of k are presented in fig 4a as can be seen from the figure the result has about 97 accuracy across the entire dynamic range considering the tn stiffness matrix in the gbr applying 1 and 2 to our tn prediction problem requires the following adjustments 1 each pixel in the targeted output frame is influenced by the past n days so we will have n stiffness matrices for each pixel 2 we can split the wide gbr tn values in time by calculating the stiffness matrices for each month of the year in this way the high dynamics of the gbr will be better captured 3 as illustrated in fig 2 in our proposed method k is calculated and provided to the model as a separate input this is unlike fea where the stiffness matrix k is mathematically multiplied by its relevant element 4 by assigning a 1 1 kernel and 1 filter to all the conv2d layers in fig 2 these layers implement e k however given the negative values in k the output of the conv2d layers are e k therefore in our proposed model unlike the original fea implementation where the stiffness matrix is calculated as k f x it is calculated by subtracting the known outputs of the model p from element e i e k e p given the above adjustments we reformulate 1 and 2 to better address the requirements of our gbr application as also shown in fig 1 consider the situation where n input elements e around the target pixel at coordinate x y are used to calculate its value p in f days after today p d f x y in month m the stiffness matrix k will be 3 k n m x y 1 n y 1 n d y d e d n x y p d f x y j n 0 n 1 where y sweeps the years of the training dataset n y is the number of training years d sweeps the days of month m n d is the total number of days in month m and j is an all one matrix of the same size as our elements as can be seen from 3 we will have n stiffness matrices for a given coordinate x y in a given month m of the year with a given forecasting horizon f in other words k n 1 m x y k n 2 m x y k 0 m x y represent the average variations of a pixel in month m in response to the element wise tn variations in the last n days it is worth mentioning that the stiffness matrix k in 3 depends on the month but is independent of both the day and year to better illustrate these calculations the normalized stiffness matrices for three random geolocations x y in the gbr are plotted in fig 4b these plots are made for the months of january may and september the element size is set to be 7 7 and the forecasting horizon f is one day the number of input frames n is equal to 3 resulting in three stiffness matrices per month these stiffness matrices are labeled as two days ago yesterday and today denoted by k 2 m x y k 1 m x y and k 0 m x y respectively 3 3 physics informed neural network to train the proposed fe dnn model one would require a large quantity of observational tn data in the gbr however the existing sparse tn measurements in gbr are insufficient for our data hungry dnn to overcome this problem a novel method termed physics informed neural network pinn zhu et al 2021 is employed the use of pinn enables us to merge scarce observational data with readily available ereefs simulation results and use both types of data to train our neural network despite its name the pinn is not a new neural network on its own right but a technique in defining a physics informed loss function which mixes pde solutions with measured values i e the ground truth therefore it can be applied to almost any neural network corresponding to a physical model that can be described by underlying pdes the pinn inspired loss function that we develop for our fe dnn model is illustrated in fig 5 using the mean squared error mse metric the output of the model in this figure is the output of our fe dnn network in fig 2 we use this output to calculate two loss functions as follows 4 l measured 1 n data t n output t n measured 2 5 l pde 1 n data t n output t n pde 2 where n data is the total number of data points tn measured is the observational tn values and tn pde is the simulated tn values obtained from ereefs the loss functions in 4 and 5 are then combined together to create the following overall loss metric 6 l 1 λ l measured λ l pde where λ is an adjustable hyperparameter we then use the loss function in 6 to train our model and to recursively optimize the unknown weights of the fe dnn network to summarize we overcome the observational data sparsity problem by integrating the pinn technique with the process of dnn training 4 results and discussions in this section we will start by introducing the measured tn data along with the pde simulation results for tn in the gbr we will then optimize the element size before proceeding to the accuracy analysis computational complexity and ablation studies 4 1 data sources the proposed fe dnn network is used to predict the tn distribution in the gbr the observational tn values i e tn measured in 4 are gathered from the gbr marine park authority marine monitoring program mmp which is led by australian institute of marine science aims aims mmp 2021 these measurements are sparsely gathered and thus are insufficient for training the proposed fe dnn model hence by integrating the pinn technique described in section 3 3 the simulated data from solving pdes are used to compensate for the scarcity of the measured data as discussed earlier in section 2 to obtain the pde solutions for the tn distribution in the gbr i e tn pde in 5 the ereefs modelling suite is employed ereefs has a regional model on a 4 km grid gbr4 which extends into the coral sea and covers the entire gbr area nci 2021 however this raw gbr4 biogeochemical model has another version which is interpolated onto a regular grid this version of the ereefs simulation data is downloadable from the aims website aims ereefs 2021 and we have therefore used it in our study the ereefs simulation data from the aims website aims ereefs 2021 is provided on a daily basis from 2011 to 2018 we divide this time span into 2011 to 2017 for the training and validation dataset and 2018 for the testing dataset to increase the model training speed we have spatially downsampled the dataset by a factor of 4 which has resulted in 16 km wide pixels as mentioned in section 3 the tn concentrations within river deltas in the gbr are so high that using the full range of the data values for processing makes variations in other areas appear insignificant even though there are significant and ecologically important variations in water quality including sediment and nitrogen concentrations throughout the nearshore regions and out to the midshelf waters to capture these variations we use the logarithmic scale for model training as follows 7 t n log log 10 t n 1 the added 1 in 7 is to avoid log 0 this equation is used both for tn scaling and for the stiffness matrix calculation it is worth mentioning that the logarithmic scaling is not required in other next frame prediction applications if the data is linearly distributed between its boundaries to better understand the nature of these ereefs simulation outputs the statistics of tn values are presented in table 2 all the data in this table are in linear scale 4 2 prediction accuracy as in the ereefs modelling suite the physical unit of tn in this paper is mgn m 3 which is the same as μgn l by contrast tn measurements in the mmp are made in μmol l so we need to convert the mmp values by multiplying them by 14 01 given that the molar mass of nitrogen is 14 01 g as stated earlier in section 3 1 the element size is a hyperparameter that needs to be optimized the effect of the element size on prediction accuracy is investigated in fig 6 the mse values in this figure are averaged per quarter of the training year of 2018 in all cases of the 3 day 5 day and 7 day forecasting horizons increasing the element size generally improves the performance by contrast a greater element size leads to more unpredictable marginal pixels as shown in fig 1b the horizontal dash lines in this figure indicate the average mse values for the 7 7 element size in the reported results of our fe dnn we have used the element size of 7 7 fig 7 demonstrates our fe dnn tn prediction results for a typical day of the first 8 months in 2018 i e for the test dataset the true values and the absolute differences between the true and predicted values are also plotted all the predictions are made using 7 7 elements with n 3 and all the absolute differences are multiplied by 1000 the results are in logarithmic scale and they show 1 day 3 day 5 day and 7 day forecasting horizons this demonstrates the ability of our model to very closely predict tn values across the entire gbr area due to the adoption of the element size of 7 7 and based on the illustration in fig 1 three marginal pixels are left unpredicted by the proposed fe dnn model these pixels are predicted in fig 7 by employing a simple linear regression model the higher error values of the regression model are obvious in the surrounding margins of this figure especially in the longer forecasting horizons for all f 1 in fig 7a f 3 in fig 7b f 5 in fig 7c and f 7 in fig 7d the 1st 2nd and 3rd days of each month are fed to the model s input the 4th 6th 8th or 10th days of the months are forecast in those forecasting horizons respectively the absolute differences i e the prediction errors spread geographically when increasing the forecasting horizon f resulting in a larger mse fig 7 shows that not only the proposed fe dnn can result in very accurate prediction it also generates unblurred output frames i e a high coefficient of determination r 2 which are not achievable by conventional next frame prediction methods both of these advantages are demonstrated in more detail in table 3 where the mse is used to measure the prediction accuracy of our proposed model while r 2 indicates the high resolution and unblurred prediction frames here the mse is as low as 3 of the test data on average and it is almost constant for all studied f values in addition the r 2 values imply that in all prediction cases we have accurately captured around 98 99 of the predicted tn variations throughout the gbr we always use a direct forecasting approach in the reported scenarios of this paper this means that we separately train the model for each forecasting horizon however it is also possible to employ the direct recursive forecasting approach in other words we can use a previously predicted frame as input to predict the next frame and so forth to investigate how well our model predicts future values compared with simple propagation of the historical input values a study is conducted in fig 8 where the mse is employed as a mathematical distance metric to measure the pairwise distances between our prediction true future values and historical input values to better comprehend this figure a new distance index di metric is defined as 8 d i dist true future values historical input values dist prediction historical input values where dist stands for the mse distance as expected the distance between future values in f days later and historical values in the current day increases with f besides by increasing the forecasting horizon our prediction broadens its distance from the input values also the prediction keeps its constant distance with the true values even for the case of a week ahead prediction i e f 7 all these desirable distancing behaviours keep the di value close to 1 0 for all forecasting horizons finally in table 4 the performance of the proposed fe dnn model with 7 7 elements is compared with both the conv3d model by mathieu et al 2016 and the convlstm phydnet model by guen and thome 2020 at the time of writing phydnet guen and thome 2020 is ranked as the best video predicting model in multiple categories papers with code all the comparisons in table 4 are conducted for the 2018 test dataset as can be seen in this table neither of conv3d nor phydnet can accurately predict the tn dynamics in the wide gbr the mse is greater than 10 for larger f values and the mean absolute error mae is always greater than one which results in r 2 90 and di 1 the fe dnn error bias values in table 4 are close to zero indicating unbiased predictions another performance metric in table 4 is the scatter index which is calculated in percentage by dividing the root mse by the mean of the true values in each day and expressing the result as a percentage the near zero scatter index of fe dnn indicates a low relative error with respect to the mean tn the reliability analysis in table 4 calculates the percentage of relative absolute errors that are less than 0 2 according to the chinese standards saberi movahed et al 2020 high reliability values show that this approach is consistently accurate u95 is another performance metric in table 4 which is a type of uncertainty metric saberi movahed et al 2020 this metric considers the 95 confidence interval and calculates the uncertainty range of models predictions the proposed fe dnn model offers the lowest uncertainty values of the models evaluated with 95 confidence we also conduct an f test in table 4 to analyze the variance of the forecasted tn the f test is a statistical test to find out whether the predictions and the true values have the same variance the null hypothesis h0 is that the variances are equal p values greater than 0 05 reject the h0 indicating that variances are not equal based on this test our model perfectly catches the variations in tn while other models cannot adapt to the rapid tn changes in the wide gbr it is worth reminding from section 1 that both the conv3d and phydnet next frame predictors treat each frame as a whole consequently they fall short against the proposed fe dnn model that borrows the finite element concept from fea and uses the modified stiffness matrices to produce accurate predictions 4 3 computational complexity the proposed fe dnn model was implemented using keras apis of tensorflow in python the model was trained on a windows machine with intel core i7 7700hq cpu nvidia geforce 1050 gpu and 16 gb ram the computational complexity is analyzed in terms of computational resource requirements as well as the running times vaz et al 2017 in this regard we evaluate our model s demand when varying the input element size this analysis for one day tn prediction in the entire gbr is illustrated in fig 9 the results are obtained during inference where the weights and biases are fixed the memory in this figure refers to local ram consumption not the gpu memory in use it also excludes the tn input and output data and only includes the model variables the small reduction in resource demand when increasing the element size is due to an increase in the number of unpredictable marginal pixels this results in a need for predicting fewer pixels by the model overall the plots reveal almost a constant demand for both the simulation time and the memory which suggest our model is efficient the time and memory demands of the proposed fe dnn model with 7 7 elements are compared with those of the tensorflow implementation of conv3d mathieu et al 2016 and the pytorch implementation of phydnet guen and thome 2020 in table 5 fe dnn has a similar memory footprint to the contribution of mathieu et al 2016 because both of them are based on the conv3d neural networks by contrast the phydnet model demands larger ram as it is based on the convlstm neural networks one day tn prediction in the entire gbr takes longer in the fe dnn model this is due to the fact that our model sweeps the study area pixel by pixel while the other two models digest the whole input tn frame at once 4 4 ablation study in this subsection an ablation study is conducted to better understand the impacts of different blocks of our model shown in fig 2 the ablation study calculates the overall accuracy of the model when leaving a target block out of the structure du 2020 to elaborate we quantify the importance of any desired block simply by omitting it from the ensemble of the proposed fe dnn the results of the ablation study are shown in table 6 using 7 7 elements over the test dataset in 2018 the mse in the first row is calculated in presence of all the blocks and it is averaged over the 12 months as expected the mse increases in subsequent rows by removing functional parts from the structural body the difference between the mse values in the first row and any other row is an indicator of the significance of the excluded block in the prediction performance of our proposed model comparing all the mse values in this table reveals the importance of the stiffness matrix in the next frame analysis removing this fea inspired parameter reduces the model accuracy by 18 moreover the overall effect of the four dense layers of mlp in fig 2 seems to be more significant than removing the conv3d layer 4 5 limitations similar to other dnn based models the proposed fe dnn is subject to some limitations these limits are applicable when using the fe dnn in other geolocations or employing it to forecast other environmental parameters some of these limitations are listed bellow our fe dnn technique does not rely on pdes or their solutions as a dnn fe dnn learns the behavior of the underlying system only by looking at the training data so the main limitation of the developed model is the availability of training data it is worth noting that access to suitable training data is one of the main limitations of any dnn model the current fe dnn model is trained to forecast the tn distribution in the gbr applying the developed model to another study area requires re training the model with local data which must be available for each pixel on a daily time step to achieve comparable results the proposed fe dnn is only suitable for any spatio temporal data that are gathered or interpolated in regular spatial nodes in regular time intervals for example the model can be applied to remotely sensed observation forecasting only if its data are regularly interpolated in both the time and spatial domains to fill gaps due to sun glint clouds or other observational quality issues though this limitation will be present for other similar forecasting models that require constant spatiotemporal training data computational time and memory resources are two important limitations of this model to be able to train the model for the entire gbr in a decent time we have downsampled the dataset into spatially 16 km wide pixels in the absence of downsampling the lack of computation resources would impose a significant problem however this problem can be addressed by using a more powerful computing unit 4 6 future directions future research can involve actions either to address the limitations discussed in section 4 5 or to enhance the capabilities of the proposed model some of these actions are discussed below fea is a numerical technique to solve large scale pdes arising in engineering and mathematical physics fea can also deal with arbitrarily shaped regions as long as a discrete representation of the region i e the meshing exists huang et al 2020 relying on the fea concept the proposed fe dnn is applicable to almost any physical or environmental next frame forecasting problem with ruling pdes while the next frame prediction of tn is carried out in this contribution and the next frame prediction of sea surface temperature is conducted by guen and thome 2020 the proposed fe dnn model can be applied in future research to many other environmental parameter such as heat transfer water flow small particle movements etc remote sensing data could be extremely useful in training our data driven dnn based model while remote sensing tn data for the wide gbr is not available ocean color algorithms have been developed to provide remote sensing observations of other water quality variables including chlorophyll a total suspended sediments secchi depth and benthic photosynthetically active radiation magno canto et al 2019 petus et al 2019 there are some published works in the literature that have tried to estimate nitrogen distribution over wide areas using correlations with other remotely sensed environmental parameters for example sarangi 2012 estimated nitrogen in southern indian waters using remotely sensed sea surface temperature and wang et al 2018 estimated nitrogen in the coastal regions of east china sea using remotely sensed sea surface salinity and sea surface reflectance having said that an accurate algorithm for retrieving tn levels at oceanic scales in optically diverse waters from remote sensing observations has yet to be developed finally training of computationally expensive dnn solutions like the fe dnn requires a variety of hardware resources this demand for computational resources can be handled by existing parallel processing techniques e g the shared memory multiprocessors or the distributed computing systems dcs jahanbakht et al 2021 gpus fpgas and multi core cpus are few examples of the shared memory parallelization techniques dcs on the other hand consist of a network of cooperating computers that offer high performance data processing using an on premise dcs or a cloud based distributed computing service like amazon aws microsoft azure etc can be the next step for fe dnn implementation research 5 conclusion inspired by the well known fea we proposed the fe dnn model for next frame prediction of physical parameters in wide spatial coordinates our model is applicable to any environmental modelling scenarios which are governed by underlying pdes we applied our novel model to the problem of tn distribution prediction in the gbr to the best of our knowledge our study is the first to use a data driven machine learning approach for nitrogen prediction in the gbr one challenge in training our dnn based model is the scarcity of observational tn data in the gbr to address this problem we employed the pinn technique to merge the large amounts of simulated data with the sparse measurement data this enabled us to successfully train our proposed fe dnn model for tn forecasting the performed analyses revealed that our next frame predictor model achieves a very high accuracy with a low prediction mse while yielding high resolution prediction frames with very high r 2 values the calculated r 2 metric was more than 98 resulting in unblurred tn prediction frames in the entire gbr we believe that our model and this study can be beneficial and support internally significant water quality programs like the australias reef 2050 reef 2021 plan this can help improve ecosystem recovery and resilience by informed decision making based on accurate prediction modelling furthermore it can be adopted by existing hindcasting simulators to provide accurate forecasting predictions software and data availability the observational tn values are gathered from the gbr marine park authority mmp which is led by australian institute of marine science aims mmp 2021 the pde solutions for the tn distribution in the gbr are obtained from the ereefs modelling suite to elaborate the ereefs regional gbr4 biogeochemical simulation data are downloaded from the aims website aims ereefs 2021 the proposed fe dnn model is implemented by keras apis of tensorflow 2 5 0 in python 3 8 funding this work is funded by the australian government research training program scholarship this research was supported partially by the australian government through the australian research council s discovery projects funding scheme project dp220101634 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the ereefs model simulations were produced as part of the ereefs project ereefs info a collaboration between the science industry endowment fund sief the commonwealth scientific industrial research organisation csiro the australian institute of marine science aims the bureau of meteorology bom and the great barrier reef foundation gbrf with support from bhp billinton mitsubishi alliance the australian and queensland governments and with observations obtained through the integrated marine observing system imos the marine monitoring program is funded by the gbr marine park authority and data provided by aims 
25627, bottom up methods are increasingly used to assess the vulnerability of water systems to climate change central to these methods is the climate stress test where the system is subjected to various climatic changes to test for unacceptable outcomes we present a framework for climate stress testing on a monthly timestep suitable for systems whose dominant dynamic is seasonal or longer eg water resource systems with carry over storage the framework integrates multi site stochastic climate generation with perturbation methods and in built rainfall runoff modelling the stochastic generation includes a low frequency component suitable for representing multi annual fluctuations multiple perturbation options are provided ranging from simple delta change through to altered seasonality and low frequency dynamics the framework runs rapidly supporting comprehensive multi dimensional stress testing without recourse to supercomputing facilities we demonstrate the framework on a large water resource system in southern australia the matlab octave framework is freely available for download from https doi org 10 5281 zenodo 5617008 keywords climate stress test stochastic generation water resources climate change bottom up 1 introduction climate change is a key risk to water systems and recent years have seen many new methods to characterise this risk see eg review by wilby and murphy 2019 the vulnerability of a given water system to climate change depends on many factors including location specific projections of climate change and runoff the robustness of the system to these changes and the specific aspect of system performance in view eg brown and wilby 2012 nathan et al 2019 vulnerability assessments must represent each of these broad factors to inform risk management and help establish robust policy methods of climate change risk assessment can be categorized as either top down or bottom up top down is the traditional approach and uses global climate model gcm projections as the starting point in a chain of modelling steps that typically includes simulations of the rainfall runoff response and system operations eg reservoir release rules allocations etc the way the gcm projections are used in top down methods varies in many studies the bias corrected climate projections are used directly as forcing data for subsequent modelling steps eg christensen et al 2004 chiew et al 2009 garcía ruiz et al 2011 in others the distribution of gcm projections inform the distribution and or range of future climate changes tested given the potential for scenario selection to directly impact the robustness of outcomes mcphail et al 2020 there is increasing effort to ensure appropriate scenario choices giudici et al 2020 although the impact of scenario choice on management decisions themselves as opposed to metrics is still an active research area mcphail et al 2020 in contrast bottom up methods consider a range of plausible future climate sequences selected to investigate the system of interest and identify conditions which cause stress to the system stochastic methods are often used to generate future climatic sequences in bottom up studies eg brown and wilby 2012 turner et al 2014 henley et al 2019 a broad range of plausible futures may be tested with the ranges of key variables or stressors eg change in precipitation often extended beyond the envelope of gcm scenarios a so called scenario neutral approach prudhomme et al 2010 rather than starting with a gcm scenario and asking what does this scenario mean for the system the bottom up procedure starts with the system and asks what combination of future conditions cause stress to this system thus the procedure is often called a climate stress test brown and wilby 2012 having identified the combinations of future conditions that result in stakeholder defined unacceptable outcomes the probability of these outcomes may be assessed by checking what proportion of gcms contain similar conditions in their projections eg brown and wilby 2012 thus gcm projections can have an important role in bottom up assessments but the way they are used is different to the top down approach the bottom up approach has significant advantages the focus on specific system vulnerabilities is often more relevant to policy decisions brown and wilby 2012 wilby and murphy 2019 the approach facilitates systematic assessment of policy options by their robustness to future changes of varying degree eg weaver et al 2013 further as newer gcm results become available they can be rapidly mapped onto existing analyses in contrast to the top down approach which may require the entire modelling chain to be re run because of these advantages bottom up methods have been applied in many systems in the past decade including in the united states eg poff et al 2016 united kingdom eg prudhomme et al 2010 continental europe eg culley et al 2016 africa eg ghile et al 2014 asia eg yang et al 2016 and australia eg turner et al 2014 culley et al 2019 despite these advantages implementing bottom up methods presents technical challenges particularly regarding the generation of future climatic sequences which is the focus of the present framework a minority of studies use simple methods such as scaling the historic timeseries eg ghile et al 2014 françois et al 2018 but most adopt stochastic methods because they provide a greater range of possible future sequences and thus a more exhaustive stress test brown and wilby 2012 many stochastic methods are available see eg wilks and wilby 1999 srikanthan and mcmahon 2001 with some implementations specifically designed for use within bottom up assessments steinschneider and brown 2013 guo et al 2018 bennett et al 2021 key technical issues that need to be resolved when applying these methods include i how to generate different data types with appropriate cross correlations for example precipitation and temperature at a site may be correlated in time ii how to represent spatially correlated behaviour if the system is heterogeneous iii how to represent climatic fluctuations that occur at lower multi year frequencies and iv if the boundary condition being perturbed is at the hydroclimatic level precipitation temperature etc how to convert climatic sequences into streamflow accounting for non linear responses note that these are not limitations of stress testing itself but rather are broader issues not specific to stress testing the multi faceted nature of these technical issues and tasks can be daunting for new investigators possibly impeding uptake of bottom up methods particularly in budget conscious publicly funded research projects one way to lessen this impediment is to publicly release frameworks that demonstrate how all the parts of the analysis fit together and which can be applied to new systems with relatively minimal effort this is the motivation of the present article and framework here we focus on the monthly timestep which differs from the daily timestep used in most bottom up studies eg whateley et al 2014 mukundan et al 2019 freeman et al 2020 the choice of timestep depends on the context for example a study of future flooding requires a timestep of daily or shorter because floods typically occur on timescales of days to weeks however in many water resource systems particularly those involving carry over storage the dominant dynamic is seasonal or longer nathan et al 2019 in such cases the greater detail provided by a daily timestep may not translate into greater decision relevant information obtained from the stress test relative to a longer timestep helgeson et al 2020 given the significant computational burden of stochastic climate stress testing using water resource models john et al 2021c a timestep consistent with important system dynamics and no finer frees up computational resources and allows greater possibilities within the test itself wang et al 2011 these possibilities could include a more extensive test eg a greater number of stressors or longer sequences possibly allowing a more defensible assessment of uncertainty helgeson et al 2020 and or allowing the analysis to be undertaken without recourse to expensive supercomputing facilities recent studies also demonstrate the feasibility of hybrid timestep approaches where computationally expensive parts of the process are run on longer timesteps and disaggregated to shorter timesteps only where required john et al 2021a 2021c the suitability of the monthly timestep for water resources studies and the relative lack of methods on this timestep suggests a gap which we seek to fill this paper outlines a freely available and integrated framework for generating stochastic data on a monthly timestep and perturbing it for climate stress testing the framework includes both climatic data and streamflow data it generates climatic data first and includes conversion to streamflow using a built in monthly timestep rainfall runoff model system modelling eg river operations is context specific and out of scope but must be on a monthly timestep in order to be paired with this framework likewise site specific calibration of the rainfall runoff model is out of scope although we do outline the methods used in the case study in supplementary material section s5 the framework is intended to be applicable from the scale of small catchments up to large river basins approximately 101 106 km2 the paper is structured as follows section 2 provides a technical description of the framework while section 3 describes a case study application on a large water resources system in southern australia section 4 briefly discusses limitations of the framework and issues relating to the appropriate level of complexity and future research directions in reducing computational effort for climate stress testing 2 framework description 2 1 overview fig 1 provides an overview of the framework moving from historic timeseries through to stochastic data generation rainfall runoff modelling and perturbation methods to generate the hydroclimatic data sets required for stress testing the starting point is the set of historic timeseries data for the area of interest the user can optionally divide this area into portions termed sub areas in which case historic climate data spatially lumped must be provided for each sub area subdivision can useful to i create demarcations that are necessary for a given application eg upstream versus downstream of a reservoir ii divide a heterogeneous study area into more homogeneous sub areas and iii capture distinct but correlated behaviour eg in different tributaries all these principles of subdivision are demonstrated in the case study see section 3 4 the framework is designed for spatially averaged data across sub areas and as such i the user must pre process raw gauged data to obtain spatial averages prior to input into the framework ii spatial fields are not directly digestible by the framework spatial averages must be taken to produce monthly timeseries for each sub area the bulk of the framework lies in the generation of perturbed stochastic climate data steps are undertaken to first stochastically generate annual timeseries for each sub area and later disaggregate to monthly timestep although the steps for perturbation of climate are shown in fig 1 as distinct from the steps for stochastic generation in practice they are intertwined section 2 4 6 the generation of streamflow data is the final step in the framework and has two sub steps rainfall runoff modelling using a monthly rainfall runoff model and perturbation of the rainfall runoff response the generated streamflow data can be input into a model of the system of interest and a vulnerability assessment undertaken readers are directed to brown et al 2019 or wilby and murphy 2019 for guidance in these later steps and also for putting the climate generation into the context of decision making as with any stochastic technique the adopted period of historic data is important because it forms the basis for subsequent stochastic generation ie generated data is expected to have similar statistics to this data see next subsection furthermore perturbations will be relative to this data eg a perturbation of 10 will be relative to the mean over the chosen period strictly speaking it will be relative to the baseline stochastic series which should have a close but not exact match with the selected period thus users should make every effort to ensure the input data is representative of historical conditions usually longer historic timeseries are preferable as they cover a greater range and provide a better sample of past conditions 2 2 baseline stochastic climate generation the aim of stochastic data generation is to provide arbitrarily long synthetic timeseries with similar statistical properties to historical data the relevant statistical properties include averages variability on different timescales monthly annual multiannual autocorrelation and cross correlations between different locations in space here we are particularly concerned with low frequency ie multi annual or longer variability and persistence such as that driven by climate teleconnections see eg kiem et al 2004 because this may cause stress in systems with carry over storage mcmahon et al 2007 even in the absence of climate change however unless special consideration is given to low frequency behaviour stochastic schemes tend to miss this dynamic entirely thyer and kuczera 2000 srikanthan and mcmahon 2001 therefore as discussed in the next subsection we adapt an earlier scheme mcmahon et al 2008 which explicitly separates out low frequency behaviour in precipitation and generates it using a separate stochastic scheme to that used for the high frequency component please note the process of stochastic climate generation begins with p later t and pet are generated as a function of p based their historic correlations thus p is described first section 2 2 1 then t 2 2 2 and pet 2 2 4 although the final timestep of the stochastic data is monthly many of the operations are done on a longer timestep which is subject to later disaggregation section 2 2 3 the annual timestep is the default choice for the longer timestep and this article is written assuming this choice but alternative choices are possible as discussed in section 2 2 2 1 2 2 1 generation of annual precipitation p timeseries fig 2 provides an overview of the method for stochastic generation of annual precipitation the following subsections explain each step section 2 2 1 1 outlines the process of splitting the precipitation into high and low frequency components section 2 2 1 2 describes the stochastic generation of the high frequency component section 2 2 1 3 describes the model used for stochastic generation of the low frequency component and section 2 2 1 4 focusses specifically on how the low frequency model parameters are chosen to preserve relevant historical statistics and how this is tailored to each individual sub area 2 2 1 1 division of annual precipitation into high and low frequency components the tendency for low frequency dynamics to be missed by common stochastic generation techniques has prompted the development of specialized schemes to overcome this limitation eg thyer and kuczera 2000 mcmahon et al 2008 steinschneider and brown 2013 here we adapt mcmahon et al 2008 s scheme to explicitly define and separate low frequency behaviour using a variant of the empirical mode decomposition technique and stochastically generate the high and low frequency components separately note low frequency in this context means oscillations which occur with period of approximately ten years or longer but it is noted that the distinction between high and low frequency is largely under the user s control as discussed below empirical mode decomposition emd huang et al 1998 torres et al 2011 is a non parametric technique capable of analyzing non stationary timeseries such as those stemming from non linear systems the emd process iteratively removes variation from a time series highest frequencies first as shown in fig 3 in each iteration the removed variation is called an intrinsic mode function or imf and each imf has a mean of zero the sum of all imfs plus the residual gives the original time series the advantages of using emd in hydroclimatic contexts include that it can handle both nonlinear and nonstationary time series this is a major advantage over other decomposition techniques such as fourier analysis or wavelets mcmahon et al 2008 instead of the original emd algorithm we adopt complete ensemble empirical mode decomposition with adaptive noise or ceemdan torres et al 2011 colominas et al 2014 http bioingenieria edu ar grupos ldnlys metorres metorres files ceemdan v2014 m last accessed 22 oct 2021 and incorporate its code into the framework ceemdan has multiple additional features over the original emd and subsequent ensemble emd eemd that improve the robustness of imf extraction reduce information sharing referred to as mode mixing between imfs and better ensure the set of decomposition products sum exactly to the original series unlike eemd colominas et al 2012 2014 here categorisation into high and low frequency involves lumping the first n imfs and categorizing these as high frequency where n is a user specified parameter the remainder are then lumped to form the low frequency component for example mcmahon et al 2008 choose n so that high captures all intra decadal oscillations whereas low captures inter decadal oscillations deciding n is subjective the user needs to consider what might cause stress and expose vulnerabilities of the system which depends on the strength of different modes of variation in observed climate and also on the ability of the system to buffer against these variations it is important to note that emd and ceemdan have limitations which may be particularly relevant in systems where el niño southern oscillation enso variation is the dominant multi year dynamic enso oscillations typically take between four and seven years and in the shorter case the oscillations may be too rapid to be properly characterised this is because ceemdan along with other emd like algorithms works by identifying local minima and maxima in timeseries and ideally a sequence of at least five points mid low mid high mid is required for ceemdan to resolve the cycle in cases where enso dynamics are expected to expose important system vulnerabilities a shorter aggregated timestep may be required such as the six month timestep adopted by mcmahon et al 2008 most of the framework method is agnostic to this choice except that the disaggregation step section 2 2 3 would require extra care for example if the 6 month periods were nov apr and may oct historic patterns from nov apr should not be donated to may oct nor vice versa after applying ceemdan to each sub area separately the high frequency components and low frequency components are treated differently the high frequency components are stochastically generated such that each sub area is distinct but spatial cross correlations are maintained as discussed in the next subsection in contrast the low frequency components are aggregated together and largely treated as one with a specialized method of stochastic generation as discussed in section 2 2 1 3 and section 2 2 1 4 2 2 1 2 stochastic generation of high frequency component the high frequency component of annual precipitation is stochastically generated using the multi site method of matalas 1967 this method described as weakly stationary by the original authors preserves lag 1 autocorrelation at each site individually in addition to the lag 0 cross correlations between sites note in this context a site means a sub area for more detail the reader is directed to the original paper matalas 1967 and subsequent reviews such as srikanthan and mcmahon 2001 in cases where the user elects not to divide the study area into sub areas but rather treat it as a single entity the matalas method is the same as a lag 1 autocorrelation ar 1 model as with many statistical techniques the performance of the matalas method improves if the input data are approximately normally distributed to achieve this the framework provides the option to normalize the high frequency component data prior to the matalas step using a box cox transformation box and cox 1964 the box cox parameter is chosen on a per sub area basis to minimize the absolute value of the skew post generation the same parameter is used to reverse the transformation 2 2 1 3 stochastic generation of low frequency component adopted model the earlier demonstration of a similar workflow by mcmahon et al 2008 adopted an ar 1 model for generating the low frequency component of precipitation however as shown in fig 4 a and b initial testing revealed that the ar 1 model was unable to replicate the dynamics of the low frequency component of our test site even in the case where the generated timeseries has similar standard deviation and autocorrelation to the historic low frequency timeseries the ar 1 model tends to exhibit sustained departures from the mean in this case zero resulting in a small number of inappropriately long spells seen as a fat tail in the distribution in fig 4d i to make matters worse when the ar1 timeseries happens to be close to the mean its tendency for short term fluctuations leads to a large number of very short spells above and below the mean as seen by the high frequency of one to five year spells in fig 4d i for these reasons we instead adopt a broken line process mejia et al 1972 for stochastic generation of the low frequency component as the name implies a broken line process is simply a set of connected linear segments with an equal usually multi year length of time assigned to each segment and the value at each transition point being selected randomly from a normal distribution with zero mean the two parameters of the broken line process are the segment duration and the amplitude standard deviation of the process both parameters can take any positive real number the values at a given transition point are independent of all the others ie zero autocorrelation but because the segment duration is usually multiple years the annual timeseries exhibits non zero autocorrelation fig 4c importantly in this initial test a broken line process parameterised to match the standard deviation and autocorrelation matched the run length distribution much more closely than the ar 1 model did ie the most frequent run length is similar to historic and the tail is less fat than the ar 1 fig 4d ii in concept the broken line process is related to the broken line model garcia et al 1972 specifically a broken line model is composed of multiple broken line processes added together each operating at different frequencies however given the promising replication of statistics including run length a single process is considered sufficient for the present purpose that said if users wish to swap the broken line process for a different model whether the broken line model or another long memory model this would be straightforward to code 2 2 1 4 stochastic generation of low frequency component parameterisation and treatment across different sub areas as noted most aspects of the treatment of the low frequency component of precipitation are conducted in common across all sub areas for model fitting the historic low frequency component timeseries for all sub areas are combined into a single timeseries using a weighted areal average or optionally a bespoke user defined weighting the same segment duration parameter is adopted across all sub areas calculated from the number of zero crossings in the combined historic timeseries using the formula in supplementary material s1 also the same sequence of random deviates is used across all sub areas so that the generated low frequency timeseries for a given sub area varies from the others only in amplitude but not in timing ie all sub areas move together into and out of multi year wet and dry periods the justification for this is the assumption that the processes driving low frequency oscillations such as climate teleconnections are acting on all sub areas in common across the study area the amplitude parameter is chosen for each sub area separately so that the observed historic value of the hurst coefficient hurst 1951 is replicated in the baseline precipitation data the hurst coefficient measures the persistence in the timeseries high persistence means consecutive values are more dependent upon one another koutsoyiannis 2005 and thus multi year droughts will tend to be longer as will multi year runs of wet conditions note during perturbation the hurst coefficient value can be directly perturbed as one of the five stressors in the stress test in which case the perturbed value will be preserved rather than the historic see section 2 4 3 the following two clarifications may help to avoid confusion 1 for a given sub area the two values being matched are i the hurst value of the historic precipitation timeseries ie the raw observed data not the low frequency component and ii the hurst value of the stochastic timeseries after the high and low frequency components have been added together since the high frequency component is essentially just red noise the persistence of the final timeseries will tend to increase with increases in the amplitude assigned to the low frequency component in short higher amplitude means higher hurst coefficient 2 although the synthetic replicates may be hundreds or thousands of years long it is not wise to calculate hurst coefficient values over this duration because the calculations become sensitive to low frequency oscillations whose period exceeds the historic record to sidestep this issue the stochastic timeseries is divided into segments of equal length to the historic record for example if the stochastic replicate is 2000 years long and the historic record 100 years the replicate is divided into twenty fragments and a separate hurst value calculated for each fragment the key value is then the average hurst value among this set of twenty values and the solver seeks to match this value to the target hurst value after the application of emd matalas and broken line methods the result is each sub area has its own stochastic timeseries of annual precipitation the sub areas move together through multi year periods of wet and dry conditions for a given sub area the historic hurst coefficient or a perturbed value if relevant is replicated assessed over sub periods of the same length as the historic data the historic autocorrelations and cross correlations between sub areas are typically close but this is not exact the matalas method can exactly replicate these properties but the matalas method governs the high frequency component only the low frequency component method focusses on ensuring hurst values are preserved in the combined high low synthetic timeseries and this may result in the cross correlations and autocorrelations varying from historic values generally these departures are small 2 2 2 generation of annual temperature t timeseries fig 5 provides an overview of the remainder of the stochastic generation process the first step is generation of annual t timeseries for each sub area which is discussed in the next paragraph we note the following two points 1 the temperature information can be based on daily maximums daily minimums or daily means in line with user preference and 2 the process used for t ie developing a relationship with p based on historical observations and then using this to create synthetic t from synthetic p is relatively generic and could be used to generate any factor that is dependent upon p annual t is generated based on the stochastic timeseries of annual p and the historic relationship between t and p assumed to be linear often there is considerable scatter in the historic relationship so that for a given p value there is a distribution of t values as shown in fig 5 the residuals typically exhibit spatial cross correlation and autocorrelation to represent this behaviour the first step is to create a single timeseries of autocorrelated random numbers of zero mean with the autocorrelation set to the average across all sub areas ie the autocorrelation of the residuals is calculated in each sub area separately and then these numbers are averaged then for each sub area i an initial timeseries of t is generated using the historic line of best fit ii the random number sequence is scaled to have the same variance as the residuals of the historic relationship for that sub area and iii the two timeseries are added together to give the final timeseries of t it is recommended that the user check the normality of the historic residuals before applying this process this process assumes the cross correlation in the residuals is perfect it was considered that this was an appropriate simplification given that the cross correlations in p are preserved in the matalas process see above and p is the basis for t also this simple scheme for t is considered appropriate for water resources systems as they are generally less sensitive to t than to p 2 2 3 conversion from annual to monthly data conversion from annual to monthly timestep is achieved by the method of fragments eg srikanthan and mcmahon 2001 for both p and t we note that if desired other methods could also be incorporated into the framework by future users such as the non parametric k nearest neighbour knn bootstrap sampling approach of lall and sharma 1996 for each synthetic year the method of fragments randomly selects a year from history to donate its pattern for p the donated pattern is the proportion of precipitation in each month whereas for t the donated pattern is the variation of monthly t in c about the average for that year for a given synthetic year the same historical year is donor across all sub areas this maintains historic spatial cross correlations in monthly data realism is increased by restricting potential donors to historical years with similar hydroclimatology this is achieved by categorizing historical years as dry driest 33 3 of years by p wet wettest 33 3 or medium remainder synthetic years can only be assigned a pattern from a historical year in the same category note after calculating threshold values between categories based on historic data the threshold values remain unchanged throughout in the case of perturbation section 2 4 towards eg lower p more than 33 of synthetic years will be categorized as dry so that the perturbed timeseries will use patterns from historical dry years more often than historical wet years in this example case 2 2 4 generation of monthly pet timeseries monthly pet is generated via linear regression with monthly t for some locations it may be acceptable to use a single relationship across all calendar months but for others including our test case there are considerable seasonal differences in the t pet relationship from month to month for example the slope in the coolest month july is much lower than the slope in january fig 5 thus for each of the 12 calendar months a separate linear regression is used for each sub area separately for our test case most of the variability in month to month pet is explained by the calendar month so it is not considered necessary to account for scatter in the relationship for a given month and no special consideration of spatial cross correlation is given aside from applying these simple deterministic relationships for applications of the framework in locations with different et dynamics users may code a different approach such as by adding an error model like the one described in the previous sub section 2 3 simulating streamflow monthly q is generated using the wapaba model wang et al 2011 which is built into the framework other monthly rainfall runoff models could also be used if the user is willing to add the required code see eg xu and singh 1998 or topalović et al 2020 wapaba was selected from among a set of three tested monthly models because its performance over varied climatic conditions was superior not shown for the study area of the project that funded framework development the case study in section 3 wapaba has five parameters two govern the partitioning of rainfall between streamflow evapotranspiration and changes in storage α1 and α2 one determines the total capacity of the soil moisture store smax and the final two determine the quick slow flow split and drainage rate of slow flow β and k 1 respectively further information is provided in the supporting material section s4 wapaba inputs are monthly p and monthly pet appropriate model parameters are assumed to be known which is why they are listed as an input in fig 1 further information on model calibration is provided in the supplementary material s5 a key difficulty is that the sub areas adopted for stochastic generation may span multiple catchments and or be only partially gauged for example a single sub area may contain parts of multiple river catchments an example is shown for the case study in this paper see section 3 and associated figures the suggested approach depends upon the context if there are generally many gauged catchments per sub area and the problem is that no single gauge captures the whole sub area then it is suggested to identify a single gauged catchment within the sub area that is considered suitable to act as a representative catchment for the whole sub area the simulated streamflow from the representative catchment can be used in the way that best suits the case study but the simplest method would be that the simulated streamflow outputs in units of depth mm month could be assumed to apply over the whole sub area for an example of a more nuanced approach see the case study in section 3 note since synthetic q is generated using sub area wide forcing data p and pet model calibration should also use sub area wide historic forcing data p and pet see supplementary material fig s4 if sub area streamflow is ungauged the problem becomes a prediction in ungauged basins problem and an appropriate regionalisation technique must be applied to decide appropriate parameters see eg blöschl et al 2013 hrachowitz et al 2013 2 4 perturbation in the present context to perturb means to change an aspect of hydroclimate stressor for the purposes of testing the impact of the change on system performance for example the stressor mean temperature might increase by varying amounts or the stressor mean precipitation might go up or down detailed guidance on constructing a stress test experiment is beyond the scope of this paper but in summary the stressors can each be considered as individual axes in a stress testing space eg prudhomme et al 2010 while it is possible to limit the testing regime so that it focusses on one axis at a time it is more insightful to test combinations of changes this enables assessment of system performance based on any interactions between the climate stressors to achieve this bottom up studies typically cover the space via uniform sampling producing a regular grid the underlying stochastic generation framework should in principle be able to generate data corresponding to any point in this space that is any combination of stressor perturbation thus the adopted technical method for each of the five stressors must be applicable even if the other stressors are also being simultaneously perturbed and this is a key technical challenge for this framework in climate stress testing studies it is common to include only two stressors usually 1 changes in mean precipitation and 2 changes in mean temperature eg prudhomme et al 2010 turner et al 2014 whateley et al 2014 culley et al 2016 françois et al 2018 freeman et al 2020 however the rapid nature of the framework due mainly to its monthly timestep allows greater possibilities to expand the types of perturbation considered thus we have included an additional three stressors changes in low frequency behaviour of precipitation changes in precipitation seasonality and changes in rainfall runoff relationship the rationale for each stressor is given in the subsections below in general we suggest that the primary factor in selection of stressors should be to identify possible vulnerabilities of the system although stress testing studies commonly overlay gcm projections to understand future risk this does not mean that the selection of stressors should be limited to those that can be simulated by gcms for example the hydrological processes relevant to changes in rainfall runoff relationship are considerably out of scope for gcms but this stressor is still system relevant for our case study section 3 and thus should not be omitted the five stressors included in the framework have been selected based on relevance to the project that funded the frameworks development section 3 but we feel they may also be relevant in many other contexts however if they are considered irrelevant in some locations it is trivial to remove one or more stressor by simply ensuring values of perturbation are always zero for that stressor conversely it is possible in principle to add other stressors to the existing set if this is required for an application of the framework provided the user is willing to alter the framework code note in this framework the perturbations are applied to the baseline data without first enforcing that baseline data statistics exactly match historic statistics eg averages this is in line with the expectation that stochastic data should have similar but not identical statistics to historic thus strictly speaking a given perturbation is considered relative to the baseline stochastic series rather than the historic timeseries in practice the baseline timeseries generally match historic statistics well as can be judged by the figures provided with the case study section 3 however this is usually related to the length of the generated timeseries with longer synthetic timeseries more likely to provide a closer match to historic statistics due to the averaging out of short term fluctuations following the subsections on individual stressors a summary of the code s order of operations is given section 2 4 6 to clarify which perturbations are applied first finally guidance on perturbation axis limits is provided in section 2 4 7 2 4 1 stressor 1 precipitation temporal average rationale this is a first order control on hydrologic response in water systems and therefore on system performance thus stressor 1 is routinely included in climate stress tests as noted method perturbations in temporal average precipitation are applied as a proportional change that is constant across the synthetic timeseries for example if the perturbation value is 5 every value in the precipitation timeseries is multiplied by 1 05 if the value is 20 every value in the precipitation timeseries is multiplied by 0 8 2 4 2 stressor 2 temperature temporal average rationale since t is regarded as the most reliable gcm variable and since t directly influences evaporative demand it is very commonly included in climate stress tests however the influence of this stressor on system performance may vary depending on the system and the aspect of performance in view and on whether the system is water or energy limited method perturbations in temporal average temperature are applied as an absolute change in c for example if the perturbation value is 1 5 every value in the synthetic temperature timeseries is increased by 1 5 note perturbing stressor 1 or stressor 2 means historic relationships between p and t will cease to be preserved as noted in section 2 2 2 the origin of the unperturbed t timeseries is the unperturbed p timeseries which is then transformed into t via the historic regression relationship subject to noise perturbing stressor 1 and or stressor 2 causes this relationship to shift vertically laterally or both in the stochastic data relative to the historic relationship 2 4 3 stressor 3 low frequency behaviour of precipitation rationale this is important to system performance in carryover systems as outlined in section 2 2 above particularly where the main considerations are reliability of water supply future changes in the characteristics amplitude period of this behaviour may expose system vulnerabilities even in the absence of a change in average precipitation there is much diversity among gcms regarding projected future changes in low frequency oscillation such as enso eg bellenger et al 2014 but in general gcms project an increase in the frequency of strong el niño and la nina events but disagree about the strength of this change cai et al 2018 few studies have examined projections for longer interdecadal cycles for an exception see wei et al 2018 but like enso these oscillations are sensitive to ocean dynamics liu 2012 which in turn respond to climate drivers making future changes possible method as briefly mentioned in section 2 2 1 4 perturbations of the low frequency component are quantified as changes in hurst coefficient recapping 2 2 1 4 for baseline stochastic data no perturbation the amplitude parameter in the broken line process is tuned to ensure the historic hurst coefficient is matched higher amplitude means higher hurst coefficient for stressor 3 the hurst value itself is perturbed that is the target hurst is higher or lower than the historic hurst value in line with the perturbation to achieve the perturbed hurst value one possible approach would be to retune the amplitude parameter without altering the segment duration parameter this would result in oscillations of similar period but different magnitude to historic however since in reality the hurst coefficient is affected by both the period and magnitude of low frequency oscillations we allow both parameters to vary in response to perturbation according to the method given in supplementary material section s3 note stressor 3 perturbation is done early in the process before t and pet are generated from p see section 2 4 6 and fig 4 thus both t and pet exhibit low frequency behaviour in response to the low frequency behaviour of p 2 4 4 stressor 4 precipitation seasonality rationale the importance of precipitation seasonality depends on context it may be less important in some carry over storage systems as discussed above for projects concerned with stream ecology see case study section 3 seasonality of flows may be a core concern for species diversity tonkin et al 2017 particularly since some species are sensitive to flows at certain times of year as eg breeding triggers eg poff et al 1997 changes in seasonality may therefore have significant impacts on ecology particularly in unregulated systems also depending on the synchronicity of precipitation and demand changes to precipitation seasonality may also impact flow volumes eg if rain is shifted towards months of higher evaporative demand less streamflow will ensue method the seasonality stressor is based on two seasons warm and cold relevant to the temperate climate in the example case study this may be easily adapted to more tropical climates with wet and dry seasons the user needs to decide which months are in which season to allow for austral and boreal differences in seasonality perturbation values are quantified as an increase in the proportion of annual p that falls in the warm season with negative values denoting decreases thus in the case where the unperturbed p is 900 mm with a warm cold split of 300 600 a perturbation of 5 means 45 mm will be subtracted from the cold season and added to the warm season so that the new split is 345 555 note when combined with perturbation in other stressors unexpected results can occur for example a small positive perturbation in stressor 4 combined with a large negative perturbation in stressor 1 will give a decrease in warm season p even though an increase would be expected from the seasonality perturbation acting alone perturbation of seasonality is achieved through three steps 1 conversion of perturbation value into a reallocation amount in mm for example if the perturbation value was 0 08 and the perturbation was being applied to a timeseries with mean annual precipitation of 1000 mm yr this means 80 mm yr on average will be reallocated from winter to summer months 2 for each calendar month assign reallocation amounts in mm the reallocation pattern is defined as described in supplementary material s2 1 accounting for the seasonal definitions specified by the user if the pattern adopted here does not suit an application it can be easily redefined as required since it is a redistribution some months will gain and some will lose with a net change of zero note following this step one possible option would be to simply add or subtract as required the reallocation amounts directly to the timeseries for example if the result of step 2 for january were 17 mm then we could simply add 17 mm to all the january months in the synthetic timeseries this would mean that there are no longer any dry january months in the perturbed timeseries as the minimum possible value would be 17 mm an undesirable outcome for this reason step 3 is based on an alternative approach using adjustment factors 3 calculate monthly adjustment factors based on reallocation amounts for each calendar month calculate an adjustment factor such that when all instances of the month are multiplied by the factor the average change equals the absolute change from step 2 see supplementary material s2 2 this multiplicative approach preserves dry months in the original timeseries to a much greater degree than an additive approach but it causes a small water balance error in most years which is corrected by simply scaling all months in the year up or down by the required amount rarely more than 2 in our test case 2 4 5 stressor 5 relationship between rainfall and runoff rationale recent studies in australia eg saft et al 2015 and the usa avanzi et al 2020 demonstrate that it is possible for a multi year dry period to cause a shift in rainfall runoff response this means that even after we take the lower rainfall into account the streamflow during the multi year dry period is still lower than expected a climate change induced permanent reduction in future precipitation could result in a permanent shift in rainfall runoff relationship and because the processes causing the shift are poorly understood the shift is not possible to predict in advance which is why it is treated here as a stressor in its own right it could be argued that such measures are redundant since we should expect that hydrological models can simulate these transitions although some hydrological models can do so if calibrated jointly to the before and after fowler et al 2016 studies in australia indicate that these models could not have anticipated the transition in advance of it occurring that is if calibrated to the before only they do not accurately simulate the after in independent evaluation fowler et al 2018 until these issues are resolved a separate stressor may be justified to explicitly test different plausible future shifts in rainfall runoff relationship method simplistically a possible implementation for stressor 5 might be to multiply every streamflow value by a set factor like for precipitation in stressor 1 however this is inconsistent with observed behaviour during australia s millennium drought namely the observed proportional impact was much greater for dry years than wet years the framework replicates this behaviour by modifying methods from saft et al 2015 as discussed below saft et al 2015 generated scatter plots between annual p x axis and annual q y axis where the latter was transformed to linearise the relationship a shift in rainfall runoff relationship means a vertical shift on this p q plot with every year shifted by the same amount in transformed space however the effect of reversing the transform means that the actual change varies depending on the flow value for the transform saft et al 2015 applied a one parameter box cox transformation box and cox 1964 which is adopted here also however whereas saft et al 2015 allowed a different box cox λ value for each catchment here a single value is assumed to apply across all sub areas perturbation axis limits and gradations are then defined within the box cox transformed space one way to choose the λ value demonstrated in supplementary material s7 is to minimize the skew across all sub areas simultaneously practically the steps undertaken to transform a given synthetic sequence given a negative perturbation value p ie corresponding to a decrease rather than an increase are as follows these steps are shown graphically using the case study data later in the paper see section 3 for each synthetic year 1 calculate the total flow for the year ie sum the 12 monthly simulated flow values 2 apply box cox transformation to the output of 1 3 perturb by subtracting p from the output of 2 4 if the output of 3 is negative set it to 0 in the perturbed series it will be a zero flow year 5 otherwise apply reverse box cox transformation on the output of 3 6 disaggregate the output of 5 to monthly using the original flow pattern from 1 2 4 6 perturbation order of operations the order in which the perturbations take place is different to the order in which they are presented above to avoid potential confusion we explicitly list the order of operations 1 generate high frequency component of p annual 2 undertake method in section 2 2 1 and 2 4 3 which generates the low frequency component and the annual timeseries of p in such a way as to match the desired perturbed hurst coefficient thus the low frequency behaviour stressor 3 is the first perturbation in the order of operations 3 generate annual temperature from annual precipitation using the output of 2 and historic t p correlation section 2 2 2 4 perturb precipitation temporal average stressor 1 5 perturb temperature temporal average stressor 2 6 disaggregate to monthly section 2 2 3 7 perturb seasonality stressor 4 8 generate monthly pet from monthly t section 2 2 4 9 rainfall runoff modelling section 2 3 10 perturb rainfall runoff relationship stressor 5 2 4 7 guidance for setting perturbation axis limits as noted the stressors can each be considered as individual axes in a stress testing space the axis limits are thus the extrema in each direction by which the stressor in question is perturbed setting perturbation axis limits is a subjective choice that may vary by case study axis limits are often set to envelop the range of outcomes presented by gcm models see eg henley et al 2019 with additional margin on either side reflecting that future conditions may be beyond the range presented by gcms brown and wilby 2012 it is expected that this will be an appropriate method for axis 1 long term mean p axis 2 long term mean t and axis 4 seasonality of precipitation in most cases and an assessment of gcm simulations for the region in question is recommended for these axes however axis 3 low frequency behaviour of precipitation is not an area of strength for gcms although work is ongoing and newer generations of models are improving see references in section 2 4 3 above and bayr et al 2018 for long term oscillations such as the pacific decadal oscillation quality of simulations varies by continent and the simulated oscillations often have the incorrect period wei et al 2018 for the shorter enso models are often biased towards the la niña state while too weak atmospheric feedbacks can cause quite different enso dynamics to observed bayr et al 2018 p3171 given these critiques it seems reasonable to refer to independent information when setting the limits of axis 3 given axis 3 is defined as a change in hurst coefficient a useful starting point might be published values for observed hurst coefficients for the region in question but also at wider spatial scales continental or global to characterise the limits of observed hurst values gcms provide no information about changes in rainfall runoff relationships so axis 5 limits must be decided based on hydrological factors if the study region has seen a historic change in rainfall runoff relationship this may provide a useful reference point see case study section 3 in cases where no shift has occurred historically it is difficult and subjective to decide the range of future values to test one possible route would be to adopt identical limits to the case study herein section 3 but since axis 5 limits are contingent on the adopted box cox λ value adopting the case study limits assumes the λ chosen here is a suitable choice this may be the case if the skew of the transformed streamflow is not significantly different from zero at any sub area see testing methods presented in supplementary material s7 3 case study application 3 1 the goulburn broken campaspe loddon gbcl system we demonstrate the framework on south east australia s 40 000 km2 goulburn broken campaspe loddon gbcl system fig 6 a this region is made of up the traditional lands of the yorta yorta taunarung and dja dja wurrung clans part of the murray darling basin australia s most important agricultural region gbcl rivers supply approximately 12 of the basin s water from approximately 2 of the area the system comprises a main regulated waterway the goulburn river impounded by a large carry over reservoir lake eildon and supplemented by smaller impounded waterways campaspe loddon and broken rivers connected together by transfer channels not shown in fig 6 due to the murray darling basin s water trading scheme water impounded within the goulburn system is used over a wide area and is often traded as far as 700 km away the main water use is agricultural irrigation but the system also provides municipal supply for numerous towns gbcl rivers support significant aquatic habitats including for turtles platypus fish such as the critically endangered murray cod and ecologically significant riparian forests koster et al 2012 horne et al 2020 federal and state governments have invested significantly in water entitlements purchased for environmental use skinner and langford 2013 and this water is actively managed and released for environmental benefit doolan et al 2017 which may include objectives to inundate key wetlands or provide flow events that trigger fish spawning at specific times of year often this is achieved by supplementing topping up ecologically important flow events originating in unregulated tributaries watts et al 2011 docker and johnson 2017 the stress test arises from a study of the vulnerability of environmental water management to climate variability and change the full study encompasses all four rivers goulburn broken campaspe and loddon so this is the extent of the climate data generation fig 6b for the hydrological components of this demonstration we focus on the goulburn river only fig 6a as an example of a regulated carry over system lake eildon s capacity 3 106 m3 is more than double the historic median annual inflow downstream of lake eildon the regulated portion of the goulburn river is divided into four reaches see triangles in fig 6 by environmental flow managers which require consideration in the setup of the stress test note the reach division is an arbitrary feature of our case study users are encouraged to sub divide their study area in whichever way best suits their study area and serves their application see also section 3 4 1 as described in john et al 2021c the streamflow outputs of the framework are passed to a monthly timestep river systems model which represents the operation of the dams and transfer channels the vulnerability of the system to climate change and variability is assessed via two criteria supply reliability an output of the river systems model and ecological health an output of associated ecological models forced with outputs of the river systems model here we briefly summarise the results of this vulnerability assessment with further detail provided in john et al 2021c 3 2 data gridded historic precipitation and temperature data are from the australian water availability project awap jones et al 2009 note the adopted formulation for temperature is tmax the maximum daily temperature gridded potential evapotranspiration data are from the scientific information for land owners project silo jeffrey et al 2001 of the pet options provided by silo we adopt morton s wet environment evaporation over land morton 1983 because the main purpose of pet for this study is to force the rainfall runoff model and this formulation has been used in prior rainfall runoff modelling studies in this region eg fowler et al 2020a streamflow data are from the australian bureau of meteorology s hydrologic reference stations project turner et al 2012 as compiled in the camels aus dataset fowler et al 2020b information on storage capacity is from https www g mwater com au last accessed october 22 2021 3 3 stress test set up selection of stressors earlier section 2 4 a rationale for inclusion of each of the five stressors was given in general terms the specific reasons cited for each stressor are all true for the gbcl case study for example stressor 3 low frequency behaviour is important to system performance because local climate variability is relatively high on a global scale leading to spells of relatively higher cumulative deficit below the median eg peel et al 2005 while the large size of lake eildon buffers the system against short term up to five years variation longer scale oscillations may go beyond buffering capacity and so reveal system vulnerabilities regarding stressor 4 precipitation seasonality many regulated systems may be insensitive to this stressor because the storage mitigates against seasonal lows in the gbcl system seasonality is important regardless because unregulated tributaries generate ecologically important flow events at specific times of year as mentioned above in addition seasonality of flows affects announcement of seasonal volumes of water allocation which can in turn influence system operation and water trade lastly regarding stressor 5 changes in rainfall runoff relationship the 13 year millennium drought 1997 2009 saw changes in rainfall runoff relationship for many gbcl waterways including the campaspe river loddon river and unregulated tributaries of the goulburn river eg saft et al 2015 the streamflow in some waterways was approximately 70 below the expected flow given the observed rainfall some catchments have recovered returning to their earlier relationship between p and q whereas others seemingly remain in a low flow state peterson et al 2021 these recent flow reductions are an important issue for project partners and the method needs to allow for the possibility that further future shifts in rainfall runoff relationship may occur in line with the above all five of the stressors in the framework are adopted in the case study table 1 outlines how the axis limits are set with various factors taken into consideration 3 4 stress test set up spatial and rainfall runoff modelling considerations 3 4 1 division into sub areas as noted in section 2 1 there are at least three reasons why sub division of the study area is useful in a given application below we discuss how each reason relates to this case study users should note that the following comments are case specific and do not imply a general rule other applications may be more straightforward reason 1 create demarcations that are necessary for a given application eg upstream versus downstream of a reservoir since the unregulated tributaries of the goulburn river are instrumental in providing environmental flows it is not acceptable within this application to lump them together with the regulated tributaries ie the area upstream of lake eildon similar considerations apply to the other reservoirs although they are not in focus here thus the downstream boundaries of some sub areas have been drawn at the major reservoirs on the goulburn campaspe and loddon rivers reason 2 divide a heterogeneous study area into more homogeneous sub areas an example of applying this principle based on climate data is the distinction between sub areas b and c this distinction makes spatially lumped averages meaningful because they are more representative of underlying conditions further it improves confidence in rainfall runoff modelling by avoiding unreasonable requirements of models ie the requirement to represent many contrasting areas simultaneously we also apply this principle based on similarity of hydrological response for example fig 6c shows how a small portion marked in light green of lake eildon s catchment area is grouped with areas outside this is because this portion is part of the strathbogie ranges region d which are known to behave differently hydrologically compared to nearby catchments similar considerations apply to the boundary between sub areas c and e because boundaries based on similarity do not follow reach inflow catchment area boundaries fig 5c it is necessary to apply a post hoc step where flows from sub areas are added together in appropriate weighted averages to provide total reach inflows see supplementary material s4 while the last step may be inconvenient the workflow overall merely reflects the varied hydroclimate of the study area fig 6b lastly the large flat and dry area in the north of the study area called the riverine plain sub area g requires special consideration despite its large size this area contributes very little streamflow and is lumped together as a single unit reflecting its relative homogeneity reason 3 capture distinct but correlated behaviour eg in different tributaries this principle has already been touched on above when it was noted that the unregulated and regulated tribuaries need to be distinct rather than lumped for this case study the principle is also important among the unregulated tributaries for example inflows to reaches 1 and 4 are largely determined by sub areas b and d respectively keeping these sub areas distinct allows for a more nuanced representation capturing how these tributary inflows combine to provide flows for the ecologically important downstream reaches 3 4 2 rainfall runoff modelling and representative catchments the inbuilt wapaba rainfall runoff model is adopted for streamflow generation wang et al 2011 a potential difficulty is that none of the sub areas are gauged in their entirety although all of them have multiple gauged subcatchments with good quality records and stationarity of anthropogenic impact as indicated by their inclusion in the camels aus dataset fowler et al 2020b the solution is to select one of these subcatchments where possible one whose hydroclimate is close to average for the sub area and to assume it is representative of the whole sub area scaling simulated flows as required to represent the whole sub area since the sub area boundaries are intentionally set to enclose similar regions this method provides an adequate solution normally an approach like this might suffer from issues with flow routing but these issues are moot due to the monthly not daily timestep selected representative catchments for each sub area are listed in supplementary material s4 the models are calibrated using a method fowler et al 2016 which equally weights the performance in wet and dry periods see supplementary material s5 for more information this method uses the kling gupta efficiency kge gupta et al 2009 as the measure of model performance the kge considers the model s ability to match three relevant aspects for water resource and ecological modelling namely i the long term average flow ii the variability in flows and iii flow timing as measured by temporal correlation although the kge s focus on high flows has led some studies to adopt it blended together with a low flow metric eg knoben et al 2020 trotter et al 2021 these studies used daily models given monthly streamflows are less skewed than daily the tendency for undue focus on high flows is diminished 3 5 results 1 separation into high and low frequency components for brevity we show only highlights of the case study application but the user can interrogate the results set by downloading the framework and running the example the first such highlight is the separation of annual p into high and low frequency components fig 7 rather than showing each intrinsic mode function imf see fig 3 only the combined high and combined low frequency sequences are shown in this case the distinction between high and low frequency is made between imfs 2 and 3 broadly this gives a separation into inter decadal and intra decadal variation which is considered appropriate since the capacity of lake eildon 3 106 ml is sufficient to buffer the system against short 5 year variations but is less robust to interdecadal fluctuations the resulting low frequency timeseries has 11 zero crossings in 107 years as noted giving an average period of approximately 20 years despite the varied hydroclimate between sub areas the low frequency sequences tend to enter multi year dry periods and wet periods at the same time this means that the area weighted average fig 7d note sub area g is excluded from the weighting because it is so dry as to contribute little streamflow yet so large as to dominate the weightings is broadly representative of each of the underlying sequences recall that the mismatches in amplitude between the adopted combined timeseries and a given sub area timeseries are later resolved by the hurst coefficient matching procedure section 2 2 1 4 and 2 4 3 the mismatches in timing with which the different sub areas undergo multi year dry and wet periods does cause some issues with generated data as discussed in the limitations section section 4 1 3 6 results 2 stochastic generation for baseline no perturbation we now proceed to evaluate the quality of baseline stochastic outputs no perturbation based on a replicate length of 3000 years the evaluation is done in two parts an evaluation of seasonality fig 8 and an evaluation of the frequency distribution of annual flows and multi annual flow sums fig 9 note to avoid conflating the comparison of seasonality with the assessment of bias fig 8 only shows percentages in each month rather than absolute values fig 8 reveals an excellent match in seasonality across p tmax and pet a c and a good match for q d note that these comparisons are for sub area b as an example but the results are broadly consistent with other sub areas the match in seasonality in p and tmax is unsurprising given the method directly adopts historic within year patterns while the seasonal pattern for pet is also well matched the variability within each month is underestimated which is consistent with the decision not to explicitly represent the variability around the t pet relationship for each calendar month section 2 2 4 plot d relates to the representative catchment for sub area b this plot thus reflects the ability of the wapaba model to represent seasonal dynamics in this example catchment the comparison reveals underestimation during the driest part of the year and over variable november and december flows but these issues are relatively minor and overall the model seasonal performance is good further information on wapaba calibration performance across all sub areas including a comparison during the historic period is given in supplementary material s6 changing focus now to assess the match in annual streamflow volumes fig 9a indicates the stochastic replicates show very little bias and a similar frequency distribution to historic whereas fig 8 focused on performance in a single example sub area fig 9a assesses performance over a wide area because it compares aggregated flows across the four key reaches of the goulburn river downstream of lake eildon thus the pleasing match reflects performance across the five sub areas that provide reach tributary inflows which represents significant climatic variability see fig 6b the next consideration is whether multi year dry periods and wet periods are well matched fig 9b is based on the same information as fig 9a but the streamflow sequences are accumulated into 5 year rolling sums prior to the analysis overall the stochastic data matches the historic 5 year rolling sums well the lower tail of the distribution is particularly well matched with the median line among the replicates following the historic relatively closely and the lower limit indicating that the stochastic data has five year droughts more severe than those in the historic record as we would expect for a 3 000 year sequence the match at the upper end is less close with a persistent 5 10 underestimation of five year wet sequences in the 10th to 30th percentile range this may also reflect the limits of the stochastic workflow ie ceemdan based split followed by broken line process to replicate the way the years are organised into sequences however considering the many aspects of the stochastic method and the varied hydroclimatic conditions represented the overall result in fig 9b is very favourable 3 7 results 3 changes in rainfall runoff relationship the fifth perturbation stressor change in rainfall runoff relationship is an innovative part of this study so we briefly focus on this aspect of the case study parts of the case study region have experienced historic shifts in rainfall runoff relationship and fig 10 a shows two such catchments specifically the representative catchments for sub areas c and e the latter being the one with the greatest shift among all the sub areas and one catchment that has remained stable sub area a fig 10b shows the practical workflow to shift an existing synthetic monthly sequence down by 15 units in the box cox transformed space recall that the axis varies from 15 units to 50 units table 1 it is hoped that this visual demonstration of the steps helps to clarify the process 3 8 results 4 perturbation of stochastic replicates this subsection examines changes in stochastic data after perturbation fig 11 a to e repeat the seasonality and streamflow volume frequency curves from figs 8 and 9 for each perturbation stressor in each case a higher risk red and lower risk blue perturbation is performed and the result is compared with the baseline except for temperature which is not expected to decrease so a lower risk scenario is not considered fig 10f shows a combination with each stressor exhibiting a small perturbation towards higher risk perhaps unsurprisingly the highest impact scenario in terms of streamflow volumes is the stressor 1 30 reduction in mean annual precipitation which reduces streamflow by 80 90 the next highest is the stressor 5 reduction in rainfall runoff relationship by 25 units which reduced streamflow volumes by 30 and by as much as 50 in drier years for stressor 4 seasonality of precipitation the shift from cold month precipitation to warm month precipitation is reflected in both the seasonal streamflow plots but also in a decrease in average streamflow because more precipitation is falling concurrently with high evaporative demand and thus a larger proportion goes to evapotranspiration the stressor 3 results low frequency behaviour indicate minimal impact on long term average streamflow as expected but the steeper gradient fig 1c iii indicates a more variable climate with larger multi year oscillations around the mean the final test showing a combination of small perturbations in all axes fig 11f is a thought provoking result because none of the axes are very far perturbed yet the streamflow volumes reduce by approximately half thus interactions between the axes are important and may dominate how climate change impacts on water resources a theme that is explored elsewhere john et al 2021b it suggests the value of frameworks such as this one that can explore multiple aspects of climatic change simultaneously and in a systematic way 3 9 results 5 vulnerability assessment for case study for completeness we also summarise the results of the vulnerability assessment for this case study but is noted that the system and ecological modelling required is out of scope of the framework s publicly released code fig 12 shows the system response to climatic changes in terms of changes in allocation reliability for high reliability shares fig 12a and ecological health for large bodied fish fig 12b note that john et al 2021c also assessed a range of other aspects eg low reliability water shares ecological endpoints such as river dependent vegetation and small bodied fish the colour gradients in fig 12 indicate greatest sensitivity to two of the five axes axis 1 plong term average and axis 5 rainfall runoff relationship axis 2 t also has a clear influence on outcomes but is secondary to the first two the insensitivity to axis 4 seasonality is because regulated environmental releases can offset changes in seasonality of inflows thus when repeated for unregulated locations the vulnerability assessment shows an increased sensitivity to seasonality not shown the insensitivity to axis 3 plow frequency arises partly because fig 12 reports on the median outcome whereas this axis causes a widening in the distribution of outcomes without changing the central tendency eg more extreme multi year droughts floods with unchanged median as mentioned above fig 12 also shows interactions between any two of the axes for example a reduction in precipitation of 15 causes the years fully allocated to reduce from 75 to 40 however if this occurs simultaneously with a temperature increase of 3 c the outcome lowers further to 20 this is consistent with the comments above about the significance of interactions while the system is vulnerable to changes in average precipitation most gcms project modest changes in this stressor within the planning window of 2020 2040 in contrast shifts in rainfall runoff relationship have occurred rapidly in the past eg within 1 3 years peterson et al 2021 so the potential for this stressor to impact the system during planning timeframes is relatively acute underscoring the importance of hydrological research on this topic 4 discussion 4 1 limitations the framework is complex and its output multifaceted so we supplement the above figures assessing the outputs ie figs 7 11 with fourteen further figures in the supplementary material section s8 while most of these figures confirm the ability of the framework to replicate the features of historical data there is one category with room for improvement namely spatial correlation figures s8 2 to s8 7 demonstrate that the historic data show a lower spatial correlation between sub areas associated with a greater propensity for a given drought or flood to be more severe in one sub area compared to another one possible contributor could be the annual monthly disaggregation step since the existing mechanism to allocate patterns from wet medium and dry years may not be nuanced enough to capture the complexity of monthly dynamics however given the problem also exists at annual and multi annual timescales it may be associated with the assumption fig 7d that a single combined low frequency sequence is adequate across all sub areas when in fact there were regional differences in timing of the historic low frequency sequences this assumption synchronises the stochastic data across sub areas to a greater degree than in reality given the pleasing results fig 9 at system scale ie after aggregating the sub areas we suggest this limitation is not critical to this case study nor is it likely to be critical to similar cases studies nonetheless future research may examine options to remedy this limitation with a key challenge being defining dynamics across sub areas that is we know each sub area should diverge from the combined sequence but how should these temporal divergences be arrange spatially across different sub areas we leave such questions to future research a higher level limitation may exist regarding the generality of the framework it is unclear whether the order of operations formulated here section 2 4 6 is generalisable to any desired set of stressors it is possible that an additional stressor may exist for which there is no sequential order of operations that allows all stressors to behave independently such cases may require a methodology that examines tradeoffs on the fly between different desired traits of stochastic data eg guo et al 2018 outputting the closest possible stochastic sequence for a given set of perturbations this is very different to the adopted methodology nonetheless this may be somewhat of a hypothetical limitation since the authors have not yet come across a stressor type that would break the current methodology suggesting it has significant potential 4 2 computational effort and appropriate model complexity the purpose of this subsection is threefold i to report on the computation effort required ii to discuss issues of appropriate model complexity related to the framework and or raised while working on the case study and iii to briefly discuss possible future directions regarding sampling of the climate stress testing space in the introduction it was stated that a key benefit of a monthly timestep stochastic framework is improved run times so here we report the computational time taken in the case study working in matlab on a 2016 era laptop hp elitebook with intel i7 7500u 2 7 ghz processor the generation of 3 000 years of stochastic data takes approximately 1 1 s to cover the stress testing space this 1 1 s task is repeated for each possible combination of gradations mentioned in table 1 so there are 12 9 8 8 9 62 208 combinations for a total of 186 106 simulated years the total run time is approximately 19 h thus a laptop can complete all the stochastic generation for the case study in less than one day continuous computing time however runtime considerations are not limited to stochastic data generation the run time of subsequent modelling steps is usually also important in our case study a pre existing detailed daily river systems model was available but it took 10 min to run the 120 year historic sequence which would translate to approximately 30 years in run time for 186 106 simulated years it is hard to imagine an extensive stress test when each model run is so time consuming reframing the model as a monthly model and ensuring the model included only those details relevant for the study as described by john et al 2021c reduced run time by between three and four orders of magnitude to a similar total time as the data generation these design choices including but not limited to timestep enabled a much more thorough exploration of the stress testing space within a modest computational budget in cases where a system model already exists either from previous research or from system operators it would seem straightforward and tempting to adopt such models rather than build a model for each new context however our experience suggests that such inherited models should be carefully assessed for appropriateness for each new research question time spent up front to gain the right model for the question pays dividends later the challenges here are not only technical in our case additional and ongoing effort promoting trust in the new model among project partners was and is key to acceptance among stakeholders our experience designing the framework and case study suggests that issues of appropriate model complexity are important within stress testing studies and need to be considered up front ie prior to investment in more detailed experimental design for example the choice to implement a monthly stochastic framework went hand in hand with the decision to simplify the river systems model including but not limited to the decision on timestep this underscores the importance of an initial holistic planning phase in decision scaling and scenario neutral studies during which model complexity is agreed although issues of appropriate model complexity have been discussed for decades within hydrology eg grayson et al 2002 they are commonly framed in terms of a trade off between realism and identifiability predictive accuracy eg akaike 1973 in the context of stress testing the tradeoff is between complexity realism and the assessment of uncertainty a notion that is relatively new within the climatic sciences eg helgeson et al 2020 the quality of uncertainty assessment in the context of climate stress testing may depend on such details as how many stressors and gradations can be tested within a given computational budget a final issue concerns the appropriate exploration of highly dimensional stress testing spaces sampling may seem straightforward for the common situation of a 2 or 3 dimensional climate stress testing study but a 5 stressor study as here entails orders of magnitudes more computational effort to achieve the same sampling density in response some authors have proposed schemes for initial screening of a larger number of stressors in order to determine those most relevant and produce a lower dimensional space more amenable to high density sampling culley et al 2021 a further question is whether all areas of the sampling space are equally important for example areas suitable for lower sampling density could include areas where system failure is rare or where stressor combinations are physically implausible or considered unlikely more broadly it raises the question of whether a uniform sampling approach remains appropriate in different contexts such as calibration of multi parameter models uniform sampling is usually avoided in favour of directional searches towards some optimum value eg duan et al 1992 or to explore the shape of some behavioural region eg vrugt et al 2008 2013 it is possible that future researchers may adapt these techniques to direct available computing resources to resolve the parts of the space that matter most such as better exploring the form of the failure surface and its boundaries in summary the issues discussed above are i whether to accept an inherited model or build one suited to the context ii if building a model how to tailor it to obtain the best tradeoff between model complexity and uncertainty estimation and iii appropriate sampling of highly dimensional climate stress testing spaces as the science of bottom up climate risk assessment advances and matures we suggest that explicit consideration and discussion of these issues should become more commonplace 5 conclusions although bottom up climate stress tests are becoming more common there are still relatively few frameworks for generating climate and streamflow data for multi site many dimensional more than two stress tests in this paper we have presented a freely available monthly timestep climate stress testing framework while most climate stress testing is focused on the daily timestep monthly timesteps are suitable for systems whose dominant dynamic is seasonal or longer which is the case for many water resource systems furthermore the longer timestep means the framework can generate data rapidly making extensive stress testing possible with a limited computational budget this allows for the inclusion of stressor types such as modulating low frequency climatic behaviour precipitation seasonality and allowing for future changes in rainfall runoff relationship the case study demonstrated how the framework can be applied to a relatively complicated case study with varied climatic conditions across a relatively large spatial scale the requirement for eight different sub areas and four reach inflows was a suitable test to demonstrate the flexibility of the framework the results indicate a good match between historic data and the baseline stochastic output and in particular a pleasing replication of multi year runs testing confirmed that the five stressors can be perturbed plausibly and simultaneously to cover the stress testing space within a modest computational budget less than 24 h computing time on a 2016 era laptop the matlab octave code is freely downloadable from https doi org 10 5281 zenodo 5617008 and is subject to a gnu general public license v3 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements keirnan fowler acknowledges support from linkage project lp170100598 vulnerability of environmental water to a variable and changing climate funded by the australian research council the department of environment land water and planning victoria the victorian environmental water holder and the bureau of meteorology australia dr avril horne was funded by australian research council decra de180100550 andrew john and natasha ballis acknowledge the support of the australian research council via australian postgraduate awards the authors gratefully acknowledge the contributions of the editor and three anonymous reviewers whose comments significantly improved this paper and the framework the authors acknowledge that the framework incorporates code from other sources including the code for complete ensemble empirical model decomposition with adaptive noise ceemdan and code from package tablicious by andrew janke see the framework code for links to the source material in these cases appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105339 
25627, bottom up methods are increasingly used to assess the vulnerability of water systems to climate change central to these methods is the climate stress test where the system is subjected to various climatic changes to test for unacceptable outcomes we present a framework for climate stress testing on a monthly timestep suitable for systems whose dominant dynamic is seasonal or longer eg water resource systems with carry over storage the framework integrates multi site stochastic climate generation with perturbation methods and in built rainfall runoff modelling the stochastic generation includes a low frequency component suitable for representing multi annual fluctuations multiple perturbation options are provided ranging from simple delta change through to altered seasonality and low frequency dynamics the framework runs rapidly supporting comprehensive multi dimensional stress testing without recourse to supercomputing facilities we demonstrate the framework on a large water resource system in southern australia the matlab octave framework is freely available for download from https doi org 10 5281 zenodo 5617008 keywords climate stress test stochastic generation water resources climate change bottom up 1 introduction climate change is a key risk to water systems and recent years have seen many new methods to characterise this risk see eg review by wilby and murphy 2019 the vulnerability of a given water system to climate change depends on many factors including location specific projections of climate change and runoff the robustness of the system to these changes and the specific aspect of system performance in view eg brown and wilby 2012 nathan et al 2019 vulnerability assessments must represent each of these broad factors to inform risk management and help establish robust policy methods of climate change risk assessment can be categorized as either top down or bottom up top down is the traditional approach and uses global climate model gcm projections as the starting point in a chain of modelling steps that typically includes simulations of the rainfall runoff response and system operations eg reservoir release rules allocations etc the way the gcm projections are used in top down methods varies in many studies the bias corrected climate projections are used directly as forcing data for subsequent modelling steps eg christensen et al 2004 chiew et al 2009 garcía ruiz et al 2011 in others the distribution of gcm projections inform the distribution and or range of future climate changes tested given the potential for scenario selection to directly impact the robustness of outcomes mcphail et al 2020 there is increasing effort to ensure appropriate scenario choices giudici et al 2020 although the impact of scenario choice on management decisions themselves as opposed to metrics is still an active research area mcphail et al 2020 in contrast bottom up methods consider a range of plausible future climate sequences selected to investigate the system of interest and identify conditions which cause stress to the system stochastic methods are often used to generate future climatic sequences in bottom up studies eg brown and wilby 2012 turner et al 2014 henley et al 2019 a broad range of plausible futures may be tested with the ranges of key variables or stressors eg change in precipitation often extended beyond the envelope of gcm scenarios a so called scenario neutral approach prudhomme et al 2010 rather than starting with a gcm scenario and asking what does this scenario mean for the system the bottom up procedure starts with the system and asks what combination of future conditions cause stress to this system thus the procedure is often called a climate stress test brown and wilby 2012 having identified the combinations of future conditions that result in stakeholder defined unacceptable outcomes the probability of these outcomes may be assessed by checking what proportion of gcms contain similar conditions in their projections eg brown and wilby 2012 thus gcm projections can have an important role in bottom up assessments but the way they are used is different to the top down approach the bottom up approach has significant advantages the focus on specific system vulnerabilities is often more relevant to policy decisions brown and wilby 2012 wilby and murphy 2019 the approach facilitates systematic assessment of policy options by their robustness to future changes of varying degree eg weaver et al 2013 further as newer gcm results become available they can be rapidly mapped onto existing analyses in contrast to the top down approach which may require the entire modelling chain to be re run because of these advantages bottom up methods have been applied in many systems in the past decade including in the united states eg poff et al 2016 united kingdom eg prudhomme et al 2010 continental europe eg culley et al 2016 africa eg ghile et al 2014 asia eg yang et al 2016 and australia eg turner et al 2014 culley et al 2019 despite these advantages implementing bottom up methods presents technical challenges particularly regarding the generation of future climatic sequences which is the focus of the present framework a minority of studies use simple methods such as scaling the historic timeseries eg ghile et al 2014 françois et al 2018 but most adopt stochastic methods because they provide a greater range of possible future sequences and thus a more exhaustive stress test brown and wilby 2012 many stochastic methods are available see eg wilks and wilby 1999 srikanthan and mcmahon 2001 with some implementations specifically designed for use within bottom up assessments steinschneider and brown 2013 guo et al 2018 bennett et al 2021 key technical issues that need to be resolved when applying these methods include i how to generate different data types with appropriate cross correlations for example precipitation and temperature at a site may be correlated in time ii how to represent spatially correlated behaviour if the system is heterogeneous iii how to represent climatic fluctuations that occur at lower multi year frequencies and iv if the boundary condition being perturbed is at the hydroclimatic level precipitation temperature etc how to convert climatic sequences into streamflow accounting for non linear responses note that these are not limitations of stress testing itself but rather are broader issues not specific to stress testing the multi faceted nature of these technical issues and tasks can be daunting for new investigators possibly impeding uptake of bottom up methods particularly in budget conscious publicly funded research projects one way to lessen this impediment is to publicly release frameworks that demonstrate how all the parts of the analysis fit together and which can be applied to new systems with relatively minimal effort this is the motivation of the present article and framework here we focus on the monthly timestep which differs from the daily timestep used in most bottom up studies eg whateley et al 2014 mukundan et al 2019 freeman et al 2020 the choice of timestep depends on the context for example a study of future flooding requires a timestep of daily or shorter because floods typically occur on timescales of days to weeks however in many water resource systems particularly those involving carry over storage the dominant dynamic is seasonal or longer nathan et al 2019 in such cases the greater detail provided by a daily timestep may not translate into greater decision relevant information obtained from the stress test relative to a longer timestep helgeson et al 2020 given the significant computational burden of stochastic climate stress testing using water resource models john et al 2021c a timestep consistent with important system dynamics and no finer frees up computational resources and allows greater possibilities within the test itself wang et al 2011 these possibilities could include a more extensive test eg a greater number of stressors or longer sequences possibly allowing a more defensible assessment of uncertainty helgeson et al 2020 and or allowing the analysis to be undertaken without recourse to expensive supercomputing facilities recent studies also demonstrate the feasibility of hybrid timestep approaches where computationally expensive parts of the process are run on longer timesteps and disaggregated to shorter timesteps only where required john et al 2021a 2021c the suitability of the monthly timestep for water resources studies and the relative lack of methods on this timestep suggests a gap which we seek to fill this paper outlines a freely available and integrated framework for generating stochastic data on a monthly timestep and perturbing it for climate stress testing the framework includes both climatic data and streamflow data it generates climatic data first and includes conversion to streamflow using a built in monthly timestep rainfall runoff model system modelling eg river operations is context specific and out of scope but must be on a monthly timestep in order to be paired with this framework likewise site specific calibration of the rainfall runoff model is out of scope although we do outline the methods used in the case study in supplementary material section s5 the framework is intended to be applicable from the scale of small catchments up to large river basins approximately 101 106 km2 the paper is structured as follows section 2 provides a technical description of the framework while section 3 describes a case study application on a large water resources system in southern australia section 4 briefly discusses limitations of the framework and issues relating to the appropriate level of complexity and future research directions in reducing computational effort for climate stress testing 2 framework description 2 1 overview fig 1 provides an overview of the framework moving from historic timeseries through to stochastic data generation rainfall runoff modelling and perturbation methods to generate the hydroclimatic data sets required for stress testing the starting point is the set of historic timeseries data for the area of interest the user can optionally divide this area into portions termed sub areas in which case historic climate data spatially lumped must be provided for each sub area subdivision can useful to i create demarcations that are necessary for a given application eg upstream versus downstream of a reservoir ii divide a heterogeneous study area into more homogeneous sub areas and iii capture distinct but correlated behaviour eg in different tributaries all these principles of subdivision are demonstrated in the case study see section 3 4 the framework is designed for spatially averaged data across sub areas and as such i the user must pre process raw gauged data to obtain spatial averages prior to input into the framework ii spatial fields are not directly digestible by the framework spatial averages must be taken to produce monthly timeseries for each sub area the bulk of the framework lies in the generation of perturbed stochastic climate data steps are undertaken to first stochastically generate annual timeseries for each sub area and later disaggregate to monthly timestep although the steps for perturbation of climate are shown in fig 1 as distinct from the steps for stochastic generation in practice they are intertwined section 2 4 6 the generation of streamflow data is the final step in the framework and has two sub steps rainfall runoff modelling using a monthly rainfall runoff model and perturbation of the rainfall runoff response the generated streamflow data can be input into a model of the system of interest and a vulnerability assessment undertaken readers are directed to brown et al 2019 or wilby and murphy 2019 for guidance in these later steps and also for putting the climate generation into the context of decision making as with any stochastic technique the adopted period of historic data is important because it forms the basis for subsequent stochastic generation ie generated data is expected to have similar statistics to this data see next subsection furthermore perturbations will be relative to this data eg a perturbation of 10 will be relative to the mean over the chosen period strictly speaking it will be relative to the baseline stochastic series which should have a close but not exact match with the selected period thus users should make every effort to ensure the input data is representative of historical conditions usually longer historic timeseries are preferable as they cover a greater range and provide a better sample of past conditions 2 2 baseline stochastic climate generation the aim of stochastic data generation is to provide arbitrarily long synthetic timeseries with similar statistical properties to historical data the relevant statistical properties include averages variability on different timescales monthly annual multiannual autocorrelation and cross correlations between different locations in space here we are particularly concerned with low frequency ie multi annual or longer variability and persistence such as that driven by climate teleconnections see eg kiem et al 2004 because this may cause stress in systems with carry over storage mcmahon et al 2007 even in the absence of climate change however unless special consideration is given to low frequency behaviour stochastic schemes tend to miss this dynamic entirely thyer and kuczera 2000 srikanthan and mcmahon 2001 therefore as discussed in the next subsection we adapt an earlier scheme mcmahon et al 2008 which explicitly separates out low frequency behaviour in precipitation and generates it using a separate stochastic scheme to that used for the high frequency component please note the process of stochastic climate generation begins with p later t and pet are generated as a function of p based their historic correlations thus p is described first section 2 2 1 then t 2 2 2 and pet 2 2 4 although the final timestep of the stochastic data is monthly many of the operations are done on a longer timestep which is subject to later disaggregation section 2 2 3 the annual timestep is the default choice for the longer timestep and this article is written assuming this choice but alternative choices are possible as discussed in section 2 2 2 1 2 2 1 generation of annual precipitation p timeseries fig 2 provides an overview of the method for stochastic generation of annual precipitation the following subsections explain each step section 2 2 1 1 outlines the process of splitting the precipitation into high and low frequency components section 2 2 1 2 describes the stochastic generation of the high frequency component section 2 2 1 3 describes the model used for stochastic generation of the low frequency component and section 2 2 1 4 focusses specifically on how the low frequency model parameters are chosen to preserve relevant historical statistics and how this is tailored to each individual sub area 2 2 1 1 division of annual precipitation into high and low frequency components the tendency for low frequency dynamics to be missed by common stochastic generation techniques has prompted the development of specialized schemes to overcome this limitation eg thyer and kuczera 2000 mcmahon et al 2008 steinschneider and brown 2013 here we adapt mcmahon et al 2008 s scheme to explicitly define and separate low frequency behaviour using a variant of the empirical mode decomposition technique and stochastically generate the high and low frequency components separately note low frequency in this context means oscillations which occur with period of approximately ten years or longer but it is noted that the distinction between high and low frequency is largely under the user s control as discussed below empirical mode decomposition emd huang et al 1998 torres et al 2011 is a non parametric technique capable of analyzing non stationary timeseries such as those stemming from non linear systems the emd process iteratively removes variation from a time series highest frequencies first as shown in fig 3 in each iteration the removed variation is called an intrinsic mode function or imf and each imf has a mean of zero the sum of all imfs plus the residual gives the original time series the advantages of using emd in hydroclimatic contexts include that it can handle both nonlinear and nonstationary time series this is a major advantage over other decomposition techniques such as fourier analysis or wavelets mcmahon et al 2008 instead of the original emd algorithm we adopt complete ensemble empirical mode decomposition with adaptive noise or ceemdan torres et al 2011 colominas et al 2014 http bioingenieria edu ar grupos ldnlys metorres metorres files ceemdan v2014 m last accessed 22 oct 2021 and incorporate its code into the framework ceemdan has multiple additional features over the original emd and subsequent ensemble emd eemd that improve the robustness of imf extraction reduce information sharing referred to as mode mixing between imfs and better ensure the set of decomposition products sum exactly to the original series unlike eemd colominas et al 2012 2014 here categorisation into high and low frequency involves lumping the first n imfs and categorizing these as high frequency where n is a user specified parameter the remainder are then lumped to form the low frequency component for example mcmahon et al 2008 choose n so that high captures all intra decadal oscillations whereas low captures inter decadal oscillations deciding n is subjective the user needs to consider what might cause stress and expose vulnerabilities of the system which depends on the strength of different modes of variation in observed climate and also on the ability of the system to buffer against these variations it is important to note that emd and ceemdan have limitations which may be particularly relevant in systems where el niño southern oscillation enso variation is the dominant multi year dynamic enso oscillations typically take between four and seven years and in the shorter case the oscillations may be too rapid to be properly characterised this is because ceemdan along with other emd like algorithms works by identifying local minima and maxima in timeseries and ideally a sequence of at least five points mid low mid high mid is required for ceemdan to resolve the cycle in cases where enso dynamics are expected to expose important system vulnerabilities a shorter aggregated timestep may be required such as the six month timestep adopted by mcmahon et al 2008 most of the framework method is agnostic to this choice except that the disaggregation step section 2 2 3 would require extra care for example if the 6 month periods were nov apr and may oct historic patterns from nov apr should not be donated to may oct nor vice versa after applying ceemdan to each sub area separately the high frequency components and low frequency components are treated differently the high frequency components are stochastically generated such that each sub area is distinct but spatial cross correlations are maintained as discussed in the next subsection in contrast the low frequency components are aggregated together and largely treated as one with a specialized method of stochastic generation as discussed in section 2 2 1 3 and section 2 2 1 4 2 2 1 2 stochastic generation of high frequency component the high frequency component of annual precipitation is stochastically generated using the multi site method of matalas 1967 this method described as weakly stationary by the original authors preserves lag 1 autocorrelation at each site individually in addition to the lag 0 cross correlations between sites note in this context a site means a sub area for more detail the reader is directed to the original paper matalas 1967 and subsequent reviews such as srikanthan and mcmahon 2001 in cases where the user elects not to divide the study area into sub areas but rather treat it as a single entity the matalas method is the same as a lag 1 autocorrelation ar 1 model as with many statistical techniques the performance of the matalas method improves if the input data are approximately normally distributed to achieve this the framework provides the option to normalize the high frequency component data prior to the matalas step using a box cox transformation box and cox 1964 the box cox parameter is chosen on a per sub area basis to minimize the absolute value of the skew post generation the same parameter is used to reverse the transformation 2 2 1 3 stochastic generation of low frequency component adopted model the earlier demonstration of a similar workflow by mcmahon et al 2008 adopted an ar 1 model for generating the low frequency component of precipitation however as shown in fig 4 a and b initial testing revealed that the ar 1 model was unable to replicate the dynamics of the low frequency component of our test site even in the case where the generated timeseries has similar standard deviation and autocorrelation to the historic low frequency timeseries the ar 1 model tends to exhibit sustained departures from the mean in this case zero resulting in a small number of inappropriately long spells seen as a fat tail in the distribution in fig 4d i to make matters worse when the ar1 timeseries happens to be close to the mean its tendency for short term fluctuations leads to a large number of very short spells above and below the mean as seen by the high frequency of one to five year spells in fig 4d i for these reasons we instead adopt a broken line process mejia et al 1972 for stochastic generation of the low frequency component as the name implies a broken line process is simply a set of connected linear segments with an equal usually multi year length of time assigned to each segment and the value at each transition point being selected randomly from a normal distribution with zero mean the two parameters of the broken line process are the segment duration and the amplitude standard deviation of the process both parameters can take any positive real number the values at a given transition point are independent of all the others ie zero autocorrelation but because the segment duration is usually multiple years the annual timeseries exhibits non zero autocorrelation fig 4c importantly in this initial test a broken line process parameterised to match the standard deviation and autocorrelation matched the run length distribution much more closely than the ar 1 model did ie the most frequent run length is similar to historic and the tail is less fat than the ar 1 fig 4d ii in concept the broken line process is related to the broken line model garcia et al 1972 specifically a broken line model is composed of multiple broken line processes added together each operating at different frequencies however given the promising replication of statistics including run length a single process is considered sufficient for the present purpose that said if users wish to swap the broken line process for a different model whether the broken line model or another long memory model this would be straightforward to code 2 2 1 4 stochastic generation of low frequency component parameterisation and treatment across different sub areas as noted most aspects of the treatment of the low frequency component of precipitation are conducted in common across all sub areas for model fitting the historic low frequency component timeseries for all sub areas are combined into a single timeseries using a weighted areal average or optionally a bespoke user defined weighting the same segment duration parameter is adopted across all sub areas calculated from the number of zero crossings in the combined historic timeseries using the formula in supplementary material s1 also the same sequence of random deviates is used across all sub areas so that the generated low frequency timeseries for a given sub area varies from the others only in amplitude but not in timing ie all sub areas move together into and out of multi year wet and dry periods the justification for this is the assumption that the processes driving low frequency oscillations such as climate teleconnections are acting on all sub areas in common across the study area the amplitude parameter is chosen for each sub area separately so that the observed historic value of the hurst coefficient hurst 1951 is replicated in the baseline precipitation data the hurst coefficient measures the persistence in the timeseries high persistence means consecutive values are more dependent upon one another koutsoyiannis 2005 and thus multi year droughts will tend to be longer as will multi year runs of wet conditions note during perturbation the hurst coefficient value can be directly perturbed as one of the five stressors in the stress test in which case the perturbed value will be preserved rather than the historic see section 2 4 3 the following two clarifications may help to avoid confusion 1 for a given sub area the two values being matched are i the hurst value of the historic precipitation timeseries ie the raw observed data not the low frequency component and ii the hurst value of the stochastic timeseries after the high and low frequency components have been added together since the high frequency component is essentially just red noise the persistence of the final timeseries will tend to increase with increases in the amplitude assigned to the low frequency component in short higher amplitude means higher hurst coefficient 2 although the synthetic replicates may be hundreds or thousands of years long it is not wise to calculate hurst coefficient values over this duration because the calculations become sensitive to low frequency oscillations whose period exceeds the historic record to sidestep this issue the stochastic timeseries is divided into segments of equal length to the historic record for example if the stochastic replicate is 2000 years long and the historic record 100 years the replicate is divided into twenty fragments and a separate hurst value calculated for each fragment the key value is then the average hurst value among this set of twenty values and the solver seeks to match this value to the target hurst value after the application of emd matalas and broken line methods the result is each sub area has its own stochastic timeseries of annual precipitation the sub areas move together through multi year periods of wet and dry conditions for a given sub area the historic hurst coefficient or a perturbed value if relevant is replicated assessed over sub periods of the same length as the historic data the historic autocorrelations and cross correlations between sub areas are typically close but this is not exact the matalas method can exactly replicate these properties but the matalas method governs the high frequency component only the low frequency component method focusses on ensuring hurst values are preserved in the combined high low synthetic timeseries and this may result in the cross correlations and autocorrelations varying from historic values generally these departures are small 2 2 2 generation of annual temperature t timeseries fig 5 provides an overview of the remainder of the stochastic generation process the first step is generation of annual t timeseries for each sub area which is discussed in the next paragraph we note the following two points 1 the temperature information can be based on daily maximums daily minimums or daily means in line with user preference and 2 the process used for t ie developing a relationship with p based on historical observations and then using this to create synthetic t from synthetic p is relatively generic and could be used to generate any factor that is dependent upon p annual t is generated based on the stochastic timeseries of annual p and the historic relationship between t and p assumed to be linear often there is considerable scatter in the historic relationship so that for a given p value there is a distribution of t values as shown in fig 5 the residuals typically exhibit spatial cross correlation and autocorrelation to represent this behaviour the first step is to create a single timeseries of autocorrelated random numbers of zero mean with the autocorrelation set to the average across all sub areas ie the autocorrelation of the residuals is calculated in each sub area separately and then these numbers are averaged then for each sub area i an initial timeseries of t is generated using the historic line of best fit ii the random number sequence is scaled to have the same variance as the residuals of the historic relationship for that sub area and iii the two timeseries are added together to give the final timeseries of t it is recommended that the user check the normality of the historic residuals before applying this process this process assumes the cross correlation in the residuals is perfect it was considered that this was an appropriate simplification given that the cross correlations in p are preserved in the matalas process see above and p is the basis for t also this simple scheme for t is considered appropriate for water resources systems as they are generally less sensitive to t than to p 2 2 3 conversion from annual to monthly data conversion from annual to monthly timestep is achieved by the method of fragments eg srikanthan and mcmahon 2001 for both p and t we note that if desired other methods could also be incorporated into the framework by future users such as the non parametric k nearest neighbour knn bootstrap sampling approach of lall and sharma 1996 for each synthetic year the method of fragments randomly selects a year from history to donate its pattern for p the donated pattern is the proportion of precipitation in each month whereas for t the donated pattern is the variation of monthly t in c about the average for that year for a given synthetic year the same historical year is donor across all sub areas this maintains historic spatial cross correlations in monthly data realism is increased by restricting potential donors to historical years with similar hydroclimatology this is achieved by categorizing historical years as dry driest 33 3 of years by p wet wettest 33 3 or medium remainder synthetic years can only be assigned a pattern from a historical year in the same category note after calculating threshold values between categories based on historic data the threshold values remain unchanged throughout in the case of perturbation section 2 4 towards eg lower p more than 33 of synthetic years will be categorized as dry so that the perturbed timeseries will use patterns from historical dry years more often than historical wet years in this example case 2 2 4 generation of monthly pet timeseries monthly pet is generated via linear regression with monthly t for some locations it may be acceptable to use a single relationship across all calendar months but for others including our test case there are considerable seasonal differences in the t pet relationship from month to month for example the slope in the coolest month july is much lower than the slope in january fig 5 thus for each of the 12 calendar months a separate linear regression is used for each sub area separately for our test case most of the variability in month to month pet is explained by the calendar month so it is not considered necessary to account for scatter in the relationship for a given month and no special consideration of spatial cross correlation is given aside from applying these simple deterministic relationships for applications of the framework in locations with different et dynamics users may code a different approach such as by adding an error model like the one described in the previous sub section 2 3 simulating streamflow monthly q is generated using the wapaba model wang et al 2011 which is built into the framework other monthly rainfall runoff models could also be used if the user is willing to add the required code see eg xu and singh 1998 or topalović et al 2020 wapaba was selected from among a set of three tested monthly models because its performance over varied climatic conditions was superior not shown for the study area of the project that funded framework development the case study in section 3 wapaba has five parameters two govern the partitioning of rainfall between streamflow evapotranspiration and changes in storage α1 and α2 one determines the total capacity of the soil moisture store smax and the final two determine the quick slow flow split and drainage rate of slow flow β and k 1 respectively further information is provided in the supporting material section s4 wapaba inputs are monthly p and monthly pet appropriate model parameters are assumed to be known which is why they are listed as an input in fig 1 further information on model calibration is provided in the supplementary material s5 a key difficulty is that the sub areas adopted for stochastic generation may span multiple catchments and or be only partially gauged for example a single sub area may contain parts of multiple river catchments an example is shown for the case study in this paper see section 3 and associated figures the suggested approach depends upon the context if there are generally many gauged catchments per sub area and the problem is that no single gauge captures the whole sub area then it is suggested to identify a single gauged catchment within the sub area that is considered suitable to act as a representative catchment for the whole sub area the simulated streamflow from the representative catchment can be used in the way that best suits the case study but the simplest method would be that the simulated streamflow outputs in units of depth mm month could be assumed to apply over the whole sub area for an example of a more nuanced approach see the case study in section 3 note since synthetic q is generated using sub area wide forcing data p and pet model calibration should also use sub area wide historic forcing data p and pet see supplementary material fig s4 if sub area streamflow is ungauged the problem becomes a prediction in ungauged basins problem and an appropriate regionalisation technique must be applied to decide appropriate parameters see eg blöschl et al 2013 hrachowitz et al 2013 2 4 perturbation in the present context to perturb means to change an aspect of hydroclimate stressor for the purposes of testing the impact of the change on system performance for example the stressor mean temperature might increase by varying amounts or the stressor mean precipitation might go up or down detailed guidance on constructing a stress test experiment is beyond the scope of this paper but in summary the stressors can each be considered as individual axes in a stress testing space eg prudhomme et al 2010 while it is possible to limit the testing regime so that it focusses on one axis at a time it is more insightful to test combinations of changes this enables assessment of system performance based on any interactions between the climate stressors to achieve this bottom up studies typically cover the space via uniform sampling producing a regular grid the underlying stochastic generation framework should in principle be able to generate data corresponding to any point in this space that is any combination of stressor perturbation thus the adopted technical method for each of the five stressors must be applicable even if the other stressors are also being simultaneously perturbed and this is a key technical challenge for this framework in climate stress testing studies it is common to include only two stressors usually 1 changes in mean precipitation and 2 changes in mean temperature eg prudhomme et al 2010 turner et al 2014 whateley et al 2014 culley et al 2016 françois et al 2018 freeman et al 2020 however the rapid nature of the framework due mainly to its monthly timestep allows greater possibilities to expand the types of perturbation considered thus we have included an additional three stressors changes in low frequency behaviour of precipitation changes in precipitation seasonality and changes in rainfall runoff relationship the rationale for each stressor is given in the subsections below in general we suggest that the primary factor in selection of stressors should be to identify possible vulnerabilities of the system although stress testing studies commonly overlay gcm projections to understand future risk this does not mean that the selection of stressors should be limited to those that can be simulated by gcms for example the hydrological processes relevant to changes in rainfall runoff relationship are considerably out of scope for gcms but this stressor is still system relevant for our case study section 3 and thus should not be omitted the five stressors included in the framework have been selected based on relevance to the project that funded the frameworks development section 3 but we feel they may also be relevant in many other contexts however if they are considered irrelevant in some locations it is trivial to remove one or more stressor by simply ensuring values of perturbation are always zero for that stressor conversely it is possible in principle to add other stressors to the existing set if this is required for an application of the framework provided the user is willing to alter the framework code note in this framework the perturbations are applied to the baseline data without first enforcing that baseline data statistics exactly match historic statistics eg averages this is in line with the expectation that stochastic data should have similar but not identical statistics to historic thus strictly speaking a given perturbation is considered relative to the baseline stochastic series rather than the historic timeseries in practice the baseline timeseries generally match historic statistics well as can be judged by the figures provided with the case study section 3 however this is usually related to the length of the generated timeseries with longer synthetic timeseries more likely to provide a closer match to historic statistics due to the averaging out of short term fluctuations following the subsections on individual stressors a summary of the code s order of operations is given section 2 4 6 to clarify which perturbations are applied first finally guidance on perturbation axis limits is provided in section 2 4 7 2 4 1 stressor 1 precipitation temporal average rationale this is a first order control on hydrologic response in water systems and therefore on system performance thus stressor 1 is routinely included in climate stress tests as noted method perturbations in temporal average precipitation are applied as a proportional change that is constant across the synthetic timeseries for example if the perturbation value is 5 every value in the precipitation timeseries is multiplied by 1 05 if the value is 20 every value in the precipitation timeseries is multiplied by 0 8 2 4 2 stressor 2 temperature temporal average rationale since t is regarded as the most reliable gcm variable and since t directly influences evaporative demand it is very commonly included in climate stress tests however the influence of this stressor on system performance may vary depending on the system and the aspect of performance in view and on whether the system is water or energy limited method perturbations in temporal average temperature are applied as an absolute change in c for example if the perturbation value is 1 5 every value in the synthetic temperature timeseries is increased by 1 5 note perturbing stressor 1 or stressor 2 means historic relationships between p and t will cease to be preserved as noted in section 2 2 2 the origin of the unperturbed t timeseries is the unperturbed p timeseries which is then transformed into t via the historic regression relationship subject to noise perturbing stressor 1 and or stressor 2 causes this relationship to shift vertically laterally or both in the stochastic data relative to the historic relationship 2 4 3 stressor 3 low frequency behaviour of precipitation rationale this is important to system performance in carryover systems as outlined in section 2 2 above particularly where the main considerations are reliability of water supply future changes in the characteristics amplitude period of this behaviour may expose system vulnerabilities even in the absence of a change in average precipitation there is much diversity among gcms regarding projected future changes in low frequency oscillation such as enso eg bellenger et al 2014 but in general gcms project an increase in the frequency of strong el niño and la nina events but disagree about the strength of this change cai et al 2018 few studies have examined projections for longer interdecadal cycles for an exception see wei et al 2018 but like enso these oscillations are sensitive to ocean dynamics liu 2012 which in turn respond to climate drivers making future changes possible method as briefly mentioned in section 2 2 1 4 perturbations of the low frequency component are quantified as changes in hurst coefficient recapping 2 2 1 4 for baseline stochastic data no perturbation the amplitude parameter in the broken line process is tuned to ensure the historic hurst coefficient is matched higher amplitude means higher hurst coefficient for stressor 3 the hurst value itself is perturbed that is the target hurst is higher or lower than the historic hurst value in line with the perturbation to achieve the perturbed hurst value one possible approach would be to retune the amplitude parameter without altering the segment duration parameter this would result in oscillations of similar period but different magnitude to historic however since in reality the hurst coefficient is affected by both the period and magnitude of low frequency oscillations we allow both parameters to vary in response to perturbation according to the method given in supplementary material section s3 note stressor 3 perturbation is done early in the process before t and pet are generated from p see section 2 4 6 and fig 4 thus both t and pet exhibit low frequency behaviour in response to the low frequency behaviour of p 2 4 4 stressor 4 precipitation seasonality rationale the importance of precipitation seasonality depends on context it may be less important in some carry over storage systems as discussed above for projects concerned with stream ecology see case study section 3 seasonality of flows may be a core concern for species diversity tonkin et al 2017 particularly since some species are sensitive to flows at certain times of year as eg breeding triggers eg poff et al 1997 changes in seasonality may therefore have significant impacts on ecology particularly in unregulated systems also depending on the synchronicity of precipitation and demand changes to precipitation seasonality may also impact flow volumes eg if rain is shifted towards months of higher evaporative demand less streamflow will ensue method the seasonality stressor is based on two seasons warm and cold relevant to the temperate climate in the example case study this may be easily adapted to more tropical climates with wet and dry seasons the user needs to decide which months are in which season to allow for austral and boreal differences in seasonality perturbation values are quantified as an increase in the proportion of annual p that falls in the warm season with negative values denoting decreases thus in the case where the unperturbed p is 900 mm with a warm cold split of 300 600 a perturbation of 5 means 45 mm will be subtracted from the cold season and added to the warm season so that the new split is 345 555 note when combined with perturbation in other stressors unexpected results can occur for example a small positive perturbation in stressor 4 combined with a large negative perturbation in stressor 1 will give a decrease in warm season p even though an increase would be expected from the seasonality perturbation acting alone perturbation of seasonality is achieved through three steps 1 conversion of perturbation value into a reallocation amount in mm for example if the perturbation value was 0 08 and the perturbation was being applied to a timeseries with mean annual precipitation of 1000 mm yr this means 80 mm yr on average will be reallocated from winter to summer months 2 for each calendar month assign reallocation amounts in mm the reallocation pattern is defined as described in supplementary material s2 1 accounting for the seasonal definitions specified by the user if the pattern adopted here does not suit an application it can be easily redefined as required since it is a redistribution some months will gain and some will lose with a net change of zero note following this step one possible option would be to simply add or subtract as required the reallocation amounts directly to the timeseries for example if the result of step 2 for january were 17 mm then we could simply add 17 mm to all the january months in the synthetic timeseries this would mean that there are no longer any dry january months in the perturbed timeseries as the minimum possible value would be 17 mm an undesirable outcome for this reason step 3 is based on an alternative approach using adjustment factors 3 calculate monthly adjustment factors based on reallocation amounts for each calendar month calculate an adjustment factor such that when all instances of the month are multiplied by the factor the average change equals the absolute change from step 2 see supplementary material s2 2 this multiplicative approach preserves dry months in the original timeseries to a much greater degree than an additive approach but it causes a small water balance error in most years which is corrected by simply scaling all months in the year up or down by the required amount rarely more than 2 in our test case 2 4 5 stressor 5 relationship between rainfall and runoff rationale recent studies in australia eg saft et al 2015 and the usa avanzi et al 2020 demonstrate that it is possible for a multi year dry period to cause a shift in rainfall runoff response this means that even after we take the lower rainfall into account the streamflow during the multi year dry period is still lower than expected a climate change induced permanent reduction in future precipitation could result in a permanent shift in rainfall runoff relationship and because the processes causing the shift are poorly understood the shift is not possible to predict in advance which is why it is treated here as a stressor in its own right it could be argued that such measures are redundant since we should expect that hydrological models can simulate these transitions although some hydrological models can do so if calibrated jointly to the before and after fowler et al 2016 studies in australia indicate that these models could not have anticipated the transition in advance of it occurring that is if calibrated to the before only they do not accurately simulate the after in independent evaluation fowler et al 2018 until these issues are resolved a separate stressor may be justified to explicitly test different plausible future shifts in rainfall runoff relationship method simplistically a possible implementation for stressor 5 might be to multiply every streamflow value by a set factor like for precipitation in stressor 1 however this is inconsistent with observed behaviour during australia s millennium drought namely the observed proportional impact was much greater for dry years than wet years the framework replicates this behaviour by modifying methods from saft et al 2015 as discussed below saft et al 2015 generated scatter plots between annual p x axis and annual q y axis where the latter was transformed to linearise the relationship a shift in rainfall runoff relationship means a vertical shift on this p q plot with every year shifted by the same amount in transformed space however the effect of reversing the transform means that the actual change varies depending on the flow value for the transform saft et al 2015 applied a one parameter box cox transformation box and cox 1964 which is adopted here also however whereas saft et al 2015 allowed a different box cox λ value for each catchment here a single value is assumed to apply across all sub areas perturbation axis limits and gradations are then defined within the box cox transformed space one way to choose the λ value demonstrated in supplementary material s7 is to minimize the skew across all sub areas simultaneously practically the steps undertaken to transform a given synthetic sequence given a negative perturbation value p ie corresponding to a decrease rather than an increase are as follows these steps are shown graphically using the case study data later in the paper see section 3 for each synthetic year 1 calculate the total flow for the year ie sum the 12 monthly simulated flow values 2 apply box cox transformation to the output of 1 3 perturb by subtracting p from the output of 2 4 if the output of 3 is negative set it to 0 in the perturbed series it will be a zero flow year 5 otherwise apply reverse box cox transformation on the output of 3 6 disaggregate the output of 5 to monthly using the original flow pattern from 1 2 4 6 perturbation order of operations the order in which the perturbations take place is different to the order in which they are presented above to avoid potential confusion we explicitly list the order of operations 1 generate high frequency component of p annual 2 undertake method in section 2 2 1 and 2 4 3 which generates the low frequency component and the annual timeseries of p in such a way as to match the desired perturbed hurst coefficient thus the low frequency behaviour stressor 3 is the first perturbation in the order of operations 3 generate annual temperature from annual precipitation using the output of 2 and historic t p correlation section 2 2 2 4 perturb precipitation temporal average stressor 1 5 perturb temperature temporal average stressor 2 6 disaggregate to monthly section 2 2 3 7 perturb seasonality stressor 4 8 generate monthly pet from monthly t section 2 2 4 9 rainfall runoff modelling section 2 3 10 perturb rainfall runoff relationship stressor 5 2 4 7 guidance for setting perturbation axis limits as noted the stressors can each be considered as individual axes in a stress testing space the axis limits are thus the extrema in each direction by which the stressor in question is perturbed setting perturbation axis limits is a subjective choice that may vary by case study axis limits are often set to envelop the range of outcomes presented by gcm models see eg henley et al 2019 with additional margin on either side reflecting that future conditions may be beyond the range presented by gcms brown and wilby 2012 it is expected that this will be an appropriate method for axis 1 long term mean p axis 2 long term mean t and axis 4 seasonality of precipitation in most cases and an assessment of gcm simulations for the region in question is recommended for these axes however axis 3 low frequency behaviour of precipitation is not an area of strength for gcms although work is ongoing and newer generations of models are improving see references in section 2 4 3 above and bayr et al 2018 for long term oscillations such as the pacific decadal oscillation quality of simulations varies by continent and the simulated oscillations often have the incorrect period wei et al 2018 for the shorter enso models are often biased towards the la niña state while too weak atmospheric feedbacks can cause quite different enso dynamics to observed bayr et al 2018 p3171 given these critiques it seems reasonable to refer to independent information when setting the limits of axis 3 given axis 3 is defined as a change in hurst coefficient a useful starting point might be published values for observed hurst coefficients for the region in question but also at wider spatial scales continental or global to characterise the limits of observed hurst values gcms provide no information about changes in rainfall runoff relationships so axis 5 limits must be decided based on hydrological factors if the study region has seen a historic change in rainfall runoff relationship this may provide a useful reference point see case study section 3 in cases where no shift has occurred historically it is difficult and subjective to decide the range of future values to test one possible route would be to adopt identical limits to the case study herein section 3 but since axis 5 limits are contingent on the adopted box cox λ value adopting the case study limits assumes the λ chosen here is a suitable choice this may be the case if the skew of the transformed streamflow is not significantly different from zero at any sub area see testing methods presented in supplementary material s7 3 case study application 3 1 the goulburn broken campaspe loddon gbcl system we demonstrate the framework on south east australia s 40 000 km2 goulburn broken campaspe loddon gbcl system fig 6 a this region is made of up the traditional lands of the yorta yorta taunarung and dja dja wurrung clans part of the murray darling basin australia s most important agricultural region gbcl rivers supply approximately 12 of the basin s water from approximately 2 of the area the system comprises a main regulated waterway the goulburn river impounded by a large carry over reservoir lake eildon and supplemented by smaller impounded waterways campaspe loddon and broken rivers connected together by transfer channels not shown in fig 6 due to the murray darling basin s water trading scheme water impounded within the goulburn system is used over a wide area and is often traded as far as 700 km away the main water use is agricultural irrigation but the system also provides municipal supply for numerous towns gbcl rivers support significant aquatic habitats including for turtles platypus fish such as the critically endangered murray cod and ecologically significant riparian forests koster et al 2012 horne et al 2020 federal and state governments have invested significantly in water entitlements purchased for environmental use skinner and langford 2013 and this water is actively managed and released for environmental benefit doolan et al 2017 which may include objectives to inundate key wetlands or provide flow events that trigger fish spawning at specific times of year often this is achieved by supplementing topping up ecologically important flow events originating in unregulated tributaries watts et al 2011 docker and johnson 2017 the stress test arises from a study of the vulnerability of environmental water management to climate variability and change the full study encompasses all four rivers goulburn broken campaspe and loddon so this is the extent of the climate data generation fig 6b for the hydrological components of this demonstration we focus on the goulburn river only fig 6a as an example of a regulated carry over system lake eildon s capacity 3 106 m3 is more than double the historic median annual inflow downstream of lake eildon the regulated portion of the goulburn river is divided into four reaches see triangles in fig 6 by environmental flow managers which require consideration in the setup of the stress test note the reach division is an arbitrary feature of our case study users are encouraged to sub divide their study area in whichever way best suits their study area and serves their application see also section 3 4 1 as described in john et al 2021c the streamflow outputs of the framework are passed to a monthly timestep river systems model which represents the operation of the dams and transfer channels the vulnerability of the system to climate change and variability is assessed via two criteria supply reliability an output of the river systems model and ecological health an output of associated ecological models forced with outputs of the river systems model here we briefly summarise the results of this vulnerability assessment with further detail provided in john et al 2021c 3 2 data gridded historic precipitation and temperature data are from the australian water availability project awap jones et al 2009 note the adopted formulation for temperature is tmax the maximum daily temperature gridded potential evapotranspiration data are from the scientific information for land owners project silo jeffrey et al 2001 of the pet options provided by silo we adopt morton s wet environment evaporation over land morton 1983 because the main purpose of pet for this study is to force the rainfall runoff model and this formulation has been used in prior rainfall runoff modelling studies in this region eg fowler et al 2020a streamflow data are from the australian bureau of meteorology s hydrologic reference stations project turner et al 2012 as compiled in the camels aus dataset fowler et al 2020b information on storage capacity is from https www g mwater com au last accessed october 22 2021 3 3 stress test set up selection of stressors earlier section 2 4 a rationale for inclusion of each of the five stressors was given in general terms the specific reasons cited for each stressor are all true for the gbcl case study for example stressor 3 low frequency behaviour is important to system performance because local climate variability is relatively high on a global scale leading to spells of relatively higher cumulative deficit below the median eg peel et al 2005 while the large size of lake eildon buffers the system against short term up to five years variation longer scale oscillations may go beyond buffering capacity and so reveal system vulnerabilities regarding stressor 4 precipitation seasonality many regulated systems may be insensitive to this stressor because the storage mitigates against seasonal lows in the gbcl system seasonality is important regardless because unregulated tributaries generate ecologically important flow events at specific times of year as mentioned above in addition seasonality of flows affects announcement of seasonal volumes of water allocation which can in turn influence system operation and water trade lastly regarding stressor 5 changes in rainfall runoff relationship the 13 year millennium drought 1997 2009 saw changes in rainfall runoff relationship for many gbcl waterways including the campaspe river loddon river and unregulated tributaries of the goulburn river eg saft et al 2015 the streamflow in some waterways was approximately 70 below the expected flow given the observed rainfall some catchments have recovered returning to their earlier relationship between p and q whereas others seemingly remain in a low flow state peterson et al 2021 these recent flow reductions are an important issue for project partners and the method needs to allow for the possibility that further future shifts in rainfall runoff relationship may occur in line with the above all five of the stressors in the framework are adopted in the case study table 1 outlines how the axis limits are set with various factors taken into consideration 3 4 stress test set up spatial and rainfall runoff modelling considerations 3 4 1 division into sub areas as noted in section 2 1 there are at least three reasons why sub division of the study area is useful in a given application below we discuss how each reason relates to this case study users should note that the following comments are case specific and do not imply a general rule other applications may be more straightforward reason 1 create demarcations that are necessary for a given application eg upstream versus downstream of a reservoir since the unregulated tributaries of the goulburn river are instrumental in providing environmental flows it is not acceptable within this application to lump them together with the regulated tributaries ie the area upstream of lake eildon similar considerations apply to the other reservoirs although they are not in focus here thus the downstream boundaries of some sub areas have been drawn at the major reservoirs on the goulburn campaspe and loddon rivers reason 2 divide a heterogeneous study area into more homogeneous sub areas an example of applying this principle based on climate data is the distinction between sub areas b and c this distinction makes spatially lumped averages meaningful because they are more representative of underlying conditions further it improves confidence in rainfall runoff modelling by avoiding unreasonable requirements of models ie the requirement to represent many contrasting areas simultaneously we also apply this principle based on similarity of hydrological response for example fig 6c shows how a small portion marked in light green of lake eildon s catchment area is grouped with areas outside this is because this portion is part of the strathbogie ranges region d which are known to behave differently hydrologically compared to nearby catchments similar considerations apply to the boundary between sub areas c and e because boundaries based on similarity do not follow reach inflow catchment area boundaries fig 5c it is necessary to apply a post hoc step where flows from sub areas are added together in appropriate weighted averages to provide total reach inflows see supplementary material s4 while the last step may be inconvenient the workflow overall merely reflects the varied hydroclimate of the study area fig 6b lastly the large flat and dry area in the north of the study area called the riverine plain sub area g requires special consideration despite its large size this area contributes very little streamflow and is lumped together as a single unit reflecting its relative homogeneity reason 3 capture distinct but correlated behaviour eg in different tributaries this principle has already been touched on above when it was noted that the unregulated and regulated tribuaries need to be distinct rather than lumped for this case study the principle is also important among the unregulated tributaries for example inflows to reaches 1 and 4 are largely determined by sub areas b and d respectively keeping these sub areas distinct allows for a more nuanced representation capturing how these tributary inflows combine to provide flows for the ecologically important downstream reaches 3 4 2 rainfall runoff modelling and representative catchments the inbuilt wapaba rainfall runoff model is adopted for streamflow generation wang et al 2011 a potential difficulty is that none of the sub areas are gauged in their entirety although all of them have multiple gauged subcatchments with good quality records and stationarity of anthropogenic impact as indicated by their inclusion in the camels aus dataset fowler et al 2020b the solution is to select one of these subcatchments where possible one whose hydroclimate is close to average for the sub area and to assume it is representative of the whole sub area scaling simulated flows as required to represent the whole sub area since the sub area boundaries are intentionally set to enclose similar regions this method provides an adequate solution normally an approach like this might suffer from issues with flow routing but these issues are moot due to the monthly not daily timestep selected representative catchments for each sub area are listed in supplementary material s4 the models are calibrated using a method fowler et al 2016 which equally weights the performance in wet and dry periods see supplementary material s5 for more information this method uses the kling gupta efficiency kge gupta et al 2009 as the measure of model performance the kge considers the model s ability to match three relevant aspects for water resource and ecological modelling namely i the long term average flow ii the variability in flows and iii flow timing as measured by temporal correlation although the kge s focus on high flows has led some studies to adopt it blended together with a low flow metric eg knoben et al 2020 trotter et al 2021 these studies used daily models given monthly streamflows are less skewed than daily the tendency for undue focus on high flows is diminished 3 5 results 1 separation into high and low frequency components for brevity we show only highlights of the case study application but the user can interrogate the results set by downloading the framework and running the example the first such highlight is the separation of annual p into high and low frequency components fig 7 rather than showing each intrinsic mode function imf see fig 3 only the combined high and combined low frequency sequences are shown in this case the distinction between high and low frequency is made between imfs 2 and 3 broadly this gives a separation into inter decadal and intra decadal variation which is considered appropriate since the capacity of lake eildon 3 106 ml is sufficient to buffer the system against short 5 year variations but is less robust to interdecadal fluctuations the resulting low frequency timeseries has 11 zero crossings in 107 years as noted giving an average period of approximately 20 years despite the varied hydroclimate between sub areas the low frequency sequences tend to enter multi year dry periods and wet periods at the same time this means that the area weighted average fig 7d note sub area g is excluded from the weighting because it is so dry as to contribute little streamflow yet so large as to dominate the weightings is broadly representative of each of the underlying sequences recall that the mismatches in amplitude between the adopted combined timeseries and a given sub area timeseries are later resolved by the hurst coefficient matching procedure section 2 2 1 4 and 2 4 3 the mismatches in timing with which the different sub areas undergo multi year dry and wet periods does cause some issues with generated data as discussed in the limitations section section 4 1 3 6 results 2 stochastic generation for baseline no perturbation we now proceed to evaluate the quality of baseline stochastic outputs no perturbation based on a replicate length of 3000 years the evaluation is done in two parts an evaluation of seasonality fig 8 and an evaluation of the frequency distribution of annual flows and multi annual flow sums fig 9 note to avoid conflating the comparison of seasonality with the assessment of bias fig 8 only shows percentages in each month rather than absolute values fig 8 reveals an excellent match in seasonality across p tmax and pet a c and a good match for q d note that these comparisons are for sub area b as an example but the results are broadly consistent with other sub areas the match in seasonality in p and tmax is unsurprising given the method directly adopts historic within year patterns while the seasonal pattern for pet is also well matched the variability within each month is underestimated which is consistent with the decision not to explicitly represent the variability around the t pet relationship for each calendar month section 2 2 4 plot d relates to the representative catchment for sub area b this plot thus reflects the ability of the wapaba model to represent seasonal dynamics in this example catchment the comparison reveals underestimation during the driest part of the year and over variable november and december flows but these issues are relatively minor and overall the model seasonal performance is good further information on wapaba calibration performance across all sub areas including a comparison during the historic period is given in supplementary material s6 changing focus now to assess the match in annual streamflow volumes fig 9a indicates the stochastic replicates show very little bias and a similar frequency distribution to historic whereas fig 8 focused on performance in a single example sub area fig 9a assesses performance over a wide area because it compares aggregated flows across the four key reaches of the goulburn river downstream of lake eildon thus the pleasing match reflects performance across the five sub areas that provide reach tributary inflows which represents significant climatic variability see fig 6b the next consideration is whether multi year dry periods and wet periods are well matched fig 9b is based on the same information as fig 9a but the streamflow sequences are accumulated into 5 year rolling sums prior to the analysis overall the stochastic data matches the historic 5 year rolling sums well the lower tail of the distribution is particularly well matched with the median line among the replicates following the historic relatively closely and the lower limit indicating that the stochastic data has five year droughts more severe than those in the historic record as we would expect for a 3 000 year sequence the match at the upper end is less close with a persistent 5 10 underestimation of five year wet sequences in the 10th to 30th percentile range this may also reflect the limits of the stochastic workflow ie ceemdan based split followed by broken line process to replicate the way the years are organised into sequences however considering the many aspects of the stochastic method and the varied hydroclimatic conditions represented the overall result in fig 9b is very favourable 3 7 results 3 changes in rainfall runoff relationship the fifth perturbation stressor change in rainfall runoff relationship is an innovative part of this study so we briefly focus on this aspect of the case study parts of the case study region have experienced historic shifts in rainfall runoff relationship and fig 10 a shows two such catchments specifically the representative catchments for sub areas c and e the latter being the one with the greatest shift among all the sub areas and one catchment that has remained stable sub area a fig 10b shows the practical workflow to shift an existing synthetic monthly sequence down by 15 units in the box cox transformed space recall that the axis varies from 15 units to 50 units table 1 it is hoped that this visual demonstration of the steps helps to clarify the process 3 8 results 4 perturbation of stochastic replicates this subsection examines changes in stochastic data after perturbation fig 11 a to e repeat the seasonality and streamflow volume frequency curves from figs 8 and 9 for each perturbation stressor in each case a higher risk red and lower risk blue perturbation is performed and the result is compared with the baseline except for temperature which is not expected to decrease so a lower risk scenario is not considered fig 10f shows a combination with each stressor exhibiting a small perturbation towards higher risk perhaps unsurprisingly the highest impact scenario in terms of streamflow volumes is the stressor 1 30 reduction in mean annual precipitation which reduces streamflow by 80 90 the next highest is the stressor 5 reduction in rainfall runoff relationship by 25 units which reduced streamflow volumes by 30 and by as much as 50 in drier years for stressor 4 seasonality of precipitation the shift from cold month precipitation to warm month precipitation is reflected in both the seasonal streamflow plots but also in a decrease in average streamflow because more precipitation is falling concurrently with high evaporative demand and thus a larger proportion goes to evapotranspiration the stressor 3 results low frequency behaviour indicate minimal impact on long term average streamflow as expected but the steeper gradient fig 1c iii indicates a more variable climate with larger multi year oscillations around the mean the final test showing a combination of small perturbations in all axes fig 11f is a thought provoking result because none of the axes are very far perturbed yet the streamflow volumes reduce by approximately half thus interactions between the axes are important and may dominate how climate change impacts on water resources a theme that is explored elsewhere john et al 2021b it suggests the value of frameworks such as this one that can explore multiple aspects of climatic change simultaneously and in a systematic way 3 9 results 5 vulnerability assessment for case study for completeness we also summarise the results of the vulnerability assessment for this case study but is noted that the system and ecological modelling required is out of scope of the framework s publicly released code fig 12 shows the system response to climatic changes in terms of changes in allocation reliability for high reliability shares fig 12a and ecological health for large bodied fish fig 12b note that john et al 2021c also assessed a range of other aspects eg low reliability water shares ecological endpoints such as river dependent vegetation and small bodied fish the colour gradients in fig 12 indicate greatest sensitivity to two of the five axes axis 1 plong term average and axis 5 rainfall runoff relationship axis 2 t also has a clear influence on outcomes but is secondary to the first two the insensitivity to axis 4 seasonality is because regulated environmental releases can offset changes in seasonality of inflows thus when repeated for unregulated locations the vulnerability assessment shows an increased sensitivity to seasonality not shown the insensitivity to axis 3 plow frequency arises partly because fig 12 reports on the median outcome whereas this axis causes a widening in the distribution of outcomes without changing the central tendency eg more extreme multi year droughts floods with unchanged median as mentioned above fig 12 also shows interactions between any two of the axes for example a reduction in precipitation of 15 causes the years fully allocated to reduce from 75 to 40 however if this occurs simultaneously with a temperature increase of 3 c the outcome lowers further to 20 this is consistent with the comments above about the significance of interactions while the system is vulnerable to changes in average precipitation most gcms project modest changes in this stressor within the planning window of 2020 2040 in contrast shifts in rainfall runoff relationship have occurred rapidly in the past eg within 1 3 years peterson et al 2021 so the potential for this stressor to impact the system during planning timeframes is relatively acute underscoring the importance of hydrological research on this topic 4 discussion 4 1 limitations the framework is complex and its output multifaceted so we supplement the above figures assessing the outputs ie figs 7 11 with fourteen further figures in the supplementary material section s8 while most of these figures confirm the ability of the framework to replicate the features of historical data there is one category with room for improvement namely spatial correlation figures s8 2 to s8 7 demonstrate that the historic data show a lower spatial correlation between sub areas associated with a greater propensity for a given drought or flood to be more severe in one sub area compared to another one possible contributor could be the annual monthly disaggregation step since the existing mechanism to allocate patterns from wet medium and dry years may not be nuanced enough to capture the complexity of monthly dynamics however given the problem also exists at annual and multi annual timescales it may be associated with the assumption fig 7d that a single combined low frequency sequence is adequate across all sub areas when in fact there were regional differences in timing of the historic low frequency sequences this assumption synchronises the stochastic data across sub areas to a greater degree than in reality given the pleasing results fig 9 at system scale ie after aggregating the sub areas we suggest this limitation is not critical to this case study nor is it likely to be critical to similar cases studies nonetheless future research may examine options to remedy this limitation with a key challenge being defining dynamics across sub areas that is we know each sub area should diverge from the combined sequence but how should these temporal divergences be arrange spatially across different sub areas we leave such questions to future research a higher level limitation may exist regarding the generality of the framework it is unclear whether the order of operations formulated here section 2 4 6 is generalisable to any desired set of stressors it is possible that an additional stressor may exist for which there is no sequential order of operations that allows all stressors to behave independently such cases may require a methodology that examines tradeoffs on the fly between different desired traits of stochastic data eg guo et al 2018 outputting the closest possible stochastic sequence for a given set of perturbations this is very different to the adopted methodology nonetheless this may be somewhat of a hypothetical limitation since the authors have not yet come across a stressor type that would break the current methodology suggesting it has significant potential 4 2 computational effort and appropriate model complexity the purpose of this subsection is threefold i to report on the computation effort required ii to discuss issues of appropriate model complexity related to the framework and or raised while working on the case study and iii to briefly discuss possible future directions regarding sampling of the climate stress testing space in the introduction it was stated that a key benefit of a monthly timestep stochastic framework is improved run times so here we report the computational time taken in the case study working in matlab on a 2016 era laptop hp elitebook with intel i7 7500u 2 7 ghz processor the generation of 3 000 years of stochastic data takes approximately 1 1 s to cover the stress testing space this 1 1 s task is repeated for each possible combination of gradations mentioned in table 1 so there are 12 9 8 8 9 62 208 combinations for a total of 186 106 simulated years the total run time is approximately 19 h thus a laptop can complete all the stochastic generation for the case study in less than one day continuous computing time however runtime considerations are not limited to stochastic data generation the run time of subsequent modelling steps is usually also important in our case study a pre existing detailed daily river systems model was available but it took 10 min to run the 120 year historic sequence which would translate to approximately 30 years in run time for 186 106 simulated years it is hard to imagine an extensive stress test when each model run is so time consuming reframing the model as a monthly model and ensuring the model included only those details relevant for the study as described by john et al 2021c reduced run time by between three and four orders of magnitude to a similar total time as the data generation these design choices including but not limited to timestep enabled a much more thorough exploration of the stress testing space within a modest computational budget in cases where a system model already exists either from previous research or from system operators it would seem straightforward and tempting to adopt such models rather than build a model for each new context however our experience suggests that such inherited models should be carefully assessed for appropriateness for each new research question time spent up front to gain the right model for the question pays dividends later the challenges here are not only technical in our case additional and ongoing effort promoting trust in the new model among project partners was and is key to acceptance among stakeholders our experience designing the framework and case study suggests that issues of appropriate model complexity are important within stress testing studies and need to be considered up front ie prior to investment in more detailed experimental design for example the choice to implement a monthly stochastic framework went hand in hand with the decision to simplify the river systems model including but not limited to the decision on timestep this underscores the importance of an initial holistic planning phase in decision scaling and scenario neutral studies during which model complexity is agreed although issues of appropriate model complexity have been discussed for decades within hydrology eg grayson et al 2002 they are commonly framed in terms of a trade off between realism and identifiability predictive accuracy eg akaike 1973 in the context of stress testing the tradeoff is between complexity realism and the assessment of uncertainty a notion that is relatively new within the climatic sciences eg helgeson et al 2020 the quality of uncertainty assessment in the context of climate stress testing may depend on such details as how many stressors and gradations can be tested within a given computational budget a final issue concerns the appropriate exploration of highly dimensional stress testing spaces sampling may seem straightforward for the common situation of a 2 or 3 dimensional climate stress testing study but a 5 stressor study as here entails orders of magnitudes more computational effort to achieve the same sampling density in response some authors have proposed schemes for initial screening of a larger number of stressors in order to determine those most relevant and produce a lower dimensional space more amenable to high density sampling culley et al 2021 a further question is whether all areas of the sampling space are equally important for example areas suitable for lower sampling density could include areas where system failure is rare or where stressor combinations are physically implausible or considered unlikely more broadly it raises the question of whether a uniform sampling approach remains appropriate in different contexts such as calibration of multi parameter models uniform sampling is usually avoided in favour of directional searches towards some optimum value eg duan et al 1992 or to explore the shape of some behavioural region eg vrugt et al 2008 2013 it is possible that future researchers may adapt these techniques to direct available computing resources to resolve the parts of the space that matter most such as better exploring the form of the failure surface and its boundaries in summary the issues discussed above are i whether to accept an inherited model or build one suited to the context ii if building a model how to tailor it to obtain the best tradeoff between model complexity and uncertainty estimation and iii appropriate sampling of highly dimensional climate stress testing spaces as the science of bottom up climate risk assessment advances and matures we suggest that explicit consideration and discussion of these issues should become more commonplace 5 conclusions although bottom up climate stress tests are becoming more common there are still relatively few frameworks for generating climate and streamflow data for multi site many dimensional more than two stress tests in this paper we have presented a freely available monthly timestep climate stress testing framework while most climate stress testing is focused on the daily timestep monthly timesteps are suitable for systems whose dominant dynamic is seasonal or longer which is the case for many water resource systems furthermore the longer timestep means the framework can generate data rapidly making extensive stress testing possible with a limited computational budget this allows for the inclusion of stressor types such as modulating low frequency climatic behaviour precipitation seasonality and allowing for future changes in rainfall runoff relationship the case study demonstrated how the framework can be applied to a relatively complicated case study with varied climatic conditions across a relatively large spatial scale the requirement for eight different sub areas and four reach inflows was a suitable test to demonstrate the flexibility of the framework the results indicate a good match between historic data and the baseline stochastic output and in particular a pleasing replication of multi year runs testing confirmed that the five stressors can be perturbed plausibly and simultaneously to cover the stress testing space within a modest computational budget less than 24 h computing time on a 2016 era laptop the matlab octave code is freely downloadable from https doi org 10 5281 zenodo 5617008 and is subject to a gnu general public license v3 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements keirnan fowler acknowledges support from linkage project lp170100598 vulnerability of environmental water to a variable and changing climate funded by the australian research council the department of environment land water and planning victoria the victorian environmental water holder and the bureau of meteorology australia dr avril horne was funded by australian research council decra de180100550 andrew john and natasha ballis acknowledge the support of the australian research council via australian postgraduate awards the authors gratefully acknowledge the contributions of the editor and three anonymous reviewers whose comments significantly improved this paper and the framework the authors acknowledge that the framework incorporates code from other sources including the code for complete ensemble empirical model decomposition with adaptive noise ceemdan and code from package tablicious by andrew janke see the framework code for links to the source material in these cases appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105339 
25628,this research utilizes an object oriented bayesian network oobn to model the relationships between the sustainable development goal sdgs and resilience and sustainability at national regional and global levels the ability of the oobn to learn the parameters i e the conditional probability distributions between the variables included in the network was exploited to explore the impacts of progress of sdgs on the sustainability and resilience of nations the resulting oobn is used to examine different situations pertinent to policy analysis and design at the times of disasters particularly in the wake of the covid 19 pandemic three case studies are used to illustrate the step by step process of using the proposed oobn as well as the expected results of its application in policy analysis and evaluation contexts the proposed is able to provide insight regarding which sdgs will have more significant impacts on both resilience and sustainability as well as their constituent components the results of this research indicate how data induced oobns can be utilised by policy makers to prioritize new policies and evaluate the impacts of existing policies on both the resilience and sustainability of societies keywords object oriented bayesian networks sustainable development goals resilience inclusive wealth data driven bayesian networks covid 19 policies 1 introduction sustainable development goals sdg provide a set of universal development goals introduced by the united nations in 2015 to be achieved by 2030 the collection of 17 goals is at the heart of the united nation s 2030 agenda which plots a more prosperous future for humans and the planet willis 2016 stevens and kanie 2016 unlike its preceding set of development goals the millennium development goals the sdg propose an inclusive approach that is more global in nature i e suitable for all nations whether developed or developing the sdg comprise 169 targets and 230 indicators 1 1 for a full list of sustainable development goals targets and indicators see https sustainabledevelopment un org sdgs aspiring to attain a more sustainable future for all nations by eradicating poverty and inequalities improving education and health conditions and protecting global natural assets ferranti 2019 a major advantage of using sdg as a sustainable development agenda for policy making purposes is their realisation of the interlinkages between human well being economic prosperity and healthy natural systems in addition since they are the output of an evolving and collaborative process they are applicable to all nations as they take into consideration different national realities capacities and levels of development schmidt traub et al 2017 saito et al 2017 nonetheless integrating the sdg into national development frameworks imposes multiple challenges on policymakers mainly due to their indivisibility i e their implementation should be based on a holistic approach that considers the interlinked multi dimensional nature of the goals and the feedback effects that may occur among the targets swain and ranganathan 2021 policymakers are also required to prioritize the goals and their subsequent targets as well as determining the scale and pace of achieving them moreover governments need to overcome the challenge of allocating sufficient resources for the realisation of the goals and efficiently plan how such resources will be utilised to achieve the sdg allen et al 2016 while achieving as much progress as possible in the sdg over the span of the designated 15 years is compelling for most countries the pursuit of such achievement involves additional levels of complexity one of those complexities is related to the interlinkages between the different sdgs and their constituent targets sdg are typically evaluated and monitored individually which implies that the interlinkages between the 17 goals and the three dimensions of sustainable development economic social and environmental can be foregone or remain unaccounted for swain and ranganathan 2021 saito et al 2017 allen metternicht and wiedmann 2019a spaiser et al 2017 another important complexity is pertinent to the longevity of the progress accomplished in the sdg for example if a country is able to achieve all the sdgs by 2030 then it will be faced with an important question is such achievement guaranteed to be sustained for extended periods of time or will the country suffer a setback as a result of exhausting its resources to achieve the sdg within the span of 15 years this issue of how to maintain the progress in sdg which is usually referred to as the sdg sustainability accentuates the need to link the sdg with a framework that enables policymakers to evaluate how sustainable the achievements of sdg are palavalli et al 2018 another argument related to using sdg as a development guiding framework is their coherence with the concepts of resilience and reduction of disaster the importance of aligning sustainable development with resilience and disaster risk management has been argued long before devising the sdg by numerous researchers o brien et al 2006 mercer 2010 derissen et al 2011 redman 2014 murray et al 2017 such a notion became of particular importance recently due to the increase in the frequency and severity of disasters especially natural disasters induced by climate change and the ensuing human and economic losses barros and field 2014 munich 2017 the probabilities and consequences of current changes in planetary systems emphasise that resilience needs to be actively managed in order to maintain development on a sustainable track folke et al 2020 for example the rapid spread of pandemics like covid 19 and the subsequent stresses imposed on the nation s produced and human capitals has resulted in many countries declaring that they will need financial support in order to be able to achieve the sdgs in the designated time frame allen et al 2021 folke et al 2021 although this relevance of resilience to sustainability motivated the inclusion of targets that focus on reducing disaster risks in the sdg framework those targets are not as bold or well defined as the ones related to development ono and nagaishi 2015 in fact one of the shortcomings of the sdg is that they lack a comprehensive incorporation of the key resilience capacities and dimensions as the framework focuses mainly on adaptive responses toward disasters for instance target 1 5 which is considered the core resilience target uses the number of people affected by disasters as a performance indicator such indicator measures the effect of a disaster but does not reflect reducing the vulnerability nor increasing the preparedness toward hazards assarkhaniki et al 2020 bahadur et al 2015a b similarly target 13 1 aspires to strengthen the resilience of countries through increasing their adaptive capacity to climate related hazards that partial view of resilience is not in line with recent research that considers resilience as a product of interconnected capacities assarkhaniki et al 2020 bahadur et al 2015a b this has led to different researchers stressing that in order to ensure resilient development is achieved resilience needs to be tracked through its main composing capacities and linked to the goals and targets of the sdgs framework bahadur et al 2015a b those shortcomings of sdgs in terms of representing resilience can be addressed by integrating them with other complementary frameworks to form a coherent and consistent picture to inform policy making peters et al 2016 etinay et al 2018 issues related to sustainability and resilience are complex comprise interlinked systems and can only be resolved using holistic representation and analysis approaches that yield deeper realisation of such dilemmas chan et al 2004 additionally the interdisciplinary nature of those complex issues requires collaboration between scientists from different fields as well as effective communication between the scientific and the policy making communities heemskerk et al 2003 challenges such as these in addition to other complexities like the deep uncertainty entrenched in understanding expressing and managing the two concepts necessitate employing methods that can combine bottom up and top down views of the included sub systems explore multiple plausible futures support social learning and handle sparse imprecise data little et al 2019 allen metternicht and wiedmann 2017a modelling has played and still plays a central role in managing topics related to socio environmental systems as well as topics related to resilience schlüter et al 2019 models have been used for a multitude of purposes such as policy assessment the optimisation of management strategies analysing system component interactions and facilitating participatory processes liu et al 2008 schlueter et al 2012 kelly et al 2013 voinov et al 2016 aly and managi 2018 egli et al 2019 models can also function as vehicles that bring new perceptions of sustainability and resilience issues to attention at national and international levels in addition to being an effective tool for creating political consensus in the process of international negotiations van daalen et al 2002 the ever growing literature on sustainability and resilience modelling applications and guidelines demonstrates how models can be an invaluable for supporting the achievement of sdg in an equitable and truly sustainable manner modelling the sustainability and the resilience of the development guided by the sdgs however is a challenging task that needs to be carried out carefully for multiple reasons one of those reasons is the need to select frameworks to represent each of the concepts i e the sustainability and the resilience so that they are coherent with the sdgs additionally linking each of the frameworks with the sdgs needs to be done in a way that is intuitive measurable and revealing to the impacts of sdgs progress on the social economic and environmental aspects of societies achour et al 2015 there is also the need to assimilate knowledge and perspectives that are fragmented across different disciplines as well as incorporating different underlying assumptions which can be conflicting schlueter et al 2012 to be informative for policy making and decision support purposes any model built around resilience and sustainability of sdgs needs to have a set of certain characteristics first it should be able to identify long term problems and strategic issues geertman and stillwell 2003 van delden et al 2007 as well as being able to represent complex systems that contain large number of variables and high level of uncertainty van delden et al 2007 mcintosh et al 2007 the model also needs to be able to handle different types of input data e g qualitative and quantitative data while its output can be easily reviewed and analysed in addition to be easy to interpret volk et al 2008 ahalt et al 2014 the scalability of the model is another important feature along with having a component based structure so it can be easily modified extended and updated elsawah et al 2020 little et al 2019 in addition the model should be capable of translating scientific knowledge into policy relevant information with an intuitive communicable format volk et al 2008 1 1 this paper this paper aims to use modelling to drive thinking about sustainable development through its three intertwined composing systems while systemically consider resilience through its main driving capacities in order to do so we attempt to unravel the interlinkages between resilience and sustainability of sdgs at a national scale this study proposes a data induced object oriented bayesian network oobn that integrates two acknowledged sustainability and resilience evaluation frameworks with the united nation s sdgs to capture how progress toward sdgs jointly affects the nations capital assets and resilience capacities linking sdgs with sustainability and resilience frameworks provides a better translation between the high level thematic concepts and policy actions in addition to allowing policy makers to prioritize strategies based on the trade offs between resilience and sustainability oobns was selected as a modelling technique because of its conformity with a wide range of the desirable characteristics that need to be present in the models addressing such complex issues as will be discussed in the coming sections this application utilizes data about sdg indicators and resilience and sustainability indicators to infer the parameters defining the relationships between the variables included in the oobn to the best of our knowledge no previous research has made to use oobns to mutually model sdgs resilience and sustainability at a national level in contrast to the wide body of literature that attempts to model sdgs individually this paper focuses on modeling the sdgs collectively while this approach can be complex from a policy making perspective such complexity is important and beneficial for the policy making process for a number of reasons first tackling the sdgs individually can lead to devising incoherent policies with development plans in some sectors which negatively affect development plans of other sectors mainali et al 2018 second one of the main principles that distinguishes the 2030 agenda is that it is meant to be indivisible which means that it should be viewed as a unified whole and implemented in an integrated manner bennich et al 2020 this indivisibility of the sdgs implies that they need to be effectively prioritized and that policy makers need to be aware of the complex feedback effects that link the goals allen metternicht and wiedmann 2019a additionally having a holistic consideration for the sdgs will help policy makers in building effective alliances and communication channels with key stakeholders who play a vital role in advancing the achievement of the goals nilsson et al 2018 for such reasons ensuring that the sdgs are jointly considered in the policy making process was identified as one of the priorities for research efforts that aim at supporting the implementation of the 2030 agenda allen et al 2021 the rest of this paper is organised as follows in section 2 research background we discuss the definitions of resilience and sustainability and limitations of the sdgs from the point of view of each of the concepts section 2 also provides details about the resilience and sustainability evaluation frameworks selected for inclusion in the model and how the connections between the two concepts and sdgs are established section 3 object oriented bayesian networks describes the modelling technique used to represent the concepts of resilience and sustainability then the methodology used in developing the oobn and the model formulation details are provided in the methodology section finally the model results are discussed in section five followed by the conclusion 2 research background this section presents an overview of the research concerned with the sdgs sustainability and linking resilience to the sdg framework it first broadly presents the attempts to inspect the relationship between long term sustainability and the sdg as well as the attempts to conceptualize resilience from sdg perspective after that we discuss the frameworks chosen to assess both sustainability and resilience within the scope of this paper 2 1 previous work on sdgs sustainability and resilience since their inception different aspects of the sdgs have been studied by the scientific community in order to foster and support their implementation and reinforce the knowledge base related to them leal filho et al 2018 recent reviews of the literature discussing sdgs show that most of this literature has directed its focus toward studying the interlinkages and trade offs among the sdgs the policy issues that can hinder the implementation of the sdgs monitoring and evaluation of the sdgs implementation and the interactions between the sdgs at finer levels such as target and indicator levels bennich et al 2020 breuer et al 2019 allen et al 2019a b c however the themes of sdgs sustainability and resilience have not gained as much traction as the other topics related to sdgs in particular modelling falls short in addressing all 17 sdgs and their subsequent targets as well as considering long term sustainability beyond the 2030 time frame additionally few models provide a holistic interpretation of the sdgs that enables the assessment of the economic social and environmental sustainability pillars along with the resilience of these pillars zimm et al 2018 researchers interested in inspecting the sustainability of the sdgs mainly focus on studying the structure of the sdgs and how they relate to the three components of sustainable development they observed that the three forms of capitals that characterises traditional sustainability models do not have a balanced representation in the sdgs as a matter of fact social capital is dominant over the environmental and economic capitals costanza et al 2016 kettunen et al 2018 fioramonti et al 2019 this imbalanced distribution of capitals can lead to a false impression of sustainability even when the sdgs are achieved giannetti et al 2020 this observation was further studied using different analytical methods jain and jain 2020 for instance performed correlation regression and path analysis to explore the relationships between the sdgs and three sustainability components they represented the social and economic capitals in terms of the human development index hdi and the environmental capital was expressed using the ecological footprint and the earth s biocapacity their analysis showed that the progress in sdgs had adverse impacts on the environmental capital and a positive impact on the other types of capital this finding is in accord with the structure of the sdgs which leans more toward social aspects of development another approach followed by wackernagel et al 2017 classified the goals into three categories based on resources dependence the first category contained the goals that lead to a decrease in the nations resources dependence i e the activities that result in saving resources hence increase their availability the second class incorporated the goals that requires consuming more resources in order to be achieved such improving the health care and education infrastructure the final category represented the goals that have no effect on the resources based on this categorisation they found that the resources dependent goals make up 67 6 of the goals while the resources sustaining goals represent only 13 6 they concluded that the sdgs in its current format compromises resources security which is a major aspiration of sustainable development this line of research stresses that the progress in sdgs should not be pursued in isolation of a continued monitoring of the three main capital assets if sustainability to be achieved therefore another sustainability model that put explicit emphasis on jointly preserving the environmental social and economic capitals needs to be adopted alongside the sdgs biggeri et al 2019 nonetheless no attempts have been made so far to establish such a link between the sdgs and the desired sustainable development framework on the other hand resilience is intended to be rooted in the sdgs because like most of the post 2015 development frameworks the framework adopts the position that resilience is a component of sustainability marchese et al 2018 allen et al 2019a b c assarkhaniki et al 2020 despite such conceptualisation the sdgs have been criticised for not providing a comprehensive representation of resilience in terms of its main capacities nor do implementing the goals guarantee reaching a resilient development ono and nagaishi 2015 assarkhaniki et al 2020 the attempts to overcome this drawback in the sdgs included some researchers proposing new indicators for the targets that explicitly state resilience as an aim such as target 1 5 the main reason for choosing this particular target is that it is the one concerned with building the resilience of those in vulnerable situations and reduce their exposure and vulnerability to climate related extreme events and other economic social and environmental shocks by 2030 so it is considered the core resilience target another motivation to seek additional indicators for this target is that it is considered extremely broad in regard to the shocks and stresses it addresses the proposed new indicator measures resilience through its three main capacities taking into consideration the level of exposure to hazards bahadur et al 2015a b that new indicator however was only proposed for a single target i e target 1 5 and just presented the theoretical framework without calculating it using real life data the main idea behind such modified indicators is that three major considerations need to be present when measuring resilience 1 resilience is delivered through a set of resilience capacities as this ensures the nations ability to handle shocks and disturbances and continue development despite of them 2 exposure to hazards and risks should be part of measuring resilience because the intensity of such risks can offset the progress in building the resilience capacities and 3 resilience development cannot be achieved without seamless integration of the frameworks developed throughout 2015 and 2016 bahadur et al 2015a b bahadur et al 2013 peters et al 2016 etinay et al 2018 in order to effectively address the aforementioned issues related to the sustainability and the resilience of the sdgs and to incorporate the two concepts in a model we need to settle on a suitable conceptualisation and measuring framework of each that can be coherently integrated with the sdgs a suitable which in this context means aligned with the model s purpose and the technique used to develop it conceptualisation and measuring framework needs to 1 have a holistic view that considers the society s environmental social and economic systems to ensure that funding the ambitious sdg development agenda does not impose undesirable long term externalities on the three sustainability pillars as well as ensuring a balanced development across these pillars dalby et al 2019 cutter et al 2015 coopman et al 2016 fukuda parr and mcneill 2019 2 be applicable on a national scale to be consistent with the need to localize the sdgs i e translating them from their global scale into national scales salvia et al 2019 smith et al 2018 galli et al 2018 allen et al 2016 3 have a component based structure which is important for breaking disciplinary silos and to facilitate applying the integrative modelling approaches necessary to achieve the sdg hamilton et al 2015 van soest et al 2019 moyer and hedden 2020 collste et al 2017 liu et al 2015 coopman et al 2016 4 have available data qualitative or quantitative as this is a critical element for performing any analysis or assessment related to the sdgs ordaz 2019 kraak et al 2018 georgeson and maslin 2018 allen et al 2018 5 have measurement in the form of a composite indicator ci as they represent an increasingly recognized approach for policy analysis and national performance benchmarking singh et al 2009 saltelli 2007 a ci is a hierarchical indicator composed of harmonised aggregated sub indicators nardo et al 2006 2 2 sustainability definition and assessment framework the concept of sustainability can be traced back to the 1897 report of the world commission on environment and development which defined sustainable development as the type of development that meets the needs of the present without compromising the ability of future generations to meet their needs borowy 2013 such definition indicates that intergenerational welfare being the ultimate goal of sustainable development can be achieved by maintaining a non decreasing flow of goods services and utility form the environmental social and reproducible capitals among generations dasgupta and mäler 2000 harlow et al 2013 several sustainability assessment approaches have appeared over the years such as environmental pressure life cycle assessment process energy or life cycle cost assessment these approaches however are considered non integrated as they only focus on a single dimension of sustainability i e either the economic the social or the environmental dimension schoenaker et al 2015 other indicators that have a more holistic view of sustainability are genuine adjusted net savings the well being index the green net national product and the inclusive wealth index hanley 2000 dasgupta et al 2015 the most recent of these indicators is the inclusive wealth index iwi which measures the changes in the nations capital assets thus reflects their economic growth environmental stewardship and social vigour in a sensible form fenichel et al 2016 as the name suggests the iwi measures a nation s wealth at any point in time by gauging its three main capital assets namely reproducible capital rc human capital hc and natural capital nc arrow et al 2012 unu ihdp unep 2012 the wealth per capita wpc at any time is the sum of the stocks of the three main capital assets weighted at the social value of each stock also known as the stock s shadow price and normalised by the population at that time dasgupta 2009 arrow et al 2012 wpc is also adjusted to include the effect of total factor productivity and co 2 emissions unu ihdp 2012 so it can be represented as a function of the aforementioned components as shown in equation 1 1 w p c f r c h c n c t f p c o 2 such a sustainability measurement is therefore very suitable for our model because of its component based structure hierarchical form data availability through the periodically published inclusive wealth reports its ease of interpretation and communication and its applicability on a national level additionally when integrated with the sdgs the inclusive wealth framework reveals the influence of achieving the sdgs on all the three capital assets comprising nations resource base as well as any carbon damages resulted from progressing toward sdgs i e it is an adequate measure for the sustainability of the sdgs 2 3 resilience definition and measuring framework the concept of resilience has been used widely in numerous domains bhamra et al 2011 by multiple disciplines which leads to resilience having multiple definitions and consequently many disparate measuring approaches considering the main components of socio environmental systems the definitions change between these components as shown in table 1 such lack of a common definition for resilience hampers decision makers efforts to deliver resilience supporting policies consequently the united nations office for disaster risk reduction and the world humanitarian summit are leading an ongoing effort to develop a common definition for resilience with a special focus on understanding it as an outcome rather than a process peters et al 2016 unfortunately current resilience definitions do not facilitate formulating a resilience metric that can be applied across different domains or in a system of systems context as they do not have an intuitive clear links to the determinants of resilience ayyub 2014 three main system capacities are considered as the determinants of resilience or the resilience pillars and are commonly used in literature to represent resilience vugrin et al 2011 hossain et al 2019 bahadur et al 2015a b absorptive capacity refers to the system s ability to absorb shocks and to be minimally impacted after a disruptive event adaptive capacity the system s ability to adapt and cope with consequences of a disruptive event without the aid of external recovery activities restorative capacity represents the system s recoverability from a disruptive event in order to overcome the weaknesses of the sdgs in handling resilience additional frameworks that are designed around resilience and risk reduction have been integrated with the sdgs namely the sendai framework for disaster risk reduction sfdrr and the world risk index wri the sfdrr was initiated at the third united nations world conference on disaster risk reduction in 2015 as the first item on the un s post 2015 development agenda unisdr 2015 the post 2015 agenda includes three other un flagship agreements the sustainable development goals the paris climate agreement and the habitat iii urban agenda maini et al 2017 sfdrr has the same time span as the sdgs which extends between 2015 and 2030 and represents a key component of the united nations ongoing efforts to reinforce the policies and practices that increase the governments capabilities to manage disaster risks the sendai framework was preceded by another disaster risk reduction framework the hyogo framework for action hfa which was initiated in 2005 unisdr 2005 hfa identified the main sectors that countries should focus on in order to increase their resilience against disasters and the indicators included in the framework were utilised by researchers in order to assess resilience on national and sub national scales e g kammouh et al 2018 and handayani et al 2019 unlike previous efforts the framework focuses on managing the conditions of hazards rather than just the responses to hazards by considering the interplay between the exposure to hazards the vulnerability of systems and the capacities that communities need to develop to be well prepared against disasters and shocks aitsi selmi et al 2015 wahlström 2015 wri on the other hand is an index developed by the un university s institute for environment and human security unu ehs that describes the disaster risk for various countries as well as assessing their vulnerability to suffering from hazards birkmann and wisner 2006 wri is calculated as a function of exposure to hazards and vulnerability of societies therefore incorporating it in measuring the resilience of the sdgs mean that the exposure to hazards is accounted for in the wri assessment vulnerability of societies is an indigenous factor that depends on three main factors susceptibility coping and adaption birkmann et al 2011 although wri does not contain a direct reference to resilience different scholars established a direct link between vulnerability and resilience lavell et al 2012 bergstrand et al 2015 joakim et al 2015 so that when vulnerability increases the level of resilience automatically decreases kasperson and kasperson 2013 for instance adger 2000 sees resilience as a loose antonym for vulnerability such a notion suggests that resilience and vulnerability can be represented along a continuum berkes 2007 birkmann and wisner 2006 this relation between resilience and vulnerability particularly holds when the formalisation of vulnerability includes the inverse of coping capacity and adaptive capacity yohe and tol 2002 o brien et al 2007 which is indeed the formalisation adopted by the wri this view is also shared by the intergovernmental panel on climate change ipcc barnett et al 2008 based on this view vulnerability can be converted to resilience and vice versa similarly the three resilience capacities can be calculated from the three vulnerability components integrating the sfdrr with the sdgs when modelling the resilience of the latter is logical and important for a number of reasons first the two frameworks when applied together is recognized to compose an efficient resilience agenda murray et al 2017 therefore integrating the two frameworks is recommended by different experts such as the inter agency and expert group on the sdg indicator and the open ended intergovernmental expert working group peters et al 2016 the sfdrr is designed to be coherent with the sdgs rose et al 2020 in fact one of the main purposes of the sfdrr is to support the other post 2015 frameworks peters et al 2016 this clearly appears in the 38 targets that directly link the sdgs to the sfdrr another reason is that different countries that are prone to natural hazards are committed to both frameworks as an integrated policy to build resilience while remaining on a sustainable development track an example of this is the framework for resilient development applied by some of the pacific island countries pacific community f s secretariat of the pacific regional environment programme et al 2016 moreover resilience in the sfdrr is to be built against multi hazards which reflects how such framework pursues a more holistic and inclusive strategy that protects humans livelihoods cultural heritage infrastructure economic systems and environmental systems this appears in the seven global targets set by the sfdrr to be achieved by 2030 which are shown in table 2 in order to integrate sdgs with the selected resilience and sustainability frameworks we map each of the frameworks to the relevant sdg indicators using either evidence from literature or via logical mapping for instance the mapping between the sfdrr targets and the sdg indicators is based on the linkage mentioned in the framework s guidebook unisdr 2015 however because no direct mapping exists in the literature between sdfrr targets and the resilience capacities they are mapped logically absorptive capacity for example is related to the system s ability to resist damage in the cases of extreme events hence it is mapped to all the sfdrr measures that are related to reducing losses e g number of injured people attributed to disasters and direct economic losses attributed to disasters similarly adaptive capacity is understood as the long term strategies and structural changes that lead to reduced negative impacts at the times of hazards as such adaptive capacity is mapped to the measures that increase the preparedness of the system such as having multi hazard monitoring and forecasting systems restorative capacity is the system s ability to heal after a shock and it includes the resources and measures used to regain the system s functional form measures mapped to this capacity include total government resources as a proportion of gdp and total available official international support as they directly increase community resourcefulness details of the mapping step are included in table a1 in appendix 1 finally the values for resilience and its comprising capacities are derived from the wri data 3 object oriented bayesian networks oobn bayesian networks bn and in particular oobn were selected to model the relationships between resilience sustainability and the sdgs bns are probabilistic graphical models that represent the relationships between a set of variables through conditional probabilities koski and noble 2011 nielsen and jensen 2009 a bn consists of two major components aguilera et al 2011 first a qualitative component that is a directed acyclic graph dag in which the nodes indicate the set of random variables x x 1 x n included in the model and the arcs denote the statistical dependence between the nodes they connect second a quantitative component which is the conditional probability distribution for each variable in the graph given its parents i e p x i pa x i where pa x i are the parents of x i the efficiency of bns to model complex multidisciplinary problems can be increased by breaking down such problems into their main constituent components then representing each component with a modular network benjamin fink and reilly 2017 that notion of using modular networks to represent parts of complex domains as well as the utility of reusing such modular networks in similar problems lead to the introduction of object oriented bayesian networks oobns where concepts of object oriented programming are utilised to build more flexible and efficient bns koller and pfeffer 1997 oobns allow generalisation over multiple sub networks that have similar probabilistic models in an oobn setting classes define sub networks that have similar structure while an object oriented network fragment oonf represents an instantiated class koller and pfeffer 1997 each of the oonfs represents a partially separable component of the system and the oonfs are loosely coupled to represent the whole system even though some researchers argue that oonfs instantiated from the same class will have similar structure and parameter values koller and pfeffer 1997 oonfs can be more adaptive and case specific by changing the parameters based on changes in the situation to be modelled liu et al 2016 as such parameter values for oonfs can be learned for each unique situation coupling between oonfs is achieved through interface variables which is the class representing variables that are common between the different sub systems interface variables can be represented by two types of nodes input nodes and output nodes neil et al 2000 input nodes cannot have parents in other classes while output nodes can have parents from classes other than the interface variables class bangsø et al 2003 bn is an apt approach because of its ability to handle most of the modelling requirements of such complex problems first the very probabilistic nature of bns is appropriate for representing the inherent uncertainty of the socio environmental systems underlying the modelled concepts uusitalo 2007 spanning over different disciplines and scales socio environmental systems models are mostly developed and applied in conditions of partial information and require knowledge from different sources to be integrated in the model little et al 2019 aguilera et al 2011 additionally both qualitative and quantitative data need to be included in these models elsawah et al 2020 bns have the ability to handle different types of data and produce decent model results even when data is partial or uncertain newton 2010 aguilera et al 2011 chen and pollino 2012 one more valuable feature of bns is that they have a causal graphical structure that can be relatively easily understood by stakeholders and non technical users cain et al 2003 henriksen et al 2007 such a feature increases the chances for participation of stakeholders and decision makers in the modeling process and increases the final model s efficiency and usability voinov and bousquet 2010 that graphical structure also increases the model s transparency and interpretability in comparison to other types of models stritih et al 2020 and permits for easily visualising causalities propagation consequently allowing for top down analysis and bottom up analysis through backward reasoning and forward reasoning respectively chen and pollino 2012 bns are commonly used in the context of resilience and risk analysis however most of the applications using them in that context focus on technical systems with no inclusion of environmental or social systems hossain et al 2019 kameshwar et al 2019 a few researchers however have utilised bns to model the resilience of multiple systems albeit on a sub national scale e g kammouh et al 2020 bertone et al 2018 tabandeh et al 2019 de iuliis et al 2021 b and de iuliis et al 2021 a in the context of sdgs giné garriga et al 2018 proposed an approach for a single sdg management using an oobn additionally requejo castro et al 2020 utilised bns ability to learn network structure from data to explore the relationships between sdg6 and the rest of the sdgs these few activities show that bns have very limited applications in modelling sdg or the resilience of multiple systems 4 oobn modeling methodology this section describes the methods used to formalise the oobn model representing sustainability resilience and their links to the sdgs based on the selected respective composite indicators unlike other approaches that attempted to discover the relationships between the variables comprising composite indices through bn structure learning requejo castro et al 2019 our approach capitalises on the mathematical form of the selected index to link the variables together such mathematical form is considered as a universally accepted formalism that embodies knowledge elicited from experts and crafted by modellers nardo et al 2006 bn parameter learning is applied however to infer the conditional probabilities between the variables in the network a major advantage of relying on knowledge based methods such as the mathematical form of the index to inform the network structure is minimising the uncertainty associated with learning the structure from data as well as avoiding network structure overfitting marcot 2017 rizzo and blackburn 2018 unlike the network parameters uncertainty the uncertainty associated with learning the network structure from data is usually harder to quantify and requires more complex treatment methods o hagan 2012 uusitalo et al 2015 moreover expert opinion and knowledge embedded in literature are used in several cases to validate the network structure learnt from data marcot 2017 requejo castro et al 2019 therefore we chose to directly employ the existing validated knowledge 4 1 stepwise formalisation of the oobn fig 1 outlines the methodology applied to develop the oobn before discussing the details of these steps the key assumptions of the model are presented as the proposed model depicts sustainability resilience and sdgs using indices measured at country level for 250 countries around the globe each entry in the data used for parameter learning marks a country c i with an array of features x y measured at a certain year y so that every x j y x y maps to a node in the bn i e j the number of nodes in the developed network for this application y 2014 was chosen to be the control year because most of the variables have data available for this year selecting the most recent year where the most data is available to learn the bn parameters and consequently predict the relationship between countries resilience sustainability and sdgs is based on the assumption that the system s states prior to the current state only affect the future through the current state murphy and russell 2002 this assumption which is the first key assumption in our model hinges on one of the markov chains characteristics which states that the future states of some system are dependent only on the current state of that system keselj 2009 thomas and sobanjo 2013 the second key assumption is related to preserving the hierarchical structure of the composite indices and states that nodes in the developed network can only be parents of nodes that are at the same or the following hierarchical level finally we assume that the missing data for any variable included in the bn is either missing completely at random mcar or missing at random mar mcar data means that any missing values are independent from the variable it is related to or from any other variable in the bn mar data on the other hand are independent from the variable it is related to but may be conditional on other variables in the bn nardo et al 2006 this assumption is important because most of the data imputation methods require that the data to be imputed is not systematically missing bhaskaran and smeeth 2014 aljuaid and sasi 2016 4 1 1 determine oobn structure as shown in fig 1 the first step s1 is to conclude the network structure from the frameworks selected to represent the different concepts that will be included in the network i e the sdgs the inclusive wealth framework the sfdrr and the wri in order to determine the oonfs that will compose the full oobn an oonf class is assigned to each of the composite indices ci used to represent each framework afterwards the common variables between the indices will determine the interface between the oonfs cis are multi variable indicators that have been used widely by different multi dimensional and multi disciplinary frameworks nardo et al 2006 bandura 2008 different researchers have utilised cis to inform the structure of bns benefiting from the hierarchical structure they provide e g srinivas 1994 johnson et al 2017 nannapaneni et al 2017 romanko et al 2019 requejo castro et al 2019 kammouh et al 2020 complying with the oop notations the composite index ci bayesian network is a special type of bn thus they inherit from the abstract class bayesian network as shown in fig 2 as an abstract class bayesian network can be considered as a blueprint for other classes an abstract class is a special type of classes that can not be directly instantiated nonetheless it can be used as a super class that helps in defining other classes clarke et al 2012 berardi et al 2005 ci bayesian network in our case is a concrete class that extends the abstract class bayesian network the ci bayesian network class is used to instantiate the two oonfs of resilience bn and sustainability bn with the interface vars being the variables common between the two oonfs in an oobn the interface vars represent the nodes that exist in different fragments or oonfs therefore they are used to couple the oonfs together in our case the interface vars will be the sdgs indicators that are common between the sustainability bn and the resilience bn 4 1 1 1 sustainability oonf this network fragment models the index selected to represent sustainability i e the inclusive wealth index as mentioned with more details in section 2 2 this index measures the wealth of nations as a function of their capital assets in addition to the co 2 emissions produced by their productive base and the total factor productivity thus this index can be presented as shown in equation 2 from equation 2 wpc will be a childless node at the very bottom of the sustainability network fragment while the wealth per capita components appearing on the rhs of equation 2 will be parent nodes of wpc 2 w p c f n a t u r a l c a p i t a l p r o d u c e d c a p i t a l h u m a n c a p i t a l c o 2 d a m a g e s t f p similarly based on the mapping between the sdgs and the iwi each of the five of components of the wpc has the sdg indicator parents shown in table 3 4 1 1 2 resilience oonf the resilience network fragment is constructed in the same manner as the sustainability network fragment by tracing the components of the resilience composite index as such resilience can be presented as a function of its main capacities as shown in equation 3 such presentation means that the resilience node will be a childless node at the very bottom of the resilience oonf with three parent nodes each representing one of the capacities shown in equation 3 3 r e s i l i e n c e f a b s o r p t i v e c a p a c i t y a d a p t i v e c a p a c i t y r e s t o r a t i v e c a p a c i t y as explained in details in section 2 3 in order to be able to assess resilience in national contexts we adopted the approach in literature that considers resilience as the complement of vulnerability nonetheless the uncertainty about the definition of resilience and its similarity with vulnerability of systems can be explicitly introduced to the model by using a special type of variable modelling namely the noisy max function the noisy max model is an extension to the binary noisy or model that generalises it to incorporate multi valued variables as well as reducing the effort needed for knowledge elicitation and improving the quality of the parameters learned from data diez 1993 henrion 2013 in such model a variable x with s x number of states is conditioned on the set of its parents z in addition to a leak factor θ and can be written in the form of equation 4 the leak factor θ can be interpreted as the effect of some missing factor s on the variable under consideration and is measured as the probability of achieving the variable x even at the absence its parents z in the case where a noisy max node parameters are learned from data the observed probabilities is assumed to include the leak probability which then can be calculated using equation 5 where z i is the ith parent of the variable x p i is the effect of z i in the absence of all other parents and p θ is the probability of x in the absence of all the causes that explicitly included in the model i e the leak factor oniśko et al 2001 4 θ x p x 0 z 0 5 p x z i 1 1 p θ z i z 1 p i 1 p θ modelling resilience as a noisy max variable enables us to inspect the uncertainty about the similarities between resilience and vulnerability such uncertainty is manifested in the leak factor associated with resilience after parameter learning in the particular case of using the data from the wri to represent resilience and its comprising three capacities as presented in equation 3 the leak factor associated with each state of resilience is the probability that this resilience state is affected by hidden factors other than the three chosen capacities the parents of the three resilience capacities are the seven sendai targets while the parents of the sendai targets are the sdg indicators the parents of each specific node in the three sets of nodes are shown in table 4 and table 5 in addition to the logical benefits of using the sfdrr that we mentioned in the problem background section using such framework provides another important value to the model as it creates an intermediate level of nodes between the sdgs and the resilience capacities which reduces the model s complexity the model complexity and the effect of intermediate levels of variables will be discussed in further details in section 4 1 5 finally to conclude this first step the sdg indicators that are common in the two oonfs i e the interface nodes are determined table 6 shows these 12 interface nodes out of the total 51 sdg indicators included in the full network 4 1 2 data collection and pre processing data collection and pre processing is covered by steps s2 to s6 shown in fig 1 and starts with collecting the data associated with each variable in the oobn from its respective source the variables data can be generally grouped into three main groups sustainability variables data resilience variables data and sdgs indicators data data for sustainability variables were obtained from the inclusive wealth report 2018 managi and kumar 2018 which includes data till the year 2014 for 140 countries resilience variables data were obtained from the world risk report 2014 birkmann et al 2014 which contained data for 171 countries summaries of the sustainability data and resilience data are shown in table 7 no data were available for the sendai targets a to g hence they were treated as latent variables and data for them is approximated at a later step the remaining group of variables is the sdg indicators group and for each variable in this group data were collected from the respective source as shown in table a2 in appendix 1 data for the selected year for all the sdg indicators are not always available in such case we search for available data for other years for the same indicator if data for other years are available then knn imputation is used to find a value approximation for the desired year knn imputation is a pre replace method that replaces missing values prior to data processing troyanskaya et al 2001 this method classifies data into clusters then replaces the missing values with the corresponding values from their nearest neighbour based on euclidean distance aljuaid and sasi 2016 this method was chosen for data imputation at this stage for its simplicity and ability to produce results even when small number of data points is available moreover it imputes the missing values considering other values that are mostly similar to the instance of interest which increases the accuracy of the overall learning process kumutha and palaniammal 2013 if no data are available for other years for the variable under consideration we check whether the parents of that variable have available data or are they latent variable i e no data available for them if the variable parents have data then the variable data are estimated using simple aggregation of normalised parent values nardo et al 2006 otherwise the variable is excluded from the network to minimize the influence of missing data on the final results applying this sequence on the sdg indicators selected to be included in the network because of their direct connection with sustainability and resilience led to decreasing the number of originally chosen indicators from 69 indicators to 51 indicators 4 1 3 variables bivariate correlation analysis one of the fundamental assumptions in probabilistic inference which applies to bayesian networks is that the parent nodes of some child node are uncorrelated as long as they are not linked spiegelhalter and lauritzen 1990 failing to establish links between the correlated variables in a bn leads to an incorrect or an inaccurate representation of the network probabilistic structure which causes the influence dynamics of the network to be biased marcot 2017 in order to avoid this problem experts recommend that input variables should be tested for correlation before any learning or analysis is performed and the highly correlated ones either be linked or excluded from the model marcot 2017 as fig 1 shows step s7 checking for correlation between the network variables comes after data collection and pre processing and for that task we use pearson correlation coefficient with a threshold of 0 85 to determine the highly correlated variables because there is no standard value for the threshold of high correlation researchers rely on rules of thumb to determine that threshold with some considering values between 0 7 and 1 as an indication of high correlation marcot 2017 and others consider values between 0 7 and 0 9 as high while values between 0 9 and 1 as very high mukaka 2012 the result of the correlation analysis presented in fig 3 highlights five positions where the correlation coefficient is greater than the selected threshold the first is between indicator 3 1 1 maternal mortality ratio and indicator 3 2 1 proportion of births attended by skilled health personnel we chose to exclude indicator 3 2 1 as there are more data points available for the other indicator the second pair of highly correlated indicators are 3 9 2 mortality rate attributed to unsafe water unsafe sanitation and lack of hygiene and 3 2 1 from these two we will keep 3 9 2 based on the previous exclusion of 3 2 1 the indicator 3 2 1 was also highly correlated with 3 8 1 coverage of essential health services which is kept in the network indicators 3 9 2 and 3 1 1 were also highly correlated but the former has more available data and is more related to technical systems hence it was the one kept in the network finally the indicators 9 1 2 passenger and freight volumes by mode of transport was excluded as it was highly correlated with 9 4 1 co2 emission per unit of value added however 9 4 1 has much higher available data and is connected to more variables in both the resilience and the sustainability oonfs 4 1 4 continuous variables discretization after determining the final network structure the final set of variables that will be included in the network and the data associated with each variable the next step s8 in fig 1 is to discretize the data associated with each variable to convert it from a continuous form to a discrete form variables included in the model contains a mixture of discrete and continuous variables nonetheless in a lot of cases continuous variables need to be discretized because many bayesian networks structure and parameters learning algorithms assume all variables are discrete chen et al 2017 the term discretization refers to the process of binning continuous data so that each bin represents a certain category and the data points lying within the boundaries of a bin is substituted with that bin s label kotsiantis and kanellopoulos 2006 the discretization scheme of a network s continuous variables has a considerable impact on the network performance accuracy and interpretability chen et al 2017 hammond and eric bickel 2011 in spite of the significant impact it has on the bn performance and results the used discretization method remains one of the most under reported features in bn models in literature for instance a review of bns applications in environmental modelling revealed that 48 6 of the reviewed papers did not include any information about how continuous data was discretized aguilera et al 2011 the process of variable discretization entails approximating the statistical properties or moments such as the mean variance and skewness of the original continuous variable thus a good discretization algorithm will provide the closest approximation to the variable moments hammond and eric bickel 2011 discretzation algorithms can be classified according to different characteristics among the most common classifications is viewing them as manual unsupervised and supervised algorithms chen and pollino 2012 the first category is manual also known as expert discretization which is the most commonly used in practice and depends on an expert opinion to manually select the thresholds for the discretization bins such selection is mostly based on how meaningful the resulting bins are chen and pollino 2012 aguilera et al 2011 this group of algorithms is known to provide easily interpretable intervals that incorporates expert opinion in the modelling process however it is sensitive to the expert s degree of knowledge about the problem domain and therefore is not guaranteed to provide consistent results uusitalo 2007 beuzen et al 2018 moreover this class of algorithms is not the most efficient in complex models with a large number of variables or in multidisciplinary models that integrate data from multiple domains chen et al 2017 unsupervised discretization refers to the algorithms that do not utilise class information for discretization but rather rely on the user directly defining the number of discretization intervals such as the equal width ew and equal frequency ef algorithms ew and ef are probably the most frequently used discretization algorithms in the literature aguilera et al 2011 due to their simplicity being computationally inexpensive and requiring no expert knowledge about the system being modelled beuzen et al 2018 these methods however provide reasonable results only when the original distribution in a uniform continuous distribution otherwise their output will suffer from information loss imbalanced probabilities or occurrences being assigned into different bins nojavan et al 2017 the final discretization algorithms family i e supervised algorithms employs the state and statistical characteristics of variables in the discretization process thus they usually outperform their unsupervised counterparts dougherty et al 1995 although supervised algorithms are the most computationally expensive among the three categories they have the ability to minimize information loss during discretization as well as increasing the predictive ability of the bn additionally they don t require the involvement of an expert therefore they are suitable for usage in complex multidisciplinary networks beuzen et al 2018 mizianty et al 2010 one of the effective supervised discretization algorithms that can approximate any probability distribution while maintaining the performance of the model is gaussians mixture model gmm khanmohammadi and chou 2016 mixture models describe the density of a continuous random variable as a weighted sum of finite known probability density functions pdf the gaussian distribution in particular is adopted in several applications because of its simplicity ease of modification and for providing sufficient trad off between tractability and expressiveness reynolds et al 2000 mclachlan and peel 2004 mabrouk et al 2015 the gmm describing a continuous random variable x is formulated as shown in equation 6 where x is the random variable to be discretized θ is the gmm parameters set and n is the number of resulting gaussians or discretization bins for each of the resulting gaussians i w i μ i and κ i are its weight mean and covariance matrix respectively khanmohammadi and chou 2016 6 p x θ σ i 1 n w i p x μ i κ i the number of gaussians n is a discretization parameter that needs to be decided before running the algorithm while selecting a small n is compelling because it results in a small number of states for the considered variable and consequently a simpler bn such small n might not be able to capture the density of the data on the other hand selecting a large n makes the model prone to overfitting in order to find the most proper number of gaussians the akaike information criterion aic and the bayesian information criterion bic were used to determine the optimum n by choosing the one associated with the least value of both aic and bic balance the model s fitness and complexity by imposing a penalty on every extra parameter added to the model burnham and anderson 2004 the threshold of each interval is chosen based on the parameters mean and standard deviation of the gaussian defining this category for each gaussian component g i the threshold of data category corresponding to this component will be μ i 3σ i μ i 3σ i and such interval will include 99 7 of the values in this component khanmohammadi and chou 2016 4 1 5 network complexity analysis network complexity is one the main factors that affect the accuracy and efficiency of bns the inference of large networks that are used to represent complex system can be np hard at worst cases cooper 1990 one of the intuitive representations of a bn complexity is the maximum size of its cpts marcot 2012 mccann et al 2006 the size of the cpt of a node d with a number of states d s can be represented by equation 7 where p i s is the number of states of the ith parent of the node d for instance a node representing a discrete variable which can take two states and that have two discrete parents each has three states will have a cpt with the size of 2 3 3 18 now if this node had an additional parent with three states as well its cpt will increase to be 54 the cpt size of a node grows exponentially with the number of its parents and can become computationally prohibitive 7 c p t d d s i 1 n p i s measuring the complexity of the proposed network in terms of the maximum cpt size it turns out to be highly complex with a maximum cpt size of 2 23 for the variable human capital different techniques can be used to handle this complexity problem such as removing correlated variables exclude the nodes that have the least effect on the model output and node divorcing chen and pollino 2012 mccann et al 2006 node divorcing reduces a bn s complexity by reducing the number of parents of a complex node through dividing its relatively large number of parents into groups that feed in intermediate latent nodes which in turn become the new parents of the complex node this can be applied in our case by grouping sdgs indicators into groups of sdgs and then link the sdgs to the sustainability and resilience components for example indicators 4 1 2 and 4 4 1 will be the parents of the new node sdg 4 which in turn will be a new parent for the node human capital by applying this divorcing technique we were able to reduce the network complexity to be 2 16 the complexity check and divorcing concludes steps s9 and s10 in fig 1 4 1 6 latent variables parameterization and discretization latent variables are variables for which data is missing systemically or are variables that have never been observed holmes 2008 while there are learning algorithms that can handle bn models that contain latent variables this type of variables becomes problematic when they form an intermediate layer in the bn because they will cut the influence path between input nodes and output nodes i e the sensitivity of the output nodes to the input nodes will be dampened by this layer of latent variables marcot et al 2006 therefore it is recommended to attempt to quantify latent variables using the available data holmes 2008 marcot 2017 in the proposed bn this predicament occurs in two locations the first occurrence appeared as a by product of the nodes divorcing step where 13 sdg nodes formed a layer of latent variables between the observed sdg indicators and the lower level components of the sustainability and resilience oonfs the second occurrence takes place in the layer of the nodes representing the seven sendai targets a to g as no countries started to collect data about the progress in these targets thus they are all latent variables fig 4 shows the proposed oobn and the two latent layers indicated ll1 and ll2 respectively in order to avoid sensitivity dampening between input nodes and output nodes we apply factor analysis fa based parameterization for the variables in the latent layers to acquire approximated values for these variables that can be used in the oobn parameters learning fa is a multivariate statistical method used to condense a set of observed variables into a smaller set of unobserved latent variables known as factors yong and pearce 2013 the idea behind our application of fa to quantify latent variables i e fa based parameterization is to represent each latent variable as an index that gives a picture about the overall performance of its parent variables this simply achieved by converting the parent variables data into numbers that can be aggregated to form the latent variable index a similar idea was applied by tripathi and singal 2019 to calculate a water quality index the latent variable based on a selected set of water quality parameters the main difference between our application and theirs is that they applied a dimensionality reduction method prior to the fa step in order to select the most influential factors to include in their model while we keep all the parameters parents of the latent variables and use number of factors number of parameters 1 to assign weights to all parameters based on their importance for the latent variable under consideration in the following an example of the parameterization of one latent variable which is sendai target a or sendai ta for short will be shown to demonstrate the followed steps the same steps were applied for all the latent variables in ll1 and ll2 the first step in applying fa is to define the observed variables that will be used to quantify sendai ta we can see that sendai ta s direct parents are sdg 1 sdg 3 and sdg 15 as shown in fig 4 these parents are themselves latent variables that were parameterized in order to minimize error propagation between latent variables we select the original sdg indicators that were connected to sendai ta before network divorcing which were 1 5 1 3 6 1 3 9 1 3 9 2 3 d 1 and 15 3 1 for these six parameters we choose to keep five factors after selecting the number of factors the standard practice is to rotate the factorial axes using a rotation algorithm so the factor loadings don t change for this we used the varimax rotation tripathi and singal 2019 this step is performed to enhance the interpretability of the results as it produces a clear pattern of factor loading nardo et al 2006 after estimating and rotating the factor loadings they are then squared and scaled to unity to give the weights of individual parameters weights in this step can be interpreted as the proportion of total unit variance of the parameter that is explained by the factor nicoletti et al 1999 table 8 shows the squared and scaled factor loadings for the parameters of sendai ta the following step is to associate a wieght to each of the five factors equal to the ratio between the variance explained by that factor to the total explained variance of the data set table 9 shows the explained variance by each factor and the weight assigned to each factor finally the latent variable sendai ta is calculated by aggregating its six parents using the weights resulting from fa according to equation 8 this equation combines both weighted arithmetic and weighted harmonic means where f j is the weight of each of the m factors m 5 in our case p i is the weight of each of the n parameters n 6 in the example and v i is the parameter observed value tripathi and singal 2019 8 s e n d a i t a j 1 m i 1 n p i i 1 n p i v i f j j 1 m f j applying equation 8 enabled us to achieve step s11 in fig 1 which is followed by discretizing the new continuous values of the latent variable i e s12 in fig 1 discretization is performed using the same methodology explained in section 4 1 4 4 1 7 parameters learning and validation at this stage step s13 in fig 1 is applied and the conditional probabilities between the oobn variables is acquired for this task parameter learning is performed using the well known expectation maximization em algorithm which essentially estimates the maximum likelihood of the parameters for the underlying distribution of a given data set lauritzen 1995 em applies two steps interchangeably the expectation step e and the maximization step m the algorithm first estimates a parameters vector θ by applying calculating the maximum likelihood as if no parameters is missing m step then the expected value is calculated based on the estimate θ obtained from the m step the two steps is repeatedly applied till convergence i e there is no changes in the estimates and in the variance covariance matrix bilmes et al 1998 em is used for a broad range of applications because its ease of construction having a reliable convergence and its ability to work in the cases when the available data is incomplete nardo et al 2006 after completing the parameters learning step the final step in the data driven oobn development process is model validation s14 in fig 1 which evaluates the performance of the parameters learning process given the available data and the model structure for this purpose we apply the k fold cross validation approach the k fold cross validation approach is a version of the cross validation methods that breaks the data set into k equal size sub sets then trains the model on k 1 sub sets and tests it using the last k afterwards marcot 2012 this process is repeated for k times with different random k selected for testing each run boyce et al 2002 to validate the proposed oobn an extreme case of k fold cross validation known as leave one out in which the network is trained on n 1 records and tested on the remaining single record the leave one out version was chosen because of the relatively small size of the available model training data choosing the nodes of resilience and wpc as test nodes running the validation algorithm revealed that model was able to predict the values of the two test nodes with an accuracy of 0 88 and 0 77 respectively 5 case studies and analysis the network proposed in this work aims at modelling the resilience and sustainability of the sdgs and their constituent indicators this task was performed utilising a range of data driven techniques to determine the network s conditional probabilities which define the relationships between the variables included in it the network resulting from this process is presented in fig 4 in this section we demonstrate how the proposed network model can be utilised to answer questions related to policy analysis and evaluation at the time of disasters three case studies are chosen for the demonstration of the oobn utility and also to show the step by step process of querying and using the network the three applications also show the different ways in which the network model can be queried each of the case studies represent a different spatial scale i e national regional and global they were also chosen to represent different economic classes i e developed countries and developing countries as well as using data with different granularities the first case study examines the japanese post covid 19 integrated policy and how this policy can benefit holistic resilience and long term sustainability the second studies the positive and negative impacts of covid 19 on the sdgs and how does that reflect on resilience and sustainability the final case study focus on sub saharan africa and how the region can benefit from focusing on environmental issues when planning post covid 19 strategies 5 1 impacts of covid 19 long term responses on resilience and sustainability a case study from japan the outbreak of covid 19 had an unprecedented impacts on the social and economic systems of all nations due to the pandemic the world economy is expected to suffer economic burdens of usd 12 trillion in 2021 imf 2020 while the global job losses are estimated to reach usd 130 million worth of full time positions ilo 2021 in response to this global crisis multilateral as well as domestic spending programs have been initiated such as the un s usd 2 billion global humanitarian response plan unca 2020 as to the former on country level governments have launched different stimulus packages to help in offsetting the impacts of covid 19 for example the australian government has planned a fiscal stimulus of 10 4 of its gdp to be spent by the end of fy2021 the united states of america plans 14 of the gdp germany plans 27 of the gdp and japan plans 42 of the gdp imf 2021 in addition to these imperative spendings national efforts to alleviate the risks and impacts of covid 19 are expected to include long term actions directed toward improving health care systems building more reliable critical infrastructure promoting the use of more efficient technologies and building the society s capacity so it can cope with future external shocks marome and shaw 2021 hörisch 2021 nonetheless these long term actions are not guaranteed to result in inclusive resilience or help the nations that apply them to achieve the 2030 agenda dewit et al 2020 the oobn presented in this paper serves as an effective tool that helps policy makers examine the impacts of their policy options on the national resilience and sustainability this section discusses the case of japan as a country for which resilience is of high priority due to the severe disasters it frequently faces achour et al 2015 here we use the developed oobn to explore how the japanese policy choices in the wake of covid 19 affect the country s resilience and sustainability through the measuring criteria included in the model for countries like japan whose risk exposure is amongst the highest in the world behlert et al 2020 the occurrence of disasters such as floods and earthquakes during a pandemic forms a case of compounding disasters which requires integrating holistic resilience with sustainability to recover from the long term effects of such case potutan and arakida 2021 dewit et al 2020 therefore the japanese government dedicated a special stimulus of usd 120 9 billion on december 2019 for resilience oriented purposes that are not limited to the measures directly related to fighting covid 19 japan cabinet secretariat 2020 that sum of usd 120 9 billion is intended to fund policies that have a more holistic view of resilience such as improving national resilience plans and reducing the economic risks posed by the covid 19 countermeasures japan cabinet secretariat 2020 in order to assess the impacts of policies that follow such a holistic or an integrated approach we use the following steps 1 first determine the sdg indicators that are related to the policy options the nodes related to these indicators represent the evidence nodes that will be used to feed the network with the policy evidence ideally establishing links between sdg indicators and policy options is performed in a participatory setting that includes policy makers experts and stakeholders allen et al 2017a b maes et al 2019 but this is beyond the scope of this paper herein we make use of the connections already established and mentioned in literature and official documents such as voluntary national reviews vnrs in the case there is no data available for indicators evidence for sdgs can be directly used in this particular case of japanese policy the related indicators are shown in table 10 2 the model is run using evidence representing the business as usual bau scenario i e if the trend of progress of the indicators continue with the same rate this step gives us the no policy probabilities for the different states of both resilience and sustainability in this step we use evidence for all the indicators included in the network in order to eliminate the risk of latent variables 3 model users can then update it with the post policy evidence i e the states of the nodes related to the policy indicators are updated this gives us the post policy probabilities of resilience and sustainability 4 comparing the no policy and post policy probabilities of resilience and sustainability shows the effect of the policy actions on both of the concepts fig 5 a shows the probabilities of the states of the variables of interest using the no policy evidence while fig 5 b shows the impacts of implementing the policy actions in table 10 following such an integrated policy is expected to boost achieving the a more desirable state of the japanese wealth per capita by 2 this policy is also expected to increase the chance of achieving a higher resilience state by 5 and increase the chance of achieving the most desirable resilience state by 1 in order to incorporate the impact of data vacuum i e the inconsistency of the existing data and its incompleteness dang and serajuddin 2020 del río castro et al 2020 the states of both resilience and sustainability are represented as probability distributions themselves figs 6 and 7 show the probability distribution representing each of the states where each probability distribution indicates the expected value for that state and the range of values covered by this state along with the probability of the covered range in fig 6 for example the resilience state c1 is associated with a resilience index between the values of 63 and 78 with a most expected value of 70 this approach of representing the states as probability distributions reduces the obscurity of the states by revealing the range of values binned in each state as well as reporting the statistical features of those values e g the most expected value and the standard deviation of the values this example manifests how useful can the model presented in this paper be in comparing policies and assessing their impact on the societies long term resilience and sustainability particularly at times of compound risks where integrated policies need to be applied the japanese approach can be a lesson for all developed countries that integrating the 2030 agenda indicators in their efforts to face crises can have a long term positive effects on the sustainability and resilience of these countries it also shows that diversifying the resources allocated to post disaster restoration between short term and long term actions can help in increasing their ability to absorb future shocks and hazards 5 2 how the interplay of the sdgs restricted by covid 19 and the sdgs reinforced by covid 19 affects resilience and sustainability of nations the outbreak of the covid 19 virus has adverse impacts on the social economic and environmental systems allover the world in addition to testing the resilience and resourcefulness of all the affected nations the pandemic is expected to hinder the nations efforts to achieve the sdg by 2030 ekwebelem et al 2021 munasinghe 2020 lópez feldman et al 2020 fleetwood 2020 although the main focus of research efforts was to measure the negative impacts of covid 19 on the sdg and to provide policy recommendations on how to minimize such impacts a new line of research emerged recently which adopt a more comprehensive approach in studying the covid 19 impacts on sdg i e take into consideration the promoting effects of covid 19 on some of the 2030 agenda targets based on this comprehensive approach we utilise the proposed network model to study how the interplay between the restricting and reinforcing impacts of covid 19 on sdg can affect resilience and sustainability this is achieved by comparing the prior probabilities of resilience and sustainability with their probabilities after using the evidence reflecting the positive and negative impacts of covid 19 on the sdg indicators the steps used to discover the effects of sdgs interplay are 1 first is run using the evidence representing the bau scenario i e with the prior probabilities of the indicators the goals resilience and sustainability 2 evidence about the impacts of covid 19 on sdg at an indicator level is collected form literature we chose to use indicator level evidence instead of goal level evidence because regarding a goal as being restricted or reinforced by covid 19 entails that is constituent indicators are all either restricted or reinforced which is not always the case as analysis shows 3 the model is run using the evidence representing the interactions between the restricted and the reinforced indicators these interactions are presented to the model by assigning low desirability states to the restricted indicators and high desirability states to the reinforced indicators 4 the probabilities of resilience and sustainability associated each each scenario are then compared in general indicators and goals that are threatened by the pandemic are the ones that are directly related to health and economic drivers for example sdg 1 and its subsequent indicators which aims at ending poverty and reducing the number of deaths caused by disasters will be greatly affected by covid 19 in a negative way vigo et al 2020 united nations 2021 similarly the goals and indicators related to equality well being promotion education sustaining economic growth ensuring sustainable consumption and production reducing deaths and economic losses due to disasters i e the issues related to goals sdg 3 sdg 4 sdg 5 sdg 8 sdg 10 and sdg 13 will be negatively affected nicola et al 2020 ekwebelem et al 2021 rahayu and rahwadwiati 2021 cénat 2020 kinney et al 2020 lee 2021 on the other hand the promoting impacts of covid 19 on the sdg are mainly originating from the relaxed stress on natural systems caused by lock downs and travel restrictions those measures induced a reduction in co 2 and ghg emissions as well as other air pollutants chowdhury et al 2021 chen et al 2020 le quéré et al 2020 covid 19 countermeasures have also helped in revitalising different natural systems such as water bodies and caused a reduction in energy consumption which lead to a drop in the rate of fossil fuels usage goffman 2020 saadat et al 2020 arora et al 2020 escap 2020 these effects are related to sdg 6 sdg 7 sdg 14 and sdg 17 the details of the restricted and reinforced indicators under each sdg are summarized in fig 8 based on the evidence from fig 8 the probabilities of the states of resilience and sustainability change as presented in fig 9 the interplay of the positive and negative effects of covid 19 caused an increase in both the probabilities of the least desirable and most desirable resilience states i e resilience state 0 and resilience state 1 respectively this increase in probability of achieving the least desirable resilience state can be attributed to the direct stress covid 19 imposes on the nations resilience capacities and the losses in lives and the economic losses caused by the pandemic the increase in the most desirable state however is mainly caused by the adaptive measures taken to overcome the disaster and the preventive measures taken to prepare for similar future disasters nonetheless the overall trend of the three states did not change which means that for most of the countries more plans and investments need to be directed toward building national resilience in terms of sustainability here represented by wealth per capita the restoration of ecosystems and the forced decarbonization of the world economy caused wealth trajectories to move toward a more sustainable path such a sustainability result shows that governments and policy makers should take advantage of the current unique circumstances to integrate more sustainable options in their policies 5 3 can environmental stewardship help sub saharan africa offset covid 19 externalities sub saharan africa is among the regions that have been greatly affected by the covid 19 pandemic due to the fragility of their economic situation before the pandemic as well as their lack of the necessary resources to face such disaster shimeles et al 2018 gralak et al 2020 prior to the outbreak of covid 19 the sub saharan africa region was already lagging in terms of the sdg progress the spread of the deadly virus caused additional economic and social stresses for the region for example sub saharan africa countries suffered around 3 of losses in their gdp capita since the pandemic outbreak in addition the percentage of people living in extreme poverty in the region increased from 39 to 41 in just one year 2019 2020 united nations 2021 consequences like these will make attaining the 2030 agenda goals more challenging for countries in sub saharan africa in their effort to provide policy recommendations that can help sub saharan africa achieve the sdg within the designated time frame some researchers suggest that addressing the environmental issues can play a significant role in helping the region make faster progress in achieving the sdg omisore 2018 in line with this recommendation we use the proposed oobn to investigate the impacts of addressing the environment related sdg on the long term sustainability and resilience of sub saharan africa to determine the impacts of fostering environmental goals on the overall resilience and sustainability of sub saharan africa countries the following steps are used 1 we start by using the prior conditional probabilities to provide the bau resilience and sustainability probabilities 2 the goals that are most relevant to environmental management in post pandemic scenarios are identified 3 use the environmentally relevant goals as interventions and evidence for the network to represent the scenario focusing on enhancing the environmental stewardship 4 comparing the bau and the post policy probabilities of resilience and sustainability shows the effect of attending to the sdg related to ecosystems in order to determine which goals to focus on or to use as policy levers we utilise a set of goals that have been identified in literature as the most relevant to environmental management in post pandemic world these sdg are sdg 3 sdg 6 sdg 11 sdg 12 and sdg 15 chowdhury et al 2021 similar to the steps followed in section 5 1 we enter the evidence to the network without interventions bau scenario followed by the evidence representing the intervention and compare the probabilities from both scenarios the bau scenario evidence for sub saharan africa countries was retrieved from the united nations 2021 and the intervention was informed by evidence from literature chowdhury et al 2021 cheng et al 2021 fig 10 shows the results of focusing on the recommended sdgs the main important outcome of this result is that focusing on the goals that have an environmental focus can double the probability to achieve the most desirable resilience state as well as significantly increase the probability to achieve a sustainable state 6 conclusion in this paper we proposed an oobn that models the resilience and sustainability of the sdgs and their subsequent indicators the structure of the proposed network was informed by two acknowledged composite indices which represent resilience and sustainability to represent the sustainability component of the oobn the iwi was selected while the wri combined with the sfdrr was selected to represent the resilience component to determine the network parameters i e the conditional probability distributions that connect the network variables parameter learning was utilised using the em algorithm additionally factor analysis methods where utilised to approximate latent variables that have the potential to dampen the influence in the network the proposed oobn application offers an effective tool for policy makers that enables them to prioritize the sdg indicators implementation as well as test the impacts of new policies on both the resilience and sustainability of a nations the application also indicates which areas that need to be tackled in order to achieve selected sustainability or resilience levels additionally a method was proposed to quantify the uncertainty associated with using concepts from different disciplines where the definition of such concepts is vague or no data available about them the proposed oobn has demonstrated that with its current formation changes in certain sdgs particularly sdg 8 sdg 9 and sdg 10 will have significant impact on both sustainability and resilience of the systems one of the major challenges that hinder the attempts to measure the progress in the sdgs or model topics related to them is the data vacuum problem the approach we chose for the purpose of this paper provides multiple advantages in regard to dealing with such problem first its ability to learn from limited amount of data makes it very adaptable that it can be used in different contexts and across different scales global national regional moreover it allows for combining multiple methods for model development which is useful in accounting for data unavailability also having the ability to report second degree uncertainty i e uncertainty about the probable results is very important for building confidence about the performed analysis and to quantify the value of information so decision makers can determine the amount of needed information in order to reach a certain level of confidence one of the main limitations of the applied approach however is associated with continuous data discretization the discretized data is converted from a continuous form to a categorical form and the output comes in such a categorical form as there is no means to convert the categorical output back into its original continuous form the analysis has to be done using the categorical data which can impact the quality of the analysis and reduce its interpretability the proposed model can be extended and enhanced in a number of ways first policy options can be included in the model and linked to the sdg indicators for instance sdg 6 and its subsequent indicators can be linked to a variable that represent the average distance to water source to model how that affect water consumption and personal hygiene and consequently how that affect resilience and sustainability assigning costs to policy options will also be beneficial for policy makers as it will allows them to value the utility of each option so they can optimize their decisions moreover the proposed model does not account for the interactions among the sdgs so considering these interactions will enable the policy makers to take them into account and avoid the problems arising from conflicts between goals finally the oobn formalisation can benefit from utilising object oriented programming concepts such as polymorphism so multiple algorithms can be available for the network and the network can choose to implement the one that is most suitable for the data used or the particular application declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the core of our implementation is based on the smile reasoning engine for graphical probabilistic models available free of charge for academic research and teaching use from bayesfusion llc https www bayesfusion com appendix 1 table a1 mapping between sdg indicators and components of the sustainability index table a1 sdg indicator human capital natural capital produced capital co 2 damages tfp 1 5 1 x 1 5 2 x 1 5 2 x 1 a 2 x x 2 1 1 x 2 2 3 x 2 3 1 x x 2 4 1 x 2 5 2 x 2 a 1 x x 3 1 1 x 3 2 1 x 3 6 1 x x 3 8 1 x x 3 9 1 x x x 3 9 2 x x 4 1 2 x 4 4 1 x 6 2 1 x x 6 3 1 x x 6 3 2 x 6 4 2 x 7 1 1 x x 7 2 1 x x 7 3 1 x x x 8 2 1 x 8 5 1 x 8 5 2 x 9 1 1 x 9 4 1 x x 10 4 1 x x 11 1 1 x x 11 5 1 x 11 5 2 x 11 6 2 x x 13 1 1 x 13 2 2 x x x 14 2 1 x 14 4 1 x 15 1 1 x 15 2 1 x 15 3 1 x table a2 sdg indicators data summary table a2 sdg indicator description source countries with available data 11 1 1 proportion of urban population living in slums informal settlements or inadequate housing un habitat 105 15 3 1 proportion of land that is degraded over total land area fao 124 3 9 1 mortality rate attributed to house hold and ambient air pollution the world bank data 171 3 9 2 mortality rate attributed to unsafe water unsafe sanitation and lack of hygiene exposure to unsafe water sanitation and hygiene for all wash services world health organisation the world bank 167 7 3 1 energy intensity measured in terms of primary energy and gdp un statistics division 171 3 8 1 coverage of essential health services world health organisation 171 2 1 1 prevalence of undernourishment fao 171 1 5 1 number of deaths missing persons and persons affected by disaster per 100 000 people un office for disaster risk reduction 92 3 6 1 death rate due to road traffic injuries world health organisation 171 2 3 1 volume of production per labour unit by classes of farming pastoral forestry enterprise size fao 171 1 5 2 direct disaster economic loss in relation to global gross domestic product gdp sendai framework monitor un office for disaster risk reduction 69 9 4 1 co2 emission per unit of value added international energy agency iea un industrial development organisation unido 171 9 1 2 passenger and freight volumes by mode of transport un conference on trade and development unctad 136 3 c 1 health worker density and distribution global health observatory world health organisation 144 3 9 3 mortality rate attributed to unintentional poisoning world health organisation 156 1 3 1 proportion of population covered by social protection floors systems international labour organisation ilo world development indicators database 67 6 5 1 degree of integrated water resources management implementation 0 100 un environment programme 172 7 a 1 mobilized amount of united states dollars per year starting in 2020 accountable towards the 100 billion commitment oecd the international renewable energy agency irena 131 17 3 2 foreign direct investments fdi official development assistance and south south cooperation as a proportion of total domestic budget world development indicators 171 3 d 1 international health regulations ihr capacity and health emergency preparedness world health organisation global health observatory 159 3 8 2 number of people covered by health insurance or a public health system per 1 000 population world health organisation 31 10 5 1 financial soundness indicators international monetary fund imf 126 1 5 3 number of countries with national and local disaster risk reduction strategies un office for disaster risk reduction 93 3 a 1 age standardized prevalence of current tobacco use among persons aged 15 years and older world health organisation 171 8 9 1 frequency rates of fatal and non fatal occupational injuries oecd statistical office 57 9 a 1 total official international support official development assistance plus other official flows to infrastructure oecd 171 2 1 2 prevalence of moderate or severe food insecurity in the population based on the food insecurity experience scale fies fao 44 12 1 1 number of countries with sustainable consumption and production scp national action plans un environment programme 74 2 3 2 average income of small scale food producers by sex and indigenous status fao 171 6 6 1 change in the extent of water related ecosystems over time global surface water explorer un environment programme 171 10 4 1 labour share of gdp comprising wages and social protection transfers ilo 171 6 3 2 proportion of bodies of water with good ambient water quality un environment programme 33 2 a 1 the agriculture orientation index for government expenditures fao imf un stats 151 3 1 1 maternal mortality ratio world health organisation unicef 171 8 5 2 unemployment rate ilo 133 2 5 2 proportion of local breeds classified as being at risk not at risk or at unknown level of risk of extinction fao 77 7 1 1 proportion of population with access to electricity world bank 171 7 2 1 renewable energy share in the total final energy consumption iea un statistics division 171 6 4 2 level of water stress freshwater withdrawal as a proportion of available freshwater resources fao 171 15 2 1 progress towards sustainable forest management un environment progaramme 205 6 2 1 proportion of population using safely managed sanitation services including a hand washing facility with soap and water who unicf 171 15 1 1 forest area as a proportion of total land area fao 163 4 4 1 proportion of youth and adults with information and communications technology un statistical division 46 8 2 1 annual growth rate of real gdp per employed person ilo 171 6 3 1 proportion of wastewater safely treated un habitat 79 8 5 1 average hourly earnings of female and male employees by occupation age and persons with disabilities un statistics division ilo 52 1 a 2 proportion of total government spending on essential services education health and social protection unesco institute of statistics 117 4 1 2 proportion of children under 5 years of age who are developmentally on track in health learning and psychosocial well being unesco institute of statistics 74 3 2 1 under five mortality rate who 171 11 6 2 annual mean levels of fine particulate matter e g pm2 5 and pm10 in cities who 137 
25628,this research utilizes an object oriented bayesian network oobn to model the relationships between the sustainable development goal sdgs and resilience and sustainability at national regional and global levels the ability of the oobn to learn the parameters i e the conditional probability distributions between the variables included in the network was exploited to explore the impacts of progress of sdgs on the sustainability and resilience of nations the resulting oobn is used to examine different situations pertinent to policy analysis and design at the times of disasters particularly in the wake of the covid 19 pandemic three case studies are used to illustrate the step by step process of using the proposed oobn as well as the expected results of its application in policy analysis and evaluation contexts the proposed is able to provide insight regarding which sdgs will have more significant impacts on both resilience and sustainability as well as their constituent components the results of this research indicate how data induced oobns can be utilised by policy makers to prioritize new policies and evaluate the impacts of existing policies on both the resilience and sustainability of societies keywords object oriented bayesian networks sustainable development goals resilience inclusive wealth data driven bayesian networks covid 19 policies 1 introduction sustainable development goals sdg provide a set of universal development goals introduced by the united nations in 2015 to be achieved by 2030 the collection of 17 goals is at the heart of the united nation s 2030 agenda which plots a more prosperous future for humans and the planet willis 2016 stevens and kanie 2016 unlike its preceding set of development goals the millennium development goals the sdg propose an inclusive approach that is more global in nature i e suitable for all nations whether developed or developing the sdg comprise 169 targets and 230 indicators 1 1 for a full list of sustainable development goals targets and indicators see https sustainabledevelopment un org sdgs aspiring to attain a more sustainable future for all nations by eradicating poverty and inequalities improving education and health conditions and protecting global natural assets ferranti 2019 a major advantage of using sdg as a sustainable development agenda for policy making purposes is their realisation of the interlinkages between human well being economic prosperity and healthy natural systems in addition since they are the output of an evolving and collaborative process they are applicable to all nations as they take into consideration different national realities capacities and levels of development schmidt traub et al 2017 saito et al 2017 nonetheless integrating the sdg into national development frameworks imposes multiple challenges on policymakers mainly due to their indivisibility i e their implementation should be based on a holistic approach that considers the interlinked multi dimensional nature of the goals and the feedback effects that may occur among the targets swain and ranganathan 2021 policymakers are also required to prioritize the goals and their subsequent targets as well as determining the scale and pace of achieving them moreover governments need to overcome the challenge of allocating sufficient resources for the realisation of the goals and efficiently plan how such resources will be utilised to achieve the sdg allen et al 2016 while achieving as much progress as possible in the sdg over the span of the designated 15 years is compelling for most countries the pursuit of such achievement involves additional levels of complexity one of those complexities is related to the interlinkages between the different sdgs and their constituent targets sdg are typically evaluated and monitored individually which implies that the interlinkages between the 17 goals and the three dimensions of sustainable development economic social and environmental can be foregone or remain unaccounted for swain and ranganathan 2021 saito et al 2017 allen metternicht and wiedmann 2019a spaiser et al 2017 another important complexity is pertinent to the longevity of the progress accomplished in the sdg for example if a country is able to achieve all the sdgs by 2030 then it will be faced with an important question is such achievement guaranteed to be sustained for extended periods of time or will the country suffer a setback as a result of exhausting its resources to achieve the sdg within the span of 15 years this issue of how to maintain the progress in sdg which is usually referred to as the sdg sustainability accentuates the need to link the sdg with a framework that enables policymakers to evaluate how sustainable the achievements of sdg are palavalli et al 2018 another argument related to using sdg as a development guiding framework is their coherence with the concepts of resilience and reduction of disaster the importance of aligning sustainable development with resilience and disaster risk management has been argued long before devising the sdg by numerous researchers o brien et al 2006 mercer 2010 derissen et al 2011 redman 2014 murray et al 2017 such a notion became of particular importance recently due to the increase in the frequency and severity of disasters especially natural disasters induced by climate change and the ensuing human and economic losses barros and field 2014 munich 2017 the probabilities and consequences of current changes in planetary systems emphasise that resilience needs to be actively managed in order to maintain development on a sustainable track folke et al 2020 for example the rapid spread of pandemics like covid 19 and the subsequent stresses imposed on the nation s produced and human capitals has resulted in many countries declaring that they will need financial support in order to be able to achieve the sdgs in the designated time frame allen et al 2021 folke et al 2021 although this relevance of resilience to sustainability motivated the inclusion of targets that focus on reducing disaster risks in the sdg framework those targets are not as bold or well defined as the ones related to development ono and nagaishi 2015 in fact one of the shortcomings of the sdg is that they lack a comprehensive incorporation of the key resilience capacities and dimensions as the framework focuses mainly on adaptive responses toward disasters for instance target 1 5 which is considered the core resilience target uses the number of people affected by disasters as a performance indicator such indicator measures the effect of a disaster but does not reflect reducing the vulnerability nor increasing the preparedness toward hazards assarkhaniki et al 2020 bahadur et al 2015a b similarly target 13 1 aspires to strengthen the resilience of countries through increasing their adaptive capacity to climate related hazards that partial view of resilience is not in line with recent research that considers resilience as a product of interconnected capacities assarkhaniki et al 2020 bahadur et al 2015a b this has led to different researchers stressing that in order to ensure resilient development is achieved resilience needs to be tracked through its main composing capacities and linked to the goals and targets of the sdgs framework bahadur et al 2015a b those shortcomings of sdgs in terms of representing resilience can be addressed by integrating them with other complementary frameworks to form a coherent and consistent picture to inform policy making peters et al 2016 etinay et al 2018 issues related to sustainability and resilience are complex comprise interlinked systems and can only be resolved using holistic representation and analysis approaches that yield deeper realisation of such dilemmas chan et al 2004 additionally the interdisciplinary nature of those complex issues requires collaboration between scientists from different fields as well as effective communication between the scientific and the policy making communities heemskerk et al 2003 challenges such as these in addition to other complexities like the deep uncertainty entrenched in understanding expressing and managing the two concepts necessitate employing methods that can combine bottom up and top down views of the included sub systems explore multiple plausible futures support social learning and handle sparse imprecise data little et al 2019 allen metternicht and wiedmann 2017a modelling has played and still plays a central role in managing topics related to socio environmental systems as well as topics related to resilience schlüter et al 2019 models have been used for a multitude of purposes such as policy assessment the optimisation of management strategies analysing system component interactions and facilitating participatory processes liu et al 2008 schlueter et al 2012 kelly et al 2013 voinov et al 2016 aly and managi 2018 egli et al 2019 models can also function as vehicles that bring new perceptions of sustainability and resilience issues to attention at national and international levels in addition to being an effective tool for creating political consensus in the process of international negotiations van daalen et al 2002 the ever growing literature on sustainability and resilience modelling applications and guidelines demonstrates how models can be an invaluable for supporting the achievement of sdg in an equitable and truly sustainable manner modelling the sustainability and the resilience of the development guided by the sdgs however is a challenging task that needs to be carried out carefully for multiple reasons one of those reasons is the need to select frameworks to represent each of the concepts i e the sustainability and the resilience so that they are coherent with the sdgs additionally linking each of the frameworks with the sdgs needs to be done in a way that is intuitive measurable and revealing to the impacts of sdgs progress on the social economic and environmental aspects of societies achour et al 2015 there is also the need to assimilate knowledge and perspectives that are fragmented across different disciplines as well as incorporating different underlying assumptions which can be conflicting schlueter et al 2012 to be informative for policy making and decision support purposes any model built around resilience and sustainability of sdgs needs to have a set of certain characteristics first it should be able to identify long term problems and strategic issues geertman and stillwell 2003 van delden et al 2007 as well as being able to represent complex systems that contain large number of variables and high level of uncertainty van delden et al 2007 mcintosh et al 2007 the model also needs to be able to handle different types of input data e g qualitative and quantitative data while its output can be easily reviewed and analysed in addition to be easy to interpret volk et al 2008 ahalt et al 2014 the scalability of the model is another important feature along with having a component based structure so it can be easily modified extended and updated elsawah et al 2020 little et al 2019 in addition the model should be capable of translating scientific knowledge into policy relevant information with an intuitive communicable format volk et al 2008 1 1 this paper this paper aims to use modelling to drive thinking about sustainable development through its three intertwined composing systems while systemically consider resilience through its main driving capacities in order to do so we attempt to unravel the interlinkages between resilience and sustainability of sdgs at a national scale this study proposes a data induced object oriented bayesian network oobn that integrates two acknowledged sustainability and resilience evaluation frameworks with the united nation s sdgs to capture how progress toward sdgs jointly affects the nations capital assets and resilience capacities linking sdgs with sustainability and resilience frameworks provides a better translation between the high level thematic concepts and policy actions in addition to allowing policy makers to prioritize strategies based on the trade offs between resilience and sustainability oobns was selected as a modelling technique because of its conformity with a wide range of the desirable characteristics that need to be present in the models addressing such complex issues as will be discussed in the coming sections this application utilizes data about sdg indicators and resilience and sustainability indicators to infer the parameters defining the relationships between the variables included in the oobn to the best of our knowledge no previous research has made to use oobns to mutually model sdgs resilience and sustainability at a national level in contrast to the wide body of literature that attempts to model sdgs individually this paper focuses on modeling the sdgs collectively while this approach can be complex from a policy making perspective such complexity is important and beneficial for the policy making process for a number of reasons first tackling the sdgs individually can lead to devising incoherent policies with development plans in some sectors which negatively affect development plans of other sectors mainali et al 2018 second one of the main principles that distinguishes the 2030 agenda is that it is meant to be indivisible which means that it should be viewed as a unified whole and implemented in an integrated manner bennich et al 2020 this indivisibility of the sdgs implies that they need to be effectively prioritized and that policy makers need to be aware of the complex feedback effects that link the goals allen metternicht and wiedmann 2019a additionally having a holistic consideration for the sdgs will help policy makers in building effective alliances and communication channels with key stakeholders who play a vital role in advancing the achievement of the goals nilsson et al 2018 for such reasons ensuring that the sdgs are jointly considered in the policy making process was identified as one of the priorities for research efforts that aim at supporting the implementation of the 2030 agenda allen et al 2021 the rest of this paper is organised as follows in section 2 research background we discuss the definitions of resilience and sustainability and limitations of the sdgs from the point of view of each of the concepts section 2 also provides details about the resilience and sustainability evaluation frameworks selected for inclusion in the model and how the connections between the two concepts and sdgs are established section 3 object oriented bayesian networks describes the modelling technique used to represent the concepts of resilience and sustainability then the methodology used in developing the oobn and the model formulation details are provided in the methodology section finally the model results are discussed in section five followed by the conclusion 2 research background this section presents an overview of the research concerned with the sdgs sustainability and linking resilience to the sdg framework it first broadly presents the attempts to inspect the relationship between long term sustainability and the sdg as well as the attempts to conceptualize resilience from sdg perspective after that we discuss the frameworks chosen to assess both sustainability and resilience within the scope of this paper 2 1 previous work on sdgs sustainability and resilience since their inception different aspects of the sdgs have been studied by the scientific community in order to foster and support their implementation and reinforce the knowledge base related to them leal filho et al 2018 recent reviews of the literature discussing sdgs show that most of this literature has directed its focus toward studying the interlinkages and trade offs among the sdgs the policy issues that can hinder the implementation of the sdgs monitoring and evaluation of the sdgs implementation and the interactions between the sdgs at finer levels such as target and indicator levels bennich et al 2020 breuer et al 2019 allen et al 2019a b c however the themes of sdgs sustainability and resilience have not gained as much traction as the other topics related to sdgs in particular modelling falls short in addressing all 17 sdgs and their subsequent targets as well as considering long term sustainability beyond the 2030 time frame additionally few models provide a holistic interpretation of the sdgs that enables the assessment of the economic social and environmental sustainability pillars along with the resilience of these pillars zimm et al 2018 researchers interested in inspecting the sustainability of the sdgs mainly focus on studying the structure of the sdgs and how they relate to the three components of sustainable development they observed that the three forms of capitals that characterises traditional sustainability models do not have a balanced representation in the sdgs as a matter of fact social capital is dominant over the environmental and economic capitals costanza et al 2016 kettunen et al 2018 fioramonti et al 2019 this imbalanced distribution of capitals can lead to a false impression of sustainability even when the sdgs are achieved giannetti et al 2020 this observation was further studied using different analytical methods jain and jain 2020 for instance performed correlation regression and path analysis to explore the relationships between the sdgs and three sustainability components they represented the social and economic capitals in terms of the human development index hdi and the environmental capital was expressed using the ecological footprint and the earth s biocapacity their analysis showed that the progress in sdgs had adverse impacts on the environmental capital and a positive impact on the other types of capital this finding is in accord with the structure of the sdgs which leans more toward social aspects of development another approach followed by wackernagel et al 2017 classified the goals into three categories based on resources dependence the first category contained the goals that lead to a decrease in the nations resources dependence i e the activities that result in saving resources hence increase their availability the second class incorporated the goals that requires consuming more resources in order to be achieved such improving the health care and education infrastructure the final category represented the goals that have no effect on the resources based on this categorisation they found that the resources dependent goals make up 67 6 of the goals while the resources sustaining goals represent only 13 6 they concluded that the sdgs in its current format compromises resources security which is a major aspiration of sustainable development this line of research stresses that the progress in sdgs should not be pursued in isolation of a continued monitoring of the three main capital assets if sustainability to be achieved therefore another sustainability model that put explicit emphasis on jointly preserving the environmental social and economic capitals needs to be adopted alongside the sdgs biggeri et al 2019 nonetheless no attempts have been made so far to establish such a link between the sdgs and the desired sustainable development framework on the other hand resilience is intended to be rooted in the sdgs because like most of the post 2015 development frameworks the framework adopts the position that resilience is a component of sustainability marchese et al 2018 allen et al 2019a b c assarkhaniki et al 2020 despite such conceptualisation the sdgs have been criticised for not providing a comprehensive representation of resilience in terms of its main capacities nor do implementing the goals guarantee reaching a resilient development ono and nagaishi 2015 assarkhaniki et al 2020 the attempts to overcome this drawback in the sdgs included some researchers proposing new indicators for the targets that explicitly state resilience as an aim such as target 1 5 the main reason for choosing this particular target is that it is the one concerned with building the resilience of those in vulnerable situations and reduce their exposure and vulnerability to climate related extreme events and other economic social and environmental shocks by 2030 so it is considered the core resilience target another motivation to seek additional indicators for this target is that it is considered extremely broad in regard to the shocks and stresses it addresses the proposed new indicator measures resilience through its three main capacities taking into consideration the level of exposure to hazards bahadur et al 2015a b that new indicator however was only proposed for a single target i e target 1 5 and just presented the theoretical framework without calculating it using real life data the main idea behind such modified indicators is that three major considerations need to be present when measuring resilience 1 resilience is delivered through a set of resilience capacities as this ensures the nations ability to handle shocks and disturbances and continue development despite of them 2 exposure to hazards and risks should be part of measuring resilience because the intensity of such risks can offset the progress in building the resilience capacities and 3 resilience development cannot be achieved without seamless integration of the frameworks developed throughout 2015 and 2016 bahadur et al 2015a b bahadur et al 2013 peters et al 2016 etinay et al 2018 in order to effectively address the aforementioned issues related to the sustainability and the resilience of the sdgs and to incorporate the two concepts in a model we need to settle on a suitable conceptualisation and measuring framework of each that can be coherently integrated with the sdgs a suitable which in this context means aligned with the model s purpose and the technique used to develop it conceptualisation and measuring framework needs to 1 have a holistic view that considers the society s environmental social and economic systems to ensure that funding the ambitious sdg development agenda does not impose undesirable long term externalities on the three sustainability pillars as well as ensuring a balanced development across these pillars dalby et al 2019 cutter et al 2015 coopman et al 2016 fukuda parr and mcneill 2019 2 be applicable on a national scale to be consistent with the need to localize the sdgs i e translating them from their global scale into national scales salvia et al 2019 smith et al 2018 galli et al 2018 allen et al 2016 3 have a component based structure which is important for breaking disciplinary silos and to facilitate applying the integrative modelling approaches necessary to achieve the sdg hamilton et al 2015 van soest et al 2019 moyer and hedden 2020 collste et al 2017 liu et al 2015 coopman et al 2016 4 have available data qualitative or quantitative as this is a critical element for performing any analysis or assessment related to the sdgs ordaz 2019 kraak et al 2018 georgeson and maslin 2018 allen et al 2018 5 have measurement in the form of a composite indicator ci as they represent an increasingly recognized approach for policy analysis and national performance benchmarking singh et al 2009 saltelli 2007 a ci is a hierarchical indicator composed of harmonised aggregated sub indicators nardo et al 2006 2 2 sustainability definition and assessment framework the concept of sustainability can be traced back to the 1897 report of the world commission on environment and development which defined sustainable development as the type of development that meets the needs of the present without compromising the ability of future generations to meet their needs borowy 2013 such definition indicates that intergenerational welfare being the ultimate goal of sustainable development can be achieved by maintaining a non decreasing flow of goods services and utility form the environmental social and reproducible capitals among generations dasgupta and mäler 2000 harlow et al 2013 several sustainability assessment approaches have appeared over the years such as environmental pressure life cycle assessment process energy or life cycle cost assessment these approaches however are considered non integrated as they only focus on a single dimension of sustainability i e either the economic the social or the environmental dimension schoenaker et al 2015 other indicators that have a more holistic view of sustainability are genuine adjusted net savings the well being index the green net national product and the inclusive wealth index hanley 2000 dasgupta et al 2015 the most recent of these indicators is the inclusive wealth index iwi which measures the changes in the nations capital assets thus reflects their economic growth environmental stewardship and social vigour in a sensible form fenichel et al 2016 as the name suggests the iwi measures a nation s wealth at any point in time by gauging its three main capital assets namely reproducible capital rc human capital hc and natural capital nc arrow et al 2012 unu ihdp unep 2012 the wealth per capita wpc at any time is the sum of the stocks of the three main capital assets weighted at the social value of each stock also known as the stock s shadow price and normalised by the population at that time dasgupta 2009 arrow et al 2012 wpc is also adjusted to include the effect of total factor productivity and co 2 emissions unu ihdp 2012 so it can be represented as a function of the aforementioned components as shown in equation 1 1 w p c f r c h c n c t f p c o 2 such a sustainability measurement is therefore very suitable for our model because of its component based structure hierarchical form data availability through the periodically published inclusive wealth reports its ease of interpretation and communication and its applicability on a national level additionally when integrated with the sdgs the inclusive wealth framework reveals the influence of achieving the sdgs on all the three capital assets comprising nations resource base as well as any carbon damages resulted from progressing toward sdgs i e it is an adequate measure for the sustainability of the sdgs 2 3 resilience definition and measuring framework the concept of resilience has been used widely in numerous domains bhamra et al 2011 by multiple disciplines which leads to resilience having multiple definitions and consequently many disparate measuring approaches considering the main components of socio environmental systems the definitions change between these components as shown in table 1 such lack of a common definition for resilience hampers decision makers efforts to deliver resilience supporting policies consequently the united nations office for disaster risk reduction and the world humanitarian summit are leading an ongoing effort to develop a common definition for resilience with a special focus on understanding it as an outcome rather than a process peters et al 2016 unfortunately current resilience definitions do not facilitate formulating a resilience metric that can be applied across different domains or in a system of systems context as they do not have an intuitive clear links to the determinants of resilience ayyub 2014 three main system capacities are considered as the determinants of resilience or the resilience pillars and are commonly used in literature to represent resilience vugrin et al 2011 hossain et al 2019 bahadur et al 2015a b absorptive capacity refers to the system s ability to absorb shocks and to be minimally impacted after a disruptive event adaptive capacity the system s ability to adapt and cope with consequences of a disruptive event without the aid of external recovery activities restorative capacity represents the system s recoverability from a disruptive event in order to overcome the weaknesses of the sdgs in handling resilience additional frameworks that are designed around resilience and risk reduction have been integrated with the sdgs namely the sendai framework for disaster risk reduction sfdrr and the world risk index wri the sfdrr was initiated at the third united nations world conference on disaster risk reduction in 2015 as the first item on the un s post 2015 development agenda unisdr 2015 the post 2015 agenda includes three other un flagship agreements the sustainable development goals the paris climate agreement and the habitat iii urban agenda maini et al 2017 sfdrr has the same time span as the sdgs which extends between 2015 and 2030 and represents a key component of the united nations ongoing efforts to reinforce the policies and practices that increase the governments capabilities to manage disaster risks the sendai framework was preceded by another disaster risk reduction framework the hyogo framework for action hfa which was initiated in 2005 unisdr 2005 hfa identified the main sectors that countries should focus on in order to increase their resilience against disasters and the indicators included in the framework were utilised by researchers in order to assess resilience on national and sub national scales e g kammouh et al 2018 and handayani et al 2019 unlike previous efforts the framework focuses on managing the conditions of hazards rather than just the responses to hazards by considering the interplay between the exposure to hazards the vulnerability of systems and the capacities that communities need to develop to be well prepared against disasters and shocks aitsi selmi et al 2015 wahlström 2015 wri on the other hand is an index developed by the un university s institute for environment and human security unu ehs that describes the disaster risk for various countries as well as assessing their vulnerability to suffering from hazards birkmann and wisner 2006 wri is calculated as a function of exposure to hazards and vulnerability of societies therefore incorporating it in measuring the resilience of the sdgs mean that the exposure to hazards is accounted for in the wri assessment vulnerability of societies is an indigenous factor that depends on three main factors susceptibility coping and adaption birkmann et al 2011 although wri does not contain a direct reference to resilience different scholars established a direct link between vulnerability and resilience lavell et al 2012 bergstrand et al 2015 joakim et al 2015 so that when vulnerability increases the level of resilience automatically decreases kasperson and kasperson 2013 for instance adger 2000 sees resilience as a loose antonym for vulnerability such a notion suggests that resilience and vulnerability can be represented along a continuum berkes 2007 birkmann and wisner 2006 this relation between resilience and vulnerability particularly holds when the formalisation of vulnerability includes the inverse of coping capacity and adaptive capacity yohe and tol 2002 o brien et al 2007 which is indeed the formalisation adopted by the wri this view is also shared by the intergovernmental panel on climate change ipcc barnett et al 2008 based on this view vulnerability can be converted to resilience and vice versa similarly the three resilience capacities can be calculated from the three vulnerability components integrating the sfdrr with the sdgs when modelling the resilience of the latter is logical and important for a number of reasons first the two frameworks when applied together is recognized to compose an efficient resilience agenda murray et al 2017 therefore integrating the two frameworks is recommended by different experts such as the inter agency and expert group on the sdg indicator and the open ended intergovernmental expert working group peters et al 2016 the sfdrr is designed to be coherent with the sdgs rose et al 2020 in fact one of the main purposes of the sfdrr is to support the other post 2015 frameworks peters et al 2016 this clearly appears in the 38 targets that directly link the sdgs to the sfdrr another reason is that different countries that are prone to natural hazards are committed to both frameworks as an integrated policy to build resilience while remaining on a sustainable development track an example of this is the framework for resilient development applied by some of the pacific island countries pacific community f s secretariat of the pacific regional environment programme et al 2016 moreover resilience in the sfdrr is to be built against multi hazards which reflects how such framework pursues a more holistic and inclusive strategy that protects humans livelihoods cultural heritage infrastructure economic systems and environmental systems this appears in the seven global targets set by the sfdrr to be achieved by 2030 which are shown in table 2 in order to integrate sdgs with the selected resilience and sustainability frameworks we map each of the frameworks to the relevant sdg indicators using either evidence from literature or via logical mapping for instance the mapping between the sfdrr targets and the sdg indicators is based on the linkage mentioned in the framework s guidebook unisdr 2015 however because no direct mapping exists in the literature between sdfrr targets and the resilience capacities they are mapped logically absorptive capacity for example is related to the system s ability to resist damage in the cases of extreme events hence it is mapped to all the sfdrr measures that are related to reducing losses e g number of injured people attributed to disasters and direct economic losses attributed to disasters similarly adaptive capacity is understood as the long term strategies and structural changes that lead to reduced negative impacts at the times of hazards as such adaptive capacity is mapped to the measures that increase the preparedness of the system such as having multi hazard monitoring and forecasting systems restorative capacity is the system s ability to heal after a shock and it includes the resources and measures used to regain the system s functional form measures mapped to this capacity include total government resources as a proportion of gdp and total available official international support as they directly increase community resourcefulness details of the mapping step are included in table a1 in appendix 1 finally the values for resilience and its comprising capacities are derived from the wri data 3 object oriented bayesian networks oobn bayesian networks bn and in particular oobn were selected to model the relationships between resilience sustainability and the sdgs bns are probabilistic graphical models that represent the relationships between a set of variables through conditional probabilities koski and noble 2011 nielsen and jensen 2009 a bn consists of two major components aguilera et al 2011 first a qualitative component that is a directed acyclic graph dag in which the nodes indicate the set of random variables x x 1 x n included in the model and the arcs denote the statistical dependence between the nodes they connect second a quantitative component which is the conditional probability distribution for each variable in the graph given its parents i e p x i pa x i where pa x i are the parents of x i the efficiency of bns to model complex multidisciplinary problems can be increased by breaking down such problems into their main constituent components then representing each component with a modular network benjamin fink and reilly 2017 that notion of using modular networks to represent parts of complex domains as well as the utility of reusing such modular networks in similar problems lead to the introduction of object oriented bayesian networks oobns where concepts of object oriented programming are utilised to build more flexible and efficient bns koller and pfeffer 1997 oobns allow generalisation over multiple sub networks that have similar probabilistic models in an oobn setting classes define sub networks that have similar structure while an object oriented network fragment oonf represents an instantiated class koller and pfeffer 1997 each of the oonfs represents a partially separable component of the system and the oonfs are loosely coupled to represent the whole system even though some researchers argue that oonfs instantiated from the same class will have similar structure and parameter values koller and pfeffer 1997 oonfs can be more adaptive and case specific by changing the parameters based on changes in the situation to be modelled liu et al 2016 as such parameter values for oonfs can be learned for each unique situation coupling between oonfs is achieved through interface variables which is the class representing variables that are common between the different sub systems interface variables can be represented by two types of nodes input nodes and output nodes neil et al 2000 input nodes cannot have parents in other classes while output nodes can have parents from classes other than the interface variables class bangsø et al 2003 bn is an apt approach because of its ability to handle most of the modelling requirements of such complex problems first the very probabilistic nature of bns is appropriate for representing the inherent uncertainty of the socio environmental systems underlying the modelled concepts uusitalo 2007 spanning over different disciplines and scales socio environmental systems models are mostly developed and applied in conditions of partial information and require knowledge from different sources to be integrated in the model little et al 2019 aguilera et al 2011 additionally both qualitative and quantitative data need to be included in these models elsawah et al 2020 bns have the ability to handle different types of data and produce decent model results even when data is partial or uncertain newton 2010 aguilera et al 2011 chen and pollino 2012 one more valuable feature of bns is that they have a causal graphical structure that can be relatively easily understood by stakeholders and non technical users cain et al 2003 henriksen et al 2007 such a feature increases the chances for participation of stakeholders and decision makers in the modeling process and increases the final model s efficiency and usability voinov and bousquet 2010 that graphical structure also increases the model s transparency and interpretability in comparison to other types of models stritih et al 2020 and permits for easily visualising causalities propagation consequently allowing for top down analysis and bottom up analysis through backward reasoning and forward reasoning respectively chen and pollino 2012 bns are commonly used in the context of resilience and risk analysis however most of the applications using them in that context focus on technical systems with no inclusion of environmental or social systems hossain et al 2019 kameshwar et al 2019 a few researchers however have utilised bns to model the resilience of multiple systems albeit on a sub national scale e g kammouh et al 2020 bertone et al 2018 tabandeh et al 2019 de iuliis et al 2021 b and de iuliis et al 2021 a in the context of sdgs giné garriga et al 2018 proposed an approach for a single sdg management using an oobn additionally requejo castro et al 2020 utilised bns ability to learn network structure from data to explore the relationships between sdg6 and the rest of the sdgs these few activities show that bns have very limited applications in modelling sdg or the resilience of multiple systems 4 oobn modeling methodology this section describes the methods used to formalise the oobn model representing sustainability resilience and their links to the sdgs based on the selected respective composite indicators unlike other approaches that attempted to discover the relationships between the variables comprising composite indices through bn structure learning requejo castro et al 2019 our approach capitalises on the mathematical form of the selected index to link the variables together such mathematical form is considered as a universally accepted formalism that embodies knowledge elicited from experts and crafted by modellers nardo et al 2006 bn parameter learning is applied however to infer the conditional probabilities between the variables in the network a major advantage of relying on knowledge based methods such as the mathematical form of the index to inform the network structure is minimising the uncertainty associated with learning the structure from data as well as avoiding network structure overfitting marcot 2017 rizzo and blackburn 2018 unlike the network parameters uncertainty the uncertainty associated with learning the network structure from data is usually harder to quantify and requires more complex treatment methods o hagan 2012 uusitalo et al 2015 moreover expert opinion and knowledge embedded in literature are used in several cases to validate the network structure learnt from data marcot 2017 requejo castro et al 2019 therefore we chose to directly employ the existing validated knowledge 4 1 stepwise formalisation of the oobn fig 1 outlines the methodology applied to develop the oobn before discussing the details of these steps the key assumptions of the model are presented as the proposed model depicts sustainability resilience and sdgs using indices measured at country level for 250 countries around the globe each entry in the data used for parameter learning marks a country c i with an array of features x y measured at a certain year y so that every x j y x y maps to a node in the bn i e j the number of nodes in the developed network for this application y 2014 was chosen to be the control year because most of the variables have data available for this year selecting the most recent year where the most data is available to learn the bn parameters and consequently predict the relationship between countries resilience sustainability and sdgs is based on the assumption that the system s states prior to the current state only affect the future through the current state murphy and russell 2002 this assumption which is the first key assumption in our model hinges on one of the markov chains characteristics which states that the future states of some system are dependent only on the current state of that system keselj 2009 thomas and sobanjo 2013 the second key assumption is related to preserving the hierarchical structure of the composite indices and states that nodes in the developed network can only be parents of nodes that are at the same or the following hierarchical level finally we assume that the missing data for any variable included in the bn is either missing completely at random mcar or missing at random mar mcar data means that any missing values are independent from the variable it is related to or from any other variable in the bn mar data on the other hand are independent from the variable it is related to but may be conditional on other variables in the bn nardo et al 2006 this assumption is important because most of the data imputation methods require that the data to be imputed is not systematically missing bhaskaran and smeeth 2014 aljuaid and sasi 2016 4 1 1 determine oobn structure as shown in fig 1 the first step s1 is to conclude the network structure from the frameworks selected to represent the different concepts that will be included in the network i e the sdgs the inclusive wealth framework the sfdrr and the wri in order to determine the oonfs that will compose the full oobn an oonf class is assigned to each of the composite indices ci used to represent each framework afterwards the common variables between the indices will determine the interface between the oonfs cis are multi variable indicators that have been used widely by different multi dimensional and multi disciplinary frameworks nardo et al 2006 bandura 2008 different researchers have utilised cis to inform the structure of bns benefiting from the hierarchical structure they provide e g srinivas 1994 johnson et al 2017 nannapaneni et al 2017 romanko et al 2019 requejo castro et al 2019 kammouh et al 2020 complying with the oop notations the composite index ci bayesian network is a special type of bn thus they inherit from the abstract class bayesian network as shown in fig 2 as an abstract class bayesian network can be considered as a blueprint for other classes an abstract class is a special type of classes that can not be directly instantiated nonetheless it can be used as a super class that helps in defining other classes clarke et al 2012 berardi et al 2005 ci bayesian network in our case is a concrete class that extends the abstract class bayesian network the ci bayesian network class is used to instantiate the two oonfs of resilience bn and sustainability bn with the interface vars being the variables common between the two oonfs in an oobn the interface vars represent the nodes that exist in different fragments or oonfs therefore they are used to couple the oonfs together in our case the interface vars will be the sdgs indicators that are common between the sustainability bn and the resilience bn 4 1 1 1 sustainability oonf this network fragment models the index selected to represent sustainability i e the inclusive wealth index as mentioned with more details in section 2 2 this index measures the wealth of nations as a function of their capital assets in addition to the co 2 emissions produced by their productive base and the total factor productivity thus this index can be presented as shown in equation 2 from equation 2 wpc will be a childless node at the very bottom of the sustainability network fragment while the wealth per capita components appearing on the rhs of equation 2 will be parent nodes of wpc 2 w p c f n a t u r a l c a p i t a l p r o d u c e d c a p i t a l h u m a n c a p i t a l c o 2 d a m a g e s t f p similarly based on the mapping between the sdgs and the iwi each of the five of components of the wpc has the sdg indicator parents shown in table 3 4 1 1 2 resilience oonf the resilience network fragment is constructed in the same manner as the sustainability network fragment by tracing the components of the resilience composite index as such resilience can be presented as a function of its main capacities as shown in equation 3 such presentation means that the resilience node will be a childless node at the very bottom of the resilience oonf with three parent nodes each representing one of the capacities shown in equation 3 3 r e s i l i e n c e f a b s o r p t i v e c a p a c i t y a d a p t i v e c a p a c i t y r e s t o r a t i v e c a p a c i t y as explained in details in section 2 3 in order to be able to assess resilience in national contexts we adopted the approach in literature that considers resilience as the complement of vulnerability nonetheless the uncertainty about the definition of resilience and its similarity with vulnerability of systems can be explicitly introduced to the model by using a special type of variable modelling namely the noisy max function the noisy max model is an extension to the binary noisy or model that generalises it to incorporate multi valued variables as well as reducing the effort needed for knowledge elicitation and improving the quality of the parameters learned from data diez 1993 henrion 2013 in such model a variable x with s x number of states is conditioned on the set of its parents z in addition to a leak factor θ and can be written in the form of equation 4 the leak factor θ can be interpreted as the effect of some missing factor s on the variable under consideration and is measured as the probability of achieving the variable x even at the absence its parents z in the case where a noisy max node parameters are learned from data the observed probabilities is assumed to include the leak probability which then can be calculated using equation 5 where z i is the ith parent of the variable x p i is the effect of z i in the absence of all other parents and p θ is the probability of x in the absence of all the causes that explicitly included in the model i e the leak factor oniśko et al 2001 4 θ x p x 0 z 0 5 p x z i 1 1 p θ z i z 1 p i 1 p θ modelling resilience as a noisy max variable enables us to inspect the uncertainty about the similarities between resilience and vulnerability such uncertainty is manifested in the leak factor associated with resilience after parameter learning in the particular case of using the data from the wri to represent resilience and its comprising three capacities as presented in equation 3 the leak factor associated with each state of resilience is the probability that this resilience state is affected by hidden factors other than the three chosen capacities the parents of the three resilience capacities are the seven sendai targets while the parents of the sendai targets are the sdg indicators the parents of each specific node in the three sets of nodes are shown in table 4 and table 5 in addition to the logical benefits of using the sfdrr that we mentioned in the problem background section using such framework provides another important value to the model as it creates an intermediate level of nodes between the sdgs and the resilience capacities which reduces the model s complexity the model complexity and the effect of intermediate levels of variables will be discussed in further details in section 4 1 5 finally to conclude this first step the sdg indicators that are common in the two oonfs i e the interface nodes are determined table 6 shows these 12 interface nodes out of the total 51 sdg indicators included in the full network 4 1 2 data collection and pre processing data collection and pre processing is covered by steps s2 to s6 shown in fig 1 and starts with collecting the data associated with each variable in the oobn from its respective source the variables data can be generally grouped into three main groups sustainability variables data resilience variables data and sdgs indicators data data for sustainability variables were obtained from the inclusive wealth report 2018 managi and kumar 2018 which includes data till the year 2014 for 140 countries resilience variables data were obtained from the world risk report 2014 birkmann et al 2014 which contained data for 171 countries summaries of the sustainability data and resilience data are shown in table 7 no data were available for the sendai targets a to g hence they were treated as latent variables and data for them is approximated at a later step the remaining group of variables is the sdg indicators group and for each variable in this group data were collected from the respective source as shown in table a2 in appendix 1 data for the selected year for all the sdg indicators are not always available in such case we search for available data for other years for the same indicator if data for other years are available then knn imputation is used to find a value approximation for the desired year knn imputation is a pre replace method that replaces missing values prior to data processing troyanskaya et al 2001 this method classifies data into clusters then replaces the missing values with the corresponding values from their nearest neighbour based on euclidean distance aljuaid and sasi 2016 this method was chosen for data imputation at this stage for its simplicity and ability to produce results even when small number of data points is available moreover it imputes the missing values considering other values that are mostly similar to the instance of interest which increases the accuracy of the overall learning process kumutha and palaniammal 2013 if no data are available for other years for the variable under consideration we check whether the parents of that variable have available data or are they latent variable i e no data available for them if the variable parents have data then the variable data are estimated using simple aggregation of normalised parent values nardo et al 2006 otherwise the variable is excluded from the network to minimize the influence of missing data on the final results applying this sequence on the sdg indicators selected to be included in the network because of their direct connection with sustainability and resilience led to decreasing the number of originally chosen indicators from 69 indicators to 51 indicators 4 1 3 variables bivariate correlation analysis one of the fundamental assumptions in probabilistic inference which applies to bayesian networks is that the parent nodes of some child node are uncorrelated as long as they are not linked spiegelhalter and lauritzen 1990 failing to establish links between the correlated variables in a bn leads to an incorrect or an inaccurate representation of the network probabilistic structure which causes the influence dynamics of the network to be biased marcot 2017 in order to avoid this problem experts recommend that input variables should be tested for correlation before any learning or analysis is performed and the highly correlated ones either be linked or excluded from the model marcot 2017 as fig 1 shows step s7 checking for correlation between the network variables comes after data collection and pre processing and for that task we use pearson correlation coefficient with a threshold of 0 85 to determine the highly correlated variables because there is no standard value for the threshold of high correlation researchers rely on rules of thumb to determine that threshold with some considering values between 0 7 and 1 as an indication of high correlation marcot 2017 and others consider values between 0 7 and 0 9 as high while values between 0 9 and 1 as very high mukaka 2012 the result of the correlation analysis presented in fig 3 highlights five positions where the correlation coefficient is greater than the selected threshold the first is between indicator 3 1 1 maternal mortality ratio and indicator 3 2 1 proportion of births attended by skilled health personnel we chose to exclude indicator 3 2 1 as there are more data points available for the other indicator the second pair of highly correlated indicators are 3 9 2 mortality rate attributed to unsafe water unsafe sanitation and lack of hygiene and 3 2 1 from these two we will keep 3 9 2 based on the previous exclusion of 3 2 1 the indicator 3 2 1 was also highly correlated with 3 8 1 coverage of essential health services which is kept in the network indicators 3 9 2 and 3 1 1 were also highly correlated but the former has more available data and is more related to technical systems hence it was the one kept in the network finally the indicators 9 1 2 passenger and freight volumes by mode of transport was excluded as it was highly correlated with 9 4 1 co2 emission per unit of value added however 9 4 1 has much higher available data and is connected to more variables in both the resilience and the sustainability oonfs 4 1 4 continuous variables discretization after determining the final network structure the final set of variables that will be included in the network and the data associated with each variable the next step s8 in fig 1 is to discretize the data associated with each variable to convert it from a continuous form to a discrete form variables included in the model contains a mixture of discrete and continuous variables nonetheless in a lot of cases continuous variables need to be discretized because many bayesian networks structure and parameters learning algorithms assume all variables are discrete chen et al 2017 the term discretization refers to the process of binning continuous data so that each bin represents a certain category and the data points lying within the boundaries of a bin is substituted with that bin s label kotsiantis and kanellopoulos 2006 the discretization scheme of a network s continuous variables has a considerable impact on the network performance accuracy and interpretability chen et al 2017 hammond and eric bickel 2011 in spite of the significant impact it has on the bn performance and results the used discretization method remains one of the most under reported features in bn models in literature for instance a review of bns applications in environmental modelling revealed that 48 6 of the reviewed papers did not include any information about how continuous data was discretized aguilera et al 2011 the process of variable discretization entails approximating the statistical properties or moments such as the mean variance and skewness of the original continuous variable thus a good discretization algorithm will provide the closest approximation to the variable moments hammond and eric bickel 2011 discretzation algorithms can be classified according to different characteristics among the most common classifications is viewing them as manual unsupervised and supervised algorithms chen and pollino 2012 the first category is manual also known as expert discretization which is the most commonly used in practice and depends on an expert opinion to manually select the thresholds for the discretization bins such selection is mostly based on how meaningful the resulting bins are chen and pollino 2012 aguilera et al 2011 this group of algorithms is known to provide easily interpretable intervals that incorporates expert opinion in the modelling process however it is sensitive to the expert s degree of knowledge about the problem domain and therefore is not guaranteed to provide consistent results uusitalo 2007 beuzen et al 2018 moreover this class of algorithms is not the most efficient in complex models with a large number of variables or in multidisciplinary models that integrate data from multiple domains chen et al 2017 unsupervised discretization refers to the algorithms that do not utilise class information for discretization but rather rely on the user directly defining the number of discretization intervals such as the equal width ew and equal frequency ef algorithms ew and ef are probably the most frequently used discretization algorithms in the literature aguilera et al 2011 due to their simplicity being computationally inexpensive and requiring no expert knowledge about the system being modelled beuzen et al 2018 these methods however provide reasonable results only when the original distribution in a uniform continuous distribution otherwise their output will suffer from information loss imbalanced probabilities or occurrences being assigned into different bins nojavan et al 2017 the final discretization algorithms family i e supervised algorithms employs the state and statistical characteristics of variables in the discretization process thus they usually outperform their unsupervised counterparts dougherty et al 1995 although supervised algorithms are the most computationally expensive among the three categories they have the ability to minimize information loss during discretization as well as increasing the predictive ability of the bn additionally they don t require the involvement of an expert therefore they are suitable for usage in complex multidisciplinary networks beuzen et al 2018 mizianty et al 2010 one of the effective supervised discretization algorithms that can approximate any probability distribution while maintaining the performance of the model is gaussians mixture model gmm khanmohammadi and chou 2016 mixture models describe the density of a continuous random variable as a weighted sum of finite known probability density functions pdf the gaussian distribution in particular is adopted in several applications because of its simplicity ease of modification and for providing sufficient trad off between tractability and expressiveness reynolds et al 2000 mclachlan and peel 2004 mabrouk et al 2015 the gmm describing a continuous random variable x is formulated as shown in equation 6 where x is the random variable to be discretized θ is the gmm parameters set and n is the number of resulting gaussians or discretization bins for each of the resulting gaussians i w i μ i and κ i are its weight mean and covariance matrix respectively khanmohammadi and chou 2016 6 p x θ σ i 1 n w i p x μ i κ i the number of gaussians n is a discretization parameter that needs to be decided before running the algorithm while selecting a small n is compelling because it results in a small number of states for the considered variable and consequently a simpler bn such small n might not be able to capture the density of the data on the other hand selecting a large n makes the model prone to overfitting in order to find the most proper number of gaussians the akaike information criterion aic and the bayesian information criterion bic were used to determine the optimum n by choosing the one associated with the least value of both aic and bic balance the model s fitness and complexity by imposing a penalty on every extra parameter added to the model burnham and anderson 2004 the threshold of each interval is chosen based on the parameters mean and standard deviation of the gaussian defining this category for each gaussian component g i the threshold of data category corresponding to this component will be μ i 3σ i μ i 3σ i and such interval will include 99 7 of the values in this component khanmohammadi and chou 2016 4 1 5 network complexity analysis network complexity is one the main factors that affect the accuracy and efficiency of bns the inference of large networks that are used to represent complex system can be np hard at worst cases cooper 1990 one of the intuitive representations of a bn complexity is the maximum size of its cpts marcot 2012 mccann et al 2006 the size of the cpt of a node d with a number of states d s can be represented by equation 7 where p i s is the number of states of the ith parent of the node d for instance a node representing a discrete variable which can take two states and that have two discrete parents each has three states will have a cpt with the size of 2 3 3 18 now if this node had an additional parent with three states as well its cpt will increase to be 54 the cpt size of a node grows exponentially with the number of its parents and can become computationally prohibitive 7 c p t d d s i 1 n p i s measuring the complexity of the proposed network in terms of the maximum cpt size it turns out to be highly complex with a maximum cpt size of 2 23 for the variable human capital different techniques can be used to handle this complexity problem such as removing correlated variables exclude the nodes that have the least effect on the model output and node divorcing chen and pollino 2012 mccann et al 2006 node divorcing reduces a bn s complexity by reducing the number of parents of a complex node through dividing its relatively large number of parents into groups that feed in intermediate latent nodes which in turn become the new parents of the complex node this can be applied in our case by grouping sdgs indicators into groups of sdgs and then link the sdgs to the sustainability and resilience components for example indicators 4 1 2 and 4 4 1 will be the parents of the new node sdg 4 which in turn will be a new parent for the node human capital by applying this divorcing technique we were able to reduce the network complexity to be 2 16 the complexity check and divorcing concludes steps s9 and s10 in fig 1 4 1 6 latent variables parameterization and discretization latent variables are variables for which data is missing systemically or are variables that have never been observed holmes 2008 while there are learning algorithms that can handle bn models that contain latent variables this type of variables becomes problematic when they form an intermediate layer in the bn because they will cut the influence path between input nodes and output nodes i e the sensitivity of the output nodes to the input nodes will be dampened by this layer of latent variables marcot et al 2006 therefore it is recommended to attempt to quantify latent variables using the available data holmes 2008 marcot 2017 in the proposed bn this predicament occurs in two locations the first occurrence appeared as a by product of the nodes divorcing step where 13 sdg nodes formed a layer of latent variables between the observed sdg indicators and the lower level components of the sustainability and resilience oonfs the second occurrence takes place in the layer of the nodes representing the seven sendai targets a to g as no countries started to collect data about the progress in these targets thus they are all latent variables fig 4 shows the proposed oobn and the two latent layers indicated ll1 and ll2 respectively in order to avoid sensitivity dampening between input nodes and output nodes we apply factor analysis fa based parameterization for the variables in the latent layers to acquire approximated values for these variables that can be used in the oobn parameters learning fa is a multivariate statistical method used to condense a set of observed variables into a smaller set of unobserved latent variables known as factors yong and pearce 2013 the idea behind our application of fa to quantify latent variables i e fa based parameterization is to represent each latent variable as an index that gives a picture about the overall performance of its parent variables this simply achieved by converting the parent variables data into numbers that can be aggregated to form the latent variable index a similar idea was applied by tripathi and singal 2019 to calculate a water quality index the latent variable based on a selected set of water quality parameters the main difference between our application and theirs is that they applied a dimensionality reduction method prior to the fa step in order to select the most influential factors to include in their model while we keep all the parameters parents of the latent variables and use number of factors number of parameters 1 to assign weights to all parameters based on their importance for the latent variable under consideration in the following an example of the parameterization of one latent variable which is sendai target a or sendai ta for short will be shown to demonstrate the followed steps the same steps were applied for all the latent variables in ll1 and ll2 the first step in applying fa is to define the observed variables that will be used to quantify sendai ta we can see that sendai ta s direct parents are sdg 1 sdg 3 and sdg 15 as shown in fig 4 these parents are themselves latent variables that were parameterized in order to minimize error propagation between latent variables we select the original sdg indicators that were connected to sendai ta before network divorcing which were 1 5 1 3 6 1 3 9 1 3 9 2 3 d 1 and 15 3 1 for these six parameters we choose to keep five factors after selecting the number of factors the standard practice is to rotate the factorial axes using a rotation algorithm so the factor loadings don t change for this we used the varimax rotation tripathi and singal 2019 this step is performed to enhance the interpretability of the results as it produces a clear pattern of factor loading nardo et al 2006 after estimating and rotating the factor loadings they are then squared and scaled to unity to give the weights of individual parameters weights in this step can be interpreted as the proportion of total unit variance of the parameter that is explained by the factor nicoletti et al 1999 table 8 shows the squared and scaled factor loadings for the parameters of sendai ta the following step is to associate a wieght to each of the five factors equal to the ratio between the variance explained by that factor to the total explained variance of the data set table 9 shows the explained variance by each factor and the weight assigned to each factor finally the latent variable sendai ta is calculated by aggregating its six parents using the weights resulting from fa according to equation 8 this equation combines both weighted arithmetic and weighted harmonic means where f j is the weight of each of the m factors m 5 in our case p i is the weight of each of the n parameters n 6 in the example and v i is the parameter observed value tripathi and singal 2019 8 s e n d a i t a j 1 m i 1 n p i i 1 n p i v i f j j 1 m f j applying equation 8 enabled us to achieve step s11 in fig 1 which is followed by discretizing the new continuous values of the latent variable i e s12 in fig 1 discretization is performed using the same methodology explained in section 4 1 4 4 1 7 parameters learning and validation at this stage step s13 in fig 1 is applied and the conditional probabilities between the oobn variables is acquired for this task parameter learning is performed using the well known expectation maximization em algorithm which essentially estimates the maximum likelihood of the parameters for the underlying distribution of a given data set lauritzen 1995 em applies two steps interchangeably the expectation step e and the maximization step m the algorithm first estimates a parameters vector θ by applying calculating the maximum likelihood as if no parameters is missing m step then the expected value is calculated based on the estimate θ obtained from the m step the two steps is repeatedly applied till convergence i e there is no changes in the estimates and in the variance covariance matrix bilmes et al 1998 em is used for a broad range of applications because its ease of construction having a reliable convergence and its ability to work in the cases when the available data is incomplete nardo et al 2006 after completing the parameters learning step the final step in the data driven oobn development process is model validation s14 in fig 1 which evaluates the performance of the parameters learning process given the available data and the model structure for this purpose we apply the k fold cross validation approach the k fold cross validation approach is a version of the cross validation methods that breaks the data set into k equal size sub sets then trains the model on k 1 sub sets and tests it using the last k afterwards marcot 2012 this process is repeated for k times with different random k selected for testing each run boyce et al 2002 to validate the proposed oobn an extreme case of k fold cross validation known as leave one out in which the network is trained on n 1 records and tested on the remaining single record the leave one out version was chosen because of the relatively small size of the available model training data choosing the nodes of resilience and wpc as test nodes running the validation algorithm revealed that model was able to predict the values of the two test nodes with an accuracy of 0 88 and 0 77 respectively 5 case studies and analysis the network proposed in this work aims at modelling the resilience and sustainability of the sdgs and their constituent indicators this task was performed utilising a range of data driven techniques to determine the network s conditional probabilities which define the relationships between the variables included in it the network resulting from this process is presented in fig 4 in this section we demonstrate how the proposed network model can be utilised to answer questions related to policy analysis and evaluation at the time of disasters three case studies are chosen for the demonstration of the oobn utility and also to show the step by step process of querying and using the network the three applications also show the different ways in which the network model can be queried each of the case studies represent a different spatial scale i e national regional and global they were also chosen to represent different economic classes i e developed countries and developing countries as well as using data with different granularities the first case study examines the japanese post covid 19 integrated policy and how this policy can benefit holistic resilience and long term sustainability the second studies the positive and negative impacts of covid 19 on the sdgs and how does that reflect on resilience and sustainability the final case study focus on sub saharan africa and how the region can benefit from focusing on environmental issues when planning post covid 19 strategies 5 1 impacts of covid 19 long term responses on resilience and sustainability a case study from japan the outbreak of covid 19 had an unprecedented impacts on the social and economic systems of all nations due to the pandemic the world economy is expected to suffer economic burdens of usd 12 trillion in 2021 imf 2020 while the global job losses are estimated to reach usd 130 million worth of full time positions ilo 2021 in response to this global crisis multilateral as well as domestic spending programs have been initiated such as the un s usd 2 billion global humanitarian response plan unca 2020 as to the former on country level governments have launched different stimulus packages to help in offsetting the impacts of covid 19 for example the australian government has planned a fiscal stimulus of 10 4 of its gdp to be spent by the end of fy2021 the united states of america plans 14 of the gdp germany plans 27 of the gdp and japan plans 42 of the gdp imf 2021 in addition to these imperative spendings national efforts to alleviate the risks and impacts of covid 19 are expected to include long term actions directed toward improving health care systems building more reliable critical infrastructure promoting the use of more efficient technologies and building the society s capacity so it can cope with future external shocks marome and shaw 2021 hörisch 2021 nonetheless these long term actions are not guaranteed to result in inclusive resilience or help the nations that apply them to achieve the 2030 agenda dewit et al 2020 the oobn presented in this paper serves as an effective tool that helps policy makers examine the impacts of their policy options on the national resilience and sustainability this section discusses the case of japan as a country for which resilience is of high priority due to the severe disasters it frequently faces achour et al 2015 here we use the developed oobn to explore how the japanese policy choices in the wake of covid 19 affect the country s resilience and sustainability through the measuring criteria included in the model for countries like japan whose risk exposure is amongst the highest in the world behlert et al 2020 the occurrence of disasters such as floods and earthquakes during a pandemic forms a case of compounding disasters which requires integrating holistic resilience with sustainability to recover from the long term effects of such case potutan and arakida 2021 dewit et al 2020 therefore the japanese government dedicated a special stimulus of usd 120 9 billion on december 2019 for resilience oriented purposes that are not limited to the measures directly related to fighting covid 19 japan cabinet secretariat 2020 that sum of usd 120 9 billion is intended to fund policies that have a more holistic view of resilience such as improving national resilience plans and reducing the economic risks posed by the covid 19 countermeasures japan cabinet secretariat 2020 in order to assess the impacts of policies that follow such a holistic or an integrated approach we use the following steps 1 first determine the sdg indicators that are related to the policy options the nodes related to these indicators represent the evidence nodes that will be used to feed the network with the policy evidence ideally establishing links between sdg indicators and policy options is performed in a participatory setting that includes policy makers experts and stakeholders allen et al 2017a b maes et al 2019 but this is beyond the scope of this paper herein we make use of the connections already established and mentioned in literature and official documents such as voluntary national reviews vnrs in the case there is no data available for indicators evidence for sdgs can be directly used in this particular case of japanese policy the related indicators are shown in table 10 2 the model is run using evidence representing the business as usual bau scenario i e if the trend of progress of the indicators continue with the same rate this step gives us the no policy probabilities for the different states of both resilience and sustainability in this step we use evidence for all the indicators included in the network in order to eliminate the risk of latent variables 3 model users can then update it with the post policy evidence i e the states of the nodes related to the policy indicators are updated this gives us the post policy probabilities of resilience and sustainability 4 comparing the no policy and post policy probabilities of resilience and sustainability shows the effect of the policy actions on both of the concepts fig 5 a shows the probabilities of the states of the variables of interest using the no policy evidence while fig 5 b shows the impacts of implementing the policy actions in table 10 following such an integrated policy is expected to boost achieving the a more desirable state of the japanese wealth per capita by 2 this policy is also expected to increase the chance of achieving a higher resilience state by 5 and increase the chance of achieving the most desirable resilience state by 1 in order to incorporate the impact of data vacuum i e the inconsistency of the existing data and its incompleteness dang and serajuddin 2020 del río castro et al 2020 the states of both resilience and sustainability are represented as probability distributions themselves figs 6 and 7 show the probability distribution representing each of the states where each probability distribution indicates the expected value for that state and the range of values covered by this state along with the probability of the covered range in fig 6 for example the resilience state c1 is associated with a resilience index between the values of 63 and 78 with a most expected value of 70 this approach of representing the states as probability distributions reduces the obscurity of the states by revealing the range of values binned in each state as well as reporting the statistical features of those values e g the most expected value and the standard deviation of the values this example manifests how useful can the model presented in this paper be in comparing policies and assessing their impact on the societies long term resilience and sustainability particularly at times of compound risks where integrated policies need to be applied the japanese approach can be a lesson for all developed countries that integrating the 2030 agenda indicators in their efforts to face crises can have a long term positive effects on the sustainability and resilience of these countries it also shows that diversifying the resources allocated to post disaster restoration between short term and long term actions can help in increasing their ability to absorb future shocks and hazards 5 2 how the interplay of the sdgs restricted by covid 19 and the sdgs reinforced by covid 19 affects resilience and sustainability of nations the outbreak of the covid 19 virus has adverse impacts on the social economic and environmental systems allover the world in addition to testing the resilience and resourcefulness of all the affected nations the pandemic is expected to hinder the nations efforts to achieve the sdg by 2030 ekwebelem et al 2021 munasinghe 2020 lópez feldman et al 2020 fleetwood 2020 although the main focus of research efforts was to measure the negative impacts of covid 19 on the sdg and to provide policy recommendations on how to minimize such impacts a new line of research emerged recently which adopt a more comprehensive approach in studying the covid 19 impacts on sdg i e take into consideration the promoting effects of covid 19 on some of the 2030 agenda targets based on this comprehensive approach we utilise the proposed network model to study how the interplay between the restricting and reinforcing impacts of covid 19 on sdg can affect resilience and sustainability this is achieved by comparing the prior probabilities of resilience and sustainability with their probabilities after using the evidence reflecting the positive and negative impacts of covid 19 on the sdg indicators the steps used to discover the effects of sdgs interplay are 1 first is run using the evidence representing the bau scenario i e with the prior probabilities of the indicators the goals resilience and sustainability 2 evidence about the impacts of covid 19 on sdg at an indicator level is collected form literature we chose to use indicator level evidence instead of goal level evidence because regarding a goal as being restricted or reinforced by covid 19 entails that is constituent indicators are all either restricted or reinforced which is not always the case as analysis shows 3 the model is run using the evidence representing the interactions between the restricted and the reinforced indicators these interactions are presented to the model by assigning low desirability states to the restricted indicators and high desirability states to the reinforced indicators 4 the probabilities of resilience and sustainability associated each each scenario are then compared in general indicators and goals that are threatened by the pandemic are the ones that are directly related to health and economic drivers for example sdg 1 and its subsequent indicators which aims at ending poverty and reducing the number of deaths caused by disasters will be greatly affected by covid 19 in a negative way vigo et al 2020 united nations 2021 similarly the goals and indicators related to equality well being promotion education sustaining economic growth ensuring sustainable consumption and production reducing deaths and economic losses due to disasters i e the issues related to goals sdg 3 sdg 4 sdg 5 sdg 8 sdg 10 and sdg 13 will be negatively affected nicola et al 2020 ekwebelem et al 2021 rahayu and rahwadwiati 2021 cénat 2020 kinney et al 2020 lee 2021 on the other hand the promoting impacts of covid 19 on the sdg are mainly originating from the relaxed stress on natural systems caused by lock downs and travel restrictions those measures induced a reduction in co 2 and ghg emissions as well as other air pollutants chowdhury et al 2021 chen et al 2020 le quéré et al 2020 covid 19 countermeasures have also helped in revitalising different natural systems such as water bodies and caused a reduction in energy consumption which lead to a drop in the rate of fossil fuels usage goffman 2020 saadat et al 2020 arora et al 2020 escap 2020 these effects are related to sdg 6 sdg 7 sdg 14 and sdg 17 the details of the restricted and reinforced indicators under each sdg are summarized in fig 8 based on the evidence from fig 8 the probabilities of the states of resilience and sustainability change as presented in fig 9 the interplay of the positive and negative effects of covid 19 caused an increase in both the probabilities of the least desirable and most desirable resilience states i e resilience state 0 and resilience state 1 respectively this increase in probability of achieving the least desirable resilience state can be attributed to the direct stress covid 19 imposes on the nations resilience capacities and the losses in lives and the economic losses caused by the pandemic the increase in the most desirable state however is mainly caused by the adaptive measures taken to overcome the disaster and the preventive measures taken to prepare for similar future disasters nonetheless the overall trend of the three states did not change which means that for most of the countries more plans and investments need to be directed toward building national resilience in terms of sustainability here represented by wealth per capita the restoration of ecosystems and the forced decarbonization of the world economy caused wealth trajectories to move toward a more sustainable path such a sustainability result shows that governments and policy makers should take advantage of the current unique circumstances to integrate more sustainable options in their policies 5 3 can environmental stewardship help sub saharan africa offset covid 19 externalities sub saharan africa is among the regions that have been greatly affected by the covid 19 pandemic due to the fragility of their economic situation before the pandemic as well as their lack of the necessary resources to face such disaster shimeles et al 2018 gralak et al 2020 prior to the outbreak of covid 19 the sub saharan africa region was already lagging in terms of the sdg progress the spread of the deadly virus caused additional economic and social stresses for the region for example sub saharan africa countries suffered around 3 of losses in their gdp capita since the pandemic outbreak in addition the percentage of people living in extreme poverty in the region increased from 39 to 41 in just one year 2019 2020 united nations 2021 consequences like these will make attaining the 2030 agenda goals more challenging for countries in sub saharan africa in their effort to provide policy recommendations that can help sub saharan africa achieve the sdg within the designated time frame some researchers suggest that addressing the environmental issues can play a significant role in helping the region make faster progress in achieving the sdg omisore 2018 in line with this recommendation we use the proposed oobn to investigate the impacts of addressing the environment related sdg on the long term sustainability and resilience of sub saharan africa to determine the impacts of fostering environmental goals on the overall resilience and sustainability of sub saharan africa countries the following steps are used 1 we start by using the prior conditional probabilities to provide the bau resilience and sustainability probabilities 2 the goals that are most relevant to environmental management in post pandemic scenarios are identified 3 use the environmentally relevant goals as interventions and evidence for the network to represent the scenario focusing on enhancing the environmental stewardship 4 comparing the bau and the post policy probabilities of resilience and sustainability shows the effect of attending to the sdg related to ecosystems in order to determine which goals to focus on or to use as policy levers we utilise a set of goals that have been identified in literature as the most relevant to environmental management in post pandemic world these sdg are sdg 3 sdg 6 sdg 11 sdg 12 and sdg 15 chowdhury et al 2021 similar to the steps followed in section 5 1 we enter the evidence to the network without interventions bau scenario followed by the evidence representing the intervention and compare the probabilities from both scenarios the bau scenario evidence for sub saharan africa countries was retrieved from the united nations 2021 and the intervention was informed by evidence from literature chowdhury et al 2021 cheng et al 2021 fig 10 shows the results of focusing on the recommended sdgs the main important outcome of this result is that focusing on the goals that have an environmental focus can double the probability to achieve the most desirable resilience state as well as significantly increase the probability to achieve a sustainable state 6 conclusion in this paper we proposed an oobn that models the resilience and sustainability of the sdgs and their subsequent indicators the structure of the proposed network was informed by two acknowledged composite indices which represent resilience and sustainability to represent the sustainability component of the oobn the iwi was selected while the wri combined with the sfdrr was selected to represent the resilience component to determine the network parameters i e the conditional probability distributions that connect the network variables parameter learning was utilised using the em algorithm additionally factor analysis methods where utilised to approximate latent variables that have the potential to dampen the influence in the network the proposed oobn application offers an effective tool for policy makers that enables them to prioritize the sdg indicators implementation as well as test the impacts of new policies on both the resilience and sustainability of a nations the application also indicates which areas that need to be tackled in order to achieve selected sustainability or resilience levels additionally a method was proposed to quantify the uncertainty associated with using concepts from different disciplines where the definition of such concepts is vague or no data available about them the proposed oobn has demonstrated that with its current formation changes in certain sdgs particularly sdg 8 sdg 9 and sdg 10 will have significant impact on both sustainability and resilience of the systems one of the major challenges that hinder the attempts to measure the progress in the sdgs or model topics related to them is the data vacuum problem the approach we chose for the purpose of this paper provides multiple advantages in regard to dealing with such problem first its ability to learn from limited amount of data makes it very adaptable that it can be used in different contexts and across different scales global national regional moreover it allows for combining multiple methods for model development which is useful in accounting for data unavailability also having the ability to report second degree uncertainty i e uncertainty about the probable results is very important for building confidence about the performed analysis and to quantify the value of information so decision makers can determine the amount of needed information in order to reach a certain level of confidence one of the main limitations of the applied approach however is associated with continuous data discretization the discretized data is converted from a continuous form to a categorical form and the output comes in such a categorical form as there is no means to convert the categorical output back into its original continuous form the analysis has to be done using the categorical data which can impact the quality of the analysis and reduce its interpretability the proposed model can be extended and enhanced in a number of ways first policy options can be included in the model and linked to the sdg indicators for instance sdg 6 and its subsequent indicators can be linked to a variable that represent the average distance to water source to model how that affect water consumption and personal hygiene and consequently how that affect resilience and sustainability assigning costs to policy options will also be beneficial for policy makers as it will allows them to value the utility of each option so they can optimize their decisions moreover the proposed model does not account for the interactions among the sdgs so considering these interactions will enable the policy makers to take them into account and avoid the problems arising from conflicts between goals finally the oobn formalisation can benefit from utilising object oriented programming concepts such as polymorphism so multiple algorithms can be available for the network and the network can choose to implement the one that is most suitable for the data used or the particular application declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the core of our implementation is based on the smile reasoning engine for graphical probabilistic models available free of charge for academic research and teaching use from bayesfusion llc https www bayesfusion com appendix 1 table a1 mapping between sdg indicators and components of the sustainability index table a1 sdg indicator human capital natural capital produced capital co 2 damages tfp 1 5 1 x 1 5 2 x 1 5 2 x 1 a 2 x x 2 1 1 x 2 2 3 x 2 3 1 x x 2 4 1 x 2 5 2 x 2 a 1 x x 3 1 1 x 3 2 1 x 3 6 1 x x 3 8 1 x x 3 9 1 x x x 3 9 2 x x 4 1 2 x 4 4 1 x 6 2 1 x x 6 3 1 x x 6 3 2 x 6 4 2 x 7 1 1 x x 7 2 1 x x 7 3 1 x x x 8 2 1 x 8 5 1 x 8 5 2 x 9 1 1 x 9 4 1 x x 10 4 1 x x 11 1 1 x x 11 5 1 x 11 5 2 x 11 6 2 x x 13 1 1 x 13 2 2 x x x 14 2 1 x 14 4 1 x 15 1 1 x 15 2 1 x 15 3 1 x table a2 sdg indicators data summary table a2 sdg indicator description source countries with available data 11 1 1 proportion of urban population living in slums informal settlements or inadequate housing un habitat 105 15 3 1 proportion of land that is degraded over total land area fao 124 3 9 1 mortality rate attributed to house hold and ambient air pollution the world bank data 171 3 9 2 mortality rate attributed to unsafe water unsafe sanitation and lack of hygiene exposure to unsafe water sanitation and hygiene for all wash services world health organisation the world bank 167 7 3 1 energy intensity measured in terms of primary energy and gdp un statistics division 171 3 8 1 coverage of essential health services world health organisation 171 2 1 1 prevalence of undernourishment fao 171 1 5 1 number of deaths missing persons and persons affected by disaster per 100 000 people un office for disaster risk reduction 92 3 6 1 death rate due to road traffic injuries world health organisation 171 2 3 1 volume of production per labour unit by classes of farming pastoral forestry enterprise size fao 171 1 5 2 direct disaster economic loss in relation to global gross domestic product gdp sendai framework monitor un office for disaster risk reduction 69 9 4 1 co2 emission per unit of value added international energy agency iea un industrial development organisation unido 171 9 1 2 passenger and freight volumes by mode of transport un conference on trade and development unctad 136 3 c 1 health worker density and distribution global health observatory world health organisation 144 3 9 3 mortality rate attributed to unintentional poisoning world health organisation 156 1 3 1 proportion of population covered by social protection floors systems international labour organisation ilo world development indicators database 67 6 5 1 degree of integrated water resources management implementation 0 100 un environment programme 172 7 a 1 mobilized amount of united states dollars per year starting in 2020 accountable towards the 100 billion commitment oecd the international renewable energy agency irena 131 17 3 2 foreign direct investments fdi official development assistance and south south cooperation as a proportion of total domestic budget world development indicators 171 3 d 1 international health regulations ihr capacity and health emergency preparedness world health organisation global health observatory 159 3 8 2 number of people covered by health insurance or a public health system per 1 000 population world health organisation 31 10 5 1 financial soundness indicators international monetary fund imf 126 1 5 3 number of countries with national and local disaster risk reduction strategies un office for disaster risk reduction 93 3 a 1 age standardized prevalence of current tobacco use among persons aged 15 years and older world health organisation 171 8 9 1 frequency rates of fatal and non fatal occupational injuries oecd statistical office 57 9 a 1 total official international support official development assistance plus other official flows to infrastructure oecd 171 2 1 2 prevalence of moderate or severe food insecurity in the population based on the food insecurity experience scale fies fao 44 12 1 1 number of countries with sustainable consumption and production scp national action plans un environment programme 74 2 3 2 average income of small scale food producers by sex and indigenous status fao 171 6 6 1 change in the extent of water related ecosystems over time global surface water explorer un environment programme 171 10 4 1 labour share of gdp comprising wages and social protection transfers ilo 171 6 3 2 proportion of bodies of water with good ambient water quality un environment programme 33 2 a 1 the agriculture orientation index for government expenditures fao imf un stats 151 3 1 1 maternal mortality ratio world health organisation unicef 171 8 5 2 unemployment rate ilo 133 2 5 2 proportion of local breeds classified as being at risk not at risk or at unknown level of risk of extinction fao 77 7 1 1 proportion of population with access to electricity world bank 171 7 2 1 renewable energy share in the total final energy consumption iea un statistics division 171 6 4 2 level of water stress freshwater withdrawal as a proportion of available freshwater resources fao 171 15 2 1 progress towards sustainable forest management un environment progaramme 205 6 2 1 proportion of population using safely managed sanitation services including a hand washing facility with soap and water who unicf 171 15 1 1 forest area as a proportion of total land area fao 163 4 4 1 proportion of youth and adults with information and communications technology un statistical division 46 8 2 1 annual growth rate of real gdp per employed person ilo 171 6 3 1 proportion of wastewater safely treated un habitat 79 8 5 1 average hourly earnings of female and male employees by occupation age and persons with disabilities un statistics division ilo 52 1 a 2 proportion of total government spending on essential services education health and social protection unesco institute of statistics 117 4 1 2 proportion of children under 5 years of age who are developmentally on track in health learning and psychosocial well being unesco institute of statistics 74 3 2 1 under five mortality rate who 171 11 6 2 annual mean levels of fine particulate matter e g pm2 5 and pm10 in cities who 137 
25629,understanding the evolution of natural systems spatio temporal dynamics is paramount in modern ecology we focused on highlighting and analysing temporal and spatial dynamics of remotely sensed chlorophyll a concentration this pigment is linked with phytoplankton production which in turn play a pivotal role in marine environment satellite platforms offer a synoptic view of surface chlorophyll a concentration for the last two decades coupling this source of information with statistical and machine learning techniques could help highlighting eventual patterns we merged the mediterranean chlorophyll a satellite data for the last two decades into a single dataset we tested several techniques for reconstructing missing data and performed a general analysis finally we implemented a dynamic time warping self organizing map algorithm to cluster our series showing that an elastic distance measure outperforms a non elastic one the proposed satellite data management and analysis provided insights on spatio temporal chlorophyll a dynamics in the mediterranean basin keywords chlorophyll a remote sensing time series analysis data imputation dynamic time warping self organizing maps 1 introduction understanding the dynamics of natural systems is one of the main challenges in environmental science several studies suggest that we live in a new epoch named anthropocene in which anthropic pressures contribute to shape ecosystems evolution lewis and maslin 2015 adding the climate change scenario to the equation it becomes evident that the spatio temporal analysis of natural processes is a crucial topic in modern ecology barange et al 2014 grimm et al 2013 approximately 70 of the earth s surface is covered by the oceans which deeply influence the climate of our planet interacting with the atmospheric and terrestrial ecosystems chlorophyll a concentration is one of the most exploited environmental variables associated with the marine ecosystems d ortenzio and ribera d alcalà 2009 lavigne et al 2012 nieto and mélin 2017 sammartino et al 2018 this variable is intimately linked with the biomass and the physiological state of phytoplankton therefore it has been widely used in phytoplankton primary production studies behrenfeld and falkowski 1997 daniels et al 2013 lee et al 2015 phytoplankton production is a pivotal oceanographic process involved in the air sea interactions doney 1995 longhurst and glen harrison 1989 the structuring of marine food webs anderson et al 2018 kwak and park 2020 shurin jonathan et al 2006 and the sinking of organic matter sabine et al 2004 these processes related to the primary production deeply influence several aspects of the biogeosphere and the earth s climate blanchard et al 2012 hays et al 2005 moore et al 2018 trolle et al 2014 in this framework we believe that studying the chlorophyll a spatio temporal dynamics is crucial to enhance our understanding of marine ecosystems to pursue this objective we exploited the invaluable source of information provided by satellite information in particular the seawifs and the modis aqua satellites provided a synoptic view of the ocean color in the past decades the products derived from the activity of these two satellites are freely available at the nasa ocean color website https oceancolor gsfc nasa gov the longest record of chlorophyll a remotely sensed data is represented by the 9 km mean monthly grids these data range from 1998 to present thus providing roughly 23 years of records at the time of this writing it should be noted that the remotely sensed chlorophyll a for each grid pixel repeated over time can be seen as a time series exploiting this valuable source of information we decided to perform a spatio temporal analysis of these remotely sensed time series in order to deal with large amount of data the ecologist community is increasingly employing machine learning ml techniques catucci and scardi 2020 chen et al 2020 franceschini et al 2019 mattei et al 2021 mattei and scardi 2020 park et al 2014 peters et al 2014 young et al 2011 this family of approaches allow to perform several tasks without an explicit formulation of the relationships between the variables in play moreover several machine learning approaches are robust with respect to noisy data these features are particularly useful in dealing with natural system and the process that occurs in it which are dominated by complex non linear dynamics lek and guégan 1999 olden et al 2008 peters et al 2014 recknagel 2001 we aimed at developing an approach that would provide insights into the studied system evolving dynamics in a climate change context this work comprised three main phases firstly we gathered the 9 km seawifs and modis aqua monthly mean chlorophyll a data huot et al 2007 then we merged all the grids in single dataset in which each row represented a scanned pixel in the mediterranean basin while the 276 columns corresponded to the monthly surface chlorophyll a concentration from 1998 to 2020 in order to develop and test our methodology we limited the study area to the mediterranean basin limiting the approach development area allowed to carry out several tests avoiding the limitations that would occur from time consuming computation in fact even the mediterranean subset comprised over 39000 time series afterward we excluded from our dataset all the series that showed a missing data percentage larger than 25 of the series length 69 out of 276 values for the remaining series showing missing values we tested several approaches for missing data imputation of multiple univariate and spatially related time series finally we tested a dynamic time warping self organizing map dtw som d berndt and clifford 1994 kohonen 1982 2013 algorithm for clustering time series that were structured in time and space we analysed for each cluster whether a trend was present or not and the characteristics of the series in relation to both geographical area and possible trend we performed the same procedure using a som with the euclidean distance as distance metric to assess if the dtw som outperformed a standard implementation euclidean som 2 materials and methods 2 1 data gathering and pre processing the satellite data used in this work are freely available at the nasa ocean color website https oceancolor gsfc nasa gov we selected the chlorophyll a 9 km monthly mean data since they covered the largest time span among the remotely sensed products we exploited the data gathered from the seawifs and the modis aqua missions the former was active from september 1997 to december 2010 while the latter started on july 2003 and is ongoing we used the seawifs data from 1998 to 2003 and the modis aqua ones from 2004 to 2020 for a total of 23 years 276 months the 9 km chlorophyll a monthly mean values are stored in a 2160 4320 matrix including missing data and land points with a resolution of 0 082 0 082 latitude x longitude firstly we downloaded the 276 monthly files relative to the period 1998 2020 and we extracted from each grid the mediterranean pixels this procedure generated 276 grids with a dimension of 188 509 latitude x longitude the latitude of each grid ranged from 45 7917 to 30 2083 while the longitude ranged from 6 0416 to 36 2917 secondly we filtered out all the land pixels and the data that the satellite gathered from lakes finally we merged all the extracted data in a single dataset in which each row accounted for a mediterranean pixel and the columns corresponded to the months from 1998 to 2020 the only exception was the grid relative to november 1999 the monthly mean data relative to this month were not available to tackle this issue we downloaded from the same website all the available daily data for november 1999 and created a monthly mean averaging them in this configuration each row of the dataset represented a time series of chlorophyll a concentration for a given pixel of the mediterranean basin after the data collection and the pre processing stage see fig 1 the dataset comprised 39166 time series 39166 rows x 276 columns since the range of the series max min could vary by several order of magnitude we decided to apply a logarithmic transformation to our series before performing any further analysis this transformation minimized the effect of very large values which could have skewed the whole work 2 2 missing data imputation inspecting the mediterranean monthly chlorophyll a series we found that roughly 80 of the series 31363 showed no missing values while less than 5 1896 lacked more than 69 values over 276 25 of the series length we opted for discarding the latter share of data and reconstructing the rest of the series after this procedure our dataset comprised 37270 rows time series 5907 of which showed missing values to a maximum of 25 of the whole series the missing data resulted from the cloud coverage which can disrupt the interaction between the satellite sensors and the ocean surface this is a common issue of remote sensing platforms thus being able to reconstruct missing values from the available ones could enhance the value of the satellite information jouini et al 2013 we tested several techniques to impute the missing data in our series we created a test set 1565 time series to assess the quality of the imputation and compare the results of the different approaches and a training set 35705 time series for the techniques that required one the test subset was extracted only among the series that did not show any missing value to ensure that the test set encompassed the overall variability of our series we arbitrarily divided the complete series in five clusters using the k means algorithm the k means algorithm has been implemented using the python package scikit learn pedregosa et al 2011 the algorithm initialization was set to k means arthur and vassilvitskii 2007 which used an optimized centroid initialization the maximum number of algorithm iterations was set to 1000 and the procedure was repeated 50 times the best solution in terms of stress was selected as the optimal one afterward we randomly sampled 5 of the patterns from each cluster to create the test set finally we created artificial missing data by randomly deleting 25 of each test set series we used the r package imputets moritz and bartz beielstein 2017 to test approaches such as linear interpolation spline interpolation stineman interpolation and kalman smoothing all these techniques have been performed with and without seasonal decomposition and seasonal split we also wrote an algorithm to test a monthly reconstruction in which we imputed a missing value averaging all the available values for the same month along the series for example if the january 2000 monthly mean value was missing we took all the january values available in the same series from 1998 to 2020 we computed their average and use it to impute the missing value the rationale behind this approach relied on the nature of the series which were composed by monthly mean values finally we applied more complex approaches belonging to the ml domain such soms and recurrent neural networks rnns in particular long short term memory lstm hochreiter and schmidhuber 1997 and gated recurrent unit gru chung et al 2014 the som algorithm kohonen 1982 2013 was implemented in a fortran program written by the authors and available on the github profile of the first author see software availability section we opted for the fortran code due to the possibility to customize the algorithm see section 2 3 and 3 2 and the fast computational performances which are an advantage especially in dealing with large dataset the som algorithm is a powerful tool it can strongly reduce the data complexity while preserving their topology céréghino and park 2009 kohonen 2013 nkiaka et al 2016 park et al 2006 we utilized the training set to train several soms with different architectures from 96 to 1400 nodes as rule of thumb the optimal number of nodes should be 5 x n s a m p l e s so we did not largely exceed this number in our tests vesanto and alhoniemi 2000 for the imputation phase we used the euclidean distance as distance measure and a hexagonal non toroidal topology the weights were updated online during 1500 epochs we chose a gaussian neighbourhood function the initial radius value was set to 2 3 of all unit to unit distance and decreased linearly to 0 during the epochs the initial learning rate value was set to 0 05 and it decreased linearly to 0 01 over the number of epochs the presence of incomplete series into the training set was not an issue since the som can easily deal with missing values in fact the best matching unit bmu searching procedure exploited only available data when a series showed missing values once completed the training procedures we mapped the test set series on the trained soms imputed the missing values using the som codebooks the lstm and gru approaches were implemented exploiting the tensorflow package abadi et al 2016 using the python programming language using these rnn approaches the model tries to learn a function that using past observations can predict an output connor et al 1994 saad et al 1998 we tested solutions that used from 3 to 12 previous values to predict the next one in the series we also tested single and double hidden layer architectures for the first hidden layer we tried solutions from 50 to 500 nodes and from 25 to 250 for the second one the selected activation function and the optimizer were relu and adam kingma and ba 2017 respectively this approach was tested after a first imputation step performed with the monthly reconstruction which returned the best outcome among the simple imputation techniques subsequently we scaled the data into 0 1 range and trained the rnns to predict the test set artificial missing values it is important to note that these ml techniques do not require any formal assumption neither on the mutual independence nor on the distribution of the variables they are also robust to noisy data which are the norm handling environmental variables lek and guégan 1999 olden et al 2008 recknagel 2001 these features can also be an asset in dealing with time series we assessed the imputation accuracy of these techniques by comparing their estimates to the artificially removed values of the training set we used the mean squared error mse and the mean absolute error mae as metrics to evaluate the imputation quality the optimal solution for our case study was the som with 1200 nodes see table 1 and fig 2 section 3 1 the number of nodes was selected on the basis of mae mse and mse trend with respect to the number of nodes once completed the data imputation phase the dataset comprised 37270 complete monthly mean chlorophyll a concentration time series we used the newly available information to compute the average chlorophyll a concentration in the mediterranean basin from 1998 to 2020 see fig 3 section 3 1 we also checked whether or not the time series showed a monotonic trend using the seasonal version of the mann kendal test with α 0 05 kendall 1975 mann 1945 see fig 4 section 3 1 2 3 univariate and spatially related time series clustering the need for a clustering procedure arose from the numerosity of the series in our dataset in fact it was not possible to investigate the dynamics of all the mediterranean pixels thus we decided analyse time series typologies we opted for the som algorithm since reducing the data complexity is one of its strengths céréghino and park 2009 kohonen 2013 two main differences distinguished the application of this versatile algorithm for the data imputation task from the clustering one the first one concerned the dimension of the map for the imputation task the best results were achieved through large maps over 1000 nodes which provided a detailed representation of the data space on the other hand a clustering procedure aimed at extracting patterns from a complex scenario required a smaller number of nodes in this way we could compress over 37 000 records into fewer time series typologies the second difference involved the distance metric used to find the bmu during the som training the clustering procedure could benefit by an elastic distance measure since the records in our dataset are time dependant bagnall et al 2017 accordingly we integrated in our program the dtw as distance metric d j berndt and clifford 1994 the dtw approach consists in computing a cost matrix pairing all the series values and finding the shortest path from the start to the end of the series this metric allows to calculate the optimal matching between two sequences considering non linear similarities we also integrated the possibility to define a window parameter which constraint the search of the shortest path to a given window around the matrix diagonal sakoe and chiba 1978 the importance of this parameter did not only concern the computational time of the algorithm even though we wanted to consider a non linear pairing between our series we had to take into account the ecological characteristics of our data in fact the monthly mean data differences reflected the peculiarities in the seasonal behaviour of the mediterranean chlorophyll a time series for this reason we decided to test only small window values 1 and 2 to be consistent from an ecological perspective larger window values could have found similarities between series in which the time shift was caused by relevant seasonal differences that we wanted to preserve in our analysis we tested several soms architecture to find the optimal one for the clustering task we trained all the possible combination from 3 to 6 rows and from 4 to 10 columns avoiding squared maps i e 3 4 3 10 4 5 4 10 6 10 we also tried solution ranging from 4 to 10 rows and 3 to 6 columns 4 3 10 3 5 4 10 4 10 6 but we did not find any significant difference between the two approaches since several stochastic processes occur during the som training som weights initialization and pattern extraction during the training epochs we repeated each training 10 times the training modalities were the same described in section 2 2 we did not scale the chlorophyll a time series because we wanted to keep the magnitude signal into the dtw som training we joined two different criteria to evaluate the quality of the clustering procedure considering that we aimed at highlighting the mediterranean basin chlorophyll a dynamics the first criterion took into account the trend of the time series as previously done for the mediterranean pixels we tested the significance of a monotonic trend for each som codebook using the seasonal version of the mann kendall test α 0 05 subsequently we compared the trend significance and direction of each pixel series with the ones of their bmu the trend agreement between the mediterranean chlorophyll a series and their bmu was used as accuracy measure for the som clustering this measure assessed how well the time series typologies derived from the clustering represented the series assigned to them the second aspect that we considered to evaluate the clustering was the proportion of som codebook that showed a significant trend this proportion indicated whether or not an increase in the nodes number was followed by a more detailed dataset features representation following these criteria we selected the optimal clustering solution subsequently we analysed the characteristics of the som codebooks with respect to their trend and the features of the series assigned to them as conclusive analysis we mapped the chlorophyll a series associated with codebooks that showed a significant trend to analyse the clustering results from a geographical perspective 3 results and discussion 3 1 missing data imputation the gathered satellite time series presented missing values caused by cloud coverage this is one of the main issues in dealing with remotely sensed information to tackle this loss of information we tested several imputation techniques the imputets r package offered a user friendly implementation of the most common missing data reconstruction approaches we exploited this software for the linear interpolation the spline interpolation the stineman interpolation the seasonally split and seasonally decomposed kalman smoothing these are among the most direct and general techniques to implement moreover they are not time consuming since their computational time is negligible on the other hand these approaches are rarely the most accurate ones we found that the kalman smoothing solutions which took into account the seasonal aspects of our data offered the best reconstruction among the abovementioned techniques table 1 bearing in mind the ecological and temporal features of our series we decided to test a monthly reconstruction this approach exploits all the available information about the missing month and uses the average as imputation this dataset tailored technique showed good results outperforming the tested general solutions proposed by a standard package m s e 0 0089 m a e 0 0650 using the monthly reconstructed series we also tested the lstm and gru approaches which belong to the rnns family we trained several architectures with one and two hidden layers and found that in our case the single hidden ones showed better performances testing the numbers of previous values needed to predict the next one in the series emerged that 12 was the optimal one this is probably related to the monthly nature of our data and is consistent with the better results obtained with the seasonally decomposed and seasonally split kalman smoothing the best results using the lstm and gru were achieved with 200 and 300 hidden nodes respectively the two approaches did not show any significant differences in terms of imputation performances but both of them improved the accuracy level achieved by the monthly reconstruction table 1 the tested technique that returned the best results for missing data imputation was the som algorithm we tested several architectures and selected the one with 1200 nodes on the basis of mse and mae m s e 0 0030 m a e 0 0327 and the mse trend with respect the number of nodes the mse showed a decreasing trend for different percentages of missing values as the number of nodes increased fig 2 the error trend started to increase in maps with a number of nodes larger than 1200 for this reason we selected the solution associated with the mse minimum as the optimal one for this task the som was trained using over 35 000 time series thus having access to a large pool of information moreover reconstructing the time series missing data using the som algorithm mimed a k nearest neighbors approach with the advantage of a variable k instead of a fixed one in fact the larger the data density of a som area the larger the value of k neighbors from which the codebook is derived this approach was able to exploit spatial and temporal similarities among the series in order to impute the missing data after the missing data imputation phase the mediterranean dataset comprised 37270 complete monthly mean chlorophyll a time series 37270 rows and 276 columns thanks to this procedure we were able to reconstruct roughly 6000 series thus enhancing the amount of information provided by the remotely sensed data this enhancement allowed to construct a more detailed picture of the mediterranean from space and also added more detail to the clustering procedure see section 3 2 exploiting the new available information we computed the average chlorophyll a concentration in the mediterranean basin from 1998 to 2020 fig 3 fig 3 clearly shows that the western portion of the basin is characterized by larger chlorophyll a values with respect to south central and eastern areas this feature is mainly caused by the water inputs from the atlantic ocean through straits of gibraltar the second general feature is represented by the large values associated with coastal areas the inputs from surrounding terrestrial ecosystems boost the nutrient availability in coastal waters thus enhancing phytoplankton productivity among these areas the ones showing larger values comprises the italian coasts of the north adriatic sea and the north egypt shores these are the areas in which are located the delta of the po and nile rivers respectively also gulf areas are characterized by considerable chlorophyll a values e g gulf of gabes these sea portions have relevant interaction with terrestrial environment proportionally to their area the missing data imputation allowed also to check the monotonic trend significance and direction using the mann kendall test we used the seasonal version of the test setting the period parameter to 12 since we were dealing with monthly observations the significance of the trend was ascertained setting α 0 05 and comparing it with the p value while the direction was determined by the tau parameter the larger the absolute value of τ the stronger is the trend intensity while the sign of this parameter indicates the direction of the trend the mann kendall test highlighted that 25237 series 67 71 showed no significant monotonic trend while 7411 19 89 and 4622 12 4 ones presented a significant decreasing and increasing monotonic trend respectively fig 4 the mann kendall τ map fig 4 presents an overall picture of the mediterranean chlorophyll a temporal dynamics in the last two decades we can highlight 4 areas associated with decreasing trend the first one is located in western portion of the basin is largely fragmentated and shows low tau absolute values the second one covers a large portion of the central southern mediterranean sea the trend strength of these series ranges from low to high the third decreasing trend area occupies the northern portion of the aegean sea and presents moderate to large tau absolute values the last area showing decreasing trend series was located in the eastern portion of the mediterranean sea this area comprises few series and mainly associated with low intensity trend turning to the increasing trend series they are less abundant 12 4 vs 19 89 and more scattered than the decreasing ones the increasing trends covers a large portion of the coastal areas of the whole basin the areas in which the increasing trend series are more abundant are the central portion of the adriatic sea the north eastern ionian sea the eastern aegean sea and the area south of the cyprus island from the map the strength of the increasing trend series seems to be mainly associated with the distance from the coastline accordingly the series closer to the coast seem to show larger values of the mann kendall tau 3 2 chlorophyll a time series clustering the transition from a general dataset analysis to a more detailed one was hindered by the large amount of collected data we decided to perform a clustering procedure to further analyse the chlorophyll a series and mine more information from the satellite data the mediterranean dataset built during this work comprised a large amount of spatially related time series for which we had neither class labels nor ground truth for this reason we opted for the som algorithm to perform the clustering the possibility to represent the space of the data in a bidimensional map is one of the main strengths of this unsupervised ml technique this map is a valuable tool for displaying differences and similarities among the time series since we were dealing with time dependent observations we also decided to implement a windowed dtw as distance metric in our dtw som fortran code see section 2 3 this elastic distance allowed to account for phase shifting in finding the bmu during the som training we tested architectures from 12 to 60 nodes each architecture was trained 10 times for both a window parameter equal to 1 and 2 we also decided not to scale the mediterranean chlorophyll a series since we wanted to keep the magnitude signal of the time series these relatively small maps compressed over 37 000 series into few codebooks which can be interpreted as time series typologies this compression diminished the information related to the single time series but enabled the extraction of general patterns that otherwise would be difficult to highlight and analyse bearing in mind this purpose we defined two specific criteria to select the optimal dtw som among the trained ones the first criterion was the agreement between the chlorophyll a time series monotonic trend and the trend of their bmu codebook we computed an accuracy measure by summing the agreements of decreasing increasing and no significant trend over the total number of series 37270 the larger the accuracy value the larger the agreement between the dtw som clustering and the satellite data not surprisingly the map dimension was positively related with the accuracy level on the other hand we reduced the number of nodes to find a solution that could be visually interpreted from an ecological point of view for this reason we used the proportion of codebooks showing a significant trend to determine whether or not the augmented complexity in the map structure was justified by an enhanced representation of the dataset characteristics considering these criteria we chose the clustering solution with 28 nodes 4 7 and window 2 as the best candidate to analyse from a spatio temporal and ecological perspectives this map showed the best compromise between accuracy and proportion of significant nodes fig 5 we found that for our dataset the window parameter equal to 2 performed slightly better than window 1 and both the windowed dtw soms solutions outperformed the euclidean som implementation the dtw soms showed larger accuracy and significant codes proportion level even with smaller maps it is important to note that larger maps showed higher accuracy with both the euclidean and the dtw soms nevertheless this gain in accuracy came at the expenses of a clear interpretation and visual representation thus we did not consider them 3 3 analysis of the chlorophyll a time series dynamics once we found the optimal dtw som to represent the evolution of the chlorophyll a dynamics in the mediterranean sea we plotted the bidimensional codebooks map and analysed its characteristics fig 6 we did not color the units that showed no significant trend while the increasing and decreasing ones were filled with red and blue respectively the number of som codebooks showing a significant increasing trend was greater than the decreasing one even though the decreasing trend series outnumbered the increasing ones this feature depended on the larger level of variability associated with the increasing trend chlorophyll a time series in fact the red units show different magnitude levels which ranges from very high to low ones this characteristic can be highlighted from the bottom left corner towards the right side of the map on the other hand the decreasing trend units where mainly associated to low magnitude chlorophyll a series the blue node showing larger values shared the portion of the map dominated by the increasing trend codebooks thus emphasizing the role of the magnitude signal in the som training accordingly an overall trend of decreasing codebooks values can be highlighted from the bottom left corner to the upper right one not surprisingly the bulk of the map units showed no significant trend this was consistent with the overall dataset characteristics see section 3 1 the white units were the most heterogeneous ones and comprised time series typologies with extremely different features in fig 7 we can see how the dtw som with 28 nodes captured the dynamics of the mediterranean chlorophyll a regardless the inevitable loss of information with respect to the real series we can see that the main signal has been captured fig 4 vs fig 7 with the exception of the scattered decreasing trend in the western portion of the basin and the increasing trend in both the southern tyrrhenian sea and south of cyprus island it should be noted that the majority of the lost information was characterized by a low mann kendall tau level thus representing a weaker trend signal the greatest value of this simplification was the possibility to extract temporal and spatial patterns from over 37 000 time series the information enhancement provided by the data imputation enabled in this phase a greater level of details with respect the incomplete satellite time series to further investigate the emerging patterns in the mediterranean chlorophyll a time series we mapped the codebooks showing a significant trend assigning to each of them a different color fig 8 the map showed in fig 8 helped us analysing the relationships between the nodes and the tau map respectively presented in figs 6 and 7 from a geographical perspective the map highlighted that the chlorophyll a magnitude signal was larger near the coastline and in the gulf areas in fact the codebooks associated with these areas were mainly the dark red red orange and yellow ones which comprised the series with the largest values particularly evident examples of this feature are represented by the gulf of gabes in the tunisian coastal zone the nile delta and the adriatic sea in which the magnitude signal determine a clear decreasing gradient from the coast towards the open sea the indian red increasing codebook third node of the second row starting from bottom showed medium chlorophyll a values and was mainly associated with open sea series the bulk of this series were located in the adriatic and ionian sea the other areas showing characterized by this series typology were in the balearic sea in the tyrrhenian sea off the coast of tunisia in front of the gulf of gabes and in a small portion of the eastern aegean sea the fuchsia and the purple nodes fourth and eleventh elements of the map starting from the bottom left corner represented the majority of the increasing trend series in the aegean sea these codebooks showed medium low chlorophyll a values and were mainly associated with open sea areas the last two increasing trend codebooks were the dark orchid and the pink ones these were the increasing trend series with the lowest magnitude signal and characterized three main areas the first one was located in the ionian sea and transitioned into the aegean one the second one was the libyan coastline the last one comprised the coast of several states of the eastern mediterranean i e turkey syria lebanon and israel even though these time series typologies showed low chlorophyll a values despite being close to the coastline it was consistent from a theoretical perspective as already stated in section 3 1 the eastern portion of the mediterranean basin is characterized by lower values with respect to the western areas accordingly these series were among the ones with the largest magnitude signal in this spatial context comparing the information provided by figs 7 and 8 emerged that the chlorophyll a series characterized by large values presented also a large mann kendall tau accordingly the larger the magnitude signal the lager the trend strength the decreasing trend codebooks referred to three main areas the first two areas were represented by the cyan and dark cyan nodes i e 14th and 20th nodes starting from the bottom left corner these series occupied a large portion of the central south mediterranean basin plus a portion of the south eastern areas they presented low chlorophyll a values and respected the overall trend of decreasing magnitude signal from west to east the third area was associated with the light cyan node the 3rd one from the bottom left corner and was the only decreasing trend codebook with medium chlorophyll a values this node was close to the increasing trend ones due to similar magnitude and geographical characteristics in fact the series associated with this node were located in the northern enclosed portion of the aegean sea and comprised also coastal areas the analysis carried out in this work suggests that the chlorophyll a dynamics in the mediterranean basin are changing furthermore this change is influenced by both the geographic area and the chlorophyll a concentration magnitude it is worth stressing that the satellite data provide information only about the surface layers of the water column thus our analysis regards the surface chlorophyll a dynamics on the plus side the remotely sensed data utilized in this paper are freely available on a global scale thus providing a precious source of information for several oceanographical studies one of the main cons of the proposed workflow is the computational time of the tested approaches testing several techniques for the missing data imputation different metrics for the clustering algorithm and multiple map sizes for all of them could be time consuming on the other hand all the performed tests allowed to highlight the power of the som algorithm in reconstructing missing data when univariate and spatially related time series come to play furthermore we showed that the dtw som outperformed a classical implementation using the euclidean as distance metric these enhanced performances derived from the ability to find similarities from a non linear coupling of the time series during the dtw som training finally we exploited the clustering abilities of the latter technique to assign different geographical areas to a small number of time series typologies this procedure allowed to extract analyse and visually represent the spatial and temporal patterns of the mediterranean chlorophyll a dynamics in the last two decades 4 conclusions nowadays remote sensing platforms provide medium to long period information on several crucial aspects of natural systems in this work we exploited the data gathered from the seawifs and the modis aqua missions to perform an analysis of the chlorophyll a dynamics in the mediterranean basin satellite data are often affected by missing values thus being able to fill these gaps is important to fully harness their potentiality we compared several imputation approaches to perform the latter task we found that the som algorithm was the best suited one to reconstruct multiple univariate and spatially related time series this reconstruction provided a general picture of the mediterranean basin dynamics in the last two decades in order to further mine valuable information from the satellite chlorophyll a time series we performed a clustering procedure this task was carried out using a som that implemented the dtw elastic distance measure this approach outperformed the non elastic som implementation the dtw som clustering allowed to identify time series typologies and extract both temporal and spatial patterns from them the workflow approach proposed in this work aimed at investigating big dataset of remotely sensed time series which were structured in time and space in our opinion the assessment of natural system dynamics should be a key topic in modern ecology especially in a climate change context and that machine learning approaches could be an asset to fit this scope software availability section the dynamic time warping self organizing map algorithm used in this work is freely available under a gnu general public license version 3 0 at the fist author s github repository https github com francescomattei dtw som the software was written in fortran 90 and compiled with the gfortran compiler a guide on how to use the fortran program is available at the same github repository declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25629,understanding the evolution of natural systems spatio temporal dynamics is paramount in modern ecology we focused on highlighting and analysing temporal and spatial dynamics of remotely sensed chlorophyll a concentration this pigment is linked with phytoplankton production which in turn play a pivotal role in marine environment satellite platforms offer a synoptic view of surface chlorophyll a concentration for the last two decades coupling this source of information with statistical and machine learning techniques could help highlighting eventual patterns we merged the mediterranean chlorophyll a satellite data for the last two decades into a single dataset we tested several techniques for reconstructing missing data and performed a general analysis finally we implemented a dynamic time warping self organizing map algorithm to cluster our series showing that an elastic distance measure outperforms a non elastic one the proposed satellite data management and analysis provided insights on spatio temporal chlorophyll a dynamics in the mediterranean basin keywords chlorophyll a remote sensing time series analysis data imputation dynamic time warping self organizing maps 1 introduction understanding the dynamics of natural systems is one of the main challenges in environmental science several studies suggest that we live in a new epoch named anthropocene in which anthropic pressures contribute to shape ecosystems evolution lewis and maslin 2015 adding the climate change scenario to the equation it becomes evident that the spatio temporal analysis of natural processes is a crucial topic in modern ecology barange et al 2014 grimm et al 2013 approximately 70 of the earth s surface is covered by the oceans which deeply influence the climate of our planet interacting with the atmospheric and terrestrial ecosystems chlorophyll a concentration is one of the most exploited environmental variables associated with the marine ecosystems d ortenzio and ribera d alcalà 2009 lavigne et al 2012 nieto and mélin 2017 sammartino et al 2018 this variable is intimately linked with the biomass and the physiological state of phytoplankton therefore it has been widely used in phytoplankton primary production studies behrenfeld and falkowski 1997 daniels et al 2013 lee et al 2015 phytoplankton production is a pivotal oceanographic process involved in the air sea interactions doney 1995 longhurst and glen harrison 1989 the structuring of marine food webs anderson et al 2018 kwak and park 2020 shurin jonathan et al 2006 and the sinking of organic matter sabine et al 2004 these processes related to the primary production deeply influence several aspects of the biogeosphere and the earth s climate blanchard et al 2012 hays et al 2005 moore et al 2018 trolle et al 2014 in this framework we believe that studying the chlorophyll a spatio temporal dynamics is crucial to enhance our understanding of marine ecosystems to pursue this objective we exploited the invaluable source of information provided by satellite information in particular the seawifs and the modis aqua satellites provided a synoptic view of the ocean color in the past decades the products derived from the activity of these two satellites are freely available at the nasa ocean color website https oceancolor gsfc nasa gov the longest record of chlorophyll a remotely sensed data is represented by the 9 km mean monthly grids these data range from 1998 to present thus providing roughly 23 years of records at the time of this writing it should be noted that the remotely sensed chlorophyll a for each grid pixel repeated over time can be seen as a time series exploiting this valuable source of information we decided to perform a spatio temporal analysis of these remotely sensed time series in order to deal with large amount of data the ecologist community is increasingly employing machine learning ml techniques catucci and scardi 2020 chen et al 2020 franceschini et al 2019 mattei et al 2021 mattei and scardi 2020 park et al 2014 peters et al 2014 young et al 2011 this family of approaches allow to perform several tasks without an explicit formulation of the relationships between the variables in play moreover several machine learning approaches are robust with respect to noisy data these features are particularly useful in dealing with natural system and the process that occurs in it which are dominated by complex non linear dynamics lek and guégan 1999 olden et al 2008 peters et al 2014 recknagel 2001 we aimed at developing an approach that would provide insights into the studied system evolving dynamics in a climate change context this work comprised three main phases firstly we gathered the 9 km seawifs and modis aqua monthly mean chlorophyll a data huot et al 2007 then we merged all the grids in single dataset in which each row represented a scanned pixel in the mediterranean basin while the 276 columns corresponded to the monthly surface chlorophyll a concentration from 1998 to 2020 in order to develop and test our methodology we limited the study area to the mediterranean basin limiting the approach development area allowed to carry out several tests avoiding the limitations that would occur from time consuming computation in fact even the mediterranean subset comprised over 39000 time series afterward we excluded from our dataset all the series that showed a missing data percentage larger than 25 of the series length 69 out of 276 values for the remaining series showing missing values we tested several approaches for missing data imputation of multiple univariate and spatially related time series finally we tested a dynamic time warping self organizing map dtw som d berndt and clifford 1994 kohonen 1982 2013 algorithm for clustering time series that were structured in time and space we analysed for each cluster whether a trend was present or not and the characteristics of the series in relation to both geographical area and possible trend we performed the same procedure using a som with the euclidean distance as distance metric to assess if the dtw som outperformed a standard implementation euclidean som 2 materials and methods 2 1 data gathering and pre processing the satellite data used in this work are freely available at the nasa ocean color website https oceancolor gsfc nasa gov we selected the chlorophyll a 9 km monthly mean data since they covered the largest time span among the remotely sensed products we exploited the data gathered from the seawifs and the modis aqua missions the former was active from september 1997 to december 2010 while the latter started on july 2003 and is ongoing we used the seawifs data from 1998 to 2003 and the modis aqua ones from 2004 to 2020 for a total of 23 years 276 months the 9 km chlorophyll a monthly mean values are stored in a 2160 4320 matrix including missing data and land points with a resolution of 0 082 0 082 latitude x longitude firstly we downloaded the 276 monthly files relative to the period 1998 2020 and we extracted from each grid the mediterranean pixels this procedure generated 276 grids with a dimension of 188 509 latitude x longitude the latitude of each grid ranged from 45 7917 to 30 2083 while the longitude ranged from 6 0416 to 36 2917 secondly we filtered out all the land pixels and the data that the satellite gathered from lakes finally we merged all the extracted data in a single dataset in which each row accounted for a mediterranean pixel and the columns corresponded to the months from 1998 to 2020 the only exception was the grid relative to november 1999 the monthly mean data relative to this month were not available to tackle this issue we downloaded from the same website all the available daily data for november 1999 and created a monthly mean averaging them in this configuration each row of the dataset represented a time series of chlorophyll a concentration for a given pixel of the mediterranean basin after the data collection and the pre processing stage see fig 1 the dataset comprised 39166 time series 39166 rows x 276 columns since the range of the series max min could vary by several order of magnitude we decided to apply a logarithmic transformation to our series before performing any further analysis this transformation minimized the effect of very large values which could have skewed the whole work 2 2 missing data imputation inspecting the mediterranean monthly chlorophyll a series we found that roughly 80 of the series 31363 showed no missing values while less than 5 1896 lacked more than 69 values over 276 25 of the series length we opted for discarding the latter share of data and reconstructing the rest of the series after this procedure our dataset comprised 37270 rows time series 5907 of which showed missing values to a maximum of 25 of the whole series the missing data resulted from the cloud coverage which can disrupt the interaction between the satellite sensors and the ocean surface this is a common issue of remote sensing platforms thus being able to reconstruct missing values from the available ones could enhance the value of the satellite information jouini et al 2013 we tested several techniques to impute the missing data in our series we created a test set 1565 time series to assess the quality of the imputation and compare the results of the different approaches and a training set 35705 time series for the techniques that required one the test subset was extracted only among the series that did not show any missing value to ensure that the test set encompassed the overall variability of our series we arbitrarily divided the complete series in five clusters using the k means algorithm the k means algorithm has been implemented using the python package scikit learn pedregosa et al 2011 the algorithm initialization was set to k means arthur and vassilvitskii 2007 which used an optimized centroid initialization the maximum number of algorithm iterations was set to 1000 and the procedure was repeated 50 times the best solution in terms of stress was selected as the optimal one afterward we randomly sampled 5 of the patterns from each cluster to create the test set finally we created artificial missing data by randomly deleting 25 of each test set series we used the r package imputets moritz and bartz beielstein 2017 to test approaches such as linear interpolation spline interpolation stineman interpolation and kalman smoothing all these techniques have been performed with and without seasonal decomposition and seasonal split we also wrote an algorithm to test a monthly reconstruction in which we imputed a missing value averaging all the available values for the same month along the series for example if the january 2000 monthly mean value was missing we took all the january values available in the same series from 1998 to 2020 we computed their average and use it to impute the missing value the rationale behind this approach relied on the nature of the series which were composed by monthly mean values finally we applied more complex approaches belonging to the ml domain such soms and recurrent neural networks rnns in particular long short term memory lstm hochreiter and schmidhuber 1997 and gated recurrent unit gru chung et al 2014 the som algorithm kohonen 1982 2013 was implemented in a fortran program written by the authors and available on the github profile of the first author see software availability section we opted for the fortran code due to the possibility to customize the algorithm see section 2 3 and 3 2 and the fast computational performances which are an advantage especially in dealing with large dataset the som algorithm is a powerful tool it can strongly reduce the data complexity while preserving their topology céréghino and park 2009 kohonen 2013 nkiaka et al 2016 park et al 2006 we utilized the training set to train several soms with different architectures from 96 to 1400 nodes as rule of thumb the optimal number of nodes should be 5 x n s a m p l e s so we did not largely exceed this number in our tests vesanto and alhoniemi 2000 for the imputation phase we used the euclidean distance as distance measure and a hexagonal non toroidal topology the weights were updated online during 1500 epochs we chose a gaussian neighbourhood function the initial radius value was set to 2 3 of all unit to unit distance and decreased linearly to 0 during the epochs the initial learning rate value was set to 0 05 and it decreased linearly to 0 01 over the number of epochs the presence of incomplete series into the training set was not an issue since the som can easily deal with missing values in fact the best matching unit bmu searching procedure exploited only available data when a series showed missing values once completed the training procedures we mapped the test set series on the trained soms imputed the missing values using the som codebooks the lstm and gru approaches were implemented exploiting the tensorflow package abadi et al 2016 using the python programming language using these rnn approaches the model tries to learn a function that using past observations can predict an output connor et al 1994 saad et al 1998 we tested solutions that used from 3 to 12 previous values to predict the next one in the series we also tested single and double hidden layer architectures for the first hidden layer we tried solutions from 50 to 500 nodes and from 25 to 250 for the second one the selected activation function and the optimizer were relu and adam kingma and ba 2017 respectively this approach was tested after a first imputation step performed with the monthly reconstruction which returned the best outcome among the simple imputation techniques subsequently we scaled the data into 0 1 range and trained the rnns to predict the test set artificial missing values it is important to note that these ml techniques do not require any formal assumption neither on the mutual independence nor on the distribution of the variables they are also robust to noisy data which are the norm handling environmental variables lek and guégan 1999 olden et al 2008 recknagel 2001 these features can also be an asset in dealing with time series we assessed the imputation accuracy of these techniques by comparing their estimates to the artificially removed values of the training set we used the mean squared error mse and the mean absolute error mae as metrics to evaluate the imputation quality the optimal solution for our case study was the som with 1200 nodes see table 1 and fig 2 section 3 1 the number of nodes was selected on the basis of mae mse and mse trend with respect to the number of nodes once completed the data imputation phase the dataset comprised 37270 complete monthly mean chlorophyll a concentration time series we used the newly available information to compute the average chlorophyll a concentration in the mediterranean basin from 1998 to 2020 see fig 3 section 3 1 we also checked whether or not the time series showed a monotonic trend using the seasonal version of the mann kendal test with α 0 05 kendall 1975 mann 1945 see fig 4 section 3 1 2 3 univariate and spatially related time series clustering the need for a clustering procedure arose from the numerosity of the series in our dataset in fact it was not possible to investigate the dynamics of all the mediterranean pixels thus we decided analyse time series typologies we opted for the som algorithm since reducing the data complexity is one of its strengths céréghino and park 2009 kohonen 2013 two main differences distinguished the application of this versatile algorithm for the data imputation task from the clustering one the first one concerned the dimension of the map for the imputation task the best results were achieved through large maps over 1000 nodes which provided a detailed representation of the data space on the other hand a clustering procedure aimed at extracting patterns from a complex scenario required a smaller number of nodes in this way we could compress over 37 000 records into fewer time series typologies the second difference involved the distance metric used to find the bmu during the som training the clustering procedure could benefit by an elastic distance measure since the records in our dataset are time dependant bagnall et al 2017 accordingly we integrated in our program the dtw as distance metric d j berndt and clifford 1994 the dtw approach consists in computing a cost matrix pairing all the series values and finding the shortest path from the start to the end of the series this metric allows to calculate the optimal matching between two sequences considering non linear similarities we also integrated the possibility to define a window parameter which constraint the search of the shortest path to a given window around the matrix diagonal sakoe and chiba 1978 the importance of this parameter did not only concern the computational time of the algorithm even though we wanted to consider a non linear pairing between our series we had to take into account the ecological characteristics of our data in fact the monthly mean data differences reflected the peculiarities in the seasonal behaviour of the mediterranean chlorophyll a time series for this reason we decided to test only small window values 1 and 2 to be consistent from an ecological perspective larger window values could have found similarities between series in which the time shift was caused by relevant seasonal differences that we wanted to preserve in our analysis we tested several soms architecture to find the optimal one for the clustering task we trained all the possible combination from 3 to 6 rows and from 4 to 10 columns avoiding squared maps i e 3 4 3 10 4 5 4 10 6 10 we also tried solution ranging from 4 to 10 rows and 3 to 6 columns 4 3 10 3 5 4 10 4 10 6 but we did not find any significant difference between the two approaches since several stochastic processes occur during the som training som weights initialization and pattern extraction during the training epochs we repeated each training 10 times the training modalities were the same described in section 2 2 we did not scale the chlorophyll a time series because we wanted to keep the magnitude signal into the dtw som training we joined two different criteria to evaluate the quality of the clustering procedure considering that we aimed at highlighting the mediterranean basin chlorophyll a dynamics the first criterion took into account the trend of the time series as previously done for the mediterranean pixels we tested the significance of a monotonic trend for each som codebook using the seasonal version of the mann kendall test α 0 05 subsequently we compared the trend significance and direction of each pixel series with the ones of their bmu the trend agreement between the mediterranean chlorophyll a series and their bmu was used as accuracy measure for the som clustering this measure assessed how well the time series typologies derived from the clustering represented the series assigned to them the second aspect that we considered to evaluate the clustering was the proportion of som codebook that showed a significant trend this proportion indicated whether or not an increase in the nodes number was followed by a more detailed dataset features representation following these criteria we selected the optimal clustering solution subsequently we analysed the characteristics of the som codebooks with respect to their trend and the features of the series assigned to them as conclusive analysis we mapped the chlorophyll a series associated with codebooks that showed a significant trend to analyse the clustering results from a geographical perspective 3 results and discussion 3 1 missing data imputation the gathered satellite time series presented missing values caused by cloud coverage this is one of the main issues in dealing with remotely sensed information to tackle this loss of information we tested several imputation techniques the imputets r package offered a user friendly implementation of the most common missing data reconstruction approaches we exploited this software for the linear interpolation the spline interpolation the stineman interpolation the seasonally split and seasonally decomposed kalman smoothing these are among the most direct and general techniques to implement moreover they are not time consuming since their computational time is negligible on the other hand these approaches are rarely the most accurate ones we found that the kalman smoothing solutions which took into account the seasonal aspects of our data offered the best reconstruction among the abovementioned techniques table 1 bearing in mind the ecological and temporal features of our series we decided to test a monthly reconstruction this approach exploits all the available information about the missing month and uses the average as imputation this dataset tailored technique showed good results outperforming the tested general solutions proposed by a standard package m s e 0 0089 m a e 0 0650 using the monthly reconstructed series we also tested the lstm and gru approaches which belong to the rnns family we trained several architectures with one and two hidden layers and found that in our case the single hidden ones showed better performances testing the numbers of previous values needed to predict the next one in the series emerged that 12 was the optimal one this is probably related to the monthly nature of our data and is consistent with the better results obtained with the seasonally decomposed and seasonally split kalman smoothing the best results using the lstm and gru were achieved with 200 and 300 hidden nodes respectively the two approaches did not show any significant differences in terms of imputation performances but both of them improved the accuracy level achieved by the monthly reconstruction table 1 the tested technique that returned the best results for missing data imputation was the som algorithm we tested several architectures and selected the one with 1200 nodes on the basis of mse and mae m s e 0 0030 m a e 0 0327 and the mse trend with respect the number of nodes the mse showed a decreasing trend for different percentages of missing values as the number of nodes increased fig 2 the error trend started to increase in maps with a number of nodes larger than 1200 for this reason we selected the solution associated with the mse minimum as the optimal one for this task the som was trained using over 35 000 time series thus having access to a large pool of information moreover reconstructing the time series missing data using the som algorithm mimed a k nearest neighbors approach with the advantage of a variable k instead of a fixed one in fact the larger the data density of a som area the larger the value of k neighbors from which the codebook is derived this approach was able to exploit spatial and temporal similarities among the series in order to impute the missing data after the missing data imputation phase the mediterranean dataset comprised 37270 complete monthly mean chlorophyll a time series 37270 rows and 276 columns thanks to this procedure we were able to reconstruct roughly 6000 series thus enhancing the amount of information provided by the remotely sensed data this enhancement allowed to construct a more detailed picture of the mediterranean from space and also added more detail to the clustering procedure see section 3 2 exploiting the new available information we computed the average chlorophyll a concentration in the mediterranean basin from 1998 to 2020 fig 3 fig 3 clearly shows that the western portion of the basin is characterized by larger chlorophyll a values with respect to south central and eastern areas this feature is mainly caused by the water inputs from the atlantic ocean through straits of gibraltar the second general feature is represented by the large values associated with coastal areas the inputs from surrounding terrestrial ecosystems boost the nutrient availability in coastal waters thus enhancing phytoplankton productivity among these areas the ones showing larger values comprises the italian coasts of the north adriatic sea and the north egypt shores these are the areas in which are located the delta of the po and nile rivers respectively also gulf areas are characterized by considerable chlorophyll a values e g gulf of gabes these sea portions have relevant interaction with terrestrial environment proportionally to their area the missing data imputation allowed also to check the monotonic trend significance and direction using the mann kendall test we used the seasonal version of the test setting the period parameter to 12 since we were dealing with monthly observations the significance of the trend was ascertained setting α 0 05 and comparing it with the p value while the direction was determined by the tau parameter the larger the absolute value of τ the stronger is the trend intensity while the sign of this parameter indicates the direction of the trend the mann kendall test highlighted that 25237 series 67 71 showed no significant monotonic trend while 7411 19 89 and 4622 12 4 ones presented a significant decreasing and increasing monotonic trend respectively fig 4 the mann kendall τ map fig 4 presents an overall picture of the mediterranean chlorophyll a temporal dynamics in the last two decades we can highlight 4 areas associated with decreasing trend the first one is located in western portion of the basin is largely fragmentated and shows low tau absolute values the second one covers a large portion of the central southern mediterranean sea the trend strength of these series ranges from low to high the third decreasing trend area occupies the northern portion of the aegean sea and presents moderate to large tau absolute values the last area showing decreasing trend series was located in the eastern portion of the mediterranean sea this area comprises few series and mainly associated with low intensity trend turning to the increasing trend series they are less abundant 12 4 vs 19 89 and more scattered than the decreasing ones the increasing trends covers a large portion of the coastal areas of the whole basin the areas in which the increasing trend series are more abundant are the central portion of the adriatic sea the north eastern ionian sea the eastern aegean sea and the area south of the cyprus island from the map the strength of the increasing trend series seems to be mainly associated with the distance from the coastline accordingly the series closer to the coast seem to show larger values of the mann kendall tau 3 2 chlorophyll a time series clustering the transition from a general dataset analysis to a more detailed one was hindered by the large amount of collected data we decided to perform a clustering procedure to further analyse the chlorophyll a series and mine more information from the satellite data the mediterranean dataset built during this work comprised a large amount of spatially related time series for which we had neither class labels nor ground truth for this reason we opted for the som algorithm to perform the clustering the possibility to represent the space of the data in a bidimensional map is one of the main strengths of this unsupervised ml technique this map is a valuable tool for displaying differences and similarities among the time series since we were dealing with time dependent observations we also decided to implement a windowed dtw as distance metric in our dtw som fortran code see section 2 3 this elastic distance allowed to account for phase shifting in finding the bmu during the som training we tested architectures from 12 to 60 nodes each architecture was trained 10 times for both a window parameter equal to 1 and 2 we also decided not to scale the mediterranean chlorophyll a series since we wanted to keep the magnitude signal of the time series these relatively small maps compressed over 37 000 series into few codebooks which can be interpreted as time series typologies this compression diminished the information related to the single time series but enabled the extraction of general patterns that otherwise would be difficult to highlight and analyse bearing in mind this purpose we defined two specific criteria to select the optimal dtw som among the trained ones the first criterion was the agreement between the chlorophyll a time series monotonic trend and the trend of their bmu codebook we computed an accuracy measure by summing the agreements of decreasing increasing and no significant trend over the total number of series 37270 the larger the accuracy value the larger the agreement between the dtw som clustering and the satellite data not surprisingly the map dimension was positively related with the accuracy level on the other hand we reduced the number of nodes to find a solution that could be visually interpreted from an ecological point of view for this reason we used the proportion of codebooks showing a significant trend to determine whether or not the augmented complexity in the map structure was justified by an enhanced representation of the dataset characteristics considering these criteria we chose the clustering solution with 28 nodes 4 7 and window 2 as the best candidate to analyse from a spatio temporal and ecological perspectives this map showed the best compromise between accuracy and proportion of significant nodes fig 5 we found that for our dataset the window parameter equal to 2 performed slightly better than window 1 and both the windowed dtw soms solutions outperformed the euclidean som implementation the dtw soms showed larger accuracy and significant codes proportion level even with smaller maps it is important to note that larger maps showed higher accuracy with both the euclidean and the dtw soms nevertheless this gain in accuracy came at the expenses of a clear interpretation and visual representation thus we did not consider them 3 3 analysis of the chlorophyll a time series dynamics once we found the optimal dtw som to represent the evolution of the chlorophyll a dynamics in the mediterranean sea we plotted the bidimensional codebooks map and analysed its characteristics fig 6 we did not color the units that showed no significant trend while the increasing and decreasing ones were filled with red and blue respectively the number of som codebooks showing a significant increasing trend was greater than the decreasing one even though the decreasing trend series outnumbered the increasing ones this feature depended on the larger level of variability associated with the increasing trend chlorophyll a time series in fact the red units show different magnitude levels which ranges from very high to low ones this characteristic can be highlighted from the bottom left corner towards the right side of the map on the other hand the decreasing trend units where mainly associated to low magnitude chlorophyll a series the blue node showing larger values shared the portion of the map dominated by the increasing trend codebooks thus emphasizing the role of the magnitude signal in the som training accordingly an overall trend of decreasing codebooks values can be highlighted from the bottom left corner to the upper right one not surprisingly the bulk of the map units showed no significant trend this was consistent with the overall dataset characteristics see section 3 1 the white units were the most heterogeneous ones and comprised time series typologies with extremely different features in fig 7 we can see how the dtw som with 28 nodes captured the dynamics of the mediterranean chlorophyll a regardless the inevitable loss of information with respect to the real series we can see that the main signal has been captured fig 4 vs fig 7 with the exception of the scattered decreasing trend in the western portion of the basin and the increasing trend in both the southern tyrrhenian sea and south of cyprus island it should be noted that the majority of the lost information was characterized by a low mann kendall tau level thus representing a weaker trend signal the greatest value of this simplification was the possibility to extract temporal and spatial patterns from over 37 000 time series the information enhancement provided by the data imputation enabled in this phase a greater level of details with respect the incomplete satellite time series to further investigate the emerging patterns in the mediterranean chlorophyll a time series we mapped the codebooks showing a significant trend assigning to each of them a different color fig 8 the map showed in fig 8 helped us analysing the relationships between the nodes and the tau map respectively presented in figs 6 and 7 from a geographical perspective the map highlighted that the chlorophyll a magnitude signal was larger near the coastline and in the gulf areas in fact the codebooks associated with these areas were mainly the dark red red orange and yellow ones which comprised the series with the largest values particularly evident examples of this feature are represented by the gulf of gabes in the tunisian coastal zone the nile delta and the adriatic sea in which the magnitude signal determine a clear decreasing gradient from the coast towards the open sea the indian red increasing codebook third node of the second row starting from bottom showed medium chlorophyll a values and was mainly associated with open sea series the bulk of this series were located in the adriatic and ionian sea the other areas showing characterized by this series typology were in the balearic sea in the tyrrhenian sea off the coast of tunisia in front of the gulf of gabes and in a small portion of the eastern aegean sea the fuchsia and the purple nodes fourth and eleventh elements of the map starting from the bottom left corner represented the majority of the increasing trend series in the aegean sea these codebooks showed medium low chlorophyll a values and were mainly associated with open sea areas the last two increasing trend codebooks were the dark orchid and the pink ones these were the increasing trend series with the lowest magnitude signal and characterized three main areas the first one was located in the ionian sea and transitioned into the aegean one the second one was the libyan coastline the last one comprised the coast of several states of the eastern mediterranean i e turkey syria lebanon and israel even though these time series typologies showed low chlorophyll a values despite being close to the coastline it was consistent from a theoretical perspective as already stated in section 3 1 the eastern portion of the mediterranean basin is characterized by lower values with respect to the western areas accordingly these series were among the ones with the largest magnitude signal in this spatial context comparing the information provided by figs 7 and 8 emerged that the chlorophyll a series characterized by large values presented also a large mann kendall tau accordingly the larger the magnitude signal the lager the trend strength the decreasing trend codebooks referred to three main areas the first two areas were represented by the cyan and dark cyan nodes i e 14th and 20th nodes starting from the bottom left corner these series occupied a large portion of the central south mediterranean basin plus a portion of the south eastern areas they presented low chlorophyll a values and respected the overall trend of decreasing magnitude signal from west to east the third area was associated with the light cyan node the 3rd one from the bottom left corner and was the only decreasing trend codebook with medium chlorophyll a values this node was close to the increasing trend ones due to similar magnitude and geographical characteristics in fact the series associated with this node were located in the northern enclosed portion of the aegean sea and comprised also coastal areas the analysis carried out in this work suggests that the chlorophyll a dynamics in the mediterranean basin are changing furthermore this change is influenced by both the geographic area and the chlorophyll a concentration magnitude it is worth stressing that the satellite data provide information only about the surface layers of the water column thus our analysis regards the surface chlorophyll a dynamics on the plus side the remotely sensed data utilized in this paper are freely available on a global scale thus providing a precious source of information for several oceanographical studies one of the main cons of the proposed workflow is the computational time of the tested approaches testing several techniques for the missing data imputation different metrics for the clustering algorithm and multiple map sizes for all of them could be time consuming on the other hand all the performed tests allowed to highlight the power of the som algorithm in reconstructing missing data when univariate and spatially related time series come to play furthermore we showed that the dtw som outperformed a classical implementation using the euclidean as distance metric these enhanced performances derived from the ability to find similarities from a non linear coupling of the time series during the dtw som training finally we exploited the clustering abilities of the latter technique to assign different geographical areas to a small number of time series typologies this procedure allowed to extract analyse and visually represent the spatial and temporal patterns of the mediterranean chlorophyll a dynamics in the last two decades 4 conclusions nowadays remote sensing platforms provide medium to long period information on several crucial aspects of natural systems in this work we exploited the data gathered from the seawifs and the modis aqua missions to perform an analysis of the chlorophyll a dynamics in the mediterranean basin satellite data are often affected by missing values thus being able to fill these gaps is important to fully harness their potentiality we compared several imputation approaches to perform the latter task we found that the som algorithm was the best suited one to reconstruct multiple univariate and spatially related time series this reconstruction provided a general picture of the mediterranean basin dynamics in the last two decades in order to further mine valuable information from the satellite chlorophyll a time series we performed a clustering procedure this task was carried out using a som that implemented the dtw elastic distance measure this approach outperformed the non elastic som implementation the dtw som clustering allowed to identify time series typologies and extract both temporal and spatial patterns from them the workflow approach proposed in this work aimed at investigating big dataset of remotely sensed time series which were structured in time and space in our opinion the assessment of natural system dynamics should be a key topic in modern ecology especially in a climate change context and that machine learning approaches could be an asset to fit this scope software availability section the dynamic time warping self organizing map algorithm used in this work is freely available under a gnu general public license version 3 0 at the fist author s github repository https github com francescomattei dtw som the software was written in fortran 90 and compiled with the gfortran compiler a guide on how to use the fortran program is available at the same github repository declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
