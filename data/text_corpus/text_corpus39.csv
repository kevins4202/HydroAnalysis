index,text
195,rainfall is arguably the most important yet most variable input for rainfall runoff hydrologic models in this study the authors search for the characteristics of radar rainfall estimates that are most important for skillful streamflow predictions they perform comprehensive hydrologic investigations of radar rainfall characteristics including spatiotemporal resolution radar range visibility statistical characterization of rainfall variability all vis a vis basin characteristics such as size and river network topology since the true rainfall fields are unknown the authors exploit a paradigm of using two independently constructed radar rainfall products i e multi radar multi sensor and ifc zr used operationally by the iowa flood center ifc using the distributed hydrologic model called the hillslope link model for the domain of the state of iowa they evaluate streamflow prediction at 140 usgs gauge stations that monitor rivers in iowa through spatial and temporal rainfall aggregation experiments the authors show that the impact of spatial and temporal resolution of rainfall is significant typically for smaller basins while starts reducing significantly for basins larger than 1 000 km2 other rainfall characteristics they explored do not reveal a strong signature in the relationship of rainfall differences between the two products and hydrograph errors however exploring the product similarities rather than differences reveals that the basin wide rainfall volume has the most significant effect on streamflow prediction the results from this study are generalizable for all rainfall observing systems keywords radar rainfall error resolution width function basin wide rainfall volume error mrms ifc 1 introduction despite numerous ongoing efforts towards characterizing uncertainties of rainfall runoff hydrologic model predictions modelers continue to struggle to apportion the sources of error explicitly into those due to the rainfall input and those due to the model structure e g andréassian et al 2001 liu and gupta 2007 renard et al 2010 there is a consensus in the hydrologic community that this remains a major scientific and operational challenge renard et al 2010 point out four sources of uncertainty in hydrologic rainfall runoff models a input uncertainty such as in rainfall estimates b model structural uncertainty c output uncertainty such as errors in rating curves that impact flow estimates and d parametric uncertainty related to model parameters and uncertainties in calibration data in this study we focus on the errors in radar rainfall estimates model inputs and their propagation through a hydrologic model used for streamflow forecasting we consider the rating curves uncertainty negligible and as our model is not calibrated the model uncertainty includes both the structural and parametric errors the spatial and temporal variability of precipitation has been acknowledged to have substantial effects on basin response with the deployment of the next generation weather radars nexrad in the continental united states in the early 1990s klazura and imy 1993 the operational estimation of rainfall over large areas at fine spatial and temporal resolutions became possible consequently the research attention shifted towards investigating the effect of the spatiotemporal resolution of radar rainfall products on hydrologic simulations e g ogden and julien 1994 lobligeois et al 2014 sapriza azuri et al 2015 quintero et al 2016 however as many researchers e g krajewski and smith 2002 berne and krajewski 2013 aghakouchak et al 2010 krajewski et al 2010 pointed out there are several other sources of uncertainties in radar estimation of rainfall including calibration signal attenuation range anomalous propagation beam blockage bright bands random errors algorithms and variability in radar reflectivity and rainfall intensity z r relationships despite significant efforts a comprehensive characterization of uncertainties associated with radar rainfall estimations has not been achieved yet see villarini and krajewski 2010 for a review hence efforts will continue in this direction in the future e g seo et al 2015 2018 ciach and gebremichael 2020 given the complex structure and often significant level of radar rainfall uncertainty an important research challenge is to figure out its effect on hydrologic prediction and in this work specifically on streamflow forecasting here we present a comprehensive hydrologic investigation of several characteristics of radar rainfall products the key question we attempt to answer in this study is what are the characteristics of rainfall input that are most important for skillful streamflow predictions since the true areal rainfall is unknown it is difficult to decouple the effects of the rainfall input uncertainty from that of the model uncertainty on the model prediction to address this difficulty we organize our investigation using two independently constructed radar rainfall products i e 1 multi radar multi sensor mrms rainfall zhang et al 2016 2011 and 2 iowa flood center ifc zr radar rainfall which has roots in hydro nexrad seo et al 2011 krajewski et al 2011 seo et al 2015 we use the two products to drive the same highly distributed hydrologic rainfall runoff model called hillslope link model e g krajewski et al 2017 quintero et al 2016 2020 this paper is organized as follows section 2 reviews some key studies reported on radar rainfall error propagation to rainfall runoff model hydrographs in section 3 we discuss the experimental methods exploring relationships between rainfall characteristics and errors in hydrographs section 4 presents our main results and discusses the findings section 5 summarizes and presents the conclusions and limitations of this work 2 review of studies on rainfall error propagation to hydrographs there is a plethora of studies that explored the effect of rainfall on hydrologic response this study however is focused on the propagation of errors in radar rainfall products to the hydrologic response by identifying the characteristics that lead to the most skillful prediction of streamflow interesting results have been put forth by studies in the past to depict the relationship between the radar rainfall errors and catchment runoff characteristics across scales in an early study on the topic collier and knowles 1986 showed that the error in radar rainfall results in equal or smaller hydrograph errors in some basins while amplified hydrograph errors in other basins wyss et al 1990 suggested that the errors in radar rainfalls are less significant than the errors resulting from the transformation of rainfall to runoff moreover the use of spatially distributed rainfall is more important to dry catchments than wet catchments e g shah et al 1996 sharif et al 2004 ogden and julien 1994 studied the effect of the spatial resolution of radar rainfall using a distributed rainfall runoff model at the 32 km2 macks creek watershed in southwestern idaho and the 121 km2 taylor arroyo watershed in southeastern colorado they showed at the catchment outlet that the spatial resolution of rainfall data can significantly affect the peak discharge prediction and the severity of the effect is dependent on whether the watersheds are simulated with or without accounting for infiltration several studies e g lobligeois et al 2014 ochoa rodriguez et al 2015 nikolopoulos et al 2010 nikolopoulos et al 2011 quintero et al 2016 sapriza azuri et al 2015 showed that the impact of spatial and temporal variability of rainfall on hydrologic model results decreases with the increase in the catchment sizes for instance lobligeois et al 2014 evaluated 3620 flood events across 181 french catchments using radar rainfall inputs they showed that even coarser resolution radar rainfall and hydrologic models can accurately capture the hydrologic response of catchments under oceanic climatic conditions atlantic coast of france with relatively uniform precipitation fields for the catchments where precipitation fields are highly variable in space their results illustrated marked improvement in hydrologic performance using high resolution rainfall products a few studies e g nicótina et al 2008 zoccatelli et al 2015 have presented somewhat contrasting results for instance nicótina et al 2008 showed that the role of spatial rainfall variability on streamflow response is minimal for basins as large as 8000 km2 which they attribute to the dominance of subsurface flow on the total streamflow volume using high space time resolution radar rainfall for five extreme flash flood events across 27 basins ranging between 36 and 982 km2 and rainfall spatial organization index zoccatelli et al 2015 showed that the flood prediction errors modulated by the hillslope and channel residence times are scale independent relative to the basin scale clearly there is no consensus on the impact of spatial and temporal resolution of radar rainfall inputs on the hydrologic model outputs and performance segond et al 2007 nikolopoulos et al 2011 lobligeois et al 2014 pointed out some issues on this disparity most of these studies are constrained to very few catchments based on synthetic flows comparing simulation results against reference simulations and models are parameterized to a single resolution of precipitation while evaluating across several spatial and temporal resolutions of the hydrologic model also the hydrologic models in these studies are calibrated and the calibration process compensates for the errors in the input by adjusting the parameter values owing to these contrasting results several authors e g andréassian et al 2001 woods and sivapalan 1999 suggested the assessment of rainfall volume rather than its distribution while others recommended testing different hydrologic modeling configurations kouwen and garland 1989 pointed out that the appropriate level of the spatial resolution of radar rainfall on hydrologic models for flood forecasting is dependent on the scale of the area considered and maintained that it is still somewhat subjective pessoa et al 1993 further showed that the errors resulting from the spatial and temporal averaging of rainfall are within acceptable limits compared to the errors originating from radar reflectivity rainfall intensity z r relationships none of the studies cited above investigated the role of the river network geometry in aggregating the space time rainfall and its subsequent effect on the simulated hydrologic response smith et al 2005 explored the influence of spatial variability of rainfall on hydrologic response in an urban watershed of dead run in maryland from the river network perspective using a distance metric in the river network called the normalized flow distance they showed that the drainage network structures dampen the consequent catchment hydrologic response which otherwise is expected to be significantly affected when viewed from euclidean perspective sangati and borga 2009 nikolopoulos et al 2011 demonstrated for the mountainous 623 km2 fella river basin in northeastern italy that rainfall volume error during the spatial aggregation process is the main source of radar rainfall error in simulating flash flood response their results showed an increase in the peak discharge and runoff volume errors for the larger values of a dimensionless parameter i e the ratio of rainfall resolution lr to the characteristic length of the basin lw which is the case of smaller basin scales for a given spatial resolution of rainfall though range dependent radar rainfall errors have been mentioned in the literature e g borga 2002 cunha et al 2012 sharif et al 2004 few studies have explored the interaction between the range dependent radar errors in terms of river network width functions and hydrologic predictions seo et al 1999 extended their efforts in reducing the systematic errors in radar rainfall data used for operational hydrologic forecasting by the national weather service nws suggesting procedures to estimate real time mean field bias through the use of radar and gauge pairs from the observation network also see yoo et al 2014 which has significant implication on rainfall volume estimation since it is not practical to have a dense network of rain gauges everywhere working with multiple radar products of different spatial and temporal resolutions across multiple basin scales could be a potential direction in this effort 3 study area and data 3 1 study area to investigate different aspects of radar rainfall error propagation to runoff we used data from the entire state of iowa the study area see fig 1 considered here is in the context of the domain of the iowa flood center ifc streamflow forecasting model which comprises about 420 000 hillslope and link pairs over the state krajewski et al 2017 3 2 streamflow data for evaluation of the model output i e streamflow hydrograph we used observed streamflow time series for years 2016 and 2017 at 140 usgs stream gauge stations across iowa fig 1 the basins defined by these gauges show hydro geological variability across the state out of these basins about 65 drain to the mississippi river while 35 drain to the missouri river basin e g ghimire et al 2018 larimer 1957 these basins range from 7 37 000 km2 in size which enables us to characterize variability across spatial scales basins in the northeastern part of the state are characterized by the rugged steep terrain while the rest of the state is mostly flat to mild sloped agricultural watersheds 3 3 radar rainfall products as an overall strategy for our study we considered two radar based rainfall products for this evaluation mrms rainfall with 1 km2 by 1 h resolution and ifc zr rainfall with 0 35 km2 by 5 min resolution though both products are based on essentially the same level ii data collected by the seven wsr 88dp radars overlooking iowa see fig 1 they are developed independently mrms is gauge corrected rainfall while ifc zr is not on the other hand the ifc zr rainfall accumulation includes a correction for the effect of advection seo and krajewski 2015 and the mrms does not the two products also differ in the data quality control and rainfall estimation algorithms in the absence of true rainfall working with two radar rainfall products allows us to investigate the relationship between streamflow prediction errors and the radar product differences by doing so we hope to identify the key aspects of radar rainfall estimation important for skillful streamflow prediction 3 4 hydrologic modeling framework using hillslope link model we employed the distributed model at the iowa flood center ifc referred to as the hillslope link model hlm in which landscape is decomposed into hillslopes and links krajewski et al 2017 quintero et al 2016 the rainfall runoff conversion process occurs at hillslopes and generated flows are routed through corresponding links see fig 2 the average size of hillslopes is 0 4 km2 hlm is a non linear physically based distributed model with four storage components modeling each hillslope link channel storage q t m3 s pondage on the surface sp t m soil top layer storage st t m and subsurface storage ss t m the water transport model uses a non linear velocity formulation and aggregates water from two upstream links as appropriate the result is a system of non linear ordinary differential equations that are solved using a specialized numerical solver called asynch small et al 2013 refer to appendix a for more details on the non linear model structure of hlm we selected this model as it has origins in the scaling properties of the river network and thus readily allows scale based analyses mantilla and gupta 2005 ayalew et al 2015 this version of the model used for our study is relatively simple to implement yet credibly reproduces observed runoff hydrographs across scales e g quintero et al 2020 also the model is not calibrated to any basin or scale or rainfall input and thus in principle given a more accurate input should result in better streamflow prediction still there is no guarantee that this is the case as the model s structural uncertainty may interact with the input uncertainty in a complex way 3 5 river network topology and width function we adopted a 90m resolution digital elevation model dem from the national elevation dataset ned of usgs to obtain stream and river network topology for our study domain the network topology partitions the landscape into a system of hillslopes areas and drainage networks lengths refer to ayalew et al 2014 and mantilla and gupta 2005 for details thus the derived network topology is an integral component of the hlm modeling framework discussed above due to its role in shaping the observed hydrographs in the network the flow aggregation transport and hence hydrologic response are connected strongly to the spatial distribution of a drainage network characterized by its width function wf e g lee and delleur 1976 perez et al 2018 rodriguez iturbe and rinaldo 1997 the wf represents the distribution of the number of stream links at a flow distance x from the outlet of the basin assuming that every drop of water in the stream network travels to the outlet with the same velocity and that the flow is not attenuated the wf represents the distribution of flow travel time in the river network moussa 2008 mathematically in a discrete case wf is given by e g perez et al 2018 1 w f x i d 1 n i i d for 0 x x m a x where the index function i id is defined as 2 i i d 1 x δ x d i d x 0 o t h e r w i s e the unique identification tag of a stream link is id and each link has a distance to the basin outlet d id the wf is evaluated at a range of distances from 0 at the outlet to the maximum xmax the most distant headwater stream of the basin at δx interval as a demonstration we present in fig 3 the wf of the cedar river basin at conesville where the peak of wf x for this basin is about 350 km from the outlet and the base of wf x is about 500 km hydrologically the base of wf x represents the maximum travel time for the basin which is about 8 5 days for the cedar river basin if a constant channel velocity of 0 7 m s is assumed 4 methods the key strategy of our investigation is to use two different radar rainfall products with the same rainfall runoff model and explain the streamflow prediction errors in terms of radar rainfall difference characteristics streamflow errors are the differences between hydrologic model prediction and the corresponding discharge observations the rainfall runoff model is hlm the same version as that used operationally by the iowa flood center krajewski et al 2017 and evaluated in previous studies e g quintero et al 2020 to investigate how rainfall difference error propagates through the hydrologic model to hydrographs we systematically examined several key aspects of radar rainfall products and how they intersect river basin characteristics the calibration free scheme of hlm facilitates uncomplicated yet hydrologically meaningful physical interpretation of streamflow predictions the error propagation to hydrographs is evaluated through standard performance indices such as kling gupta efficiency kge normalized mean absolute error mae hydrograph timing ht and percent peak difference pd for 2016 and 2017 for details on their computation refer to appendix b 4 1 resolution effect to diagnose the impact of rainfall resolutions we aggregated rainfalls at fifteen equivalent spatial scales resolutions i e 1 1 original resolution 2 2 3 3 4 4 5 5 6 6 7 7 8 8 10 10 12 12 16 16 20 20 24 24 32 32 and 64 64 km2 for both rainfall products keeping the temporal resolution fixed at 1 h likewise we used nine temporal scales i e 1 original resolution 2 3 4 6 9 12 18 and 24 h scales for both products setting the original spatial resolutions during the aggregation process we maintained the conservation of rainfall volume the asynch solver small et al 2013 used in hlm defines forcings in terms of rate to solve a system of odes the solver converts all forcing rates to the units of meters per minute m min internally to use a common time window enabling us to obtain the solution for discharge at any time in the river network refer to appendix a for details using the spatial and temporal rainfall aggregation experiments fig s1 of the supplementary material shows the details of the spatial rainfall aggregation process thereby showing equivalent spatial scales between mrms and ifc zr rainfall products in fig 4 we present an example of spatially aggregated ifc zr and mrms rainfall products for a monthly accumulated rainfall of september 2016 at their original resolutions both products show similar spatial structures however there is an apparent difference in their magnitudes here we show as examples the comparison between mrms and ifc zr at their equivalent spatial resolutions of 2 2 km2 fig 4a and 4c and 8 8 km2 fig 4b and 4d demonstrating the magnitude difference in space in the literature there are no clear indications of underlying radar rainfall error distributions therefore understanding the distribution of observed radar rainfall errors differences across resolutions from aggregation experiments could provide insights into the consequent streamflow prediction errors in fig 5 we present the distribution of mrms rainfall errors across two representative basins in iowa the distribution for both large 16 800 km2 and small 800 km2 basins demonstrates that errors are magnified and more negatively skewed with the increase in spatial aggregation of rainfall the errors both mean and variance are generally larger for a smaller basin than they are for a larger basin we discuss the implications for hydrologic simulation in section 5 4 2 river network topology effect next we investigated the river network aggregation of rainfall over basins corresponding to the usgs stream gauge stations in iowa we calculated the mean areal precipitation map at usgs basins and in fig 6 we present the boxplots of rainfall difference between mrms and ifc zr across a range of basin sizes in iowa we limit the selection to 20 basins as they encapsulate a wide range of basin scales 700 30 000 km2 clearly the map differences are positively biased toward the mrms rainfall the variability of these differences is generally more apparent for smaller basins we discuss in detail the propagation rainfall differences errors to streamflow prediction errors in section 5 to understand a direct relationship between the rainfall errors and streamflow errors we evaluated differences at rainfall event scales over a two year analysis conceptually when the analysis is carried out over the longer time scales such as a month or a year we implicitly overlook the small scale temporal variability of rainfall inherent especially in convective rainfall systems to account for such variability we selected all events over the analysis period that are concurrent between both rainfall products we used 24 h of the dry down period to distinguish between two consecutive rainstorms and hence identify storm events and accumulated rainfall the consequent observed and simulated runoff characteristics such as peak flow difference the timing of the peaks and runoff volume difference were computed based on the basin travel time through the network assuming a constant flow velocity of 0 7 m s 4 3 radar visibility effect identifying the connection between radar rainfall and the wf of a basin has implications for the watershed response we define the range at which a weather radar can detect the precipitation as radar visibility range rvr we used rvr to identify hillslopes that are deemed to be captured by radars surrounding the state of iowa seven radars i e davenport kdvn des moines kdmx kansas city keax la crosse karx minneapolis kmpx omaha koax and sioux falls kfsd see figs 1 and 3 provide coverage to the entire state when rvr of 230 km is considered as the radar literature e g vignal and krajewski 2001 indicates the radar rainfall estimation errors are range dependent for instance vignal and krajewski 2001 showed that radar rainfall estimation errors are significant beyond 150 km even when vertical profile reflectivity correction is employed for this reason we considered the rvr threshold of 150 km circles shown in fig 3 in our analysis for usgs basins in iowa firstly we computed their width functions then we identified the hillslopes that are within the rvr threshold and then computed the corresponding width function the width function for the hillslopes within the rvr threshold was overlaid on the width function for the entire basin the difference between the two corresponds to the hillslopes that are not within good rvr in fig 3 we show the hillslopes of the cedar basin that are not covered by any radars within the rvr threshold of 150 km we then computed rvr in terms of the mean distance to the two important descriptors of the wf i e the centroid and the peak for example white circles shown in fig 3 are some select hillslopes that are at distance equal to the distance to the peak of wf the mean distance is computed as the mean of rvrs from radars overlooking this basin if this distance is greater than 150 km this means the hillslopes are not covered by circles representing rvr somewhat arbitrary threshold in fig 3 and one could expect higher errors in the amount of rainfall they receive 4 4 radar rainfall product similarity effect several experiments discussed above were intended to explore the differences between two rainfall products to explain the difference in simulated hydrographs instead of seeking differences between the two here we explored the similarities between the two products to explain the difference in hydrographs first we compared the rainfall volume at the basin scale between the two products on an hourly basis we computed a simple multiplier m which is the ratio of basin wide mrms rainfall volume to basin wide ifc zr rainfall volume see eq 3 3 m i 1 n a i m r m s i i 1 n a i i f c z r i i 4 e t m r m s i i f c z r i m where etmrmsi mrmsi and ifc zri correspond to equalized to mrms mrms and ifc zr rainfall respectively at any hillslope pixel i with area ai eq 4 demonstrates the construction of another rainfall product i e the equalized to mrms rainfall which matches basin wide mrms rainfall volume the spatial structure inherent in the ifc zr rainfall was preserved in this process the idea behind basin wide rainfall equalization as opposed to statewide domain equalization comes from the concept that the rainfall runoff conversion process is hydrologically relevant at the watershed scale we explain our reasoning in the results and discussion section in detail to diagnose whether this approach of equalizing basin wide rainfall volume can explain the difference in hydrographs we distributed the same rainfall volume uniformly over the basin in other words we distorted the rainfall over the basin to make it uniform while preserving rainfall volume over the basin to connect it to the rainfall resolution based experiment one could perceive it as the resolution of 64 64 km2 the goal is to investigate whether spatial rainfall structure is important in addition to the basin wide rainfall volume for streamflow prediction 5 results and discussion in this section we illustrate important results from our experiment in several key aspects of radar rainfall products outlined in the methods section we limit our discussion to mrms and ifc zr rainfall products 5 1 relationship between the spatiotemporal resolution of rainfall and streamflow errors to explore the hydrologic effect of using different spatial and temporal resolutions of rainfall we conducted hydrologic simulations for 2016 and 2017 note that our diagnosis here is between resolutions for a rainfall product and between equivalent resolutions across products since rainfall resolutions can interact differently across basin scales see fig 5 it is imperative therefore to evaluate their hydrologic effects in a comprehensive framework we present here hydrologic evaluation of streamflow predictions i e streamflow errors upon forcing hlm with fourteen spatial and nine temporal aggregation scales resolutions for both radar rainfall products note that here we explore the individual effects of spatial and temporal resolutions of radar rainfall rather than their combined effect in fig 7 we show how kge among other streamflow error measures varies across resolutions and basin sizes for 2016 we present here the percentage change in kge from model predictions relative to the original product highest resolution 1 1km2 and 1 h across different spatial and temporal resolutions of mrms rainfall two key features emerge out of these plots one related to the spatial dependence of kge and the other related to its variability across spatial and temporal resolutions if we exclude the scale of 64 64 km2 4096 km2 from interpretation the variability across spatial resolutions is relatively smaller than those across temporal resolutions the variability of kge represented in terms of the percentage difference between rainfall resolutions is generally larger for smaller basins while its impact starts reducing above basin size of around 1000 km2 one could expect such impact of spatial resolution at smaller basins when compared against the scale of corresponding rainfall resolution see the solid gray lines in fig 7 since kge measures the efficiency as a function of correlation bias and variance ratio it is difficult to signal out which contributed the most to kge across basins see fig s2 however it provides a comprehensive measure of all for the sake of brevity we presented here an example of kge across spatial and temporal aggregation scales for 2016 at least one could see from fig s2 that the rainfall resolution affects the correlation component of kge the most compared to the other two see the deterioration of correlation for 64 64 km2 also at the smaller scales one could signal out the effect of temporal aggregation more easily on the correlation component than the other two components of kge our results on streamflow response due to the impact of rainfall spatial variability across a wide range 7 37 000 km2 of basin scales n 140 generally demonstrate a similar pattern as previous studies lobligeois et al 2014 quintero et al 2016 nikolopoulos et al 2011 sangati and borga 2009 ochoa rodriguez et al 2015 for instance sangati and borga 2009 and nikolopoulos et al 2011 demonstrated though for a smaller number of mountainous basins a significant difference in flood response for lr lw 0 3 and spatial resolution of 8 km i e for the basin size 700 km2 with a major contribution coming from the volume bias than the spatial structure during the spatial aggregation process as these studies alluded to the magnitudes of the hydrologic response could differ based on geographic locations of the basins hydroclimatic conditions the number of events considered and hydrologic model structures among others in fig 8 we present one of the spread characteristics i e the range of three error measures i e kge pd and ht for 2016 for mrms rainfall based simulations the left column shows the range of measures across spatial scales while the right column corresponds to the range of these measures across temporal scales following our discussion above we see spatial scale dependence of the variability of kge note that the spatial variability is more prominent because of the departure we saw especially at the scale 64 64 km2 fig 8a at this scale most of our monitoring stations show significant distortion of radar rainfall spatial structure also see fig 7 across temporal scales fig 8b however the variability in kge is not as extensive even for basin sizes below 1000 km2 in fig 8 c and 8 d we present the range of pd across spatial and temporal scales respectively despite a slight difference in the variability the overall spatial dependence of pd is similar here we can attribute the spatial scale dependence to the fact that the rainfall bias potentially random as evidenced by our illustration in fig 5 is filtered out by the river network as basin size increases the pattern of range of ht shown in fig 8 e and f are somewhat difficult to recognize notice that the range of ht for smaller basins is not that different from large basins since ht interacts with the travel time of water in the river network corresponding inter resolution change across basin scales can be expected to vary minimally our results show that errors from the spatial and temporal aggregation of rainfall are magnified when propagated to hydrographs especially for smaller basins due to its implication on the rainfall volume bias at the basin scale we will return to this aspect of rainfall volume bias in a later section we performed similar analyses for 2017 which show similar variability across spatial and temporal resolutions and basin sizes the repetition of this experiment and analyses for the ifc zr rainfall product results in a small disparity in the pattern of variability across scales in addition to the anticipated difference in terms of performance measures for the sake of brevity we do not repeat here plots like figs 7 and 8 for ifc zr rainfall analyses of simulation results from these two products enable us to see that propagation of rainfall errors across resolutions is not as apparent for the larger basin sizes generally greater than around 1000 km2 fig s3 shows an example of simulated hydrographs for cedar river near waterloo a 13 300 km2 using ifc zr and mrms rainfalls at equivalent spatial resolution 2 2 km2 first it shows that the model has skill in replicating the observed hydrograph there is however some discrepancy in observed and simulated hydrographs owing to a complex interaction between rainfall errors and model errors despite this complexity the apparent difference in predictions between mrms and ifc zr helps us diagnose sources of error in rainfall the resulting difference in performance between these two products across usgs stations over iowa is depicted in fig 9 also see fig s3 the left column of fig 9 shows that a larger number of stations show superior kge for the mrms product this plot also shows a range of variability in terms of model performance from 0 0 8 one could perceive it as the model depicting skill in streamflow predictions the second column shows the comparison of pd here the pattern is different between the two years for 2016 more stations show overestimation of peaks with mrms than that with ifc zr when evaluated against the observations in 2017 the pattern is virtually opposite with ifc zr predictions showing overestimation of peaks relative to observations the third column depicts the comparison of ht between two products it shows that the ifc zr product renders slightly more delay in the timing of hydrographs at more stations than the mrms product in 2016 overall the results in fig 9 demonstrate that rainfall errors also see fig 6 are amplified at many locations when propagated through the model to streamflow hydrographs in 2017 however the pattern is systematically shifted towards mrms at significantly more stations at this stage the authors are unaware of any rainfall estimation algorithm over this period that could have potentially contributed to the shift one key message emerges out of these three metrics that is the bias and variance components are more dominant compared to correlation in contributing to the overall performance difference between two products it raises a question on the origin of the apparent difference in model simulations forced with two rainfall products subsequent sections explore the source of these differences 5 2 relationship between rainfall statistical characterization and streamflow errors one would expect a strong relationship between the rainfall errors differences and streamflow errors when there is a systematic rainfall discrepancy interacting linearly with the model to investigate this relationship we evaluated the map difference at annual monthly and storm event scales for the sake of brevity we present in fig 10 the relationship between rainfall statistical moments differences between two rainfall products and consequent streamflow errors for 2016 fig 10 a and b show the relationship of percentage difference in streamflow error metric in terms of kge and normalized mae respectively with percentage difference in the first moment mean of map across all storm events in fig 10 c and d we show the same for the second moment variance of map for these events the amplification of runoff errors is evident from these plots though without any strong relationship pattern with rainfall moments difference in fig 10 e we present a scatterplot of streamflow peak difference conditional on ifc zr and mrms rainfalls for all events and usgs basins there is no clear structure emerging from this plot between streamflow peak difference and map difference across all storm events in 2016 this is apparent also for 2017 apart from a complex interaction between rainfall and the non linear model structure we find it difficult to attribute the streamflow error to a difference in a particular aspect of rainfall 5 3 relationship between the radar range interaction with width function wf and streamflow errors the watershed response to rainfall can largely be described by its wf if one considers a constant flow velocity through the drainage network some major basins in iowa like the cedar river basin and des moines river basins are not covered entirely within the good radar visibility range of 150 km see a demonstration in fig 3 here we explore whether an interaction of rvr with wf of basins impacts consequent streamflow response in fig 11 we show plots of streamflow errors in terms of kge and normalized mae for both rainfall products we show the scatterplots of streamflow errors with rvr relative to the peak of wf at the top and rvr relative to the centroid of wf at the bottom four panels respectively in terms of both descriptors of wf it is apparent that kge does not show a clear signature while normalized mae shows a weaker relationship with rvr one would anticipate an increase in streamflow errors decrease in kge while an increase in normalized mae with the increase in rvr particularly beyond 150 km however there is significant variability in streamflow error even for those basin scales which has an rvr of less than 150 km irrespective of wf descriptors used as a reference this result is true for both rainfall products we are not able to discern from these results a clear dependence across basin scales though we only show mean rvr based comparisons here for the sake of brevity we performed the analyses in terms of minimum and maximum distances to peaks and centroids of wf though a weaker relationship emerges in terms of mae from these analyses it is hard to conclusively attribute streamflow errors to the interaction of rvr with wf across a range of basin scales one reason is that both rainfall products are based on the same radar data the range effect is therefore built in in both and the algorithmic differences do not reveal it the absence of a reference rainfall which approximates the true rainfall well makes the streamflow error attribution more difficult and requires further investigation 5 4 relationship between rainfall similarity and streamflow errors thus far our results from sections 5 1 5 3 do not provide a clear indication of apportioning errors in streamflow to differences between two radar rainfall products such results prompted us to adopt the opposite strategy instead of identifying differences to explain streamflow errors we make the two rainfall products as similar as we can while keeping their spatial structures intact after all the very same model shows vastly different performance for different basins when used with the two products see the scatter in fig 9 for 2016 and 2017 arguably we observed from the analysis above that mrms demonstrates slightly better overall performance hence the first experiment we carried out was constructing the equalized to mrms etmrms rainfall by matching ifc zr rainfall volume with mrms rainfall volume on an hourly basis at a basin wide scale we present an example of the equalized to mrms rainfall for the turkey river basin in fig 12 the etmrms rainfall preserves the spatial structure of ifc zr rainfall while matching the basin wide rainfall volume to that of the mrms product fig s4 shows an example of a simulated hydrograph for the etmrms rainfall on top of other simulations presented in fig s3 as it can be seen the streamflow hydrograph forced with the etmrms rainfall virtually follows the streamflow hydrograph obtained from the mrms including the ability to encapsulate major storm events before implementing the concept of basin wide rainfall volume equalization we started with the concept of equalizing rainfall at the domain scale of iowa when we forced the hlm model with the etmrms rainfall for the iowa domain the resulting streamflow hydrograph did not follow a similar pattern as that from mrms rainfall it makes physical sense because the rainfall runoff transformation processes are pertinent at the basin scale not the state domain scale therefore we implemented the rainfall volume equalization scheme at the basin wide scale fig 13 shows results from the rainfall volume equalization experiment here we present the performance metrics associated with hydrologic predictions before and after basin wide equalization of rainfalls we present three metrics for comparison i e kge pd and ht there are significant disparities in all three error measures between ifc zr and mrms signified by the blue dots after we matched ifc zr rainfall volume with mrms the corresponding rain gauge based bias correction is propagated with that of mrms hence the largest gain in the mean ratio correlation and variance ratio for the equalized to mrms also demonstrate very similar performance to that of mrms clearly all three components of kge are recovered resulting in the kge of etmrms virtually like mrms we show here that the propagation of rainfall uncertainty can be explained through two independent rainfall products keeping aside the uncertainty from the model or the inability to acquire true rainfall as such many stations show systematic over and underestimation of peaks by mrms and ifc zr for 2016 and 2017 respectively as illustrated by pd see fig 13b and e this systematic behavior arises typically from the systematic over or underestimation of rainfall the etmrms rainfall forced simulations demonstrate similar pd as mrms another metric ht see 13c 13f is within 24 h at majority of the stations because ht is closely related to the correlation of simulated and observed hydrographs its disparity between mrms and ifc zr is not as scattered as kge and pd again the etmrms hydrograph achieves similar ht as mrms in fig s5 of supplementary material we focus on demonstrating how kge shifts toward the 1 1 line due to rainfall volume equalization at 10 representative basins also we show through the rainfall volume equalization in the opposite direction i e equalized to ifc fig s6 that rainfall volume equalization recovers most of the streamflow errors in other words it adds to our argument that the basin wide rainfall volume bias error is the most prominent feature of rainfall that propagates to the streamflow error with this claim it brings another question is correcting the basin wide rainfall volume bias enough to explain streamflow prediction errors to understand this we constructed a basin wide uniform rainfall on an hourly basis with the same rainfall volume as mrms or etmrms in other words we introduced the most extreme distortion of the spatial structure of rainfall fig 14 illustrates the streamflow errors in terms of three performance measures upon forcing hlm with uniform rainfall we compare the same set of measures as discussed above interestingly the uniform rainfall results in a significant reduction of scattering in model performance measures relative to the one between ifc zr and mrms see figs 9 and 13 the reduction in scatter strongly supports our earlier argument that basin wide rainfall volume error explains most of the streamflow error however the remaining uncertainty in streamflow therefore should arise from other aspects the most likely one is the difference in rainfall spatial structure to test how significant is the spatial structure in explaining the errors in streamflow hydrographs we analyze the errors in terms of differences between the four rainfall products defined up to now in fig 15 we present an example for the turkey river basin also refer to fig 12 illustrating the differences in rainfall mrms ifc shows the difference between mrms and ifc zr rainfall depicting the systematic bias towards mrms as indicated by the mean value of the difference mrms equalized to mrms shows the difference between the mrms and etmrms rainfall this difference is very small mean 0 and with a similar variance as that of mrms ifc it means that the similarity in streamflow characteristics revealed by performance metrics between mrms and etmrms are combined results of the similarity of basin wide rainfall volume and associated rainfall spatial structure additionally the mrms uniform depicts the significant distortion in the spatial structure of rainfall note the mean difference 0 but with a significantly larger variance with a multimodal distribution it explains to a large extent why streamflow errors cannot be explained solely by similarity in rainfall volume to substantiate this argument further we investigate the correlation between the mrms and etmrms note that its spatial structure is the same as ifc zr at hillslope scales across more than 100 events we compute the expected value of the correlation e r across events for each usgs basin fig s7 of supplementary material illustrates the relation between e r and basin sizes it leads to two key messages first most of the stations show a high correlation 0 75 between etmrms and mrms that is the spatial structure between the two is very similar which in conjunction with the same basin wide rainfall volume aptly describes the difference in corresponding streamflow hydrographs streamflow errors second e r demonstrates some spatial scale dependence a small correlation for small basin scales agrees with our discussion in section 5 1 finally we highlight the importance of basin wide rainfall volume through the rainfall equalization experiment at nested basins for example we present in fig 16 the dependence of kge across nested basins of cedar river at conesville here we first equalize ifc zr rainfall volume to mrms at the basin outlet shown in blue in the other setup we equalize ifc zr rainfall volume to mrms at individual basins shown in red as we discussed above we computed kge for each case with mrms as a reference streamflow error in terms of kge is basin scale dependent the streamflow errors increase systematically as rainfall volume errors propagate upstream as basin size decreases to elucidate it further the basin wide rainfall volume is more hydrologically important at small basin scales 6 summary and conclusions in this paper we explored several characteristics of rainfall that eventually propagate to streamflow hydrographs our objective was to hydrologically investigate the aspects of rainfall that mostly contribute toward explaining streamflow errors it is well documented that the uncertainty in hydrologic predictions requires an understanding of the sources of uncertainty given its complexity we investigated the uncertainty arising from the rainfall inputs alone in this paper also the true rainfall is unknown and thus it is difficult to isolate rainfall input error from other sources of uncertainty in the hydrologic model predictions we therefore approached this study using two independently constructed radar rainfall products i e mrms and ifc zr using the physically distributed hydrologic model called hlm in which a landscape is partitioned into a system of hillslope and stream links we evaluated the performance of our hydrologic predictions streamflow errors the model evaluation was undertaken at about 140 usgs stream gauge locations over iowa we devised a host of numerical experiments to systematically characterize the propagation of rainfall errors to streamflow hydrographs in this pursuit we investigated several key aspects including the spatiotemporal resolution of rainfall statistical characterization of rainfall the interaction of rvr with basin wf and most importantly the basin wide rainfall volume bias error we summarize our key results as follows i the basin wide rainfall volume bias error is the most important aspect of rainfall that explains most of the errors in hydrologic predictions streamflow errors however correctly estimated rainfall volume is not sufficient for good predictive performance the rainfall spatial structure should also be well estimated at least in terms of the key spatial features e g locations of the intense rainfall this conclusion is substantiated by our investigation of the effect of spatial resolution model performance deteriorates substantially only at the very coarse resolution relative to basin scale small scale rainfall variability effect is filtered out by the river network at the basin scale they play a role only at local scales note that there could be additional attributes of rainfall explaining the remaining discrepancy which needs further exploring ii the impact of the spatiotemporal resolution of rainfall on runoff errors is generally spatial scale dependent the effect of the inter resolution variability transpires into large differences in streamflow hydrographs typically for smaller basin sizes while it generally starts reducing for basin sizes of around 1000 km2 we conclude that the inter resolution variability of rainfall in space and time does not have a major effect on streamflow hydrographs at the majority of the usgs monitored basins iii among several other rainfall characteristics explored we did not observe any strong signature explaining the propagation of rainfall errors to streamflow hydrograph errors a practical conclusion that emerges from our study is that rain gauge based rainfall correction should be done at the basin scale basin is defined here by the location of interest e g a community located by a river to improve streamflow prediction for that community a network of rain gauges can be used to adjust basin wide rainfall volume this is in contrast to the earlier postulates of mean field corrections e g smith and krajewski 1991 our study is not without limitations there could be other rainfall characteristics one could explore which we could not address in this paper or were out of its scope though we studied it in the context of radar rainfall products our findings can also be generalized in the context of rainfall products from other sources we especially find this study applicable for real time streamflow forecasting when we force the model with different rainfall products with different resolutions latency and levels of performance using the basin wide rainfall volume error correction scheme implemented in this paper additionally our study did not include ifc bridge sensors observations quintero et al 2021 in the evaluation process which might have substantiated our conclusion regarding small basin sizes further evaluation at these scales will be reported in future issues since iowa is one of the hubs of the wind energy industry in the u s the wind turbine is likely to have implications on the radar measurement of rainfall ghimire and krajewski 2020 demonstrate generally significant streamflow flood prediction errors at smaller basin communities that have a large coverage of wind farms they illustrate that such errors systematically decrease with basin size since usgs typically monitors streamflow at larger basins the effect of wind farm clutter does not reveal profoundly on streamflow hydrographs at such scales credit authorship contribution statement ganesh r ghimire conceptualization formal analysis investigation methodology software validation visualization writing original draft writing review editing witold f krajewski conceptualization resources supervision methodology validation writing original draft writing review editing tibebu b ayalew methodology validation writing original draft writing review editing radoslaw goska methodology software validation visualization writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge the iowa flood center at the university of iowa where this study was conducted the iowa flood center provided financial support for this work the first author acknowledges the support from the ballard and seashore fellowship the second author is grateful for the support of the rose joseph summers endowment the university of iowa high performance computing group provided resources required for our numerical experiments with the hlm appendix a a general representation of the non linear model structure of hlm is shown in fig 2 we present here a modeling approach of four storage components and transport of flow in channel links eq a1 shows the mass transport equation for channel storage in each link a1 d q d t 60 v r 10 3 1 λ 1 l a a r λ 2 q q r λ 1 q c 2 q p c q t c q s c q i n t where ar km2 and qr m3 s represent reference drainage area and discharge respectively their values are unit in the model the reference velocity vr and λ1 and λ2 the exponents of discharge and drainage area respectively are the model parameters the component v r q λ 1 a λ 2 corresponds to the non linear flow routing velocity in the channel ghimire et al 2018 ayalew et al 2014 likewise l is the length of link qpc is the flux from surface pondage to channel qpc k2sp m min qsc is the flux from top layer into the channel qtc ktcst m min called interflow qsc is the flux from subsurface layer to channel qsc k3ss m min and qin is the inflow from upstream links into the channel the mass conservation equations for surface top layer and bottom layers are given by a2 d s p d t c 1 p t q p c q p t e p a3 d s t d t q p t q t c q t s e t a4 d s s d t q t s q s c e s where p t ep et and es are precipitation and potential evaporation from surface top layer and subsurface respectively in mm h qts is the flux from top layer to subsurface qts kist m min while qpt is the flux from surface to top layer qpt ktsp m min moreover there are four other states equations given by a5 d s p r e c i p d t c 1 p t a6 d v r d t q p c t a7 d q b d t v b l a h q s c 60 q b q b i n t a8 d s e v a p d t e p o t t 0 001 60 where sprecip t m is the total fallen precipitation from time 0 to t vr t m3 s is the total flux of water from runoff from time 0 to t and qb t m3 s is the channel discharge from the base flow the values of constants k2 kt ki k3 c1 c2 and computations of evaporation are a9 k 2 v h l a h 60 10 3 mi n 1 a10 k 3 v g l a h 60 10 3 mi n 1 a11 k i k 2 β mi n 1 a12 k t k 2 a b 1 s t s l α mi n 1 where l and ah are length and area of each hillslope unit respectively likewise the evaporation fluxes are presented as a13 e p s p s r u e p o t t c o r r m mi n 1 a14 e t s t s l u e p o t t c o r r m mi n 1 a15 e s s s h b s l u e p o t t c o r r m m i n 1 a16 c o r r s s s r s t s l s s h b s l there are several parameters presented in equations a9 a16 that are constant in space and time the channel flow routing parameters vr λ1 and λ2 are 0 33 ms 1 0 2 and 0 1 respectively for the operational forecasting model for our implementation we rather used the variable values of vr and λ1 based on power law velocity models for the strahler stream order of channels epot is the potential evaporation vh is the hillslope velocity 0 02 ms 1 infiltration rate from subsurface to channel k3 2 0425 10 6 min 1 percentage of infiltration from topsoil layer to subsurface β 0 005 hillslope unit depth hb 0 5 m topsoil depth sl 0 1 m surface to topsoil infiltration multiplicative factor b 99 and the surface to topsoil infiltration exponent factor α 3 appendix b we computed errors in terms of several statistical error metrics associated with the model simulated hydrographs relative to observed hydrographs the major performance measures we report here are kling gupta efficiency kge gupta et al 2009 normalized mean absolute error nmae hydrograph timing ht and percent peak difference pd kge metric is a function of three measures correlation variance ratio and bias the ideal value of kge is equal to one b1 k g e 1 e d where ed is given by e d r 1 2 α 1 2 β 1 2 where r is a correlation variance ratio α σ s σ o and mean ratio β μ s μ o the standard deviation of the simulated hydrograph is σs σo is the standard deviation of the observed hydrograph μs is the mean of the simulated hydrograph and μo is the mean of the observed hydrograph for further interpretation of the kge for model evaluation one could refer to camici et al 2020 knoben et al 2019 pool et al 2018 likewise mae is given by b2 m a e q o q s n where qo and qs are observed and simulated hydrograph time series values and n is the total number of time series nodes we normalized mae drainage area to obtain normalized mae ht is associated with the cross correlation between the simulated and observed time series we computed it as the number of hours a simulated time series needs to be shifted so that the cross correlation is maximized the positive value of ht indicates delay in the timing of hydrograph while the negative value of ht indicates early timing of simulated hydrograph relative to observed hydrograph the percent peak difference pd was computed as b3 p d p e a k s i m p e a k o b s p e a k o b s 100 where peaksim and peakobs are annual peaks of simulated and observed streamflow time series respectively 
195,rainfall is arguably the most important yet most variable input for rainfall runoff hydrologic models in this study the authors search for the characteristics of radar rainfall estimates that are most important for skillful streamflow predictions they perform comprehensive hydrologic investigations of radar rainfall characteristics including spatiotemporal resolution radar range visibility statistical characterization of rainfall variability all vis a vis basin characteristics such as size and river network topology since the true rainfall fields are unknown the authors exploit a paradigm of using two independently constructed radar rainfall products i e multi radar multi sensor and ifc zr used operationally by the iowa flood center ifc using the distributed hydrologic model called the hillslope link model for the domain of the state of iowa they evaluate streamflow prediction at 140 usgs gauge stations that monitor rivers in iowa through spatial and temporal rainfall aggregation experiments the authors show that the impact of spatial and temporal resolution of rainfall is significant typically for smaller basins while starts reducing significantly for basins larger than 1 000 km2 other rainfall characteristics they explored do not reveal a strong signature in the relationship of rainfall differences between the two products and hydrograph errors however exploring the product similarities rather than differences reveals that the basin wide rainfall volume has the most significant effect on streamflow prediction the results from this study are generalizable for all rainfall observing systems keywords radar rainfall error resolution width function basin wide rainfall volume error mrms ifc 1 introduction despite numerous ongoing efforts towards characterizing uncertainties of rainfall runoff hydrologic model predictions modelers continue to struggle to apportion the sources of error explicitly into those due to the rainfall input and those due to the model structure e g andréassian et al 2001 liu and gupta 2007 renard et al 2010 there is a consensus in the hydrologic community that this remains a major scientific and operational challenge renard et al 2010 point out four sources of uncertainty in hydrologic rainfall runoff models a input uncertainty such as in rainfall estimates b model structural uncertainty c output uncertainty such as errors in rating curves that impact flow estimates and d parametric uncertainty related to model parameters and uncertainties in calibration data in this study we focus on the errors in radar rainfall estimates model inputs and their propagation through a hydrologic model used for streamflow forecasting we consider the rating curves uncertainty negligible and as our model is not calibrated the model uncertainty includes both the structural and parametric errors the spatial and temporal variability of precipitation has been acknowledged to have substantial effects on basin response with the deployment of the next generation weather radars nexrad in the continental united states in the early 1990s klazura and imy 1993 the operational estimation of rainfall over large areas at fine spatial and temporal resolutions became possible consequently the research attention shifted towards investigating the effect of the spatiotemporal resolution of radar rainfall products on hydrologic simulations e g ogden and julien 1994 lobligeois et al 2014 sapriza azuri et al 2015 quintero et al 2016 however as many researchers e g krajewski and smith 2002 berne and krajewski 2013 aghakouchak et al 2010 krajewski et al 2010 pointed out there are several other sources of uncertainties in radar estimation of rainfall including calibration signal attenuation range anomalous propagation beam blockage bright bands random errors algorithms and variability in radar reflectivity and rainfall intensity z r relationships despite significant efforts a comprehensive characterization of uncertainties associated with radar rainfall estimations has not been achieved yet see villarini and krajewski 2010 for a review hence efforts will continue in this direction in the future e g seo et al 2015 2018 ciach and gebremichael 2020 given the complex structure and often significant level of radar rainfall uncertainty an important research challenge is to figure out its effect on hydrologic prediction and in this work specifically on streamflow forecasting here we present a comprehensive hydrologic investigation of several characteristics of radar rainfall products the key question we attempt to answer in this study is what are the characteristics of rainfall input that are most important for skillful streamflow predictions since the true areal rainfall is unknown it is difficult to decouple the effects of the rainfall input uncertainty from that of the model uncertainty on the model prediction to address this difficulty we organize our investigation using two independently constructed radar rainfall products i e 1 multi radar multi sensor mrms rainfall zhang et al 2016 2011 and 2 iowa flood center ifc zr radar rainfall which has roots in hydro nexrad seo et al 2011 krajewski et al 2011 seo et al 2015 we use the two products to drive the same highly distributed hydrologic rainfall runoff model called hillslope link model e g krajewski et al 2017 quintero et al 2016 2020 this paper is organized as follows section 2 reviews some key studies reported on radar rainfall error propagation to rainfall runoff model hydrographs in section 3 we discuss the experimental methods exploring relationships between rainfall characteristics and errors in hydrographs section 4 presents our main results and discusses the findings section 5 summarizes and presents the conclusions and limitations of this work 2 review of studies on rainfall error propagation to hydrographs there is a plethora of studies that explored the effect of rainfall on hydrologic response this study however is focused on the propagation of errors in radar rainfall products to the hydrologic response by identifying the characteristics that lead to the most skillful prediction of streamflow interesting results have been put forth by studies in the past to depict the relationship between the radar rainfall errors and catchment runoff characteristics across scales in an early study on the topic collier and knowles 1986 showed that the error in radar rainfall results in equal or smaller hydrograph errors in some basins while amplified hydrograph errors in other basins wyss et al 1990 suggested that the errors in radar rainfalls are less significant than the errors resulting from the transformation of rainfall to runoff moreover the use of spatially distributed rainfall is more important to dry catchments than wet catchments e g shah et al 1996 sharif et al 2004 ogden and julien 1994 studied the effect of the spatial resolution of radar rainfall using a distributed rainfall runoff model at the 32 km2 macks creek watershed in southwestern idaho and the 121 km2 taylor arroyo watershed in southeastern colorado they showed at the catchment outlet that the spatial resolution of rainfall data can significantly affect the peak discharge prediction and the severity of the effect is dependent on whether the watersheds are simulated with or without accounting for infiltration several studies e g lobligeois et al 2014 ochoa rodriguez et al 2015 nikolopoulos et al 2010 nikolopoulos et al 2011 quintero et al 2016 sapriza azuri et al 2015 showed that the impact of spatial and temporal variability of rainfall on hydrologic model results decreases with the increase in the catchment sizes for instance lobligeois et al 2014 evaluated 3620 flood events across 181 french catchments using radar rainfall inputs they showed that even coarser resolution radar rainfall and hydrologic models can accurately capture the hydrologic response of catchments under oceanic climatic conditions atlantic coast of france with relatively uniform precipitation fields for the catchments where precipitation fields are highly variable in space their results illustrated marked improvement in hydrologic performance using high resolution rainfall products a few studies e g nicótina et al 2008 zoccatelli et al 2015 have presented somewhat contrasting results for instance nicótina et al 2008 showed that the role of spatial rainfall variability on streamflow response is minimal for basins as large as 8000 km2 which they attribute to the dominance of subsurface flow on the total streamflow volume using high space time resolution radar rainfall for five extreme flash flood events across 27 basins ranging between 36 and 982 km2 and rainfall spatial organization index zoccatelli et al 2015 showed that the flood prediction errors modulated by the hillslope and channel residence times are scale independent relative to the basin scale clearly there is no consensus on the impact of spatial and temporal resolution of radar rainfall inputs on the hydrologic model outputs and performance segond et al 2007 nikolopoulos et al 2011 lobligeois et al 2014 pointed out some issues on this disparity most of these studies are constrained to very few catchments based on synthetic flows comparing simulation results against reference simulations and models are parameterized to a single resolution of precipitation while evaluating across several spatial and temporal resolutions of the hydrologic model also the hydrologic models in these studies are calibrated and the calibration process compensates for the errors in the input by adjusting the parameter values owing to these contrasting results several authors e g andréassian et al 2001 woods and sivapalan 1999 suggested the assessment of rainfall volume rather than its distribution while others recommended testing different hydrologic modeling configurations kouwen and garland 1989 pointed out that the appropriate level of the spatial resolution of radar rainfall on hydrologic models for flood forecasting is dependent on the scale of the area considered and maintained that it is still somewhat subjective pessoa et al 1993 further showed that the errors resulting from the spatial and temporal averaging of rainfall are within acceptable limits compared to the errors originating from radar reflectivity rainfall intensity z r relationships none of the studies cited above investigated the role of the river network geometry in aggregating the space time rainfall and its subsequent effect on the simulated hydrologic response smith et al 2005 explored the influence of spatial variability of rainfall on hydrologic response in an urban watershed of dead run in maryland from the river network perspective using a distance metric in the river network called the normalized flow distance they showed that the drainage network structures dampen the consequent catchment hydrologic response which otherwise is expected to be significantly affected when viewed from euclidean perspective sangati and borga 2009 nikolopoulos et al 2011 demonstrated for the mountainous 623 km2 fella river basin in northeastern italy that rainfall volume error during the spatial aggregation process is the main source of radar rainfall error in simulating flash flood response their results showed an increase in the peak discharge and runoff volume errors for the larger values of a dimensionless parameter i e the ratio of rainfall resolution lr to the characteristic length of the basin lw which is the case of smaller basin scales for a given spatial resolution of rainfall though range dependent radar rainfall errors have been mentioned in the literature e g borga 2002 cunha et al 2012 sharif et al 2004 few studies have explored the interaction between the range dependent radar errors in terms of river network width functions and hydrologic predictions seo et al 1999 extended their efforts in reducing the systematic errors in radar rainfall data used for operational hydrologic forecasting by the national weather service nws suggesting procedures to estimate real time mean field bias through the use of radar and gauge pairs from the observation network also see yoo et al 2014 which has significant implication on rainfall volume estimation since it is not practical to have a dense network of rain gauges everywhere working with multiple radar products of different spatial and temporal resolutions across multiple basin scales could be a potential direction in this effort 3 study area and data 3 1 study area to investigate different aspects of radar rainfall error propagation to runoff we used data from the entire state of iowa the study area see fig 1 considered here is in the context of the domain of the iowa flood center ifc streamflow forecasting model which comprises about 420 000 hillslope and link pairs over the state krajewski et al 2017 3 2 streamflow data for evaluation of the model output i e streamflow hydrograph we used observed streamflow time series for years 2016 and 2017 at 140 usgs stream gauge stations across iowa fig 1 the basins defined by these gauges show hydro geological variability across the state out of these basins about 65 drain to the mississippi river while 35 drain to the missouri river basin e g ghimire et al 2018 larimer 1957 these basins range from 7 37 000 km2 in size which enables us to characterize variability across spatial scales basins in the northeastern part of the state are characterized by the rugged steep terrain while the rest of the state is mostly flat to mild sloped agricultural watersheds 3 3 radar rainfall products as an overall strategy for our study we considered two radar based rainfall products for this evaluation mrms rainfall with 1 km2 by 1 h resolution and ifc zr rainfall with 0 35 km2 by 5 min resolution though both products are based on essentially the same level ii data collected by the seven wsr 88dp radars overlooking iowa see fig 1 they are developed independently mrms is gauge corrected rainfall while ifc zr is not on the other hand the ifc zr rainfall accumulation includes a correction for the effect of advection seo and krajewski 2015 and the mrms does not the two products also differ in the data quality control and rainfall estimation algorithms in the absence of true rainfall working with two radar rainfall products allows us to investigate the relationship between streamflow prediction errors and the radar product differences by doing so we hope to identify the key aspects of radar rainfall estimation important for skillful streamflow prediction 3 4 hydrologic modeling framework using hillslope link model we employed the distributed model at the iowa flood center ifc referred to as the hillslope link model hlm in which landscape is decomposed into hillslopes and links krajewski et al 2017 quintero et al 2016 the rainfall runoff conversion process occurs at hillslopes and generated flows are routed through corresponding links see fig 2 the average size of hillslopes is 0 4 km2 hlm is a non linear physically based distributed model with four storage components modeling each hillslope link channel storage q t m3 s pondage on the surface sp t m soil top layer storage st t m and subsurface storage ss t m the water transport model uses a non linear velocity formulation and aggregates water from two upstream links as appropriate the result is a system of non linear ordinary differential equations that are solved using a specialized numerical solver called asynch small et al 2013 refer to appendix a for more details on the non linear model structure of hlm we selected this model as it has origins in the scaling properties of the river network and thus readily allows scale based analyses mantilla and gupta 2005 ayalew et al 2015 this version of the model used for our study is relatively simple to implement yet credibly reproduces observed runoff hydrographs across scales e g quintero et al 2020 also the model is not calibrated to any basin or scale or rainfall input and thus in principle given a more accurate input should result in better streamflow prediction still there is no guarantee that this is the case as the model s structural uncertainty may interact with the input uncertainty in a complex way 3 5 river network topology and width function we adopted a 90m resolution digital elevation model dem from the national elevation dataset ned of usgs to obtain stream and river network topology for our study domain the network topology partitions the landscape into a system of hillslopes areas and drainage networks lengths refer to ayalew et al 2014 and mantilla and gupta 2005 for details thus the derived network topology is an integral component of the hlm modeling framework discussed above due to its role in shaping the observed hydrographs in the network the flow aggregation transport and hence hydrologic response are connected strongly to the spatial distribution of a drainage network characterized by its width function wf e g lee and delleur 1976 perez et al 2018 rodriguez iturbe and rinaldo 1997 the wf represents the distribution of the number of stream links at a flow distance x from the outlet of the basin assuming that every drop of water in the stream network travels to the outlet with the same velocity and that the flow is not attenuated the wf represents the distribution of flow travel time in the river network moussa 2008 mathematically in a discrete case wf is given by e g perez et al 2018 1 w f x i d 1 n i i d for 0 x x m a x where the index function i id is defined as 2 i i d 1 x δ x d i d x 0 o t h e r w i s e the unique identification tag of a stream link is id and each link has a distance to the basin outlet d id the wf is evaluated at a range of distances from 0 at the outlet to the maximum xmax the most distant headwater stream of the basin at δx interval as a demonstration we present in fig 3 the wf of the cedar river basin at conesville where the peak of wf x for this basin is about 350 km from the outlet and the base of wf x is about 500 km hydrologically the base of wf x represents the maximum travel time for the basin which is about 8 5 days for the cedar river basin if a constant channel velocity of 0 7 m s is assumed 4 methods the key strategy of our investigation is to use two different radar rainfall products with the same rainfall runoff model and explain the streamflow prediction errors in terms of radar rainfall difference characteristics streamflow errors are the differences between hydrologic model prediction and the corresponding discharge observations the rainfall runoff model is hlm the same version as that used operationally by the iowa flood center krajewski et al 2017 and evaluated in previous studies e g quintero et al 2020 to investigate how rainfall difference error propagates through the hydrologic model to hydrographs we systematically examined several key aspects of radar rainfall products and how they intersect river basin characteristics the calibration free scheme of hlm facilitates uncomplicated yet hydrologically meaningful physical interpretation of streamflow predictions the error propagation to hydrographs is evaluated through standard performance indices such as kling gupta efficiency kge normalized mean absolute error mae hydrograph timing ht and percent peak difference pd for 2016 and 2017 for details on their computation refer to appendix b 4 1 resolution effect to diagnose the impact of rainfall resolutions we aggregated rainfalls at fifteen equivalent spatial scales resolutions i e 1 1 original resolution 2 2 3 3 4 4 5 5 6 6 7 7 8 8 10 10 12 12 16 16 20 20 24 24 32 32 and 64 64 km2 for both rainfall products keeping the temporal resolution fixed at 1 h likewise we used nine temporal scales i e 1 original resolution 2 3 4 6 9 12 18 and 24 h scales for both products setting the original spatial resolutions during the aggregation process we maintained the conservation of rainfall volume the asynch solver small et al 2013 used in hlm defines forcings in terms of rate to solve a system of odes the solver converts all forcing rates to the units of meters per minute m min internally to use a common time window enabling us to obtain the solution for discharge at any time in the river network refer to appendix a for details using the spatial and temporal rainfall aggregation experiments fig s1 of the supplementary material shows the details of the spatial rainfall aggregation process thereby showing equivalent spatial scales between mrms and ifc zr rainfall products in fig 4 we present an example of spatially aggregated ifc zr and mrms rainfall products for a monthly accumulated rainfall of september 2016 at their original resolutions both products show similar spatial structures however there is an apparent difference in their magnitudes here we show as examples the comparison between mrms and ifc zr at their equivalent spatial resolutions of 2 2 km2 fig 4a and 4c and 8 8 km2 fig 4b and 4d demonstrating the magnitude difference in space in the literature there are no clear indications of underlying radar rainfall error distributions therefore understanding the distribution of observed radar rainfall errors differences across resolutions from aggregation experiments could provide insights into the consequent streamflow prediction errors in fig 5 we present the distribution of mrms rainfall errors across two representative basins in iowa the distribution for both large 16 800 km2 and small 800 km2 basins demonstrates that errors are magnified and more negatively skewed with the increase in spatial aggregation of rainfall the errors both mean and variance are generally larger for a smaller basin than they are for a larger basin we discuss the implications for hydrologic simulation in section 5 4 2 river network topology effect next we investigated the river network aggregation of rainfall over basins corresponding to the usgs stream gauge stations in iowa we calculated the mean areal precipitation map at usgs basins and in fig 6 we present the boxplots of rainfall difference between mrms and ifc zr across a range of basin sizes in iowa we limit the selection to 20 basins as they encapsulate a wide range of basin scales 700 30 000 km2 clearly the map differences are positively biased toward the mrms rainfall the variability of these differences is generally more apparent for smaller basins we discuss in detail the propagation rainfall differences errors to streamflow prediction errors in section 5 to understand a direct relationship between the rainfall errors and streamflow errors we evaluated differences at rainfall event scales over a two year analysis conceptually when the analysis is carried out over the longer time scales such as a month or a year we implicitly overlook the small scale temporal variability of rainfall inherent especially in convective rainfall systems to account for such variability we selected all events over the analysis period that are concurrent between both rainfall products we used 24 h of the dry down period to distinguish between two consecutive rainstorms and hence identify storm events and accumulated rainfall the consequent observed and simulated runoff characteristics such as peak flow difference the timing of the peaks and runoff volume difference were computed based on the basin travel time through the network assuming a constant flow velocity of 0 7 m s 4 3 radar visibility effect identifying the connection between radar rainfall and the wf of a basin has implications for the watershed response we define the range at which a weather radar can detect the precipitation as radar visibility range rvr we used rvr to identify hillslopes that are deemed to be captured by radars surrounding the state of iowa seven radars i e davenport kdvn des moines kdmx kansas city keax la crosse karx minneapolis kmpx omaha koax and sioux falls kfsd see figs 1 and 3 provide coverage to the entire state when rvr of 230 km is considered as the radar literature e g vignal and krajewski 2001 indicates the radar rainfall estimation errors are range dependent for instance vignal and krajewski 2001 showed that radar rainfall estimation errors are significant beyond 150 km even when vertical profile reflectivity correction is employed for this reason we considered the rvr threshold of 150 km circles shown in fig 3 in our analysis for usgs basins in iowa firstly we computed their width functions then we identified the hillslopes that are within the rvr threshold and then computed the corresponding width function the width function for the hillslopes within the rvr threshold was overlaid on the width function for the entire basin the difference between the two corresponds to the hillslopes that are not within good rvr in fig 3 we show the hillslopes of the cedar basin that are not covered by any radars within the rvr threshold of 150 km we then computed rvr in terms of the mean distance to the two important descriptors of the wf i e the centroid and the peak for example white circles shown in fig 3 are some select hillslopes that are at distance equal to the distance to the peak of wf the mean distance is computed as the mean of rvrs from radars overlooking this basin if this distance is greater than 150 km this means the hillslopes are not covered by circles representing rvr somewhat arbitrary threshold in fig 3 and one could expect higher errors in the amount of rainfall they receive 4 4 radar rainfall product similarity effect several experiments discussed above were intended to explore the differences between two rainfall products to explain the difference in simulated hydrographs instead of seeking differences between the two here we explored the similarities between the two products to explain the difference in hydrographs first we compared the rainfall volume at the basin scale between the two products on an hourly basis we computed a simple multiplier m which is the ratio of basin wide mrms rainfall volume to basin wide ifc zr rainfall volume see eq 3 3 m i 1 n a i m r m s i i 1 n a i i f c z r i i 4 e t m r m s i i f c z r i m where etmrmsi mrmsi and ifc zri correspond to equalized to mrms mrms and ifc zr rainfall respectively at any hillslope pixel i with area ai eq 4 demonstrates the construction of another rainfall product i e the equalized to mrms rainfall which matches basin wide mrms rainfall volume the spatial structure inherent in the ifc zr rainfall was preserved in this process the idea behind basin wide rainfall equalization as opposed to statewide domain equalization comes from the concept that the rainfall runoff conversion process is hydrologically relevant at the watershed scale we explain our reasoning in the results and discussion section in detail to diagnose whether this approach of equalizing basin wide rainfall volume can explain the difference in hydrographs we distributed the same rainfall volume uniformly over the basin in other words we distorted the rainfall over the basin to make it uniform while preserving rainfall volume over the basin to connect it to the rainfall resolution based experiment one could perceive it as the resolution of 64 64 km2 the goal is to investigate whether spatial rainfall structure is important in addition to the basin wide rainfall volume for streamflow prediction 5 results and discussion in this section we illustrate important results from our experiment in several key aspects of radar rainfall products outlined in the methods section we limit our discussion to mrms and ifc zr rainfall products 5 1 relationship between the spatiotemporal resolution of rainfall and streamflow errors to explore the hydrologic effect of using different spatial and temporal resolutions of rainfall we conducted hydrologic simulations for 2016 and 2017 note that our diagnosis here is between resolutions for a rainfall product and between equivalent resolutions across products since rainfall resolutions can interact differently across basin scales see fig 5 it is imperative therefore to evaluate their hydrologic effects in a comprehensive framework we present here hydrologic evaluation of streamflow predictions i e streamflow errors upon forcing hlm with fourteen spatial and nine temporal aggregation scales resolutions for both radar rainfall products note that here we explore the individual effects of spatial and temporal resolutions of radar rainfall rather than their combined effect in fig 7 we show how kge among other streamflow error measures varies across resolutions and basin sizes for 2016 we present here the percentage change in kge from model predictions relative to the original product highest resolution 1 1km2 and 1 h across different spatial and temporal resolutions of mrms rainfall two key features emerge out of these plots one related to the spatial dependence of kge and the other related to its variability across spatial and temporal resolutions if we exclude the scale of 64 64 km2 4096 km2 from interpretation the variability across spatial resolutions is relatively smaller than those across temporal resolutions the variability of kge represented in terms of the percentage difference between rainfall resolutions is generally larger for smaller basins while its impact starts reducing above basin size of around 1000 km2 one could expect such impact of spatial resolution at smaller basins when compared against the scale of corresponding rainfall resolution see the solid gray lines in fig 7 since kge measures the efficiency as a function of correlation bias and variance ratio it is difficult to signal out which contributed the most to kge across basins see fig s2 however it provides a comprehensive measure of all for the sake of brevity we presented here an example of kge across spatial and temporal aggregation scales for 2016 at least one could see from fig s2 that the rainfall resolution affects the correlation component of kge the most compared to the other two see the deterioration of correlation for 64 64 km2 also at the smaller scales one could signal out the effect of temporal aggregation more easily on the correlation component than the other two components of kge our results on streamflow response due to the impact of rainfall spatial variability across a wide range 7 37 000 km2 of basin scales n 140 generally demonstrate a similar pattern as previous studies lobligeois et al 2014 quintero et al 2016 nikolopoulos et al 2011 sangati and borga 2009 ochoa rodriguez et al 2015 for instance sangati and borga 2009 and nikolopoulos et al 2011 demonstrated though for a smaller number of mountainous basins a significant difference in flood response for lr lw 0 3 and spatial resolution of 8 km i e for the basin size 700 km2 with a major contribution coming from the volume bias than the spatial structure during the spatial aggregation process as these studies alluded to the magnitudes of the hydrologic response could differ based on geographic locations of the basins hydroclimatic conditions the number of events considered and hydrologic model structures among others in fig 8 we present one of the spread characteristics i e the range of three error measures i e kge pd and ht for 2016 for mrms rainfall based simulations the left column shows the range of measures across spatial scales while the right column corresponds to the range of these measures across temporal scales following our discussion above we see spatial scale dependence of the variability of kge note that the spatial variability is more prominent because of the departure we saw especially at the scale 64 64 km2 fig 8a at this scale most of our monitoring stations show significant distortion of radar rainfall spatial structure also see fig 7 across temporal scales fig 8b however the variability in kge is not as extensive even for basin sizes below 1000 km2 in fig 8 c and 8 d we present the range of pd across spatial and temporal scales respectively despite a slight difference in the variability the overall spatial dependence of pd is similar here we can attribute the spatial scale dependence to the fact that the rainfall bias potentially random as evidenced by our illustration in fig 5 is filtered out by the river network as basin size increases the pattern of range of ht shown in fig 8 e and f are somewhat difficult to recognize notice that the range of ht for smaller basins is not that different from large basins since ht interacts with the travel time of water in the river network corresponding inter resolution change across basin scales can be expected to vary minimally our results show that errors from the spatial and temporal aggregation of rainfall are magnified when propagated to hydrographs especially for smaller basins due to its implication on the rainfall volume bias at the basin scale we will return to this aspect of rainfall volume bias in a later section we performed similar analyses for 2017 which show similar variability across spatial and temporal resolutions and basin sizes the repetition of this experiment and analyses for the ifc zr rainfall product results in a small disparity in the pattern of variability across scales in addition to the anticipated difference in terms of performance measures for the sake of brevity we do not repeat here plots like figs 7 and 8 for ifc zr rainfall analyses of simulation results from these two products enable us to see that propagation of rainfall errors across resolutions is not as apparent for the larger basin sizes generally greater than around 1000 km2 fig s3 shows an example of simulated hydrographs for cedar river near waterloo a 13 300 km2 using ifc zr and mrms rainfalls at equivalent spatial resolution 2 2 km2 first it shows that the model has skill in replicating the observed hydrograph there is however some discrepancy in observed and simulated hydrographs owing to a complex interaction between rainfall errors and model errors despite this complexity the apparent difference in predictions between mrms and ifc zr helps us diagnose sources of error in rainfall the resulting difference in performance between these two products across usgs stations over iowa is depicted in fig 9 also see fig s3 the left column of fig 9 shows that a larger number of stations show superior kge for the mrms product this plot also shows a range of variability in terms of model performance from 0 0 8 one could perceive it as the model depicting skill in streamflow predictions the second column shows the comparison of pd here the pattern is different between the two years for 2016 more stations show overestimation of peaks with mrms than that with ifc zr when evaluated against the observations in 2017 the pattern is virtually opposite with ifc zr predictions showing overestimation of peaks relative to observations the third column depicts the comparison of ht between two products it shows that the ifc zr product renders slightly more delay in the timing of hydrographs at more stations than the mrms product in 2016 overall the results in fig 9 demonstrate that rainfall errors also see fig 6 are amplified at many locations when propagated through the model to streamflow hydrographs in 2017 however the pattern is systematically shifted towards mrms at significantly more stations at this stage the authors are unaware of any rainfall estimation algorithm over this period that could have potentially contributed to the shift one key message emerges out of these three metrics that is the bias and variance components are more dominant compared to correlation in contributing to the overall performance difference between two products it raises a question on the origin of the apparent difference in model simulations forced with two rainfall products subsequent sections explore the source of these differences 5 2 relationship between rainfall statistical characterization and streamflow errors one would expect a strong relationship between the rainfall errors differences and streamflow errors when there is a systematic rainfall discrepancy interacting linearly with the model to investigate this relationship we evaluated the map difference at annual monthly and storm event scales for the sake of brevity we present in fig 10 the relationship between rainfall statistical moments differences between two rainfall products and consequent streamflow errors for 2016 fig 10 a and b show the relationship of percentage difference in streamflow error metric in terms of kge and normalized mae respectively with percentage difference in the first moment mean of map across all storm events in fig 10 c and d we show the same for the second moment variance of map for these events the amplification of runoff errors is evident from these plots though without any strong relationship pattern with rainfall moments difference in fig 10 e we present a scatterplot of streamflow peak difference conditional on ifc zr and mrms rainfalls for all events and usgs basins there is no clear structure emerging from this plot between streamflow peak difference and map difference across all storm events in 2016 this is apparent also for 2017 apart from a complex interaction between rainfall and the non linear model structure we find it difficult to attribute the streamflow error to a difference in a particular aspect of rainfall 5 3 relationship between the radar range interaction with width function wf and streamflow errors the watershed response to rainfall can largely be described by its wf if one considers a constant flow velocity through the drainage network some major basins in iowa like the cedar river basin and des moines river basins are not covered entirely within the good radar visibility range of 150 km see a demonstration in fig 3 here we explore whether an interaction of rvr with wf of basins impacts consequent streamflow response in fig 11 we show plots of streamflow errors in terms of kge and normalized mae for both rainfall products we show the scatterplots of streamflow errors with rvr relative to the peak of wf at the top and rvr relative to the centroid of wf at the bottom four panels respectively in terms of both descriptors of wf it is apparent that kge does not show a clear signature while normalized mae shows a weaker relationship with rvr one would anticipate an increase in streamflow errors decrease in kge while an increase in normalized mae with the increase in rvr particularly beyond 150 km however there is significant variability in streamflow error even for those basin scales which has an rvr of less than 150 km irrespective of wf descriptors used as a reference this result is true for both rainfall products we are not able to discern from these results a clear dependence across basin scales though we only show mean rvr based comparisons here for the sake of brevity we performed the analyses in terms of minimum and maximum distances to peaks and centroids of wf though a weaker relationship emerges in terms of mae from these analyses it is hard to conclusively attribute streamflow errors to the interaction of rvr with wf across a range of basin scales one reason is that both rainfall products are based on the same radar data the range effect is therefore built in in both and the algorithmic differences do not reveal it the absence of a reference rainfall which approximates the true rainfall well makes the streamflow error attribution more difficult and requires further investigation 5 4 relationship between rainfall similarity and streamflow errors thus far our results from sections 5 1 5 3 do not provide a clear indication of apportioning errors in streamflow to differences between two radar rainfall products such results prompted us to adopt the opposite strategy instead of identifying differences to explain streamflow errors we make the two rainfall products as similar as we can while keeping their spatial structures intact after all the very same model shows vastly different performance for different basins when used with the two products see the scatter in fig 9 for 2016 and 2017 arguably we observed from the analysis above that mrms demonstrates slightly better overall performance hence the first experiment we carried out was constructing the equalized to mrms etmrms rainfall by matching ifc zr rainfall volume with mrms rainfall volume on an hourly basis at a basin wide scale we present an example of the equalized to mrms rainfall for the turkey river basin in fig 12 the etmrms rainfall preserves the spatial structure of ifc zr rainfall while matching the basin wide rainfall volume to that of the mrms product fig s4 shows an example of a simulated hydrograph for the etmrms rainfall on top of other simulations presented in fig s3 as it can be seen the streamflow hydrograph forced with the etmrms rainfall virtually follows the streamflow hydrograph obtained from the mrms including the ability to encapsulate major storm events before implementing the concept of basin wide rainfall volume equalization we started with the concept of equalizing rainfall at the domain scale of iowa when we forced the hlm model with the etmrms rainfall for the iowa domain the resulting streamflow hydrograph did not follow a similar pattern as that from mrms rainfall it makes physical sense because the rainfall runoff transformation processes are pertinent at the basin scale not the state domain scale therefore we implemented the rainfall volume equalization scheme at the basin wide scale fig 13 shows results from the rainfall volume equalization experiment here we present the performance metrics associated with hydrologic predictions before and after basin wide equalization of rainfalls we present three metrics for comparison i e kge pd and ht there are significant disparities in all three error measures between ifc zr and mrms signified by the blue dots after we matched ifc zr rainfall volume with mrms the corresponding rain gauge based bias correction is propagated with that of mrms hence the largest gain in the mean ratio correlation and variance ratio for the equalized to mrms also demonstrate very similar performance to that of mrms clearly all three components of kge are recovered resulting in the kge of etmrms virtually like mrms we show here that the propagation of rainfall uncertainty can be explained through two independent rainfall products keeping aside the uncertainty from the model or the inability to acquire true rainfall as such many stations show systematic over and underestimation of peaks by mrms and ifc zr for 2016 and 2017 respectively as illustrated by pd see fig 13b and e this systematic behavior arises typically from the systematic over or underestimation of rainfall the etmrms rainfall forced simulations demonstrate similar pd as mrms another metric ht see 13c 13f is within 24 h at majority of the stations because ht is closely related to the correlation of simulated and observed hydrographs its disparity between mrms and ifc zr is not as scattered as kge and pd again the etmrms hydrograph achieves similar ht as mrms in fig s5 of supplementary material we focus on demonstrating how kge shifts toward the 1 1 line due to rainfall volume equalization at 10 representative basins also we show through the rainfall volume equalization in the opposite direction i e equalized to ifc fig s6 that rainfall volume equalization recovers most of the streamflow errors in other words it adds to our argument that the basin wide rainfall volume bias error is the most prominent feature of rainfall that propagates to the streamflow error with this claim it brings another question is correcting the basin wide rainfall volume bias enough to explain streamflow prediction errors to understand this we constructed a basin wide uniform rainfall on an hourly basis with the same rainfall volume as mrms or etmrms in other words we introduced the most extreme distortion of the spatial structure of rainfall fig 14 illustrates the streamflow errors in terms of three performance measures upon forcing hlm with uniform rainfall we compare the same set of measures as discussed above interestingly the uniform rainfall results in a significant reduction of scattering in model performance measures relative to the one between ifc zr and mrms see figs 9 and 13 the reduction in scatter strongly supports our earlier argument that basin wide rainfall volume error explains most of the streamflow error however the remaining uncertainty in streamflow therefore should arise from other aspects the most likely one is the difference in rainfall spatial structure to test how significant is the spatial structure in explaining the errors in streamflow hydrographs we analyze the errors in terms of differences between the four rainfall products defined up to now in fig 15 we present an example for the turkey river basin also refer to fig 12 illustrating the differences in rainfall mrms ifc shows the difference between mrms and ifc zr rainfall depicting the systematic bias towards mrms as indicated by the mean value of the difference mrms equalized to mrms shows the difference between the mrms and etmrms rainfall this difference is very small mean 0 and with a similar variance as that of mrms ifc it means that the similarity in streamflow characteristics revealed by performance metrics between mrms and etmrms are combined results of the similarity of basin wide rainfall volume and associated rainfall spatial structure additionally the mrms uniform depicts the significant distortion in the spatial structure of rainfall note the mean difference 0 but with a significantly larger variance with a multimodal distribution it explains to a large extent why streamflow errors cannot be explained solely by similarity in rainfall volume to substantiate this argument further we investigate the correlation between the mrms and etmrms note that its spatial structure is the same as ifc zr at hillslope scales across more than 100 events we compute the expected value of the correlation e r across events for each usgs basin fig s7 of supplementary material illustrates the relation between e r and basin sizes it leads to two key messages first most of the stations show a high correlation 0 75 between etmrms and mrms that is the spatial structure between the two is very similar which in conjunction with the same basin wide rainfall volume aptly describes the difference in corresponding streamflow hydrographs streamflow errors second e r demonstrates some spatial scale dependence a small correlation for small basin scales agrees with our discussion in section 5 1 finally we highlight the importance of basin wide rainfall volume through the rainfall equalization experiment at nested basins for example we present in fig 16 the dependence of kge across nested basins of cedar river at conesville here we first equalize ifc zr rainfall volume to mrms at the basin outlet shown in blue in the other setup we equalize ifc zr rainfall volume to mrms at individual basins shown in red as we discussed above we computed kge for each case with mrms as a reference streamflow error in terms of kge is basin scale dependent the streamflow errors increase systematically as rainfall volume errors propagate upstream as basin size decreases to elucidate it further the basin wide rainfall volume is more hydrologically important at small basin scales 6 summary and conclusions in this paper we explored several characteristics of rainfall that eventually propagate to streamflow hydrographs our objective was to hydrologically investigate the aspects of rainfall that mostly contribute toward explaining streamflow errors it is well documented that the uncertainty in hydrologic predictions requires an understanding of the sources of uncertainty given its complexity we investigated the uncertainty arising from the rainfall inputs alone in this paper also the true rainfall is unknown and thus it is difficult to isolate rainfall input error from other sources of uncertainty in the hydrologic model predictions we therefore approached this study using two independently constructed radar rainfall products i e mrms and ifc zr using the physically distributed hydrologic model called hlm in which a landscape is partitioned into a system of hillslope and stream links we evaluated the performance of our hydrologic predictions streamflow errors the model evaluation was undertaken at about 140 usgs stream gauge locations over iowa we devised a host of numerical experiments to systematically characterize the propagation of rainfall errors to streamflow hydrographs in this pursuit we investigated several key aspects including the spatiotemporal resolution of rainfall statistical characterization of rainfall the interaction of rvr with basin wf and most importantly the basin wide rainfall volume bias error we summarize our key results as follows i the basin wide rainfall volume bias error is the most important aspect of rainfall that explains most of the errors in hydrologic predictions streamflow errors however correctly estimated rainfall volume is not sufficient for good predictive performance the rainfall spatial structure should also be well estimated at least in terms of the key spatial features e g locations of the intense rainfall this conclusion is substantiated by our investigation of the effect of spatial resolution model performance deteriorates substantially only at the very coarse resolution relative to basin scale small scale rainfall variability effect is filtered out by the river network at the basin scale they play a role only at local scales note that there could be additional attributes of rainfall explaining the remaining discrepancy which needs further exploring ii the impact of the spatiotemporal resolution of rainfall on runoff errors is generally spatial scale dependent the effect of the inter resolution variability transpires into large differences in streamflow hydrographs typically for smaller basin sizes while it generally starts reducing for basin sizes of around 1000 km2 we conclude that the inter resolution variability of rainfall in space and time does not have a major effect on streamflow hydrographs at the majority of the usgs monitored basins iii among several other rainfall characteristics explored we did not observe any strong signature explaining the propagation of rainfall errors to streamflow hydrograph errors a practical conclusion that emerges from our study is that rain gauge based rainfall correction should be done at the basin scale basin is defined here by the location of interest e g a community located by a river to improve streamflow prediction for that community a network of rain gauges can be used to adjust basin wide rainfall volume this is in contrast to the earlier postulates of mean field corrections e g smith and krajewski 1991 our study is not without limitations there could be other rainfall characteristics one could explore which we could not address in this paper or were out of its scope though we studied it in the context of radar rainfall products our findings can also be generalized in the context of rainfall products from other sources we especially find this study applicable for real time streamflow forecasting when we force the model with different rainfall products with different resolutions latency and levels of performance using the basin wide rainfall volume error correction scheme implemented in this paper additionally our study did not include ifc bridge sensors observations quintero et al 2021 in the evaluation process which might have substantiated our conclusion regarding small basin sizes further evaluation at these scales will be reported in future issues since iowa is one of the hubs of the wind energy industry in the u s the wind turbine is likely to have implications on the radar measurement of rainfall ghimire and krajewski 2020 demonstrate generally significant streamflow flood prediction errors at smaller basin communities that have a large coverage of wind farms they illustrate that such errors systematically decrease with basin size since usgs typically monitors streamflow at larger basins the effect of wind farm clutter does not reveal profoundly on streamflow hydrographs at such scales credit authorship contribution statement ganesh r ghimire conceptualization formal analysis investigation methodology software validation visualization writing original draft writing review editing witold f krajewski conceptualization resources supervision methodology validation writing original draft writing review editing tibebu b ayalew methodology validation writing original draft writing review editing radoslaw goska methodology software validation visualization writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge the iowa flood center at the university of iowa where this study was conducted the iowa flood center provided financial support for this work the first author acknowledges the support from the ballard and seashore fellowship the second author is grateful for the support of the rose joseph summers endowment the university of iowa high performance computing group provided resources required for our numerical experiments with the hlm appendix a a general representation of the non linear model structure of hlm is shown in fig 2 we present here a modeling approach of four storage components and transport of flow in channel links eq a1 shows the mass transport equation for channel storage in each link a1 d q d t 60 v r 10 3 1 λ 1 l a a r λ 2 q q r λ 1 q c 2 q p c q t c q s c q i n t where ar km2 and qr m3 s represent reference drainage area and discharge respectively their values are unit in the model the reference velocity vr and λ1 and λ2 the exponents of discharge and drainage area respectively are the model parameters the component v r q λ 1 a λ 2 corresponds to the non linear flow routing velocity in the channel ghimire et al 2018 ayalew et al 2014 likewise l is the length of link qpc is the flux from surface pondage to channel qpc k2sp m min qsc is the flux from top layer into the channel qtc ktcst m min called interflow qsc is the flux from subsurface layer to channel qsc k3ss m min and qin is the inflow from upstream links into the channel the mass conservation equations for surface top layer and bottom layers are given by a2 d s p d t c 1 p t q p c q p t e p a3 d s t d t q p t q t c q t s e t a4 d s s d t q t s q s c e s where p t ep et and es are precipitation and potential evaporation from surface top layer and subsurface respectively in mm h qts is the flux from top layer to subsurface qts kist m min while qpt is the flux from surface to top layer qpt ktsp m min moreover there are four other states equations given by a5 d s p r e c i p d t c 1 p t a6 d v r d t q p c t a7 d q b d t v b l a h q s c 60 q b q b i n t a8 d s e v a p d t e p o t t 0 001 60 where sprecip t m is the total fallen precipitation from time 0 to t vr t m3 s is the total flux of water from runoff from time 0 to t and qb t m3 s is the channel discharge from the base flow the values of constants k2 kt ki k3 c1 c2 and computations of evaporation are a9 k 2 v h l a h 60 10 3 mi n 1 a10 k 3 v g l a h 60 10 3 mi n 1 a11 k i k 2 β mi n 1 a12 k t k 2 a b 1 s t s l α mi n 1 where l and ah are length and area of each hillslope unit respectively likewise the evaporation fluxes are presented as a13 e p s p s r u e p o t t c o r r m mi n 1 a14 e t s t s l u e p o t t c o r r m mi n 1 a15 e s s s h b s l u e p o t t c o r r m m i n 1 a16 c o r r s s s r s t s l s s h b s l there are several parameters presented in equations a9 a16 that are constant in space and time the channel flow routing parameters vr λ1 and λ2 are 0 33 ms 1 0 2 and 0 1 respectively for the operational forecasting model for our implementation we rather used the variable values of vr and λ1 based on power law velocity models for the strahler stream order of channels epot is the potential evaporation vh is the hillslope velocity 0 02 ms 1 infiltration rate from subsurface to channel k3 2 0425 10 6 min 1 percentage of infiltration from topsoil layer to subsurface β 0 005 hillslope unit depth hb 0 5 m topsoil depth sl 0 1 m surface to topsoil infiltration multiplicative factor b 99 and the surface to topsoil infiltration exponent factor α 3 appendix b we computed errors in terms of several statistical error metrics associated with the model simulated hydrographs relative to observed hydrographs the major performance measures we report here are kling gupta efficiency kge gupta et al 2009 normalized mean absolute error nmae hydrograph timing ht and percent peak difference pd kge metric is a function of three measures correlation variance ratio and bias the ideal value of kge is equal to one b1 k g e 1 e d where ed is given by e d r 1 2 α 1 2 β 1 2 where r is a correlation variance ratio α σ s σ o and mean ratio β μ s μ o the standard deviation of the simulated hydrograph is σs σo is the standard deviation of the observed hydrograph μs is the mean of the simulated hydrograph and μo is the mean of the observed hydrograph for further interpretation of the kge for model evaluation one could refer to camici et al 2020 knoben et al 2019 pool et al 2018 likewise mae is given by b2 m a e q o q s n where qo and qs are observed and simulated hydrograph time series values and n is the total number of time series nodes we normalized mae drainage area to obtain normalized mae ht is associated with the cross correlation between the simulated and observed time series we computed it as the number of hours a simulated time series needs to be shifted so that the cross correlation is maximized the positive value of ht indicates delay in the timing of hydrograph while the negative value of ht indicates early timing of simulated hydrograph relative to observed hydrograph the percent peak difference pd was computed as b3 p d p e a k s i m p e a k o b s p e a k o b s 100 where peaksim and peakobs are annual peaks of simulated and observed streamflow time series respectively 
196,planning water resource systems is challenged primarily by two realities first uncertainty is inherent in the predictions of future supplies and demands due for example to hydrological variability and climate change to build societal resilience water planners should seek to enhance the adaptability and robustness of water resource system interventions second water resource developments typically involve competing interests which implies considering the trade offs and synergies implied by the highest performing combinations of development options is useful this work describes a real options based planning framework that generates adaptive and robust water system design alternatives able to consider and trade off different goals the framework can address different types of uncertainties and suggests the highest performing designs across multiple evaluation criteria such as financial costs and water supply service performance metrics using a global city s water resource and supply system as a demonstration of the approach we explore the trade offs between a long term water management plan s infrastructure services service resilience reliability vulnerability and its financial costs under supply and demand uncertainty the set of trade off solutions consist of different investment plans which are adaptive and robust to future changing conditions results show that the highest performing plans lower net present value npv of needed investments by up to 18 while maintaining similar performance across the other objectives the real option value of delaying investments as much as possible approaches up to 14 of total npv graphical abstract keywords water resource systems planning adaptive infrastructure planning real options analysis robust decision making multi objective search stakeholder driven environmental planning 1 introduction planning future interventions in water resource systems faces unprecedented challenges due to climate change socioeconomic growth and increased urbanization milly et al 2008 brekke et al 2009 best 2019 the services and performance of future water resource systems are impacted by the uncertain nature of long term future conditions unpredictable changes in water demands and future hydrological flows and their potentially amplified hydrologic variability increases the risks of future water supply failures fletcher et al 2019 schewe et al 2019 and the sophistication required to prevent them salas et al 2018 equally both service providers utilities river basin organizations etc and their customers and stakeholders have grown in sophistication increasingly demanding their interests be considered in the decision making process carr et al 2012 van bruggen et al 2019 the benefits of certain characteristics water planning have become increasingly clear firstly considering the multiple objectives of water systems hitch 1960 banzhaf 2009 reed et al 2013 paton et al 2014 kasprzyk et al 2013 and thereby achieving multi dimensional efficiency i e the ability to appropriately trade off the benefits implied by the best solutions is typically appropriate second in the face of multiple uncertainties smith et al 2019 harou et al 2020 with different levels of predictability achieving resilience i e recovering quickly from stress or failure robustness i e performing acceptably across a variety of plausible conditions and adaptability i e meeting system requirements by responding to changing conditions have become core objectives of water planning dessai and hulme 2007 charlton and arnell 2011 castelletti et al 2010 reed et al 2013 wise et al 2014 maier et al 2014 kwakkel et al 2015 herman and giuliani 2018 we expand on these below to capture different stakeholder interests water resources management can be strengthened by multi criteria approaches which help reconcile competing water interests e g hurford et al 2020 geressu et al 2020 performance measures of interest when evaluating water intervention options include ones that describe economic or financial performance water supply security metrics such as reliability and resilience hall et al 2019 brown et al 2020 or other social and environment impact measures the development of multi objective optimization approaches identifying plans that represent the best achievable trade offs and synergies between objectives kollat and reed 2007b has made this approach practicable for real system design problems e g matrosov et al 2015 multi objective evolutionary algorithms moeas have found a wide range of applications in water resources planning under uncertainty reed et al 2013 maier et al 2014 a water supply development plan of a water utility typically proposes a set of supply augmentation and or demand reduction water conservation interventions over a planning time horizon yakowitz 1982 luss 1982 padula et al 2013 plans aimed at performing well under a single scenario are likely to have sub optimal performance in other scenarios ben haim 2006 huskova et al 2016 instead approaches aiming at robustness lempert 2003 lempert et al 2006 evaluate plans over multiple plausible futures simulated concurrently kang and lansey 2012 to select actions that are insensitive to a wide range of outcomes because many designs are possible optimization can help automate the search for efficient and robust water supply portfolios for capacity expansion kasprzyk et al 2013 mortazavi naeini et al 2014 huskova et al 2016 a robust but fixed plan that performs well under a range of plausible futures has the disadvantage of not considering future water planners ability to adapt to future conditions as they manifest such static plans quickly become out of date as new information is gained even just a few years after a plan is written for instance huskova et al 2016 introduced a robust planning approach in the presence of trade offs between conflicting objectives but it is not adaptive and does not allow for learning over time trindade et al 2019 hall et al 2020 use a risk based framework that identify plans that robustly achieve targets for tolerable risk and other performance objectives under varying climate scenarios other examples include the work of kasprzyk et al 2013 mortazavi naeini et al 2014 2015 beh et al 2017 borgomeo et al 2018 and geressu and harou 2019 adaptive approaches on the contrary produce multiple strategies each optimal for different trajectories or pathways haasnoot et al 2013 that are developed to dynamically address uncertainty over time allowing for modifications to investment strategies as new information about uncertain conditions becomes available charlton and arnell 2011 paton et al 2014 woodward et al 2014 beh et al 2015 maier et al 2016 gorelick et al 2019 herman et al 2019 erfani et al 2018 recent literature has investigated quantitative evaluation of adaptation through adaptive pathways and real options analysis roa adaptive strategies implement adaptation by optimizing signposts and triggers that dictate the activation of the next action on a pathway haasnoot et al 2012 kwakkel et al 2015 herman and giuliani 2018 trindade et al 2019 and have been applied to flood risk infrastructure sequencing drought management and stormwater management ranger et al 2013 zeff et al 2016 manocha and babovic 2018 dynamic adaptive policy pathways dapp identify adaptive strategies under an uncertain future haasnoot et al 2013 kwakkel et al 2015 by prescribing continuous monitoring and adaptation johnson and geldner 2019 rule based planning frameworks face the challenge of selecting useful indicators and thresholds that define when an action is triggered to address this issue murgatroyd and hall 2021 introduced a framework for optimal rule based planning strategies that helps planners identify a set of candidate indicators based on their ability to predict future risk of failure in a water supply system in this strand of research where signposts associated with measuring the actual values of uncertain factors are used to select options recent studies have sought to develop adaptive plans that can also respond robustly to changing conditions molina perez et al 2019 groves et al 2021 roa enables adaptation by considering present investment decisions that are allowed to be corrected in subsequent modeled planning stages responding to changes in uncertainty over time dixit and pindyck 1995 dittrich et al 2016 and is implemented through different techniques including decision trees lattices monte carlo analysis trigeorgis 1996 lander and pinches 1998 chow and regan 2011 de neufville and scholtes 2011 and multi stage stochastic optimization programs zhao et al 2004 de weck et al 2004 wang and de neufville 2005a b erfani et al 2018 in water resources management roa has been applied in various studies to examine the implications of future uncertainties when irreversible investment commitments are considered woodward et al 2014 ray and brown 2015 marques et al 2015 beh et al 2015 erfani et al 2018 while the adaptability feature of both approaches roa and adaptive pathways are considered over the planning horizon roa exercises the adaptability at pre defined decision stages whilst adaptive pathway approaches do so based on the state of the system and its threshold values the use of roa methods for evaluating flexible strategies in long term climate change adaptation decisions has been both encouraged buurman and babovic 2016 hino and hall 2017 wreford et al 2020 erfani et al 2020 ginbo et al 2020 and contested kwakkel 2020 thus far there have been limitations to the implementation of roa principles for infrastructure planning and scheduling first roa as a single objective approach is limited in capturing diverse stakeholder values for instance erfani et al 2018 optimized adaptive plans using a single least cost objective and an aggregate supply demand formulation which cannot accommodate tangible performance based outcomes hall et al 2012 padula et al 2013 brown et al 2015 second as suggested by herman et al 2020 adaptive frameworks are dependent on an uncertainty specification which is quantified either as a probability distribution or an ensemble of realizations when assigning probabilities to future scenarios is not possible the use of roa is considered to be impractical shortridge and camp 2019 kwakkel 2020 in zhang and babovic 2012 woodward et al 2013 and marques et al 2015 all uncertain future conditions are represented by probability distributions without consideration of robustness in other works where uncertainty is represented as a set of alternative future states of the world e g jeuland and whittington 2014 ray et al 2018 robustness is addressed a posteriori by re evaluating pre defined system configurations over multiple future climatic and non climatic uncertainties this paper introduces a simulation optimization framework that addresses known roa limitations and combines roa principles with robustness analysis to enable adaptive and robust infrastructure planning while exploring the trade offs between multiple objectives we extend the classical multistage stochastic capacity expansion problem ruszczyński and shapiro 2003 where corrective decisions allow the model to compensate for insufficient or excessive investment made at earlier decision stages this way water plans identified are adaptive in that they flexibly activate delay and replace interventions to adapt to the future uncertainties and are robust in that they perform satisfactorily well over a range of plausible future conditions uncertainties are either represented by a scenario tree where each scenario represents a probabilistically weighted future state aiming for adaptation or through a range of plausible equiprobable future states aiming for robustness we consider multiple objectives to explore trade offs inherent in society s conflicting goals for water resources systems and to help identify synergies co benefits between different measures of performance the next section describes the proposed approach the scenario tree construction and the adaptive and robust multi objective optimization formulation section 3 describes an application to a global city in section 4 the case study s results are presented and discussed in section 5 section 6 concludes the paper 2 method the proposed framework seeks both adaptability and robustness in addressing uncertainty and multi dimensional efficiency for dealing with competing societal goals we build roa principles into the planning framework by using a scenario tree to enable adaptability of the investment decisions that can be modified to accommodate changes as they materialize maier et al 2016 roa principles involve various options such as delaying investments in the interest of acquiring information evaluating phased investments and modifying or abandoning actions through the consideration of an ensemble of future states robustness is built into the planning framework by ensuring solutions perform acceptably well over a range of future conditions herman et al 2015 maier et al 2016 fig 1 shows an overview of the proposed framework we use a physically based water resource system simulation which for different plans tracks multiple performance metrics over time the water resource system simulations are coupled with a multi objective evolutionary algorithm moea coello et al 2007 nicklow et al 2010 to identify those adaptive plans on the scenario tree that are also robust across an ensemble of future states and that appropriately trade off various decision relevant interests 2 1 trade off formulation for adaptive and robust planning for adaptive analysis we randomly generate a set of multiple forecasted values of the uncertain parameter and use a scenario tree construction technique to generate a scenario tree we construct the scenario tree by implementing an optimization problem that minimizes the so called probability distance between the uncertainty sets following the algorithm presented by gröwe kuska et al 2003 the problem takes the original uncertainty set and produces a scenario tree out of it for multistage decision making the original uncertainty set in our study is the demand uncertainty that is defined by the bounds of urban water demand forecasting models the algorithm optimally creates a scenario tree by successively bundling the tree nodes into separate sets to be represented by a new node while maintaining the probability information of the constructed tree as close as possible to the original uncertain stochastic process the quality of the constructed tree is controlled by a metric that calculates the percentage of information lost known as relative probability distance heitsch and römisch 2011 this is set to 5 in this study as we assume that this is an acceptable loss of information the tolerance indicates how well the constructed tree approximates the original stochastic process and consequently determines the number of scenarios that are embedded in the scenario tree erfani et al 2018 demonstrated this scenario tree construction method in a water supply capacity expansion problem the multi objective search algorithm then uses the scenario tree to implement roa principles as discussed above while this enables adaptive water resource investment decisions the plans are not necessarily robust to uncertainty to seek for robustness the framework has the ability to represent uncertainty as an ensemble of future states and exploit them to identify adaptive plans that are also robust to these uncertainties to achieve this the framework searches for plans across an ensemble of future states on each branch of the scenario tree using a predefined robustness measure different measures of robustness have been used in the literature lempert et al 2006 herman et al 2015 mcphail et al 2018 each of which allows stakeholders to achieve a different performance requirement 2 2 mathematical formulation below we formulate the multi objective optimization problem that allows implementation of roa principles for adaptive and robust planning let n be the set of nodes on the scenario tree that structures the evolution of the uncertain parameters and z be a vector of the uncertain parameters that are represented as an ensemble of future states the formulation below obtains optimal investment decisions for each node of the scenario tree that are robust to z 1 m i n i m i z e f x z f 1 f k 2 s t i m e d s n i 1 n n t 3 d s n i 0 1 n n t i i 4 x d s n i in the above formulation f x z is the vector of k objective functions f i i 1 k that each is aggregated based on the robustness metric used over an ensemble of z states x is a decision variable vector representing a set of interventions k is the number of objectives for trade off analysis i is the set of all interventions n t is the set of nodes belonging to stage t d s n i is a binary variable denoting if intervention i in portfolio x is selected or not in node n of the scenario tree and m e represents the set of mutually exclusive interventions fig 2 shows a flowchart of the proposed framework in step 1 the search algorithm generates an initial random population of candidate plans on the scenario tree each plan is a portfolio of interventions sequenced over time at each node of the tree a plan could involve for instance a mix of supply side and demand side interventions each with their associated costs and different impacts on supply capacity expansion and demand reduction respectively the population is generated making sure that plans have the same interventions on the node they share to prevent an implemented option from becoming unavailable in subsequent stages of the scenario tree each option has a construction lead time that establishes the period between the decision to implement an option and becoming operational in step 2 each plan in the population is passed to a water resource system simulator to be evaluated over an ensemble of future conditions z these future conditions could include hydrological flows or demand forecasts over the planning period the performance of each solution relative to the objective functions is evaluated in each simulated scenario this results in different performance metrics across z that are then aggregated and passed to the search algorithm as objective function values step 3 analysts can specify a stopping criterion which if satisfied terminates the search process like a pre defined number of iterations or a convergence metric until then the search algorithm employs population update strategies to select interventions for the next evolution step 4 this results in a set of adaptive and robust plans that trade off multiple objectives step 5 the different efficient plans generate a pareto frontier allowing planners to examine the trade offs for example between investment costs and different facets of system performance 3 application 3 1 background we apply the proposed trade off informed framework for adaptive and robust planning to the london urban water supply area which is located in the thames river basin in southeast england the water supply is managed by thames water a private water utility serving 15 million customers across london and the thames valley the region has a relatively high population density and faces a projected 25 increase in population by 2040 thames water 2014 environment agency 2013 however the actual population growth is uncertain making it a suitable case study to investigate the use of the proposed approach furthermore water utilities and regulators in england and wales are considering a move from a traditional single objective least cost optimization approach ukwir 2016 padula et al 2013 to identifying a best value plan that balances multiple performance criteria and seeks adaptability thames water 2019 environment agency 2021 this is because single objective approaches require all metrics to be aggregated commensurated into a single metric typically monetary and their preferences over one another explicitly articulated in a single metric this results in a single optimized solution that can potentially lead to imbalanced and unpopular decisions matrosov et al 2015 multi objective approaches allow incorporating different metrics with different units of measure without a need to either aggregate them or pre specify their preferences this results in a set efficient i e best available trade off solutions hence the proposed multi objective approach is likely to be appealing to water utility planners in the uk and beyond this study uses a 2020 2070 planning period in which water supply management intervention decisions are made every 5 years following water company regulation in england and wales we consider 11 new supply of type reservoir transfer waste water reuse desalination and 4 new demand management interventions of type active leakage control pipe repair water efficiency metering for the global city s water resource system shown in table 1 each option has characteristics related to its ability to store and or manage water construction period design life and mutual exclusivity unlike aggregated supply demand modeling approaches where interventions contributions to supply expansion or demand reduction is a single number yield in the optimization padula et al 2013 erfani et al 2018 in this approach physically based supply interventions and their operating rules are simulated over time whilst demand management options reduce aggregate annual demand release from supply options during droughts occur according to london s seasonal lower thames control diagram ltcd refer to matrosov et al 2011 release from reservoir and groundwater options is subject to available storage while desalination and reuse options release water indefinitely as needed the storage capacity of modular supply interventions can be expanded at a future stage by paying a relevant expansion cost for instance the proposed reservoir can be built with a fixed res or modular resm storage capacity listed in table 1 as two separate interventions that are mutually exclusive i e at most one of the two interventions can be selected the non modular res intervention builds a 150 mm 3 reservoir that releases 267 ml d when activated during droughts periods of low storage and low flow in the thames the modular resm intervention initially builds a reservoir of 100 mm 3 storage capacity which can be expanded to 150 mm 3 at a later decision stage if required for the modular option the utility has to pay a premium upfront to reserve the right for further expansion see erfani et al 2018 for a synthetic example of a reservoir option demonstrating this roa principles this study uses the interactive river aquifer simulation iras 2010 model of the regional water resources and supply system matrosov et al 2011 which tracks flows and storages spatially with a weekly time step options that provide or save less than 10 ml day were ignored and lead times of options related to design and planning permission were not considered therefore results in section 4 are approximate and attempts to compare them directly with twul s published plan which considers more detailed data would not be valid 3 2 addressing uncertainties in this study we use a scenario tree to structure the uncertainty about future water demands for adaptive planning where branching is allowed from one demand scenario to another however each member of the hydrologic ensemble presented in section 3 2 2 is independent and therefore the time series associated with one ensemble member can only be compared with the same ensemble member time series for a different time slice prudhomme et al 2013 each future flow time series represents a unique condition of climate uncertainty and therefore once a simulation begins under one time series that same time series must be used until the end of the planning period to maintain the hydrological consistency of each supply state this implies a scenario tree cannot be used to represent the supply related uncertainty in this case and future climate change supply impacts are represented as an ensemble of equally likely hydrological flow time series in our case study the ensemble of streamflows included nonstationary hydrological conditions where extreme hydrologic events like droughts can be experienced at any point of the planning horizon if a scenario reduction technique were used to bundle hydrological scenarios that would potentially result in droughts happening in consecutive time periods for some branches of the tree and therefore biasing the timing of activation for interventions following the framework discussed earlier this implies the planning decisions made in this application will be adaptive to demand uncertainty and robust to supply uncertainty the proposed approach does not prescribe how uncertainties should be represented allowing planners to decide based on the structure of the data whether to seek adaptability scenario tree or robustness ensemble of future states adaptability allows for learning over time while robustness ensures the plan performs acceptably under a wide range of plausible future conditions in most cases how to structure the uncertainty modeling is dictated by available data or the nature of the problem for instance if in a hydrological time series the temporal correlation were weak supply uncertainty could be represented as a scenario tree where time slices from different time series would be mixed and matched to create branches of the tree heitsch and romisch 2005 latorre et al 2007 séguin et al 2017 therefore allowing adaptive decisions to learn from supply variability if not hydrological flows could be represented as spatially and temporally coherent future time series 3 2 1 demand uncertainty scenario tree the demand scenario tree is extracted out of the london demand uncertainty space as shown in fig 3 a the demand uncertainty is approximated by 21 scenarios shown in fig 3 b it is also drawn out in panel a let t be the assumed time horizon and t t the decision points spaced at regular time intervals the decision in t 1 which is the same for all scenarios is called the here and now decision and is made in the first time step before any uncertainty is realized in the subsequent decision points wait and see decisions are made with the information about uncertainty which is revealed in previous stages these decisions are adaptive in that they can vary with the uncertain parameters and can take different values in each scenario for instance in the context of our example at t 2 the demand uncertainty of the first stage is revealed we make the decision in t 2 for each scenario with the benefit of knowing the value of the uncertain parameter in t 1 but with no other knowledge of future data this proceeds until we make the decision in the last time period t with the benefit of knowing the values of the uncertain parameter at t 1 therefore adaptive decisions are not locked in that they are modified based on new information the here and now decisions are also robust to supply uncertainty since they are optimized based on an aggregation of a hydrological ensemble so that it performs acceptably well robustness however treat solutions as locked by aggregating their performance given a set of ensembles non anticipative constraints link here and now and wait and see decision variables belonging to different scenarios for instance at t 2 the demand uncertainty is represented by three decision nodes implying that the three investment decisions are the same within some subgroups of scenarios which are indistinguishable based on the information available up to that point it can be seen that the subgroups of scenarios s 1 s 9 s 10 s 12 and s 13 s 21 share common investment decisions at t 2 this is enforced by non anticipative constraints that ensure investment decisions at time t only utilize any information that is available up to this stage the proposed methodology exploits the tree structure to provide flexibility in allowing initial water resource investment decisions to adapt to future changes in water demand erfani et al 2018 this multi period decision process enables the virtual planners implied by this model based approach to modify or delay investment plans as information on future demand is gradually revealed 3 2 2 climate projections uncertainty to illustrate the water resource systems behavior during the eventual future climate change impacted supplies supply uncertainty is represented by a set of transient climate change forced daily river flow and monthly groundwater levels for the uk prudhomme et al 2013 available from the national river flow archive online database the scenarios represent equally probable hydrological flows and were derived from the set of transient climate projections obtained from the met office hadley centre regional climate model hadrm3 ppe by dynamically downscaling the global climate model the dataset consists of an ensemble of 11 equally probable flow time series for the thames basin between 1950 and 2098 prudhomme et al 2013 3 3 case study formulation the case study problem formulation uses the following vector of objective functions 5 f x z f 1 f 2 f 3 f 4 where z is the ensemble of hydrological flows and f 1 f 4 are the objectives considered for trade off analysis the first objective f 1 minimizes the total capital and operational cost of implementing new supply and demand interventions in a portfolio the cost is annualized and discounted with discount rate r over the planning time horizon and weighted by the probability p n derived from the scenario tree construction algorithm that node n is realized using 6 f 1 t t n n t i i p n 1 r t t c i d l i d s n i where t c i is the total discounted cost capital and operational of implementing intervention i in node n at time stage t and d l i is the design life of intervention i the costs are normalized to each intervention s expected design life by dividing the investment cost of each intervention by its expected lifetime the use of total investment cost per year allows for equal comparison between interventions that have unequal design lives the second objective f 2 maximizes system service resilience which is defined by how quickly the system recovers from a failure moy et al 1986 a definition of a failure is problem dependent in this study a failure associated with the service resilience objective eq 7 occurs when the london aggregate storage level drops below a certain threshold ltcd level 3 and a non essential water use ban is imposed the aim of the objective is to minimize the maximum duration of the imposed partial water use ban the average discounted maximum duration of the failure across all scenarios weighted by their probability w s is then minimized using 7 f 2 s s w s max t 1 r t d s t where w s is the probability of scenario occurrence d s t is the duration of failure in scenario s in time t the probability of scenario occurrence is derived based on the probabilities of the scenario tree nodes and is calculated by multiplying all state transition probabilities on the scenario path eq 8 8 w s p n n n s n s n where n is the set of all nodes on the demand scenario tree and n s is the set of nodes that belong to the path of scenario s the third objective f 3 is to maximize system reliability which is calculated based on how frequently the system fails the average discounted frequency of no failures across all scenarios weighted by their probability w s is maximized using 9 f 3 s s w s 1 t t 1 r t z s t t s t 100 where z s t is the number of time steps weeks the system was in failure in time period t in scenario s and t s t is the number of time steps weeks in time t we choose to minimize ltcd level 2 failures where a water saving media campaign is imposed to reduce demand the fourth objective f 4 reflects how well a plan meets a desired level of service los requirement over the considered future scenarios we calculate the fraction of future scenarios where a plan performs within the desired los 10 f 4 s s k s s 100 where the binary variable k s is 1 if a plan in a scenario performs within a required los or 0 otherwise and s is the total number of scenarios since the time dimension is not present in eq 10 the performance of this objective is not discounted an objective in our case study is to minimize the number of scenarios in which ltcd level 3 failure frequency exceeds 1 in 20 years eq 10 we implicitly optimize vulnerability i e limiting the magnitude of failure by using objectives relating to different ltcd failure levels los3 resilience los2 reliability and los3 return period requirement thames water uses these different failure levels as thresholds to implement escalating supply and demand management measures in this application robustness is sought both by ensuring insensitivity to future conditions by optimizing the average performance over an ensemble of futures for total cost system service resilience and system reliability and by minimizing ltcd level 3 failure occurrences that exceed a 1 in 20 year frequency for the climate impacted flow scenarios the average statistic was selected to provide a relatively stable performance the performance objectives of cost reliability and service resilience are all discounted to not bias financial metrics within the search the financial costs are net present values npv of capital and operational expenditures incurred by implementing new interventions and using them if only financial metrics were to be discounted in multi objective problems expensive and high performing assets would tend to get selected only towards the end of the planning period as they would offer high benefit at a largely discounted cost the use of equal rates of discount for both monetary and non monetary performance is suggested in literature when benefits are hard to monetize keeler and cretin 1983 therefore the engineering performance objectives were discounted using the same discount rate as the financial objective to ensure an equal rate of time preference these were calculated in accordance with the uk government s green book discount rates which reflect the rate at which society values the present compared to the future treasury 2003 to identify the approximate pareto frontier we use the epsilon dominance non dominated sorting genetic algorithm ii e nsgaii kollat and reed 2006 that has been shown to effectively solve complex many objective optimization problems reed et al 2013 further computational details of how the search process was conducted are found in the supplementary materials see appendix a 4 results 4 1 solving the multi objective adaptive and robust water resource planning problem fig 4 shows the approximately pareto optimal adaptive and robust water resources city plans projected onto the two dimensional cost service resilience trade off space each of the 329 points represents a unique pareto approximate adaptive and robust plan proposing a set of investment options for each decision node of the scenario tree over the 50 year planning period the size of the points shows the reliability objective i e the average percentage of years in a solution that did not have an ltcd level 2 failure while its color shows the percentage of scenarios that meet the los3 return period the investment cost is lower for adaptive and robust plans with lower service resilience reliability and fraction of scenarios that meet required los return period implying that more investment is required to improve performance of the infrastructure service objectives the pareto approximate space is roughly divided into three zones with respect to their performance across the objectives if a decision maker requires that the percentage of scenarios that meet the los3 return period is at least 90 then they must choose a solution from the third zone observed in the right portion of the pareto front our analysis is seeking to identify which infrastructure choices can achieve magnitudes of potential failures deemed acceptable by stakeholders three efficient solutions points on the pareto front in fig 4 are singled out for further analysis to reflect different plausible preferences of decision makers the adaptive and robust plans that correspond to these selected solutions are referred to as arp 1 arp 2 and arp 3 arp 1 is the lowest cost solution where at least 90 of scenarios achieve los3 failures occurring at most every at most 1 in 20 years arp 3 is a high cost solution where all scenarios meet the los3 return period requirement while arp 2 displays an example of a balanced solution between the conflicting objectives more risk averse decision makers may select arp 3 where financial performance low cost is traded in to obtain higher performance in infrastructure services service resilience reliability and vulnerability the investment trajectories for the three selected plans of the pareto front together with their performance values in terms of four objectives are shown in fig 5 the tree consists of 21 possible investment trajectories for each adaptive and robust plan based on the demand scenarios depicted in fig 3 b decisions to invest in a set of interventions are made at the beginning of each time interval t since decision points are spaced at 5 year time intervals an activated intervention becomes available either at the same or in a future time interval depending on the length of its construction period fig 5 details the short term t 1 t 4 i e decisions made in 2020 2025 2030 and 2035 investment decisions of the three adaptive and robust plans represented by black gray and white boxes showing whether a portfolio that consists of an intervention 1 11 or a combination thereof is activated at the beginning of each 5 year time interval under different demand scenarios differentiated by demand thresholds the potential supply side and demand side water resource options that can be activated at each node of the tree are detailed in table 1 demand threshold values displayed on each branch indicate which path is optimal for a given demand at each interval for instance if the planner considers that demand for water in 2025 is most likely to be less than 2030 ml d low demand growth then the lower path is the best intervention response if demand is between 2030 ml d and 2084 ml d moderate demand growth then the middle path is optimal whilst if demand is 2084 ml d or greater high demand growth then the upper path should be selected the interventions that are activated at the beginning of the tree root node should be implemented in the first time period and are selected in all demand scenarios the most pressing concern of water planners is short term investment decisions i e what to do now however these near term decisions must be compatible with future investments and the resulting investment trajectories of which they are a part must demonstrate long term water supply security under different future scenarios initial investment trajectories resulting from differing near term decisions can follow a range of future branches based on future decisions the decision determining which subsequent branch a planner should follow can be taken as new information on demand growth becomes available such results enable adaptive investment planning where initial investment decisions can be postponed and adjusted according to future possible demand conditions the scenario tree approach results in wait and see strategies that seek to delay interventions until they are required this wait and see strategy is a manifestation of real options based principles the opportunity to defer investments enables planners to reduce overall intervention costs and make more informed investment decisions as new information is unveiled the ability to defer investments enables planners to reduce overall intervention costs for example as seen in fig 5 for all three adaptive and robust plans the decision in 2025 t 2 to invest further increase supply or reduce demand is postponed for later if demand is expected to be low or moderate if demand in 2025 is expected to be high then planners can invest in further actions infrastructure or demand management depending on which adaptive and robust plan they select each adaptive and robust plan has alternative sets of interventions scheduled for implementation for instance as shown in fig 5 at the root node arp 1 2 and 3 activate portfolio 1 2 and 3 respectively consisting of the same mix of demand management options namely active leakage control alc smart metering met and pipe repair campaign pip in the more expensive arp 2 and 3 a large supply scheme is also selected alongside the demand management interventions portfolio 2 builds river severn transfer rst while portfolio 3 builds res the interventions selected in 2020 t 1 are active across all 21 scenarios in 2025 t 2 if demand is low no further investment is suggested for the short term period until 2035 t 4 where all adaptive and robust plans activate canal transfer portfolio 7 if demand in 2025 is moderate reuse scheme b rsb is activated either in 2030 t 3 or 2035 t 4 depending on the adaptive and robust plan if demand in 2025 is high the initial portfolios are all expanded differently but in 2030 t 3 the same reuse scheme a is activated in all adaptive and robust plans in practical applications at the next decision stage i e 5 years later the optimization should be performed again with the newly available demand scenarios that could be different from the original ones creaco et al 2013 to gain more insight on how the pareto approximate adaptive and robust plans differ we plot in fig 6 the long term t 1 t 10 investment decisions for the fixed size reservoir res across the three selected adaptive and robust plans in the low cost arp 1 res is activated in high demand growth scenarios scenarios 16 21 midway through the 50 year planning period in the more balanced arp 2 res is activated earlier on in the same scenarios as arp 1 as well as in others 11 out of 21 scenarios in arp 3 res is built across all scenarios 21 out of 21 as it is activated in the first time period the ability to defer or avoid an expensive or potentially controversial option shows the benefit of combining multi objective optimization with roa fig 7 depicts the activation frequency of the interventions across the 21 demand scenarios in each time step for the long term planning problem the combination of interventions in each adaptive and robust plan the time of their implementation as well as its activation frequency across the scenarios is plotted as expected from fig 5 active leakage control alc pipe repair pip and metering met are activated in all scenarios in all three adaptive and robust plans from the start of the planning period this suggests that these demand interventions are robust across the multi objective trade off as well as supply demand uncertainty indicating that demand management should be put in place early in the planning period larger interventions such as trf and res are activated in all three adaptive and robust plans at different points and frequency across the scenarios while arp 1 suggest their activation midway through the planning period arp 2 and 3 activates large interventions earlier and at a higher frequency across the scenarios this indicates that the development of both large supply schemes is recommended at some point during the next 50 years above a certain demand growth level below that level roa helps planners avoid unnecessary costly investments 4 2 activation comparison of large interventions to examine the selection of the two large interventions res and trf we compare their activation across all pareto approximate solutions fig 8 shows the same pareto approximate solution set as fig 4 displaying the values of three objectives cost resilience and reliability the color here corresponds to the activation frequency of the two large interventions across the 21 scenarios during the planning period a black circle informs that by the end of the planning period the intervention is selected all 21 scenarios while a white circle reports that the intervention is not selected in any the interventions display a different activation ratio across the 329 solutions res is selected in 155 of solutions while trf in all 329 by the end of the planning period however above a certain level of resilience 2 8 weeks res is always selected and the frequency of its activation increases for more resilient solutions in the case of trf some solutions with high resilience do not require the intervention to be active in all scenarios at a high frequency this shows that to achieve a certain required level of resilience res must be selected relatively early in the planning period high frequency of scenarios where intervention is activated shown by the black color while the activation of trf could be postponed for later and activated only under certain demand conditions 4 3 metrics for adaptability assessment to help evaluate solutions and quantify what has been gained by this analysis we adopt two metrics from the field of stochastic programming birge and louveaux 1997 escudero et al 2007 namely value of the stochastic solution vss and expected value of perfect information evpi into decision relevant metrics of adaptability and flexibility in water planning decisions to examine the implications on adaptability and flexibility across the cost resilience trade off we compare the same three 5 year plans i e arp 1 2 and 3 from the previous section vss and evpi enable comparing the plans in terms of their ability to adapt to changing conditions and in terms of the value gained from delaying irreversible investment commitments respectively this follows erfani et al 2018 who use the vss and evpi metrics to quantify the benefits of adaptability and flexibility for a single objective water resource planning approach appendix b details the calculation of vss and evpi for the multi stage formulation proposed here the vss and evpi for the three plans as a percentage of the total cost of each adaptive and robust plan are shown in fig 9 in this case study vss illustrates the difference made by using the robust multi objective multistage approach which explicitly allows for periodical adaptation to different future demand conditions instead of ignoring the uncertainty around the demand values in each stage stated in a different way vss quantifies the cost of not recognizing the uncertainty and therefore ignoring the adaptability advantage for the london case study vss expressed as a percentage of the total cost is higher for the most resilient and reliable arp 3 18 compared to arp 1 12 and arp 2 5 this indicates seeking an adaptive and robust city planning approach can have considerable financial value in this case study evpi estimates the value of improving demand forecasting again the higher evpi for arp 3 14 compared to arp 1 10 and arp 2 4 5 shows that for the more resilient and reliable adaptive and robust plan there is high value in investing in demand forecasts 5 discussion to illustrate the benefits of the proposed adaptive and robust multi stage multi objective approach applied to london s water supply planning three efficient adaptive and robust plans were selected and compared in all three of these plans three demand management interventions are recommended for implementation in 2020 they should be implemented regardless of demand growth since they are selected at the beginning of the tree large supply interventions are only activated in 2020 in the more expensive arp 2 and 3 which provide better performing services than arp 1 if demand in 2025 is below 2 141 ml d then the previously selected interventions remain active without the need to invest however if 2025 demand in 2020 is predicted as more than 2 141 ml d further investment is required to maintain desired levels of service in arp 1 and 2 by the end of the planning period all three selected optimal adaptive and robust plans implement both large supply interventions res and trf in the top path of the tree the one with the least favorable supply demand balance showing that under higher demand conditions the selection of the two large schemes occurs across the multi objective trade off regardless of preference these high level results summarize how the approach works and how its results which consider many different decision maker and societal priorities and many different futures are interpreted as encouraged by wreford et al 2020 this study shows the applicability of scenario based roa methods to adaptive climate change infrastructure investment analysis the use of a scenario tree is appropriate for cases where planning is performed regularly over discrete time intervals as is often the case in water supply and river basin planning where agencies emit planning reports on a regular basis e g every 5 years in our case study country since plans are regularly re optimized in each 5 year water supply planning cycle the only consequential decision is the decision made in the first decision time step the initial planning decisions are optimally adapted to future conditions by modifying or delaying investments as more information on future conditions is gradually revealed this definition of adaptability is unique to a multistage stochastic implementation of roa the applicability of roa for climate change adaptation studies has been challenged because of concerns that assigning weights to scenarios in roa is problematic shortridge and camp 2019 and that expected values are not meaningful summaries of option value kwakkel 2020 the proposed approach represents adaptation by applying roa principles without having to assign probabilities to each scenario for instance in this study the scenarios were randomly generated from the uncertainty space using a scenario tree construction method that is the scenario tree construction could generate multiple trees from the same uncertainty source data with different number of nodes at each time step with a calculated probability as well as different branching structure on the topic of expected value this is not a required attribute of our approach which allows consideration of multiple metrics indeed our case study does not use expected values for all objectives multistage stochastic solutions are adaptive in that here and now t t 1 planning decisions that do not depend on future observations are corrected through wait and see t t 1 decisions for each realization of the scenarios in subsequent pre determined stages in the time horizon birge and louveaux 2011 multistage stochastic models use sequential multi period decision making but not all sequential decision making is multistage stochastic examples of sequential decision making include the scheduling formulations of padula et al 2013 beh et al 2015 borgomeo et al 2016 and borgomeo et al 2018 what makes multistage stochastic different from other multi period decision making is the inclusion of a sequence of wait and see decision variables that are used as recourse to here and now planning decisions to capture the evolving information over time dapp solutions can react to the state of the system using signposts and triggers at any moment of a simulation not at pre specified time steps methods aiming to inform management decisions usually involve assumptions and limitations around the uncertainties in our case the supply data used independent non stationary climate scenarios that did not allow us to mix time slices of different hydrological time series and hence they could not be used to construct a scenario tree for adaptive supply analysis we therefore produced plans adaptive to demand and robust to supply uncertainty the proposed framework as explained in section 2 2 however is general and could consider multi dimensional scenario trees where the evolution of different sources of uncertainties not just demand are considered uncertain parameters could include the probability of hydrological scenarios as well as economic aspects such as discount rates the way that uncertain dimensions of the decision problem are modeled must be informed by problem structure and data availability note that following huskova et al 2016 and to limit the number of function evaluations necessary for the optimization to converge each intervention in our application had a fixed capacity i e the size of investments is not optimized table 1 alternative capacities for the same interventions could be added as new interventions if just a few sizes are being considered optimizing the capacity of interventions across their entire range would result in more efficient adaptive and robust plans but is left to future work 5 1 sensitivity analysis we test the sensitivity of the proposed actions of the three selected adaptive and robust plans to the use of different scenario trees the london case study was run for thirty different and optimally generated scenario trees from the stochastic london demand distribution from the same uncertainty source from each run we select an adaptive and robust plan that exhibits a similar trade off between the four objectives according to arp1 2 and 3 the results of the sensitivity analysis shown as a bar chart in fig 10 illustrate the activation frequency of the options in the first planning decision period t 1 for all three adaptive and robust plans most interventions suggested by the planning approach have a high frequency of selection more than 80 indicating that recommendations are not sensitive to the structure of the scenario tree when extracted from the same uncertainty space this finding is in accordance with the stability theory of multistage stochastic programming that suggests that multistage stochastic model results are stable with respect to the perturbation of the stochastic input process heitsch et al 2006 heitsch and römisch 2009 5 2 robustness analysis we perform a regret based analysis of the selected adaptive and robust plans to assess the deviation from optimality when simulated under a different ensemble of flow scenarios to create the new ensemble of hydrological flows we resample the nonstationary time series via bootstrapping as per paparoditis and politis 2002 for resampling of nonstationary time series fig 11 shows box plots of normalized values of regret across the 4 objectives for arp1 2 and 3 regret r for a criteria c is calculated as the difference in the performance p of the portfolio in the best performing scenario s b p c s b and that of the performance of the same plan in the scenario in question s p c s given by 11 r c p c s b p c s the adaptive and robust plan with smaller infrastructure options arp1 performs best in cost regret while the more expensive arp2 has the highest cost regret values because the options activated early in the planning period incur high operational cost values 6 conclusions given unknown future water supply and demand conditions growing global populations and the push to invest in infrastructure to prevent the worst impacts from climate change the problem of intervening in water resource systems under uncertainty has taken on a renewed urgency considering conflicting objectives while addressing uncertainties in developing human natural resource systems has become a frontier problem of water science by explicitly accommodating the aspiration for adaptability robustness and the need to balance different water system goals we feel the proposed approach is a useful contribution to water management this paper proposed an approach for multi objective real options based adaptive and robust planning of water interventions under uncertainty a scenario tree is extracted to represent the uncertain parameters which can be branched from one state to another and the uncertainties that cannot be defined by the scenario tree are considered as an ensemble of future states a multi objective search engine uses an independent water resource simulator to identify optimal interventions for each node of the structured scenario tree and evaluate the plans over an ensemble of future conditions the obtained pareto approximate investment plans are adaptive and robust to the uncertainties considered the framework was demonstrated on london s water resource system demand uncertainty was represented through a scenario tree while supply uncertainty through an ensemble of hydrological flows demand growth threshold values inform decision makers which investment trajectory would be best to follow to optimize cost service resilience reliability and vulnerability of the water supply system the plans are robust in that they are insensitive to different future hydrological conditions and adaptive in that commitments being made in the short term allow for choosing future actions based on how future demand growth unfolds the application of this real options based definition of adaptability to a multi objective water infrastructure planning problem in combination with robust optimization is novel for regulated utilities with a regular planning cycle the proposed adaptive and robust water infrastructure approach is institutionally appropriate since our definition of adaptability uses a set of pre determined time steps to demonstrate the use of the outputs of the proposed planning framework three pareto approximate solutions were selected and compared depending on how the planner believes short term demand growth will evolve and their preference on the multi objective performance trade off they select which plan to implement including whether interventions should be delayed the flexibility and adaptability assessment of the three adaptive and robust plans quantified the benefit of considering supply and demand uncertainty by calculating the stochastic value i e expected loss of using a deterministic solution and the value of knowing more about the future for more costly plans that provide higher performance in infrastructure services service resilience reliability and vulnerability the importance of handling uncertainty and having better information increases in our case london s net present value of required investments was reduced by up to 18 also the option value of best delaying investments to wait for better predictions was shown to be worth up to 14 of total npv these results help quantify the value of a roa approach this paper shows global cities can and should appropriately balance their investment performance trade off and that explicitly optimizing robustness and adaptability of water supply systems can increase future service levels at a lower cost increased societal resilience in the face of climate change is an aspiration of many cities world wide as they look to invest in resource security this paper describes a new route for investing in complex real world water systems despite future uncertainties in a systematic transparent and beneficial manner credit authorship contribution statement kevis pachos conceptualization methodology software writing original draft validation visualization ivana huskova methodology software evgenii matrosov methodology software writing review editing tohid erfani conceptualization writing review editing supervision project administration funding acquisition julien j harou conceptualization writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funding from thames water the ucl department of civil environmental and geomatic engineering doctoral training scheme and ukri s futuredams project es p011373 1 are gratefully acknowledged anna wallen and chris lambert provided comments that improved the paper data used in this study is available in the technical appendices on the thames water website https www thameswater co uk about us regulation water resources appendix a computational implementation to solve the problem explained in section 3 3 we use the epsilon dominance non dominated sorting genetic algorithm ii e nsgaii kollat and reed 2006 we ran the multi objective optimization ten times each starting from a unique population using different random seed value to account for the variability of the initial populations and to best approximate the pareto front previous studies have used similar number of seeds to reduce the computational burden while ensuring that the influence of random number generation on the results is insignificant huskova et al 2016 the initial population size was set to 512 and the algorithm operator parameters were chosen according to previous study recommendations kollat and reed 2007a matrosov et al 2015 each optimization was run for 25 000 function evaluations or until a convergence metric was satisfied table 2 summarizes the algorithm parameters including the objective ε values used for the case study the ε values were selected to capture the minimum level of precision desired in differentiating between the performance of one portfolio alternative and another in each objective 1000 0 1 0 5 and 0 5 for cost resilience reliability and fraction of scenarios that meet required los respectively to determine each run s convergence we used a hypervolume metric zitzler 1999 as the termination criterion which is widely used in multi objective optimization problems as it captures the diversity and convergence of solutions reed et al 2013 fig a 12 shows how the search algorithm has converged after 120 generations appendix b computational insight on the metrics used to evaluate the multi period moea the value of the stochastic solution vss is calculated for each plan by replacing the decision variables i e activation of intervention in each planning interval with expected values and comparing the cost requirements between the two by fixing the first stage interventions and solving for all the scenarios the moea will generate a new set of pareto adaptive and robust plans the expected value of perfect information evpi is estimated by computing the cost difference between the expected value with perfect information and the expected value with current information for each pareto adaptive and robust plan in order to allow comparison of vss and evpi values we select plans with the same level of resilience reliability and fraction of scenarios that meet required los the calculations of vss and evpi in multi stage problems are explained below these two metrics were developed for the case of two stage problems birge and louveaux 1997 and have been extended to multi stage problems escudero et al 2007 we calculate their values in a multi stage framework with two objectives for each pareto approximate solution individually vss and evpi are calculated using the cost values of the solutions that correspond to a given level of resilience reliability and fraction of scenarios where a plan performs within the desired los vss is calculated by solving the mean value problem resulting in a pareto set of first stage solutions evpi is determined by computing the cost difference between the expected value with perfect information and the expected value with current information for the minimization model the following inequalities are satisfied b 1 w s a p e v where w s denotes the expected value of the objective function obtained by replacing all random variables by their expected values w s is known in the literature as the wait and see resolution value a p denotes the optimal solution value to the adaptive and robust multi stage stochastic problem presented in this paper e v denotes the expected result of expected value problem and measures how the optimal solution of the expected value problem performs allowing the other stages decisions to be chosen optimally as functions of different scenarios from eq b 1 evpi and vss are calculated as follows b 2 e v p i a p w s b 3 v s s e v a p to calculate the evpi non anticipative constraints are relaxed at each time step so that decisions are made with perfect information about the future from eq b 2 the difference a p w s displays the value of perfect information from eq b 3 the difference e v a p known as the vss indicates the benefit of finding different solutions for each scenario by solving the stochastic program than to assume lack of uncertainty in the work of escudero et al 2007 those parameters are generalized to the multi stage case explained below let the expected result in t of using the expected value solution denoted by e v t for t 2 t be the optimal value of the a p model where the decision variables until stage t 1 x 1 x t 1 are fixed at the optimal values obtained in the solution of the average scenario model for any multi stage stochastic program the following relations hold e v t 1 e v t t 1 t 1 0 v s s t v s s t 1 t 1 t 1 the value of the stochastic solution is defined in t denoted by v s s t as v s s t a p e v t t t this sequence of non negative values represents the cost of ignoring uncertainty and not providing adaptive and robust solution to future conditions until stage t in the decision making of multi stage models vss and evpi in multi stage problems are then calculated as b 4 v s s t t v s s t and b 5 e v p i t t e v p i t 
196,planning water resource systems is challenged primarily by two realities first uncertainty is inherent in the predictions of future supplies and demands due for example to hydrological variability and climate change to build societal resilience water planners should seek to enhance the adaptability and robustness of water resource system interventions second water resource developments typically involve competing interests which implies considering the trade offs and synergies implied by the highest performing combinations of development options is useful this work describes a real options based planning framework that generates adaptive and robust water system design alternatives able to consider and trade off different goals the framework can address different types of uncertainties and suggests the highest performing designs across multiple evaluation criteria such as financial costs and water supply service performance metrics using a global city s water resource and supply system as a demonstration of the approach we explore the trade offs between a long term water management plan s infrastructure services service resilience reliability vulnerability and its financial costs under supply and demand uncertainty the set of trade off solutions consist of different investment plans which are adaptive and robust to future changing conditions results show that the highest performing plans lower net present value npv of needed investments by up to 18 while maintaining similar performance across the other objectives the real option value of delaying investments as much as possible approaches up to 14 of total npv graphical abstract keywords water resource systems planning adaptive infrastructure planning real options analysis robust decision making multi objective search stakeholder driven environmental planning 1 introduction planning future interventions in water resource systems faces unprecedented challenges due to climate change socioeconomic growth and increased urbanization milly et al 2008 brekke et al 2009 best 2019 the services and performance of future water resource systems are impacted by the uncertain nature of long term future conditions unpredictable changes in water demands and future hydrological flows and their potentially amplified hydrologic variability increases the risks of future water supply failures fletcher et al 2019 schewe et al 2019 and the sophistication required to prevent them salas et al 2018 equally both service providers utilities river basin organizations etc and their customers and stakeholders have grown in sophistication increasingly demanding their interests be considered in the decision making process carr et al 2012 van bruggen et al 2019 the benefits of certain characteristics water planning have become increasingly clear firstly considering the multiple objectives of water systems hitch 1960 banzhaf 2009 reed et al 2013 paton et al 2014 kasprzyk et al 2013 and thereby achieving multi dimensional efficiency i e the ability to appropriately trade off the benefits implied by the best solutions is typically appropriate second in the face of multiple uncertainties smith et al 2019 harou et al 2020 with different levels of predictability achieving resilience i e recovering quickly from stress or failure robustness i e performing acceptably across a variety of plausible conditions and adaptability i e meeting system requirements by responding to changing conditions have become core objectives of water planning dessai and hulme 2007 charlton and arnell 2011 castelletti et al 2010 reed et al 2013 wise et al 2014 maier et al 2014 kwakkel et al 2015 herman and giuliani 2018 we expand on these below to capture different stakeholder interests water resources management can be strengthened by multi criteria approaches which help reconcile competing water interests e g hurford et al 2020 geressu et al 2020 performance measures of interest when evaluating water intervention options include ones that describe economic or financial performance water supply security metrics such as reliability and resilience hall et al 2019 brown et al 2020 or other social and environment impact measures the development of multi objective optimization approaches identifying plans that represent the best achievable trade offs and synergies between objectives kollat and reed 2007b has made this approach practicable for real system design problems e g matrosov et al 2015 multi objective evolutionary algorithms moeas have found a wide range of applications in water resources planning under uncertainty reed et al 2013 maier et al 2014 a water supply development plan of a water utility typically proposes a set of supply augmentation and or demand reduction water conservation interventions over a planning time horizon yakowitz 1982 luss 1982 padula et al 2013 plans aimed at performing well under a single scenario are likely to have sub optimal performance in other scenarios ben haim 2006 huskova et al 2016 instead approaches aiming at robustness lempert 2003 lempert et al 2006 evaluate plans over multiple plausible futures simulated concurrently kang and lansey 2012 to select actions that are insensitive to a wide range of outcomes because many designs are possible optimization can help automate the search for efficient and robust water supply portfolios for capacity expansion kasprzyk et al 2013 mortazavi naeini et al 2014 huskova et al 2016 a robust but fixed plan that performs well under a range of plausible futures has the disadvantage of not considering future water planners ability to adapt to future conditions as they manifest such static plans quickly become out of date as new information is gained even just a few years after a plan is written for instance huskova et al 2016 introduced a robust planning approach in the presence of trade offs between conflicting objectives but it is not adaptive and does not allow for learning over time trindade et al 2019 hall et al 2020 use a risk based framework that identify plans that robustly achieve targets for tolerable risk and other performance objectives under varying climate scenarios other examples include the work of kasprzyk et al 2013 mortazavi naeini et al 2014 2015 beh et al 2017 borgomeo et al 2018 and geressu and harou 2019 adaptive approaches on the contrary produce multiple strategies each optimal for different trajectories or pathways haasnoot et al 2013 that are developed to dynamically address uncertainty over time allowing for modifications to investment strategies as new information about uncertain conditions becomes available charlton and arnell 2011 paton et al 2014 woodward et al 2014 beh et al 2015 maier et al 2016 gorelick et al 2019 herman et al 2019 erfani et al 2018 recent literature has investigated quantitative evaluation of adaptation through adaptive pathways and real options analysis roa adaptive strategies implement adaptation by optimizing signposts and triggers that dictate the activation of the next action on a pathway haasnoot et al 2012 kwakkel et al 2015 herman and giuliani 2018 trindade et al 2019 and have been applied to flood risk infrastructure sequencing drought management and stormwater management ranger et al 2013 zeff et al 2016 manocha and babovic 2018 dynamic adaptive policy pathways dapp identify adaptive strategies under an uncertain future haasnoot et al 2013 kwakkel et al 2015 by prescribing continuous monitoring and adaptation johnson and geldner 2019 rule based planning frameworks face the challenge of selecting useful indicators and thresholds that define when an action is triggered to address this issue murgatroyd and hall 2021 introduced a framework for optimal rule based planning strategies that helps planners identify a set of candidate indicators based on their ability to predict future risk of failure in a water supply system in this strand of research where signposts associated with measuring the actual values of uncertain factors are used to select options recent studies have sought to develop adaptive plans that can also respond robustly to changing conditions molina perez et al 2019 groves et al 2021 roa enables adaptation by considering present investment decisions that are allowed to be corrected in subsequent modeled planning stages responding to changes in uncertainty over time dixit and pindyck 1995 dittrich et al 2016 and is implemented through different techniques including decision trees lattices monte carlo analysis trigeorgis 1996 lander and pinches 1998 chow and regan 2011 de neufville and scholtes 2011 and multi stage stochastic optimization programs zhao et al 2004 de weck et al 2004 wang and de neufville 2005a b erfani et al 2018 in water resources management roa has been applied in various studies to examine the implications of future uncertainties when irreversible investment commitments are considered woodward et al 2014 ray and brown 2015 marques et al 2015 beh et al 2015 erfani et al 2018 while the adaptability feature of both approaches roa and adaptive pathways are considered over the planning horizon roa exercises the adaptability at pre defined decision stages whilst adaptive pathway approaches do so based on the state of the system and its threshold values the use of roa methods for evaluating flexible strategies in long term climate change adaptation decisions has been both encouraged buurman and babovic 2016 hino and hall 2017 wreford et al 2020 erfani et al 2020 ginbo et al 2020 and contested kwakkel 2020 thus far there have been limitations to the implementation of roa principles for infrastructure planning and scheduling first roa as a single objective approach is limited in capturing diverse stakeholder values for instance erfani et al 2018 optimized adaptive plans using a single least cost objective and an aggregate supply demand formulation which cannot accommodate tangible performance based outcomes hall et al 2012 padula et al 2013 brown et al 2015 second as suggested by herman et al 2020 adaptive frameworks are dependent on an uncertainty specification which is quantified either as a probability distribution or an ensemble of realizations when assigning probabilities to future scenarios is not possible the use of roa is considered to be impractical shortridge and camp 2019 kwakkel 2020 in zhang and babovic 2012 woodward et al 2013 and marques et al 2015 all uncertain future conditions are represented by probability distributions without consideration of robustness in other works where uncertainty is represented as a set of alternative future states of the world e g jeuland and whittington 2014 ray et al 2018 robustness is addressed a posteriori by re evaluating pre defined system configurations over multiple future climatic and non climatic uncertainties this paper introduces a simulation optimization framework that addresses known roa limitations and combines roa principles with robustness analysis to enable adaptive and robust infrastructure planning while exploring the trade offs between multiple objectives we extend the classical multistage stochastic capacity expansion problem ruszczyński and shapiro 2003 where corrective decisions allow the model to compensate for insufficient or excessive investment made at earlier decision stages this way water plans identified are adaptive in that they flexibly activate delay and replace interventions to adapt to the future uncertainties and are robust in that they perform satisfactorily well over a range of plausible future conditions uncertainties are either represented by a scenario tree where each scenario represents a probabilistically weighted future state aiming for adaptation or through a range of plausible equiprobable future states aiming for robustness we consider multiple objectives to explore trade offs inherent in society s conflicting goals for water resources systems and to help identify synergies co benefits between different measures of performance the next section describes the proposed approach the scenario tree construction and the adaptive and robust multi objective optimization formulation section 3 describes an application to a global city in section 4 the case study s results are presented and discussed in section 5 section 6 concludes the paper 2 method the proposed framework seeks both adaptability and robustness in addressing uncertainty and multi dimensional efficiency for dealing with competing societal goals we build roa principles into the planning framework by using a scenario tree to enable adaptability of the investment decisions that can be modified to accommodate changes as they materialize maier et al 2016 roa principles involve various options such as delaying investments in the interest of acquiring information evaluating phased investments and modifying or abandoning actions through the consideration of an ensemble of future states robustness is built into the planning framework by ensuring solutions perform acceptably well over a range of future conditions herman et al 2015 maier et al 2016 fig 1 shows an overview of the proposed framework we use a physically based water resource system simulation which for different plans tracks multiple performance metrics over time the water resource system simulations are coupled with a multi objective evolutionary algorithm moea coello et al 2007 nicklow et al 2010 to identify those adaptive plans on the scenario tree that are also robust across an ensemble of future states and that appropriately trade off various decision relevant interests 2 1 trade off formulation for adaptive and robust planning for adaptive analysis we randomly generate a set of multiple forecasted values of the uncertain parameter and use a scenario tree construction technique to generate a scenario tree we construct the scenario tree by implementing an optimization problem that minimizes the so called probability distance between the uncertainty sets following the algorithm presented by gröwe kuska et al 2003 the problem takes the original uncertainty set and produces a scenario tree out of it for multistage decision making the original uncertainty set in our study is the demand uncertainty that is defined by the bounds of urban water demand forecasting models the algorithm optimally creates a scenario tree by successively bundling the tree nodes into separate sets to be represented by a new node while maintaining the probability information of the constructed tree as close as possible to the original uncertain stochastic process the quality of the constructed tree is controlled by a metric that calculates the percentage of information lost known as relative probability distance heitsch and römisch 2011 this is set to 5 in this study as we assume that this is an acceptable loss of information the tolerance indicates how well the constructed tree approximates the original stochastic process and consequently determines the number of scenarios that are embedded in the scenario tree erfani et al 2018 demonstrated this scenario tree construction method in a water supply capacity expansion problem the multi objective search algorithm then uses the scenario tree to implement roa principles as discussed above while this enables adaptive water resource investment decisions the plans are not necessarily robust to uncertainty to seek for robustness the framework has the ability to represent uncertainty as an ensemble of future states and exploit them to identify adaptive plans that are also robust to these uncertainties to achieve this the framework searches for plans across an ensemble of future states on each branch of the scenario tree using a predefined robustness measure different measures of robustness have been used in the literature lempert et al 2006 herman et al 2015 mcphail et al 2018 each of which allows stakeholders to achieve a different performance requirement 2 2 mathematical formulation below we formulate the multi objective optimization problem that allows implementation of roa principles for adaptive and robust planning let n be the set of nodes on the scenario tree that structures the evolution of the uncertain parameters and z be a vector of the uncertain parameters that are represented as an ensemble of future states the formulation below obtains optimal investment decisions for each node of the scenario tree that are robust to z 1 m i n i m i z e f x z f 1 f k 2 s t i m e d s n i 1 n n t 3 d s n i 0 1 n n t i i 4 x d s n i in the above formulation f x z is the vector of k objective functions f i i 1 k that each is aggregated based on the robustness metric used over an ensemble of z states x is a decision variable vector representing a set of interventions k is the number of objectives for trade off analysis i is the set of all interventions n t is the set of nodes belonging to stage t d s n i is a binary variable denoting if intervention i in portfolio x is selected or not in node n of the scenario tree and m e represents the set of mutually exclusive interventions fig 2 shows a flowchart of the proposed framework in step 1 the search algorithm generates an initial random population of candidate plans on the scenario tree each plan is a portfolio of interventions sequenced over time at each node of the tree a plan could involve for instance a mix of supply side and demand side interventions each with their associated costs and different impacts on supply capacity expansion and demand reduction respectively the population is generated making sure that plans have the same interventions on the node they share to prevent an implemented option from becoming unavailable in subsequent stages of the scenario tree each option has a construction lead time that establishes the period between the decision to implement an option and becoming operational in step 2 each plan in the population is passed to a water resource system simulator to be evaluated over an ensemble of future conditions z these future conditions could include hydrological flows or demand forecasts over the planning period the performance of each solution relative to the objective functions is evaluated in each simulated scenario this results in different performance metrics across z that are then aggregated and passed to the search algorithm as objective function values step 3 analysts can specify a stopping criterion which if satisfied terminates the search process like a pre defined number of iterations or a convergence metric until then the search algorithm employs population update strategies to select interventions for the next evolution step 4 this results in a set of adaptive and robust plans that trade off multiple objectives step 5 the different efficient plans generate a pareto frontier allowing planners to examine the trade offs for example between investment costs and different facets of system performance 3 application 3 1 background we apply the proposed trade off informed framework for adaptive and robust planning to the london urban water supply area which is located in the thames river basin in southeast england the water supply is managed by thames water a private water utility serving 15 million customers across london and the thames valley the region has a relatively high population density and faces a projected 25 increase in population by 2040 thames water 2014 environment agency 2013 however the actual population growth is uncertain making it a suitable case study to investigate the use of the proposed approach furthermore water utilities and regulators in england and wales are considering a move from a traditional single objective least cost optimization approach ukwir 2016 padula et al 2013 to identifying a best value plan that balances multiple performance criteria and seeks adaptability thames water 2019 environment agency 2021 this is because single objective approaches require all metrics to be aggregated commensurated into a single metric typically monetary and their preferences over one another explicitly articulated in a single metric this results in a single optimized solution that can potentially lead to imbalanced and unpopular decisions matrosov et al 2015 multi objective approaches allow incorporating different metrics with different units of measure without a need to either aggregate them or pre specify their preferences this results in a set efficient i e best available trade off solutions hence the proposed multi objective approach is likely to be appealing to water utility planners in the uk and beyond this study uses a 2020 2070 planning period in which water supply management intervention decisions are made every 5 years following water company regulation in england and wales we consider 11 new supply of type reservoir transfer waste water reuse desalination and 4 new demand management interventions of type active leakage control pipe repair water efficiency metering for the global city s water resource system shown in table 1 each option has characteristics related to its ability to store and or manage water construction period design life and mutual exclusivity unlike aggregated supply demand modeling approaches where interventions contributions to supply expansion or demand reduction is a single number yield in the optimization padula et al 2013 erfani et al 2018 in this approach physically based supply interventions and their operating rules are simulated over time whilst demand management options reduce aggregate annual demand release from supply options during droughts occur according to london s seasonal lower thames control diagram ltcd refer to matrosov et al 2011 release from reservoir and groundwater options is subject to available storage while desalination and reuse options release water indefinitely as needed the storage capacity of modular supply interventions can be expanded at a future stage by paying a relevant expansion cost for instance the proposed reservoir can be built with a fixed res or modular resm storage capacity listed in table 1 as two separate interventions that are mutually exclusive i e at most one of the two interventions can be selected the non modular res intervention builds a 150 mm 3 reservoir that releases 267 ml d when activated during droughts periods of low storage and low flow in the thames the modular resm intervention initially builds a reservoir of 100 mm 3 storage capacity which can be expanded to 150 mm 3 at a later decision stage if required for the modular option the utility has to pay a premium upfront to reserve the right for further expansion see erfani et al 2018 for a synthetic example of a reservoir option demonstrating this roa principles this study uses the interactive river aquifer simulation iras 2010 model of the regional water resources and supply system matrosov et al 2011 which tracks flows and storages spatially with a weekly time step options that provide or save less than 10 ml day were ignored and lead times of options related to design and planning permission were not considered therefore results in section 4 are approximate and attempts to compare them directly with twul s published plan which considers more detailed data would not be valid 3 2 addressing uncertainties in this study we use a scenario tree to structure the uncertainty about future water demands for adaptive planning where branching is allowed from one demand scenario to another however each member of the hydrologic ensemble presented in section 3 2 2 is independent and therefore the time series associated with one ensemble member can only be compared with the same ensemble member time series for a different time slice prudhomme et al 2013 each future flow time series represents a unique condition of climate uncertainty and therefore once a simulation begins under one time series that same time series must be used until the end of the planning period to maintain the hydrological consistency of each supply state this implies a scenario tree cannot be used to represent the supply related uncertainty in this case and future climate change supply impacts are represented as an ensemble of equally likely hydrological flow time series in our case study the ensemble of streamflows included nonstationary hydrological conditions where extreme hydrologic events like droughts can be experienced at any point of the planning horizon if a scenario reduction technique were used to bundle hydrological scenarios that would potentially result in droughts happening in consecutive time periods for some branches of the tree and therefore biasing the timing of activation for interventions following the framework discussed earlier this implies the planning decisions made in this application will be adaptive to demand uncertainty and robust to supply uncertainty the proposed approach does not prescribe how uncertainties should be represented allowing planners to decide based on the structure of the data whether to seek adaptability scenario tree or robustness ensemble of future states adaptability allows for learning over time while robustness ensures the plan performs acceptably under a wide range of plausible future conditions in most cases how to structure the uncertainty modeling is dictated by available data or the nature of the problem for instance if in a hydrological time series the temporal correlation were weak supply uncertainty could be represented as a scenario tree where time slices from different time series would be mixed and matched to create branches of the tree heitsch and romisch 2005 latorre et al 2007 séguin et al 2017 therefore allowing adaptive decisions to learn from supply variability if not hydrological flows could be represented as spatially and temporally coherent future time series 3 2 1 demand uncertainty scenario tree the demand scenario tree is extracted out of the london demand uncertainty space as shown in fig 3 a the demand uncertainty is approximated by 21 scenarios shown in fig 3 b it is also drawn out in panel a let t be the assumed time horizon and t t the decision points spaced at regular time intervals the decision in t 1 which is the same for all scenarios is called the here and now decision and is made in the first time step before any uncertainty is realized in the subsequent decision points wait and see decisions are made with the information about uncertainty which is revealed in previous stages these decisions are adaptive in that they can vary with the uncertain parameters and can take different values in each scenario for instance in the context of our example at t 2 the demand uncertainty of the first stage is revealed we make the decision in t 2 for each scenario with the benefit of knowing the value of the uncertain parameter in t 1 but with no other knowledge of future data this proceeds until we make the decision in the last time period t with the benefit of knowing the values of the uncertain parameter at t 1 therefore adaptive decisions are not locked in that they are modified based on new information the here and now decisions are also robust to supply uncertainty since they are optimized based on an aggregation of a hydrological ensemble so that it performs acceptably well robustness however treat solutions as locked by aggregating their performance given a set of ensembles non anticipative constraints link here and now and wait and see decision variables belonging to different scenarios for instance at t 2 the demand uncertainty is represented by three decision nodes implying that the three investment decisions are the same within some subgroups of scenarios which are indistinguishable based on the information available up to that point it can be seen that the subgroups of scenarios s 1 s 9 s 10 s 12 and s 13 s 21 share common investment decisions at t 2 this is enforced by non anticipative constraints that ensure investment decisions at time t only utilize any information that is available up to this stage the proposed methodology exploits the tree structure to provide flexibility in allowing initial water resource investment decisions to adapt to future changes in water demand erfani et al 2018 this multi period decision process enables the virtual planners implied by this model based approach to modify or delay investment plans as information on future demand is gradually revealed 3 2 2 climate projections uncertainty to illustrate the water resource systems behavior during the eventual future climate change impacted supplies supply uncertainty is represented by a set of transient climate change forced daily river flow and monthly groundwater levels for the uk prudhomme et al 2013 available from the national river flow archive online database the scenarios represent equally probable hydrological flows and were derived from the set of transient climate projections obtained from the met office hadley centre regional climate model hadrm3 ppe by dynamically downscaling the global climate model the dataset consists of an ensemble of 11 equally probable flow time series for the thames basin between 1950 and 2098 prudhomme et al 2013 3 3 case study formulation the case study problem formulation uses the following vector of objective functions 5 f x z f 1 f 2 f 3 f 4 where z is the ensemble of hydrological flows and f 1 f 4 are the objectives considered for trade off analysis the first objective f 1 minimizes the total capital and operational cost of implementing new supply and demand interventions in a portfolio the cost is annualized and discounted with discount rate r over the planning time horizon and weighted by the probability p n derived from the scenario tree construction algorithm that node n is realized using 6 f 1 t t n n t i i p n 1 r t t c i d l i d s n i where t c i is the total discounted cost capital and operational of implementing intervention i in node n at time stage t and d l i is the design life of intervention i the costs are normalized to each intervention s expected design life by dividing the investment cost of each intervention by its expected lifetime the use of total investment cost per year allows for equal comparison between interventions that have unequal design lives the second objective f 2 maximizes system service resilience which is defined by how quickly the system recovers from a failure moy et al 1986 a definition of a failure is problem dependent in this study a failure associated with the service resilience objective eq 7 occurs when the london aggregate storage level drops below a certain threshold ltcd level 3 and a non essential water use ban is imposed the aim of the objective is to minimize the maximum duration of the imposed partial water use ban the average discounted maximum duration of the failure across all scenarios weighted by their probability w s is then minimized using 7 f 2 s s w s max t 1 r t d s t where w s is the probability of scenario occurrence d s t is the duration of failure in scenario s in time t the probability of scenario occurrence is derived based on the probabilities of the scenario tree nodes and is calculated by multiplying all state transition probabilities on the scenario path eq 8 8 w s p n n n s n s n where n is the set of all nodes on the demand scenario tree and n s is the set of nodes that belong to the path of scenario s the third objective f 3 is to maximize system reliability which is calculated based on how frequently the system fails the average discounted frequency of no failures across all scenarios weighted by their probability w s is maximized using 9 f 3 s s w s 1 t t 1 r t z s t t s t 100 where z s t is the number of time steps weeks the system was in failure in time period t in scenario s and t s t is the number of time steps weeks in time t we choose to minimize ltcd level 2 failures where a water saving media campaign is imposed to reduce demand the fourth objective f 4 reflects how well a plan meets a desired level of service los requirement over the considered future scenarios we calculate the fraction of future scenarios where a plan performs within the desired los 10 f 4 s s k s s 100 where the binary variable k s is 1 if a plan in a scenario performs within a required los or 0 otherwise and s is the total number of scenarios since the time dimension is not present in eq 10 the performance of this objective is not discounted an objective in our case study is to minimize the number of scenarios in which ltcd level 3 failure frequency exceeds 1 in 20 years eq 10 we implicitly optimize vulnerability i e limiting the magnitude of failure by using objectives relating to different ltcd failure levels los3 resilience los2 reliability and los3 return period requirement thames water uses these different failure levels as thresholds to implement escalating supply and demand management measures in this application robustness is sought both by ensuring insensitivity to future conditions by optimizing the average performance over an ensemble of futures for total cost system service resilience and system reliability and by minimizing ltcd level 3 failure occurrences that exceed a 1 in 20 year frequency for the climate impacted flow scenarios the average statistic was selected to provide a relatively stable performance the performance objectives of cost reliability and service resilience are all discounted to not bias financial metrics within the search the financial costs are net present values npv of capital and operational expenditures incurred by implementing new interventions and using them if only financial metrics were to be discounted in multi objective problems expensive and high performing assets would tend to get selected only towards the end of the planning period as they would offer high benefit at a largely discounted cost the use of equal rates of discount for both monetary and non monetary performance is suggested in literature when benefits are hard to monetize keeler and cretin 1983 therefore the engineering performance objectives were discounted using the same discount rate as the financial objective to ensure an equal rate of time preference these were calculated in accordance with the uk government s green book discount rates which reflect the rate at which society values the present compared to the future treasury 2003 to identify the approximate pareto frontier we use the epsilon dominance non dominated sorting genetic algorithm ii e nsgaii kollat and reed 2006 that has been shown to effectively solve complex many objective optimization problems reed et al 2013 further computational details of how the search process was conducted are found in the supplementary materials see appendix a 4 results 4 1 solving the multi objective adaptive and robust water resource planning problem fig 4 shows the approximately pareto optimal adaptive and robust water resources city plans projected onto the two dimensional cost service resilience trade off space each of the 329 points represents a unique pareto approximate adaptive and robust plan proposing a set of investment options for each decision node of the scenario tree over the 50 year planning period the size of the points shows the reliability objective i e the average percentage of years in a solution that did not have an ltcd level 2 failure while its color shows the percentage of scenarios that meet the los3 return period the investment cost is lower for adaptive and robust plans with lower service resilience reliability and fraction of scenarios that meet required los return period implying that more investment is required to improve performance of the infrastructure service objectives the pareto approximate space is roughly divided into three zones with respect to their performance across the objectives if a decision maker requires that the percentage of scenarios that meet the los3 return period is at least 90 then they must choose a solution from the third zone observed in the right portion of the pareto front our analysis is seeking to identify which infrastructure choices can achieve magnitudes of potential failures deemed acceptable by stakeholders three efficient solutions points on the pareto front in fig 4 are singled out for further analysis to reflect different plausible preferences of decision makers the adaptive and robust plans that correspond to these selected solutions are referred to as arp 1 arp 2 and arp 3 arp 1 is the lowest cost solution where at least 90 of scenarios achieve los3 failures occurring at most every at most 1 in 20 years arp 3 is a high cost solution where all scenarios meet the los3 return period requirement while arp 2 displays an example of a balanced solution between the conflicting objectives more risk averse decision makers may select arp 3 where financial performance low cost is traded in to obtain higher performance in infrastructure services service resilience reliability and vulnerability the investment trajectories for the three selected plans of the pareto front together with their performance values in terms of four objectives are shown in fig 5 the tree consists of 21 possible investment trajectories for each adaptive and robust plan based on the demand scenarios depicted in fig 3 b decisions to invest in a set of interventions are made at the beginning of each time interval t since decision points are spaced at 5 year time intervals an activated intervention becomes available either at the same or in a future time interval depending on the length of its construction period fig 5 details the short term t 1 t 4 i e decisions made in 2020 2025 2030 and 2035 investment decisions of the three adaptive and robust plans represented by black gray and white boxes showing whether a portfolio that consists of an intervention 1 11 or a combination thereof is activated at the beginning of each 5 year time interval under different demand scenarios differentiated by demand thresholds the potential supply side and demand side water resource options that can be activated at each node of the tree are detailed in table 1 demand threshold values displayed on each branch indicate which path is optimal for a given demand at each interval for instance if the planner considers that demand for water in 2025 is most likely to be less than 2030 ml d low demand growth then the lower path is the best intervention response if demand is between 2030 ml d and 2084 ml d moderate demand growth then the middle path is optimal whilst if demand is 2084 ml d or greater high demand growth then the upper path should be selected the interventions that are activated at the beginning of the tree root node should be implemented in the first time period and are selected in all demand scenarios the most pressing concern of water planners is short term investment decisions i e what to do now however these near term decisions must be compatible with future investments and the resulting investment trajectories of which they are a part must demonstrate long term water supply security under different future scenarios initial investment trajectories resulting from differing near term decisions can follow a range of future branches based on future decisions the decision determining which subsequent branch a planner should follow can be taken as new information on demand growth becomes available such results enable adaptive investment planning where initial investment decisions can be postponed and adjusted according to future possible demand conditions the scenario tree approach results in wait and see strategies that seek to delay interventions until they are required this wait and see strategy is a manifestation of real options based principles the opportunity to defer investments enables planners to reduce overall intervention costs and make more informed investment decisions as new information is unveiled the ability to defer investments enables planners to reduce overall intervention costs for example as seen in fig 5 for all three adaptive and robust plans the decision in 2025 t 2 to invest further increase supply or reduce demand is postponed for later if demand is expected to be low or moderate if demand in 2025 is expected to be high then planners can invest in further actions infrastructure or demand management depending on which adaptive and robust plan they select each adaptive and robust plan has alternative sets of interventions scheduled for implementation for instance as shown in fig 5 at the root node arp 1 2 and 3 activate portfolio 1 2 and 3 respectively consisting of the same mix of demand management options namely active leakage control alc smart metering met and pipe repair campaign pip in the more expensive arp 2 and 3 a large supply scheme is also selected alongside the demand management interventions portfolio 2 builds river severn transfer rst while portfolio 3 builds res the interventions selected in 2020 t 1 are active across all 21 scenarios in 2025 t 2 if demand is low no further investment is suggested for the short term period until 2035 t 4 where all adaptive and robust plans activate canal transfer portfolio 7 if demand in 2025 is moderate reuse scheme b rsb is activated either in 2030 t 3 or 2035 t 4 depending on the adaptive and robust plan if demand in 2025 is high the initial portfolios are all expanded differently but in 2030 t 3 the same reuse scheme a is activated in all adaptive and robust plans in practical applications at the next decision stage i e 5 years later the optimization should be performed again with the newly available demand scenarios that could be different from the original ones creaco et al 2013 to gain more insight on how the pareto approximate adaptive and robust plans differ we plot in fig 6 the long term t 1 t 10 investment decisions for the fixed size reservoir res across the three selected adaptive and robust plans in the low cost arp 1 res is activated in high demand growth scenarios scenarios 16 21 midway through the 50 year planning period in the more balanced arp 2 res is activated earlier on in the same scenarios as arp 1 as well as in others 11 out of 21 scenarios in arp 3 res is built across all scenarios 21 out of 21 as it is activated in the first time period the ability to defer or avoid an expensive or potentially controversial option shows the benefit of combining multi objective optimization with roa fig 7 depicts the activation frequency of the interventions across the 21 demand scenarios in each time step for the long term planning problem the combination of interventions in each adaptive and robust plan the time of their implementation as well as its activation frequency across the scenarios is plotted as expected from fig 5 active leakage control alc pipe repair pip and metering met are activated in all scenarios in all three adaptive and robust plans from the start of the planning period this suggests that these demand interventions are robust across the multi objective trade off as well as supply demand uncertainty indicating that demand management should be put in place early in the planning period larger interventions such as trf and res are activated in all three adaptive and robust plans at different points and frequency across the scenarios while arp 1 suggest their activation midway through the planning period arp 2 and 3 activates large interventions earlier and at a higher frequency across the scenarios this indicates that the development of both large supply schemes is recommended at some point during the next 50 years above a certain demand growth level below that level roa helps planners avoid unnecessary costly investments 4 2 activation comparison of large interventions to examine the selection of the two large interventions res and trf we compare their activation across all pareto approximate solutions fig 8 shows the same pareto approximate solution set as fig 4 displaying the values of three objectives cost resilience and reliability the color here corresponds to the activation frequency of the two large interventions across the 21 scenarios during the planning period a black circle informs that by the end of the planning period the intervention is selected all 21 scenarios while a white circle reports that the intervention is not selected in any the interventions display a different activation ratio across the 329 solutions res is selected in 155 of solutions while trf in all 329 by the end of the planning period however above a certain level of resilience 2 8 weeks res is always selected and the frequency of its activation increases for more resilient solutions in the case of trf some solutions with high resilience do not require the intervention to be active in all scenarios at a high frequency this shows that to achieve a certain required level of resilience res must be selected relatively early in the planning period high frequency of scenarios where intervention is activated shown by the black color while the activation of trf could be postponed for later and activated only under certain demand conditions 4 3 metrics for adaptability assessment to help evaluate solutions and quantify what has been gained by this analysis we adopt two metrics from the field of stochastic programming birge and louveaux 1997 escudero et al 2007 namely value of the stochastic solution vss and expected value of perfect information evpi into decision relevant metrics of adaptability and flexibility in water planning decisions to examine the implications on adaptability and flexibility across the cost resilience trade off we compare the same three 5 year plans i e arp 1 2 and 3 from the previous section vss and evpi enable comparing the plans in terms of their ability to adapt to changing conditions and in terms of the value gained from delaying irreversible investment commitments respectively this follows erfani et al 2018 who use the vss and evpi metrics to quantify the benefits of adaptability and flexibility for a single objective water resource planning approach appendix b details the calculation of vss and evpi for the multi stage formulation proposed here the vss and evpi for the three plans as a percentage of the total cost of each adaptive and robust plan are shown in fig 9 in this case study vss illustrates the difference made by using the robust multi objective multistage approach which explicitly allows for periodical adaptation to different future demand conditions instead of ignoring the uncertainty around the demand values in each stage stated in a different way vss quantifies the cost of not recognizing the uncertainty and therefore ignoring the adaptability advantage for the london case study vss expressed as a percentage of the total cost is higher for the most resilient and reliable arp 3 18 compared to arp 1 12 and arp 2 5 this indicates seeking an adaptive and robust city planning approach can have considerable financial value in this case study evpi estimates the value of improving demand forecasting again the higher evpi for arp 3 14 compared to arp 1 10 and arp 2 4 5 shows that for the more resilient and reliable adaptive and robust plan there is high value in investing in demand forecasts 5 discussion to illustrate the benefits of the proposed adaptive and robust multi stage multi objective approach applied to london s water supply planning three efficient adaptive and robust plans were selected and compared in all three of these plans three demand management interventions are recommended for implementation in 2020 they should be implemented regardless of demand growth since they are selected at the beginning of the tree large supply interventions are only activated in 2020 in the more expensive arp 2 and 3 which provide better performing services than arp 1 if demand in 2025 is below 2 141 ml d then the previously selected interventions remain active without the need to invest however if 2025 demand in 2020 is predicted as more than 2 141 ml d further investment is required to maintain desired levels of service in arp 1 and 2 by the end of the planning period all three selected optimal adaptive and robust plans implement both large supply interventions res and trf in the top path of the tree the one with the least favorable supply demand balance showing that under higher demand conditions the selection of the two large schemes occurs across the multi objective trade off regardless of preference these high level results summarize how the approach works and how its results which consider many different decision maker and societal priorities and many different futures are interpreted as encouraged by wreford et al 2020 this study shows the applicability of scenario based roa methods to adaptive climate change infrastructure investment analysis the use of a scenario tree is appropriate for cases where planning is performed regularly over discrete time intervals as is often the case in water supply and river basin planning where agencies emit planning reports on a regular basis e g every 5 years in our case study country since plans are regularly re optimized in each 5 year water supply planning cycle the only consequential decision is the decision made in the first decision time step the initial planning decisions are optimally adapted to future conditions by modifying or delaying investments as more information on future conditions is gradually revealed this definition of adaptability is unique to a multistage stochastic implementation of roa the applicability of roa for climate change adaptation studies has been challenged because of concerns that assigning weights to scenarios in roa is problematic shortridge and camp 2019 and that expected values are not meaningful summaries of option value kwakkel 2020 the proposed approach represents adaptation by applying roa principles without having to assign probabilities to each scenario for instance in this study the scenarios were randomly generated from the uncertainty space using a scenario tree construction method that is the scenario tree construction could generate multiple trees from the same uncertainty source data with different number of nodes at each time step with a calculated probability as well as different branching structure on the topic of expected value this is not a required attribute of our approach which allows consideration of multiple metrics indeed our case study does not use expected values for all objectives multistage stochastic solutions are adaptive in that here and now t t 1 planning decisions that do not depend on future observations are corrected through wait and see t t 1 decisions for each realization of the scenarios in subsequent pre determined stages in the time horizon birge and louveaux 2011 multistage stochastic models use sequential multi period decision making but not all sequential decision making is multistage stochastic examples of sequential decision making include the scheduling formulations of padula et al 2013 beh et al 2015 borgomeo et al 2016 and borgomeo et al 2018 what makes multistage stochastic different from other multi period decision making is the inclusion of a sequence of wait and see decision variables that are used as recourse to here and now planning decisions to capture the evolving information over time dapp solutions can react to the state of the system using signposts and triggers at any moment of a simulation not at pre specified time steps methods aiming to inform management decisions usually involve assumptions and limitations around the uncertainties in our case the supply data used independent non stationary climate scenarios that did not allow us to mix time slices of different hydrological time series and hence they could not be used to construct a scenario tree for adaptive supply analysis we therefore produced plans adaptive to demand and robust to supply uncertainty the proposed framework as explained in section 2 2 however is general and could consider multi dimensional scenario trees where the evolution of different sources of uncertainties not just demand are considered uncertain parameters could include the probability of hydrological scenarios as well as economic aspects such as discount rates the way that uncertain dimensions of the decision problem are modeled must be informed by problem structure and data availability note that following huskova et al 2016 and to limit the number of function evaluations necessary for the optimization to converge each intervention in our application had a fixed capacity i e the size of investments is not optimized table 1 alternative capacities for the same interventions could be added as new interventions if just a few sizes are being considered optimizing the capacity of interventions across their entire range would result in more efficient adaptive and robust plans but is left to future work 5 1 sensitivity analysis we test the sensitivity of the proposed actions of the three selected adaptive and robust plans to the use of different scenario trees the london case study was run for thirty different and optimally generated scenario trees from the stochastic london demand distribution from the same uncertainty source from each run we select an adaptive and robust plan that exhibits a similar trade off between the four objectives according to arp1 2 and 3 the results of the sensitivity analysis shown as a bar chart in fig 10 illustrate the activation frequency of the options in the first planning decision period t 1 for all three adaptive and robust plans most interventions suggested by the planning approach have a high frequency of selection more than 80 indicating that recommendations are not sensitive to the structure of the scenario tree when extracted from the same uncertainty space this finding is in accordance with the stability theory of multistage stochastic programming that suggests that multistage stochastic model results are stable with respect to the perturbation of the stochastic input process heitsch et al 2006 heitsch and römisch 2009 5 2 robustness analysis we perform a regret based analysis of the selected adaptive and robust plans to assess the deviation from optimality when simulated under a different ensemble of flow scenarios to create the new ensemble of hydrological flows we resample the nonstationary time series via bootstrapping as per paparoditis and politis 2002 for resampling of nonstationary time series fig 11 shows box plots of normalized values of regret across the 4 objectives for arp1 2 and 3 regret r for a criteria c is calculated as the difference in the performance p of the portfolio in the best performing scenario s b p c s b and that of the performance of the same plan in the scenario in question s p c s given by 11 r c p c s b p c s the adaptive and robust plan with smaller infrastructure options arp1 performs best in cost regret while the more expensive arp2 has the highest cost regret values because the options activated early in the planning period incur high operational cost values 6 conclusions given unknown future water supply and demand conditions growing global populations and the push to invest in infrastructure to prevent the worst impacts from climate change the problem of intervening in water resource systems under uncertainty has taken on a renewed urgency considering conflicting objectives while addressing uncertainties in developing human natural resource systems has become a frontier problem of water science by explicitly accommodating the aspiration for adaptability robustness and the need to balance different water system goals we feel the proposed approach is a useful contribution to water management this paper proposed an approach for multi objective real options based adaptive and robust planning of water interventions under uncertainty a scenario tree is extracted to represent the uncertain parameters which can be branched from one state to another and the uncertainties that cannot be defined by the scenario tree are considered as an ensemble of future states a multi objective search engine uses an independent water resource simulator to identify optimal interventions for each node of the structured scenario tree and evaluate the plans over an ensemble of future conditions the obtained pareto approximate investment plans are adaptive and robust to the uncertainties considered the framework was demonstrated on london s water resource system demand uncertainty was represented through a scenario tree while supply uncertainty through an ensemble of hydrological flows demand growth threshold values inform decision makers which investment trajectory would be best to follow to optimize cost service resilience reliability and vulnerability of the water supply system the plans are robust in that they are insensitive to different future hydrological conditions and adaptive in that commitments being made in the short term allow for choosing future actions based on how future demand growth unfolds the application of this real options based definition of adaptability to a multi objective water infrastructure planning problem in combination with robust optimization is novel for regulated utilities with a regular planning cycle the proposed adaptive and robust water infrastructure approach is institutionally appropriate since our definition of adaptability uses a set of pre determined time steps to demonstrate the use of the outputs of the proposed planning framework three pareto approximate solutions were selected and compared depending on how the planner believes short term demand growth will evolve and their preference on the multi objective performance trade off they select which plan to implement including whether interventions should be delayed the flexibility and adaptability assessment of the three adaptive and robust plans quantified the benefit of considering supply and demand uncertainty by calculating the stochastic value i e expected loss of using a deterministic solution and the value of knowing more about the future for more costly plans that provide higher performance in infrastructure services service resilience reliability and vulnerability the importance of handling uncertainty and having better information increases in our case london s net present value of required investments was reduced by up to 18 also the option value of best delaying investments to wait for better predictions was shown to be worth up to 14 of total npv these results help quantify the value of a roa approach this paper shows global cities can and should appropriately balance their investment performance trade off and that explicitly optimizing robustness and adaptability of water supply systems can increase future service levels at a lower cost increased societal resilience in the face of climate change is an aspiration of many cities world wide as they look to invest in resource security this paper describes a new route for investing in complex real world water systems despite future uncertainties in a systematic transparent and beneficial manner credit authorship contribution statement kevis pachos conceptualization methodology software writing original draft validation visualization ivana huskova methodology software evgenii matrosov methodology software writing review editing tohid erfani conceptualization writing review editing supervision project administration funding acquisition julien j harou conceptualization writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funding from thames water the ucl department of civil environmental and geomatic engineering doctoral training scheme and ukri s futuredams project es p011373 1 are gratefully acknowledged anna wallen and chris lambert provided comments that improved the paper data used in this study is available in the technical appendices on the thames water website https www thameswater co uk about us regulation water resources appendix a computational implementation to solve the problem explained in section 3 3 we use the epsilon dominance non dominated sorting genetic algorithm ii e nsgaii kollat and reed 2006 we ran the multi objective optimization ten times each starting from a unique population using different random seed value to account for the variability of the initial populations and to best approximate the pareto front previous studies have used similar number of seeds to reduce the computational burden while ensuring that the influence of random number generation on the results is insignificant huskova et al 2016 the initial population size was set to 512 and the algorithm operator parameters were chosen according to previous study recommendations kollat and reed 2007a matrosov et al 2015 each optimization was run for 25 000 function evaluations or until a convergence metric was satisfied table 2 summarizes the algorithm parameters including the objective ε values used for the case study the ε values were selected to capture the minimum level of precision desired in differentiating between the performance of one portfolio alternative and another in each objective 1000 0 1 0 5 and 0 5 for cost resilience reliability and fraction of scenarios that meet required los respectively to determine each run s convergence we used a hypervolume metric zitzler 1999 as the termination criterion which is widely used in multi objective optimization problems as it captures the diversity and convergence of solutions reed et al 2013 fig a 12 shows how the search algorithm has converged after 120 generations appendix b computational insight on the metrics used to evaluate the multi period moea the value of the stochastic solution vss is calculated for each plan by replacing the decision variables i e activation of intervention in each planning interval with expected values and comparing the cost requirements between the two by fixing the first stage interventions and solving for all the scenarios the moea will generate a new set of pareto adaptive and robust plans the expected value of perfect information evpi is estimated by computing the cost difference between the expected value with perfect information and the expected value with current information for each pareto adaptive and robust plan in order to allow comparison of vss and evpi values we select plans with the same level of resilience reliability and fraction of scenarios that meet required los the calculations of vss and evpi in multi stage problems are explained below these two metrics were developed for the case of two stage problems birge and louveaux 1997 and have been extended to multi stage problems escudero et al 2007 we calculate their values in a multi stage framework with two objectives for each pareto approximate solution individually vss and evpi are calculated using the cost values of the solutions that correspond to a given level of resilience reliability and fraction of scenarios where a plan performs within the desired los vss is calculated by solving the mean value problem resulting in a pareto set of first stage solutions evpi is determined by computing the cost difference between the expected value with perfect information and the expected value with current information for the minimization model the following inequalities are satisfied b 1 w s a p e v where w s denotes the expected value of the objective function obtained by replacing all random variables by their expected values w s is known in the literature as the wait and see resolution value a p denotes the optimal solution value to the adaptive and robust multi stage stochastic problem presented in this paper e v denotes the expected result of expected value problem and measures how the optimal solution of the expected value problem performs allowing the other stages decisions to be chosen optimally as functions of different scenarios from eq b 1 evpi and vss are calculated as follows b 2 e v p i a p w s b 3 v s s e v a p to calculate the evpi non anticipative constraints are relaxed at each time step so that decisions are made with perfect information about the future from eq b 2 the difference a p w s displays the value of perfect information from eq b 3 the difference e v a p known as the vss indicates the benefit of finding different solutions for each scenario by solving the stochastic program than to assume lack of uncertainty in the work of escudero et al 2007 those parameters are generalized to the multi stage case explained below let the expected result in t of using the expected value solution denoted by e v t for t 2 t be the optimal value of the a p model where the decision variables until stage t 1 x 1 x t 1 are fixed at the optimal values obtained in the solution of the average scenario model for any multi stage stochastic program the following relations hold e v t 1 e v t t 1 t 1 0 v s s t v s s t 1 t 1 t 1 the value of the stochastic solution is defined in t denoted by v s s t as v s s t a p e v t t t this sequence of non negative values represents the cost of ignoring uncertainty and not providing adaptive and robust solution to future conditions until stage t in the decision making of multi stage models vss and evpi in multi stage problems are then calculated as b 4 v s s t t v s s t and b 5 e v p i t t e v p i t 
197,we present a hybrid mixed finite element method for a novel hybrid dimensional model of single phase darcy flow in a fractured porous media in this model the fracture is treated as a d 1 dimensional interface within the d dimensional fractured porous domain for d 2 3 two classes of fracture are distinguished based on the permeability magnitude ratio between the fracture and its surrounding medium when the permeability in the fracture is significantly larger than in its surrounding medium it is considered as a conductive fracture when the permeability in the fracture is significantly smaller than in its surrounding medium it is considered as a blocking fracture the conductive fractures are treated using the classical hybrid dimensional approach of the interface model where pressure is assumed to be continuous across the fracture interfaces while the blocking fractures are treated using the recent dirac δ function approach where normal component of darcy velocity is assumed to be continuous across the interface due to the use of dirac δ function approach for the blocking fractures our numerical scheme allows for nonconforming meshes with respect to the blocking fractures this is the major novelty of our model and numerical discretization moreover our numerical scheme produces locally conservative velocity approximations and leads to a symmetric positive definite linear system involving pressure degrees of freedom on the mesh skeleton only as an application we extend the idea to a simple transport model the performance of the proposed method is demonstrated by various benchmark test cases in both two and three dimensions numerical results indicate that the proposed scheme is highly competitive with existing methods in the literature msc 65n30 65n12 76s05 76d07 keywords hybrid mixed finite element method fractured porous media hybrid dimensional model 1 introduction numerical simulations of single and multi phase flows in porous media have many applications in contaminant transportation oil recovery and underground radioactive waste deposit due to the highly conductive and blocking fractures in the porous media underground it is still challenging to construct accurate numerical approximations matthäi et al 2010 vasilyeva et al 2019 golian et al 2020 there are several commonly used mathematical models for simulating flows in porous media with conductive fractures such as the dual porosity model barenblatt et al 1960 warren and root 1963 geiger et al 2013 single porosity model ghorayeb and firoozabadi 2000 traditional discrete fracture model dfm noorishad and mehran 1982 baca et al 1984 kim and deo 1999 january 2000 karimi fard and firoozabadi 2001 geiger boschung et al 2009 zhang et al 2013 embedded dfm edfm li and lee 2008 moinfar 2013 yan et al 2016 tene et al 2017 jiang and younis 2017a hosseinimehr et al 2018 xu et al 2019 the interface models alboin et al 1999 2000 hansbo and hansbo 2002 odsæter et al 2019 and extended finite element dfm xdfm based on the interface models fumagalli and scotti 2014 huang et al 2011 schwenck 2015 salimzadeh and khalili 2015 flemisch et al 2016 finite element method based on lagrange multipliers köppel et al 2019a b schädle et al 2019 etc among the above methods the traditional dfm and the interface models have been intensively studied in the past decades the dfm is based on the principle of superposition it uses a hybrid dimensional representation of the darcy s law and treats the fractures as lower dimensional entries with the thickness of the fracture as the dimensional homogeneity factor the first dfm was introduced by noorishad and mehran 1982 in 1982 for single phase flows later baca et al 1984 considered the heat and solute transport in fractured media subsequently several significant numerical methods were applied to the dfm such as the finite element methods kim and deo 1999 january 2000 karimi fard and firoozabadi 2001 geiger boschung et al 2009 zhang et al 2013 vertex centered finite volume methods monteagudo and firoozabadi 2004 reichenberger et al 2006 monteagudo and firoozabadi 2007 zhang et al 2016 cell centered finite volume methods karimi fard et al 2003 sandve et al 2012 ahmed et al 2015 gl aser et al 2017 fang et al 2018 mixed finite element methods hoteit and firoozabadi 2005 2006 2008b a moortgat and firoozabadi 2013a b zidane and firoozabadi 2014 moortgat et al 2016 discontinuous galerkin methods antonietti et al 2019 all the above works are limited on conforming meshes i e the fractures are aligned with the interfaces of the background matrix cells therefore it may suffer from low quality cells recently xu and yang introduced the dirac δ functions xu and yang 2020 to represent the conductive fractures and reinterpreted the dfm rdfm on nonconforming meshes the basic idea is to superpose the conductivity of the fracture to that of the matrix the main contribution in xu and yang 2020 is to explicitly represent the dfm introduced in karimi fard and firoozabadi 2001 as a scalar partial differential equation therefore with suitable numerical discretizations such as the discontinuous galerkin method the rdfm can be applied to arbitrary meshes to demonstrate that the rdfm is exactly the traditional dfm if the mesh is conforming in xu and yang 2020 only finite element methods were considered therefore local mass conservation was missing later the enriched galerkin and interior penalty discontinuous galerkin methods were applied to rdfm in feng et al 2021 and the contaminant transportation was also simulated different from the traditional dfm the interface model alboin et al 1999 2000 hansbo and hansbo 2002 odsæter et al 2019 explicitly represent the fractures as interfaces of the porous media then the governing equation of the flow in the lower dimensional fracture was constructed in the interface model the matrix and fractures are considered as two systems and the communication between them was given as the jump of the normal velocity along the fractures therefore different from rdfm the interface model though hanging nodes are allowable cannot be applied to structured meshes and the fracture must be aligned with the interfaces of the meshes for the matrix to fixed this limitation the xdfm was proposed fumagalli and scotti 2014 huang et al 2011 schwenck 2015 salimzadeh and khalili 2015 flemisch et al 2016 however these methods may increase the degrees of freedom dofs significantly and can hardly be applied to fracture networks with high geometrical complexity flemisch et al 2018 as an alternative the cutfem burman et al 2019 can be applied to non conforming meshes it couples the fluid flow in all lower dimensional manifolds however this method requires the fractures to cut the domain into completely disjoint subdomains thus it is not applicable for media with complicated fractures most of the above ideas work for problems with conductive fractures however if the media contains blocking fractures most methods may not be suitable to fix this gap the projection based edfm pedfm was introduced in tene et al 2017 and jiang and younis 2017b the effective flow area between adjacent matrix grids is computed as the difference between the original interface area and the projected area of the fracture segment it will be zero if the fracture fully penetrates through the matrix cell olorode et al 2020 extended the pedfm into three dimensional compositional simulation of fractured reservoirs however the pedfm still cannot describe the complex multiphase flow behavior in the matrix blocks within barrier fractures another approach is to follow the interface model introduced in martin et al 2005 angot et al 2009 boon et al 2018 and kadeethum et al 2020 however as demonstrated above the interface model can only handle hanging nodes and the fractures must align with the interfaces of the background mesh recently xu and yang extended the rdfm xu and yang 2020 feng et al 2021 to problems with blocking fractures in xu et al 2021 the basic idea is to apply ohm s law and superpose the resistance the reciprocal of the permeability of the blocking fracture to that of the matrix then a modified partial differential equation system was introduced and the local discontinuous galerkin methods with suitable penalty were perfectly applied if the problems contains only blocking fractures the mixed finite element methods can easily be combined with rdfm in this paper we combine the ideas in alboin et al 1999 and xu et al 2021 to propose a novel model for single phase flows in porous media the idea works for problems containing both conductive and blocking fractures in particular the interface model alboin et al 1999 was used to enforce the continuity of pressure across the conductive fractures while the dirac δ functions were applied for the blocking fractures to superpose the resistance following the main idea given in xu et al 2021 the separate treatment of conductive and blocking fractures may yield difficulties in constructing mathematical models therefore one of the major novelty of the current work is the seamless combination of the conductive fracture interface model and the blocking fracture dirac δ function approach with the new mathematical model we further construct numerical discretization by using a hybrid mixed finite element method the proposed method has the following features 1 it produces locally conservative velocity approximations 2 it leads to a symmetric positive definite linear system 3 it only produces globally coupled degrees of freedom dofs of pressure on the mesh skeletons therefore the numerical scheme yields much less total dofs and is easy to implement and this is the main advantage compared with the xdfm pedfm and cutfem methods moreover thanks to the dirac δ functions for blocking fractures the method does not require any mesh conformity with respect to the blocking fractures which is another major novelty of our proposed scheme compared with the interface model to the best knowledge of the authors our approach is the simplest one that can be applied to non conforming mesh for blocking fractures that still yield locally conservative velocity approximations we note that mesh conformity with respect to the conductive fractures is still required for our method which is typical for interface models we numerically demonstrate that our hybrid mixed finite element scheme is highly competitive both in terms of computational efficiency and accuracy as an application we couple the proposed flow equation with the simple transport equations and construct the locally conservative hybrid finite volume methods for the transport equations we finally emphasis that the proposed hybrid mixed formulation is different from the mixed method in boon et al 2018 due to the use of different model for the interface conditions we believe that our model is significantly simpler for complex fracture networks since we only use one matrix domain and one codimension 1 conductive fracture domain throughout while the mixed method formulation boon et al 2018 needs to split the matrix and fracture domains into multiple disjoint sub domains and require the modeling of codimension 1 3 fracture flows which might be very tedious to perform for complex fracture networks the rest of the paper is organized as follows in section 2 we present the hybrid dimensional model under consideration we then formulate in section 3 the hybrid mixed finite element discretization of the model proposed in section 2 numerical results for various benchmark test cases are presented in section 4 we conclude in section 5 2 the hybrid dimensional model 2 1 notation we consider a bounded open domain ω m r d d 2 3 which contains several d 1 dimensional conductive or blocking fractures for simplicity the fractures are assumed to be hyperplanes with smooth boundaries we denote ω c as the d 1 dimensional open set containing all the conductive fractures and ω b as the set containing all the blocking fractures assume the d 1 dimensional domain boundary ω m γ d γ n with γ d γ n 0 furthermore we denote the following sets of d 2 dimensional boundaries intersections associated with the set of conductive fractures ω c γ c c is the set containing the intersections among conductive fractures γ c b is the set containing the intersections between 2 conductive and blocking fractures γ c m is the set containing the intersections between conductive fractures and domain boundary ω m which is further split to γ c m γ c m n γ c m d with γ c m n γ n and γ c m d γ d γ c i is the boundary of ω c that does not intersect with the domain boundary ω m we set γ c γ c c γ c b γ c m γ c i as the collections of all intersections of ω c an illustration of a typical hybrid dimensional domain in two dimensions is given in fig 1 we denote n γ as a uniquely oriented unit normal vector on a d 1 dimensional interface boundary γ and denote η γ as the in plane unit outer normal vector on the d 2 dimensional boundary γ of γ see fig 2 let ε be the thickness of the fractures which is assumed to be a small positive constant for simplicity let k m be the permeability tensor of the domain excluding the fractures ω m ω c ω b k b k m be the scalar permeability in the normal direction of blocking fractures ω b and k c k m be the permeability tensor in the tangential direction of the conductive fractures ω c 2 2 the hybrid dimensional flow model the following hybrid dimensional model is a combination of the conductive fracture treatment in alboin et al 1999 and blocking fracture treatment in xu et al 2021 we apply the dirac δ functions in the bulk domain ω m ω c excluding conductive fractures and the model reads 1a k m 1 ε k b δ ω b n ω b n ω b u p in ω m ω c 1b u f in ω m ω c where u is the darcy velocity p is the pressure f is the volume source term δ ω b is the dirac δ function that takes values on the blocking fractures ω b and zero elsewhere and n ω b is the unit normal vector on ω b the basic idea is to superpose the resistance of the porous media to reduce the darcy velocity in the normal direction of the blocking fractures within the conductive fractures excluding intersections ω c γ c we use the following d 1 dimensional darcy s law 1c ε k c 1 u c γ p c in ω c γ c 1d γ u c u in ω c γ c where u c is the tangential darcy velocity in the conductive fractures p c is the associated pressure and the velocity jump u u u n γ represents the mass exchange between the conductive fractures and the surrounding media where u x lim τ 0 u x τ n γ for all x ω c is the bulk darcy velocity evaluated on one side of the conductive fractures moreover γ and γ are the usual surface gradient and surface divergence operators the above equations give the modified darcy s law for flow in porous media containing both conductive and blocking fractures we close the hybrid dimensional system with the following set of boundary interface conditions 1e p p d on γ d 1f u n q n on γ n 1g p p c on ω c 1h u c 0 on γ c c 1i p c p d on γ c m d 1j u c η γ 0 on γ c b γ c m n γ c i where 1g ensures continuity of bulk pressure across conductive fractures the no flow boundary condition in 1j is imposed on the intersections γ c b γ c m n and γ c i and the jump term in 1h is u c e γ ω c γ c e γ u c γ η γ e γ c c which represents mass conservation along intersections γ c c note in particular that each conductive fracture containing the intersection e appears exactly twice in the above summation and the in plane normal velocity on the fracture is allowed to be discontinuous along the intersection e for example the jump u c h at node h in the configuration in fig 1 is u c h γ e h h k g h h j u c γ η γ we note that in the above model 1 the flow in the tangential direction in the blocking fractures is completely ignored as the permeability therein is much smaller than that of the surroundings on the other hand the flow in the normal direction is ignored on conductive fractures by the pressure continuity condition 1g since the permeability is much larger than that of the surroundings and the fluid has a tendency to flow along the tangential direction therein 2 3 the hybrid dimensional transport model we now consider a scalar quantity c that is transported through the porous medium subject to the velocity fields in the flow model 1 here c usually represents the concentration of a generic passive tracer similar to the flow treatment in the previous subsection transport inside the blocking fractures is ignored the concentrations c in the matrix and c c in the conductive fractures are governed by the following advection equations see e g alboin et al 2002 fumagalli and scotti 2013 odsæter et al 2019 2a ϕ m c t u c c f in ω m ω c 0 t 2b ε ϕ c c c t γ u c c c c u 0 in ω c 0 t with the following initial interface and boundary conditions 2c c c c on ω c 0 t 2d c c 0 on ω 0 c c c c 0 on ω c 0 2e c c b on ω i n 0 t c c c c b on γ i n 0 t where ϕ m c 0 c b ω i n and ϕ c c c 0 c c b γ i n represent the porosity initial concentration inflow concentration and inflow boundary in the matrix and conductive fractures respectively observe that concentration continuity 2c across the conductive fractures are enforced in the model 2 3 the hybrid mixed finite element method 3 1 preliminaries let t h k be a conforming simplicial triangulation of the domain ω m let e h be the collections of d 1 dimensional facets edges for d 2 faces for d 3 of ω m assume the mesh is fully fitted with respect to the conductive fractures i e t h c ω c e h is a d 1 dimensional simplicial triangulation of the domain ω c here the mesh t h is allowed to be unfitted with respect to the blocking fractures moreover we denote e h c as the collection of d 2 dimensional facets of t h c vertices for d 2 edges for d 3 we use the lowest order hybrid mixed finite element methods to discretize the model 1 the following finite element spaces will be needed 3a v h v l 2 t h d v k r t 0 k k t h 3b w h w l 2 t h w k p 0 k k t h 3c m h μ l 2 e h μ f p 0 f f e h 3d v h c v c l 2 t h c d v f r t 0 f f t h c 3e m h c μ l 2 e h c μ e p 0 e e e h c where r t 0 s is the raviart thomas space of lowest order on a simplex s and p 0 s is the space of constants we denote the following inner products ϕ ψ t h k t h k ϕ ψ dx ϕ ψ t h k t h k ϕ ψ ds ϕ ψ t h c f t h c f ϕ ψ ds ϕ ψ t h c f t h c f ϕ ψ dr where dx is for d dimensional integration ds is for d 1 dimensional integration and dr is for d 2 dimensional integration when d 2 f ϕ ψ dr is simply the sum of point evaluations at the two end points of a line segment f 3 2 the hybrid mixed method for the flow model the hybrid mixed method for the hybrid dimensional model 1 is given as follows find u h p h p h u h c p h c v h w h m h v h c m h c with p h γ d p 0 p d and p h c γ c m d p 0 p d where p 0 denotes the projection onto piecewise constants such that 4a k m 1 u h v h t h ω b ε k b u h n v h n ds p h v h t h p h v h n t h 0 4b u h q h t h f q h t h 0 4c u h n q h t h γ u h c q h t h c γ n q n q h ds 0 4d ε k c 1 u h c v h c t h c p h γ v h c t h c p h c v h c η t h c γ c b α ε k c 1 u h c η v h c η dr 0 4e u h c η q h c t h c 0 for all v h q h q h v h c q h c v h w h m h v h c m h c with q h γ d q h c γ c m d 0 where α 0 is a penalty parameter for the implementation of the no flow boundary condition 1j on γ c b in our numerical implementation we take α 1 0 6 we show that the scheme 4 is formally consistent with the hybrid dimensional model 1 1 eq 4a is a discretization of the darcy s law 1a in the bulk using integration by parts and the following property of dirac δ function ω m δ ω b ϕ dx ω b ϕ ds 2 eq 4b is the discretization of mass conservation 1b in the bulk 3 eq 4c simultaneously enforces i the continuity of normal velocity u h n across interior element boundaries e h t h c γ n ii the boundary condition 1f on γ n and iii mass conservation 1d within the conductive fractures in t h c 4 eq 4d is a discretization of the darcy s law 1c on the conductive fractures t h c where the pressure continuity condition 1h is also strongly enforced as p h both represents the bulk pressure on the element boundary e h and the pressure within the conductive fracture t h c moreover the last term in 4d is a penalty formulation of the no flow boundary condition 1j on γ c b note that γ c b is allowed to be not aligned with the facets of t h c 5 eq 4e is a transmission condition that simultaneously enforces i continuity of in plane normal velocity u h c η on interior facets e h c γ c c γ c m n γ c i ii the mass conservation 1h on the intersections γ c c iii the no flow boundary condition 1j on γ c m n and γ c i 6 the dirichlet boundary condition 1e and 1i are imposed strongly through the corresponding degrees of freedom dofs on p h and p h c respectively the following result further shows that the scheme 4 is well posed theorem 3 1 assume the measure of the dirichlet boundary γ d is not empty then the solution to the scheme 4 exists and is unique proof since the equations in 4 leads to a square linear system we only need to show uniqueness now we assume the source terms in 4 vanishes i e f p d g n 0 taking test function to be the same as trial functions in 4 and adding we get k m 1 u h u h t h ω b ε k b u h n 2 ds ε k c 1 u h c u h c t h c 0 hence u h u h c 0 since u h 0 the inf sup stability of the r t 0 p 0 finite element pair implies that p h p h c from 4a where c is a constant since γ d is not empty and p d 0 we get the constant c 0 finally restricting eq 4d to a single element f t h c and using the fact that u h c 0 and p h 0 we get f p h c v h c η ds 0 v h c r t 0 f which then implies that p h c 0 this completes the proof 3 3 static condensation and linear system solver the linear system 4 can be efficiently solved via static condensation where the dofs for u h p h and u h c can be locally eliminated resulting in a coupled global linear system for the dofs for p h and p h c which is symmetric and positive definite efficient linear system solvers for the resulting condensed system is an interesting topic where one could design efficient decoupling algorithms or robust monolithic preconditioners here we simply use a sparse direct solver in the computation and postpone the detailed study of linear system solvers to our future work 3 4 local pressure postprocessing we use the following well known local piecewise linear pressure postprocessing to improve the accuracy of pressure approximation in the bulk find p h w h w l 2 t h w k p 1 k k t h where p 1 k is the space of linear polynomials on element k such that 5a p h q h t h k m 1 u h q h t h 5b p h 1 t h p h 1 t h for all q h w h 3 5 the hybridized finite volume method for the transport model we consider a standard cell centered first order upwinding finite volume scheme for the transport model 2 coupled with the implicit euler method for the temporal discretization we hybridize the cell centered finite volume scheme so that the coupled unknowns live on the mesh skeletons which simplifies the definition of upwinding fluxes on the conductive fracture interactions e g point h in fig 1 hence we use piecewise constant spaces to approximate the matrix concentration c h w h on the mesh t h the matrix concentration c h m h on the matrix mesh skeleton e h and the fracture concentration c c h m h c on the fracture mesh skeleton e h c the hybridized finite volume scheme with implicit euler temporal discretization is given as follows given data c h n 1 c h n 1 w h m h at time t n 1 find c h n c h n c c h n w h m h m h c at time t n t n 1 δ t with c h n ω i n p 0 c b t n and c c h n γ i n p 0 c c b t n such that 6a ϕ m c h n c h n 1 δ t d h t h u h n c h n d h t h c h n f d h t h 6b u h n c h n d h t h ε ϕ c c h n c h n 1 δ t d h t h c u h c η c c h n d h t h c 0 6c u h c η c c h n d c h t h c 0 for all d h d h d c h w h m h m h c with d h ω i n 0 and d c h γ i n 0 where the upwinding fluxes are given as follows 6d c h n k c h n if u h n k 0 c h n if u h n k 0 6e c c h n f c h n if u h c η f 0 c c h n if u h c η f 0 3 6 remarks on the mesh restrictions and comparison with existing methods the proposed flow and transport solvers 4 6 require the mesh to be fitted to the conductive fractures while allowing for an unfitted treatment of the blocking fractures actually this is one of the major novelty of the proposed work while the derivation of numerical schemes that work on fully unfitted meshes is beyond the scope of this paper here we propose a simple mesh postprocessing technique to convert a general unfitted background matrix mesh to an immersed mesh that is fitted to all the fractures similar immersing mesh techniques were used for interface problems ilinca and hétu 2011 frei and richter 2014 auricchio et al 2015 chen et al 2017 below we illustrate the procedure of immersing a single fracture to an unfitted tetrahedral mesh in 3d i represent the fracture geometry as the zero level set of a continuous piecewise linear function ϕ h on the background mesh perturb ϕ h slightly if necessary to avoid fracture pass through the background mesh nodes ii loop over the background mesh edges find the cut edges where ϕ h has opposite sign on the two edge endpoints for each cut edge compute the coordinates of the cut vertex v c where ϕ h v c 0 and add v c to the mesh nodes iii loop over the background mesh faces find the cut faces which contains the cut vertices order the cut vertices based on their vertex label number loop over the cut vertices for each sub face that contains the cut vertex split the sub face by 2 by connecting the cut vertex with the opposite sub face node iv loop over the background mesh elements find the cut elements which contains the cut vertices order the cut vertices based on their vertex label number loop over the cut vertices for each sub element that contains the cut vertex split the sub element by 2 by connecting the cut vertex with the opposite two sub element nodes that are not aligned with the cut edge the above recursive bisection procedure guarantees that the fracture lies on the boundary of the generated immersed mesh the case with multiply intersecting fractures can be treated by recursion here we note that the generated immersed mesh is usually highly anisotropic since the background mesh is completely independent of the fracture configurations our numerical results in the next section suggest that the hybrid mixed method 4 works well on these anisotropic immersed meshes typical 2d immersed meshes for complex fracture configurations are given in figs 11 and 14 we now briefly compare our proposed fractured flow solver 4 with some existing schemes in berre et al 2021 which were used to solve a series of 4 benchmark problems in 3d fractured porous media flow among the 17 schemes in berre et al 2021 table 1 7 were shown to yield no significant deviations for all the tests see berre et al 2021 figure 18 which include the multi point flux approximation uib mpfa the lowest order mixed virtual element method uib mvem and the lowest order raviart thomas mixed finite element method uib rt0 mainly developed by the research group in the university of bergen keilegavlen et al 2021 nordbotten et al 2019 boon et al 2018 the mpfa scheme ustutt mpfa and the two point flux approximation scheme ustutt tpfa circ developed by flemisch et al 2011 the mimetic finite difference method lanl mfd lipnikov et al 2014 and the hybrid finite volumes discontinuous hydraulic head method unice unige hfv disc developed by brenner et al 2017 among these 7 schemes the first three schemes use a mixed dimensional interface model that require the modeling of co dimension 1 3 fractured flows where the mesh can be non matching across subdomains but needs to be geometrically conforming to the fractures on the other hand the last four schemes work on a mixed dimensional interface model where only fractured flow in co dimension 1 were modeled which require the mesh to be completely conforming to the fractures all of these schemes yield a locally conservative velocity approximation we further note that the two methods in berre et al 2021 that allow for general nonconforming meshes namely the lagrange multiplier method köppel et al 2019a b schädle et al 2019 and the edfm method nikitin and yanbarisov 2020 cannot handle blocking fractures and do not provide a locally conservative velocity approximation numerical results of our proposed scheme 4 for the benchmark problems in berre et al 2021 indicate that our results yield no significant deviations with the above mentioned 7 schemes see details in the next section our scheme also produce a locally conservative velocity approximation and the resulting linear system after static condensation is a symmetric positive definite spd problem with global unknowns involve pressure dofs on the mesh skeleton only the number of the global unknowns of our scheme is roughly n f which is the total number of mesh faces and the average nonzero entries per row in the system matrix is 7 a pressure dof on an interior tetrahedral face is connected to 6 neighboring face pressure dofs concerning the computational cost of our scheme it is more expensive than the tpfa scheme ustutt tpfa circ which lead to an spd system with roughly n c cell wise pressure dofs and about 5 nonzero entries per row in the system matrix is slightly less expensive than the cell based mpfa schemes uib mpfa ustutt mpfa which lead to spd systems with roughly n c cell wise pressure dofs and about 20 50 nonzero entries per row in the system matrix and is significantly cheaper than the schemes uib mvem uib rt0 lanl mfd and unice unige hfv disc which lead to saddle point systems with total number of roughly n f velocity dofs and n c pressure dofs note that n f 2 n c hence our proposed scheme is also highly competitive in terms of computational costs another distinctive advantage of our scheme over these 7 schemes is that the mesh can be completely nonconforming to the blocking fractures 4 numerics in this section we present detailed numerical results for the proposed hybrid mixed method for the four 2d benchmark test cases in flemisch et al 2018 and the four 3d benchmark test cases in berre et al 2021 we name the method 4 as hm dfm since it is a hybrid mixed method for a discrete fracture model when plotting the pressure or hydraulic head distribution over line segments we evaluate the second order postprocessed solution in 5 for the proposed method the focus of the numerical experiments is on the verification of the accuracy of our proposed flow model 1 and the associated method 4 hence we test the flow solver 4 for all the 8 benchmark cases meanwhile we also test the accuracy of velocity approximation by feeding them to the transport problem 2 which is solved using the scheme 6 for three cases namely benchmark 2 in 2d and benchmark 5 6 in 3d furthermore convergence study via mesh refinements was conducted for benchmark 2 and benchmark 6 below our numerical simulations are performed using the open source finite element software ngsolve schöberl 2014 https ngsolve org jupyter notebooks for reproducing all numerical examples in this section can be found in the git repository https github com gridfunction fracturedporousmedia visualization of meshes for the 3d benchmark examples and interactive contour plots of the pressure hydraulic head can also be found therein 4 1 benchmark 1 hydrocoin 2d this example is originally a benchmark for heterogeneous groundwater flow presented in the international hydrocoin project ski 1987 a slight modification for the geometry was made in flemisch et al 2018 section 4 1 and we follow the settings therein in particular the bulk domain is a polygon with vertices a 0 150 b 400 100 c 800 150 d 1200 100 e 1600 150 f 1600 1000 g 1500 1000 h 1000 1000 and i 0 1000 measured in meters there are two conductive fractures in the domain b g and d h the fracture b g has thickness ε 5 2 m and the fracture d h has thickness ε 33 5 m the permeability hydraulic conductivity is k m 1 0 8 m s in the bulk and k c 1 0 6 m s in the fractures dirichlet boundary condition p height is imposed on the top boundary and homogeneous neumann boundary condition is imposed on the rest of the boundary here the unknown variable p is termed as the piezometric head according to ski 1987 the quantity of interest is the distribution of the piezometric head p along the horizontal line at a depth of 200 m we apply the method 4 on a uniform triangular mesh with mesh size h 60 see the left panel of fig 3 which leads to 1115 matrix elements and 44 fracture elements on this mesh the number of the globally coupled dofs is 1779 in which 1691 dofs are associated with the bulk hybrid variable p h and 43 dofs are associated with the fracture hybrid variable p h c in the right panel of fig 3 we record the postprocessed piezometric head p h in 5 along the line segment z 200 m where z is the horizontal direction along with the reference data obtained from a mimetic finite difference method on a very fine mesh with 889 233 dofs we observe that the results for the proposed method on such a coarse mesh already shows a good agreement with the reference data 4 2 benchmark 2 regular fracture network 2d this test case is originally from geiger et al 2013 and is modified by flemisch et al 2018 which simulates a regular fracture network in a square porous media the computational domain including the fracture network and boundary conditions is shown in fig 4 the matrix permeability is set to k m i and fracture thickness is ε 1 0 4 two cases of fracture permeability was considered i a highly conductive network with k c 1 0 4 i ii a blocking fracture with k b 1 0 4 we apply the method 4 on a triangular mesh with 1348 matrix elements and 91 fracture elements see the left panel of fig 5 for the blocking fracture case we also present the result on a unfitted triangular mesh with 1442 matrix elements for the conductive fracture case the number of the globally coupled dofs is 2127 in which 2041 dofs are associated with the bulk hybrid variable p h and 86 dofs are associated with the fracture hybrid variable p h c the pressure distributions along two lines one horizontal at y 0 7 and one vertical at x 0 5 are shown in fig 6 along with the reference data obtained from a mimetic finite difference method on a very fine mesh with 1 175 056 dofs similar to the previous example we observe that the results for the proposed method show a good agreement with the reference data for the blocking fracture case the number of the globally coupled dofs is 2041 on the fitted mesh and is 2188 on the unfitted mesh the pressure distribution along the lines 0 0 1 0 9 1 0 is shown in fig 7 again we observe a very good agreement with reference data for the results on the fitted mesh the result on the unfitted mesh case is slightly off due to mesh nonconformity which is expected as it could not capture the pressure discontinuity across the blocking fractures 4 2 1 coupling with transport and convergence study with mesh refinements after the velocity fields are computed from the scheme 4 we feed them to the transport model 2 and solve it by using the hybrid finite volume scheme 6 we take the porosities ϕ m 0 1 ϕ c 0 9 in the model 2 with the initial concentrations c 0 c c 0 0 and set the left boundary as the inflow boundary for the concentrations with c b c c b 1 the final time of simulation is t 0 1 convergence of our coupled scheme 4 and 6 is checked via a mesh refinement study where the initial meshes are given in fig 5 and three level of uniform mesh refinements are applied afterwards the constant time step size is taken to be δ t 2 l 5 1 0 3 where l is the mesh refinement level since there is no analytic solution to the problem we provide a reference solution using the coupled scheme 4 and 6 on the fourth level refined fitted mesh with about 345k elements with a small time step size δ t 3 125 1 0 5 contour of matrix concentrations of the reference solution at time t 0 05 and t 0 1 are presented in fig 8 where we clearly observe the conducting and blocking effects of the respective fractures moreover we plot the computed matrix concentrations along the cut line y 0 7 in fig 9 where we observe convergence as the mesh refines finally the l 2 errors in the matrix velocity u h u r e f ω m and postprocessed matrix pressure p h p r e f ω m and the l 2 errors in the matrix concentration c h t c r e f t ω m at final time t 0 1 are recorded in table 1 for the conductive fracture case in table 2 for the blocking fracture case on fitted meshes and in table 3 for the blocking fracture case on unfitted meshes where we recall that the reference solutions are obtained on the fourth level refined fitted mesh with a small time step size δ t 3 125 1 0 5 here the rate of convergence for e r r at level i is estimated via the formula r a t e log e r r i 1 e r r i log 2 from table 1 for the conductive fracture case we observe that the convergence rate in the velocity approximation is first order and that in the postprocessed pressure approximation is second order which is consistent with the expected convergence behavior of the hybrid mixed method for the equi dimensional case raviart and thomas 1977 arnold and brezzi 1985 and the convergence rate for the concentration is about 1 2 which is also expected for the hybridized finite volume scheme due to the concentration discontinuities in the domain similar convergence behavior was observed in table 2 for the blocking fracture case on fitted meshes from table 3 we observe 1 2 order convergence for all three variables where the degraded velocity and pressure convergence is due to nonconformity of the mesh with the fractures 4 3 benchmark 3 complex fracture network 2d this test case considers a small but complex fracture network that includes permeable and blocking fractures the domain and boundary conditions are shown in fig 10 the exact coordinates for the fracture positions are provided in flemisch et al 2018 appendix c the fracture network contains ten straight immersed fractures the fracture thickness is ε 1 0 4 for all fractures and permeability is k c 1 0 4 for all fractures except for fractures 4 and 5 which are blocking fractures with k b 1 0 4 note that we are considering two subcases a and b with a pressure gradient which is predominantly vertical and horizontal respectively we apply the method 4 on two set of meshes a triangular fitted mesh with 1332 matrix elements and 88 fracture elements which was provided in the git repository https git iws uni stuttgart de benchmarks fracture flow see left of fig 11 and a triangular immersed fitted mesh with 1370 matrix elements and 211 fracture elements obtained from a background unfitted mesh using the immersing mesh technique introduced in section 3 6 see right of fig 11 the globally coupled dofs is 2066 for the fitted mesh and is 2211 for the immersed mesh the pressure distributions along the lines 0 0 5 1 0 0 9 are shown in fig 12 we observe that the results on the two meshes are very close to each other and they are in good agreements with the reference data obtained from a mimetic finite difference method on a very fine mesh with 1 8 million dofs 4 4 benchmark 4 a realistic case 2d we consider a real set of fractures from an interpreted outcrop in the sotra island near bergen in norway the size of the domain is 700 m 600 m with uniform scalar permeability k m 1 0 14 m 2 the set of fractures is composed of 64 line segments in which the permeability is k c 1 0 8 m 2 the fracture thickness is ε 1 0 2 m the exact coordinates for the fracture positions are provided in the above mentioned git repository the domain along with boundary conditions is given in fig 13 similar to the previous example we apply the method 4 on two set of conforming meshes a fitted mesh consists of 10 807 matrix elements and 1047 fracture elements provided in https git iws uni stuttgart de benchmarks fracture flow see left of fig 14 and an immersed fitted mesh consists of 5473 matrix elements and 1541 fracture elements obtained from a background unfitted mesh using the immersing mesh technique introduced in section 3 6 see right of fig 14 the number of the globally coupled dofs is 17 253 for the fitted mesh a and 9753 for the immersed mesh b the pressure distribution along the two lines y 500 m and x 625 m are shown in fig 15 along with the results for the mortar dfm method with 25 258 dofs from flemisch et al 2018 we observe that the three results are in good agreements with each other with the hdg dfm b using the least amount of dofs 4 5 benchmark 5 single fracture 3d this is the first benchmark case proposed in berre et al 2021 to be consistent with the notation in berre et al 2021 the pressure and permeabilities are renamed as hydraulic head and hydraulic conductivities respectively for this test case and the three examples following fig 16 illustrates the geometrical description here the domain ω is a cube shaped region 0 m 100 m 0 m 100 m 0 m 100 m which is crossed by a conductive planar fracture ω 2 with a thickness of ε 1 0 2 m the matrix domain consists of subdomains ω 3 1 above the fracture and ω 3 2 and ω 3 3 below the subdomain ω 3 3 represents a heterogeneity within the rock matrix the matrix conductivities are given in fig 16 and the fracture conductivity is k c 0 1 so that ε k c 1 0 3 inflow into the system occurs through a narrow band defined by 0 m 0 m 100 m 90 m 100 m similarly the outlet is a narrow band defined by 0 m 100 m 0 m 0 m 10 m at the inlet and outlet bands we impose the hydraulic head h i n 4 m and h o u t 1 m respectively the remaining parts of the boundary are assigned no flow conditions following the setup in berre et al 2021 we set c b 0 01 m 3 at the inlet boundary for the transport problem the matrix porosity ϕ is taken to be 0 2 on ω 3 1 ω 3 2 and 0 25 on ω 3 3 and the fracture porosity ϕ c is taken to be 0 4 the final time of simulation is t 1 0 9 s and the time step size is δ t 1 0 7 s we perform the method 4 and 6 on a coarse tetrahedral mesh with 10 232 matrix elements and 448 fracture elements and a fine tetrahedral mesh with 111 795 matrix elements and 1758 fracture elements the number of the globally coupled dofs on the coarse mesh is 23 377 while that on the fine mesh is 235 619 the hydraulic head along the line 0 m 100 m 100 m 100 m 0 m 0 m is shown in fig 17 along with reference data and published spread provided in the git repository https git iws uni stuttgart de benchmarks fracture flow 3d the reference data in fig 17 is obtained from the ustutt mpfa method on a mesh with approximately 1 million matrix elements while the shaded region depicts the area between the 10th and the 90th percentile of the published results in berre et al 2021 on mesh refinement level 1 left 10 k cells and refinement level 2 right 100 k cells the match number results from evaluating at 100 evenly distributed evaluation points if the value for the hm dfm method is between the respective lower and upper value we observe that our result agrees with the reference values quite well especially on the fine mesh moreover we plot the matrix concentration along the line 0 m 100 m 100 m 100 m 0 m 0 m in fig 18 and the fracture concentration along the line 0 m 100 m 80 m 100 m 0 m 20 m at final time t 1 0 9 s in fig 19 together with the published spread provided in the git repository which depicts the area between the 10th and the 90th percentile of the published results in berre et al 2021 using similar first order finite volume schemes with implicit euler time stepping and δ t 1 0 7 s we observe that our results agree quite well with the provided data 4 6 benchmark 6 regular fracture network 3d this is the second benchmark case proposed in berre et al 2021 which is a 3d analog of benchmark 2 the domain is given by the unit cube ω 0 m 1 m 3 and contains 9 regularly oriented fractures as illustrated in fig 20 dirichlet boundary condition p h 1 m is imposed on the boundary γ d x y z ω x y z 0 875 m neumann boundary condition u n 1 m s is imposed on the boundary ω i n x y z ω x y z 0 25 m and no flow boundary condition is imposed on the remaining boundaries the heterogeneous matrix conductivity is illustrated in fig 20 and the fracture conductivity is either k c 1 0 4 m 2 which represents a conductive fracture or k b 1 0 4 m 2 which represents a blocking fracture the fracture thickness is ε 1 0 4 m for the transport equation matrix porosity is taken to be ϕ 0 1 conductive fracture concentration is ϕ c 0 9 and the inflow boundary condition c b 1 m 3 is set on the inlet boundary ω i n final time of the simulation is t 0 25 s we perform the method 4 on a coarse fitted tetrahedral mesh with 4375 matrix elements and 944 fracture elements and a fine tetrahedral mesh with 36 336 matrix elements and 4524 fracture elements the number of the globally coupled dofs on the coarse mesh is 13 373 for the conductive fracture case and 8334 for the blocking fracture case only dofs for p h are global dofs in this case while that on the fine mesh is 94 738 for the conductive fracture case and 70 881 for the blocking fracture case the hydraulic head along the diagonal line 0 m 0 m 0 m 1 m 1 m 1 m is shown in fig 21 for the conductive fracture case and in fig 22 for the blocking fracture case we observe that our results agree with the reference values very well which were obtained from the ustutt mpfa method on a mesh with approximately 1 million matrix elements the small derivation of our result on the left panel of fig 21 with the reference data is acceptable due to the use of a very coarse mesh we further performed a convergence study of the flow and transport solvers 4 and 6 via mesh refinements and record the l 2 errors in matrix velocity and postprocessed pressure and the l 2 errors in matrix concentration at final time t 0 25 in table 4 for the conductive fracture case and in table 5 for the blocking fracture case where the initial mesh is the coarse one with 4375 tetrahedral elements a total of three uniform mesh refinements was performed and the solution on the third level mesh was used as the reference solution to calculate the associated errors the time step size is taken to be δ t 2 l 2 5 1 0 3 s where l is the mesh refinement level on the finest mesh there are about 2 25 million tetrahedral elements and 4 5 million globally coupled dofs from both tables we observe convergence of our schemes and in particular the convergence rate for the velocity is approaching first order that for the postprocessed pressure is approaching second order and for the concentration is about first order finally in fig 23 we plot slices of concentrations computed on the 3rd refined mesh at final time t 0 25 along the five vertical planes x 0 1 x 0 3 x 0 5 x 0 7 and x 0 9 and in fig 24 we plot the evolution of mean concentration over time on the following three regions ω a 0 5 m 1 m 0 m 0 5 m 0 m 0 5 m ω b 0 5 m 0 75 m 0 5 m 0 75 m 0 75 m 1 m ω c 0 75 m 1 m 0 75 m 1 m 0 5 m 0 75 m ω a 0 5 m 1 m 0 m 0 5 m 0 m 0 5 m ω b 0 5 m 0 75 m 0 5 m 0 75 m 0 75 m 1 m ω c 0 75 m 1 m 0 75 m 1 m 0 5 m 0 75 m from the results in fig 23 we clearly observe the different flow pattern for the conductive fracture case in the first row and the blocking fracture case in the second row we further note that the mean concentrations reported in fig 24 were presented in berre et al 2021 figure 10 only on the coarse mesh with about 4 k matrix elements and a coarse time step size δ t 2 5 1 0 3 s our results on four set of meshes are close to each other and improve slightly as the mesh and time step size refines and they are also qualitatively similar to the majority of the coarse grid results in berre et al 2021 figure 10 4 7 benchmark 7 network with small features 3d this is the third benchmark case proposed in berre et al 2021 in which small geometric features exist that may cause trouble for conforming meshing strategies the domain is the box ω 0 m 1 m 0 m 2 25 m 0 m 1 m containing 8 fractures see fig 25 homogeneous dirichlet boundary condition is imposed on the outlet boundary ω o u t x y z 0 x 1 y 2 25 z 1 3 o r z 2 3 inflow boundary condition u n 1 m s is imposed on the inlet boundary ω i n x y z 0 x 1 y 0 1 3 z 2 3 and no flow boundary condition is imposed on the remaining boundaries the conductivity in the matrix is k m 1 m 2 and that in the fracture is k c 1 0 4 m 2 fracture thickness is ε 0 01 m we perform the method 4 on a coarse tetrahedral mesh with 31 812 matrix elements and 3961 fracture elements and a fine tetrahedral mesh with 147 702 matrix elements and 9441 fracture elements the number of the globally coupled dofs on the coarse mesh is 83 022 while that on the fine mesh is 343 359 the hydraulic head along the line 0 5 m 1 1 m 0 m 0 5 m 1 1 m 1 m is shown in fig 26 where the reference data is obtained with the ustutt mpfa scheme on a grid with approximately 1 0 6 matrix cells here we observe a very good agreement with the reference data even on the coarse mesh 4 8 benchmark 8 field case 3d this is the last benchmark case proposed in berre et al 2021 the geometry is based on a postprocessed outcrop from the island of algerøyna outside bergen norway which contains 52 fracture the simulation domain is the box ω 500 m 350 m 100 m 1500 m 100 m 500 m the fracture geometry is depicted in fig 27 homogeneous dirichlet boundary condition is imposed on the outlet boundary ω o u t 500 100 400 100 100 ω o u t 0 350 100 400 100 100 ω o u t 1 uniform unit inflow u n 1 m s is imposed on the inlet boundary ω i n 500 1200 1500 300 500 ω i n 0 500 200 1500 300 500 ω i n 1 conductivity is k m 1 m 2 in the matrix and k c 1 0 4 m 2 in the fracture fracture thickness is ε 1 0 2 m we perform the method 4 on a tetrahedral mesh with 241 338 matrix elements and 47 154 fracture elements the number of the globally coupled dofs is 696 487 the hydraulic head along the two diagonal lines 500 m 100 m 100 m 350 m 1500 m 500 m and 350 m 100 m 100 m 500 m 1500 m 500 m are shown in fig 28 along with published results from berre et al 2021 similar to benchmark 4 in 2d no reference data on refined meshes was provided for this problem due to its complexity comparing with the published results in fig 28 we observe that our method still performs quite well 5 conclusion a novel hybrid mixed method for single phase flow in fractured porous media has been presented the proposed model combines the interface model for the conductive fractures and time dirac δ functions approach for the blocking fractures the distinctive features for the numerical methods include local mass conservation symmetric positive definite linear system and allowing the computational mesh to be completely non conforming to the blocking fractures ample benchmark tests show the excellent performance of the proposed scheme which is also highly competitive with existing work in the literature extension to the method to more complex fractured flow models and adaptation of the method to more general meshes consists of our on going work we will also investigate efficient preconditioning procedures for the associated linear system problem in the near future credit authorship contribution statement guosheng fu methodology software writing original draft funding acquisition yang yang conceptualization writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
197,we present a hybrid mixed finite element method for a novel hybrid dimensional model of single phase darcy flow in a fractured porous media in this model the fracture is treated as a d 1 dimensional interface within the d dimensional fractured porous domain for d 2 3 two classes of fracture are distinguished based on the permeability magnitude ratio between the fracture and its surrounding medium when the permeability in the fracture is significantly larger than in its surrounding medium it is considered as a conductive fracture when the permeability in the fracture is significantly smaller than in its surrounding medium it is considered as a blocking fracture the conductive fractures are treated using the classical hybrid dimensional approach of the interface model where pressure is assumed to be continuous across the fracture interfaces while the blocking fractures are treated using the recent dirac δ function approach where normal component of darcy velocity is assumed to be continuous across the interface due to the use of dirac δ function approach for the blocking fractures our numerical scheme allows for nonconforming meshes with respect to the blocking fractures this is the major novelty of our model and numerical discretization moreover our numerical scheme produces locally conservative velocity approximations and leads to a symmetric positive definite linear system involving pressure degrees of freedom on the mesh skeleton only as an application we extend the idea to a simple transport model the performance of the proposed method is demonstrated by various benchmark test cases in both two and three dimensions numerical results indicate that the proposed scheme is highly competitive with existing methods in the literature msc 65n30 65n12 76s05 76d07 keywords hybrid mixed finite element method fractured porous media hybrid dimensional model 1 introduction numerical simulations of single and multi phase flows in porous media have many applications in contaminant transportation oil recovery and underground radioactive waste deposit due to the highly conductive and blocking fractures in the porous media underground it is still challenging to construct accurate numerical approximations matthäi et al 2010 vasilyeva et al 2019 golian et al 2020 there are several commonly used mathematical models for simulating flows in porous media with conductive fractures such as the dual porosity model barenblatt et al 1960 warren and root 1963 geiger et al 2013 single porosity model ghorayeb and firoozabadi 2000 traditional discrete fracture model dfm noorishad and mehran 1982 baca et al 1984 kim and deo 1999 january 2000 karimi fard and firoozabadi 2001 geiger boschung et al 2009 zhang et al 2013 embedded dfm edfm li and lee 2008 moinfar 2013 yan et al 2016 tene et al 2017 jiang and younis 2017a hosseinimehr et al 2018 xu et al 2019 the interface models alboin et al 1999 2000 hansbo and hansbo 2002 odsæter et al 2019 and extended finite element dfm xdfm based on the interface models fumagalli and scotti 2014 huang et al 2011 schwenck 2015 salimzadeh and khalili 2015 flemisch et al 2016 finite element method based on lagrange multipliers köppel et al 2019a b schädle et al 2019 etc among the above methods the traditional dfm and the interface models have been intensively studied in the past decades the dfm is based on the principle of superposition it uses a hybrid dimensional representation of the darcy s law and treats the fractures as lower dimensional entries with the thickness of the fracture as the dimensional homogeneity factor the first dfm was introduced by noorishad and mehran 1982 in 1982 for single phase flows later baca et al 1984 considered the heat and solute transport in fractured media subsequently several significant numerical methods were applied to the dfm such as the finite element methods kim and deo 1999 january 2000 karimi fard and firoozabadi 2001 geiger boschung et al 2009 zhang et al 2013 vertex centered finite volume methods monteagudo and firoozabadi 2004 reichenberger et al 2006 monteagudo and firoozabadi 2007 zhang et al 2016 cell centered finite volume methods karimi fard et al 2003 sandve et al 2012 ahmed et al 2015 gl aser et al 2017 fang et al 2018 mixed finite element methods hoteit and firoozabadi 2005 2006 2008b a moortgat and firoozabadi 2013a b zidane and firoozabadi 2014 moortgat et al 2016 discontinuous galerkin methods antonietti et al 2019 all the above works are limited on conforming meshes i e the fractures are aligned with the interfaces of the background matrix cells therefore it may suffer from low quality cells recently xu and yang introduced the dirac δ functions xu and yang 2020 to represent the conductive fractures and reinterpreted the dfm rdfm on nonconforming meshes the basic idea is to superpose the conductivity of the fracture to that of the matrix the main contribution in xu and yang 2020 is to explicitly represent the dfm introduced in karimi fard and firoozabadi 2001 as a scalar partial differential equation therefore with suitable numerical discretizations such as the discontinuous galerkin method the rdfm can be applied to arbitrary meshes to demonstrate that the rdfm is exactly the traditional dfm if the mesh is conforming in xu and yang 2020 only finite element methods were considered therefore local mass conservation was missing later the enriched galerkin and interior penalty discontinuous galerkin methods were applied to rdfm in feng et al 2021 and the contaminant transportation was also simulated different from the traditional dfm the interface model alboin et al 1999 2000 hansbo and hansbo 2002 odsæter et al 2019 explicitly represent the fractures as interfaces of the porous media then the governing equation of the flow in the lower dimensional fracture was constructed in the interface model the matrix and fractures are considered as two systems and the communication between them was given as the jump of the normal velocity along the fractures therefore different from rdfm the interface model though hanging nodes are allowable cannot be applied to structured meshes and the fracture must be aligned with the interfaces of the meshes for the matrix to fixed this limitation the xdfm was proposed fumagalli and scotti 2014 huang et al 2011 schwenck 2015 salimzadeh and khalili 2015 flemisch et al 2016 however these methods may increase the degrees of freedom dofs significantly and can hardly be applied to fracture networks with high geometrical complexity flemisch et al 2018 as an alternative the cutfem burman et al 2019 can be applied to non conforming meshes it couples the fluid flow in all lower dimensional manifolds however this method requires the fractures to cut the domain into completely disjoint subdomains thus it is not applicable for media with complicated fractures most of the above ideas work for problems with conductive fractures however if the media contains blocking fractures most methods may not be suitable to fix this gap the projection based edfm pedfm was introduced in tene et al 2017 and jiang and younis 2017b the effective flow area between adjacent matrix grids is computed as the difference between the original interface area and the projected area of the fracture segment it will be zero if the fracture fully penetrates through the matrix cell olorode et al 2020 extended the pedfm into three dimensional compositional simulation of fractured reservoirs however the pedfm still cannot describe the complex multiphase flow behavior in the matrix blocks within barrier fractures another approach is to follow the interface model introduced in martin et al 2005 angot et al 2009 boon et al 2018 and kadeethum et al 2020 however as demonstrated above the interface model can only handle hanging nodes and the fractures must align with the interfaces of the background mesh recently xu and yang extended the rdfm xu and yang 2020 feng et al 2021 to problems with blocking fractures in xu et al 2021 the basic idea is to apply ohm s law and superpose the resistance the reciprocal of the permeability of the blocking fracture to that of the matrix then a modified partial differential equation system was introduced and the local discontinuous galerkin methods with suitable penalty were perfectly applied if the problems contains only blocking fractures the mixed finite element methods can easily be combined with rdfm in this paper we combine the ideas in alboin et al 1999 and xu et al 2021 to propose a novel model for single phase flows in porous media the idea works for problems containing both conductive and blocking fractures in particular the interface model alboin et al 1999 was used to enforce the continuity of pressure across the conductive fractures while the dirac δ functions were applied for the blocking fractures to superpose the resistance following the main idea given in xu et al 2021 the separate treatment of conductive and blocking fractures may yield difficulties in constructing mathematical models therefore one of the major novelty of the current work is the seamless combination of the conductive fracture interface model and the blocking fracture dirac δ function approach with the new mathematical model we further construct numerical discretization by using a hybrid mixed finite element method the proposed method has the following features 1 it produces locally conservative velocity approximations 2 it leads to a symmetric positive definite linear system 3 it only produces globally coupled degrees of freedom dofs of pressure on the mesh skeletons therefore the numerical scheme yields much less total dofs and is easy to implement and this is the main advantage compared with the xdfm pedfm and cutfem methods moreover thanks to the dirac δ functions for blocking fractures the method does not require any mesh conformity with respect to the blocking fractures which is another major novelty of our proposed scheme compared with the interface model to the best knowledge of the authors our approach is the simplest one that can be applied to non conforming mesh for blocking fractures that still yield locally conservative velocity approximations we note that mesh conformity with respect to the conductive fractures is still required for our method which is typical for interface models we numerically demonstrate that our hybrid mixed finite element scheme is highly competitive both in terms of computational efficiency and accuracy as an application we couple the proposed flow equation with the simple transport equations and construct the locally conservative hybrid finite volume methods for the transport equations we finally emphasis that the proposed hybrid mixed formulation is different from the mixed method in boon et al 2018 due to the use of different model for the interface conditions we believe that our model is significantly simpler for complex fracture networks since we only use one matrix domain and one codimension 1 conductive fracture domain throughout while the mixed method formulation boon et al 2018 needs to split the matrix and fracture domains into multiple disjoint sub domains and require the modeling of codimension 1 3 fracture flows which might be very tedious to perform for complex fracture networks the rest of the paper is organized as follows in section 2 we present the hybrid dimensional model under consideration we then formulate in section 3 the hybrid mixed finite element discretization of the model proposed in section 2 numerical results for various benchmark test cases are presented in section 4 we conclude in section 5 2 the hybrid dimensional model 2 1 notation we consider a bounded open domain ω m r d d 2 3 which contains several d 1 dimensional conductive or blocking fractures for simplicity the fractures are assumed to be hyperplanes with smooth boundaries we denote ω c as the d 1 dimensional open set containing all the conductive fractures and ω b as the set containing all the blocking fractures assume the d 1 dimensional domain boundary ω m γ d γ n with γ d γ n 0 furthermore we denote the following sets of d 2 dimensional boundaries intersections associated with the set of conductive fractures ω c γ c c is the set containing the intersections among conductive fractures γ c b is the set containing the intersections between 2 conductive and blocking fractures γ c m is the set containing the intersections between conductive fractures and domain boundary ω m which is further split to γ c m γ c m n γ c m d with γ c m n γ n and γ c m d γ d γ c i is the boundary of ω c that does not intersect with the domain boundary ω m we set γ c γ c c γ c b γ c m γ c i as the collections of all intersections of ω c an illustration of a typical hybrid dimensional domain in two dimensions is given in fig 1 we denote n γ as a uniquely oriented unit normal vector on a d 1 dimensional interface boundary γ and denote η γ as the in plane unit outer normal vector on the d 2 dimensional boundary γ of γ see fig 2 let ε be the thickness of the fractures which is assumed to be a small positive constant for simplicity let k m be the permeability tensor of the domain excluding the fractures ω m ω c ω b k b k m be the scalar permeability in the normal direction of blocking fractures ω b and k c k m be the permeability tensor in the tangential direction of the conductive fractures ω c 2 2 the hybrid dimensional flow model the following hybrid dimensional model is a combination of the conductive fracture treatment in alboin et al 1999 and blocking fracture treatment in xu et al 2021 we apply the dirac δ functions in the bulk domain ω m ω c excluding conductive fractures and the model reads 1a k m 1 ε k b δ ω b n ω b n ω b u p in ω m ω c 1b u f in ω m ω c where u is the darcy velocity p is the pressure f is the volume source term δ ω b is the dirac δ function that takes values on the blocking fractures ω b and zero elsewhere and n ω b is the unit normal vector on ω b the basic idea is to superpose the resistance of the porous media to reduce the darcy velocity in the normal direction of the blocking fractures within the conductive fractures excluding intersections ω c γ c we use the following d 1 dimensional darcy s law 1c ε k c 1 u c γ p c in ω c γ c 1d γ u c u in ω c γ c where u c is the tangential darcy velocity in the conductive fractures p c is the associated pressure and the velocity jump u u u n γ represents the mass exchange between the conductive fractures and the surrounding media where u x lim τ 0 u x τ n γ for all x ω c is the bulk darcy velocity evaluated on one side of the conductive fractures moreover γ and γ are the usual surface gradient and surface divergence operators the above equations give the modified darcy s law for flow in porous media containing both conductive and blocking fractures we close the hybrid dimensional system with the following set of boundary interface conditions 1e p p d on γ d 1f u n q n on γ n 1g p p c on ω c 1h u c 0 on γ c c 1i p c p d on γ c m d 1j u c η γ 0 on γ c b γ c m n γ c i where 1g ensures continuity of bulk pressure across conductive fractures the no flow boundary condition in 1j is imposed on the intersections γ c b γ c m n and γ c i and the jump term in 1h is u c e γ ω c γ c e γ u c γ η γ e γ c c which represents mass conservation along intersections γ c c note in particular that each conductive fracture containing the intersection e appears exactly twice in the above summation and the in plane normal velocity on the fracture is allowed to be discontinuous along the intersection e for example the jump u c h at node h in the configuration in fig 1 is u c h γ e h h k g h h j u c γ η γ we note that in the above model 1 the flow in the tangential direction in the blocking fractures is completely ignored as the permeability therein is much smaller than that of the surroundings on the other hand the flow in the normal direction is ignored on conductive fractures by the pressure continuity condition 1g since the permeability is much larger than that of the surroundings and the fluid has a tendency to flow along the tangential direction therein 2 3 the hybrid dimensional transport model we now consider a scalar quantity c that is transported through the porous medium subject to the velocity fields in the flow model 1 here c usually represents the concentration of a generic passive tracer similar to the flow treatment in the previous subsection transport inside the blocking fractures is ignored the concentrations c in the matrix and c c in the conductive fractures are governed by the following advection equations see e g alboin et al 2002 fumagalli and scotti 2013 odsæter et al 2019 2a ϕ m c t u c c f in ω m ω c 0 t 2b ε ϕ c c c t γ u c c c c u 0 in ω c 0 t with the following initial interface and boundary conditions 2c c c c on ω c 0 t 2d c c 0 on ω 0 c c c c 0 on ω c 0 2e c c b on ω i n 0 t c c c c b on γ i n 0 t where ϕ m c 0 c b ω i n and ϕ c c c 0 c c b γ i n represent the porosity initial concentration inflow concentration and inflow boundary in the matrix and conductive fractures respectively observe that concentration continuity 2c across the conductive fractures are enforced in the model 2 3 the hybrid mixed finite element method 3 1 preliminaries let t h k be a conforming simplicial triangulation of the domain ω m let e h be the collections of d 1 dimensional facets edges for d 2 faces for d 3 of ω m assume the mesh is fully fitted with respect to the conductive fractures i e t h c ω c e h is a d 1 dimensional simplicial triangulation of the domain ω c here the mesh t h is allowed to be unfitted with respect to the blocking fractures moreover we denote e h c as the collection of d 2 dimensional facets of t h c vertices for d 2 edges for d 3 we use the lowest order hybrid mixed finite element methods to discretize the model 1 the following finite element spaces will be needed 3a v h v l 2 t h d v k r t 0 k k t h 3b w h w l 2 t h w k p 0 k k t h 3c m h μ l 2 e h μ f p 0 f f e h 3d v h c v c l 2 t h c d v f r t 0 f f t h c 3e m h c μ l 2 e h c μ e p 0 e e e h c where r t 0 s is the raviart thomas space of lowest order on a simplex s and p 0 s is the space of constants we denote the following inner products ϕ ψ t h k t h k ϕ ψ dx ϕ ψ t h k t h k ϕ ψ ds ϕ ψ t h c f t h c f ϕ ψ ds ϕ ψ t h c f t h c f ϕ ψ dr where dx is for d dimensional integration ds is for d 1 dimensional integration and dr is for d 2 dimensional integration when d 2 f ϕ ψ dr is simply the sum of point evaluations at the two end points of a line segment f 3 2 the hybrid mixed method for the flow model the hybrid mixed method for the hybrid dimensional model 1 is given as follows find u h p h p h u h c p h c v h w h m h v h c m h c with p h γ d p 0 p d and p h c γ c m d p 0 p d where p 0 denotes the projection onto piecewise constants such that 4a k m 1 u h v h t h ω b ε k b u h n v h n ds p h v h t h p h v h n t h 0 4b u h q h t h f q h t h 0 4c u h n q h t h γ u h c q h t h c γ n q n q h ds 0 4d ε k c 1 u h c v h c t h c p h γ v h c t h c p h c v h c η t h c γ c b α ε k c 1 u h c η v h c η dr 0 4e u h c η q h c t h c 0 for all v h q h q h v h c q h c v h w h m h v h c m h c with q h γ d q h c γ c m d 0 where α 0 is a penalty parameter for the implementation of the no flow boundary condition 1j on γ c b in our numerical implementation we take α 1 0 6 we show that the scheme 4 is formally consistent with the hybrid dimensional model 1 1 eq 4a is a discretization of the darcy s law 1a in the bulk using integration by parts and the following property of dirac δ function ω m δ ω b ϕ dx ω b ϕ ds 2 eq 4b is the discretization of mass conservation 1b in the bulk 3 eq 4c simultaneously enforces i the continuity of normal velocity u h n across interior element boundaries e h t h c γ n ii the boundary condition 1f on γ n and iii mass conservation 1d within the conductive fractures in t h c 4 eq 4d is a discretization of the darcy s law 1c on the conductive fractures t h c where the pressure continuity condition 1h is also strongly enforced as p h both represents the bulk pressure on the element boundary e h and the pressure within the conductive fracture t h c moreover the last term in 4d is a penalty formulation of the no flow boundary condition 1j on γ c b note that γ c b is allowed to be not aligned with the facets of t h c 5 eq 4e is a transmission condition that simultaneously enforces i continuity of in plane normal velocity u h c η on interior facets e h c γ c c γ c m n γ c i ii the mass conservation 1h on the intersections γ c c iii the no flow boundary condition 1j on γ c m n and γ c i 6 the dirichlet boundary condition 1e and 1i are imposed strongly through the corresponding degrees of freedom dofs on p h and p h c respectively the following result further shows that the scheme 4 is well posed theorem 3 1 assume the measure of the dirichlet boundary γ d is not empty then the solution to the scheme 4 exists and is unique proof since the equations in 4 leads to a square linear system we only need to show uniqueness now we assume the source terms in 4 vanishes i e f p d g n 0 taking test function to be the same as trial functions in 4 and adding we get k m 1 u h u h t h ω b ε k b u h n 2 ds ε k c 1 u h c u h c t h c 0 hence u h u h c 0 since u h 0 the inf sup stability of the r t 0 p 0 finite element pair implies that p h p h c from 4a where c is a constant since γ d is not empty and p d 0 we get the constant c 0 finally restricting eq 4d to a single element f t h c and using the fact that u h c 0 and p h 0 we get f p h c v h c η ds 0 v h c r t 0 f which then implies that p h c 0 this completes the proof 3 3 static condensation and linear system solver the linear system 4 can be efficiently solved via static condensation where the dofs for u h p h and u h c can be locally eliminated resulting in a coupled global linear system for the dofs for p h and p h c which is symmetric and positive definite efficient linear system solvers for the resulting condensed system is an interesting topic where one could design efficient decoupling algorithms or robust monolithic preconditioners here we simply use a sparse direct solver in the computation and postpone the detailed study of linear system solvers to our future work 3 4 local pressure postprocessing we use the following well known local piecewise linear pressure postprocessing to improve the accuracy of pressure approximation in the bulk find p h w h w l 2 t h w k p 1 k k t h where p 1 k is the space of linear polynomials on element k such that 5a p h q h t h k m 1 u h q h t h 5b p h 1 t h p h 1 t h for all q h w h 3 5 the hybridized finite volume method for the transport model we consider a standard cell centered first order upwinding finite volume scheme for the transport model 2 coupled with the implicit euler method for the temporal discretization we hybridize the cell centered finite volume scheme so that the coupled unknowns live on the mesh skeletons which simplifies the definition of upwinding fluxes on the conductive fracture interactions e g point h in fig 1 hence we use piecewise constant spaces to approximate the matrix concentration c h w h on the mesh t h the matrix concentration c h m h on the matrix mesh skeleton e h and the fracture concentration c c h m h c on the fracture mesh skeleton e h c the hybridized finite volume scheme with implicit euler temporal discretization is given as follows given data c h n 1 c h n 1 w h m h at time t n 1 find c h n c h n c c h n w h m h m h c at time t n t n 1 δ t with c h n ω i n p 0 c b t n and c c h n γ i n p 0 c c b t n such that 6a ϕ m c h n c h n 1 δ t d h t h u h n c h n d h t h c h n f d h t h 6b u h n c h n d h t h ε ϕ c c h n c h n 1 δ t d h t h c u h c η c c h n d h t h c 0 6c u h c η c c h n d c h t h c 0 for all d h d h d c h w h m h m h c with d h ω i n 0 and d c h γ i n 0 where the upwinding fluxes are given as follows 6d c h n k c h n if u h n k 0 c h n if u h n k 0 6e c c h n f c h n if u h c η f 0 c c h n if u h c η f 0 3 6 remarks on the mesh restrictions and comparison with existing methods the proposed flow and transport solvers 4 6 require the mesh to be fitted to the conductive fractures while allowing for an unfitted treatment of the blocking fractures actually this is one of the major novelty of the proposed work while the derivation of numerical schemes that work on fully unfitted meshes is beyond the scope of this paper here we propose a simple mesh postprocessing technique to convert a general unfitted background matrix mesh to an immersed mesh that is fitted to all the fractures similar immersing mesh techniques were used for interface problems ilinca and hétu 2011 frei and richter 2014 auricchio et al 2015 chen et al 2017 below we illustrate the procedure of immersing a single fracture to an unfitted tetrahedral mesh in 3d i represent the fracture geometry as the zero level set of a continuous piecewise linear function ϕ h on the background mesh perturb ϕ h slightly if necessary to avoid fracture pass through the background mesh nodes ii loop over the background mesh edges find the cut edges where ϕ h has opposite sign on the two edge endpoints for each cut edge compute the coordinates of the cut vertex v c where ϕ h v c 0 and add v c to the mesh nodes iii loop over the background mesh faces find the cut faces which contains the cut vertices order the cut vertices based on their vertex label number loop over the cut vertices for each sub face that contains the cut vertex split the sub face by 2 by connecting the cut vertex with the opposite sub face node iv loop over the background mesh elements find the cut elements which contains the cut vertices order the cut vertices based on their vertex label number loop over the cut vertices for each sub element that contains the cut vertex split the sub element by 2 by connecting the cut vertex with the opposite two sub element nodes that are not aligned with the cut edge the above recursive bisection procedure guarantees that the fracture lies on the boundary of the generated immersed mesh the case with multiply intersecting fractures can be treated by recursion here we note that the generated immersed mesh is usually highly anisotropic since the background mesh is completely independent of the fracture configurations our numerical results in the next section suggest that the hybrid mixed method 4 works well on these anisotropic immersed meshes typical 2d immersed meshes for complex fracture configurations are given in figs 11 and 14 we now briefly compare our proposed fractured flow solver 4 with some existing schemes in berre et al 2021 which were used to solve a series of 4 benchmark problems in 3d fractured porous media flow among the 17 schemes in berre et al 2021 table 1 7 were shown to yield no significant deviations for all the tests see berre et al 2021 figure 18 which include the multi point flux approximation uib mpfa the lowest order mixed virtual element method uib mvem and the lowest order raviart thomas mixed finite element method uib rt0 mainly developed by the research group in the university of bergen keilegavlen et al 2021 nordbotten et al 2019 boon et al 2018 the mpfa scheme ustutt mpfa and the two point flux approximation scheme ustutt tpfa circ developed by flemisch et al 2011 the mimetic finite difference method lanl mfd lipnikov et al 2014 and the hybrid finite volumes discontinuous hydraulic head method unice unige hfv disc developed by brenner et al 2017 among these 7 schemes the first three schemes use a mixed dimensional interface model that require the modeling of co dimension 1 3 fractured flows where the mesh can be non matching across subdomains but needs to be geometrically conforming to the fractures on the other hand the last four schemes work on a mixed dimensional interface model where only fractured flow in co dimension 1 were modeled which require the mesh to be completely conforming to the fractures all of these schemes yield a locally conservative velocity approximation we further note that the two methods in berre et al 2021 that allow for general nonconforming meshes namely the lagrange multiplier method köppel et al 2019a b schädle et al 2019 and the edfm method nikitin and yanbarisov 2020 cannot handle blocking fractures and do not provide a locally conservative velocity approximation numerical results of our proposed scheme 4 for the benchmark problems in berre et al 2021 indicate that our results yield no significant deviations with the above mentioned 7 schemes see details in the next section our scheme also produce a locally conservative velocity approximation and the resulting linear system after static condensation is a symmetric positive definite spd problem with global unknowns involve pressure dofs on the mesh skeleton only the number of the global unknowns of our scheme is roughly n f which is the total number of mesh faces and the average nonzero entries per row in the system matrix is 7 a pressure dof on an interior tetrahedral face is connected to 6 neighboring face pressure dofs concerning the computational cost of our scheme it is more expensive than the tpfa scheme ustutt tpfa circ which lead to an spd system with roughly n c cell wise pressure dofs and about 5 nonzero entries per row in the system matrix is slightly less expensive than the cell based mpfa schemes uib mpfa ustutt mpfa which lead to spd systems with roughly n c cell wise pressure dofs and about 20 50 nonzero entries per row in the system matrix and is significantly cheaper than the schemes uib mvem uib rt0 lanl mfd and unice unige hfv disc which lead to saddle point systems with total number of roughly n f velocity dofs and n c pressure dofs note that n f 2 n c hence our proposed scheme is also highly competitive in terms of computational costs another distinctive advantage of our scheme over these 7 schemes is that the mesh can be completely nonconforming to the blocking fractures 4 numerics in this section we present detailed numerical results for the proposed hybrid mixed method for the four 2d benchmark test cases in flemisch et al 2018 and the four 3d benchmark test cases in berre et al 2021 we name the method 4 as hm dfm since it is a hybrid mixed method for a discrete fracture model when plotting the pressure or hydraulic head distribution over line segments we evaluate the second order postprocessed solution in 5 for the proposed method the focus of the numerical experiments is on the verification of the accuracy of our proposed flow model 1 and the associated method 4 hence we test the flow solver 4 for all the 8 benchmark cases meanwhile we also test the accuracy of velocity approximation by feeding them to the transport problem 2 which is solved using the scheme 6 for three cases namely benchmark 2 in 2d and benchmark 5 6 in 3d furthermore convergence study via mesh refinements was conducted for benchmark 2 and benchmark 6 below our numerical simulations are performed using the open source finite element software ngsolve schöberl 2014 https ngsolve org jupyter notebooks for reproducing all numerical examples in this section can be found in the git repository https github com gridfunction fracturedporousmedia visualization of meshes for the 3d benchmark examples and interactive contour plots of the pressure hydraulic head can also be found therein 4 1 benchmark 1 hydrocoin 2d this example is originally a benchmark for heterogeneous groundwater flow presented in the international hydrocoin project ski 1987 a slight modification for the geometry was made in flemisch et al 2018 section 4 1 and we follow the settings therein in particular the bulk domain is a polygon with vertices a 0 150 b 400 100 c 800 150 d 1200 100 e 1600 150 f 1600 1000 g 1500 1000 h 1000 1000 and i 0 1000 measured in meters there are two conductive fractures in the domain b g and d h the fracture b g has thickness ε 5 2 m and the fracture d h has thickness ε 33 5 m the permeability hydraulic conductivity is k m 1 0 8 m s in the bulk and k c 1 0 6 m s in the fractures dirichlet boundary condition p height is imposed on the top boundary and homogeneous neumann boundary condition is imposed on the rest of the boundary here the unknown variable p is termed as the piezometric head according to ski 1987 the quantity of interest is the distribution of the piezometric head p along the horizontal line at a depth of 200 m we apply the method 4 on a uniform triangular mesh with mesh size h 60 see the left panel of fig 3 which leads to 1115 matrix elements and 44 fracture elements on this mesh the number of the globally coupled dofs is 1779 in which 1691 dofs are associated with the bulk hybrid variable p h and 43 dofs are associated with the fracture hybrid variable p h c in the right panel of fig 3 we record the postprocessed piezometric head p h in 5 along the line segment z 200 m where z is the horizontal direction along with the reference data obtained from a mimetic finite difference method on a very fine mesh with 889 233 dofs we observe that the results for the proposed method on such a coarse mesh already shows a good agreement with the reference data 4 2 benchmark 2 regular fracture network 2d this test case is originally from geiger et al 2013 and is modified by flemisch et al 2018 which simulates a regular fracture network in a square porous media the computational domain including the fracture network and boundary conditions is shown in fig 4 the matrix permeability is set to k m i and fracture thickness is ε 1 0 4 two cases of fracture permeability was considered i a highly conductive network with k c 1 0 4 i ii a blocking fracture with k b 1 0 4 we apply the method 4 on a triangular mesh with 1348 matrix elements and 91 fracture elements see the left panel of fig 5 for the blocking fracture case we also present the result on a unfitted triangular mesh with 1442 matrix elements for the conductive fracture case the number of the globally coupled dofs is 2127 in which 2041 dofs are associated with the bulk hybrid variable p h and 86 dofs are associated with the fracture hybrid variable p h c the pressure distributions along two lines one horizontal at y 0 7 and one vertical at x 0 5 are shown in fig 6 along with the reference data obtained from a mimetic finite difference method on a very fine mesh with 1 175 056 dofs similar to the previous example we observe that the results for the proposed method show a good agreement with the reference data for the blocking fracture case the number of the globally coupled dofs is 2041 on the fitted mesh and is 2188 on the unfitted mesh the pressure distribution along the lines 0 0 1 0 9 1 0 is shown in fig 7 again we observe a very good agreement with reference data for the results on the fitted mesh the result on the unfitted mesh case is slightly off due to mesh nonconformity which is expected as it could not capture the pressure discontinuity across the blocking fractures 4 2 1 coupling with transport and convergence study with mesh refinements after the velocity fields are computed from the scheme 4 we feed them to the transport model 2 and solve it by using the hybrid finite volume scheme 6 we take the porosities ϕ m 0 1 ϕ c 0 9 in the model 2 with the initial concentrations c 0 c c 0 0 and set the left boundary as the inflow boundary for the concentrations with c b c c b 1 the final time of simulation is t 0 1 convergence of our coupled scheme 4 and 6 is checked via a mesh refinement study where the initial meshes are given in fig 5 and three level of uniform mesh refinements are applied afterwards the constant time step size is taken to be δ t 2 l 5 1 0 3 where l is the mesh refinement level since there is no analytic solution to the problem we provide a reference solution using the coupled scheme 4 and 6 on the fourth level refined fitted mesh with about 345k elements with a small time step size δ t 3 125 1 0 5 contour of matrix concentrations of the reference solution at time t 0 05 and t 0 1 are presented in fig 8 where we clearly observe the conducting and blocking effects of the respective fractures moreover we plot the computed matrix concentrations along the cut line y 0 7 in fig 9 where we observe convergence as the mesh refines finally the l 2 errors in the matrix velocity u h u r e f ω m and postprocessed matrix pressure p h p r e f ω m and the l 2 errors in the matrix concentration c h t c r e f t ω m at final time t 0 1 are recorded in table 1 for the conductive fracture case in table 2 for the blocking fracture case on fitted meshes and in table 3 for the blocking fracture case on unfitted meshes where we recall that the reference solutions are obtained on the fourth level refined fitted mesh with a small time step size δ t 3 125 1 0 5 here the rate of convergence for e r r at level i is estimated via the formula r a t e log e r r i 1 e r r i log 2 from table 1 for the conductive fracture case we observe that the convergence rate in the velocity approximation is first order and that in the postprocessed pressure approximation is second order which is consistent with the expected convergence behavior of the hybrid mixed method for the equi dimensional case raviart and thomas 1977 arnold and brezzi 1985 and the convergence rate for the concentration is about 1 2 which is also expected for the hybridized finite volume scheme due to the concentration discontinuities in the domain similar convergence behavior was observed in table 2 for the blocking fracture case on fitted meshes from table 3 we observe 1 2 order convergence for all three variables where the degraded velocity and pressure convergence is due to nonconformity of the mesh with the fractures 4 3 benchmark 3 complex fracture network 2d this test case considers a small but complex fracture network that includes permeable and blocking fractures the domain and boundary conditions are shown in fig 10 the exact coordinates for the fracture positions are provided in flemisch et al 2018 appendix c the fracture network contains ten straight immersed fractures the fracture thickness is ε 1 0 4 for all fractures and permeability is k c 1 0 4 for all fractures except for fractures 4 and 5 which are blocking fractures with k b 1 0 4 note that we are considering two subcases a and b with a pressure gradient which is predominantly vertical and horizontal respectively we apply the method 4 on two set of meshes a triangular fitted mesh with 1332 matrix elements and 88 fracture elements which was provided in the git repository https git iws uni stuttgart de benchmarks fracture flow see left of fig 11 and a triangular immersed fitted mesh with 1370 matrix elements and 211 fracture elements obtained from a background unfitted mesh using the immersing mesh technique introduced in section 3 6 see right of fig 11 the globally coupled dofs is 2066 for the fitted mesh and is 2211 for the immersed mesh the pressure distributions along the lines 0 0 5 1 0 0 9 are shown in fig 12 we observe that the results on the two meshes are very close to each other and they are in good agreements with the reference data obtained from a mimetic finite difference method on a very fine mesh with 1 8 million dofs 4 4 benchmark 4 a realistic case 2d we consider a real set of fractures from an interpreted outcrop in the sotra island near bergen in norway the size of the domain is 700 m 600 m with uniform scalar permeability k m 1 0 14 m 2 the set of fractures is composed of 64 line segments in which the permeability is k c 1 0 8 m 2 the fracture thickness is ε 1 0 2 m the exact coordinates for the fracture positions are provided in the above mentioned git repository the domain along with boundary conditions is given in fig 13 similar to the previous example we apply the method 4 on two set of conforming meshes a fitted mesh consists of 10 807 matrix elements and 1047 fracture elements provided in https git iws uni stuttgart de benchmarks fracture flow see left of fig 14 and an immersed fitted mesh consists of 5473 matrix elements and 1541 fracture elements obtained from a background unfitted mesh using the immersing mesh technique introduced in section 3 6 see right of fig 14 the number of the globally coupled dofs is 17 253 for the fitted mesh a and 9753 for the immersed mesh b the pressure distribution along the two lines y 500 m and x 625 m are shown in fig 15 along with the results for the mortar dfm method with 25 258 dofs from flemisch et al 2018 we observe that the three results are in good agreements with each other with the hdg dfm b using the least amount of dofs 4 5 benchmark 5 single fracture 3d this is the first benchmark case proposed in berre et al 2021 to be consistent with the notation in berre et al 2021 the pressure and permeabilities are renamed as hydraulic head and hydraulic conductivities respectively for this test case and the three examples following fig 16 illustrates the geometrical description here the domain ω is a cube shaped region 0 m 100 m 0 m 100 m 0 m 100 m which is crossed by a conductive planar fracture ω 2 with a thickness of ε 1 0 2 m the matrix domain consists of subdomains ω 3 1 above the fracture and ω 3 2 and ω 3 3 below the subdomain ω 3 3 represents a heterogeneity within the rock matrix the matrix conductivities are given in fig 16 and the fracture conductivity is k c 0 1 so that ε k c 1 0 3 inflow into the system occurs through a narrow band defined by 0 m 0 m 100 m 90 m 100 m similarly the outlet is a narrow band defined by 0 m 100 m 0 m 0 m 10 m at the inlet and outlet bands we impose the hydraulic head h i n 4 m and h o u t 1 m respectively the remaining parts of the boundary are assigned no flow conditions following the setup in berre et al 2021 we set c b 0 01 m 3 at the inlet boundary for the transport problem the matrix porosity ϕ is taken to be 0 2 on ω 3 1 ω 3 2 and 0 25 on ω 3 3 and the fracture porosity ϕ c is taken to be 0 4 the final time of simulation is t 1 0 9 s and the time step size is δ t 1 0 7 s we perform the method 4 and 6 on a coarse tetrahedral mesh with 10 232 matrix elements and 448 fracture elements and a fine tetrahedral mesh with 111 795 matrix elements and 1758 fracture elements the number of the globally coupled dofs on the coarse mesh is 23 377 while that on the fine mesh is 235 619 the hydraulic head along the line 0 m 100 m 100 m 100 m 0 m 0 m is shown in fig 17 along with reference data and published spread provided in the git repository https git iws uni stuttgart de benchmarks fracture flow 3d the reference data in fig 17 is obtained from the ustutt mpfa method on a mesh with approximately 1 million matrix elements while the shaded region depicts the area between the 10th and the 90th percentile of the published results in berre et al 2021 on mesh refinement level 1 left 10 k cells and refinement level 2 right 100 k cells the match number results from evaluating at 100 evenly distributed evaluation points if the value for the hm dfm method is between the respective lower and upper value we observe that our result agrees with the reference values quite well especially on the fine mesh moreover we plot the matrix concentration along the line 0 m 100 m 100 m 100 m 0 m 0 m in fig 18 and the fracture concentration along the line 0 m 100 m 80 m 100 m 0 m 20 m at final time t 1 0 9 s in fig 19 together with the published spread provided in the git repository which depicts the area between the 10th and the 90th percentile of the published results in berre et al 2021 using similar first order finite volume schemes with implicit euler time stepping and δ t 1 0 7 s we observe that our results agree quite well with the provided data 4 6 benchmark 6 regular fracture network 3d this is the second benchmark case proposed in berre et al 2021 which is a 3d analog of benchmark 2 the domain is given by the unit cube ω 0 m 1 m 3 and contains 9 regularly oriented fractures as illustrated in fig 20 dirichlet boundary condition p h 1 m is imposed on the boundary γ d x y z ω x y z 0 875 m neumann boundary condition u n 1 m s is imposed on the boundary ω i n x y z ω x y z 0 25 m and no flow boundary condition is imposed on the remaining boundaries the heterogeneous matrix conductivity is illustrated in fig 20 and the fracture conductivity is either k c 1 0 4 m 2 which represents a conductive fracture or k b 1 0 4 m 2 which represents a blocking fracture the fracture thickness is ε 1 0 4 m for the transport equation matrix porosity is taken to be ϕ 0 1 conductive fracture concentration is ϕ c 0 9 and the inflow boundary condition c b 1 m 3 is set on the inlet boundary ω i n final time of the simulation is t 0 25 s we perform the method 4 on a coarse fitted tetrahedral mesh with 4375 matrix elements and 944 fracture elements and a fine tetrahedral mesh with 36 336 matrix elements and 4524 fracture elements the number of the globally coupled dofs on the coarse mesh is 13 373 for the conductive fracture case and 8334 for the blocking fracture case only dofs for p h are global dofs in this case while that on the fine mesh is 94 738 for the conductive fracture case and 70 881 for the blocking fracture case the hydraulic head along the diagonal line 0 m 0 m 0 m 1 m 1 m 1 m is shown in fig 21 for the conductive fracture case and in fig 22 for the blocking fracture case we observe that our results agree with the reference values very well which were obtained from the ustutt mpfa method on a mesh with approximately 1 million matrix elements the small derivation of our result on the left panel of fig 21 with the reference data is acceptable due to the use of a very coarse mesh we further performed a convergence study of the flow and transport solvers 4 and 6 via mesh refinements and record the l 2 errors in matrix velocity and postprocessed pressure and the l 2 errors in matrix concentration at final time t 0 25 in table 4 for the conductive fracture case and in table 5 for the blocking fracture case where the initial mesh is the coarse one with 4375 tetrahedral elements a total of three uniform mesh refinements was performed and the solution on the third level mesh was used as the reference solution to calculate the associated errors the time step size is taken to be δ t 2 l 2 5 1 0 3 s where l is the mesh refinement level on the finest mesh there are about 2 25 million tetrahedral elements and 4 5 million globally coupled dofs from both tables we observe convergence of our schemes and in particular the convergence rate for the velocity is approaching first order that for the postprocessed pressure is approaching second order and for the concentration is about first order finally in fig 23 we plot slices of concentrations computed on the 3rd refined mesh at final time t 0 25 along the five vertical planes x 0 1 x 0 3 x 0 5 x 0 7 and x 0 9 and in fig 24 we plot the evolution of mean concentration over time on the following three regions ω a 0 5 m 1 m 0 m 0 5 m 0 m 0 5 m ω b 0 5 m 0 75 m 0 5 m 0 75 m 0 75 m 1 m ω c 0 75 m 1 m 0 75 m 1 m 0 5 m 0 75 m ω a 0 5 m 1 m 0 m 0 5 m 0 m 0 5 m ω b 0 5 m 0 75 m 0 5 m 0 75 m 0 75 m 1 m ω c 0 75 m 1 m 0 75 m 1 m 0 5 m 0 75 m from the results in fig 23 we clearly observe the different flow pattern for the conductive fracture case in the first row and the blocking fracture case in the second row we further note that the mean concentrations reported in fig 24 were presented in berre et al 2021 figure 10 only on the coarse mesh with about 4 k matrix elements and a coarse time step size δ t 2 5 1 0 3 s our results on four set of meshes are close to each other and improve slightly as the mesh and time step size refines and they are also qualitatively similar to the majority of the coarse grid results in berre et al 2021 figure 10 4 7 benchmark 7 network with small features 3d this is the third benchmark case proposed in berre et al 2021 in which small geometric features exist that may cause trouble for conforming meshing strategies the domain is the box ω 0 m 1 m 0 m 2 25 m 0 m 1 m containing 8 fractures see fig 25 homogeneous dirichlet boundary condition is imposed on the outlet boundary ω o u t x y z 0 x 1 y 2 25 z 1 3 o r z 2 3 inflow boundary condition u n 1 m s is imposed on the inlet boundary ω i n x y z 0 x 1 y 0 1 3 z 2 3 and no flow boundary condition is imposed on the remaining boundaries the conductivity in the matrix is k m 1 m 2 and that in the fracture is k c 1 0 4 m 2 fracture thickness is ε 0 01 m we perform the method 4 on a coarse tetrahedral mesh with 31 812 matrix elements and 3961 fracture elements and a fine tetrahedral mesh with 147 702 matrix elements and 9441 fracture elements the number of the globally coupled dofs on the coarse mesh is 83 022 while that on the fine mesh is 343 359 the hydraulic head along the line 0 5 m 1 1 m 0 m 0 5 m 1 1 m 1 m is shown in fig 26 where the reference data is obtained with the ustutt mpfa scheme on a grid with approximately 1 0 6 matrix cells here we observe a very good agreement with the reference data even on the coarse mesh 4 8 benchmark 8 field case 3d this is the last benchmark case proposed in berre et al 2021 the geometry is based on a postprocessed outcrop from the island of algerøyna outside bergen norway which contains 52 fracture the simulation domain is the box ω 500 m 350 m 100 m 1500 m 100 m 500 m the fracture geometry is depicted in fig 27 homogeneous dirichlet boundary condition is imposed on the outlet boundary ω o u t 500 100 400 100 100 ω o u t 0 350 100 400 100 100 ω o u t 1 uniform unit inflow u n 1 m s is imposed on the inlet boundary ω i n 500 1200 1500 300 500 ω i n 0 500 200 1500 300 500 ω i n 1 conductivity is k m 1 m 2 in the matrix and k c 1 0 4 m 2 in the fracture fracture thickness is ε 1 0 2 m we perform the method 4 on a tetrahedral mesh with 241 338 matrix elements and 47 154 fracture elements the number of the globally coupled dofs is 696 487 the hydraulic head along the two diagonal lines 500 m 100 m 100 m 350 m 1500 m 500 m and 350 m 100 m 100 m 500 m 1500 m 500 m are shown in fig 28 along with published results from berre et al 2021 similar to benchmark 4 in 2d no reference data on refined meshes was provided for this problem due to its complexity comparing with the published results in fig 28 we observe that our method still performs quite well 5 conclusion a novel hybrid mixed method for single phase flow in fractured porous media has been presented the proposed model combines the interface model for the conductive fractures and time dirac δ functions approach for the blocking fractures the distinctive features for the numerical methods include local mass conservation symmetric positive definite linear system and allowing the computational mesh to be completely non conforming to the blocking fractures ample benchmark tests show the excellent performance of the proposed scheme which is also highly competitive with existing work in the literature extension to the method to more complex fractured flow models and adaptation of the method to more general meshes consists of our on going work we will also investigate efficient preconditioning procedures for the associated linear system problem in the near future credit authorship contribution statement guosheng fu methodology software writing original draft funding acquisition yang yang conceptualization writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
198,in the nonstationary hydrological frequency analysis ns hfa the profile likelihood pl method has shown superiority in uncertainty estimation due to its higher accuracy and realistic asymmetric estimates however its wide application has been hindered by two issues namely the high computational burden and the numerical instability problems arising when dealing with short datasets this paper aimed to solve or mitigate these two issues to reduce its computational burden the classical regula falsi numerical method was incorporated into the pl method namely the rf pl method for estimating the bounds of the confidence intervals through an explicit analytical expression of the reparametrized log likelihood function rather than computing the full profile the generalized maximum likelihood principle which constrains the distribution shape parameter to a physically statistically reasonable range was extended to the proposed rf pl method namely the rf gpl method to handle short datasets the proposed methods were applied to eight annual maximum series of flow and precipitation from north america which present temporal trends in the mean and or the standard deviation to demonstrate their efficiency the results showed that the rf pl method substantially reduced the computational time of the pl method by 94 96 without degrading its estimation accuracy moreover the rf gpl was proven to be effective in avoiding or substantially mitigating the numerical instability issue of the rf pl method for small sample sizes in addition to reducing the uncertainty in the estimates as expected the superiority of the rf gpl method decreases with the increase of the sample size which is beneficial in enhancing the estimation of the shape parameter in the rf pl method therefore the rf gpl method is advantageous over the rf pl method for short datasets whereas its outperformance would be case dependent for long datasets these advancements overcome the common hurdles of the pl method and consequently enable its practical implementation more widely in the ns hfa keywords nonstationarity extreme events uncertainty confidence intervals numerical stability computational efficiency 1 introduction the hydrological frequency analysis hfa is an essential technique employed to analyze the recurrence of flood precipitation extreme events and thus guide the water related management planning and infrastructure design conventionally the hfa relies on the stationary assumption which requires the statistical characteristics of the underlying process to remain invariant through time and thus it is often referred to as the stationary hfa s hfa however the changes in the hydro climatic systems posed by external drivers such as climate change and anthropogenic activities have often resulted in the presence of nonstationarity fowler et al 2021 milly et al 2008 prosdocimi et al 2015 wright et al 2019 as a result the nonstationary hfa ns hfa in which the extreme value distribution varies over time has been adopted when the stationary assumption is not valid françois et al 2019 khaliq et al 2006 salas et al 2018 in the ns hfa apart from the point estimates the representation of uncertainty is of paramount importance given that the data available for the statistical analysis is often limited coles et al 2003 ouarda et al 2019 assessing the uncertainty is not only relevant for evaluating the analysis reliability serinaldi and kilsby 2015 but also has implications in urban planning infrastructure design and hazards mitigation for instance in the realm of resilience based design and management the infrastructure reliability hinges on its capability to operate and adapt to the uncertainty chester and allenby 2018 linkov and palma oliveira 2017 park et al 2011 seager et al 2017 there are two general statistical frameworks for conducting the ns hfa namely the bayesian and the frequentist frameworks the former handles the model parameters as random variables and allows the use of a prior distribution whereas the latter considers the model parameters as unknown real values and relies on the available observations only in the classical frequentist framework the uncertainty is commonly captured by the confidence interval in this context the confidence interval is the range of possible values of the estimated parameter associated with a certain confidence level i e with a certain probability that the interval will contain the true parameter value there are several approaches to estimating the confidence interval including the bootstrap the delta and the profile likelihood pl methods salas et al 2018 serinaldi and kilsby 2015 the non parametric and parametric versions of the bootstrap technique are based on resampling with replacement from the standardized observations and sampling from the estimated nonstationary distribution respectively the other two methods are based on the maximum likelihood ml principle which has been extensively utilized in the ns hfa due to its flexibility to introduce the nonstationarity into the model structure coles 2001 katz 2013 the delta method is based upon the assumption of asymptotic normality of the ml estimator and thus yields symmetric confidence intervals in contrast the pl method does not require the assumption of normality and thus can yield asymmetric confidence intervals coles 2001 gilleland and katz 2016 which concurs with the commonly skewed nature of quantiles in particular the pl method is more competent in capturing the uncertainty of the high return period quantiles wang et al 2017 wu et al 2019 this makes the pl method more accurate coles 2001 for instance bolívar cimé et al 2015 evaluated the coverage frequencies of the bootstrap delta and pl methods in a simulation setting and demonstrated the advantage of the pl method in preventing underestimation of large quantiles therefore the pl method is more robust and reliable than its frequentist counterparts chen et al 2016 coles 2001 obeysekera and salas 2014 and theoretically superior compared to the other simpler methods the theoretical superiority of the pl method has boosted its widespread use in the fields such as economy biology and epidemiology among others e g filimonov et al 2017 kreutz et al 2013 raue et al 2009 tönsing et al 2018 in the context of the s hfa it also has been applied for both annual maximum series ams and peaks over threshold series e g giuntoli et al 2021 lu et al 2013 schendel and thongwichian 2017 in the ns hfa however its implementation has been more troublesome as it is not originally devised for nonstationary scenarios cooley 2013 its early application was conducted by obeysekera and salas 2014 for design oriented purposes in which the concept of the expected waiting time was adopted and the estimation of a design quantile and its confidence interval required iterative solutions most recently song et al 2018 derived the confidence interval of the nonstationary quantiles indirectly by employing the estimated profile likelihood of the shape parameter and the associated distribution parameters to date however the use of the delta and bootstrap methods prevails in practice primarily due to their relatively easier implementation or simpler mathematical formulation e g ansa thasneem et al 2021 mondal and mujumdar 2016 šraj et al 2016 yan et al 2017 there are two key hurdles for implementing the pl method in the ns hfa the first and probably the most widely argued in the literature is its computational burdensome obeysekera and salas 2014 serinaldi and kilsby 2015 wu et al 2019 its high computational expense can be ascribed to two facts a the plenty of numerical maximizations of the likelihood function required for deriving the full profile likelihood over the temporal and frequency domains and or b the additional computations to implicitly incorporate the quantiles into the likelihood function obeysekera and salas 2014 highlighted the need for more effective computational algorithms to facilitate its implementation for nonstationary scenarios the general statistical literature e g venzon and moolgavkar 1988 has suggested turning to the numerical methods to overcome such computational burden of the pl method however no advancements have been made in this regard so far as a result the pl method has not been widely applied in the ns hfa despite its superiority over other methods françois et al 2019 hesarkazzazi et al 2021 salas et al 2018 the other hurdle is that the pl method has a potential issue on numerical stability when dealing with relatively short datasets due to its core principle of ml estimation martins and stedinger 2000 the ml estimator is efficient when the sample size is large enough but it may be unstable and diverge when dealing with short datasets martins and stedinger 2000 proposed the generalized ml gml method to resolve such a problem for parameter estimation in the s hfa the key idea behind the gml method is to introduce an informative prior distribution for the shape parameter whose estimation is more difficult than other distribution parameters such that eliminating the potentially invalid values of this parameter and consequently avoiding numerical divergence the prior distribution also called the geophysical prior in martins and stedinger 2000 restricts the shape parameter to a range of statistically and physically reasonable values later el adlouni et al 2007 extended the gml method to the ns hfa for parameter estimation using the same geophysical prior of martins and stedinger 2000 the geophysical prior has also been adopted in the gml method in several other studies gilleland and katz 2016 thiombiano et al 2018 tramblay et al 2013 xavier et al 2020 whereas a slightly modified prior distribution whose density is broader within the same support of the shape parameter and thus is less informative has been employed in cannon 2010 and vasiliades et al 2015 it is worth noting that the gml method is not a fully bayesian method martins and stedinger 2000 as it treats the model parameters as unknown real values despite the incorporation of the prior distribution for the shape parameter into the method the gml method could thus be considered as a semi frequentist method since the pl method is also based on the ml principle its stable implementation for short datasets might be attained by following the same rationale of the gml method in summary the practical implementation of the pl method in the ns hfa remains unpopular to date despite its theoretical superiority both the heavy computational burden and the potential numerical problems especially for short time series are the two key issues that hinder its application in practice therefore this paper aims to tackle these two obstacles to facilitate its wide implementation in the ns hfa this paper proposes using the regula falsi numerical technique to reduce its computational burden for estimating the confidence interval and adopting the gml principle to enhance its numerical stability the applications of the proposed methods which are named the rf pl and rf gpl methods respectively are demonstrated using eight hydro meteorological datasets from north america that exhibit different patterns of nonstationarity 2 materials and methods 2 1 datasets in this paper eight ams from mexico the usa and canada were employed these datasets were retrieved from the flow gage or meteorological stations of bow river at calgary alberta eccc 05bh004 bow river at banff alberta eccc 05bb001 acambaro guanajuato conagua 11002 yurecuaro michoacan conagua 16141 milk river alberta eccc 11aa005 chilliwack river british columbia eccc 08mh016 okanagan river british columbia 08nm050 and aberjona river massachusetts usgs 02146507 01102500 six of these datasets are flow ams while the other two are precipitation ams these datasets have sample sizes of 107 109 78 89 108 89 97 and 78 and are referred to as d1 d2 d3 d4 d5 d6 d7 and d8 respectively throughout this paper the exploratory data analysis was conducted using the non parametric mann kendall mk test to detect the monotonic trends in the first two moments of the datasets namely the mean and standard deviation which are indicative of the presence of nonstationarity the trend in the standard deviation was assessed using the mk test coupled with the moving window method in which the series of the standard deviation was derived by shifting the window with both length and step set as 10 years for skipping potential decadal variability and window overlapping vidrio sahagún et al 2021 the mk test was conducted at a significance level of 0 05 as illustrated in fig 1 the datasets d1 to d4 exhibit significant trends in the mean but not in the standard deviation while d5 and d6 show significant trends in the standard deviation but not in the mean and d7 and d8 exhibit trends in both the mean and the standard deviation thus these datasets which are collected from different climate regions and in particular present distinct nonstationary patterns are considered appropriate to illustrate the application of the proposed methods in the ns hfa 2 2 the ns hfa model the generalized extreme value gev distribution is one of the most commonly employed distributions in both the ns hfa and s hfa e g agilan and umamahesh 2017 gado and nguyen 2016 and ouarda et al 2018 since this paper aims to demonstrate the computational efficiency and numerical stability of the proposed methods the gev distribution was adopted herein for the sake of illustration note that the methodology presented in this paper is also applicable to any other distributions the cumulative gev function of the random variable y is defined as 1 f y θ f y ξ α κ exp 1 κ y ξ α 1 κ κ 0 exp exp y ξ α κ 0 where the parameter vector θ is composed of κ α and ξ namely the shape scale and location parameters respectively the tail behavior of the distribution is described by κ and the gev is categorized into three subfamilies gumbel or extreme value type i distribution with support y when κ 0 fréchet or type ii which is the heavy tailed case with support y ξ α κ when κ 0 and weibull or type iii which is the upper endpoint case with support y ξ α κ when κ 0 the quantile estimate yt i e the magnitude associated with the return period t 1 p where p is the exceedance probability i e p 1 f y θ is given by 2 y t f y 1 p θ ξ α κ 1 log 1 p κ κ 0 ξ α log log 1 p κ 0 under nonstationarity the distribution is allowed to vary over time by using the covariate dependent distribution parameters the ns hfa has been conducted using either the temporal i e the time or the physical covariate s the use of the temporal covariate as a surrogate of the time dependent physical driver s has been popular especially when the research objective is to develop methodologies to conduct the ns hfa in the historical period only without extrapolating beyond e g prosdocimi and kjeldsen 2021 serago and vogel 2018 the successful attribution of the nonstationarity to physical driver s benefits the ns hfa in general however the use of the physical covariate s in practice is often constrained due to the difficult attribution of the nonstationarity brunner et al 2021 easterling et al 2016 slater et al 2021 and the often weak to moderate relationships between the nonstationary patterns e g temporal trends and the physical driver s causative process es archfield et al 2016 burn and whitfield 2017 ray and goel 2019 thus this paper adopted the temporal covariate a variety of nonstationary structures have been adopted in the literature in which the distribution parameters except the shape parameter are expressed as either linear or nonlinear functions of the selected covariate the use of a time variant shape parameter has often been excluded as it is difficult to estimate due to the sample size limitations coles 2001 katz 2013 given the selected gev distribution and the research objective of this paper two parsimonious nonstationary models were considered herein these two models consist of the gev distribution coupled with two different linear nonstationary structures in which the location parameter ξ t and both the location and scale parameters ξ t and α t change linearly over time respectively these nonstationary models are denoted as m 1 0 0 and m 1 1 0 and are given by 3 m 1 0 0 g e v ξ t α κ ξ t ξ 0 ξ 1 t α c o n s t a n t κ c o n s t a n t 4 m 1 1 0 g e v ξ t α t κ ξ t ξ 0 ξ 1 t α t α 0 α 1 t κ c o n s t a n t where ξ0 and ξ1 and α0 and α1 are the regression coefficients of the location and scale parameters respectively for the datasets used in this paper m 1 0 0 was employed if the temporal trend was detected only in the mean d1 d4 while m 1 1 0 was used when a trend in the standard deviation presents d5 d8 this model determination is supported by the distribution moments equations e g see stedinger 2017 the models adopted were verified using both the model assessment and the diagnostic assessment conducted at the significance level of 5 the model assessment was conducted in terms of both the goodness of fit and uncertainty according to the deviance statistic d coles 2001 and the average width aw of the confidence interval respectively the aw was estimated using fifty equally log spaced ts in the range of 2 yr t 100 yr which were used to map the frequency surface in the following section the diagnostic assessment of the models was performed using the kolmogorov smirnov test applied to the standardized datasets coles 2001 ragno et al 2019 in these assessments the stationary model m s was also included 2 3 the profile likelihood method under nonstationarity the pl method has its roots in the principle of ml estimation which relies on the likelihood the likelihood is the joint probability of the observations y t y 1 y 2 yn as a function of a given parameter set θ the likelihood l θ and log likelihood l θ functions for the independent but not necessarily identically distributed realizations of the random variables yt with probability density function f yt θ are 5 l θ t 1 n f y t θ 6 l θ log l θ t 1 n log f y t θ the ml estimator of θ denoted as θ is the parameter vector that maximizes l θ or l θ in practice it is more convenient and easier to use l θ rather than l θ due to its tractability in the optimization procedure katz 2013 the profile likelihood for a target parameter component θ j denoted as l p θ j is derived from the log likelihood function maximized with respect to all other parameter components of θ i e θ j and is defined as coles 2001 7 l p θ j max θ j l θ j θ j specifically l p θ j is often derived following the steps a θ j is fixed to a certain pre defined value b l is maximized by re optimizing θ j i e all parameters except θ j and c l is evaluated at θ θ j θ j these steps are repeated for a number of specified values of θ j lying in a sufficiently large range then the obtained l p θ j is used to approximate the 100 1 β confidence interval c β of θ j defined as coles 2001 8 c β θ j 2 l θ l p θ j q 1 β where β is the level of significance 0 05 herein and q 1 β is the 1 β quantile of the chi square distribution with one degree of freedom χ 1 2 in this approach θ j is not restricted to be a distribution parameter but can be also a quantile yt of the accordingly re parameterized log likelihood function such reparameterization in the s hfa often consists of substituting the quantile expression of the corresponding distribution into the log likelihood function to incorporate yt as a model parameter coles 2001 in the pl method employed by obeysekera and salas 2014 for the ns hfa the confidence interval was obtained for a design quantile y t e w t t i at a given point in time ti that satisfies the pre specified expected waiting time tewt of interest the adopted ns hfa model consisted of the gev distribution with a linearly varying location parameter i e m 1 0 0 in this paper the values of ξ0 associated to the given y t e w t t i were estimated iteratively such that the specified tewt was preserved hence finding such a correspondence between ξ0 and y t e w t t i allows to essentially derive the l p θ j for θ j y t e w t t i and θ j ξ1 α κ without using an explicit analytical expression of l y t ewt t i ξ 1 α κ in this paper the concept of the effective return level katz et al 2002 which is the time variant return level quantile y t t corresponding to a particular t was introduced to the pl method such that explicit analytical expressions for l y t t i ξ 1 α κ or l y t t i ξ 1 α 0 α 1 κ could be formulated this allowed bypassing the iterations for implicitly incorporating y t t i into l and consequently decrease the computational demand and derive the log likelihood analytically under nonstationarity the y t t of the gev distribution is given by 9 y t t f y 1 p θ t ξ t α t κ t 1 log 1 p κ t κ 0 ξ t α t log log 1 p κ 0 and the log likelihood function of the gev distribution is 10 l θ t t 1 n log α t 1 1 κ t log 1 κ t y t ξ t α t 1 κ t y t ξ t α t 1 κ t where 1 κ t y t ξ t α t 0 for t 1 2 n the l y t t i i e the profile likelihood of y t t at the given ti is derived using a suitable reparameterization of eq 10 for m 1 0 0 and m 1 1 0 eq 9 can be rearranged as eqs 11 and 12 respectively 11 ξ 0 y t t i ξ 1 t i α κ 1 log 1 p κ 12 ξ 0 y t t i ξ 1 t i α 0 α 1 t i κ 1 log 1 p κ consequently the log likelihood functions for m 1 0 0 and m 1 1 0 are reparametrized as 13 l y t t i ξ 1 α κ t 1 n log α 1 1 κ log κ α y t y t t i ξ 1 t i t log 1 p κ κ α y t y t t i ξ 1 t i t log 1 p κ 1 κ 14 ℓ y t t i ξ 1 α 0 α 1 κ t 1 n log α 0 α 1 t 1 1 κ log 1 κ α 0 α 1 t y t y t t i ξ 1 t i t α 0 α 1 t i α 0 α 1 t 1 log 1 p κ 1 κ α 0 α 1 t y t y t t i ξ 1 t i t α 0 α 1 t i α 0 α 1 t 1 log 1 p κ 1 κ thus l p y t t i is obtained by providing different values of y t t i and maximizing l with respect to all other nonstationary distribution parameters θ j the nelder mead simplex method lagarias et al 1998 which has been widely adopted in statistical hydrology and ns hfa abbaszadeh et al 2019 li and zheng 2016 ouarda et al 2019 zhang et al 2020 was used for the optimization eq 7 it is also worth highlighting that the above reparameterizations can be easily adapted for different nonstationary structures e g the nonlinear structures for ξ t α t and or κ t 2 4 the regula falsi profile likelihood rf pl method the conventional pl estimation procedure involves estimating the full l p θ j over a certain pre defined range of the target parameter θ j since estimating the bounds of the confidence intervals is the ultimate goal it would be unnecessary to compute the full profile which is computationally expensive thus there is an opportunity to simplify the computation by just focusing on approximating the bounds of the confidence intervals which are the intersections of l p θ j with the confidence threshold denoted as l p thr θ j where l p thr θ j l θ 1 2 q 1 β this paper proposed using the classical regula falsi method see ostrowski 1973 stoer 2002 also known as the method of false position or secant method in the pl method to identify the confidence interval bounds this proposed method was named as rf pl method herein the regula falsi method is one of the easiest methods for finding the zeros of a function while there are more elaborate techniques ranging from modified versions of the regula falsi to newton s and steffensen s methods e g see chen 2007 chen and li 2006 grewal 2018 stoer 2002 in general finding the zeros or roots of a given function f x means to find the argument x for which f x 0 in multiple real world problems finding the zeros analytically is often very difficult if not impossible and thus numerical methods are turned to for iteratively approximating them in the regula falsi method the approximation starts with the points xa and xb which bound a sufficiently large interval such that it contains a zero of f x then the regula falsi method consists of the following steps in each iteration a calculate x c x b f x a x a f x b f x a f x b which is the approximation of the zero b if the error is less than or equal to the pre defined error tolerance ε the estimation is considered successful and no more iterations are required c otherwise the interval is redefined such that the interval still contains the zero i e as xa xc or xc xb whether f xa f xc 0 or f xc f xb 0 respectively and a new iteration is performed here x θ j y t t i and f x l p θ j l p thr θ j the ε 0 01 l θ l p thr θ j was adopted and verified to be adequate for the target problem after scrutiny the θ which includes the point estimate θ j was obtained by maximizing l p θ j as in the ml estimation method the pl and the rf pl procedures were compared in terms of the required computational time the computational time is the time needed for computing the full l p θ j and estimating the confidence interval bounds of θ j and θ j to yield estimates with approximately the same ε for a set of pre selected ts throughout the observation period of a dataset in the pl and rf pl methods respectively the fifty equally log spaced ts in the range of 2 yr t 100 yr were also considered herein both procedures were implemented and executed on a computer with an intel core i7 7700hq 2 80 ghz x64 based processor 16 gb random access memory ram and windows 10 home 64 bit operating system 2 5 the generalized profile likelihood method to resolve the potential issue of numerical instability for short datasets the gml principle was extended to the pl method in this paper which is named the generalized pl gpl method similar to the gml method the gpl method is also considered semi frequentist in the gpl method a geophysical prior for κ was adopted to prevent its unrealistic estimates avoiding the unrealistic estimates of κ is also beneficial in estimating other distribution parameters more reliably due to their joint computation in these methods in the gml method martins and stedinger 2000 proposed the beta distribution π κ b e t a p 6 q 9 with support in the interval 0 5 κ 0 5 as the prior distribution for κ of the gev distribution this beta distribution has 90 of the probability mass concentrated in 0 1 κ 0 3 mode κ 0 12 and e κ 0 1 this defined range of κ was determined based on reasonable values of the shape parameter and is also supported by a number of studies e g papalexiou and koutsoyiannis 2013 sun et al 2015 villarini and smith 2010 in this paper the geophysical prior adopted by martins and stedinger 2000 was used the generalized likelihood and log likelihood functions are given by eqs 15 and 16 respectively 15 g l θ l θ π κ 16 g l θ l θ log π κ using the eq 16 the generalized profile likelihood g l p θ j and its corresponding confidence interval c β are given by 17 g l p θ j max θ j g l θ j θ j 18 c β θ j 2 g l θ g l p θ j q 1 β for m 1 0 0 and m 1 1 0 the g l p θ j y t t i is obtained by extending eqs 13 and 14 respectively 19 g ℓ y t t i ξ 1 α κ t 1 n log α 1 1 κ log κ α y t y t t i ξ 1 t i t log 1 p κ κ α y t y t t i ξ 1 t i t log 1 p κ 1 κ log 0 5 κ p 1 0 5 κ q 1 b p q 20 g ℓ y t t i ξ 1 α 0 α 1 κ t 1 n log α 0 α 1 t 1 1 κ log 1 κ α 0 α 1 t y t y t t i ξ 1 t i t α 0 α 1 t i α 0 α 1 t 1 log 1 p κ 1 κ α 0 α 1 t y t y t t i ξ 1 t i t α 0 α 1 t i α 0 α 1 t 1 log 1 p κ 1 κ log 0 5 κ p 1 0 5 κ q 1 b p q where b p q γ p γ q γ p q in which γ is the gamma function the geophysical prior can be further refined if case specific information such as the regional skewness is available similar to the rf pl method the regula falsi numerical technique was coupled with the gpl to estimate the bounds of the confidence intervals rather than the full g l p θ j which was referred to as the rf gpl method herein in the rf gpl method the θ which includes the point estimate θ j was obtained by maximizing g l p θ j as in the gml method as the rf gpl method was proposed to deal with the issue of numerical instability of the rf pl method a number of subseries of different sample sizes were generated from the eight datasets to test its feasibility and convenience under nonstationarity each observation is a realization of a different distribution as the underlying stochastic process varies as a function of time hence the recording time of each data point matters as a result a set of subseries was generated by extracting the data points from subperiods using the moving windows approach to preserve their temporal dependence here the window length l 30 n and shift s 1 were used to extract the subseries of different sample sizes starting from 30 thus the number of the subseries depended on the sample size of the original dataset 2 6 performance assessment metrics the average width awt and the average asymmetry degree aadt of the confidence intervals were employed to compare the estimates of the rf pl and rf gpl methods these metrics assess the width and the symmetricity of the confidence intervals with respect to the point estimates respectively over the corresponding observation period for a given t they are calculated by 21 a w t 1 m t 1 m m t t u m t t l 22 a a d t 1 m t 1 m m t t u y t t m t t u m t t l 0 5 where y t t is the point estimate and m t t u and m t t l are the upper and lower bounds of the confidence interval for the t at t respectively to examine the numerical instability of the two methods the number of instability cases ic was used here the instability was defined as the convergence failure of the optimization algorithm for l or the presence of complex values in the upper and or lower bounds of the confidence interval at least at one point of time over the observation period thus for a given sample size the ic is the number of subseries that are subject to the numerical instability 23 i c k 1 n c k where c k 0 if m t t 1 m u m t t 1 m l r 1 otherwise where m is the length of the subseries and n is the number of subseries of a given sample size generated using the original dataset 3 results and discussion the results from the model assessment table a1 in appendix show that the determined models are optimal based upon the statistic d and or the aw except d5 and d6 for d5 and d6 m s is not inferior to both m 1 0 0 and m 1 1 0 according to the assessment criteria as shown in fig 1 the data points of these two datasets appear to be more asymmetrically distributed about the time invariant mean the blue line through the time such a variation pattern might imply a potential temporal trend in the skewness which could lead to the relatively low performance of the nonstationary models for these two datasets in addition the diagnostic assessment confirmed that the employed ns hfa models fit all datasets significantly well hence the ns hfa models selected for the datasets namely m 1 0 0 for d1 d4 and m 1 1 0 for d5 d8 are considered appropriate and used in the proposed rf pl and rf gpl methods in the following sections 3 1 the reduction of computational demand by regula falsi profile likelihood rf pl method fig 2 illustrates the implementation of the pl method for generating the confidence intervals changing over t for the dataset d1 as an example as shown in fig 2 a the full profile l p y t t i is estimated over a range of y t t i values that includes the intersections of l p y t t i with l p thr y t t i at the ti and t of interest here t 100 yr and ti 1911 the point estimate y t t i and the bounds of its confidence interval are indicated by the black and red dots on l p y t t i respectively in fig 2 a as well as shown in fig 2 b the surface of the estimated l p y t t i over a certain range of the frequency domain 2 yr t 100 yr for the same ti is then yielded by connecting the full l p y t t i of different ts finally the frequency curves and the bounds of their confidence intervals which are commonly used in practice are derived by repeating the procedure for all the time slices of interest fig 2 c as illustrated the computation of the full l p y t t i is in fact unnecessary to assess the confidence intervals of the frequency curves and only the black and red dots are needed for practical application purposes the proposed rf pl method bypasses the derivation of the full l p y t t i by just deriving its point estimate y t t i and intersections with l p thr y t t i so that the computational time can be substantially reduced fig 3 displays the derived y t t i and its confidence interval at the last year of the observation period by the pl and rf pl methods for two datasets d1 and d8 as examples as shown in fig 3 the two methods yielded practically identical results as both the upper and lower bounds of their confidence intervals overlapped in addition it is apparent that the confidence interval is asymmetric with respect to y t t i the aadt for t 100 yr for d1 ti 2018 and d8 ti 2019 are 0 18 and 0 22 in both methods the asymmetricity feature of the pl and the rf pl methods is an attribute deemed appropriate for the hfa similar results were observed in all other datasets not shown therefore these methods performed equivalently in generating the confidence intervals nevertheless the rf pl method substantially reduced the computational burden as it took only from 4 4 to 6 2 of the time required by the pl method for all the datasets fig 4 among the datasets the considerably higher computational time of d5 in both the pl and rf pl methods would be ascribed to the combination of its sample length 108 the complexity of the model m 1 1 0 and the numerical challenges due to its statistical characteristics as discussed in section 3 3 the result of reducing the computational time is not surprising as the rf pl method avoids the computation of the entire l p y t t i to quantify the confidence interval as described previously this advantage of the rf pl method was consistent in all datasets irrespective of the different nonstationarity patterns modeled using either m 1 0 0 or m 1 1 0 thus these results demonstrate that the proposed rf pl method is capable of yielding equivalent estimates to the pl method while significantly reducing the computational demand in the ns hfa the computational benefit of the rf pl method would be particularly attractive for regional national continental and global scale studies in which a large number of datasets are analyzed it is worth noting that the rf pl method which was formulated based upon the gev distribution with the linear nonstationary structures for the location and or scale parameters in this paper is also applicable for any other distributions and nonstationary structures to apply the rf pl method for other distributions and or nonstationary structures their corresponding reparametrized log likelihood function e g eqs 13 and 14 are needed in addition besides the regula falsi method other numerical techniques could be also employed the alternatives include the modified versions of the regula falsi method and the newton s and steffensen s methods among others however the classical regula falsi method may be particularly convenient as it does not require the derivatives of the corresponding log likelihood function and guarantees convergence as long as the initial interval xa xb is sufficiently large such that it contains the zero of the underlying function 3 2 the performance of regula falsi generalized profile likelihood rf gpl the rf gpl and rf pl methods were compared in terms of both numerical stability and the derived confidence interval using the subseries of different sample sizes extracted from the original datasets since the estimation of extreme events located in the right tail of a distribution is often more prone to the divergence issue obeysekera and salas 2014 the comparison of the aw aad and ic was focused on the t 100 yr herein fig 5 shows the ic of the two methods for sample sizes ranging from 30 to the full sample for all datasets it is apparent that the rf gpl method outperformed the rf pl method in terms of numerical stability as it consistently reported lower ics for all sample sizes and datasets in particular the instability problem did not occur in the rf gpl method when m 1 0 0 was used for datasets d1 d4 for all sample sizes while it was considerably reduced when m 1 1 0 was adopted for datasets d5 d8 the higher ics reported when using m 1 1 0 d5 d8 compared to when using m 1 0 0 d1 d4 in both methods suggest that the increase of the model complexity would trigger numerical instability besides the numerical instability of these methods might be also associated with the dataset specific characteristics when the same nonstationary model is adopted for example the instability issue of the rf pl method was milder for d6 and d7 compared to d5 and d8 all modeled with m 1 1 0 as ics were only reported when the sample size is 35 for d6 and d7 in addition the improvement offered by the rf gpl method appeared to be little for d6 all these results demonstrate that overall the rf gpl method can avoid or substantially mitigate the instability problem irrespective of the nonstationary patterns and the nonstationary structures of the ns hfa model especially for short datasets however the results also revealed that its efficiency would depend on both the complexity of the ns hfa model and the dataset characteristics particularly the skewness that determines the shape parameter as discussed in section 3 3 and thus might vary to some degree from case to case fig 6 illustratively compares the y t t i and its confidence interval at ti 2018 between the smallest sample size n 30 fig 6 a and the full sample size n 107 fig 6 b by both the rf pl and rf gpl methods for d1 as an example as shown in fig 6 the y t t i by the rf gpl was less sensitive to the sample size compared to that by the rf pl as it yielded closer estimates between the two sample sizes as for the confidence interval the increase of the aw along with the reduction of the sample size was observed in both methods whereas the aw increment was more prominent in the rf pl method this sample result visually demonstrates that the rf gpl may yield a narrower yet realistic confidence interval which is often ascribed to avoiding the very high upper bound of the confidence interval especially for short samples figs 7 and 8 further display the aw 100 yr and aad 100 yr respectively derived by both methods for all subseries on one hand compared to the rf pl method the rf gpl method produced lower aws and a considerably narrower variation range of the aws without the exceptionally high values in general the medians of aws of the rf gpl method were often lower than d2 d5 d6 and d8 or approximately the same as d1 d3 and d4 of those of the rf pl method in particular the reduction in the aws in the rf gpl method was more apparent for smaller sample sizes on the other hand the aad 100 yr of the rf gpl method exhibited a less consistent behavior throughout the datasets fig 8 compared to the rf pl method the rf gpl method yielded consistently higher aad medians d3 d5 and d8 or lower aad medians d2 d6 and d7 with either a larger or smaller variation range while for d1 and d4 a more erratic pattern was observed note that differing from the aw the aad relies on the upper and lower bounds of the confidence interval as well as the y t t i given a sample size and original dataset the variability of the aad reflects the differences in the statistical characteristics among the sub samples for the rf pl and rf gpl methods while for the rf gpl method the varying role of the geophysical prior among the sub samples would also lead to the different estimates of y t t i and its lower and upper bounds and consequently the aad eq 22 all these results suggest that the rf gpl has the capacity to reduce the uncertainty in the ns hfa by narrowing the confidence intervals in general especially for short datasets while yielding asymmetrical conference intervals despite that the aad may vary largely among different sample sizes and datasets furthermore the advantages of the rf gpl method might be less apparent for long datasets for example a wider range and a higher median of the aw 100 yr of d1 for sample sizes 90 were yielded by the rf gpl compared to the rf pl fig 7 hence to further compare the rf gpl and rf pl methods for long datasets table 1 presents the relative differences in the awt aadt and y t denoted as δawt δaadt and δ y t respectively for several ts when using the full samples the y t is the average of y t t over the observation period the negative δ y t δawt and δaadt indicate that the y t awt and aadt yielded by the rf gpl method are lower than those by the rf pl method in general the y t of both methods were only slightly different as the absolute values of δ y t are small for all datasets except d5 the dataset d5 exhibits a notable asymmetry in the trend of the standard deviation skewed towards the high flows fig 1 which might suggest that m 1 1 0 did not capture its nonstationarity well and might be a cause of the large deviation between the two methods moreover the rf gpl method reduced the confidence interval in most cases as indicated by the negative δawt however the positive δawt was also found in a few cases e g for t 100 yr of d1 and d8 suggesting that the rf gpl method might be marginally or not advantageous over the rf pl for some long datasets in which the numerical instability is not likely to occur in both methods regarding the asymmetricity degree it was overall decreased in the rf gpl method as indicated by the negative δaadt although its increase was also found in some cases both the negative and positive δaadt were consistent with the large variations of aad previously observed in the rf gpl method fig 8 therefore the case specific exploration would be desired to select either the rf pl or the rf gpl method for long datasets 3 3 the role of the geophysical prior in the shape parameter the behavior of κ was further investigated to better understand the role of the geophysical prior in the rf gpl method note that the point estimate of κ κ is time invariant whereas the upper κ u and lower bounds κ l of its confidence interval vary in both the temporal and frequency domains these κ estimates were derived along with the estimates of l p y t t i from θ j in eq 7 fig 9 showcases the κ and the average κ u and κ l over the observation period denoted as κ u and κ l respectively derived by both the rf pl and rf gpl methods for all subseries for t 100 yr as expected the geophysical prior influenced κ κ u and κ l in the rf gpl method by constraining them within its defined support and preventing large deviations from it it is apparent that the rf gpl method reduced the variability in κ as well as its confidence interval by lowering the κ u and or elevating the κ l such effects were more apparent for the subseries of small sample sizes in particular the introduction of the prior distribution in the rf gpl method eliminated the questionably high κ u a high κ is associated with a high y t t i which in turn may produce a negative argument of the logarithm in the log likelihood function eqs 13 and 14 and cause the numerical instability among the datasets the numerical instability in the rf pl method was more prominent for d5 and d8 for a variety of sample sizes including large sample sizes fig 5 as shown in fig 9 a larger number of κ u estimated by the rf pl method fell outside the support of the geophysical prior i e 0 5 κ 0 5 for d5 and d8 compared to all other datasets approximately 99 and 59 of the κ u in the rf pl method located outside the support for d5 and d8 respectively in addition 3 of κ l values of the rf pl method were also higher than 0 5 for d5 in contrast the rf pl method estimated only a few 8 or no κ u larger than 0 5 for d6 and d7 respectively these coincide with the higher ic reported in the d5 and d8 than in the d6 and d7 these results suggest that the improved numerical stability of the rf gpl method could be generally ascribed to lowering κ u in the rf gpl thus the skewness which primarily determines κ would be a key dataset specific characteristic affecting the numerical stability of these methods these results advocate for using the informative prior distribution of κ as an effective solution to avoid or mitigate the numerical problems in particular for short datasets as shown previously the geophysical prior used in this paper is in general appropriate as its use in the rf gpl method was found to enhance the numerical stability of the rf pl method however this prior distribution might not always be optimal fig 9 also exhibits consistently higher estimates of the κ u κ and κ l of the rf pl method than those of the rf gpl for all sample sizes for dataset d5 in such a case the geophysical prior might substantially deviate from the true κ although unknown and in turn the prior distribution would introduce bias to the estimates that may persist in large sample sizes this could explain the large δ y t between the rf pl and rf gpl methods of d5 when using the full sample size table 1 under such circumstances the use of a less informative prior distribution may be an alternative solution to diminish the potential bias introduced into the analysis however this might in turn lead to the increase of the numerical instability for example when using a less informative prior distribution than the geophysical prior of martins and stedinger 2000 for d5 namely with the same mode κ and e κ but a higher entropy 50 and wider support 0 75 κ 0 75 the estimated δ y t was lower 25 83 for t 100 yr whereas the ic was increased by 12 table 1 and fig 5 therefore the trade off between the reduction of the potential bias and the effectiveness in mitigating numerical divergence should be considered when modifying the prior distribution of κ on the other hand the use of case specific prior information e g regional skewness information whenever is available can also help to refine the geophysical prior in the rf gpl method and thus further enhance its performance however it does not necessarily guarantee numerical stability especially for datasets of high skewness and thus high κ 3 4 future research recommendations there is a need of performing the ns hfa when there is evidence of nonstationarity o brien and burn 2014 ragno et al 2018 vu and mishra 2019 the novelty of this paper is primarily methodological while the proposed methods the rf pl and rf gpl methods are applicable for both the at site and regional ns hfa the proposed rf gpl method not only advances the pl method to make it more practically feasible by improving its computational efficiency and numerical stability but also reduces the uncertainty in the ns hfa the latter improvement might enable the use of more complex nonstationary structures e g with more covariates and or more elaborated link functions which has been constrained due to the high uncertainty associated with their use ouarda et al 2019 serinaldi and kilsby 2015 similar to other methods for the ns hfa the estimates of the rf pl and rf gpl methods can be further utilized in hazard and risk assessments read and vogel 2015 salas and obeysekera 2014 and in turn guide the mitigation management decisions and infrastructure design to cope with the hydrological hazards especially under changing conditions e g climate change and changes in land use cover beyond the advancements made by the rf pl and rf gpl methods for the ns hfa there are still future research needs and in turn opportunities for improvement from the perspective of the ns hfa models used in this paper there are several limitations that are generic for the ns hfa the temporal covariate was adopted to illustrate the proposed methods while these methods are in fact applicable for any selected covariate s including the physical covariates whenever they can be adequately identified the use of the physical covariate s e g changes in land use cover etc in the ns hfa models is particularly desired in addition besides the gev distribution there are other distributions such as the pearson type iii log pearson type iii log normal and generalized logistic among others often considered in the ns hfa thus the extension of the proposed rf pl and rf gpl methods in which the ns hfa models also consist of these distributions is required for the generalized implementation of these methods in the ns hfa moreover in the rf gpl method the geophysical prior of martins and stedinger 2000 commonly utilized in the literature was adopted the preliminary results from the use of the less informative prior distribution for d5 demonstrated the effect of the prior distribution on the numerical stability of this method further investigation on the sensitivity of the rf gpl method to the prior distribution of κ e g using a sensitivity analysis in terms of both the numerical stability and the bias in estimates is recommended as future research such an investigation might shed light on the optimal prior distribution to some degree although the prior distribution might depend on the datasets and the study regions to present there are several methods to estimate the uncertainty in the ns hfa although among these methods the pl method has been well acknowledged to be superior in theory and was further advanced in this paper the comprehensive comparison of the proposed methods with other existing methods is still desired this is particularly important for practitioners to select an appropriate method in addition the sampling uncertainty which originates from the parameter estimation due to limited sample sizes and is often large and commonly considered fundamental serinaldi and kilsby 2015 was quantified here by the proposed methods whereas the uncertainty originated from other sources in the ns hfa such as the measurement error kjeldsen et al 2014 kuczera 1996 merz and thieken 2005 model determination kjeldsen et al 2014 merz and thieken 2005 and parameter estimation approach debele et al 2017 griffis and stedinger 2007 šraj et al 2016 among others was not considered the quantification of the overall uncertainty originating from multiple sources is often challenging but always favorable similar to other existing methods for the ns hfa the proposed methods can be applied for making future predictions in several ways the methods can be directly applied to the projected future precipitation and flow extremes alternatively when the temporal covariate is used to capture the nonstationarity in these methods the ns hfa model can be used for extrapolation assuming that the temporal trend s remains the same in the future when a physical covariate is adopted in the ns hfa model the methods can make predictions using the projected covariate assuming that the covariate itself and its link function for capturing the nonstationarity remain valid in the future thus the adequate projections of the extremes or covariates and or the validity of the adopted assumptions are required for reliably implementing these methods for making future predictions in addition it has been well acknowledged that the ml and pl methods suffer numerical instability when sample sizes are small yet there is not a standard definition for what a small sample size is as shown in the results fig 5 given the same nonstationary pattern and model temporal trends in the mean only d1 d4 or temporal trends in the standard deviation and or mean d5 d8 the numerical instability occurred at different sample sizes in both the rf pl and rf gpl methods in addition the numerical instability issue is likely to be more prominent when nonstationarity is detected in the standard deviation and m 1 1 0 is employed these results suggest that there is not a fixed threshold to distinguish between small and large samples based upon the numerical stability as the threshold would depend on the nonstationary pattern the ns hfa model adopted and the statistical characteristics of the dataset e g high order moments in the s hfa martins and stedinger 2000 also demonstrated that defining if a sample is small or large depends on the statistical characteristics of the dataset e g the skewness shape parameter due to the lack of a general definition of small large samples in the hfa in terms of numerical stability the implementations of these methods would require case specific examinations to avoid the numerical divergence issue in practice 4 conclusion to facilitate the practical implementation of the pl method in the ns hfa the issues of its high computational burden and potential numerical divergence were tackled in this paper the rf pl method in which the regula falsi numerical method was applied to estimate the confidence interval bounds using an explicit analytical expression of the reparametrized log likelihood function rather than computing the full profile was proposed to reduce the computational burden following the notion of the gml method the rf pl method was further extended to the rf gpl method by introducing a prior distribution to the shape parameter to enhance its numerical stability in particular for short datasets to illustrate the efficiency of the proposed methods they were applied to eight flow and precipitation amss from north america exhibiting different nonstationary patterns and modeled by two different ns hfa models the results demonstrated that the rf pl method effectively reduces the computational time of the pl method by 94 96 without degrading the estimation accuracy moreover the proposed rf gpl method was proven successful in not only avoiding or substantially mitigating the numerical instability of the rf pl method but also decreasing the uncertainty level by constraining the estimated shape parameter especially for small sample sizes therefore the proposed methods resolved the issues faced by the pl method and consequently can foster its practical implementation in the ns hfa credit authorship contribution statement cuauhtémoc tonatiuh vidrio sahagún conceptualization methodology formal analysis investigation writing original draft jianxun he conceptualization methodology writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first author of this paper is funded by a doctoral scholarship from the national council for science and technology of mexico conacyt and the universidad de guadalajara this work is also partially funded by the discovery grant of natural sciences and engineering research council held by the second author appendix the model assessment and the diagnosis assessment of the ns hfa models table a1 shows the pairwise comparison among the m 1 0 0 and m 1 1 0 as well as their stationary counterpart m s using the statistics d test the results indicate that the m 1 0 0 and m 1 1 0 significantly outperform other models when fitting d1 d4 and d7 d8 respectively whereas the m s is not significantly outperformed by the nonstationary models for d5 and d6 moreover table a1 also presents the aw of the confidence intervals produced by the models at the fifty equally log spaced ts used in this paper based upon the aw the m 1 0 0 is superior when fitting d1 and d2 while the m 1 1 0 is superior when fitting d8 the selected models based on the aw for d1 d2 and d8 are consistent with those selected based on the statistic d while the model selection is inconsistent for all other datasets the selection of different models according to distinct evaluation criteria such as the goodness of fit and the uncertainty is not uncommon and has been documented in the literature e g ouarda et al 2019 in addition from the uncertainty perspective the stationary model often outperforms its nonstationary counterparts due to its lower model complexity serinaldi and kilsby 2015 as a result the models used for the datasets except d5 and d6 are optimal based upon the goodness of fit and or uncertainty in general for d5 and d6 the m s is not inferior to the nonstationary models according to the assessment criteria as shown in fig 1 the data points of these two datasets appear to be more asymmetrically distributed about the time invariant mean the blue line with the time such a variation pattern might imply a potential temporal trend in the skewness however the shape parameter was assumed constant in the nonstationary models in this paper and in the vast majority of the literature which might lead to the relatively low performance of the nonstationary models for these two datasets 
198,in the nonstationary hydrological frequency analysis ns hfa the profile likelihood pl method has shown superiority in uncertainty estimation due to its higher accuracy and realistic asymmetric estimates however its wide application has been hindered by two issues namely the high computational burden and the numerical instability problems arising when dealing with short datasets this paper aimed to solve or mitigate these two issues to reduce its computational burden the classical regula falsi numerical method was incorporated into the pl method namely the rf pl method for estimating the bounds of the confidence intervals through an explicit analytical expression of the reparametrized log likelihood function rather than computing the full profile the generalized maximum likelihood principle which constrains the distribution shape parameter to a physically statistically reasonable range was extended to the proposed rf pl method namely the rf gpl method to handle short datasets the proposed methods were applied to eight annual maximum series of flow and precipitation from north america which present temporal trends in the mean and or the standard deviation to demonstrate their efficiency the results showed that the rf pl method substantially reduced the computational time of the pl method by 94 96 without degrading its estimation accuracy moreover the rf gpl was proven to be effective in avoiding or substantially mitigating the numerical instability issue of the rf pl method for small sample sizes in addition to reducing the uncertainty in the estimates as expected the superiority of the rf gpl method decreases with the increase of the sample size which is beneficial in enhancing the estimation of the shape parameter in the rf pl method therefore the rf gpl method is advantageous over the rf pl method for short datasets whereas its outperformance would be case dependent for long datasets these advancements overcome the common hurdles of the pl method and consequently enable its practical implementation more widely in the ns hfa keywords nonstationarity extreme events uncertainty confidence intervals numerical stability computational efficiency 1 introduction the hydrological frequency analysis hfa is an essential technique employed to analyze the recurrence of flood precipitation extreme events and thus guide the water related management planning and infrastructure design conventionally the hfa relies on the stationary assumption which requires the statistical characteristics of the underlying process to remain invariant through time and thus it is often referred to as the stationary hfa s hfa however the changes in the hydro climatic systems posed by external drivers such as climate change and anthropogenic activities have often resulted in the presence of nonstationarity fowler et al 2021 milly et al 2008 prosdocimi et al 2015 wright et al 2019 as a result the nonstationary hfa ns hfa in which the extreme value distribution varies over time has been adopted when the stationary assumption is not valid françois et al 2019 khaliq et al 2006 salas et al 2018 in the ns hfa apart from the point estimates the representation of uncertainty is of paramount importance given that the data available for the statistical analysis is often limited coles et al 2003 ouarda et al 2019 assessing the uncertainty is not only relevant for evaluating the analysis reliability serinaldi and kilsby 2015 but also has implications in urban planning infrastructure design and hazards mitigation for instance in the realm of resilience based design and management the infrastructure reliability hinges on its capability to operate and adapt to the uncertainty chester and allenby 2018 linkov and palma oliveira 2017 park et al 2011 seager et al 2017 there are two general statistical frameworks for conducting the ns hfa namely the bayesian and the frequentist frameworks the former handles the model parameters as random variables and allows the use of a prior distribution whereas the latter considers the model parameters as unknown real values and relies on the available observations only in the classical frequentist framework the uncertainty is commonly captured by the confidence interval in this context the confidence interval is the range of possible values of the estimated parameter associated with a certain confidence level i e with a certain probability that the interval will contain the true parameter value there are several approaches to estimating the confidence interval including the bootstrap the delta and the profile likelihood pl methods salas et al 2018 serinaldi and kilsby 2015 the non parametric and parametric versions of the bootstrap technique are based on resampling with replacement from the standardized observations and sampling from the estimated nonstationary distribution respectively the other two methods are based on the maximum likelihood ml principle which has been extensively utilized in the ns hfa due to its flexibility to introduce the nonstationarity into the model structure coles 2001 katz 2013 the delta method is based upon the assumption of asymptotic normality of the ml estimator and thus yields symmetric confidence intervals in contrast the pl method does not require the assumption of normality and thus can yield asymmetric confidence intervals coles 2001 gilleland and katz 2016 which concurs with the commonly skewed nature of quantiles in particular the pl method is more competent in capturing the uncertainty of the high return period quantiles wang et al 2017 wu et al 2019 this makes the pl method more accurate coles 2001 for instance bolívar cimé et al 2015 evaluated the coverage frequencies of the bootstrap delta and pl methods in a simulation setting and demonstrated the advantage of the pl method in preventing underestimation of large quantiles therefore the pl method is more robust and reliable than its frequentist counterparts chen et al 2016 coles 2001 obeysekera and salas 2014 and theoretically superior compared to the other simpler methods the theoretical superiority of the pl method has boosted its widespread use in the fields such as economy biology and epidemiology among others e g filimonov et al 2017 kreutz et al 2013 raue et al 2009 tönsing et al 2018 in the context of the s hfa it also has been applied for both annual maximum series ams and peaks over threshold series e g giuntoli et al 2021 lu et al 2013 schendel and thongwichian 2017 in the ns hfa however its implementation has been more troublesome as it is not originally devised for nonstationary scenarios cooley 2013 its early application was conducted by obeysekera and salas 2014 for design oriented purposes in which the concept of the expected waiting time was adopted and the estimation of a design quantile and its confidence interval required iterative solutions most recently song et al 2018 derived the confidence interval of the nonstationary quantiles indirectly by employing the estimated profile likelihood of the shape parameter and the associated distribution parameters to date however the use of the delta and bootstrap methods prevails in practice primarily due to their relatively easier implementation or simpler mathematical formulation e g ansa thasneem et al 2021 mondal and mujumdar 2016 šraj et al 2016 yan et al 2017 there are two key hurdles for implementing the pl method in the ns hfa the first and probably the most widely argued in the literature is its computational burdensome obeysekera and salas 2014 serinaldi and kilsby 2015 wu et al 2019 its high computational expense can be ascribed to two facts a the plenty of numerical maximizations of the likelihood function required for deriving the full profile likelihood over the temporal and frequency domains and or b the additional computations to implicitly incorporate the quantiles into the likelihood function obeysekera and salas 2014 highlighted the need for more effective computational algorithms to facilitate its implementation for nonstationary scenarios the general statistical literature e g venzon and moolgavkar 1988 has suggested turning to the numerical methods to overcome such computational burden of the pl method however no advancements have been made in this regard so far as a result the pl method has not been widely applied in the ns hfa despite its superiority over other methods françois et al 2019 hesarkazzazi et al 2021 salas et al 2018 the other hurdle is that the pl method has a potential issue on numerical stability when dealing with relatively short datasets due to its core principle of ml estimation martins and stedinger 2000 the ml estimator is efficient when the sample size is large enough but it may be unstable and diverge when dealing with short datasets martins and stedinger 2000 proposed the generalized ml gml method to resolve such a problem for parameter estimation in the s hfa the key idea behind the gml method is to introduce an informative prior distribution for the shape parameter whose estimation is more difficult than other distribution parameters such that eliminating the potentially invalid values of this parameter and consequently avoiding numerical divergence the prior distribution also called the geophysical prior in martins and stedinger 2000 restricts the shape parameter to a range of statistically and physically reasonable values later el adlouni et al 2007 extended the gml method to the ns hfa for parameter estimation using the same geophysical prior of martins and stedinger 2000 the geophysical prior has also been adopted in the gml method in several other studies gilleland and katz 2016 thiombiano et al 2018 tramblay et al 2013 xavier et al 2020 whereas a slightly modified prior distribution whose density is broader within the same support of the shape parameter and thus is less informative has been employed in cannon 2010 and vasiliades et al 2015 it is worth noting that the gml method is not a fully bayesian method martins and stedinger 2000 as it treats the model parameters as unknown real values despite the incorporation of the prior distribution for the shape parameter into the method the gml method could thus be considered as a semi frequentist method since the pl method is also based on the ml principle its stable implementation for short datasets might be attained by following the same rationale of the gml method in summary the practical implementation of the pl method in the ns hfa remains unpopular to date despite its theoretical superiority both the heavy computational burden and the potential numerical problems especially for short time series are the two key issues that hinder its application in practice therefore this paper aims to tackle these two obstacles to facilitate its wide implementation in the ns hfa this paper proposes using the regula falsi numerical technique to reduce its computational burden for estimating the confidence interval and adopting the gml principle to enhance its numerical stability the applications of the proposed methods which are named the rf pl and rf gpl methods respectively are demonstrated using eight hydro meteorological datasets from north america that exhibit different patterns of nonstationarity 2 materials and methods 2 1 datasets in this paper eight ams from mexico the usa and canada were employed these datasets were retrieved from the flow gage or meteorological stations of bow river at calgary alberta eccc 05bh004 bow river at banff alberta eccc 05bb001 acambaro guanajuato conagua 11002 yurecuaro michoacan conagua 16141 milk river alberta eccc 11aa005 chilliwack river british columbia eccc 08mh016 okanagan river british columbia 08nm050 and aberjona river massachusetts usgs 02146507 01102500 six of these datasets are flow ams while the other two are precipitation ams these datasets have sample sizes of 107 109 78 89 108 89 97 and 78 and are referred to as d1 d2 d3 d4 d5 d6 d7 and d8 respectively throughout this paper the exploratory data analysis was conducted using the non parametric mann kendall mk test to detect the monotonic trends in the first two moments of the datasets namely the mean and standard deviation which are indicative of the presence of nonstationarity the trend in the standard deviation was assessed using the mk test coupled with the moving window method in which the series of the standard deviation was derived by shifting the window with both length and step set as 10 years for skipping potential decadal variability and window overlapping vidrio sahagún et al 2021 the mk test was conducted at a significance level of 0 05 as illustrated in fig 1 the datasets d1 to d4 exhibit significant trends in the mean but not in the standard deviation while d5 and d6 show significant trends in the standard deviation but not in the mean and d7 and d8 exhibit trends in both the mean and the standard deviation thus these datasets which are collected from different climate regions and in particular present distinct nonstationary patterns are considered appropriate to illustrate the application of the proposed methods in the ns hfa 2 2 the ns hfa model the generalized extreme value gev distribution is one of the most commonly employed distributions in both the ns hfa and s hfa e g agilan and umamahesh 2017 gado and nguyen 2016 and ouarda et al 2018 since this paper aims to demonstrate the computational efficiency and numerical stability of the proposed methods the gev distribution was adopted herein for the sake of illustration note that the methodology presented in this paper is also applicable to any other distributions the cumulative gev function of the random variable y is defined as 1 f y θ f y ξ α κ exp 1 κ y ξ α 1 κ κ 0 exp exp y ξ α κ 0 where the parameter vector θ is composed of κ α and ξ namely the shape scale and location parameters respectively the tail behavior of the distribution is described by κ and the gev is categorized into three subfamilies gumbel or extreme value type i distribution with support y when κ 0 fréchet or type ii which is the heavy tailed case with support y ξ α κ when κ 0 and weibull or type iii which is the upper endpoint case with support y ξ α κ when κ 0 the quantile estimate yt i e the magnitude associated with the return period t 1 p where p is the exceedance probability i e p 1 f y θ is given by 2 y t f y 1 p θ ξ α κ 1 log 1 p κ κ 0 ξ α log log 1 p κ 0 under nonstationarity the distribution is allowed to vary over time by using the covariate dependent distribution parameters the ns hfa has been conducted using either the temporal i e the time or the physical covariate s the use of the temporal covariate as a surrogate of the time dependent physical driver s has been popular especially when the research objective is to develop methodologies to conduct the ns hfa in the historical period only without extrapolating beyond e g prosdocimi and kjeldsen 2021 serago and vogel 2018 the successful attribution of the nonstationarity to physical driver s benefits the ns hfa in general however the use of the physical covariate s in practice is often constrained due to the difficult attribution of the nonstationarity brunner et al 2021 easterling et al 2016 slater et al 2021 and the often weak to moderate relationships between the nonstationary patterns e g temporal trends and the physical driver s causative process es archfield et al 2016 burn and whitfield 2017 ray and goel 2019 thus this paper adopted the temporal covariate a variety of nonstationary structures have been adopted in the literature in which the distribution parameters except the shape parameter are expressed as either linear or nonlinear functions of the selected covariate the use of a time variant shape parameter has often been excluded as it is difficult to estimate due to the sample size limitations coles 2001 katz 2013 given the selected gev distribution and the research objective of this paper two parsimonious nonstationary models were considered herein these two models consist of the gev distribution coupled with two different linear nonstationary structures in which the location parameter ξ t and both the location and scale parameters ξ t and α t change linearly over time respectively these nonstationary models are denoted as m 1 0 0 and m 1 1 0 and are given by 3 m 1 0 0 g e v ξ t α κ ξ t ξ 0 ξ 1 t α c o n s t a n t κ c o n s t a n t 4 m 1 1 0 g e v ξ t α t κ ξ t ξ 0 ξ 1 t α t α 0 α 1 t κ c o n s t a n t where ξ0 and ξ1 and α0 and α1 are the regression coefficients of the location and scale parameters respectively for the datasets used in this paper m 1 0 0 was employed if the temporal trend was detected only in the mean d1 d4 while m 1 1 0 was used when a trend in the standard deviation presents d5 d8 this model determination is supported by the distribution moments equations e g see stedinger 2017 the models adopted were verified using both the model assessment and the diagnostic assessment conducted at the significance level of 5 the model assessment was conducted in terms of both the goodness of fit and uncertainty according to the deviance statistic d coles 2001 and the average width aw of the confidence interval respectively the aw was estimated using fifty equally log spaced ts in the range of 2 yr t 100 yr which were used to map the frequency surface in the following section the diagnostic assessment of the models was performed using the kolmogorov smirnov test applied to the standardized datasets coles 2001 ragno et al 2019 in these assessments the stationary model m s was also included 2 3 the profile likelihood method under nonstationarity the pl method has its roots in the principle of ml estimation which relies on the likelihood the likelihood is the joint probability of the observations y t y 1 y 2 yn as a function of a given parameter set θ the likelihood l θ and log likelihood l θ functions for the independent but not necessarily identically distributed realizations of the random variables yt with probability density function f yt θ are 5 l θ t 1 n f y t θ 6 l θ log l θ t 1 n log f y t θ the ml estimator of θ denoted as θ is the parameter vector that maximizes l θ or l θ in practice it is more convenient and easier to use l θ rather than l θ due to its tractability in the optimization procedure katz 2013 the profile likelihood for a target parameter component θ j denoted as l p θ j is derived from the log likelihood function maximized with respect to all other parameter components of θ i e θ j and is defined as coles 2001 7 l p θ j max θ j l θ j θ j specifically l p θ j is often derived following the steps a θ j is fixed to a certain pre defined value b l is maximized by re optimizing θ j i e all parameters except θ j and c l is evaluated at θ θ j θ j these steps are repeated for a number of specified values of θ j lying in a sufficiently large range then the obtained l p θ j is used to approximate the 100 1 β confidence interval c β of θ j defined as coles 2001 8 c β θ j 2 l θ l p θ j q 1 β where β is the level of significance 0 05 herein and q 1 β is the 1 β quantile of the chi square distribution with one degree of freedom χ 1 2 in this approach θ j is not restricted to be a distribution parameter but can be also a quantile yt of the accordingly re parameterized log likelihood function such reparameterization in the s hfa often consists of substituting the quantile expression of the corresponding distribution into the log likelihood function to incorporate yt as a model parameter coles 2001 in the pl method employed by obeysekera and salas 2014 for the ns hfa the confidence interval was obtained for a design quantile y t e w t t i at a given point in time ti that satisfies the pre specified expected waiting time tewt of interest the adopted ns hfa model consisted of the gev distribution with a linearly varying location parameter i e m 1 0 0 in this paper the values of ξ0 associated to the given y t e w t t i were estimated iteratively such that the specified tewt was preserved hence finding such a correspondence between ξ0 and y t e w t t i allows to essentially derive the l p θ j for θ j y t e w t t i and θ j ξ1 α κ without using an explicit analytical expression of l y t ewt t i ξ 1 α κ in this paper the concept of the effective return level katz et al 2002 which is the time variant return level quantile y t t corresponding to a particular t was introduced to the pl method such that explicit analytical expressions for l y t t i ξ 1 α κ or l y t t i ξ 1 α 0 α 1 κ could be formulated this allowed bypassing the iterations for implicitly incorporating y t t i into l and consequently decrease the computational demand and derive the log likelihood analytically under nonstationarity the y t t of the gev distribution is given by 9 y t t f y 1 p θ t ξ t α t κ t 1 log 1 p κ t κ 0 ξ t α t log log 1 p κ 0 and the log likelihood function of the gev distribution is 10 l θ t t 1 n log α t 1 1 κ t log 1 κ t y t ξ t α t 1 κ t y t ξ t α t 1 κ t where 1 κ t y t ξ t α t 0 for t 1 2 n the l y t t i i e the profile likelihood of y t t at the given ti is derived using a suitable reparameterization of eq 10 for m 1 0 0 and m 1 1 0 eq 9 can be rearranged as eqs 11 and 12 respectively 11 ξ 0 y t t i ξ 1 t i α κ 1 log 1 p κ 12 ξ 0 y t t i ξ 1 t i α 0 α 1 t i κ 1 log 1 p κ consequently the log likelihood functions for m 1 0 0 and m 1 1 0 are reparametrized as 13 l y t t i ξ 1 α κ t 1 n log α 1 1 κ log κ α y t y t t i ξ 1 t i t log 1 p κ κ α y t y t t i ξ 1 t i t log 1 p κ 1 κ 14 ℓ y t t i ξ 1 α 0 α 1 κ t 1 n log α 0 α 1 t 1 1 κ log 1 κ α 0 α 1 t y t y t t i ξ 1 t i t α 0 α 1 t i α 0 α 1 t 1 log 1 p κ 1 κ α 0 α 1 t y t y t t i ξ 1 t i t α 0 α 1 t i α 0 α 1 t 1 log 1 p κ 1 κ thus l p y t t i is obtained by providing different values of y t t i and maximizing l with respect to all other nonstationary distribution parameters θ j the nelder mead simplex method lagarias et al 1998 which has been widely adopted in statistical hydrology and ns hfa abbaszadeh et al 2019 li and zheng 2016 ouarda et al 2019 zhang et al 2020 was used for the optimization eq 7 it is also worth highlighting that the above reparameterizations can be easily adapted for different nonstationary structures e g the nonlinear structures for ξ t α t and or κ t 2 4 the regula falsi profile likelihood rf pl method the conventional pl estimation procedure involves estimating the full l p θ j over a certain pre defined range of the target parameter θ j since estimating the bounds of the confidence intervals is the ultimate goal it would be unnecessary to compute the full profile which is computationally expensive thus there is an opportunity to simplify the computation by just focusing on approximating the bounds of the confidence intervals which are the intersections of l p θ j with the confidence threshold denoted as l p thr θ j where l p thr θ j l θ 1 2 q 1 β this paper proposed using the classical regula falsi method see ostrowski 1973 stoer 2002 also known as the method of false position or secant method in the pl method to identify the confidence interval bounds this proposed method was named as rf pl method herein the regula falsi method is one of the easiest methods for finding the zeros of a function while there are more elaborate techniques ranging from modified versions of the regula falsi to newton s and steffensen s methods e g see chen 2007 chen and li 2006 grewal 2018 stoer 2002 in general finding the zeros or roots of a given function f x means to find the argument x for which f x 0 in multiple real world problems finding the zeros analytically is often very difficult if not impossible and thus numerical methods are turned to for iteratively approximating them in the regula falsi method the approximation starts with the points xa and xb which bound a sufficiently large interval such that it contains a zero of f x then the regula falsi method consists of the following steps in each iteration a calculate x c x b f x a x a f x b f x a f x b which is the approximation of the zero b if the error is less than or equal to the pre defined error tolerance ε the estimation is considered successful and no more iterations are required c otherwise the interval is redefined such that the interval still contains the zero i e as xa xc or xc xb whether f xa f xc 0 or f xc f xb 0 respectively and a new iteration is performed here x θ j y t t i and f x l p θ j l p thr θ j the ε 0 01 l θ l p thr θ j was adopted and verified to be adequate for the target problem after scrutiny the θ which includes the point estimate θ j was obtained by maximizing l p θ j as in the ml estimation method the pl and the rf pl procedures were compared in terms of the required computational time the computational time is the time needed for computing the full l p θ j and estimating the confidence interval bounds of θ j and θ j to yield estimates with approximately the same ε for a set of pre selected ts throughout the observation period of a dataset in the pl and rf pl methods respectively the fifty equally log spaced ts in the range of 2 yr t 100 yr were also considered herein both procedures were implemented and executed on a computer with an intel core i7 7700hq 2 80 ghz x64 based processor 16 gb random access memory ram and windows 10 home 64 bit operating system 2 5 the generalized profile likelihood method to resolve the potential issue of numerical instability for short datasets the gml principle was extended to the pl method in this paper which is named the generalized pl gpl method similar to the gml method the gpl method is also considered semi frequentist in the gpl method a geophysical prior for κ was adopted to prevent its unrealistic estimates avoiding the unrealistic estimates of κ is also beneficial in estimating other distribution parameters more reliably due to their joint computation in these methods in the gml method martins and stedinger 2000 proposed the beta distribution π κ b e t a p 6 q 9 with support in the interval 0 5 κ 0 5 as the prior distribution for κ of the gev distribution this beta distribution has 90 of the probability mass concentrated in 0 1 κ 0 3 mode κ 0 12 and e κ 0 1 this defined range of κ was determined based on reasonable values of the shape parameter and is also supported by a number of studies e g papalexiou and koutsoyiannis 2013 sun et al 2015 villarini and smith 2010 in this paper the geophysical prior adopted by martins and stedinger 2000 was used the generalized likelihood and log likelihood functions are given by eqs 15 and 16 respectively 15 g l θ l θ π κ 16 g l θ l θ log π κ using the eq 16 the generalized profile likelihood g l p θ j and its corresponding confidence interval c β are given by 17 g l p θ j max θ j g l θ j θ j 18 c β θ j 2 g l θ g l p θ j q 1 β for m 1 0 0 and m 1 1 0 the g l p θ j y t t i is obtained by extending eqs 13 and 14 respectively 19 g ℓ y t t i ξ 1 α κ t 1 n log α 1 1 κ log κ α y t y t t i ξ 1 t i t log 1 p κ κ α y t y t t i ξ 1 t i t log 1 p κ 1 κ log 0 5 κ p 1 0 5 κ q 1 b p q 20 g ℓ y t t i ξ 1 α 0 α 1 κ t 1 n log α 0 α 1 t 1 1 κ log 1 κ α 0 α 1 t y t y t t i ξ 1 t i t α 0 α 1 t i α 0 α 1 t 1 log 1 p κ 1 κ α 0 α 1 t y t y t t i ξ 1 t i t α 0 α 1 t i α 0 α 1 t 1 log 1 p κ 1 κ log 0 5 κ p 1 0 5 κ q 1 b p q where b p q γ p γ q γ p q in which γ is the gamma function the geophysical prior can be further refined if case specific information such as the regional skewness is available similar to the rf pl method the regula falsi numerical technique was coupled with the gpl to estimate the bounds of the confidence intervals rather than the full g l p θ j which was referred to as the rf gpl method herein in the rf gpl method the θ which includes the point estimate θ j was obtained by maximizing g l p θ j as in the gml method as the rf gpl method was proposed to deal with the issue of numerical instability of the rf pl method a number of subseries of different sample sizes were generated from the eight datasets to test its feasibility and convenience under nonstationarity each observation is a realization of a different distribution as the underlying stochastic process varies as a function of time hence the recording time of each data point matters as a result a set of subseries was generated by extracting the data points from subperiods using the moving windows approach to preserve their temporal dependence here the window length l 30 n and shift s 1 were used to extract the subseries of different sample sizes starting from 30 thus the number of the subseries depended on the sample size of the original dataset 2 6 performance assessment metrics the average width awt and the average asymmetry degree aadt of the confidence intervals were employed to compare the estimates of the rf pl and rf gpl methods these metrics assess the width and the symmetricity of the confidence intervals with respect to the point estimates respectively over the corresponding observation period for a given t they are calculated by 21 a w t 1 m t 1 m m t t u m t t l 22 a a d t 1 m t 1 m m t t u y t t m t t u m t t l 0 5 where y t t is the point estimate and m t t u and m t t l are the upper and lower bounds of the confidence interval for the t at t respectively to examine the numerical instability of the two methods the number of instability cases ic was used here the instability was defined as the convergence failure of the optimization algorithm for l or the presence of complex values in the upper and or lower bounds of the confidence interval at least at one point of time over the observation period thus for a given sample size the ic is the number of subseries that are subject to the numerical instability 23 i c k 1 n c k where c k 0 if m t t 1 m u m t t 1 m l r 1 otherwise where m is the length of the subseries and n is the number of subseries of a given sample size generated using the original dataset 3 results and discussion the results from the model assessment table a1 in appendix show that the determined models are optimal based upon the statistic d and or the aw except d5 and d6 for d5 and d6 m s is not inferior to both m 1 0 0 and m 1 1 0 according to the assessment criteria as shown in fig 1 the data points of these two datasets appear to be more asymmetrically distributed about the time invariant mean the blue line through the time such a variation pattern might imply a potential temporal trend in the skewness which could lead to the relatively low performance of the nonstationary models for these two datasets in addition the diagnostic assessment confirmed that the employed ns hfa models fit all datasets significantly well hence the ns hfa models selected for the datasets namely m 1 0 0 for d1 d4 and m 1 1 0 for d5 d8 are considered appropriate and used in the proposed rf pl and rf gpl methods in the following sections 3 1 the reduction of computational demand by regula falsi profile likelihood rf pl method fig 2 illustrates the implementation of the pl method for generating the confidence intervals changing over t for the dataset d1 as an example as shown in fig 2 a the full profile l p y t t i is estimated over a range of y t t i values that includes the intersections of l p y t t i with l p thr y t t i at the ti and t of interest here t 100 yr and ti 1911 the point estimate y t t i and the bounds of its confidence interval are indicated by the black and red dots on l p y t t i respectively in fig 2 a as well as shown in fig 2 b the surface of the estimated l p y t t i over a certain range of the frequency domain 2 yr t 100 yr for the same ti is then yielded by connecting the full l p y t t i of different ts finally the frequency curves and the bounds of their confidence intervals which are commonly used in practice are derived by repeating the procedure for all the time slices of interest fig 2 c as illustrated the computation of the full l p y t t i is in fact unnecessary to assess the confidence intervals of the frequency curves and only the black and red dots are needed for practical application purposes the proposed rf pl method bypasses the derivation of the full l p y t t i by just deriving its point estimate y t t i and intersections with l p thr y t t i so that the computational time can be substantially reduced fig 3 displays the derived y t t i and its confidence interval at the last year of the observation period by the pl and rf pl methods for two datasets d1 and d8 as examples as shown in fig 3 the two methods yielded practically identical results as both the upper and lower bounds of their confidence intervals overlapped in addition it is apparent that the confidence interval is asymmetric with respect to y t t i the aadt for t 100 yr for d1 ti 2018 and d8 ti 2019 are 0 18 and 0 22 in both methods the asymmetricity feature of the pl and the rf pl methods is an attribute deemed appropriate for the hfa similar results were observed in all other datasets not shown therefore these methods performed equivalently in generating the confidence intervals nevertheless the rf pl method substantially reduced the computational burden as it took only from 4 4 to 6 2 of the time required by the pl method for all the datasets fig 4 among the datasets the considerably higher computational time of d5 in both the pl and rf pl methods would be ascribed to the combination of its sample length 108 the complexity of the model m 1 1 0 and the numerical challenges due to its statistical characteristics as discussed in section 3 3 the result of reducing the computational time is not surprising as the rf pl method avoids the computation of the entire l p y t t i to quantify the confidence interval as described previously this advantage of the rf pl method was consistent in all datasets irrespective of the different nonstationarity patterns modeled using either m 1 0 0 or m 1 1 0 thus these results demonstrate that the proposed rf pl method is capable of yielding equivalent estimates to the pl method while significantly reducing the computational demand in the ns hfa the computational benefit of the rf pl method would be particularly attractive for regional national continental and global scale studies in which a large number of datasets are analyzed it is worth noting that the rf pl method which was formulated based upon the gev distribution with the linear nonstationary structures for the location and or scale parameters in this paper is also applicable for any other distributions and nonstationary structures to apply the rf pl method for other distributions and or nonstationary structures their corresponding reparametrized log likelihood function e g eqs 13 and 14 are needed in addition besides the regula falsi method other numerical techniques could be also employed the alternatives include the modified versions of the regula falsi method and the newton s and steffensen s methods among others however the classical regula falsi method may be particularly convenient as it does not require the derivatives of the corresponding log likelihood function and guarantees convergence as long as the initial interval xa xb is sufficiently large such that it contains the zero of the underlying function 3 2 the performance of regula falsi generalized profile likelihood rf gpl the rf gpl and rf pl methods were compared in terms of both numerical stability and the derived confidence interval using the subseries of different sample sizes extracted from the original datasets since the estimation of extreme events located in the right tail of a distribution is often more prone to the divergence issue obeysekera and salas 2014 the comparison of the aw aad and ic was focused on the t 100 yr herein fig 5 shows the ic of the two methods for sample sizes ranging from 30 to the full sample for all datasets it is apparent that the rf gpl method outperformed the rf pl method in terms of numerical stability as it consistently reported lower ics for all sample sizes and datasets in particular the instability problem did not occur in the rf gpl method when m 1 0 0 was used for datasets d1 d4 for all sample sizes while it was considerably reduced when m 1 1 0 was adopted for datasets d5 d8 the higher ics reported when using m 1 1 0 d5 d8 compared to when using m 1 0 0 d1 d4 in both methods suggest that the increase of the model complexity would trigger numerical instability besides the numerical instability of these methods might be also associated with the dataset specific characteristics when the same nonstationary model is adopted for example the instability issue of the rf pl method was milder for d6 and d7 compared to d5 and d8 all modeled with m 1 1 0 as ics were only reported when the sample size is 35 for d6 and d7 in addition the improvement offered by the rf gpl method appeared to be little for d6 all these results demonstrate that overall the rf gpl method can avoid or substantially mitigate the instability problem irrespective of the nonstationary patterns and the nonstationary structures of the ns hfa model especially for short datasets however the results also revealed that its efficiency would depend on both the complexity of the ns hfa model and the dataset characteristics particularly the skewness that determines the shape parameter as discussed in section 3 3 and thus might vary to some degree from case to case fig 6 illustratively compares the y t t i and its confidence interval at ti 2018 between the smallest sample size n 30 fig 6 a and the full sample size n 107 fig 6 b by both the rf pl and rf gpl methods for d1 as an example as shown in fig 6 the y t t i by the rf gpl was less sensitive to the sample size compared to that by the rf pl as it yielded closer estimates between the two sample sizes as for the confidence interval the increase of the aw along with the reduction of the sample size was observed in both methods whereas the aw increment was more prominent in the rf pl method this sample result visually demonstrates that the rf gpl may yield a narrower yet realistic confidence interval which is often ascribed to avoiding the very high upper bound of the confidence interval especially for short samples figs 7 and 8 further display the aw 100 yr and aad 100 yr respectively derived by both methods for all subseries on one hand compared to the rf pl method the rf gpl method produced lower aws and a considerably narrower variation range of the aws without the exceptionally high values in general the medians of aws of the rf gpl method were often lower than d2 d5 d6 and d8 or approximately the same as d1 d3 and d4 of those of the rf pl method in particular the reduction in the aws in the rf gpl method was more apparent for smaller sample sizes on the other hand the aad 100 yr of the rf gpl method exhibited a less consistent behavior throughout the datasets fig 8 compared to the rf pl method the rf gpl method yielded consistently higher aad medians d3 d5 and d8 or lower aad medians d2 d6 and d7 with either a larger or smaller variation range while for d1 and d4 a more erratic pattern was observed note that differing from the aw the aad relies on the upper and lower bounds of the confidence interval as well as the y t t i given a sample size and original dataset the variability of the aad reflects the differences in the statistical characteristics among the sub samples for the rf pl and rf gpl methods while for the rf gpl method the varying role of the geophysical prior among the sub samples would also lead to the different estimates of y t t i and its lower and upper bounds and consequently the aad eq 22 all these results suggest that the rf gpl has the capacity to reduce the uncertainty in the ns hfa by narrowing the confidence intervals in general especially for short datasets while yielding asymmetrical conference intervals despite that the aad may vary largely among different sample sizes and datasets furthermore the advantages of the rf gpl method might be less apparent for long datasets for example a wider range and a higher median of the aw 100 yr of d1 for sample sizes 90 were yielded by the rf gpl compared to the rf pl fig 7 hence to further compare the rf gpl and rf pl methods for long datasets table 1 presents the relative differences in the awt aadt and y t denoted as δawt δaadt and δ y t respectively for several ts when using the full samples the y t is the average of y t t over the observation period the negative δ y t δawt and δaadt indicate that the y t awt and aadt yielded by the rf gpl method are lower than those by the rf pl method in general the y t of both methods were only slightly different as the absolute values of δ y t are small for all datasets except d5 the dataset d5 exhibits a notable asymmetry in the trend of the standard deviation skewed towards the high flows fig 1 which might suggest that m 1 1 0 did not capture its nonstationarity well and might be a cause of the large deviation between the two methods moreover the rf gpl method reduced the confidence interval in most cases as indicated by the negative δawt however the positive δawt was also found in a few cases e g for t 100 yr of d1 and d8 suggesting that the rf gpl method might be marginally or not advantageous over the rf pl for some long datasets in which the numerical instability is not likely to occur in both methods regarding the asymmetricity degree it was overall decreased in the rf gpl method as indicated by the negative δaadt although its increase was also found in some cases both the negative and positive δaadt were consistent with the large variations of aad previously observed in the rf gpl method fig 8 therefore the case specific exploration would be desired to select either the rf pl or the rf gpl method for long datasets 3 3 the role of the geophysical prior in the shape parameter the behavior of κ was further investigated to better understand the role of the geophysical prior in the rf gpl method note that the point estimate of κ κ is time invariant whereas the upper κ u and lower bounds κ l of its confidence interval vary in both the temporal and frequency domains these κ estimates were derived along with the estimates of l p y t t i from θ j in eq 7 fig 9 showcases the κ and the average κ u and κ l over the observation period denoted as κ u and κ l respectively derived by both the rf pl and rf gpl methods for all subseries for t 100 yr as expected the geophysical prior influenced κ κ u and κ l in the rf gpl method by constraining them within its defined support and preventing large deviations from it it is apparent that the rf gpl method reduced the variability in κ as well as its confidence interval by lowering the κ u and or elevating the κ l such effects were more apparent for the subseries of small sample sizes in particular the introduction of the prior distribution in the rf gpl method eliminated the questionably high κ u a high κ is associated with a high y t t i which in turn may produce a negative argument of the logarithm in the log likelihood function eqs 13 and 14 and cause the numerical instability among the datasets the numerical instability in the rf pl method was more prominent for d5 and d8 for a variety of sample sizes including large sample sizes fig 5 as shown in fig 9 a larger number of κ u estimated by the rf pl method fell outside the support of the geophysical prior i e 0 5 κ 0 5 for d5 and d8 compared to all other datasets approximately 99 and 59 of the κ u in the rf pl method located outside the support for d5 and d8 respectively in addition 3 of κ l values of the rf pl method were also higher than 0 5 for d5 in contrast the rf pl method estimated only a few 8 or no κ u larger than 0 5 for d6 and d7 respectively these coincide with the higher ic reported in the d5 and d8 than in the d6 and d7 these results suggest that the improved numerical stability of the rf gpl method could be generally ascribed to lowering κ u in the rf gpl thus the skewness which primarily determines κ would be a key dataset specific characteristic affecting the numerical stability of these methods these results advocate for using the informative prior distribution of κ as an effective solution to avoid or mitigate the numerical problems in particular for short datasets as shown previously the geophysical prior used in this paper is in general appropriate as its use in the rf gpl method was found to enhance the numerical stability of the rf pl method however this prior distribution might not always be optimal fig 9 also exhibits consistently higher estimates of the κ u κ and κ l of the rf pl method than those of the rf gpl for all sample sizes for dataset d5 in such a case the geophysical prior might substantially deviate from the true κ although unknown and in turn the prior distribution would introduce bias to the estimates that may persist in large sample sizes this could explain the large δ y t between the rf pl and rf gpl methods of d5 when using the full sample size table 1 under such circumstances the use of a less informative prior distribution may be an alternative solution to diminish the potential bias introduced into the analysis however this might in turn lead to the increase of the numerical instability for example when using a less informative prior distribution than the geophysical prior of martins and stedinger 2000 for d5 namely with the same mode κ and e κ but a higher entropy 50 and wider support 0 75 κ 0 75 the estimated δ y t was lower 25 83 for t 100 yr whereas the ic was increased by 12 table 1 and fig 5 therefore the trade off between the reduction of the potential bias and the effectiveness in mitigating numerical divergence should be considered when modifying the prior distribution of κ on the other hand the use of case specific prior information e g regional skewness information whenever is available can also help to refine the geophysical prior in the rf gpl method and thus further enhance its performance however it does not necessarily guarantee numerical stability especially for datasets of high skewness and thus high κ 3 4 future research recommendations there is a need of performing the ns hfa when there is evidence of nonstationarity o brien and burn 2014 ragno et al 2018 vu and mishra 2019 the novelty of this paper is primarily methodological while the proposed methods the rf pl and rf gpl methods are applicable for both the at site and regional ns hfa the proposed rf gpl method not only advances the pl method to make it more practically feasible by improving its computational efficiency and numerical stability but also reduces the uncertainty in the ns hfa the latter improvement might enable the use of more complex nonstationary structures e g with more covariates and or more elaborated link functions which has been constrained due to the high uncertainty associated with their use ouarda et al 2019 serinaldi and kilsby 2015 similar to other methods for the ns hfa the estimates of the rf pl and rf gpl methods can be further utilized in hazard and risk assessments read and vogel 2015 salas and obeysekera 2014 and in turn guide the mitigation management decisions and infrastructure design to cope with the hydrological hazards especially under changing conditions e g climate change and changes in land use cover beyond the advancements made by the rf pl and rf gpl methods for the ns hfa there are still future research needs and in turn opportunities for improvement from the perspective of the ns hfa models used in this paper there are several limitations that are generic for the ns hfa the temporal covariate was adopted to illustrate the proposed methods while these methods are in fact applicable for any selected covariate s including the physical covariates whenever they can be adequately identified the use of the physical covariate s e g changes in land use cover etc in the ns hfa models is particularly desired in addition besides the gev distribution there are other distributions such as the pearson type iii log pearson type iii log normal and generalized logistic among others often considered in the ns hfa thus the extension of the proposed rf pl and rf gpl methods in which the ns hfa models also consist of these distributions is required for the generalized implementation of these methods in the ns hfa moreover in the rf gpl method the geophysical prior of martins and stedinger 2000 commonly utilized in the literature was adopted the preliminary results from the use of the less informative prior distribution for d5 demonstrated the effect of the prior distribution on the numerical stability of this method further investigation on the sensitivity of the rf gpl method to the prior distribution of κ e g using a sensitivity analysis in terms of both the numerical stability and the bias in estimates is recommended as future research such an investigation might shed light on the optimal prior distribution to some degree although the prior distribution might depend on the datasets and the study regions to present there are several methods to estimate the uncertainty in the ns hfa although among these methods the pl method has been well acknowledged to be superior in theory and was further advanced in this paper the comprehensive comparison of the proposed methods with other existing methods is still desired this is particularly important for practitioners to select an appropriate method in addition the sampling uncertainty which originates from the parameter estimation due to limited sample sizes and is often large and commonly considered fundamental serinaldi and kilsby 2015 was quantified here by the proposed methods whereas the uncertainty originated from other sources in the ns hfa such as the measurement error kjeldsen et al 2014 kuczera 1996 merz and thieken 2005 model determination kjeldsen et al 2014 merz and thieken 2005 and parameter estimation approach debele et al 2017 griffis and stedinger 2007 šraj et al 2016 among others was not considered the quantification of the overall uncertainty originating from multiple sources is often challenging but always favorable similar to other existing methods for the ns hfa the proposed methods can be applied for making future predictions in several ways the methods can be directly applied to the projected future precipitation and flow extremes alternatively when the temporal covariate is used to capture the nonstationarity in these methods the ns hfa model can be used for extrapolation assuming that the temporal trend s remains the same in the future when a physical covariate is adopted in the ns hfa model the methods can make predictions using the projected covariate assuming that the covariate itself and its link function for capturing the nonstationarity remain valid in the future thus the adequate projections of the extremes or covariates and or the validity of the adopted assumptions are required for reliably implementing these methods for making future predictions in addition it has been well acknowledged that the ml and pl methods suffer numerical instability when sample sizes are small yet there is not a standard definition for what a small sample size is as shown in the results fig 5 given the same nonstationary pattern and model temporal trends in the mean only d1 d4 or temporal trends in the standard deviation and or mean d5 d8 the numerical instability occurred at different sample sizes in both the rf pl and rf gpl methods in addition the numerical instability issue is likely to be more prominent when nonstationarity is detected in the standard deviation and m 1 1 0 is employed these results suggest that there is not a fixed threshold to distinguish between small and large samples based upon the numerical stability as the threshold would depend on the nonstationary pattern the ns hfa model adopted and the statistical characteristics of the dataset e g high order moments in the s hfa martins and stedinger 2000 also demonstrated that defining if a sample is small or large depends on the statistical characteristics of the dataset e g the skewness shape parameter due to the lack of a general definition of small large samples in the hfa in terms of numerical stability the implementations of these methods would require case specific examinations to avoid the numerical divergence issue in practice 4 conclusion to facilitate the practical implementation of the pl method in the ns hfa the issues of its high computational burden and potential numerical divergence were tackled in this paper the rf pl method in which the regula falsi numerical method was applied to estimate the confidence interval bounds using an explicit analytical expression of the reparametrized log likelihood function rather than computing the full profile was proposed to reduce the computational burden following the notion of the gml method the rf pl method was further extended to the rf gpl method by introducing a prior distribution to the shape parameter to enhance its numerical stability in particular for short datasets to illustrate the efficiency of the proposed methods they were applied to eight flow and precipitation amss from north america exhibiting different nonstationary patterns and modeled by two different ns hfa models the results demonstrated that the rf pl method effectively reduces the computational time of the pl method by 94 96 without degrading the estimation accuracy moreover the proposed rf gpl method was proven successful in not only avoiding or substantially mitigating the numerical instability of the rf pl method but also decreasing the uncertainty level by constraining the estimated shape parameter especially for small sample sizes therefore the proposed methods resolved the issues faced by the pl method and consequently can foster its practical implementation in the ns hfa credit authorship contribution statement cuauhtémoc tonatiuh vidrio sahagún conceptualization methodology formal analysis investigation writing original draft jianxun he conceptualization methodology writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first author of this paper is funded by a doctoral scholarship from the national council for science and technology of mexico conacyt and the universidad de guadalajara this work is also partially funded by the discovery grant of natural sciences and engineering research council held by the second author appendix the model assessment and the diagnosis assessment of the ns hfa models table a1 shows the pairwise comparison among the m 1 0 0 and m 1 1 0 as well as their stationary counterpart m s using the statistics d test the results indicate that the m 1 0 0 and m 1 1 0 significantly outperform other models when fitting d1 d4 and d7 d8 respectively whereas the m s is not significantly outperformed by the nonstationary models for d5 and d6 moreover table a1 also presents the aw of the confidence intervals produced by the models at the fifty equally log spaced ts used in this paper based upon the aw the m 1 0 0 is superior when fitting d1 and d2 while the m 1 1 0 is superior when fitting d8 the selected models based on the aw for d1 d2 and d8 are consistent with those selected based on the statistic d while the model selection is inconsistent for all other datasets the selection of different models according to distinct evaluation criteria such as the goodness of fit and the uncertainty is not uncommon and has been documented in the literature e g ouarda et al 2019 in addition from the uncertainty perspective the stationary model often outperforms its nonstationary counterparts due to its lower model complexity serinaldi and kilsby 2015 as a result the models used for the datasets except d5 and d6 are optimal based upon the goodness of fit and or uncertainty in general for d5 and d6 the m s is not inferior to the nonstationary models according to the assessment criteria as shown in fig 1 the data points of these two datasets appear to be more asymmetrically distributed about the time invariant mean the blue line with the time such a variation pattern might imply a potential temporal trend in the skewness however the shape parameter was assumed constant in the nonstationary models in this paper and in the vast majority of the literature which might lead to the relatively low performance of the nonstationary models for these two datasets 
199,modeling fluid flow in three dimensional 3d discrete fracture networks dfns is of relevance in many engineering applications such as shale oil gas production geothermal energy extraction nuclear waste disposal and co 2 sequestration a new boundary element method bem technique with discontinuous quadratic elements in conjunction with a parallel domain decomposition method ddm is presented for the simulation of the steady state fluid flow in dfns consisting of stochastically generated 3d planar fractures arbitrarily oriented and having differing hydraulic properties numerical examples characterized by dfns of increasing complexity are proposed to show the accuracy and the efficiency of the presented technique that provides good approximations of the fluid flow around domain interfaces where the solution usually displays sharp gradients like around intersections between traces the segments originated by the intersection between two fractures intersections between traces and fracture boundaries or intersections between fractures and wellbores the conjunction with a ddm approach is a promising strategy to speed up the computations by also exploiting the advantages of parallel computing techniques the technique is implemented in the code pydfn3d available at https github com binwang0213 pydfn3d keywords discrete fracture network dfn fractured rock hydrology boundary element method bem domain decomposition method ddm subsurface fluid flow 1 introduction modeling fluid flow in fractured rocks has been for a long of relevance in many engineering applications such as oil gas production geothermal energy extraction nuclear waste disposal and co 2 sequestration iding and ringrose 2010 hyman et al 2015 sun et al 2017 wang et al 2021 2018b zhang et al 2019 ye et al 2021 two predictive models are generally used the equivalent porous medium epm and the discrete fracture network dfn epm is considered when fractures are highly connected and the fracture network can be condensed into a porous medium with an associated permeability tensor lee et al 2001 sahimi 2011 the dfn model is an alternative to epm models for multi scale fracture networks where all the fractures are explicitly represented hyman et al 2015 fumagalli et al 2019 in the last decades significant efforts have been made to simulate the fluid flow in three dimensional 3d dfns and a variety of numerical approaches are now available they can be categorized into 1 mesh based approaches 2 equivalent pipe networks epns and 3 boundary element method bem approaches in the mesh based approaches a computational mesh is generated over the entire fracture system and the flow field is then obtained by resorting to the finite element method fem or the finite volume methods fvm both methods offer considerable precision but given the geometrical complexity of dfns of practical interest as e g the one depicted in fig 1a for the use of high computational power and memory capacity may be required hyman et al 2017 especially for high quality meshes and when a local grid refinement is applied to accurately resolve the sharp pressure gradients around fracture intersections traces or in the near field of a well fumagalli et al 2019 even when the number of fractures in a dfn is relatively small the number of mesh elements may be still considerable for conformity and quality constraints a suitable mesh for fem based approaches is shown in fig 1b for a dfn consisting of 932 fractures of various sizes and orientations the mesh counts nearly 1 61 million nodes and 3 84 million triangular elements to avoid the difficulties related to the mesh generation non conforming meshes independently built on each fracture are used and the minimization of a cost functional is applied to enforce the coupling of a domain decomposition method ddm resorting to error estimators to control the accuracy of the solution berrone et al 2019b 2018 2019a polygonal meshes are also suggested as an alternative to ease the generation of conforming meshes of complex dfns like in benedetto et al 2016 fumagalli and keilegavlen 2018 dassi et al 2021 and berrone et al 2021 where the virtual element method is used in antonietti et al 2016 where mimetic finite differences mfds and mixed finite elements are combined and in chave et al 2018 where hybrid high order hho methods are applied in the epn approach a dfn is morphed into a network of one dimensional conductors pipes endowed with an appropriate conductance value and connecting the mid points of the traces in a fracture cacas et al 1990 dershowitz and fidelibus 1999 xu et al 2018 huang et al 2021 epns feature great advantages in terms of computational efficiency and simplicity and as such are suitable to model large fracture networks however unique pipe connection patterns are difficult to generate and the equivalence gives rise to unavoidable errors a graph based method has been recently proposed to reduce the error of epns andresen et al 2013 hyman et al 2017 srinivasan et al 2020 for the application of bem approaches a one dimensional discretization of fracture edges and traces is required shapiro and andersson 1985 bem reveals being very accurate to resolve sharp pressure gradients near traces and wellbores even when few one dimensional elements are used semi analytic basis functions are introduced for the representation of the variables katsikadelis 2016 wang et al 2018a a suitable bem mesh is shown in fig 1c for the same 932 fractures dfn characterized by several boundary elements about two orders of magnitude less than the number of elements of the corresponding fem mesh the literature related to the use of bem for the prediction of the fluid flow in three dimensional dfns is still quite limited shapiro and andersson 1983 1985 and andersson and dverstorp 1987 were the first to apply this method by resorting to constant boundary elements for fracture edges and traces later lenti and fidelibus 2003 proposed an advanced bem technique in which quadratic elements are used for fracture edges and constant elements for internal traces the choice of constant elements is derived from the need to handle flux discontinuities at the intersections between two traces and between a trace and an edge in a fracture an optimization procedure is also applied to reduce memory usage when solving the global matrix the authors pointed out that the use of low order constant elements may lead to up to 22 flux mismatch errors yang et al 2016 and chen et al 2015 2016 2017 2018 recently proposed a bem method to model transient shale gas flow problems in a 2d infinite domain where bem with constant elements is used to solve matrix flow whereas finite differences are used to solve 1d fracture flow and the laplace transform is used to resolve the transient flow fast solutions for the flow regime are crucial for the industrial application of dfns in this respect none of the three approaches mentioned above prevails in offering such solutions however the scientific community may benefit from any contribution to improving one of the methods in this note a new bem technique is proposed for the prediction of the steady state single phase fluid flow in 3d dfns the novelty of the technique lies in the use of discontinuous quadratic basis functions for the boundary elements in combination with a domain decomposition approach uniform permeability and aperture values are ascribed to each fracture the fluid flow is governed by a two dimensional laplace equation with sinks sources located at the traces and at the intersections with wellbores assumed linear with the proposed technique accurate predictions can be provided also where large gradients are expected furthermore with ddm the way to the development of the technique for parallel solvers is paved thus allowing the reduction of the computational times the note is organized as follows in section 2 a detailed description of the adopted formulation and of the typical discretization associated with the proposed bem technique is reported in section 3 the ddm algorithm is briefly introduced and commented in section 4 the technique is tested on three verification examples and one synthetic field application example finally the possible extensions of the technique are discussed and concluding remarks are drawn accordingly the technique is implemented in the computer code pydfn3d 2 bem formulation in this section the bem formulation adopted for the steady state fluid flow in 3d dfns is described the derivation of the integral equations and of the final algebraic system is reported given the special basis functions of the discontinuous elements the following assumptions are considered 1 fluid dynamic properties are constant and independent of the pressure 2 the rock matrix is impervious such that the fluid flow occurs only inside fractures through fracture intersections and in wellbores 3 fracture aperture and permeability are uniforms within each fracture but different values are allowed among different fractures 4 the gravity effects are ignored implicitly assuming that the thickness of the fractured reservoir is small compared with the horizontal extension dverstorp and andersson 1989 smeraglia et al 2021 however the formulation can be easily extended to include such effects 5 pressure drops caused by friction in wellbores are neglected thus pressure inside is uniform 2 1 boundary integral equations with reference to a coordinate system x x 1 x 2 x 3 arbitrarily oriented in the 3d space a plane π is considered the unit vector normal to π is n π a local reference system x x 1 x 2 is chosen on π a domain ω π is introduced the boundary of ω is denoted by γ and n is the unit normal vector to γ outward pointing from ω a 3d fracture ω is defined such that ω x x γ b f t π γ π being b f x the fracture aperture a number much smaller than each dimension of ω in π for the above mentioned hypotheses fracture aperture b f x b f is uniform in π and the top and bottom walls of the fracture are planes parallel to π the fluid velocity vectors inside the fracture are all parallel to π given the fracture intrinsic permeability k unit m 2 uniform in ω the fluid dynamic viscosity μ unit kg m 1 s 1 independent of the pressure p and by neglecting as mentioned the gravity effects the governing equation for the steady state fluid flow in ω with a generic volumetric source term q x unit s 1 can be expressed as follows chen 2007 1 k μ 2 p x q x where p x is the fluid pressure unit kg m 1 s 2 the 3d problem can be reduced into a 2d problem by averaging eq 1 along n π by splitting the operator in an in plane component π and a normal component n π one has 2 k μ π 2 b f 2 b f 2 p x d n π b f 2 b f 2 q x d n π k b f μ π 2 p x q x in which p x 1 b f b f 2 b f 2 p x d n π is the averaged pressure along n π tech q x 1 b f b f 2 b f 2 q x d n π is a distributed areal source in what follows the fracture is dimensionally reduced to the planar domain coinciding with ω is used for π and p is assimilated to p the fractures of a dfn can mutually intercept therefore for the reduction to planes the intersections are segments traces also wellbores can cross the fractures and the intersections are point intersections wellbore intersections as the rock matrix is impervious there is no leakage from the blocks bounded by the fractures and therefore there are no areal sources rather the source term q x is given by linear sources and point sources chen 2007 from the traces and the wellbore intersections respectively traces in ω are labeled by t whereas t is the set of all the traces in the same domain wellbore intersections are denoted by s and s denotes the set of all the wellbore intersections in ω therefore the source term q x can be written as 3 q x t t δ t q t s s q s δ s being δ t q t the dirac delta function defined for any sufficiently regular function φ x as δ t q t φ t q t γ φ γ d γ where q t γ unit m 2 s 1 is the concentrated volumetric source per unit trace length on t whereas δ s for a wellbore s s is the dirac delta function located at the intersection point x s between the well and the mean fracture plane defined by δ s φ φ x s and q s unit m 3 s 1 is the volumetric point source at wellbore s by substituting eq 3 into eq 2 the governing equation for the fluid flow in a fracture is obtained as dong et al 2019 4 k b f μ 2 p t t δ t q t s s q s δ s after adding dirichlet and neumann boundary conditions a boundary integral equation bie corresponding to eq 4 is obtained in each collocation point x i ω 5 k b f μ c x i p x i γ p x w x i x n d γ k b f μ γ p n x w x i x d γ t t t q t x w x i x d t s s q s w x i x s where p n is the directional derivative of p along the normal and w w n are a weighing function and its normal derivative respectively the quantity c x i is a number depending on the location of x i namely if it is in the interior or on the boundary of the domain the definition of weighing functions is given in the following by neglecting the gravity the fluid velocity u unit m s 1 is equal to k μ p the corresponding bie for the components u j x i is 6 c x i u j x i b f k b f μ γ p x x j w x i x n d γ k b f μ γ p n x w x i x x j d γ t t t q t x w x i x x j d t s s q s w x i x s x j details on derivation and notation for the above bies are given in appendix a the weighting function w corresponds to the fundamental solution of the steady state fluid flow equation for a source point x i the function w and the derivative w n with respect to the normal n are brebbia and dominguez 1994 7 w x i x 1 2 π ln 1 r x i x w x i x n 1 2 π x i x n r 2 x i x where r x i x is the distance between x i and x 2 2 discretization with discontinuous quadratic elements lenti and fidelibus 2003 proposed a bem technique for dfns in which constant basis functions for the traces were used to deal with the flux discontinuities at trace trace and trace boundary intersections however due to the low accuracy of the constant element approximation the application of the technique implies errors increasing with the geometrical complexity of the dfn in this note an advancement is proposed by resorting to quadratic basis functions and discontinuous boundary elements bes intrinsically including flux discontinuities at the nodes as shown in fig 2 the discretization of each fracture is obtained by subdividing the boundary γ and the traces t t in straight bes collocation nodes are then placed on each element three collocation nodes are placed on each be to uniquely identify a quadratic polynomial on each element three quadratic basis functions n i x i 1 3 are consequently defined on each be by the condition n i x j δ i j for i j 1 3 being δ i j the kronecker delta the basis functions related to each be are then extended to zero outside the element additional nodes are also placed at wellbore intersections s s it is worth noting that collocation nodes never coincide with trace trace intersections and trace boundary intersections where the flux of p is discontinuous this is achieved by ensuring that trace intersection points and trace boundary intersection points coincide with be endpoints whereas collocation nodes are placed in the interior of each element as shown in fig 2 where collocation points are shown in blue green or red spots the set of elemental basis functions is denoted by n n i i 1 3 and the approximation of the pressure p and of its normal derivative are 8 p ξ i 1 3 p i n i ξ p n ξ i 1 3 p n i n i ξ 1 ξ 1 where ξ is a local normalized coordinate and p i p n i are nodal values functions in n are defined as follows 9 n 1 1 2 ξ α ξ α 1 n 2 1 ξ α 1 ξ α n 3 1 2 ξ α ξ α 1 with 0 α 1 and α is a collocation factor choosing 0 α 1 the first and the third node on each element are shifted inside it by a normalized quantity equal to α a value α 0 67 is used here as suggested in fratantonio and rencis 2000 boundaries and traces are approximated by using the same functions in n using x i 1 3 x i n i ξ α 1 i e the shape functions of eq 9 with α 1 a global numbering can be introduced for the collocation nodes and for the basis functions on all the bes and j tot is defined as the set of the indexes of all the nodes in the discretization the following subsets of j tot are also introduced j γ containing the indexes of the nodes belonging to the boundary γ j t containing the indexes of the nodes belonging to trace t t j s containing the index of the node matching x s and j t t t j t j s s s j s without loss of generality the indexes of these sets are numbered consecutively starting from j γ continuing with j t and ending with j s in what follows p γ x j j γ p j γ n j x p t x j j t p j t n j x p s x j j s p j s n j x denote the discrete solution on γ on the traces and on the wellbore intersections respectively similarly the discrete counterpart of p n on γ is defined by p n γ x j j γ p n j γ n j x whereas the source on trace t t is denoted by q t t x j j t q j t n j x finally p γ t s is the array collecting column wise the unknowns p j j j p n γ the array collecting the unknowns p n j γ j j γ whereas q t collects the coefficients q j t for j j t and t t and q s collects the coefficients q s s s the use of above definitions into the bie eq 5 yields 10 c γ p γ h γ γ p γ g γ γ p n γ g γ t q t g γ s q s c t p t h t γ p γ g t γ p n γ g t t q t g t s q s c s p s h s γ p γ g s γ p n γ g s t q t g s s q s where c γ t s are matrices with diagonal entries c i i equal to k b f μ when x i is in the interior of the domain ω and equal to k b f 2 μ when x i is on the boundary the g type and h type matrices are defined as follows 11 g i j γ γ k b f μ γ w x i x n j x d γ i j j γ g i j γ t t w x i x n j x d t i j γ j j t t t g i j γ s w x i x n j x i j γ j j s g i j t γ k b f μ γ w x i x n j x d γ i j t j j γ t t g i j t t t w x i x n j x d t i j j t t t g i j t s w x i x n j x i j t j j s g i j s γ k b f μ γ w x i x n j x d γ i j s j j γ g i j s t t w x i x n j x d t i j s j j t g i i s s 1 2 π ln r i i j s g i j s s w x i x i j j s h i j γ γ k b f μ γ w n x i x n j x d γ i j j γ h i j s γ k b f μ γ w n x i x n j x d γ i j s j j γ h i j t γ k b f μ γ w n x i x n j x d γ i j t j j γ t t note that in general g t γ g γ t g s t g t s g s γ g γ s the diagonal entries of g s s go to infinity as the distance r x i x approaches zero to handle the singularity and apply the constant pressure boundary condition at a wellbore nodes of unknown pressure p s x are placed on the boundaries at distance r s from the wellbore center where the node is located this handling of the wellbores was used and validated in a previous work wang et al 2018a eq 10 can be expressed in compact form as follows 12 c γ h γ γ 0 0 h t γ c t 0 h s γ 0 c s p γ p t p s g γ γ g γ t g γ s g t γ g t t g t s g s γ g s t g s s p n γ q t q s on each node of the boundary either pressure p or flux p n can be specified corresponding to a dirichlet or a neumann condition respectively thus the columns of the matrices in eq 12 can be re ordered to have the coefficients of all the unknowns on the left hand side and the coefficients of all the known terms on the right hand side however the prevailing condition is the insulation condition i e p n 0 once the solution of eq 12 is obtained pressure and velocity at any point x i can be calculated by applying eqs 5 6 in this work a simple wellbore model is considered that assumes a wellbore has a low production injection rate large pipe diameter and short segment length in the gravitational direction thus the pressure along the wellbore is assumed to be constant aziz 2001 for a wellbore with a flow rate q w intersecting multiple fracture planes in s 1 s 2 s n intersection points each one on a different fracture one additional condition need to be added to the system 12 given q s i the flow rate at intersection s i the equation i 1 n q s i q w is added which together with the assumption of constant pressure in the well is sufficient to close the problem 2 3 fast analytical bie integration after the bem discretization is performed given the singularity of the fundamental solution w there is the need to perform singular integrations nearly singular integrations and non singular integrations the singular and nearly singular integrations require special techniques to avoid the loss of accuracy related to standard numerical integration katsikadelis 2016 element subdivision analytical and semi analytical integration adaptive gaussian quadrature coordinate transformation and bie modification liu and nishimura 2006 tanaka et al 1994 gu et al 2016 can be adopted exact analytical integration formulations are also available fratantonio and rencis 2000 zhang and zhang 2003 zhang and an 2008 in this note an exact integration formulation for discontinuous quadratic elements is derived based on the method proposed in zhang and zhang 2003 the expressions for the integrals of g type and h type matrices for a boundary or a trace integration element in eq 11 are reported in what follows the quantities a l d l e l f l i l s l t l l 0 2 and a e are defined in appendix b 13 w x i x n j x d g 1 g 2 g 3 t w x i x n n j x d h 1 h 2 h 3 t when a collocation node does not pertain to the integration element the analytical integration formulations are 14 g 1 j 8 π a 2 α 2 a 1 α g 2 j 4 π a 0 a 2 α 2 g 3 j 8 π a 2 α 2 a 1 α h 1 e 4 π f 2 α 2 f 1 α h 2 e 2 π f 0 f 2 α 2 h 3 e 4 π f 2 α 2 f 1 α where j is the jacobian of the map from physical to reference element given by eq b 1 when a collocation node is instead on the integration element the formulations are 15 g 1 j 4 π s 2 α 2 s 1 α g 2 j 2 π s 0 s 2 α 2 g 3 j 4 π s 2 α 2 s 1 α h 1 h 2 h 3 0 for the discretized form of eq 6 the derivatives with respect to x k k 1 2 of the terms of the g type and h type matrices are required and are as follows 16 w x i x x k n j x d g 1 x k g 2 x k g 3 x k t x k w x i x n n j x d h 1 x k h 1 x k h 1 x k t again when a collocation node does not pertain to the integration element the formulations are 17 g 1 x k j 4 π e k 2 a 2 e k 1 a g 2 x k j 4 π e k 0 e k 2 a 2 g 3 x k j 4 π e k 2 a 2 e k 1 a h 1 x k 1 4 π i k 2 a 2 i k 1 a h 2 x k 1 4 π i k 0 i k 2 a 2 h 3 x k 1 4 π i k 2 a 2 i k 1 a finally when a collocation node is on the integration element the formulations are 18 g 1 x k j d k 4 π α t 2 α 2 t 1 α g 2 x k j d k 2 π α t 0 t 2 α 2 g 3 x k j d k 4 π α t 2 α 2 t 1 α h 1 x k h 2 x k h 3 x k 0 as previously mentioned all the formulations above are implemented and tested in the computer code pydfn3d 3 parallel domain decomposition method for large dfn problems in the context of bem the direct coupling i e the assembly of a large system of equations including the compatibility conditions at the interfaces among adjacent domains may too computationally demanding in previous works parallel ddms were developed aimed at iteratively solving a series of small dense linear subsystems rather than a coupled large unsymmetrical sparse linear system berrone et al 2013 2014 2015 wang et al 2019 in ddm conditions at interfaces are updated at each iteration until a solution that fits the specified compatibility condition at each interface is obtained here the main idea behind a ddm strategy suitable for the proposed approach is recalled referring to wang et al 2019 and to references therein for a deeper analysis on such methods a network of n f fractures ω k and n t traces is considered like the one in fig 4 being the set of all traces on fracture ω k denoted by t k k 1 n f and the set of all the traces in the network denoted by t each trace t t is shared by exactly two fractures ω i and ω j i j 1 n f such that a map τ between each trace and the corresponding pair of fracture indexes can be introduced τ t i j being t ω i ω j compatibility conditions at the traces are the continuity of pressure and the balance of normal fluxes which can be expressed as follows 19 q i t q j t 0 p i t p i t on t i j τ t where q i t p i t are the restriction of quantities q i and p i respectively to trace t on fracture ω i for t t i then the parallel domain decomposition algorithm here used for the resolution of large dfn problems is depicted in table 1 in which the superscript is used to indicate the iteration counter it is to remark that according to the solution strategy the compatibility conditions of eq 19 are satisfied up to a given tolerance tol this quantity being directly associated with the mass flux balance over all intersections the optimal relaxation parameter β k is defined as follows wang et al 2019 20 β k t t ε p i t k ε q i t k ε q j t k t t ε q i t k ε q j t k 2 i j τ t with error terms between two iterations ε p i t k p i t k 1 p i t k and ε q i t k q i t k k 1 q i t k clearly lower values of tol yield better flux balance at the traces at the cost of a larger number of iterations and computational time a tol value of 1 1 0 6 1 1 0 9 is used for the following computations 4 numerical examples in this section four numerical examples tests 1 4 for the validation of the proposed numerical technique are illustrated the first example refers to a problem with a known exact solution the second one is a test on a single fracture with multiple traces and one wellbore intersection examples 3 4 are aimed at demonstrating the robustness and the reliability of the technique when dealing with more complex dfn systems the dfn for the last example is stochastically generated by drawing position orientation size and permeability of each fracture from given probability density functions pdfs as practice for dfns using the open source dfn generator adfne alghalandis 2017 detailed information for all the examples below is available associated with the source code pydfn3d 4 1 test 1 unit square problem consider a darcy problem in a unit square domain ω x y 0 1 0 1 with μ 1 k 1 and b f 1 see fig 5 the following boundary conditions are prescribed see fig 5 21 p x y 15 cos 4 π x sinh 4 π y π cosh 4 π error indicators are considered in what follows to evaluate and compare the accuracy of the proposed technique defined as 22 e p l 1 n t p l exact p l h 2 n e u l 1 n u l exact u l h u l exact u l h n being n the number of sampling points uniformly distributed over the domain 1600 for test 1 and h is the mesh parameter equal to the maximum diameter for fem elements or to the maximum length for bem elements the results obtained with the proposed technique are compared with the results obtained by using the finite element method fem with continuous quadratic basis functions implemented in comsol comsol 2019 and by using constant bem elements and linear bem elements the four solutions are labeled in what follows bem quad fem quad bem const and bem linear respectively in fig 6 the convergence trends of the considered error indicators for the various methods are shown concerning the approximation of the pressure term all the solutions exhibit expected optimal convergence rates given the regularity of the solution and the polynomial accuracy of each method in particular it is observed that also the methods that use discontinuous basis functions are capable of providing extremely good approximations and the approximation level of quadratic bem is comparable to the one provided by quadratic fem concerning the approximation of the velocity term the proposed bem technique has a superior convergence rate with respect to the fem with basis functions of the same order also this result is expected since the derivative of p is directly computed with bem whereas for the fem solution it is the result of post processing from the fem solution that leads to a deterioration of the convergence results 4 2 test 2 unit square with 10 fractures the second proposed example deals with a unit square fracture domain with 10 traces and one wellbore intersection fig 7 in this example several regions with sharp solution gradients are included such as trace tips intersections between traces intersections between traces and fracture boundaries or intersections between fractures and wellbores the proposed quadratic bem technique is here validated comparing the bem quad results with the results derived by using constant bem elements bem const and with the solution given by fem with quadratic basis functions fem quad since the exact solution is not known in this case an additional solution is considered as a reference term obtained using the fem on a highly refined mesh fem quad ref fluid viscosity is set to 2 pa s and fracture permeability is 3 m 2 whereas given pressure values are imposed on the traces and on the wellbore intersection as detailed in fig 7 no flux conditions are instead set on the whole domain boundary in fig 8 the mesh setup for the three solutions are shown a the piece wise constant bem mesh counting 348 elements and 348 dofs b the quadratic bem mesh counting 116 quadratic elements and 346 dofs c the fem mesh having 6887 elements and 13982 dofs finally the reference fem mesh counts about 0 6 million elements and 1 2 million dofs the shaded map of the pressure values from the bem quad solution is reported in fig 7b as expected sharp gradients at trace tips and near the intersection with the well show two sampling lines l1 and l2 in fig 7a are used for the comparison l1 crosses several traces and intercepts the wellbore intersection l2 is aligned along with one of the traces results are presented in fig 9a b for pressure and velocity on l1 for x between 0 and 0 8 thus including trace tips and trace intersections it can be observed that the proposed bem technique with discontinuous quadratic basis functions is in excellent agreement with the reference solution both for the pressures and the velocities the bem const solution is instead less accurate near trace tips and trace intersections in fig 9c d the pressure and velocity solutions near the wellbore intersection for x between 0 8 and 1 are shown as expected the fem quad solution exhibits larger discrepancies from the reference solution for the velocities thanks to the exact point source fundamental function adopted in bem methods instead the two bem based solutions successfully capture the sharp pressure and velocity gradient in this region by using only one dof in fig 10a b the line plots for the four solutions along l2 are reported also in this case the superior performance of the proposed bem technique is apparent displaying an excellent agreement with the reference solution both the fem solution and the piece wise constant bem solution instead provide poorer approximations of the velocity solution near trace intersections and trace edge intersections as far as the computational cost is concerned the proposed bem technique with quadratic element also shows advantages over the conventional fem and bem solutions for a given level of accuracy it only uses 116 elements and 346 dofs to obtain an excellent match with the highly refined reference solution 4 3 test 3 4 fractures dfn with reference to fig 11 the steady state fluid flow in a small dfn consisting of four rectangular fractures is solved the results of the proposed technique are compared with the results of a fem model with quadratic elements and a refined mesh again solution fem quad ref three horizontal fractures ω 2 4 intersect a vertical fracture ω 1 the injection well s inj crosses ω 3 and the production well s pro crosses two fractures ω 2 4 fixed pressure values for the injection well and production well are 2 mpa p inj and 1 mpa p pro respectively the aperture values of the four fractures are 0 01 m wellbore radius is 0 001 m the fluid viscosity and fracture permeability are 0 001 pa s and 3 1 0 10 m 2 respectively in fig 12 the pressure and velocity solutions of the presented bem technique are compared with the reference fem solutions for the bem solution only 87 boundary elements resulted sufficient to reach a perfect match with the fem solution the proposed bem solution of this test case is obtained using the ddm algorithm 1 which allows solving iteratively small dense local linear systems instead of a large coupled unsymmetrical global system the stopping tolerance tol is set to 1 1 0 9 and the ddm algorithm converges in 112 iterations providing a solution with maximum flux mismatch over all traces and wells equal to 4 2 1 0 9 this error can be further minimized by reducing the stopping tolerance with however a minor impact on the global flux balance the computed flux at the injection well is 13 03 m 3 day in good agreement with the one provided by the reference fem solution with 0 2 million elements 13 05 m 3 day 0 2 difference also the plots reported in fig 12c d along a line passing close to the injection well the red line in fig 12a are in very good agreement 4 4 test 4 97 fractures dfn the last example refers to a 97 fractures dfn 4 faults and 93 fractures where there are 2 vertical wells drilled through four faults fig 13a the 93 fractures are randomly generated by the open source dfn modeling tool adfne alghalandis 2017 constant pressure values of 15 mpa and 10 mpa are fixed at the injection well and production well respectively no flow boundary conditions are prescribed on all fractures uniform aperture and permeability are assigned to all the fractures fracture aperture follows a log normal distribution cacas et al 1990 fig 13b with a mean value of 2 1 and a standard deviation value of 0 27 snow 1970 the cubic law k b f 2 12 is then applied to calculate the corresponding values of fracture permeability zimmerman and bodvarsson 1996 the faults have an aperture of 3 10 4 m and a permeability of 7 5 10 9 m 2 as a whole 2146 discontinuous quadratic elements 23 elements per fracture plane are used for this test in fig 14a the bem mesh for one of the faults of the dfn is shown in fig 14b the steady state pressure solution is reported it is obtained with the ddm algorithm 2132 iterations were required to reach a maximum flux balance error over all intersections and wells of 2 6 1 0 6 m 3 s the performance of the parallel ddm algorithm in pydfn3d could be easily accelerated through a parallel implementation in high performance computing hpc systems 5 conclusions a novel numerical technique based on the boundary element method bem with discontinuous quadratic elements is presented and used in conjunction with a parallel domain decomposition method for the solution of steady state fluid flow problems in 3d discrete fracture networks dfns fast analytical integration formulations were derived to account for singular nearly singular and non singular bem integrals several examples were investigated to verify the accuracy and the applicability of the technique the following conclusions can be drawn in the context of dfns with the proposed technique the limitations of previous bem based approaches with low order basis functions are overcome in particular better approximations near trace intersections and trace boundary intersections are provided optimal convergence rates are achieved by the second order discontinuous quadratic basis functions employed for the approximation of both pressure and velocity on fracture boundaries and traces in the numerical examples an excellent agreement between the proposed method and a highly refined reference fem solution is gained the use of the point source function results highly effective for the simulation of the sharp pressure gradients in the near field of a wellbore the technique can be effectively used in conjunction with ddm approaches thus allowing for a possible parallel implementation pursuing computations speedup in future works the convergence of the ddm algorithm could be improved namely reducing the iteration count to reach a given tolerance also the fracture wise heterogeneities could be supported by adopting some advanced bem methods such as dual reciprocity bem this is useful for applications where the flow field is governed by few fractures with local heterogeneities credit authorship contribution statement bin wang conceptualization methodology software validation investigation writing original draft visualization yin feng conceptualization methodology supervision funding acquisition writing review editing xu zhou conceptualization methodology visualization sandra pieraccini methodology writing review editing stefano scialò methodology investigation writing review editing corrado fidelibus methodology investigation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national key scientific research instrument research project of nsfc no 51827804 and board of regents of the state of louisiana leqsf 2017 20 rd a 20 the authors express their appreciation to dr boyun guo university of louisiana at lafayette and tianqi ma fudan university for their support s p and s s acknowledge the support of indam gncs and acknowledge that the present research has been partially supported by miur grant dipartimenti di eccellenza 2018 2022 e11g1800 0350001 and miur project prin 201744kljl 004 appendix a derivation of the bie the governing equation for the fluid flow in a fracture is obtained in eq 4 a 1 k b f μ 2 p t t δ t q t s s q s δ s a weighted residual statement for eq 4 is as follows a 2 k b f μ ω p w d ω ω p w d ω t t t q t γ w γ d γ s s q s w x s where w is a weighting function it can be re written as follows a 3 ω w p d ω ω p w d ω ω w p d ω μ k b f t t t q t x w d t s s q s w x s by formally using twice the divergence theorem once for ω p w d ω γ w p n d γ and then ω p w d ω γ p w n d γ one obtains a 4 γ w p n d γ γ p w n d γ ω w p d ω μ k b f t t t q t x w d t s s q s w x s which can be compactly rewritten as a 5 k b f μ γ w p n d γ γ p w n d γ ω w p d ω t t t q t x w d t s s q s w x s if the weighting function w is assumed equal to the fundamental solution of laplacian operator in 2d say w x y such that ω x x w x y d x δ y then eq a 5 becomes a 6 k b f μ γ w p n d γ γ p w n d γ p t t t q t x w d t s s q s w x s the bie is finally derived from eq a 6 using the concept of half circle or half spherical problem brebbia and dominguez 1994 as a 7 k b f μ c x i p x i γ p x w x i x n d γ k b f μ γ p n x w x i x d γ t t t q t x w x i x d t s s q s w x i x s where x i denotes the source point and c x i 1 if x i is located inside the domain or c x i 1 2 if x i is located on the boundary if the boundary is smooth brebbia and dominguez 1994 appendix b analytical integrations by assuming that a quadratic bem element is a straight line as shown in figs 2 3 the integrals in eq 13 can be analytically calculated and the results of the integration subsequently reported in section 2 3 by using the integral quantities are reported in what follows the integrals depend on the locations of the two extreme nodes x 1 1 x 2 1 and x 1 3 x 2 3 of the integration element and of the collocation node x 1 i x 2 i according to the method proposed by zhang and zhang 2003 and zhang and an 2008 the following geometrical quantities are defined first b 1 d 1 x 1 3 x 1 1 2 d 2 x 2 3 x 2 1 2 c 1 x 1 3 x 1 1 2 x 1 i c 2 x 2 3 x 2 1 2 x 2 i a d 1 2 d 2 2 b 2 d 1 c 1 d 2 c 2 c c 1 2 c 2 2 e c 1 d 2 c 2 d 1 j l 2 where j is the jacobian of the map from physical to reference space for a straight be element having length l using the constants defined above the following integrals are solved b 2 f n 1 1 ξ n a ξ 2 b ξ c d ξ a n 1 1 ξ n ln a ξ 2 b ξ c d ξ s n 1 1 ξ n ln l 2 ξ ξ i d ξ with the help of the symbolic mathematics tool sympy meurer et al 2017 the values of the above integrals can be found integrals f n for 4 a c b 2 0 b 3 f 0 2 4 a c b 2 arc tan 2 a b 4 a c b 2 arc tan 2 a b 4 a c b 2 for 4 a c b 2 0 b 4 f 0 2 b 2 a 2 b 2 a b 5 f 1 1 2 a ln a b c a b c b 2 a f 0 b 6 f 2 2 a c a f 0 b a f 1 b 7 f 3 c a f 1 b a f 2 b 8 f 4 2 3 a c a f 2 b a f 3 b 9 f 5 c a f 3 b a f 4 integrals a n b 10 a 0 ln a c 2 b 2 2 a f 2 b f 1 a 1 1 2 ln a b c a b c a f 3 1 2 f 2 a 2 1 3 ln a c 2 b 2 2 3 a f 4 1 3 b f 3 integrals s n b 11 s 0 ln l 2 1 ξ i ln l 2 1 ξ i ξ i ln 1 ξ i 1 ξ i 2 s 1 1 2 1 ξ i 2 ln 1 ξ i 1 ξ i ξ i s 2 1 ξ i 3 1 3 ln l 2 1 ξ i 1 9 1 ξ i 3 1 3 ln l 2 1 ξ i 1 9 2 ξ i s 1 ξ i 2 s 0 where ξ i is the local coordinate of the collocation node when it lies on the integration element the following integrals are also computed b 12 g n 1 1 ξ n a ξ 2 b ξ c 2 d ξ t n 1 1 ξ n ξ ξ i d ξ in formula integrals g n for 4 a c b 2 0 b 13 g 1 2 c b 4 a c b 2 a b c 2 c b 4 a c b 2 a b c b 4 a c b 2 f 0 for 4 a c b 2 0 b 14 g 1 8 a 3 b 2 a 3 8 a 3 b 2 a 3 2 3 1 2 a b 2 1 2 a b 2 b 15 g 2 1 a a b c 1 a a b c c a g 0 b 16 g 3 ln a b c a b c 3 a b g 2 2 a c b 2 g 1 b c g 0 2 a 2 where the quantity g 0 is as follows for 4 a c b 2 0 b 17 g 0 2 a b 4 a c b 2 a b c 2 a b 4 a c b 2 a b c 2 a 4 a c b 2 f 0 for 4 a c b 2 0 b 18 g 0 8 a 3 b 2 a 3 8 a 3 b 2 a 3 integrals t n b 19 t 0 ln 1 ξ i 1 ξ i t 1 ξ i ln 1 ξ i 1 ξ i 2 t 2 ξ i 2 ln 1 ξ i 1 ξ i 2 ξ i finally the quantities e 1 n e 2 n and i 1 n i 2 n n 0 1 2 are defined as follows b 20 e 1 n c 1 f n d 1 f n 1 b 21 e 2 n c 2 f n d 2 f n 1 b 22 i 1 n 2 e c 1 g n d 1 g i 1 d 2 f i b 23 i 2 n 2 e c 2 g i d 2 g i 1 d 1 f i 
199,modeling fluid flow in three dimensional 3d discrete fracture networks dfns is of relevance in many engineering applications such as shale oil gas production geothermal energy extraction nuclear waste disposal and co 2 sequestration a new boundary element method bem technique with discontinuous quadratic elements in conjunction with a parallel domain decomposition method ddm is presented for the simulation of the steady state fluid flow in dfns consisting of stochastically generated 3d planar fractures arbitrarily oriented and having differing hydraulic properties numerical examples characterized by dfns of increasing complexity are proposed to show the accuracy and the efficiency of the presented technique that provides good approximations of the fluid flow around domain interfaces where the solution usually displays sharp gradients like around intersections between traces the segments originated by the intersection between two fractures intersections between traces and fracture boundaries or intersections between fractures and wellbores the conjunction with a ddm approach is a promising strategy to speed up the computations by also exploiting the advantages of parallel computing techniques the technique is implemented in the code pydfn3d available at https github com binwang0213 pydfn3d keywords discrete fracture network dfn fractured rock hydrology boundary element method bem domain decomposition method ddm subsurface fluid flow 1 introduction modeling fluid flow in fractured rocks has been for a long of relevance in many engineering applications such as oil gas production geothermal energy extraction nuclear waste disposal and co 2 sequestration iding and ringrose 2010 hyman et al 2015 sun et al 2017 wang et al 2021 2018b zhang et al 2019 ye et al 2021 two predictive models are generally used the equivalent porous medium epm and the discrete fracture network dfn epm is considered when fractures are highly connected and the fracture network can be condensed into a porous medium with an associated permeability tensor lee et al 2001 sahimi 2011 the dfn model is an alternative to epm models for multi scale fracture networks where all the fractures are explicitly represented hyman et al 2015 fumagalli et al 2019 in the last decades significant efforts have been made to simulate the fluid flow in three dimensional 3d dfns and a variety of numerical approaches are now available they can be categorized into 1 mesh based approaches 2 equivalent pipe networks epns and 3 boundary element method bem approaches in the mesh based approaches a computational mesh is generated over the entire fracture system and the flow field is then obtained by resorting to the finite element method fem or the finite volume methods fvm both methods offer considerable precision but given the geometrical complexity of dfns of practical interest as e g the one depicted in fig 1a for the use of high computational power and memory capacity may be required hyman et al 2017 especially for high quality meshes and when a local grid refinement is applied to accurately resolve the sharp pressure gradients around fracture intersections traces or in the near field of a well fumagalli et al 2019 even when the number of fractures in a dfn is relatively small the number of mesh elements may be still considerable for conformity and quality constraints a suitable mesh for fem based approaches is shown in fig 1b for a dfn consisting of 932 fractures of various sizes and orientations the mesh counts nearly 1 61 million nodes and 3 84 million triangular elements to avoid the difficulties related to the mesh generation non conforming meshes independently built on each fracture are used and the minimization of a cost functional is applied to enforce the coupling of a domain decomposition method ddm resorting to error estimators to control the accuracy of the solution berrone et al 2019b 2018 2019a polygonal meshes are also suggested as an alternative to ease the generation of conforming meshes of complex dfns like in benedetto et al 2016 fumagalli and keilegavlen 2018 dassi et al 2021 and berrone et al 2021 where the virtual element method is used in antonietti et al 2016 where mimetic finite differences mfds and mixed finite elements are combined and in chave et al 2018 where hybrid high order hho methods are applied in the epn approach a dfn is morphed into a network of one dimensional conductors pipes endowed with an appropriate conductance value and connecting the mid points of the traces in a fracture cacas et al 1990 dershowitz and fidelibus 1999 xu et al 2018 huang et al 2021 epns feature great advantages in terms of computational efficiency and simplicity and as such are suitable to model large fracture networks however unique pipe connection patterns are difficult to generate and the equivalence gives rise to unavoidable errors a graph based method has been recently proposed to reduce the error of epns andresen et al 2013 hyman et al 2017 srinivasan et al 2020 for the application of bem approaches a one dimensional discretization of fracture edges and traces is required shapiro and andersson 1985 bem reveals being very accurate to resolve sharp pressure gradients near traces and wellbores even when few one dimensional elements are used semi analytic basis functions are introduced for the representation of the variables katsikadelis 2016 wang et al 2018a a suitable bem mesh is shown in fig 1c for the same 932 fractures dfn characterized by several boundary elements about two orders of magnitude less than the number of elements of the corresponding fem mesh the literature related to the use of bem for the prediction of the fluid flow in three dimensional dfns is still quite limited shapiro and andersson 1983 1985 and andersson and dverstorp 1987 were the first to apply this method by resorting to constant boundary elements for fracture edges and traces later lenti and fidelibus 2003 proposed an advanced bem technique in which quadratic elements are used for fracture edges and constant elements for internal traces the choice of constant elements is derived from the need to handle flux discontinuities at the intersections between two traces and between a trace and an edge in a fracture an optimization procedure is also applied to reduce memory usage when solving the global matrix the authors pointed out that the use of low order constant elements may lead to up to 22 flux mismatch errors yang et al 2016 and chen et al 2015 2016 2017 2018 recently proposed a bem method to model transient shale gas flow problems in a 2d infinite domain where bem with constant elements is used to solve matrix flow whereas finite differences are used to solve 1d fracture flow and the laplace transform is used to resolve the transient flow fast solutions for the flow regime are crucial for the industrial application of dfns in this respect none of the three approaches mentioned above prevails in offering such solutions however the scientific community may benefit from any contribution to improving one of the methods in this note a new bem technique is proposed for the prediction of the steady state single phase fluid flow in 3d dfns the novelty of the technique lies in the use of discontinuous quadratic basis functions for the boundary elements in combination with a domain decomposition approach uniform permeability and aperture values are ascribed to each fracture the fluid flow is governed by a two dimensional laplace equation with sinks sources located at the traces and at the intersections with wellbores assumed linear with the proposed technique accurate predictions can be provided also where large gradients are expected furthermore with ddm the way to the development of the technique for parallel solvers is paved thus allowing the reduction of the computational times the note is organized as follows in section 2 a detailed description of the adopted formulation and of the typical discretization associated with the proposed bem technique is reported in section 3 the ddm algorithm is briefly introduced and commented in section 4 the technique is tested on three verification examples and one synthetic field application example finally the possible extensions of the technique are discussed and concluding remarks are drawn accordingly the technique is implemented in the computer code pydfn3d 2 bem formulation in this section the bem formulation adopted for the steady state fluid flow in 3d dfns is described the derivation of the integral equations and of the final algebraic system is reported given the special basis functions of the discontinuous elements the following assumptions are considered 1 fluid dynamic properties are constant and independent of the pressure 2 the rock matrix is impervious such that the fluid flow occurs only inside fractures through fracture intersections and in wellbores 3 fracture aperture and permeability are uniforms within each fracture but different values are allowed among different fractures 4 the gravity effects are ignored implicitly assuming that the thickness of the fractured reservoir is small compared with the horizontal extension dverstorp and andersson 1989 smeraglia et al 2021 however the formulation can be easily extended to include such effects 5 pressure drops caused by friction in wellbores are neglected thus pressure inside is uniform 2 1 boundary integral equations with reference to a coordinate system x x 1 x 2 x 3 arbitrarily oriented in the 3d space a plane π is considered the unit vector normal to π is n π a local reference system x x 1 x 2 is chosen on π a domain ω π is introduced the boundary of ω is denoted by γ and n is the unit normal vector to γ outward pointing from ω a 3d fracture ω is defined such that ω x x γ b f t π γ π being b f x the fracture aperture a number much smaller than each dimension of ω in π for the above mentioned hypotheses fracture aperture b f x b f is uniform in π and the top and bottom walls of the fracture are planes parallel to π the fluid velocity vectors inside the fracture are all parallel to π given the fracture intrinsic permeability k unit m 2 uniform in ω the fluid dynamic viscosity μ unit kg m 1 s 1 independent of the pressure p and by neglecting as mentioned the gravity effects the governing equation for the steady state fluid flow in ω with a generic volumetric source term q x unit s 1 can be expressed as follows chen 2007 1 k μ 2 p x q x where p x is the fluid pressure unit kg m 1 s 2 the 3d problem can be reduced into a 2d problem by averaging eq 1 along n π by splitting the operator in an in plane component π and a normal component n π one has 2 k μ π 2 b f 2 b f 2 p x d n π b f 2 b f 2 q x d n π k b f μ π 2 p x q x in which p x 1 b f b f 2 b f 2 p x d n π is the averaged pressure along n π tech q x 1 b f b f 2 b f 2 q x d n π is a distributed areal source in what follows the fracture is dimensionally reduced to the planar domain coinciding with ω is used for π and p is assimilated to p the fractures of a dfn can mutually intercept therefore for the reduction to planes the intersections are segments traces also wellbores can cross the fractures and the intersections are point intersections wellbore intersections as the rock matrix is impervious there is no leakage from the blocks bounded by the fractures and therefore there are no areal sources rather the source term q x is given by linear sources and point sources chen 2007 from the traces and the wellbore intersections respectively traces in ω are labeled by t whereas t is the set of all the traces in the same domain wellbore intersections are denoted by s and s denotes the set of all the wellbore intersections in ω therefore the source term q x can be written as 3 q x t t δ t q t s s q s δ s being δ t q t the dirac delta function defined for any sufficiently regular function φ x as δ t q t φ t q t γ φ γ d γ where q t γ unit m 2 s 1 is the concentrated volumetric source per unit trace length on t whereas δ s for a wellbore s s is the dirac delta function located at the intersection point x s between the well and the mean fracture plane defined by δ s φ φ x s and q s unit m 3 s 1 is the volumetric point source at wellbore s by substituting eq 3 into eq 2 the governing equation for the fluid flow in a fracture is obtained as dong et al 2019 4 k b f μ 2 p t t δ t q t s s q s δ s after adding dirichlet and neumann boundary conditions a boundary integral equation bie corresponding to eq 4 is obtained in each collocation point x i ω 5 k b f μ c x i p x i γ p x w x i x n d γ k b f μ γ p n x w x i x d γ t t t q t x w x i x d t s s q s w x i x s where p n is the directional derivative of p along the normal and w w n are a weighing function and its normal derivative respectively the quantity c x i is a number depending on the location of x i namely if it is in the interior or on the boundary of the domain the definition of weighing functions is given in the following by neglecting the gravity the fluid velocity u unit m s 1 is equal to k μ p the corresponding bie for the components u j x i is 6 c x i u j x i b f k b f μ γ p x x j w x i x n d γ k b f μ γ p n x w x i x x j d γ t t t q t x w x i x x j d t s s q s w x i x s x j details on derivation and notation for the above bies are given in appendix a the weighting function w corresponds to the fundamental solution of the steady state fluid flow equation for a source point x i the function w and the derivative w n with respect to the normal n are brebbia and dominguez 1994 7 w x i x 1 2 π ln 1 r x i x w x i x n 1 2 π x i x n r 2 x i x where r x i x is the distance between x i and x 2 2 discretization with discontinuous quadratic elements lenti and fidelibus 2003 proposed a bem technique for dfns in which constant basis functions for the traces were used to deal with the flux discontinuities at trace trace and trace boundary intersections however due to the low accuracy of the constant element approximation the application of the technique implies errors increasing with the geometrical complexity of the dfn in this note an advancement is proposed by resorting to quadratic basis functions and discontinuous boundary elements bes intrinsically including flux discontinuities at the nodes as shown in fig 2 the discretization of each fracture is obtained by subdividing the boundary γ and the traces t t in straight bes collocation nodes are then placed on each element three collocation nodes are placed on each be to uniquely identify a quadratic polynomial on each element three quadratic basis functions n i x i 1 3 are consequently defined on each be by the condition n i x j δ i j for i j 1 3 being δ i j the kronecker delta the basis functions related to each be are then extended to zero outside the element additional nodes are also placed at wellbore intersections s s it is worth noting that collocation nodes never coincide with trace trace intersections and trace boundary intersections where the flux of p is discontinuous this is achieved by ensuring that trace intersection points and trace boundary intersection points coincide with be endpoints whereas collocation nodes are placed in the interior of each element as shown in fig 2 where collocation points are shown in blue green or red spots the set of elemental basis functions is denoted by n n i i 1 3 and the approximation of the pressure p and of its normal derivative are 8 p ξ i 1 3 p i n i ξ p n ξ i 1 3 p n i n i ξ 1 ξ 1 where ξ is a local normalized coordinate and p i p n i are nodal values functions in n are defined as follows 9 n 1 1 2 ξ α ξ α 1 n 2 1 ξ α 1 ξ α n 3 1 2 ξ α ξ α 1 with 0 α 1 and α is a collocation factor choosing 0 α 1 the first and the third node on each element are shifted inside it by a normalized quantity equal to α a value α 0 67 is used here as suggested in fratantonio and rencis 2000 boundaries and traces are approximated by using the same functions in n using x i 1 3 x i n i ξ α 1 i e the shape functions of eq 9 with α 1 a global numbering can be introduced for the collocation nodes and for the basis functions on all the bes and j tot is defined as the set of the indexes of all the nodes in the discretization the following subsets of j tot are also introduced j γ containing the indexes of the nodes belonging to the boundary γ j t containing the indexes of the nodes belonging to trace t t j s containing the index of the node matching x s and j t t t j t j s s s j s without loss of generality the indexes of these sets are numbered consecutively starting from j γ continuing with j t and ending with j s in what follows p γ x j j γ p j γ n j x p t x j j t p j t n j x p s x j j s p j s n j x denote the discrete solution on γ on the traces and on the wellbore intersections respectively similarly the discrete counterpart of p n on γ is defined by p n γ x j j γ p n j γ n j x whereas the source on trace t t is denoted by q t t x j j t q j t n j x finally p γ t s is the array collecting column wise the unknowns p j j j p n γ the array collecting the unknowns p n j γ j j γ whereas q t collects the coefficients q j t for j j t and t t and q s collects the coefficients q s s s the use of above definitions into the bie eq 5 yields 10 c γ p γ h γ γ p γ g γ γ p n γ g γ t q t g γ s q s c t p t h t γ p γ g t γ p n γ g t t q t g t s q s c s p s h s γ p γ g s γ p n γ g s t q t g s s q s where c γ t s are matrices with diagonal entries c i i equal to k b f μ when x i is in the interior of the domain ω and equal to k b f 2 μ when x i is on the boundary the g type and h type matrices are defined as follows 11 g i j γ γ k b f μ γ w x i x n j x d γ i j j γ g i j γ t t w x i x n j x d t i j γ j j t t t g i j γ s w x i x n j x i j γ j j s g i j t γ k b f μ γ w x i x n j x d γ i j t j j γ t t g i j t t t w x i x n j x d t i j j t t t g i j t s w x i x n j x i j t j j s g i j s γ k b f μ γ w x i x n j x d γ i j s j j γ g i j s t t w x i x n j x d t i j s j j t g i i s s 1 2 π ln r i i j s g i j s s w x i x i j j s h i j γ γ k b f μ γ w n x i x n j x d γ i j j γ h i j s γ k b f μ γ w n x i x n j x d γ i j s j j γ h i j t γ k b f μ γ w n x i x n j x d γ i j t j j γ t t note that in general g t γ g γ t g s t g t s g s γ g γ s the diagonal entries of g s s go to infinity as the distance r x i x approaches zero to handle the singularity and apply the constant pressure boundary condition at a wellbore nodes of unknown pressure p s x are placed on the boundaries at distance r s from the wellbore center where the node is located this handling of the wellbores was used and validated in a previous work wang et al 2018a eq 10 can be expressed in compact form as follows 12 c γ h γ γ 0 0 h t γ c t 0 h s γ 0 c s p γ p t p s g γ γ g γ t g γ s g t γ g t t g t s g s γ g s t g s s p n γ q t q s on each node of the boundary either pressure p or flux p n can be specified corresponding to a dirichlet or a neumann condition respectively thus the columns of the matrices in eq 12 can be re ordered to have the coefficients of all the unknowns on the left hand side and the coefficients of all the known terms on the right hand side however the prevailing condition is the insulation condition i e p n 0 once the solution of eq 12 is obtained pressure and velocity at any point x i can be calculated by applying eqs 5 6 in this work a simple wellbore model is considered that assumes a wellbore has a low production injection rate large pipe diameter and short segment length in the gravitational direction thus the pressure along the wellbore is assumed to be constant aziz 2001 for a wellbore with a flow rate q w intersecting multiple fracture planes in s 1 s 2 s n intersection points each one on a different fracture one additional condition need to be added to the system 12 given q s i the flow rate at intersection s i the equation i 1 n q s i q w is added which together with the assumption of constant pressure in the well is sufficient to close the problem 2 3 fast analytical bie integration after the bem discretization is performed given the singularity of the fundamental solution w there is the need to perform singular integrations nearly singular integrations and non singular integrations the singular and nearly singular integrations require special techniques to avoid the loss of accuracy related to standard numerical integration katsikadelis 2016 element subdivision analytical and semi analytical integration adaptive gaussian quadrature coordinate transformation and bie modification liu and nishimura 2006 tanaka et al 1994 gu et al 2016 can be adopted exact analytical integration formulations are also available fratantonio and rencis 2000 zhang and zhang 2003 zhang and an 2008 in this note an exact integration formulation for discontinuous quadratic elements is derived based on the method proposed in zhang and zhang 2003 the expressions for the integrals of g type and h type matrices for a boundary or a trace integration element in eq 11 are reported in what follows the quantities a l d l e l f l i l s l t l l 0 2 and a e are defined in appendix b 13 w x i x n j x d g 1 g 2 g 3 t w x i x n n j x d h 1 h 2 h 3 t when a collocation node does not pertain to the integration element the analytical integration formulations are 14 g 1 j 8 π a 2 α 2 a 1 α g 2 j 4 π a 0 a 2 α 2 g 3 j 8 π a 2 α 2 a 1 α h 1 e 4 π f 2 α 2 f 1 α h 2 e 2 π f 0 f 2 α 2 h 3 e 4 π f 2 α 2 f 1 α where j is the jacobian of the map from physical to reference element given by eq b 1 when a collocation node is instead on the integration element the formulations are 15 g 1 j 4 π s 2 α 2 s 1 α g 2 j 2 π s 0 s 2 α 2 g 3 j 4 π s 2 α 2 s 1 α h 1 h 2 h 3 0 for the discretized form of eq 6 the derivatives with respect to x k k 1 2 of the terms of the g type and h type matrices are required and are as follows 16 w x i x x k n j x d g 1 x k g 2 x k g 3 x k t x k w x i x n n j x d h 1 x k h 1 x k h 1 x k t again when a collocation node does not pertain to the integration element the formulations are 17 g 1 x k j 4 π e k 2 a 2 e k 1 a g 2 x k j 4 π e k 0 e k 2 a 2 g 3 x k j 4 π e k 2 a 2 e k 1 a h 1 x k 1 4 π i k 2 a 2 i k 1 a h 2 x k 1 4 π i k 0 i k 2 a 2 h 3 x k 1 4 π i k 2 a 2 i k 1 a finally when a collocation node is on the integration element the formulations are 18 g 1 x k j d k 4 π α t 2 α 2 t 1 α g 2 x k j d k 2 π α t 0 t 2 α 2 g 3 x k j d k 4 π α t 2 α 2 t 1 α h 1 x k h 2 x k h 3 x k 0 as previously mentioned all the formulations above are implemented and tested in the computer code pydfn3d 3 parallel domain decomposition method for large dfn problems in the context of bem the direct coupling i e the assembly of a large system of equations including the compatibility conditions at the interfaces among adjacent domains may too computationally demanding in previous works parallel ddms were developed aimed at iteratively solving a series of small dense linear subsystems rather than a coupled large unsymmetrical sparse linear system berrone et al 2013 2014 2015 wang et al 2019 in ddm conditions at interfaces are updated at each iteration until a solution that fits the specified compatibility condition at each interface is obtained here the main idea behind a ddm strategy suitable for the proposed approach is recalled referring to wang et al 2019 and to references therein for a deeper analysis on such methods a network of n f fractures ω k and n t traces is considered like the one in fig 4 being the set of all traces on fracture ω k denoted by t k k 1 n f and the set of all the traces in the network denoted by t each trace t t is shared by exactly two fractures ω i and ω j i j 1 n f such that a map τ between each trace and the corresponding pair of fracture indexes can be introduced τ t i j being t ω i ω j compatibility conditions at the traces are the continuity of pressure and the balance of normal fluxes which can be expressed as follows 19 q i t q j t 0 p i t p i t on t i j τ t where q i t p i t are the restriction of quantities q i and p i respectively to trace t on fracture ω i for t t i then the parallel domain decomposition algorithm here used for the resolution of large dfn problems is depicted in table 1 in which the superscript is used to indicate the iteration counter it is to remark that according to the solution strategy the compatibility conditions of eq 19 are satisfied up to a given tolerance tol this quantity being directly associated with the mass flux balance over all intersections the optimal relaxation parameter β k is defined as follows wang et al 2019 20 β k t t ε p i t k ε q i t k ε q j t k t t ε q i t k ε q j t k 2 i j τ t with error terms between two iterations ε p i t k p i t k 1 p i t k and ε q i t k q i t k k 1 q i t k clearly lower values of tol yield better flux balance at the traces at the cost of a larger number of iterations and computational time a tol value of 1 1 0 6 1 1 0 9 is used for the following computations 4 numerical examples in this section four numerical examples tests 1 4 for the validation of the proposed numerical technique are illustrated the first example refers to a problem with a known exact solution the second one is a test on a single fracture with multiple traces and one wellbore intersection examples 3 4 are aimed at demonstrating the robustness and the reliability of the technique when dealing with more complex dfn systems the dfn for the last example is stochastically generated by drawing position orientation size and permeability of each fracture from given probability density functions pdfs as practice for dfns using the open source dfn generator adfne alghalandis 2017 detailed information for all the examples below is available associated with the source code pydfn3d 4 1 test 1 unit square problem consider a darcy problem in a unit square domain ω x y 0 1 0 1 with μ 1 k 1 and b f 1 see fig 5 the following boundary conditions are prescribed see fig 5 21 p x y 15 cos 4 π x sinh 4 π y π cosh 4 π error indicators are considered in what follows to evaluate and compare the accuracy of the proposed technique defined as 22 e p l 1 n t p l exact p l h 2 n e u l 1 n u l exact u l h u l exact u l h n being n the number of sampling points uniformly distributed over the domain 1600 for test 1 and h is the mesh parameter equal to the maximum diameter for fem elements or to the maximum length for bem elements the results obtained with the proposed technique are compared with the results obtained by using the finite element method fem with continuous quadratic basis functions implemented in comsol comsol 2019 and by using constant bem elements and linear bem elements the four solutions are labeled in what follows bem quad fem quad bem const and bem linear respectively in fig 6 the convergence trends of the considered error indicators for the various methods are shown concerning the approximation of the pressure term all the solutions exhibit expected optimal convergence rates given the regularity of the solution and the polynomial accuracy of each method in particular it is observed that also the methods that use discontinuous basis functions are capable of providing extremely good approximations and the approximation level of quadratic bem is comparable to the one provided by quadratic fem concerning the approximation of the velocity term the proposed bem technique has a superior convergence rate with respect to the fem with basis functions of the same order also this result is expected since the derivative of p is directly computed with bem whereas for the fem solution it is the result of post processing from the fem solution that leads to a deterioration of the convergence results 4 2 test 2 unit square with 10 fractures the second proposed example deals with a unit square fracture domain with 10 traces and one wellbore intersection fig 7 in this example several regions with sharp solution gradients are included such as trace tips intersections between traces intersections between traces and fracture boundaries or intersections between fractures and wellbores the proposed quadratic bem technique is here validated comparing the bem quad results with the results derived by using constant bem elements bem const and with the solution given by fem with quadratic basis functions fem quad since the exact solution is not known in this case an additional solution is considered as a reference term obtained using the fem on a highly refined mesh fem quad ref fluid viscosity is set to 2 pa s and fracture permeability is 3 m 2 whereas given pressure values are imposed on the traces and on the wellbore intersection as detailed in fig 7 no flux conditions are instead set on the whole domain boundary in fig 8 the mesh setup for the three solutions are shown a the piece wise constant bem mesh counting 348 elements and 348 dofs b the quadratic bem mesh counting 116 quadratic elements and 346 dofs c the fem mesh having 6887 elements and 13982 dofs finally the reference fem mesh counts about 0 6 million elements and 1 2 million dofs the shaded map of the pressure values from the bem quad solution is reported in fig 7b as expected sharp gradients at trace tips and near the intersection with the well show two sampling lines l1 and l2 in fig 7a are used for the comparison l1 crosses several traces and intercepts the wellbore intersection l2 is aligned along with one of the traces results are presented in fig 9a b for pressure and velocity on l1 for x between 0 and 0 8 thus including trace tips and trace intersections it can be observed that the proposed bem technique with discontinuous quadratic basis functions is in excellent agreement with the reference solution both for the pressures and the velocities the bem const solution is instead less accurate near trace tips and trace intersections in fig 9c d the pressure and velocity solutions near the wellbore intersection for x between 0 8 and 1 are shown as expected the fem quad solution exhibits larger discrepancies from the reference solution for the velocities thanks to the exact point source fundamental function adopted in bem methods instead the two bem based solutions successfully capture the sharp pressure and velocity gradient in this region by using only one dof in fig 10a b the line plots for the four solutions along l2 are reported also in this case the superior performance of the proposed bem technique is apparent displaying an excellent agreement with the reference solution both the fem solution and the piece wise constant bem solution instead provide poorer approximations of the velocity solution near trace intersections and trace edge intersections as far as the computational cost is concerned the proposed bem technique with quadratic element also shows advantages over the conventional fem and bem solutions for a given level of accuracy it only uses 116 elements and 346 dofs to obtain an excellent match with the highly refined reference solution 4 3 test 3 4 fractures dfn with reference to fig 11 the steady state fluid flow in a small dfn consisting of four rectangular fractures is solved the results of the proposed technique are compared with the results of a fem model with quadratic elements and a refined mesh again solution fem quad ref three horizontal fractures ω 2 4 intersect a vertical fracture ω 1 the injection well s inj crosses ω 3 and the production well s pro crosses two fractures ω 2 4 fixed pressure values for the injection well and production well are 2 mpa p inj and 1 mpa p pro respectively the aperture values of the four fractures are 0 01 m wellbore radius is 0 001 m the fluid viscosity and fracture permeability are 0 001 pa s and 3 1 0 10 m 2 respectively in fig 12 the pressure and velocity solutions of the presented bem technique are compared with the reference fem solutions for the bem solution only 87 boundary elements resulted sufficient to reach a perfect match with the fem solution the proposed bem solution of this test case is obtained using the ddm algorithm 1 which allows solving iteratively small dense local linear systems instead of a large coupled unsymmetrical global system the stopping tolerance tol is set to 1 1 0 9 and the ddm algorithm converges in 112 iterations providing a solution with maximum flux mismatch over all traces and wells equal to 4 2 1 0 9 this error can be further minimized by reducing the stopping tolerance with however a minor impact on the global flux balance the computed flux at the injection well is 13 03 m 3 day in good agreement with the one provided by the reference fem solution with 0 2 million elements 13 05 m 3 day 0 2 difference also the plots reported in fig 12c d along a line passing close to the injection well the red line in fig 12a are in very good agreement 4 4 test 4 97 fractures dfn the last example refers to a 97 fractures dfn 4 faults and 93 fractures where there are 2 vertical wells drilled through four faults fig 13a the 93 fractures are randomly generated by the open source dfn modeling tool adfne alghalandis 2017 constant pressure values of 15 mpa and 10 mpa are fixed at the injection well and production well respectively no flow boundary conditions are prescribed on all fractures uniform aperture and permeability are assigned to all the fractures fracture aperture follows a log normal distribution cacas et al 1990 fig 13b with a mean value of 2 1 and a standard deviation value of 0 27 snow 1970 the cubic law k b f 2 12 is then applied to calculate the corresponding values of fracture permeability zimmerman and bodvarsson 1996 the faults have an aperture of 3 10 4 m and a permeability of 7 5 10 9 m 2 as a whole 2146 discontinuous quadratic elements 23 elements per fracture plane are used for this test in fig 14a the bem mesh for one of the faults of the dfn is shown in fig 14b the steady state pressure solution is reported it is obtained with the ddm algorithm 2132 iterations were required to reach a maximum flux balance error over all intersections and wells of 2 6 1 0 6 m 3 s the performance of the parallel ddm algorithm in pydfn3d could be easily accelerated through a parallel implementation in high performance computing hpc systems 5 conclusions a novel numerical technique based on the boundary element method bem with discontinuous quadratic elements is presented and used in conjunction with a parallel domain decomposition method for the solution of steady state fluid flow problems in 3d discrete fracture networks dfns fast analytical integration formulations were derived to account for singular nearly singular and non singular bem integrals several examples were investigated to verify the accuracy and the applicability of the technique the following conclusions can be drawn in the context of dfns with the proposed technique the limitations of previous bem based approaches with low order basis functions are overcome in particular better approximations near trace intersections and trace boundary intersections are provided optimal convergence rates are achieved by the second order discontinuous quadratic basis functions employed for the approximation of both pressure and velocity on fracture boundaries and traces in the numerical examples an excellent agreement between the proposed method and a highly refined reference fem solution is gained the use of the point source function results highly effective for the simulation of the sharp pressure gradients in the near field of a wellbore the technique can be effectively used in conjunction with ddm approaches thus allowing for a possible parallel implementation pursuing computations speedup in future works the convergence of the ddm algorithm could be improved namely reducing the iteration count to reach a given tolerance also the fracture wise heterogeneities could be supported by adopting some advanced bem methods such as dual reciprocity bem this is useful for applications where the flow field is governed by few fractures with local heterogeneities credit authorship contribution statement bin wang conceptualization methodology software validation investigation writing original draft visualization yin feng conceptualization methodology supervision funding acquisition writing review editing xu zhou conceptualization methodology visualization sandra pieraccini methodology writing review editing stefano scialò methodology investigation writing review editing corrado fidelibus methodology investigation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national key scientific research instrument research project of nsfc no 51827804 and board of regents of the state of louisiana leqsf 2017 20 rd a 20 the authors express their appreciation to dr boyun guo university of louisiana at lafayette and tianqi ma fudan university for their support s p and s s acknowledge the support of indam gncs and acknowledge that the present research has been partially supported by miur grant dipartimenti di eccellenza 2018 2022 e11g1800 0350001 and miur project prin 201744kljl 004 appendix a derivation of the bie the governing equation for the fluid flow in a fracture is obtained in eq 4 a 1 k b f μ 2 p t t δ t q t s s q s δ s a weighted residual statement for eq 4 is as follows a 2 k b f μ ω p w d ω ω p w d ω t t t q t γ w γ d γ s s q s w x s where w is a weighting function it can be re written as follows a 3 ω w p d ω ω p w d ω ω w p d ω μ k b f t t t q t x w d t s s q s w x s by formally using twice the divergence theorem once for ω p w d ω γ w p n d γ and then ω p w d ω γ p w n d γ one obtains a 4 γ w p n d γ γ p w n d γ ω w p d ω μ k b f t t t q t x w d t s s q s w x s which can be compactly rewritten as a 5 k b f μ γ w p n d γ γ p w n d γ ω w p d ω t t t q t x w d t s s q s w x s if the weighting function w is assumed equal to the fundamental solution of laplacian operator in 2d say w x y such that ω x x w x y d x δ y then eq a 5 becomes a 6 k b f μ γ w p n d γ γ p w n d γ p t t t q t x w d t s s q s w x s the bie is finally derived from eq a 6 using the concept of half circle or half spherical problem brebbia and dominguez 1994 as a 7 k b f μ c x i p x i γ p x w x i x n d γ k b f μ γ p n x w x i x d γ t t t q t x w x i x d t s s q s w x i x s where x i denotes the source point and c x i 1 if x i is located inside the domain or c x i 1 2 if x i is located on the boundary if the boundary is smooth brebbia and dominguez 1994 appendix b analytical integrations by assuming that a quadratic bem element is a straight line as shown in figs 2 3 the integrals in eq 13 can be analytically calculated and the results of the integration subsequently reported in section 2 3 by using the integral quantities are reported in what follows the integrals depend on the locations of the two extreme nodes x 1 1 x 2 1 and x 1 3 x 2 3 of the integration element and of the collocation node x 1 i x 2 i according to the method proposed by zhang and zhang 2003 and zhang and an 2008 the following geometrical quantities are defined first b 1 d 1 x 1 3 x 1 1 2 d 2 x 2 3 x 2 1 2 c 1 x 1 3 x 1 1 2 x 1 i c 2 x 2 3 x 2 1 2 x 2 i a d 1 2 d 2 2 b 2 d 1 c 1 d 2 c 2 c c 1 2 c 2 2 e c 1 d 2 c 2 d 1 j l 2 where j is the jacobian of the map from physical to reference space for a straight be element having length l using the constants defined above the following integrals are solved b 2 f n 1 1 ξ n a ξ 2 b ξ c d ξ a n 1 1 ξ n ln a ξ 2 b ξ c d ξ s n 1 1 ξ n ln l 2 ξ ξ i d ξ with the help of the symbolic mathematics tool sympy meurer et al 2017 the values of the above integrals can be found integrals f n for 4 a c b 2 0 b 3 f 0 2 4 a c b 2 arc tan 2 a b 4 a c b 2 arc tan 2 a b 4 a c b 2 for 4 a c b 2 0 b 4 f 0 2 b 2 a 2 b 2 a b 5 f 1 1 2 a ln a b c a b c b 2 a f 0 b 6 f 2 2 a c a f 0 b a f 1 b 7 f 3 c a f 1 b a f 2 b 8 f 4 2 3 a c a f 2 b a f 3 b 9 f 5 c a f 3 b a f 4 integrals a n b 10 a 0 ln a c 2 b 2 2 a f 2 b f 1 a 1 1 2 ln a b c a b c a f 3 1 2 f 2 a 2 1 3 ln a c 2 b 2 2 3 a f 4 1 3 b f 3 integrals s n b 11 s 0 ln l 2 1 ξ i ln l 2 1 ξ i ξ i ln 1 ξ i 1 ξ i 2 s 1 1 2 1 ξ i 2 ln 1 ξ i 1 ξ i ξ i s 2 1 ξ i 3 1 3 ln l 2 1 ξ i 1 9 1 ξ i 3 1 3 ln l 2 1 ξ i 1 9 2 ξ i s 1 ξ i 2 s 0 where ξ i is the local coordinate of the collocation node when it lies on the integration element the following integrals are also computed b 12 g n 1 1 ξ n a ξ 2 b ξ c 2 d ξ t n 1 1 ξ n ξ ξ i d ξ in formula integrals g n for 4 a c b 2 0 b 13 g 1 2 c b 4 a c b 2 a b c 2 c b 4 a c b 2 a b c b 4 a c b 2 f 0 for 4 a c b 2 0 b 14 g 1 8 a 3 b 2 a 3 8 a 3 b 2 a 3 2 3 1 2 a b 2 1 2 a b 2 b 15 g 2 1 a a b c 1 a a b c c a g 0 b 16 g 3 ln a b c a b c 3 a b g 2 2 a c b 2 g 1 b c g 0 2 a 2 where the quantity g 0 is as follows for 4 a c b 2 0 b 17 g 0 2 a b 4 a c b 2 a b c 2 a b 4 a c b 2 a b c 2 a 4 a c b 2 f 0 for 4 a c b 2 0 b 18 g 0 8 a 3 b 2 a 3 8 a 3 b 2 a 3 integrals t n b 19 t 0 ln 1 ξ i 1 ξ i t 1 ξ i ln 1 ξ i 1 ξ i 2 t 2 ξ i 2 ln 1 ξ i 1 ξ i 2 ξ i finally the quantities e 1 n e 2 n and i 1 n i 2 n n 0 1 2 are defined as follows b 20 e 1 n c 1 f n d 1 f n 1 b 21 e 2 n c 2 f n d 2 f n 1 b 22 i 1 n 2 e c 1 g n d 1 g i 1 d 2 f i b 23 i 2 n 2 e c 2 g i d 2 g i 1 d 1 f i 
