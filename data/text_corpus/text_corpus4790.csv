index,text
23950,we investigate the impact of wave dependent stress on surge modelling from case studies in the north sea using a global ocean model forced with a wave atmosphere coupled model we select the storms with the largest surges and a range of sea state development from young to mature seas the modelled surges are compared to tide gauges and altimeter data the ocean model is able to accurately predict storm surges in coastal areas the consistency of the model outputs the altimeter and the tide gauge data confirms the accuracy of altimeters for storm surge measurements we show that using a wave dependent rather than a wind dependent only stress formulation gives more accurate surge simulations when the sea state is young and the sea rougher taking into account the waves in the stress formulation has a significant impact on the surges up to 20 cm keywords air sea exchanges storm surges wind stress drag coefficient wind wave coupling north sea 1 introduction storm surges are generated by atmospheric pressure gradient and wind stress in coastal areas the wind stress contribution is more effective due to shallow waters water pileup along the coast and resonant effects moon et al 2009 bertin et al 2012 in addition in nearshore areas the radiation stress which is the momentum flux carried by the waves generates nearshore currents and wave setup i e additional surge when the waves dissipate bunya et al 2010 kim et al 2010 brown et al 2010 idier et al 2012 lee et al 2013 bertin et al 2015 thuy et al 2017 choi et al 2018 here we focus on the impact of the wind stress on the surges this study tackles the question which impact has the sea state on the wind stress to answer this we determine if simulated surges are closer to observations when wind stress parameterization is wave dependent the wind stress is usually parameterized using bulk formulae that express it as a function of the wind speed at a given height generally 10 m above sea surface and of a drag coefficient 1 τ ρ a u 2 ρ a c d u 10 2 where u c d and u 10 are the friction velocity the drag coefficient and the wind speed at 10 m above the surface respectively most formulations of the drag depend only on the wind speed e g moon et al 2007 edson et al 2013 peng and li 2015 whereas others include sea state parameters e g janssen 1991 moon et al 2009 despite years of research the impact of sea state on drag remains inconclusive recently edson et al 2013 concluded that the coare 3 5 wind speed dependent formulation matches the observations well without any wave information whereas earlier works insisted on the importance of wave enhanced drag for young waves mastenbroek et al 1993 these contrasted results can be explained by the complexity of the problem there are major issues that prevent a simple answer to the question what is the wave impact on the wind stress if any 1 wind stress measurements and drag estimations are difficult to perform particularly at high winds uncertainties in observations could explain the variability of the drag at a given wind speed estimations from several recent field experiments based on direct in situ measurements e g black et al 2007 edson et al 2013 or indirect ones powell et al 2003 jarosz et al 2007 holthuijsen et al 2012 as well as laboratory tank measurements e g donelan et al 2004 takagaki et al 2012 show differences in the drag up to a factor 2 at 30 m s pineau guillou et al 2018 it is not known if these differences are due to measurement uncertainties or various environmental conditions fetch turning wind bathymetry etc 2 wind stress measurements are scarce they generally come from short dedicated campaigns with moderate winds often being lower than 20 m s there is clearly a lack of measurements at very high winds 3 all the variables used to explain the drag variability are interdependent consequently good correlations between the drag and variables such as wave age may mainly be due to self correlation andreas 2009 this contributes to a lack of confidence in these relationships in a part of the scientific community the objective of this article is to investigate the impact of a wave dependent stress on the surges here we focus on the north sea the main idea is to simulate storms with various sea states i e young and old to estimate the sensitivity of surges to the sea state development i e the wave age selected storms are simulated using wind dependent only and wave dependent stress parameterizations the simulated surges are evaluated against observations i e tide gauges and altimeters the first part of the article describes the methods to compute the surges in models and observations in the following part we describe the case studies namely the storms selection the numerical setup and the validation data then we analyse the results to estimate the impact of the wave dependent stress on the storm surges finally we discuss different points among them is the difficulty of comparing the model with observations as various processes contribute to the surges particularly in coastal areas 2 surges in models and observations the surges are the differences between the water level and the tide prediction here we describe the methods to compute the surges from the model and observations 2 1 modelled surges surges are simulated with an ocean model forced with output from an atmosphere model the ocean model resolves the classical saint venant shallow water continuity and momentum equations in barotropic mode formulated similarly as in bertin et al 2012 2 η t h η u d z 0 3 d u d t f k u α g ψ ˆ p a ρ g η τ s τ b ρ η h where η is the surface elevation u is the horizontal velocity h is the bathymetry f is the coriolis parameter α is the earth elasticity factor g is the mean gravitational acceleration ψ ˆ is the earth tidal potential p a is the sea level atmospheric pressure ρ is the water density τ s is the surface stress and τ b is the bottom stress the bottom stress is expressed as 4 τ b c h u u where h is the mean local depth and c is a dimensionless friction coefficient set as 2 5 10 3 lyard et al 2006 the water level variations are due to tide through the tidal potential ψ ˆ in eq 3 wind through the wind stress τ s in eq 3 and atmospheric pressure through the pressure gradient p a in eq 3 note that the wind stress effect is modulated with the bottom stress effect τ b in eq 3 whose influence is not studied here we investigated the relative contribution of the wind stress and pressure gradient terms in eq 3 as τ s ρ η h and p a ρ respectively in this case the wind stress contribution increases in shallow waters as it is divided by the water height comparison of the two terms shows that the wind stress term is largely dominant in the north sea that is more than 90 of the sum of the two terms pineau guillou 2018 this suggests that the currents are mainly driven by the wind whereas the effect of the atmospheric pressure is negligible in the following the surges are computed from simulations without tide no tidal potential ψ ˆ in eq 3 and with atmospheric forcing only as a consequence the modelled surges correspond only to atmospheric surges the surges due to wave breaking i e wave setup are not modelled here see discussion in section 5 to compute the surges we used the tugo shallow water global ocean model developed by legos lyard et al 2006 this model in barotropic mode resolves the classical shallow water continuity and momentum equations eqs 2 and 3 following lynch and gray 1979 the model solves the generalized wave equation 5 c e t c 0 c e 0 where c e 0 is the continuity equation eq 2 and c 0 is a relaxation coefficient towards the continuity equation this gives more explicitly 6 2 h t 2 h u t c 0 h t h u 0 where h η h is the water height and h u t is formally replaced by using the momentum equations this allows to make the elevation solver implicit hence relaxing the cfl condition for time step actually tugo can use different solvers and discretization for the dynamical equations but the most efficient so far in tides or storm surges modelling is the lgp1xlgp1 elevations and currents discretized at element vertices generalized wave equations solver hence being used in our study tugo is a reference model it allowed the development of the tidal model fes2014 carrère et al 2015 a worldwide reference model for tides which is widely used in the scientific community it also produces dynamic atmospheric corrections to correct altimeter data from atmospheric effects carrère and lyard 2003 this correction is officially used by cnes and nasa for altimeter data processing 2 2 observed surges tide gauges measure the sea level the surges are computed as the differences between the observed and predicted sea level simon 2007 7 s u r g e t i d e g a u g e o b s e r v e d s e a l e v e l t i d e p r e d i c t i o n we used 101 tide gauges in the north sea fig 1 obtained thanks to copernicus marine environment monitoring service cmems data temporal resolution is generally of 10 min 88 of the tide gauges and sometimes 1 h 12 of the tide gauges we used the tidal toolbox developed by legos allain 2013 to process the data and estimate the surges the method is the following 1 a harmonic analysis is performed on the sea level observations to estimate the harmonic constants amplitude and phase of the tidal constituents 2 from these harmonic constants a prediction is computed over the same period as the observations 3 the surges are the differences between the observed and predicted sea levels eq 7 note that this residual also includes the error associated with the prediction in addition to tide gauges radar altimeters onboard satellites also measure the sea level that is the instantaneous sea surface height ssh above the ellipsoid data are processed by providers to compute the mean sea surface mss and the sea level anomaly sla the mss corresponds to the mean of several years of ssh eventually using several satellites the sla is the difference between the instantaneous ssh and the mss many geophysical and environmental corrections are made to estimate sla and one among them is dynamic atmospheric correction dac antony et al 2014 this correction corresponds to the ocean response to atmospheric forcing atmospheric pressure and winds and generally comes from an ocean model elevations for high frequency part e g tugo carrère and lyard 2003 and inverted barometer law for low frequency part e g using ecmwf atmospheric pressure products to be consistent with the model and the tide gauges the surges from altimeters are computed as the summation of the sla and the dac 8 s u r g e a l t i m e t e r s l a d a c to compute the altimetric surges we used the sla and dac from the jason 2 1 hz x track coastal product birol et al 2016 developed by the center of topography of the ocean and hydrosphere ctoh legos toulouse along track data have a temporal resolution of 1 s which corresponds to a spatial interval of about 6 7 km between points x track is a post processing software which increases the ssh information derived from satellite altimetry in the coastal ocean areas retrieved information along tracks come closer to land up to 5 km against 10 km with the standard aviso archiving validation and interpretation of satellite oceanographic data product 3 case studies and modelling here we describe the storms selection the numerical setup and the validation data 3 1 storms selection the storms were selected according to the following criteria 1 availability of good quality data at tide gauges as well as along tracks 2 occurrence of maximum surges in the north sea and 3 presence of various sea states the objective was to select two storms with young sea states and two with old sea states a common way to characterize the sea state is to consider the wave age following the expression ξ c p u 10 where c p is the phase velocity at the peak of the wave spectrum and u 10 is the wind speed at 10 m when the wind has just started blowing the waves are short period steep and short crested at this stage the wind speed is higher than the wave phase velocity and the waves are growing it is commonly considered that the value of 1 2 separates the young and the old sea in the north sea during the storms the wave age is generally around 0 8 pineau guillou 2018 in the following sea state is considered as young when the wave age is close to 0 8 and old when the wave age is greater than 1 2 swell note that the sea state quickly evolves spatially and temporally during the storm the reference to a young or old sea state for each storm characterizes the sea state along the altimeter track during a short period of several minutes and is not representative of the whole storm to select the storms we analysedtide gauges and jason 2 altimeter data fig 1 the tide gauge database consists of 101 tide gauges in the north sea from january 2012 to october 2017 date of the extraction the duration of observations depends on sites and ranges from less than 3 years to more than 5 years surges at tide gauges were computed following eq 7 the altimeter database covers 8 years 2008 2015 of jason 2 data along tracks surges along tracks were computed following eq 8 analysis of data led to select following storms ex gonzalo friedhelm felix and gunter table 1 the tracks of the 4 selected storms are shown in fig 2 and the surface winds during the storms are shown in fig 3 these storms were moving at a speed of around 15 to 20 km h in december 2011 friedhelm crossed the north east atlantic its track is the southernmost one in the north sea the winds are very strong up to 27 m s and the sea state is old with wave age larger than 1 2 along the altimeter track in october 2014 ex gonzalo reached the northern part of the british islands this storm corresponds to the remnants of category 4 atlantic hurricane gonzalo it is the strongest storm in terms of surges but not in terms of winds no more than 22 7 m s the sea state is young with wave age around 0 9 along the altimeter track one of the characteristics of this storm is that the strong winds moved from the west to the east as a nearly north south front of around 1000 km long fig 3 b this explains the discontinuity in the mean sea level pressure observed in the ex gonzalo track over the north sea as the minimum moves along this front red curve in fig 2 in january 2015 felix and gunter crossed the north atlantic in fact three storms succeeded first elon then felix which crossed the north sea on the 10th of january and finally gunter on the 12th of january 2015 the winds were strong and reached 26 8 m s table 1 for felix the sea state was young with wave age close to 0 8 along the altimeter track whereas for gunter it was old with wave age larger than 1 2 the old sea for gunter can be easily explained by the succession of three storms in a short period few days the gunter track was farther north than the felix one which may explain why the storm had less impact in terms of surges 3 2 numerical setup the ocean model is forced with output from a coupled wave atmosphere model fig 4 here we describe the ocean model the atmosphere model and the experiments 3 2 1 ocean model we used the default configuration of the tugo shallow water global ocean model lyard et al 2006 with fes2014 spatial grid fig 5 the unstructured space discretization allows the increasing of the resolution in shallow waters as well as along strong topographic gradient areas in the north sea the resolution varies from 10 15 km offshore to 4 km along the french and english coasts and 2 km along the north of the norwegian coasts note that the resolution is not refined in the southeast of the north sea due to a lack of detailed bathymetric information in this area not enough available data the ocean model is forced with 1 10 m wind or 2 surface wind stress from the coupled wave atmosphere model fig 4 in the first case 1 the wind stress is computed from the ocean model bulk formula the drag coefficient is expressed following hellerman and rosenstein 1983 which is a wind only dependent formulation 9 1 0 3 c d 0 934 0 788 1 0 1 u 10 0 868 1 0 1 δ t 0 616 1 0 3 u 10 2 0 12 1 0 2 δ t 2 0 214 1 0 2 u 10 δ t where u 10 is the wind at 10 m and δ t is the air sea temperature difference to take into account the stability effect in the second case 2 the wind stress comes directly from the ecmwf coupled wave atmosphere model fig 6 shows the tugo drag for δ t 0 and the ecmwf drag computed over the north sea during two days for each storm friedhelm ex gonzalo felix and gunter for winds lower than 15 m s the tugo drag is quite similar to the ecmwf one but with no variability for winds stronger than 15 m s the tugo drag is generally lower than the ecmwf one for a given wind speed the variability of the ecmwf drag depends on the wave age fig 7 even if this dependency is not explicit see section 3 2 2 note that the tugo drag is quite close to the ecmwf drag for old sea state but lower for young sea state 3 2 2 atmosphere model we used the ecmwf coupled wave atmosphere model ifs integrated forecasting system to generate atmospheric forcing fig 4 we conducted the simulations without data assimilation the ifs cy41r1 cycle ecmwf 2015a has a spatial resolution of around 16 km tl1279 and 137 vertical levels it has been coupled with the spectral wave model ecwam ecmwf wave model ecmwf 2015b since 1998 ecwam uses a coarser horizontal resolution than ifs at around 28 km with 36 directions and 36 frequencies exponentially spaced with starting frequency 0 035 hz and an increment of 1 1 in ecwam the source terms are s s i n s n l s d s s b o t where s i n represents the wind input s n l represents the nonlinear wave wave interactions s d s represents the dissipation due to whitecapping and s b o t represents the bottom friction the parameterizations of these source terms are discussed in ecmwf 2015b the wind stress is represented by classical bulk formulae eq 1 we assume that the wind stress is in the wind direction that is the effects of wind wave misalignment are not accounted here the drag coefficient is expressed following janssen 1991 which is a wave dependent formulation in neutral conditions the drag coefficient can be expressed as 10 c d κ 2 log 10 z 0 2 where z 0 is the roughness length and κ is von kármán s constant 0 4 the roughness length is expressed as 11 z 0 0 11 ν u α u 2 g where ν is the kinematic viscosity and α is the charnock s parameter charnock 1955 note that the ecwam wave model uses eqs 10 and 11 but eq 11 is reduced to the second term that is roughness associated with an overall form drag of the wave field the first term that is roughness associated to the viscous properties of the flow is computed in the ifs atmosphere model the modification of the roughness length z 0 impacts the drag coefficient and the wind stress eqs 10 and 1 then the sea level eqs 2 and 3 but also the wind profile indeed when roughness increases friction also increases and this slows down the wind pineau guillou et al 2018 janssen 1991 parameterized the quasi linear wave growth effect as an effective larger charnock parameter expressed as a function of the wave induced stress τ w 12 α α 0 1 τ w τ with α 0 0 006 the wave induced stress τ w is the momentum flux transferred from the atmosphere to the waves it can be related to the wind wave growth parameter β and the directional wave spectrum e f θ 13 τ w ρ g 0 o 2 π β f θ e f θ c f d f d θ where θ is the direction f is the relative wave frequency and c f is the phase speed which is a function of frequency janssen 2004 the wave growth parameter is expressed as β β m κ 2 μ ln 4 μ μ 1 where β m is a constant 1 2 and μ is the dimensionless critical height ecmwf 2015b in eq 13 the frequency f is integrated from 0 to a high frequency limit f c prognostic part of the wave spectrum using the discretized spectrum beyond f c diagnostic part of the wave spectrum the shape of the spectrum is assumed and the resulting integral can be evaluated using a simple integration scheme ecmwf 2015b it is assumed that the diagnostic part of the wave spectrum is given as e f θ e f c θ f f c 5 for f f c the high frequency limit f c is set as f c min f m a x 2 5 f w i n d s e a where f m a x is the maximum discretized frequency and f w i n d s e a is the mean frequency of the modelled wind sea ecmwf 2015b the ocean model is forced by the atmospheric stress τ however a part of the atmospheric stress is going into the waves the momentum flux going into the ocean τ o c is the sum of two contributions fig 8 the part of the atmospheric flux which was not used to generate the waves τ o τ τ w and the momentum flux transferred from the waves to the ocean by dissipation τ d i s s ecmwf 2015b 14 τ o c τ o τ d i s s τ τ w τ d i s s a more correct approach would be to force the ocean model with τ o c rather than τ the normalized stress going into the ocean corresponds to the ratio τ o c τ output parameter of the ecmwf operational version but not available in the research version of ifs we used the normalized stress is lower than 1 when the waves are growing and greater than 1 when they are dissipating it is globally close to 1 but can reach values as high as 1 5 under extreme conditions such as with a passing front janssen 2012 in the ex gonzalo case study it could locally be greater than 2 when the front was passing pineau guillou 2018 the strong gradients suggest a potential impact on the ocean model curcic 2015 also investigated the ratio between the oceanic and atmospheric stress in tropical cyclones he found typical values between 0 85 and 1 depending on the wave state 3 2 3 experiments we simulated the 4 selected storms ex gonzalo friedhelm felix and gunter with two stress parameterizations that is the wind dependent and the wave dependent parametrizations note that felix and gunter are in the same simulation as they follow each other for each storm the model was initialized for at least 15 days with winds and atmospheric pressure coming from the ecmwf operational 1 h forecasts i e combining hourly operational forecasts computed twice a day at 00 00 and at 12 00 once initialized each storm simulation lasted 5 days and was forced by 1 the 10 m wind or 2 directly by the wind stress and the atmospheric pressure with a 1 h temporal resolution in this case the wind stress comes from the atmosphere model when forced by the 10 m wind 1 the drag is a wind dependent formulation computed by tugo eq 9 with δ t 0 hellerman and rosenstein 1983 whereas when forced by the wind stress 2 the drag is a wave dependent formulation which has seen the waves through ifs wam coupling eqs 10 13 janssen 1991 3 3 validation data among the 101 tide gauges 22 tide gauges were selected for comparison with the model fig 1 the following were the criteria 1 the tide gauges must have data available during the storms 2 the tide gauges must open up to the ocean rather than at the end of a bay 3 a maximum number of the tide gauges must be offshore where processes are different from harbours and 4 a maximum number of tide gauges must be located along the tracks the jason 2 tracks with the maximum surges are tracks 170 for friedhelm 61 for ex gonzalo 94 for felix and 170 for gunter fig 1 all the data tide gauges and jason 2 were processed as described in section 2 4 results to investigate the impact of the waves we compared the surges with wind dependent hellerman and rosenstein 1983 and wave dependent janssen 1991 parameterization during four storms two with a young sea state ex gonzalo and felix and two with an old sea state friedhelm and gunter note that felix and gunter followed each other and correspond to the same simulation we compared the simulated surges with observations namely tide gauges and jason 2 altimetric data fig 9 shows the surge comparison between the model and 3 tide gauges representative of the whole for ex gonzalo felix and gunter no data were available for friedhelm in 2011 as the cmems tide gauge database starts only in 2012 table 2 summarizes the corresponding errors between the model and the tides gauges bias root mean square error rmse and peak error defined as the difference between the maximum observed and modelled surge finally figs 10 and 11 show the surge comparison between the model and jason 2 altimetric data for ex gonzalo and friedhelm note that the grey shaded area corresponds to deep waters where the wind stress effect is lower analysis of ex gonzalo young sea and friedhelm old sea gives the following results the first result is that globally the model matches very well with the observations for the wave dependent parameterization on average the bias between the model and all the tide gauges is close to zero the rmse is 0 12 m and the peak error is 0 09 m table 2 the errors between the model and jason 2 are lower than the errors between the model and the tide gauges the bias is close to zero and the rmse is 0 08 m whereas the surge ranges up to 1 40 m fig 10 note also the very good agreement between the model jason 2 data and the tide gauge situated along the track d151tg see fig 1 for the tide gauges location unfortunately the tide gauge cromer located on the northeast coast of england at the end of the track did not record data during this storm for friedhelm the model also matches the altimeter quite well but not as well as for ex gonzalo for the wave dependent parameterization the bias and rmse reach 0 12 m and 0 08 m respectively fig 11 the differences could be due to uncertainties in altimeter corrections such as geophysical corrections for example tide however note that the agreement between the model and the altimeter is very good in shallow waters where the wind stress effect is the most significant fig 11 b to conclude the errors between the model and the observations tide gauges and altimeter data for these two storms are small enough to confirm the capability of a global model to accurately predict storm surges even in the coastal areas when its spatial resolution is fine enough to catch the storm size these results also confirm the capability of altimeters to accurately measure surges antony et al 2014 the second result is that the wave dependent parameterization yields higher surges only when the sea state is young ex gonzalo figs 9 a and 10 b otherwise the surges are similar regardless of the parameterization friedhlem fig 11 b physically this is not surprising as old sea corresponds to a situation where waves are no longer rapidly growing resulting in sharp reduction of the momentum flux from the atmosphere to the waves when the sea state is old the drag coefficients from the two parameterizations are close to each other fig 7 and the surges are then similar however in the presence of young and steep waves the drag increases with janssen s parameterization due to higher values of the charnock parameter fig 12 c and d this yields higher drag than the wind dependent formulation and later higher wind stress fig 12 e and f and higher surges the differences between these two parameterizations correspond to the effect of the waves on the surges this difference reaches 25 cm at lowestoft fig 9 a and 20 cm along jason 2 track fig 10 b we note the very good agreement in terms of significant wave height swh between the ecmwf coupled wave atmosphere model and jason 2 altimeter piolle et al 2019 for ex gonzalo figs 12 b and 13 b and for gunter fig 13 d the very strong swh gradient along the track for ex gonzalo corresponds to the passage of the front fig 13 b the third result is that the wave dependent parameterization is closer to the observations than the wind dependent one these results are consistent with those previously obtained by mastenbroek et al 1993 nicolle et al 2009 and bertin et al 2015 along the jason 2 track the rmse is reduced from 0 13 m to 0 08 m fig 10 b on average in the 21 tide gauges the peak error is reduced from 0 21 m to 0 09 m table 2 however in some tide gauges the surges are still underestimated the tides gauges can be separated into three groups group 1 a first group of 11 tide gauges in blue in fig 1 where the surges with the wave dependent stress match well with observations such as lowestoft and europlatformtg in fig 9 a this corresponds to the 4 offshore tide gauges f3platformtg d151tg europlatformtg vlaktevdraantg as well as 7 other tide gauges onshore group 2 a second group of 5 tide gauges in green in fig 1 where the surges with the wave dependent stress are still underestimated such as whitby in fig 9 a group 3 a third group of 5 tide gauges in black in fig 1 where the effect of the parameterization is not significant this corresponds to tide gauges located in the northern part of the north sea where surges are smaller than 0 50 m in this part the bathymetry ranges from 50 to 200 m and the effect of wind stress is smaller than in the southern part with shallow waters that may explain the non significant differences between the two parameterizations note that in the second group where surges are still underestimated there are no tide gauges offshore this underestimation is probably partly due to processes taking place in the very nearshore and not modelled by tugo e g wave setup see the discussion in section 5 this could suggest that comparisons with the altimeter are better as the tracks offshore are not contaminated by coastal processes such as wave setup another reason that could explain this negative bias is the lack of spatial resolution near the coast only 2 km experiments show that atmospheric surges may increase from 0 1 to 0 3 between 10 m isobath and the shoreline personal communication from x bertin here the spatial resolution is not fine enough to correctly represent this increase after the analysis of ex gonzalo young sea and friedhelm old sea analysis of felix young sea and gunter old sea give partly similar results 1 the model still matches quite well with the tide gauges bias rmse and peak error are respectively 0 cm 19 cm and 10 cm for the wave dependent parameterization see table 2 2 the wave dependent parameterization still yields to higher surges when the sea state is young the impact of the waves on the surges reaches around 11 cm at europlatform and around 20 cm along jason 2 track not shown 3 we find also that the wave dependent parameterization is closer to the tide gauge observations reducing on average the peak error from 0 14 m to 0 10 m however comparison between the model and the altimeter is not as good as for ex gonzalo and friedhelm not shown and it is difficult to conclude which parameterization is the most appropriate this suggests that the number of case studies should be increased to give more confidence in our conclusions 5 discussion here we discuss the influence of the wind direction the processes contributing to the surges and the impact on the altimetric corrections 5 1 impact of the wind direction analysis of tide gauge and altimeter data revealed that ex gonzalo was the storm with the highest surge whereas it was the one with the weakest winds only 23 m s in the north sea against 27 m s for friedhelm see table 1 this is mainly due to the wind direction fig 14 shows wind roses in the middle of the north sea 4 e 56 n during the 5 day simulations of the storms for ex gonzalo fig 14 a strong winds were mainly from the northwest direction pushing the waters along the southern coast of the north sea whereas for other storms strong winds came mainly from the west see also fig 3 this is probably not the only explanation for ex gonzalo high surges the track of this storm is the southernmost one the storm crosses the south of the north sea and the shallow waters may enhance the wind stress contribution 5 2 processes contributing to the surges even if the wave dependent parameterization yields higher surges the modelled surges are still underestimated compared with some tide gauges 5 over 21 for ex gonzalo storm the comparison between modelled and observed surges is complicated by the different processes that contribute to the surge and that are not always modelled see table 3 generally the dominant effect is the atmospheric forcing mean sea level pressure and winds this contribution is commonly of the order of 50 cm but can exceed 1 m in severe storms and hurricanes as in the case of xynthia in february 2010 at la rochelle pineau guillou et al 2012 bertin et al 2012 in case of progression in very shallow waters as for example in the south of the north sea the surges can reach up to 2 or 3 m this atmospheric contribution is taken into account in the ocean model through the atmospheric forcing another important contribution to the total surge which is here not taken into account is the wave setup that is the surge due to wave dissipation mainly by wave breaking in the nearshore areas bunya et al 2010 kim et al 2010 brown et al 2010 idier et al 2012 lee et al 2013 bertin et al 2015 thuy et al 2017 choi et al 2018 to model the wave setup the radiation stress has to be introduced the radiation stress is the momentum flux carried by the waves when the waves dissipate such as by wave breaking or strong bottom friction this generates nearshore currents and an additional surge the wave setup its contribution to the total surge may be significant for instance values of 0 5 to 1 5 m were reported in liverpool bay brown et al 2010 more than 0 5 m i e 50 or more of the total surge in the southern part of the bay of biscay idier et al 2012 and 10 to 20 cm in the central part of the bay of biscay bertin et al 2015 in coastal areas it can contribute up to 80 of the total storm surge pedreros et al 2018 during typhoons it can contribute up to 15 following thuy et al 2017 and even 40 following kim et al 2010 in the north sea choi et al 2018 reported contributions of around 20 cm and 10 of the total surge during 1953 big flood to capture the wave setup well 1 the ocean model has to be coupled one way or two way with a wave model to take into account the radiation stress and 2 the grid spatial resolution has to be high enough that is around 10 m as a consequence due to a too coarse grid 3 to 15 km in the north sea the wave setup is not modelled in this study as is generally done in global and regional models this is a limitation for comparison with coastal tide gauges but not with altimeter as wave setup is close to zero far from the coast note that the present study would benefit from more tide gauge observations offshore where wave setup is negligible other contributions are the waves whose signature may be significant at the surface when propagating in coastal areas such as meteo tsunami infragravity waves only few cm in deep ocean aucan and ardhuin 2013 but can reach more than 1 m in coastal areas sheremet et al 2014 internal solitary waves can reach 20 cm in coastal areas as well as internal waves rogue waves and tsunamis surge due to an earthquake landslide or volcanic eruption finally seiches resonance phenomena in closed or semi closed basins also contribute to the surges their amplitude can be significant in harbours and sometimes reach several tens of centimetres 5 3 impact on the altimetric corrections the accuracy of the simulated storm surges is essential as it directly impacts the accuracy of sea level anomaly products through the dynamic atmospheric correction eq 8 fig 15 shows the differences between the default sla from ctoh blue curve and the new reconstructed one red curve with the dac taking into account the waves that is from the tugo simulation forced with the ecmwf wind stress the new reconstructed sla is on average closer to zero the sla has been improved by removing some surge residual due to atmospheric effect the difference between the two sla the native and the reconstructed one reaches 40 cm near the coast note that in this 40 cm around 20 cm may be attributed to the waves impact and the other 20 cm are probably due to a better ocean model resolution and a better temporal atmospheric forcing 1 h versus 6 h in the dac product from ctoh 6 conclusions we investigated the impact of a wave dependent stress on surge modelling during ex gonzalo and friedhelm storms which are characterized respectively by young and old sea states we compared simulated surges with wind dependent and wave dependent stress hellerman and rosenstein 1983 janssen 1991 we compared the results with tide gauges and altimetric data we showed that the global ocean model accurately predicts storm surges in coastal areas rmse of 0 12 m this can be attributed partly to the unstructured grid which allows increasing the resolution in the shallow waters the consistency between the model the altimeter and the tide gauges also confirms the capability of altimeters to accurately measure surges rmse of 0 08 m along the track we showed that when the sea state is old the wind dependent formulation hellerman and rosenstein 1983 is appropriate however when the sea becomes younger and rougher the wind stress increases and a wave dependent formulation here janssen 1991 is more appropriate this reduces the peak error significantly e g from 0 21 m to 0 09 m the waves effect on the surge can reach 20 to 25 cm this result is consistent with previous studies mastenbroek et al 1993 nicolle et al 2009 bertin et al 2015 however the number of case studies should be increased to confirm these conclusions indeed taking into the waves allows to obtain surges closer to observations but could be a way to compensate other errors for example moon et al 2009 concluded that mastenbroek et al 1993 obtained good simulated surges with overestimated drag by compensating surge error due to a too coarse grid this work underlines the lack of consistency of the drag between the wave atmosphere and ocean models van nieuwkoop et al 2015 one recommendation could be to force the ocean model with the wind stress from a coupled wave atmosphere model which has seen the waves e g ecmwf model this would yield to 1 more consistency between the drag from the ocean and the atmosphere models and 2 improvement of the storm surges taking into account the wave effect to go even further we would recommend forcing the ocean model with the stress going into the ocean τ o c rather than the atmospheric stress τ in this study we showed that the ocean model better matches with tide gauges offshore compared with onshore and altimeter tracks this is probably due to coastal effects in tide gauges e g wave setup that are not modelled in the tugo ocean model and that are not seen with the altimeters as the tracks are offshore this suggests that tide gauges should not always be considered as a reference and that what we generally call errors between model tide gauges or altimeter tide gauges also includes local coastal processes consequently we should not always want the model or altimeter to match perfectly with tide gauges finally we should mention several limitations in this study 1 results are specific to the ecmwf and tugo parameterizations used here janssen 1991 hellerman and rosenstein 1983 2 the wind stress is mainly supported by the capillary and short gravity waves that is the tail of the spectrum which is crudely represented in wave models today kudryavtsev et al 2014 peureux and ardhuin 2016 dedicated efforts are being made to improve wave breaking parameterization in order to also improve the tail of the spectrum there are also many uncertainties of the wave growth parameter β 3 we assume that the wind stress is in the wind direction as it is mainly supported by high frequency waves which respond quickly to changes in the wind direction janssen 2004 possible wind wave misalignment is not accounted here 4 the ocean model is forced by the atmospheric stress τ whereas a more correct approach would be to force it with the stress going into the ocean τ o c 5 the wave setup that is the surge generated by the wave dissipation e g wave breaking is not accounted here it may reach up to 20 cm in the north sea choi et al 2018 6 we have investigated the wind stress sensitivity on the surges but other factors impact the surges such as grid spatial resolution bottom friction bathymetry and coastal geometry mao and xia 2017 despite we obtain modelled surges closer to the observations there are still some possible compensation of errors due to uncertainties on other modelling terms note that we did not go further in investigating how these other factors may impact the surge dynamics as this is not within the scope of the present paper 7 the ocean model is here forced by the atmosphere model two way ocean atmosphere coupling would probably affect the resulting water levels as both the wind u 10 and the atmospheric stress τ will be modified to account for the moving ocean surface hersbach and bidlot 2009 credit authorship contribution statement lucia pineau guillou investigation validation software writing original draft marie noëlle bouin conceptualization writing review editing supervision fabrice ardhuin conceptualization writing review editing supervision florent lyard software jean raymond bidlot software bertrand chapron conceptualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement acknowledgement is made for the use of ecmwf s computing and archive facilities in this research we thank all ecmwf staff for the warm welcome and excellent support provided we thank cmems for providing tide gauges data we also thank ctoh legos for providing jason 2 1 hz x track coastal product legos for providing tugo ocean model and esa sea state climate change initiative and ifremer for providing jason 2 significant wave height we also warmly thank xavier bertin and michel benoit for their constructive comments and suggestions finally the authors would like to thank the reviewers for their careful reading and their constructive comments which allowed us to improve this article 
23950,we investigate the impact of wave dependent stress on surge modelling from case studies in the north sea using a global ocean model forced with a wave atmosphere coupled model we select the storms with the largest surges and a range of sea state development from young to mature seas the modelled surges are compared to tide gauges and altimeter data the ocean model is able to accurately predict storm surges in coastal areas the consistency of the model outputs the altimeter and the tide gauge data confirms the accuracy of altimeters for storm surge measurements we show that using a wave dependent rather than a wind dependent only stress formulation gives more accurate surge simulations when the sea state is young and the sea rougher taking into account the waves in the stress formulation has a significant impact on the surges up to 20 cm keywords air sea exchanges storm surges wind stress drag coefficient wind wave coupling north sea 1 introduction storm surges are generated by atmospheric pressure gradient and wind stress in coastal areas the wind stress contribution is more effective due to shallow waters water pileup along the coast and resonant effects moon et al 2009 bertin et al 2012 in addition in nearshore areas the radiation stress which is the momentum flux carried by the waves generates nearshore currents and wave setup i e additional surge when the waves dissipate bunya et al 2010 kim et al 2010 brown et al 2010 idier et al 2012 lee et al 2013 bertin et al 2015 thuy et al 2017 choi et al 2018 here we focus on the impact of the wind stress on the surges this study tackles the question which impact has the sea state on the wind stress to answer this we determine if simulated surges are closer to observations when wind stress parameterization is wave dependent the wind stress is usually parameterized using bulk formulae that express it as a function of the wind speed at a given height generally 10 m above sea surface and of a drag coefficient 1 τ ρ a u 2 ρ a c d u 10 2 where u c d and u 10 are the friction velocity the drag coefficient and the wind speed at 10 m above the surface respectively most formulations of the drag depend only on the wind speed e g moon et al 2007 edson et al 2013 peng and li 2015 whereas others include sea state parameters e g janssen 1991 moon et al 2009 despite years of research the impact of sea state on drag remains inconclusive recently edson et al 2013 concluded that the coare 3 5 wind speed dependent formulation matches the observations well without any wave information whereas earlier works insisted on the importance of wave enhanced drag for young waves mastenbroek et al 1993 these contrasted results can be explained by the complexity of the problem there are major issues that prevent a simple answer to the question what is the wave impact on the wind stress if any 1 wind stress measurements and drag estimations are difficult to perform particularly at high winds uncertainties in observations could explain the variability of the drag at a given wind speed estimations from several recent field experiments based on direct in situ measurements e g black et al 2007 edson et al 2013 or indirect ones powell et al 2003 jarosz et al 2007 holthuijsen et al 2012 as well as laboratory tank measurements e g donelan et al 2004 takagaki et al 2012 show differences in the drag up to a factor 2 at 30 m s pineau guillou et al 2018 it is not known if these differences are due to measurement uncertainties or various environmental conditions fetch turning wind bathymetry etc 2 wind stress measurements are scarce they generally come from short dedicated campaigns with moderate winds often being lower than 20 m s there is clearly a lack of measurements at very high winds 3 all the variables used to explain the drag variability are interdependent consequently good correlations between the drag and variables such as wave age may mainly be due to self correlation andreas 2009 this contributes to a lack of confidence in these relationships in a part of the scientific community the objective of this article is to investigate the impact of a wave dependent stress on the surges here we focus on the north sea the main idea is to simulate storms with various sea states i e young and old to estimate the sensitivity of surges to the sea state development i e the wave age selected storms are simulated using wind dependent only and wave dependent stress parameterizations the simulated surges are evaluated against observations i e tide gauges and altimeters the first part of the article describes the methods to compute the surges in models and observations in the following part we describe the case studies namely the storms selection the numerical setup and the validation data then we analyse the results to estimate the impact of the wave dependent stress on the storm surges finally we discuss different points among them is the difficulty of comparing the model with observations as various processes contribute to the surges particularly in coastal areas 2 surges in models and observations the surges are the differences between the water level and the tide prediction here we describe the methods to compute the surges from the model and observations 2 1 modelled surges surges are simulated with an ocean model forced with output from an atmosphere model the ocean model resolves the classical saint venant shallow water continuity and momentum equations in barotropic mode formulated similarly as in bertin et al 2012 2 η t h η u d z 0 3 d u d t f k u α g ψ ˆ p a ρ g η τ s τ b ρ η h where η is the surface elevation u is the horizontal velocity h is the bathymetry f is the coriolis parameter α is the earth elasticity factor g is the mean gravitational acceleration ψ ˆ is the earth tidal potential p a is the sea level atmospheric pressure ρ is the water density τ s is the surface stress and τ b is the bottom stress the bottom stress is expressed as 4 τ b c h u u where h is the mean local depth and c is a dimensionless friction coefficient set as 2 5 10 3 lyard et al 2006 the water level variations are due to tide through the tidal potential ψ ˆ in eq 3 wind through the wind stress τ s in eq 3 and atmospheric pressure through the pressure gradient p a in eq 3 note that the wind stress effect is modulated with the bottom stress effect τ b in eq 3 whose influence is not studied here we investigated the relative contribution of the wind stress and pressure gradient terms in eq 3 as τ s ρ η h and p a ρ respectively in this case the wind stress contribution increases in shallow waters as it is divided by the water height comparison of the two terms shows that the wind stress term is largely dominant in the north sea that is more than 90 of the sum of the two terms pineau guillou 2018 this suggests that the currents are mainly driven by the wind whereas the effect of the atmospheric pressure is negligible in the following the surges are computed from simulations without tide no tidal potential ψ ˆ in eq 3 and with atmospheric forcing only as a consequence the modelled surges correspond only to atmospheric surges the surges due to wave breaking i e wave setup are not modelled here see discussion in section 5 to compute the surges we used the tugo shallow water global ocean model developed by legos lyard et al 2006 this model in barotropic mode resolves the classical shallow water continuity and momentum equations eqs 2 and 3 following lynch and gray 1979 the model solves the generalized wave equation 5 c e t c 0 c e 0 where c e 0 is the continuity equation eq 2 and c 0 is a relaxation coefficient towards the continuity equation this gives more explicitly 6 2 h t 2 h u t c 0 h t h u 0 where h η h is the water height and h u t is formally replaced by using the momentum equations this allows to make the elevation solver implicit hence relaxing the cfl condition for time step actually tugo can use different solvers and discretization for the dynamical equations but the most efficient so far in tides or storm surges modelling is the lgp1xlgp1 elevations and currents discretized at element vertices generalized wave equations solver hence being used in our study tugo is a reference model it allowed the development of the tidal model fes2014 carrère et al 2015 a worldwide reference model for tides which is widely used in the scientific community it also produces dynamic atmospheric corrections to correct altimeter data from atmospheric effects carrère and lyard 2003 this correction is officially used by cnes and nasa for altimeter data processing 2 2 observed surges tide gauges measure the sea level the surges are computed as the differences between the observed and predicted sea level simon 2007 7 s u r g e t i d e g a u g e o b s e r v e d s e a l e v e l t i d e p r e d i c t i o n we used 101 tide gauges in the north sea fig 1 obtained thanks to copernicus marine environment monitoring service cmems data temporal resolution is generally of 10 min 88 of the tide gauges and sometimes 1 h 12 of the tide gauges we used the tidal toolbox developed by legos allain 2013 to process the data and estimate the surges the method is the following 1 a harmonic analysis is performed on the sea level observations to estimate the harmonic constants amplitude and phase of the tidal constituents 2 from these harmonic constants a prediction is computed over the same period as the observations 3 the surges are the differences between the observed and predicted sea levels eq 7 note that this residual also includes the error associated with the prediction in addition to tide gauges radar altimeters onboard satellites also measure the sea level that is the instantaneous sea surface height ssh above the ellipsoid data are processed by providers to compute the mean sea surface mss and the sea level anomaly sla the mss corresponds to the mean of several years of ssh eventually using several satellites the sla is the difference between the instantaneous ssh and the mss many geophysical and environmental corrections are made to estimate sla and one among them is dynamic atmospheric correction dac antony et al 2014 this correction corresponds to the ocean response to atmospheric forcing atmospheric pressure and winds and generally comes from an ocean model elevations for high frequency part e g tugo carrère and lyard 2003 and inverted barometer law for low frequency part e g using ecmwf atmospheric pressure products to be consistent with the model and the tide gauges the surges from altimeters are computed as the summation of the sla and the dac 8 s u r g e a l t i m e t e r s l a d a c to compute the altimetric surges we used the sla and dac from the jason 2 1 hz x track coastal product birol et al 2016 developed by the center of topography of the ocean and hydrosphere ctoh legos toulouse along track data have a temporal resolution of 1 s which corresponds to a spatial interval of about 6 7 km between points x track is a post processing software which increases the ssh information derived from satellite altimetry in the coastal ocean areas retrieved information along tracks come closer to land up to 5 km against 10 km with the standard aviso archiving validation and interpretation of satellite oceanographic data product 3 case studies and modelling here we describe the storms selection the numerical setup and the validation data 3 1 storms selection the storms were selected according to the following criteria 1 availability of good quality data at tide gauges as well as along tracks 2 occurrence of maximum surges in the north sea and 3 presence of various sea states the objective was to select two storms with young sea states and two with old sea states a common way to characterize the sea state is to consider the wave age following the expression ξ c p u 10 where c p is the phase velocity at the peak of the wave spectrum and u 10 is the wind speed at 10 m when the wind has just started blowing the waves are short period steep and short crested at this stage the wind speed is higher than the wave phase velocity and the waves are growing it is commonly considered that the value of 1 2 separates the young and the old sea in the north sea during the storms the wave age is generally around 0 8 pineau guillou 2018 in the following sea state is considered as young when the wave age is close to 0 8 and old when the wave age is greater than 1 2 swell note that the sea state quickly evolves spatially and temporally during the storm the reference to a young or old sea state for each storm characterizes the sea state along the altimeter track during a short period of several minutes and is not representative of the whole storm to select the storms we analysedtide gauges and jason 2 altimeter data fig 1 the tide gauge database consists of 101 tide gauges in the north sea from january 2012 to october 2017 date of the extraction the duration of observations depends on sites and ranges from less than 3 years to more than 5 years surges at tide gauges were computed following eq 7 the altimeter database covers 8 years 2008 2015 of jason 2 data along tracks surges along tracks were computed following eq 8 analysis of data led to select following storms ex gonzalo friedhelm felix and gunter table 1 the tracks of the 4 selected storms are shown in fig 2 and the surface winds during the storms are shown in fig 3 these storms were moving at a speed of around 15 to 20 km h in december 2011 friedhelm crossed the north east atlantic its track is the southernmost one in the north sea the winds are very strong up to 27 m s and the sea state is old with wave age larger than 1 2 along the altimeter track in october 2014 ex gonzalo reached the northern part of the british islands this storm corresponds to the remnants of category 4 atlantic hurricane gonzalo it is the strongest storm in terms of surges but not in terms of winds no more than 22 7 m s the sea state is young with wave age around 0 9 along the altimeter track one of the characteristics of this storm is that the strong winds moved from the west to the east as a nearly north south front of around 1000 km long fig 3 b this explains the discontinuity in the mean sea level pressure observed in the ex gonzalo track over the north sea as the minimum moves along this front red curve in fig 2 in january 2015 felix and gunter crossed the north atlantic in fact three storms succeeded first elon then felix which crossed the north sea on the 10th of january and finally gunter on the 12th of january 2015 the winds were strong and reached 26 8 m s table 1 for felix the sea state was young with wave age close to 0 8 along the altimeter track whereas for gunter it was old with wave age larger than 1 2 the old sea for gunter can be easily explained by the succession of three storms in a short period few days the gunter track was farther north than the felix one which may explain why the storm had less impact in terms of surges 3 2 numerical setup the ocean model is forced with output from a coupled wave atmosphere model fig 4 here we describe the ocean model the atmosphere model and the experiments 3 2 1 ocean model we used the default configuration of the tugo shallow water global ocean model lyard et al 2006 with fes2014 spatial grid fig 5 the unstructured space discretization allows the increasing of the resolution in shallow waters as well as along strong topographic gradient areas in the north sea the resolution varies from 10 15 km offshore to 4 km along the french and english coasts and 2 km along the north of the norwegian coasts note that the resolution is not refined in the southeast of the north sea due to a lack of detailed bathymetric information in this area not enough available data the ocean model is forced with 1 10 m wind or 2 surface wind stress from the coupled wave atmosphere model fig 4 in the first case 1 the wind stress is computed from the ocean model bulk formula the drag coefficient is expressed following hellerman and rosenstein 1983 which is a wind only dependent formulation 9 1 0 3 c d 0 934 0 788 1 0 1 u 10 0 868 1 0 1 δ t 0 616 1 0 3 u 10 2 0 12 1 0 2 δ t 2 0 214 1 0 2 u 10 δ t where u 10 is the wind at 10 m and δ t is the air sea temperature difference to take into account the stability effect in the second case 2 the wind stress comes directly from the ecmwf coupled wave atmosphere model fig 6 shows the tugo drag for δ t 0 and the ecmwf drag computed over the north sea during two days for each storm friedhelm ex gonzalo felix and gunter for winds lower than 15 m s the tugo drag is quite similar to the ecmwf one but with no variability for winds stronger than 15 m s the tugo drag is generally lower than the ecmwf one for a given wind speed the variability of the ecmwf drag depends on the wave age fig 7 even if this dependency is not explicit see section 3 2 2 note that the tugo drag is quite close to the ecmwf drag for old sea state but lower for young sea state 3 2 2 atmosphere model we used the ecmwf coupled wave atmosphere model ifs integrated forecasting system to generate atmospheric forcing fig 4 we conducted the simulations without data assimilation the ifs cy41r1 cycle ecmwf 2015a has a spatial resolution of around 16 km tl1279 and 137 vertical levels it has been coupled with the spectral wave model ecwam ecmwf wave model ecmwf 2015b since 1998 ecwam uses a coarser horizontal resolution than ifs at around 28 km with 36 directions and 36 frequencies exponentially spaced with starting frequency 0 035 hz and an increment of 1 1 in ecwam the source terms are s s i n s n l s d s s b o t where s i n represents the wind input s n l represents the nonlinear wave wave interactions s d s represents the dissipation due to whitecapping and s b o t represents the bottom friction the parameterizations of these source terms are discussed in ecmwf 2015b the wind stress is represented by classical bulk formulae eq 1 we assume that the wind stress is in the wind direction that is the effects of wind wave misalignment are not accounted here the drag coefficient is expressed following janssen 1991 which is a wave dependent formulation in neutral conditions the drag coefficient can be expressed as 10 c d κ 2 log 10 z 0 2 where z 0 is the roughness length and κ is von kármán s constant 0 4 the roughness length is expressed as 11 z 0 0 11 ν u α u 2 g where ν is the kinematic viscosity and α is the charnock s parameter charnock 1955 note that the ecwam wave model uses eqs 10 and 11 but eq 11 is reduced to the second term that is roughness associated with an overall form drag of the wave field the first term that is roughness associated to the viscous properties of the flow is computed in the ifs atmosphere model the modification of the roughness length z 0 impacts the drag coefficient and the wind stress eqs 10 and 1 then the sea level eqs 2 and 3 but also the wind profile indeed when roughness increases friction also increases and this slows down the wind pineau guillou et al 2018 janssen 1991 parameterized the quasi linear wave growth effect as an effective larger charnock parameter expressed as a function of the wave induced stress τ w 12 α α 0 1 τ w τ with α 0 0 006 the wave induced stress τ w is the momentum flux transferred from the atmosphere to the waves it can be related to the wind wave growth parameter β and the directional wave spectrum e f θ 13 τ w ρ g 0 o 2 π β f θ e f θ c f d f d θ where θ is the direction f is the relative wave frequency and c f is the phase speed which is a function of frequency janssen 2004 the wave growth parameter is expressed as β β m κ 2 μ ln 4 μ μ 1 where β m is a constant 1 2 and μ is the dimensionless critical height ecmwf 2015b in eq 13 the frequency f is integrated from 0 to a high frequency limit f c prognostic part of the wave spectrum using the discretized spectrum beyond f c diagnostic part of the wave spectrum the shape of the spectrum is assumed and the resulting integral can be evaluated using a simple integration scheme ecmwf 2015b it is assumed that the diagnostic part of the wave spectrum is given as e f θ e f c θ f f c 5 for f f c the high frequency limit f c is set as f c min f m a x 2 5 f w i n d s e a where f m a x is the maximum discretized frequency and f w i n d s e a is the mean frequency of the modelled wind sea ecmwf 2015b the ocean model is forced by the atmospheric stress τ however a part of the atmospheric stress is going into the waves the momentum flux going into the ocean τ o c is the sum of two contributions fig 8 the part of the atmospheric flux which was not used to generate the waves τ o τ τ w and the momentum flux transferred from the waves to the ocean by dissipation τ d i s s ecmwf 2015b 14 τ o c τ o τ d i s s τ τ w τ d i s s a more correct approach would be to force the ocean model with τ o c rather than τ the normalized stress going into the ocean corresponds to the ratio τ o c τ output parameter of the ecmwf operational version but not available in the research version of ifs we used the normalized stress is lower than 1 when the waves are growing and greater than 1 when they are dissipating it is globally close to 1 but can reach values as high as 1 5 under extreme conditions such as with a passing front janssen 2012 in the ex gonzalo case study it could locally be greater than 2 when the front was passing pineau guillou 2018 the strong gradients suggest a potential impact on the ocean model curcic 2015 also investigated the ratio between the oceanic and atmospheric stress in tropical cyclones he found typical values between 0 85 and 1 depending on the wave state 3 2 3 experiments we simulated the 4 selected storms ex gonzalo friedhelm felix and gunter with two stress parameterizations that is the wind dependent and the wave dependent parametrizations note that felix and gunter are in the same simulation as they follow each other for each storm the model was initialized for at least 15 days with winds and atmospheric pressure coming from the ecmwf operational 1 h forecasts i e combining hourly operational forecasts computed twice a day at 00 00 and at 12 00 once initialized each storm simulation lasted 5 days and was forced by 1 the 10 m wind or 2 directly by the wind stress and the atmospheric pressure with a 1 h temporal resolution in this case the wind stress comes from the atmosphere model when forced by the 10 m wind 1 the drag is a wind dependent formulation computed by tugo eq 9 with δ t 0 hellerman and rosenstein 1983 whereas when forced by the wind stress 2 the drag is a wave dependent formulation which has seen the waves through ifs wam coupling eqs 10 13 janssen 1991 3 3 validation data among the 101 tide gauges 22 tide gauges were selected for comparison with the model fig 1 the following were the criteria 1 the tide gauges must have data available during the storms 2 the tide gauges must open up to the ocean rather than at the end of a bay 3 a maximum number of the tide gauges must be offshore where processes are different from harbours and 4 a maximum number of tide gauges must be located along the tracks the jason 2 tracks with the maximum surges are tracks 170 for friedhelm 61 for ex gonzalo 94 for felix and 170 for gunter fig 1 all the data tide gauges and jason 2 were processed as described in section 2 4 results to investigate the impact of the waves we compared the surges with wind dependent hellerman and rosenstein 1983 and wave dependent janssen 1991 parameterization during four storms two with a young sea state ex gonzalo and felix and two with an old sea state friedhelm and gunter note that felix and gunter followed each other and correspond to the same simulation we compared the simulated surges with observations namely tide gauges and jason 2 altimetric data fig 9 shows the surge comparison between the model and 3 tide gauges representative of the whole for ex gonzalo felix and gunter no data were available for friedhelm in 2011 as the cmems tide gauge database starts only in 2012 table 2 summarizes the corresponding errors between the model and the tides gauges bias root mean square error rmse and peak error defined as the difference between the maximum observed and modelled surge finally figs 10 and 11 show the surge comparison between the model and jason 2 altimetric data for ex gonzalo and friedhelm note that the grey shaded area corresponds to deep waters where the wind stress effect is lower analysis of ex gonzalo young sea and friedhelm old sea gives the following results the first result is that globally the model matches very well with the observations for the wave dependent parameterization on average the bias between the model and all the tide gauges is close to zero the rmse is 0 12 m and the peak error is 0 09 m table 2 the errors between the model and jason 2 are lower than the errors between the model and the tide gauges the bias is close to zero and the rmse is 0 08 m whereas the surge ranges up to 1 40 m fig 10 note also the very good agreement between the model jason 2 data and the tide gauge situated along the track d151tg see fig 1 for the tide gauges location unfortunately the tide gauge cromer located on the northeast coast of england at the end of the track did not record data during this storm for friedhelm the model also matches the altimeter quite well but not as well as for ex gonzalo for the wave dependent parameterization the bias and rmse reach 0 12 m and 0 08 m respectively fig 11 the differences could be due to uncertainties in altimeter corrections such as geophysical corrections for example tide however note that the agreement between the model and the altimeter is very good in shallow waters where the wind stress effect is the most significant fig 11 b to conclude the errors between the model and the observations tide gauges and altimeter data for these two storms are small enough to confirm the capability of a global model to accurately predict storm surges even in the coastal areas when its spatial resolution is fine enough to catch the storm size these results also confirm the capability of altimeters to accurately measure surges antony et al 2014 the second result is that the wave dependent parameterization yields higher surges only when the sea state is young ex gonzalo figs 9 a and 10 b otherwise the surges are similar regardless of the parameterization friedhlem fig 11 b physically this is not surprising as old sea corresponds to a situation where waves are no longer rapidly growing resulting in sharp reduction of the momentum flux from the atmosphere to the waves when the sea state is old the drag coefficients from the two parameterizations are close to each other fig 7 and the surges are then similar however in the presence of young and steep waves the drag increases with janssen s parameterization due to higher values of the charnock parameter fig 12 c and d this yields higher drag than the wind dependent formulation and later higher wind stress fig 12 e and f and higher surges the differences between these two parameterizations correspond to the effect of the waves on the surges this difference reaches 25 cm at lowestoft fig 9 a and 20 cm along jason 2 track fig 10 b we note the very good agreement in terms of significant wave height swh between the ecmwf coupled wave atmosphere model and jason 2 altimeter piolle et al 2019 for ex gonzalo figs 12 b and 13 b and for gunter fig 13 d the very strong swh gradient along the track for ex gonzalo corresponds to the passage of the front fig 13 b the third result is that the wave dependent parameterization is closer to the observations than the wind dependent one these results are consistent with those previously obtained by mastenbroek et al 1993 nicolle et al 2009 and bertin et al 2015 along the jason 2 track the rmse is reduced from 0 13 m to 0 08 m fig 10 b on average in the 21 tide gauges the peak error is reduced from 0 21 m to 0 09 m table 2 however in some tide gauges the surges are still underestimated the tides gauges can be separated into three groups group 1 a first group of 11 tide gauges in blue in fig 1 where the surges with the wave dependent stress match well with observations such as lowestoft and europlatformtg in fig 9 a this corresponds to the 4 offshore tide gauges f3platformtg d151tg europlatformtg vlaktevdraantg as well as 7 other tide gauges onshore group 2 a second group of 5 tide gauges in green in fig 1 where the surges with the wave dependent stress are still underestimated such as whitby in fig 9 a group 3 a third group of 5 tide gauges in black in fig 1 where the effect of the parameterization is not significant this corresponds to tide gauges located in the northern part of the north sea where surges are smaller than 0 50 m in this part the bathymetry ranges from 50 to 200 m and the effect of wind stress is smaller than in the southern part with shallow waters that may explain the non significant differences between the two parameterizations note that in the second group where surges are still underestimated there are no tide gauges offshore this underestimation is probably partly due to processes taking place in the very nearshore and not modelled by tugo e g wave setup see the discussion in section 5 this could suggest that comparisons with the altimeter are better as the tracks offshore are not contaminated by coastal processes such as wave setup another reason that could explain this negative bias is the lack of spatial resolution near the coast only 2 km experiments show that atmospheric surges may increase from 0 1 to 0 3 between 10 m isobath and the shoreline personal communication from x bertin here the spatial resolution is not fine enough to correctly represent this increase after the analysis of ex gonzalo young sea and friedhelm old sea analysis of felix young sea and gunter old sea give partly similar results 1 the model still matches quite well with the tide gauges bias rmse and peak error are respectively 0 cm 19 cm and 10 cm for the wave dependent parameterization see table 2 2 the wave dependent parameterization still yields to higher surges when the sea state is young the impact of the waves on the surges reaches around 11 cm at europlatform and around 20 cm along jason 2 track not shown 3 we find also that the wave dependent parameterization is closer to the tide gauge observations reducing on average the peak error from 0 14 m to 0 10 m however comparison between the model and the altimeter is not as good as for ex gonzalo and friedhelm not shown and it is difficult to conclude which parameterization is the most appropriate this suggests that the number of case studies should be increased to give more confidence in our conclusions 5 discussion here we discuss the influence of the wind direction the processes contributing to the surges and the impact on the altimetric corrections 5 1 impact of the wind direction analysis of tide gauge and altimeter data revealed that ex gonzalo was the storm with the highest surge whereas it was the one with the weakest winds only 23 m s in the north sea against 27 m s for friedhelm see table 1 this is mainly due to the wind direction fig 14 shows wind roses in the middle of the north sea 4 e 56 n during the 5 day simulations of the storms for ex gonzalo fig 14 a strong winds were mainly from the northwest direction pushing the waters along the southern coast of the north sea whereas for other storms strong winds came mainly from the west see also fig 3 this is probably not the only explanation for ex gonzalo high surges the track of this storm is the southernmost one the storm crosses the south of the north sea and the shallow waters may enhance the wind stress contribution 5 2 processes contributing to the surges even if the wave dependent parameterization yields higher surges the modelled surges are still underestimated compared with some tide gauges 5 over 21 for ex gonzalo storm the comparison between modelled and observed surges is complicated by the different processes that contribute to the surge and that are not always modelled see table 3 generally the dominant effect is the atmospheric forcing mean sea level pressure and winds this contribution is commonly of the order of 50 cm but can exceed 1 m in severe storms and hurricanes as in the case of xynthia in february 2010 at la rochelle pineau guillou et al 2012 bertin et al 2012 in case of progression in very shallow waters as for example in the south of the north sea the surges can reach up to 2 or 3 m this atmospheric contribution is taken into account in the ocean model through the atmospheric forcing another important contribution to the total surge which is here not taken into account is the wave setup that is the surge due to wave dissipation mainly by wave breaking in the nearshore areas bunya et al 2010 kim et al 2010 brown et al 2010 idier et al 2012 lee et al 2013 bertin et al 2015 thuy et al 2017 choi et al 2018 to model the wave setup the radiation stress has to be introduced the radiation stress is the momentum flux carried by the waves when the waves dissipate such as by wave breaking or strong bottom friction this generates nearshore currents and an additional surge the wave setup its contribution to the total surge may be significant for instance values of 0 5 to 1 5 m were reported in liverpool bay brown et al 2010 more than 0 5 m i e 50 or more of the total surge in the southern part of the bay of biscay idier et al 2012 and 10 to 20 cm in the central part of the bay of biscay bertin et al 2015 in coastal areas it can contribute up to 80 of the total storm surge pedreros et al 2018 during typhoons it can contribute up to 15 following thuy et al 2017 and even 40 following kim et al 2010 in the north sea choi et al 2018 reported contributions of around 20 cm and 10 of the total surge during 1953 big flood to capture the wave setup well 1 the ocean model has to be coupled one way or two way with a wave model to take into account the radiation stress and 2 the grid spatial resolution has to be high enough that is around 10 m as a consequence due to a too coarse grid 3 to 15 km in the north sea the wave setup is not modelled in this study as is generally done in global and regional models this is a limitation for comparison with coastal tide gauges but not with altimeter as wave setup is close to zero far from the coast note that the present study would benefit from more tide gauge observations offshore where wave setup is negligible other contributions are the waves whose signature may be significant at the surface when propagating in coastal areas such as meteo tsunami infragravity waves only few cm in deep ocean aucan and ardhuin 2013 but can reach more than 1 m in coastal areas sheremet et al 2014 internal solitary waves can reach 20 cm in coastal areas as well as internal waves rogue waves and tsunamis surge due to an earthquake landslide or volcanic eruption finally seiches resonance phenomena in closed or semi closed basins also contribute to the surges their amplitude can be significant in harbours and sometimes reach several tens of centimetres 5 3 impact on the altimetric corrections the accuracy of the simulated storm surges is essential as it directly impacts the accuracy of sea level anomaly products through the dynamic atmospheric correction eq 8 fig 15 shows the differences between the default sla from ctoh blue curve and the new reconstructed one red curve with the dac taking into account the waves that is from the tugo simulation forced with the ecmwf wind stress the new reconstructed sla is on average closer to zero the sla has been improved by removing some surge residual due to atmospheric effect the difference between the two sla the native and the reconstructed one reaches 40 cm near the coast note that in this 40 cm around 20 cm may be attributed to the waves impact and the other 20 cm are probably due to a better ocean model resolution and a better temporal atmospheric forcing 1 h versus 6 h in the dac product from ctoh 6 conclusions we investigated the impact of a wave dependent stress on surge modelling during ex gonzalo and friedhelm storms which are characterized respectively by young and old sea states we compared simulated surges with wind dependent and wave dependent stress hellerman and rosenstein 1983 janssen 1991 we compared the results with tide gauges and altimetric data we showed that the global ocean model accurately predicts storm surges in coastal areas rmse of 0 12 m this can be attributed partly to the unstructured grid which allows increasing the resolution in the shallow waters the consistency between the model the altimeter and the tide gauges also confirms the capability of altimeters to accurately measure surges rmse of 0 08 m along the track we showed that when the sea state is old the wind dependent formulation hellerman and rosenstein 1983 is appropriate however when the sea becomes younger and rougher the wind stress increases and a wave dependent formulation here janssen 1991 is more appropriate this reduces the peak error significantly e g from 0 21 m to 0 09 m the waves effect on the surge can reach 20 to 25 cm this result is consistent with previous studies mastenbroek et al 1993 nicolle et al 2009 bertin et al 2015 however the number of case studies should be increased to confirm these conclusions indeed taking into the waves allows to obtain surges closer to observations but could be a way to compensate other errors for example moon et al 2009 concluded that mastenbroek et al 1993 obtained good simulated surges with overestimated drag by compensating surge error due to a too coarse grid this work underlines the lack of consistency of the drag between the wave atmosphere and ocean models van nieuwkoop et al 2015 one recommendation could be to force the ocean model with the wind stress from a coupled wave atmosphere model which has seen the waves e g ecmwf model this would yield to 1 more consistency between the drag from the ocean and the atmosphere models and 2 improvement of the storm surges taking into account the wave effect to go even further we would recommend forcing the ocean model with the stress going into the ocean τ o c rather than the atmospheric stress τ in this study we showed that the ocean model better matches with tide gauges offshore compared with onshore and altimeter tracks this is probably due to coastal effects in tide gauges e g wave setup that are not modelled in the tugo ocean model and that are not seen with the altimeters as the tracks are offshore this suggests that tide gauges should not always be considered as a reference and that what we generally call errors between model tide gauges or altimeter tide gauges also includes local coastal processes consequently we should not always want the model or altimeter to match perfectly with tide gauges finally we should mention several limitations in this study 1 results are specific to the ecmwf and tugo parameterizations used here janssen 1991 hellerman and rosenstein 1983 2 the wind stress is mainly supported by the capillary and short gravity waves that is the tail of the spectrum which is crudely represented in wave models today kudryavtsev et al 2014 peureux and ardhuin 2016 dedicated efforts are being made to improve wave breaking parameterization in order to also improve the tail of the spectrum there are also many uncertainties of the wave growth parameter β 3 we assume that the wind stress is in the wind direction as it is mainly supported by high frequency waves which respond quickly to changes in the wind direction janssen 2004 possible wind wave misalignment is not accounted here 4 the ocean model is forced by the atmospheric stress τ whereas a more correct approach would be to force it with the stress going into the ocean τ o c 5 the wave setup that is the surge generated by the wave dissipation e g wave breaking is not accounted here it may reach up to 20 cm in the north sea choi et al 2018 6 we have investigated the wind stress sensitivity on the surges but other factors impact the surges such as grid spatial resolution bottom friction bathymetry and coastal geometry mao and xia 2017 despite we obtain modelled surges closer to the observations there are still some possible compensation of errors due to uncertainties on other modelling terms note that we did not go further in investigating how these other factors may impact the surge dynamics as this is not within the scope of the present paper 7 the ocean model is here forced by the atmosphere model two way ocean atmosphere coupling would probably affect the resulting water levels as both the wind u 10 and the atmospheric stress τ will be modified to account for the moving ocean surface hersbach and bidlot 2009 credit authorship contribution statement lucia pineau guillou investigation validation software writing original draft marie noëlle bouin conceptualization writing review editing supervision fabrice ardhuin conceptualization writing review editing supervision florent lyard software jean raymond bidlot software bertrand chapron conceptualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement acknowledgement is made for the use of ecmwf s computing and archive facilities in this research we thank all ecmwf staff for the warm welcome and excellent support provided we thank cmems for providing tide gauges data we also thank ctoh legos for providing jason 2 1 hz x track coastal product legos for providing tugo ocean model and esa sea state climate change initiative and ifremer for providing jason 2 significant wave height we also warmly thank xavier bertin and michel benoit for their constructive comments and suggestions finally the authors would like to thank the reviewers for their careful reading and their constructive comments which allowed us to improve this article 
23951,coastal hazards often result from the combination of different simultaneous oceanographic processes that occur at multiple spatial and temporal scales to predict coastal flooding and erosion it is necessary to accurately represent hydrodynamic conditions for this reason here we present a stochastic climate based wave emulator that provides the hydrodynamic conditions needed for these predictions the emulator can generate an infinitely long data series maintaining its statistical properties at different time scales from intra storm to inter annual variability and its link to large scale climate patterns the proposed methodology relies on the use of weather types and an autoregressive logistic regression model forced with different variables to simulate daily scale chronology considering the dependencies of wave conditions on the different weather types the intra storm chronology is solved by means of shuffling and stretching historical wave sequences to demonstrate the replicability of this emulator worldwide we have applied the model to 3 different locations and found good agreement when compared to the historical data furthermore to illustrate and explain the strengths and limitations of the emulator we present a different application for each of the different locations keywords weather types emulator chronology coastal hazards shoreline 1 introduction weather related coastal hazards e g flooding and erosion result from the combination of large scale oceanographic processes e g storm surge waves and tides despite large variability associated with local effects extreme damage can occur after a sequence of consecutive events e g clusters of extratropical storms or isolated events e g tropical cyclones these events are the consequence of different environmental processes that act at a range of different spatial and temporal scales making it necessary to analyse the multivariate nature of the problem and the dependence between variables leonard et al 2014 hydrodynamic conditions need to be accurately represented to predict flooding and coastal evolution at intermediate scales order of decades from a statistical point of view time series of hydrodynamic variables should be long enough to explore all sources of variability and perform a robust and reliable analysis serafin and ruggiero 2014 furthermore chronology is a key factor when trying to predict coastal hazards such as shoreline erosion since different storm sequencing with the same magnitude can result in very different erosion or accretion e g coco et al 2014 and even lead to multi hazards godoi et al 2017 for these reasons the development of an emulator capable of estimating reliable time series for different wave climates is appropriate when trying to reduce uncertainty of predictions wahl et al 2012 moreover the difficulty of developing reliable future projections leads to the use of wave ensembles as the most reliable approach to improve the range of potential evolutions and to estimate the prediction uncertainty vitousek et al 2017 vitousek et al submitted an international workshop on shoreline prediction shoreshop montaño et al 2020 held in new zealand in june 2018 identified the generation of synthetic time series as a key step to overcoming the difficulty of projecting long term shoreline evolution different statistical emulators of wave climate already exist in the literature davidson et al 2010 use the method of borgman and scheffner 1991 based on a month by month multivariate stationary simulation to synthetically reproduce 1000 time series of wave parameters to forecast seasonal to multi year shoreline change in callaghan et al 2008 a joint probability method is developed to simulate extreme beach erosion statistical downscaling based on weather types wt provides a different flexible framework capable of assessing seasonal to interannual variability guanche et al 2013 this wt framework allows to split the atmospheric circulation into a number of synoptic patterns i e wts climate based emulators have been previously used to assess extremes of waves and storm surge rueda et al 2016 coastal flooding potential anderson et al 2019 wave spectra rueda et al 2017a and long term morphodynamics antolínez et al 2016 furthermore wt based emulators are also useful to predict climate driven changes in different variables under various scenarios of climate change by analysing how global climate models gcms affect occurrence of wts in the future camus et al 2017 perez et al 2015 sheridan and lee 2010 nevertheless most of the emulators presented above have different limitations when computing long term erosion and flooding some of them focus on reproducing only the conditions of extreme storms and not on producing continuous wave time series antolínez et al 2016 callaghan et al 2008 others emulate waves at a daily scale rueda et al 2016 not taking into account the shape of a continuous storm longer than a day which is relevant when trying to synthetically reproduce the erosion produced by an individual event furthermore only the emulator from anderson et al 2019 links the emulated time series to larger scale climatic patterns i e el niño southern oscillation enso madden julian oscillation mjo which is essential to analyse the precursors of large erosion or flooding events predicted from synthetic wave series since large scale climatic patterns can have large impact at the coast barnard et al 2015 the main objective of this work is therefore to develop a climate based emulator capable of generating infinitely long time series of the wave conditions needed to drive shoreline evolution models enabling modellers to assess long term evolution in a probabilistic way the new climate based emulator relies on the use of wts and accounts for not only the historical intra seasonal seasonal and interannual scales but also realistic intra storm chronology which has not yet been achieved by any of the previous works described above the emulator considers the dependencies of wave climate from sea level pressure fields at different timescales and solves the intra storm chronology by means of shuffling and stretching observations of historical wave sequences to demonstrate some of the capabilities of the waves emulator in the context of shoreline evolution the proposed methodology has been applied and tested at 3 locations fig 1 a imperial beach usa b tairua beach new zealand and c laredo beach spain these locations have been chosen as examples of global applicability of the methodology and also because of the availability of wave and storm surge observations to enhance the capabilities of the emulator for other applications such as coastal flooding where storm surge might be a key element rueda et al 2017b the methodology is explained including this additional variable the paper is organized as follows section 2 provides the information of the different databases used section 3 describes the methodology followed to develop the emulator section 4 presents the results of the method and some shoreline applications utilizing the multivariate time series generated at the 3 locations discussion and conclusions are presented in sections 5 and 6 2 data this section summarizes the different datasets used to reproduce the climate based emulator at the three different locations section 2 1 introduces the databases employed to characterize the large scale climate patterns which are the same for the 3 different locations sections 2 2 and 2 3 present the wave and storm surge databases at each of the study sites 2 1 atmospheric data sea level pressure slp fields from the climate forecast system reanalysis cfsr saha et al 2010 are used to define the weather patterns at both annual and daily scales the reanalysis spans from 1979 to present at 0 5 spatial resolution and hourly temporal resolution the data have been subsampled to daily values and 2 spatial resolution for consistency with global climate models gcms rueda et al 2018 as the methodology presented here could also be adapted and used under climate change scenarios in addition to the slp fields we have also used the mjo index wheeler and hendon 2004 which provides information at a daily scale since 1975 the mjo is an intra seasonal oscillation characterized by large scale convective anomalies that propagate around the equator with a 30 to 60 days period madden and julian 1972 mjo is the dominant mode of intraseasonal variability in the tropics dee et al 2011 and influences tropical cyclone generation and intensity liebmann et al 1994 maloney and hartmann 2000 the development of el niño southern oscillation events kessler and kleeman 2000 and extratropical weather cassou 2008 2 2 wave data the wave data are characterized at the three locations using significant wave height hs mean period tm and direction dir the wave data used for imperial beach come from a wave buoy close to the coast at 32 56955 n and 117 16915 w at 21 m depth this station has an hourly temporal resolution and the record spans from 2000 to 2016 in the case of tairua waves at 36 988 s and 175 864 e at 10 m depth were provided by a dynamically downscaled hindcast by means of the hydrodynamic model swan and a larger scale wavewatch iii hindcast the waves have a 3 hourly temporal resolution and span from 1979 to 2017 for our third location laredo the wave data are from a hybrid wave hindcast camus et al 2013 located at 43 4275 n and 3 42627 w at 8 4 m water depth while this record with hourly temporal resolution spans from 1948 to 2015 we only use data from 1979 to match the atmospheric data 2 3 storm surge data the water level data used for the imperial beach storm surge are obtained from the national oceanic and atmospheric administration tide gauge 9410230 located at the pier in la jolla california the levels were then separated into the different components following the methodology developed by serafin and ruggiero 2014 where the water level wl signal is split into a mean sea level component η m s l the deterministic astronomical tide η a the monthly mean sea level anomaly η m m s l a an intra annual seasonal variation η s e and the storm surge η s s 1 w l η m s l η a η m m s l a η s e η s s for tairua the storm surge is obtained from the dynamic atmospheric correction dac reanalysis produced by cls space oceanography division and distributed by aviso carrère and lyard 2003 the outputs of this model span from 1992 to 2014 and are provided at a 6 h temporal resolution on a regular grid of 0 25 0 25 this reanalysis uses the mog2d barotropic model from legos and it is forced by pressure and winds from the european centre for medium range weather forecasts ecmwf in laredo the storm surge data are obtained from the global ocean surge gos reanalysis cid et al 2014 which has an hourly temporal resolution and 0 125 spatial resolution this reanalysis uses the regional ocean model system roms of rutgers university to develop the time series in the southern europe area 3 methodology the climate emulator developed in this work aims to initially predict realistic sequences of daily wts dwts based on large scale slp fields and to use such sequences to predict the parameters needed to drive coastal hazards models while the figures demonstrating the methodology within this section correspond to imperial beach similar agreement between historical and simulated conditions was found for each of the other study sites analysed the general methodology proposed as illustrated in fig 2 is composed of the four different steps explained in detail in the following sections a dwt classification section 3 1 b chronology model used to develop the synthetic time series of dwts section 3 2 c stretching method of the intra storm characteristics section 3 3 d monte carlo shuffling based simulation section 3 4 in order to show which components of the methodology were previously proposed by other authors and which are the original contributions of this work the components have been differentiated in colours in fig 2 grey corresponds to the previously developed work while purple is the new part of the methodology proposed in this paper panel a weather type classification was proposed in camus et al 2014 and later improved in camus et al 2016 and hegermiller et al 2017 the chronology model panel b is based on the approach proposed in anderson et al 2019 although a new methodology for obtaining the annual predictor has been developed leading to new realizations of dwt chronologies panels c and d stretching and shuffling are novel work and the main contributions to the proposed wave climate emulator these approaches focus on solving the intra storm chronology which is essential for driving shoreline models 3 1 weather type classification there are several different methodologies available to develop dwt classifications these can either be subjective lamb 1972 or based on automated algorithms camus et al 2014 which is the approach adopted in this study for this purpose we need to define a predictor area and a clustering technique since swell energy can propagate for thousands of kilometres over several days snodgrass et al 1966 the predictor area fig 1 is defined for each location following the estela evaluation of source and travel time of wave energy reaching a local area method pérez et al 2014 which evaluates the sources and mean travel time of the wave energy reaching a given location using the directional wave spectra from the global ifremer wave hindcast rascle and ardhuin 2013 this predictor has been demonstrated to capture not only wave but also storm surge variability over different temporal scales rueda et al 2018 for the wt classification as described in camus et al 2014 a principal component analysis pca is applied to slp fields and slp gradients inside the area defined by the estela method to obtain the temporal coefficients pcs associated with the dominant spatial variability patterns eofs to include the travel time of swell waves the slp fields and their gradients are modified dependent on the isochrones of the average travel time following hegermiller et al 2017 as 2 p t x y s l p t i ω i s l p g t i ω i f o r i 1 p where ω i represents the spatial domain between isochrones i 1 and i and p is the number of isochrones of the furthest wave generation area in order to reduce dimensionality and preserve the maximum amount of variance the pca modes explaining 95 of the total variance are selected as the predictor for the automated classification the clustering technique chosen for the classification is a regression guided k means which has already been shown to improve the downscaling of storm surge and waves based on wts camus et al 2016 cannon 2012 the classification is performed on a dataset z which concatenates the weighted pca predictor x with the prediction estimations y ˆ from a linear regression model between predictand y constructed with the wave parameters i e hs tm and dir and predictor x 3 z 1 x y ˆ where in this case equal to 0 3 is a factor which varies from 0 to 1 meaning 0 unsupervised classifications and 1 fully supervised classification the k means algorithm hastie et al 2001 is applied to the dataset to obtain a number of clusters 36 that must be a compromise between the number of data per cluster 100 days and a number large enough to describe the range of different climatologies i e wts this is because at a later stage the number of data per cluster will condition the fitting of the distributions relative to the wave parameter the emulator methodology proposed in this work could ultimately be adapted to any number of clusters as those would depend on the local variability of different climatologies worldwide and based on the user criteria the 36 clusters dwts are represented in fig 3 by the sea level pressure anomaly centroid using the data for which the centroid is the closest minimizing the within cluster distance the seasonal probability of dwt occurrence where dfj corresponds to december january february mam to march april may jja to june july august and son to september october november is presented in the lower panel of fig 3 3 2 chronology model the simulation of realistic synthetic dwt sequences is a key issue for assessing the chronology of coastal hazard events such as flooding or erosion where the time interval and severity between storms can amplify a negative outcome the chronology model must be able to capture the probability of occurrence of the dwts the persistence the transition probabilities and the intra seasonal to interannual variability for this purpose and following anderson et al 2019 an autoregressive logistic regression alr model is implemented over the dwts the alr model allows the simulation of synthetic sequences of dwts accounting for different covariates such as seasonality additional predictors and autoregressive or markov terms alr models are also capable of simultaneously dealing with covariates that vary at different time scales considering the autocorrelation between them further information on the model foundations and the implementation of covariates can be found in guanche et al 2014 to reproduce the wave climate at different timescales the model is forced with the first three principal components of an annual predictor explained in detail in section 3 2 1 the seasonality the two principal components of an intra seasonal predictor based on the mjo explained in detail in section 3 2 2 and a third order markov chain in mathematical terms the model is represented as eq 4 given in box i where β 1 i and β 2 i account for the harmonic components of the seasonality for each weather type i ω is the angular frequency β j i a w t accounts for the probability associated to the principal components of the awt β j i m j o accounts for the two principal components of the mjo and the y t h term represents the markov order the validation of the alr against the different covariates included in the model is presented in fig 4 fig 4a corresponds to the validation of seasonality and shows the comparison of 100 simulations of mean dwt probabilities in a perpetual year bottom against the historical dwt probabilities top fig 4b shows the same comparison but for the different annual wts awts section 3 2 1 here it can be appreciated how el niño years awt 6 have a completely different dwts distribution with dwt30 and dwt36 corresponding to large storms see fig 3 being the most probable dwts fig 4c presents the comparison of the dwt probabilities against the mjo active phase and its similar behaviour in the synthetic time series 3 2 1 annual predictor the annual predictor is constructed from the first three principal components of the 36 dwt probabilities aggregated every year here we used the dwts probabilities to construct the annual predictor rather than variables such as sea surface temperature in the equatorial pacific as in anderson et al 2019 to increase the generality of the annual predictor worldwide for this purpose the annual pcs are clustered into 6 different synoptic patterns following a k means algorithm hastie et al 2001 this way a number of independent awt chronologies are computed and the annual pcs needed to force the daily alr are obtained by randomly choosing a triplet pc 1 awt pc 2 awt pc 3 awt from the historical years associated with each of the different awts as we want the daily simulations to include independent annual chronologies for each simulation to explore different annual climatic transitions an alr model forced with a markov chain for the awt is previously computed fig 5 illustrates the construction of the annual predictor at imperial beach we show the first three spatial and temporal components of the pca top three panels and the k means grouping leading to the awt bottom panel the spatial eofs correspond to the 36 dwt probabilities associated with each pc the coloured dots in the temporal series match strong and very strong el niño and la niña years oni index larger than 1 5 the third pc has a strong dependency with el niño modoki of 2010 it can be appreciated that all strong el niño years fall into the same awt awt 6 while strong la niña years fall into the awt 1 this is due to the sensitivity analysis carried out to obtain the appropriate number of principal components to use for the k means clustering we found three pcs to be the optimal number to group el niño and la niña years into the same awt for both imperial beach and tairua in the case of laredo where the signal of the interannual variability is less strong we found that five pcs were needed to homogeneously sample the awts over the years 3 2 2 intra seasonal predictor to preserve intra seasonal variability a predictor based on the mjo is introduced the mjo which has a time scale of 30 to 90 days zhang et al 2009 is important for monthly and seasonal predictions the mjo index used here was defined by wheeler and hendon 2004 and is characterized by the first two pcs of equatorially averaged outgoing longwave radiation and zonal winds based on those two pcs the mjo is classified in 25 different categories based on the active phase from 1 to 8 and 4 different intensities from inactive to extremely active as defined in lafleur et al 2015 and anderson et al 2019 following the same technique as for the annual predictor a number of independent simulations of the mjo category are computed using the alr model forced with seasonality and a third order markov chain as covariates in order to develop the pcs needed to force the daily alr model we randomly pick a combination of pc 1 mjo and pc 2 mjo from the historical record associated to each of the categories from 1 to 25 3 3 wave stretching wave climates are constructed by connecting and shuffling historical chronologies of wave and storm surge conditions associated to a synoptic state defined here as the time interval in which synoptic conditions remain constant thus the duration of each different synoptic state is defined by the number of consecutive days in which the local synoptic pattern is driven by the same dwt this implies that although we are using dwts to define the duration of the different states our approach reproduces synthetic time series at the same temporal resolution as the waves provided as input of the emulator hours in order to reproduce similar storm chronologies i e ramp up peak and ultimate reduction of wave energy as in the past but enhancing the capability of extrapolating the intensity of wave chronologies associated with each synoptic state we develop a stretching technique for both hs and tm the associated historical storm surge level and wave direction are maintained the stretching technique allows broadening in magnitude the hindcast chronology with a minimum and maximum value of the wave conditions defining the synoptic state this way even when simulating the historical chronology shape and duration multiple times the reproduced magnitudes due to the stretching technique can be very different in each simulation refer to fig 2c here we present the number and mean duration of the synoptic states identified at each of the different locations for the case of imperial beach the 6208 days of wave conditions have been divided using the dwt historical chronology into 2571 different states with durations ranging from 1 to 5 days mean duration of 2 4 days for tairua the number of synoptic states found is 6126 with a mean duration of 2 3 days while in laredo 8504 different synoptic states are found with a mean duration of 1 6 days when durations are found to be longer than 5 days the technique randomly divides them into different consecutive pieces from 3 to 5 days the 5 days threshold is based on the need to have enough data to populate the distributions relative to each storm duration and dwt while this threshold can depend on the local storm durations and the number of dwts in our case it has been constant for the 3 locations studied for each wave and storm surge synoptic state the parameters that are stored for each dwt and duration are the following maximum wave height hs max minimum wave height hs min maximum tm tm max minimum tm tm min mean dir dir mean and mean storm surge ss mean in order to apply the stretching technique during the simulation the wave hindcast relative to hs and tm has been normalized with the maximum and minimum values above for each synoptic state the joint probability between the different variables is then reproduced using a multivariate gaussian copula for each dwt as copulas have been shown to correctly model multivariate problems ben alaya et al 2014 nelsen et al 2006 rueda et al 2017a copulas are able to emulate realistic combinations of variables based on the historical probabilities to accomplish this the wave height and period parameters are fit to a kernel distribution to allow a small capability of extrapolation of extremes maintaining the historical general behaviour while wave direction and storm surge are fitted to an empirical distribution the marginal functions are then transformed into a normal distribution and the dependences are modelled by a symmetric positive definite matrix of spearman s correlation coefficients in the gaussian space the copulas are then used to randomly derive waves and storm surge values from the appropriate dwt copula for each synoptic state during the monte carlo simulation section 3 4 3 4 waves and storm surge simulation the waves and storm surge simulation is performed by connecting and shuffling historical chronologies associated to each synoptic state once we have constructed the synthetic time series of the dwts the monte carlo simulation of the wave and surge parameters associated with each synoptic state is performed as explained before a stretching technique is carried out for the wave height and period while the direction and storm surge are filled with the corresponding chronology of historical values this means that for each synoptic state the following parameters are randomly extracted from the gaussian copula associated with that specific duration and dwt hs max hs min tm max tm min dir mean and ss mean then we find a historical chronology that best matches the mean direction and mean storm surge extracted from the copula and use the wave height and period parameters to stretch the normalized hindcast as shown in fig 2c a filter was introduced into the gaussian copula to simulate pairs of hs and tm that fall inside the same physical steepness limits of the historical data this shuffling approach to wave and storm surge sampling allows us to increase the temporal resolution beyond the daily resolution given by the dwts to the temporal resolution of the original data provided as an input to develop the emulator 4 results 4 1 validation of long term and extreme value distributions of wave parameters following on with the imperial beach example fig 6 shows the historical distribution of wave parameters hs tm and dir for the different dwts and the validation of the synthetic wave time series against the buoy data fig 6a shows the hindcast data and a 100 year simulation of synthetic wave parameters it is clear how the large storm dwts associated with the winter months shown in fig 3 closely agree with the larger wave height and period distributions fig 6b and c predominantly approaching imperial beach from the west and north west directions fig 6d furthermore figs 3 and 6 show how the approach is able to discern between waves generated by large scale conditions versus those generated close to the coast for example dwt20 fig 3 is associated with pressure anomalies both in the southern hemisphere and close to the san diego coast and this pattern is reflected in the wave period and direction histograms fig 6c and d 2nd row 4th column which show a bimodal behaviour these relationships as expected from the kernel and empirical distributions used for the fitting are maintained in the simulation demonstrating a good agreement between the historical and synthetic wave height period and direction the same behaviour was found for storm surge distributions not shown fig 6e illustrates the gaussian copula correlation coefficient between the parameters hs max and tm max of the synoptic states that as expected has positive values for all the dwts with higher correlation values for the dwts associated with the largest waves a detailed comparison of the historical distribution versus the 100 synthetic simulations is carried out via quantile quantile plots as shown in fig 7 a general agreement is found between the historical data and the simulations which are represented by the mean the variability envelope and the standard deviation across simulations the variability is in general larger at the tails of the distributions although in the case of wave direction this variability can also be large in the middle due to the bimodal behaviour in some dwts fig 6d in order to evaluate the performance of the emulator in reproducing extreme synthetic wave parameters the return periods of hs and tm have been compared against historical data fig 8 the simulations correctly reproduce extreme waves over the different return periods at each of the study sites as exemplified in the results for imperial beach the behaviour of the return periods for both hs and tm after reaching the maximum value of the historical record is mostly flat this is due to the fact that the emulator is meant to reproduce the historical behaviour of the data with just a small capability of extrapolation over larger return periods due to the kernel and empirical distributions used during the stretching section 3 3 4 2 applications relevant to shoreline change nearshore hydrodynamic processes play a major role in shaping shoreline evolution at seasonal to interannual scale ciavola and coco 2017 here we analyse the waves generated by the climate based emulator presented at the 3 different study sites by means of 3 different applications relevant for shoreline evolution the implications of using the synthetic waves from the proposed emulator in the calculation of the longshore wave power lwp at imperial beach usa are presented in section 4 2 1 in section 4 2 2 we analyse the implications on cross shore shoreline variations at tairua new zealand while in section 4 2 3 we will analyse the directional component of beach rotation at laredo spain 4 2 1 longshore wave power imperial beach longshore sediment transport is primarily controlled by the lwp component on dissipative and relatively alongshore uniform beaches here we present a historical analysis of the lwp relative to the different awts as defined in section 3 2 1 against the same analysis using the simulated waves the formula used for computing the lwp as defined in komar 1998 reads 5 p e c n sin θ cos θ 1 8 ρ w g h 2 c n sin θ cos θ where e is the total energy c is the wave speed n is the ratio of group to individual wave speed and θ is the angle between the wave direction and shore normal assuming a constant 178 beach orientation the cumulative annual wave power is shown in fig 9 the left panel corresponds to the historical period from 2000 to 2016 where colours represent the different awts we have only named those which fall outside the historical standard deviation represented in grey el niño years represented by awt 6 produce a large lwp anomaly compared to the yearly average as can be seen in 2009 10 and 2015 16 in southern california the shift in direction to the north by 6 during the 2009 2010 el niño event was equivalent to the one occurring in 1997 98 causing the greatest winter shoreline retreat in the southern california survey record barnard et al 2011 in 2015 2016 the shoreline retreat in southern california was four times bigger than in previous years barnard et al 2017 and imperial beach was highly eroded and coastal flooding occurred young et al 2018 similar lwp behaviour during el niño years was observed offshore of northern oregon anderson et al 2018 as opposed to el niño during la niña years more southerly waves approach the coast causing a lower lwp this historical behaviour is well captured during the simulations as shown in fig 9b where el niño awt 6 is associated to the largest lwp while la niña awt1 has the lowest values the rest of the awts have a similar overlapping area suggesting that years without an active el niño or la niña present similar lwp variability throughout the year even when the probabilities of the different dwts with that awt can be very different lines correspond to the mean lwp for each awt while the shaded area comprises the area between the maximum and minimum lwp values for each awt this behaviour confirms that the methodology correctly reproduces the interannual variability of the area and that the simulations can be linked to large scale climatic patterns such as el niño southern oscillation this way the small number of historical el niño and la niña years can be extended with the synthetic series to develop probabilistic assessment of the impact of this phenomenon 4 2 2 cross shore shoreline variation tairua beach at seasonal and interannual timescales shoreline evolution can be dominated by cross shore processes kriebel and dean 1985 miller and dean 2004 here a calibrated equilibrium shoreline change model yates et al 2009 has been used to explore shoreline evolution at tairua beach this model bases its predictions on the shoreline position disequilibrium concept wright et al 1985 and uses hs as the only wave parameter it has been successfully implemented at locations where and when gradients in alongshore sediment transport do not play a significant role castelle et al 2014 dodet et al 2019 splinter et al 2014 the model in tairua has been calibrated with 18 years of daily averaged alongshore shoreline position hereinafter referred to as shoreline data derived from a camera system blossier et al 2016 montaño et al 2020 details on the yates model calibration and implementation can be seen in montaño et al 2020 fig 10 compares the modelled shoreline annual minima erosion and maxima accretion return periods for 100 synthetic wave simulations of 100 years each in shaded area against shoreline annual minima and maxima return periods modelled using the historical wave database the annual shoreline maximum accretion is more or less constant over time for both the model results driven by the historical waves as well as those using simulated waves for erosion events historical larger variations within different years produce larger differences between the model results driven by the 100 wave simulations fig 10 good agreement between erosion modelled using historical and simulated waves for both erosion and accretion for less than 10 years return period is found while an underestimation of the erosion for larger return periods is observed this may be due to an issue with the fact that we are reproducing storms lengths determined by the dwt classification that can sometimes split the same storm into different pieces while large erosion periods usually require larger storm durations however we stress that the underestimation does not mean that we are reproducing less erosion during larger events but rather less frequent erosional events i e associated with larger return periods 4 2 3 beach rotation laredo beach rotation occurs at seasonal to interannual timescales triggered by wave energy events with a strong alongshore component compared to the mean wave direction van de lageweg et al 2013 here we analyse and compare the historical wave direction record against simulated wave directions at laredo spain fig 11a presents the annual mean direction for the historical record in black and 10 different simulations in colour the range of annual variations in the simulations is similar to the historical data with mean directions from 28 to 34 for all the 100 wave simulations performed with a mean historical direction of 31 25 mean and standard deviation values for the historical record and the simulations are included fig 11 there is only a very small difference between the historical mean and the simulated mean with the simulations giving slightly lower standard deviation than the historical waves the numbers in brackets correspond to the maximum and minimum values of the 100 simulations the lower panels correspond to the analysis of the number of consecutive years with a mean direction dir y over fig 11b and below fig 11c the average of the 37 years mean direction dir m y the comparison of the scarce number of historical wave records against the 100 simulations of more than 100 years results in a good agreement in particular with respect to the most probable durations for both the positive and negative annual wave direction anomalies the largest difference is due to the emulator defining the tails of the distributions which is a consequence of the large number of synthetic annual simulations a small probability of having up to 10 consecutive years with a positive anomaly or up to 8 consecutive years with a negative anomaly is introduced in the synthetic simulations while in the historical records the maximum persistence is 5 years these results imply that our emulator can be used to explain beach rotation with anomalous directions correctly reproducing the historical persistence distributions moreover given the fact that we are reproducing also the tails of the persistence distribution new rotation periods not registered in the historical data could be created albeit with very low probability of occurrence 5 discussion previous studies have proposed varying methods to generate wave climates to solve different problems in coastal science callaghan et al 2008 proposed a methodology to statistically simulate storm events to derive extreme beach erosion by reproducing storms with a duration and hs larger than a threshold and fit a poisson distribution to the spacing between storms while the callaghan et al 2008 approach is useful for applications focused on extreme isolated events it cannot produce continuous shoreline evolution as morphologic change models necessitate a continuous dataset to accumulate erosion and accretion periods over time in the work of davidson et al 2010 on forecasting seasonal to multi year shoreline change the wave climate emulator from borgman and scheffner 1991 is used this piecewise month by month emulator seeks to preserve univariate probability laws and the first and second order moment properties describing the intercorrelations of the data sequencing this approach does not preserve higher order moments between the variables and between different time and space elements in the data this means that interannual variability will not be well resolved between simulations and it would not be possible to link the data with larger scale climatic patterns as in the lwp application discussed in section 4 2 1 antolínez et al 2016 modelled long term morphodynamics by reproducing the persistence and sequencing of the storms by clustering different sea states the model accounted for seasonal and interannual variability by means of an alr model similar to the one described in section 3 2 but without taking into account larger temporal scale climatic predictors the method was also restricted to provide a reduced number of categories of sea states not reproducing the entire complexity of the wave climate the extreme wave and storm surge climate emulator from rueda et al 2016 and the multiscale climate emulator of multimodal wave spectra from rueda et al 2017a reproduce waves at a daily resolution by means of a wt classification and a chronology model based on markov probabilities however these approaches do not properly address the shape of synoptic states occurring for longer than a day a key need for models driving shoreline change none of the models above link the wave synthetic simulation with larger scale climatic predictors i e annual predictor intra seasonal predictor this link is relevant to explain the precursors of large erosion and or flooding events in the simulations furthermore considering multiple concurrent climate patterns is essential to develop more reliable coastal planning godoi et al 2019 the work from anderson et al 2019 was the first to our knowledge to include large scale climatic predictors and to account for intra storm chronology our emulator is developed on the same basis as it also depends on a wt technique that relies on finding the relationship between large scale climate patterns at different time scales and the local wave and storm surge parameters the main differences are the definition of the annual predictor and the shuffling and stretching of historical wave conditions in our case the annual predictor is defined by using the annual average of the dwt probabilities while in anderson et al 2019 they rely on a predictor based on the sea surface temperature this approach works well in areas in the pacific where the signal from el niño and la niña is very strong however it is not easily transferable to areas outside the pacific our annual predictor is still capable of discretizing between el niño and la niña years as shown in fig 4 at imperial beach with regard to the shape of the waves associated with each synoptic state anderson et al 2019 simulate the values of hs t dir and storm surge for a synoptic state defined in length as the number of days within the same wt they then reconstruct the shape over time with a constant value for t dir and storm surge and with a geometric hydrograph shape for hs another advantage of our method is that we reproduce more realistic storm shapes where t dir and storm surge have also their own progression over time this has been accomplished by directly using historical evolutions of hs t dir and storm surge and applying the stretching technique described in section 3 3 to hs and t although our climate emulator is transferable worldwide and has a large range of possible applications for modelling and assessing erosion and flooding accounting for different time scales from intra storm to interannual variability it also has some limitations the use of the dwts to identify the length of the synoptic states and the 5 days restriction due to the need to have enough data for each dwt and storm duration for the fitting has some effect when assessing events such as erosion very sensible to the storm duration this limitation is evident in the erosion in tairua shown in fig 10 where we see some underestimation on the larger erosion return periods even when the return period associated to the maximum waves has been validated as shown in fig 8 for these reasons further improvement in the way we define the synoptic states and their different constituents will be needed to improve the performance in situations where the effect of those limitations is important in addition although this methodology can be very flexible in terms of the wave and storm surge data provided as input of the emulator the fact that we are using historical chronology shapes and empirical and semi empirical distributions for the fitting of the parameters with a small capability of extrapolation implies that the longer the records provided the better the model will be in explaining the complexity of the wave climate at a given location 6 conclusions a climate based stochastic wave and emulator is presented for defining time series of boundary conditions for coastal applications it is capable of generating infinitely long and continuous time series statistically similar to the original data provided for the fitting maintaining its properties at different time scales from intra storm to interannual and its link to relevant large scale climate predictors this emulator relies on the use of dwts as a flexible framework for evaluating storm chronology at a daily scale by means of an alr model forced with larger time scale predictors the mjo is used for the intra seasonal scale predictor and awts obtained from dwt probabilities are used to correctly reproduce interannual variability the methodology has been validated for three very different locations to demonstrate its worldwide applicability the marginal distributions of the wave parameters for both the mean and the extreme long term regime closely agree with the historical behaviour of the time series provided for the fitting furthermore for each of the study locations an example of simple applications relevant for shoreline change modelling is presented and used to explain the strengths and limitations of the proposed methodology moreover this emulator could be adapted to generate plausible future wave climate and account for changes in the climate system by changing the probabilities of the large scale covariates that trigger the chronology model credit authorship contribution statement laura cagigal conceptualization methodology software formal analysis visualization writing original draft ana rueda conceptualization methodology software writing review editing dylan anderson conceptualization software writing review editing peter ruggiero conceptualization resources writing review editing mark a merrifield conceptualization resources writing review editing jennifer montaño conceptualization resources software writing review editing giovanni coco conceptualization supervision writing review editing funding acquisition fernando j méndez conceptualization methodology supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge all the groups that have generated and made available all the atmospheric storm surge and wave data used in this study atmospheric data from cfsr are available online at https climatedataguide ucar edu climate data climate forecast system reanaly sis cfsr the mjo data were downloaded from the australian bureau of meteorology at www bom gov au climate mjo the authors also thank katherine serafin for splitting the san diego tide gauge signal into its constitutive components and the cls space oceanographic division for producing the dac database they would also like to thank the coastal data information program cdip at the scripps institution of oceanography for making the buoy data at imperial beach available metocean solutions in nz for providing the waves at tairua beach as part of the shoreshop competition and ih cantabria for providing dow data for the laredo location this work would not have been possible without funding from the strategic environmental research and development program s grant dod serdp rc 2644 lc was also funded by a scholarship from the university of auckland gc was funded by a nz hazard platform grant 3710440 
23951,coastal hazards often result from the combination of different simultaneous oceanographic processes that occur at multiple spatial and temporal scales to predict coastal flooding and erosion it is necessary to accurately represent hydrodynamic conditions for this reason here we present a stochastic climate based wave emulator that provides the hydrodynamic conditions needed for these predictions the emulator can generate an infinitely long data series maintaining its statistical properties at different time scales from intra storm to inter annual variability and its link to large scale climate patterns the proposed methodology relies on the use of weather types and an autoregressive logistic regression model forced with different variables to simulate daily scale chronology considering the dependencies of wave conditions on the different weather types the intra storm chronology is solved by means of shuffling and stretching historical wave sequences to demonstrate the replicability of this emulator worldwide we have applied the model to 3 different locations and found good agreement when compared to the historical data furthermore to illustrate and explain the strengths and limitations of the emulator we present a different application for each of the different locations keywords weather types emulator chronology coastal hazards shoreline 1 introduction weather related coastal hazards e g flooding and erosion result from the combination of large scale oceanographic processes e g storm surge waves and tides despite large variability associated with local effects extreme damage can occur after a sequence of consecutive events e g clusters of extratropical storms or isolated events e g tropical cyclones these events are the consequence of different environmental processes that act at a range of different spatial and temporal scales making it necessary to analyse the multivariate nature of the problem and the dependence between variables leonard et al 2014 hydrodynamic conditions need to be accurately represented to predict flooding and coastal evolution at intermediate scales order of decades from a statistical point of view time series of hydrodynamic variables should be long enough to explore all sources of variability and perform a robust and reliable analysis serafin and ruggiero 2014 furthermore chronology is a key factor when trying to predict coastal hazards such as shoreline erosion since different storm sequencing with the same magnitude can result in very different erosion or accretion e g coco et al 2014 and even lead to multi hazards godoi et al 2017 for these reasons the development of an emulator capable of estimating reliable time series for different wave climates is appropriate when trying to reduce uncertainty of predictions wahl et al 2012 moreover the difficulty of developing reliable future projections leads to the use of wave ensembles as the most reliable approach to improve the range of potential evolutions and to estimate the prediction uncertainty vitousek et al 2017 vitousek et al submitted an international workshop on shoreline prediction shoreshop montaño et al 2020 held in new zealand in june 2018 identified the generation of synthetic time series as a key step to overcoming the difficulty of projecting long term shoreline evolution different statistical emulators of wave climate already exist in the literature davidson et al 2010 use the method of borgman and scheffner 1991 based on a month by month multivariate stationary simulation to synthetically reproduce 1000 time series of wave parameters to forecast seasonal to multi year shoreline change in callaghan et al 2008 a joint probability method is developed to simulate extreme beach erosion statistical downscaling based on weather types wt provides a different flexible framework capable of assessing seasonal to interannual variability guanche et al 2013 this wt framework allows to split the atmospheric circulation into a number of synoptic patterns i e wts climate based emulators have been previously used to assess extremes of waves and storm surge rueda et al 2016 coastal flooding potential anderson et al 2019 wave spectra rueda et al 2017a and long term morphodynamics antolínez et al 2016 furthermore wt based emulators are also useful to predict climate driven changes in different variables under various scenarios of climate change by analysing how global climate models gcms affect occurrence of wts in the future camus et al 2017 perez et al 2015 sheridan and lee 2010 nevertheless most of the emulators presented above have different limitations when computing long term erosion and flooding some of them focus on reproducing only the conditions of extreme storms and not on producing continuous wave time series antolínez et al 2016 callaghan et al 2008 others emulate waves at a daily scale rueda et al 2016 not taking into account the shape of a continuous storm longer than a day which is relevant when trying to synthetically reproduce the erosion produced by an individual event furthermore only the emulator from anderson et al 2019 links the emulated time series to larger scale climatic patterns i e el niño southern oscillation enso madden julian oscillation mjo which is essential to analyse the precursors of large erosion or flooding events predicted from synthetic wave series since large scale climatic patterns can have large impact at the coast barnard et al 2015 the main objective of this work is therefore to develop a climate based emulator capable of generating infinitely long time series of the wave conditions needed to drive shoreline evolution models enabling modellers to assess long term evolution in a probabilistic way the new climate based emulator relies on the use of wts and accounts for not only the historical intra seasonal seasonal and interannual scales but also realistic intra storm chronology which has not yet been achieved by any of the previous works described above the emulator considers the dependencies of wave climate from sea level pressure fields at different timescales and solves the intra storm chronology by means of shuffling and stretching observations of historical wave sequences to demonstrate some of the capabilities of the waves emulator in the context of shoreline evolution the proposed methodology has been applied and tested at 3 locations fig 1 a imperial beach usa b tairua beach new zealand and c laredo beach spain these locations have been chosen as examples of global applicability of the methodology and also because of the availability of wave and storm surge observations to enhance the capabilities of the emulator for other applications such as coastal flooding where storm surge might be a key element rueda et al 2017b the methodology is explained including this additional variable the paper is organized as follows section 2 provides the information of the different databases used section 3 describes the methodology followed to develop the emulator section 4 presents the results of the method and some shoreline applications utilizing the multivariate time series generated at the 3 locations discussion and conclusions are presented in sections 5 and 6 2 data this section summarizes the different datasets used to reproduce the climate based emulator at the three different locations section 2 1 introduces the databases employed to characterize the large scale climate patterns which are the same for the 3 different locations sections 2 2 and 2 3 present the wave and storm surge databases at each of the study sites 2 1 atmospheric data sea level pressure slp fields from the climate forecast system reanalysis cfsr saha et al 2010 are used to define the weather patterns at both annual and daily scales the reanalysis spans from 1979 to present at 0 5 spatial resolution and hourly temporal resolution the data have been subsampled to daily values and 2 spatial resolution for consistency with global climate models gcms rueda et al 2018 as the methodology presented here could also be adapted and used under climate change scenarios in addition to the slp fields we have also used the mjo index wheeler and hendon 2004 which provides information at a daily scale since 1975 the mjo is an intra seasonal oscillation characterized by large scale convective anomalies that propagate around the equator with a 30 to 60 days period madden and julian 1972 mjo is the dominant mode of intraseasonal variability in the tropics dee et al 2011 and influences tropical cyclone generation and intensity liebmann et al 1994 maloney and hartmann 2000 the development of el niño southern oscillation events kessler and kleeman 2000 and extratropical weather cassou 2008 2 2 wave data the wave data are characterized at the three locations using significant wave height hs mean period tm and direction dir the wave data used for imperial beach come from a wave buoy close to the coast at 32 56955 n and 117 16915 w at 21 m depth this station has an hourly temporal resolution and the record spans from 2000 to 2016 in the case of tairua waves at 36 988 s and 175 864 e at 10 m depth were provided by a dynamically downscaled hindcast by means of the hydrodynamic model swan and a larger scale wavewatch iii hindcast the waves have a 3 hourly temporal resolution and span from 1979 to 2017 for our third location laredo the wave data are from a hybrid wave hindcast camus et al 2013 located at 43 4275 n and 3 42627 w at 8 4 m water depth while this record with hourly temporal resolution spans from 1948 to 2015 we only use data from 1979 to match the atmospheric data 2 3 storm surge data the water level data used for the imperial beach storm surge are obtained from the national oceanic and atmospheric administration tide gauge 9410230 located at the pier in la jolla california the levels were then separated into the different components following the methodology developed by serafin and ruggiero 2014 where the water level wl signal is split into a mean sea level component η m s l the deterministic astronomical tide η a the monthly mean sea level anomaly η m m s l a an intra annual seasonal variation η s e and the storm surge η s s 1 w l η m s l η a η m m s l a η s e η s s for tairua the storm surge is obtained from the dynamic atmospheric correction dac reanalysis produced by cls space oceanography division and distributed by aviso carrère and lyard 2003 the outputs of this model span from 1992 to 2014 and are provided at a 6 h temporal resolution on a regular grid of 0 25 0 25 this reanalysis uses the mog2d barotropic model from legos and it is forced by pressure and winds from the european centre for medium range weather forecasts ecmwf in laredo the storm surge data are obtained from the global ocean surge gos reanalysis cid et al 2014 which has an hourly temporal resolution and 0 125 spatial resolution this reanalysis uses the regional ocean model system roms of rutgers university to develop the time series in the southern europe area 3 methodology the climate emulator developed in this work aims to initially predict realistic sequences of daily wts dwts based on large scale slp fields and to use such sequences to predict the parameters needed to drive coastal hazards models while the figures demonstrating the methodology within this section correspond to imperial beach similar agreement between historical and simulated conditions was found for each of the other study sites analysed the general methodology proposed as illustrated in fig 2 is composed of the four different steps explained in detail in the following sections a dwt classification section 3 1 b chronology model used to develop the synthetic time series of dwts section 3 2 c stretching method of the intra storm characteristics section 3 3 d monte carlo shuffling based simulation section 3 4 in order to show which components of the methodology were previously proposed by other authors and which are the original contributions of this work the components have been differentiated in colours in fig 2 grey corresponds to the previously developed work while purple is the new part of the methodology proposed in this paper panel a weather type classification was proposed in camus et al 2014 and later improved in camus et al 2016 and hegermiller et al 2017 the chronology model panel b is based on the approach proposed in anderson et al 2019 although a new methodology for obtaining the annual predictor has been developed leading to new realizations of dwt chronologies panels c and d stretching and shuffling are novel work and the main contributions to the proposed wave climate emulator these approaches focus on solving the intra storm chronology which is essential for driving shoreline models 3 1 weather type classification there are several different methodologies available to develop dwt classifications these can either be subjective lamb 1972 or based on automated algorithms camus et al 2014 which is the approach adopted in this study for this purpose we need to define a predictor area and a clustering technique since swell energy can propagate for thousands of kilometres over several days snodgrass et al 1966 the predictor area fig 1 is defined for each location following the estela evaluation of source and travel time of wave energy reaching a local area method pérez et al 2014 which evaluates the sources and mean travel time of the wave energy reaching a given location using the directional wave spectra from the global ifremer wave hindcast rascle and ardhuin 2013 this predictor has been demonstrated to capture not only wave but also storm surge variability over different temporal scales rueda et al 2018 for the wt classification as described in camus et al 2014 a principal component analysis pca is applied to slp fields and slp gradients inside the area defined by the estela method to obtain the temporal coefficients pcs associated with the dominant spatial variability patterns eofs to include the travel time of swell waves the slp fields and their gradients are modified dependent on the isochrones of the average travel time following hegermiller et al 2017 as 2 p t x y s l p t i ω i s l p g t i ω i f o r i 1 p where ω i represents the spatial domain between isochrones i 1 and i and p is the number of isochrones of the furthest wave generation area in order to reduce dimensionality and preserve the maximum amount of variance the pca modes explaining 95 of the total variance are selected as the predictor for the automated classification the clustering technique chosen for the classification is a regression guided k means which has already been shown to improve the downscaling of storm surge and waves based on wts camus et al 2016 cannon 2012 the classification is performed on a dataset z which concatenates the weighted pca predictor x with the prediction estimations y ˆ from a linear regression model between predictand y constructed with the wave parameters i e hs tm and dir and predictor x 3 z 1 x y ˆ where in this case equal to 0 3 is a factor which varies from 0 to 1 meaning 0 unsupervised classifications and 1 fully supervised classification the k means algorithm hastie et al 2001 is applied to the dataset to obtain a number of clusters 36 that must be a compromise between the number of data per cluster 100 days and a number large enough to describe the range of different climatologies i e wts this is because at a later stage the number of data per cluster will condition the fitting of the distributions relative to the wave parameter the emulator methodology proposed in this work could ultimately be adapted to any number of clusters as those would depend on the local variability of different climatologies worldwide and based on the user criteria the 36 clusters dwts are represented in fig 3 by the sea level pressure anomaly centroid using the data for which the centroid is the closest minimizing the within cluster distance the seasonal probability of dwt occurrence where dfj corresponds to december january february mam to march april may jja to june july august and son to september october november is presented in the lower panel of fig 3 3 2 chronology model the simulation of realistic synthetic dwt sequences is a key issue for assessing the chronology of coastal hazard events such as flooding or erosion where the time interval and severity between storms can amplify a negative outcome the chronology model must be able to capture the probability of occurrence of the dwts the persistence the transition probabilities and the intra seasonal to interannual variability for this purpose and following anderson et al 2019 an autoregressive logistic regression alr model is implemented over the dwts the alr model allows the simulation of synthetic sequences of dwts accounting for different covariates such as seasonality additional predictors and autoregressive or markov terms alr models are also capable of simultaneously dealing with covariates that vary at different time scales considering the autocorrelation between them further information on the model foundations and the implementation of covariates can be found in guanche et al 2014 to reproduce the wave climate at different timescales the model is forced with the first three principal components of an annual predictor explained in detail in section 3 2 1 the seasonality the two principal components of an intra seasonal predictor based on the mjo explained in detail in section 3 2 2 and a third order markov chain in mathematical terms the model is represented as eq 4 given in box i where β 1 i and β 2 i account for the harmonic components of the seasonality for each weather type i ω is the angular frequency β j i a w t accounts for the probability associated to the principal components of the awt β j i m j o accounts for the two principal components of the mjo and the y t h term represents the markov order the validation of the alr against the different covariates included in the model is presented in fig 4 fig 4a corresponds to the validation of seasonality and shows the comparison of 100 simulations of mean dwt probabilities in a perpetual year bottom against the historical dwt probabilities top fig 4b shows the same comparison but for the different annual wts awts section 3 2 1 here it can be appreciated how el niño years awt 6 have a completely different dwts distribution with dwt30 and dwt36 corresponding to large storms see fig 3 being the most probable dwts fig 4c presents the comparison of the dwt probabilities against the mjo active phase and its similar behaviour in the synthetic time series 3 2 1 annual predictor the annual predictor is constructed from the first three principal components of the 36 dwt probabilities aggregated every year here we used the dwts probabilities to construct the annual predictor rather than variables such as sea surface temperature in the equatorial pacific as in anderson et al 2019 to increase the generality of the annual predictor worldwide for this purpose the annual pcs are clustered into 6 different synoptic patterns following a k means algorithm hastie et al 2001 this way a number of independent awt chronologies are computed and the annual pcs needed to force the daily alr are obtained by randomly choosing a triplet pc 1 awt pc 2 awt pc 3 awt from the historical years associated with each of the different awts as we want the daily simulations to include independent annual chronologies for each simulation to explore different annual climatic transitions an alr model forced with a markov chain for the awt is previously computed fig 5 illustrates the construction of the annual predictor at imperial beach we show the first three spatial and temporal components of the pca top three panels and the k means grouping leading to the awt bottom panel the spatial eofs correspond to the 36 dwt probabilities associated with each pc the coloured dots in the temporal series match strong and very strong el niño and la niña years oni index larger than 1 5 the third pc has a strong dependency with el niño modoki of 2010 it can be appreciated that all strong el niño years fall into the same awt awt 6 while strong la niña years fall into the awt 1 this is due to the sensitivity analysis carried out to obtain the appropriate number of principal components to use for the k means clustering we found three pcs to be the optimal number to group el niño and la niña years into the same awt for both imperial beach and tairua in the case of laredo where the signal of the interannual variability is less strong we found that five pcs were needed to homogeneously sample the awts over the years 3 2 2 intra seasonal predictor to preserve intra seasonal variability a predictor based on the mjo is introduced the mjo which has a time scale of 30 to 90 days zhang et al 2009 is important for monthly and seasonal predictions the mjo index used here was defined by wheeler and hendon 2004 and is characterized by the first two pcs of equatorially averaged outgoing longwave radiation and zonal winds based on those two pcs the mjo is classified in 25 different categories based on the active phase from 1 to 8 and 4 different intensities from inactive to extremely active as defined in lafleur et al 2015 and anderson et al 2019 following the same technique as for the annual predictor a number of independent simulations of the mjo category are computed using the alr model forced with seasonality and a third order markov chain as covariates in order to develop the pcs needed to force the daily alr model we randomly pick a combination of pc 1 mjo and pc 2 mjo from the historical record associated to each of the categories from 1 to 25 3 3 wave stretching wave climates are constructed by connecting and shuffling historical chronologies of wave and storm surge conditions associated to a synoptic state defined here as the time interval in which synoptic conditions remain constant thus the duration of each different synoptic state is defined by the number of consecutive days in which the local synoptic pattern is driven by the same dwt this implies that although we are using dwts to define the duration of the different states our approach reproduces synthetic time series at the same temporal resolution as the waves provided as input of the emulator hours in order to reproduce similar storm chronologies i e ramp up peak and ultimate reduction of wave energy as in the past but enhancing the capability of extrapolating the intensity of wave chronologies associated with each synoptic state we develop a stretching technique for both hs and tm the associated historical storm surge level and wave direction are maintained the stretching technique allows broadening in magnitude the hindcast chronology with a minimum and maximum value of the wave conditions defining the synoptic state this way even when simulating the historical chronology shape and duration multiple times the reproduced magnitudes due to the stretching technique can be very different in each simulation refer to fig 2c here we present the number and mean duration of the synoptic states identified at each of the different locations for the case of imperial beach the 6208 days of wave conditions have been divided using the dwt historical chronology into 2571 different states with durations ranging from 1 to 5 days mean duration of 2 4 days for tairua the number of synoptic states found is 6126 with a mean duration of 2 3 days while in laredo 8504 different synoptic states are found with a mean duration of 1 6 days when durations are found to be longer than 5 days the technique randomly divides them into different consecutive pieces from 3 to 5 days the 5 days threshold is based on the need to have enough data to populate the distributions relative to each storm duration and dwt while this threshold can depend on the local storm durations and the number of dwts in our case it has been constant for the 3 locations studied for each wave and storm surge synoptic state the parameters that are stored for each dwt and duration are the following maximum wave height hs max minimum wave height hs min maximum tm tm max minimum tm tm min mean dir dir mean and mean storm surge ss mean in order to apply the stretching technique during the simulation the wave hindcast relative to hs and tm has been normalized with the maximum and minimum values above for each synoptic state the joint probability between the different variables is then reproduced using a multivariate gaussian copula for each dwt as copulas have been shown to correctly model multivariate problems ben alaya et al 2014 nelsen et al 2006 rueda et al 2017a copulas are able to emulate realistic combinations of variables based on the historical probabilities to accomplish this the wave height and period parameters are fit to a kernel distribution to allow a small capability of extrapolation of extremes maintaining the historical general behaviour while wave direction and storm surge are fitted to an empirical distribution the marginal functions are then transformed into a normal distribution and the dependences are modelled by a symmetric positive definite matrix of spearman s correlation coefficients in the gaussian space the copulas are then used to randomly derive waves and storm surge values from the appropriate dwt copula for each synoptic state during the monte carlo simulation section 3 4 3 4 waves and storm surge simulation the waves and storm surge simulation is performed by connecting and shuffling historical chronologies associated to each synoptic state once we have constructed the synthetic time series of the dwts the monte carlo simulation of the wave and surge parameters associated with each synoptic state is performed as explained before a stretching technique is carried out for the wave height and period while the direction and storm surge are filled with the corresponding chronology of historical values this means that for each synoptic state the following parameters are randomly extracted from the gaussian copula associated with that specific duration and dwt hs max hs min tm max tm min dir mean and ss mean then we find a historical chronology that best matches the mean direction and mean storm surge extracted from the copula and use the wave height and period parameters to stretch the normalized hindcast as shown in fig 2c a filter was introduced into the gaussian copula to simulate pairs of hs and tm that fall inside the same physical steepness limits of the historical data this shuffling approach to wave and storm surge sampling allows us to increase the temporal resolution beyond the daily resolution given by the dwts to the temporal resolution of the original data provided as an input to develop the emulator 4 results 4 1 validation of long term and extreme value distributions of wave parameters following on with the imperial beach example fig 6 shows the historical distribution of wave parameters hs tm and dir for the different dwts and the validation of the synthetic wave time series against the buoy data fig 6a shows the hindcast data and a 100 year simulation of synthetic wave parameters it is clear how the large storm dwts associated with the winter months shown in fig 3 closely agree with the larger wave height and period distributions fig 6b and c predominantly approaching imperial beach from the west and north west directions fig 6d furthermore figs 3 and 6 show how the approach is able to discern between waves generated by large scale conditions versus those generated close to the coast for example dwt20 fig 3 is associated with pressure anomalies both in the southern hemisphere and close to the san diego coast and this pattern is reflected in the wave period and direction histograms fig 6c and d 2nd row 4th column which show a bimodal behaviour these relationships as expected from the kernel and empirical distributions used for the fitting are maintained in the simulation demonstrating a good agreement between the historical and synthetic wave height period and direction the same behaviour was found for storm surge distributions not shown fig 6e illustrates the gaussian copula correlation coefficient between the parameters hs max and tm max of the synoptic states that as expected has positive values for all the dwts with higher correlation values for the dwts associated with the largest waves a detailed comparison of the historical distribution versus the 100 synthetic simulations is carried out via quantile quantile plots as shown in fig 7 a general agreement is found between the historical data and the simulations which are represented by the mean the variability envelope and the standard deviation across simulations the variability is in general larger at the tails of the distributions although in the case of wave direction this variability can also be large in the middle due to the bimodal behaviour in some dwts fig 6d in order to evaluate the performance of the emulator in reproducing extreme synthetic wave parameters the return periods of hs and tm have been compared against historical data fig 8 the simulations correctly reproduce extreme waves over the different return periods at each of the study sites as exemplified in the results for imperial beach the behaviour of the return periods for both hs and tm after reaching the maximum value of the historical record is mostly flat this is due to the fact that the emulator is meant to reproduce the historical behaviour of the data with just a small capability of extrapolation over larger return periods due to the kernel and empirical distributions used during the stretching section 3 3 4 2 applications relevant to shoreline change nearshore hydrodynamic processes play a major role in shaping shoreline evolution at seasonal to interannual scale ciavola and coco 2017 here we analyse the waves generated by the climate based emulator presented at the 3 different study sites by means of 3 different applications relevant for shoreline evolution the implications of using the synthetic waves from the proposed emulator in the calculation of the longshore wave power lwp at imperial beach usa are presented in section 4 2 1 in section 4 2 2 we analyse the implications on cross shore shoreline variations at tairua new zealand while in section 4 2 3 we will analyse the directional component of beach rotation at laredo spain 4 2 1 longshore wave power imperial beach longshore sediment transport is primarily controlled by the lwp component on dissipative and relatively alongshore uniform beaches here we present a historical analysis of the lwp relative to the different awts as defined in section 3 2 1 against the same analysis using the simulated waves the formula used for computing the lwp as defined in komar 1998 reads 5 p e c n sin θ cos θ 1 8 ρ w g h 2 c n sin θ cos θ where e is the total energy c is the wave speed n is the ratio of group to individual wave speed and θ is the angle between the wave direction and shore normal assuming a constant 178 beach orientation the cumulative annual wave power is shown in fig 9 the left panel corresponds to the historical period from 2000 to 2016 where colours represent the different awts we have only named those which fall outside the historical standard deviation represented in grey el niño years represented by awt 6 produce a large lwp anomaly compared to the yearly average as can be seen in 2009 10 and 2015 16 in southern california the shift in direction to the north by 6 during the 2009 2010 el niño event was equivalent to the one occurring in 1997 98 causing the greatest winter shoreline retreat in the southern california survey record barnard et al 2011 in 2015 2016 the shoreline retreat in southern california was four times bigger than in previous years barnard et al 2017 and imperial beach was highly eroded and coastal flooding occurred young et al 2018 similar lwp behaviour during el niño years was observed offshore of northern oregon anderson et al 2018 as opposed to el niño during la niña years more southerly waves approach the coast causing a lower lwp this historical behaviour is well captured during the simulations as shown in fig 9b where el niño awt 6 is associated to the largest lwp while la niña awt1 has the lowest values the rest of the awts have a similar overlapping area suggesting that years without an active el niño or la niña present similar lwp variability throughout the year even when the probabilities of the different dwts with that awt can be very different lines correspond to the mean lwp for each awt while the shaded area comprises the area between the maximum and minimum lwp values for each awt this behaviour confirms that the methodology correctly reproduces the interannual variability of the area and that the simulations can be linked to large scale climatic patterns such as el niño southern oscillation this way the small number of historical el niño and la niña years can be extended with the synthetic series to develop probabilistic assessment of the impact of this phenomenon 4 2 2 cross shore shoreline variation tairua beach at seasonal and interannual timescales shoreline evolution can be dominated by cross shore processes kriebel and dean 1985 miller and dean 2004 here a calibrated equilibrium shoreline change model yates et al 2009 has been used to explore shoreline evolution at tairua beach this model bases its predictions on the shoreline position disequilibrium concept wright et al 1985 and uses hs as the only wave parameter it has been successfully implemented at locations where and when gradients in alongshore sediment transport do not play a significant role castelle et al 2014 dodet et al 2019 splinter et al 2014 the model in tairua has been calibrated with 18 years of daily averaged alongshore shoreline position hereinafter referred to as shoreline data derived from a camera system blossier et al 2016 montaño et al 2020 details on the yates model calibration and implementation can be seen in montaño et al 2020 fig 10 compares the modelled shoreline annual minima erosion and maxima accretion return periods for 100 synthetic wave simulations of 100 years each in shaded area against shoreline annual minima and maxima return periods modelled using the historical wave database the annual shoreline maximum accretion is more or less constant over time for both the model results driven by the historical waves as well as those using simulated waves for erosion events historical larger variations within different years produce larger differences between the model results driven by the 100 wave simulations fig 10 good agreement between erosion modelled using historical and simulated waves for both erosion and accretion for less than 10 years return period is found while an underestimation of the erosion for larger return periods is observed this may be due to an issue with the fact that we are reproducing storms lengths determined by the dwt classification that can sometimes split the same storm into different pieces while large erosion periods usually require larger storm durations however we stress that the underestimation does not mean that we are reproducing less erosion during larger events but rather less frequent erosional events i e associated with larger return periods 4 2 3 beach rotation laredo beach rotation occurs at seasonal to interannual timescales triggered by wave energy events with a strong alongshore component compared to the mean wave direction van de lageweg et al 2013 here we analyse and compare the historical wave direction record against simulated wave directions at laredo spain fig 11a presents the annual mean direction for the historical record in black and 10 different simulations in colour the range of annual variations in the simulations is similar to the historical data with mean directions from 28 to 34 for all the 100 wave simulations performed with a mean historical direction of 31 25 mean and standard deviation values for the historical record and the simulations are included fig 11 there is only a very small difference between the historical mean and the simulated mean with the simulations giving slightly lower standard deviation than the historical waves the numbers in brackets correspond to the maximum and minimum values of the 100 simulations the lower panels correspond to the analysis of the number of consecutive years with a mean direction dir y over fig 11b and below fig 11c the average of the 37 years mean direction dir m y the comparison of the scarce number of historical wave records against the 100 simulations of more than 100 years results in a good agreement in particular with respect to the most probable durations for both the positive and negative annual wave direction anomalies the largest difference is due to the emulator defining the tails of the distributions which is a consequence of the large number of synthetic annual simulations a small probability of having up to 10 consecutive years with a positive anomaly or up to 8 consecutive years with a negative anomaly is introduced in the synthetic simulations while in the historical records the maximum persistence is 5 years these results imply that our emulator can be used to explain beach rotation with anomalous directions correctly reproducing the historical persistence distributions moreover given the fact that we are reproducing also the tails of the persistence distribution new rotation periods not registered in the historical data could be created albeit with very low probability of occurrence 5 discussion previous studies have proposed varying methods to generate wave climates to solve different problems in coastal science callaghan et al 2008 proposed a methodology to statistically simulate storm events to derive extreme beach erosion by reproducing storms with a duration and hs larger than a threshold and fit a poisson distribution to the spacing between storms while the callaghan et al 2008 approach is useful for applications focused on extreme isolated events it cannot produce continuous shoreline evolution as morphologic change models necessitate a continuous dataset to accumulate erosion and accretion periods over time in the work of davidson et al 2010 on forecasting seasonal to multi year shoreline change the wave climate emulator from borgman and scheffner 1991 is used this piecewise month by month emulator seeks to preserve univariate probability laws and the first and second order moment properties describing the intercorrelations of the data sequencing this approach does not preserve higher order moments between the variables and between different time and space elements in the data this means that interannual variability will not be well resolved between simulations and it would not be possible to link the data with larger scale climatic patterns as in the lwp application discussed in section 4 2 1 antolínez et al 2016 modelled long term morphodynamics by reproducing the persistence and sequencing of the storms by clustering different sea states the model accounted for seasonal and interannual variability by means of an alr model similar to the one described in section 3 2 but without taking into account larger temporal scale climatic predictors the method was also restricted to provide a reduced number of categories of sea states not reproducing the entire complexity of the wave climate the extreme wave and storm surge climate emulator from rueda et al 2016 and the multiscale climate emulator of multimodal wave spectra from rueda et al 2017a reproduce waves at a daily resolution by means of a wt classification and a chronology model based on markov probabilities however these approaches do not properly address the shape of synoptic states occurring for longer than a day a key need for models driving shoreline change none of the models above link the wave synthetic simulation with larger scale climatic predictors i e annual predictor intra seasonal predictor this link is relevant to explain the precursors of large erosion and or flooding events in the simulations furthermore considering multiple concurrent climate patterns is essential to develop more reliable coastal planning godoi et al 2019 the work from anderson et al 2019 was the first to our knowledge to include large scale climatic predictors and to account for intra storm chronology our emulator is developed on the same basis as it also depends on a wt technique that relies on finding the relationship between large scale climate patterns at different time scales and the local wave and storm surge parameters the main differences are the definition of the annual predictor and the shuffling and stretching of historical wave conditions in our case the annual predictor is defined by using the annual average of the dwt probabilities while in anderson et al 2019 they rely on a predictor based on the sea surface temperature this approach works well in areas in the pacific where the signal from el niño and la niña is very strong however it is not easily transferable to areas outside the pacific our annual predictor is still capable of discretizing between el niño and la niña years as shown in fig 4 at imperial beach with regard to the shape of the waves associated with each synoptic state anderson et al 2019 simulate the values of hs t dir and storm surge for a synoptic state defined in length as the number of days within the same wt they then reconstruct the shape over time with a constant value for t dir and storm surge and with a geometric hydrograph shape for hs another advantage of our method is that we reproduce more realistic storm shapes where t dir and storm surge have also their own progression over time this has been accomplished by directly using historical evolutions of hs t dir and storm surge and applying the stretching technique described in section 3 3 to hs and t although our climate emulator is transferable worldwide and has a large range of possible applications for modelling and assessing erosion and flooding accounting for different time scales from intra storm to interannual variability it also has some limitations the use of the dwts to identify the length of the synoptic states and the 5 days restriction due to the need to have enough data for each dwt and storm duration for the fitting has some effect when assessing events such as erosion very sensible to the storm duration this limitation is evident in the erosion in tairua shown in fig 10 where we see some underestimation on the larger erosion return periods even when the return period associated to the maximum waves has been validated as shown in fig 8 for these reasons further improvement in the way we define the synoptic states and their different constituents will be needed to improve the performance in situations where the effect of those limitations is important in addition although this methodology can be very flexible in terms of the wave and storm surge data provided as input of the emulator the fact that we are using historical chronology shapes and empirical and semi empirical distributions for the fitting of the parameters with a small capability of extrapolation implies that the longer the records provided the better the model will be in explaining the complexity of the wave climate at a given location 6 conclusions a climate based stochastic wave and emulator is presented for defining time series of boundary conditions for coastal applications it is capable of generating infinitely long and continuous time series statistically similar to the original data provided for the fitting maintaining its properties at different time scales from intra storm to interannual and its link to relevant large scale climate predictors this emulator relies on the use of dwts as a flexible framework for evaluating storm chronology at a daily scale by means of an alr model forced with larger time scale predictors the mjo is used for the intra seasonal scale predictor and awts obtained from dwt probabilities are used to correctly reproduce interannual variability the methodology has been validated for three very different locations to demonstrate its worldwide applicability the marginal distributions of the wave parameters for both the mean and the extreme long term regime closely agree with the historical behaviour of the time series provided for the fitting furthermore for each of the study locations an example of simple applications relevant for shoreline change modelling is presented and used to explain the strengths and limitations of the proposed methodology moreover this emulator could be adapted to generate plausible future wave climate and account for changes in the climate system by changing the probabilities of the large scale covariates that trigger the chronology model credit authorship contribution statement laura cagigal conceptualization methodology software formal analysis visualization writing original draft ana rueda conceptualization methodology software writing review editing dylan anderson conceptualization software writing review editing peter ruggiero conceptualization resources writing review editing mark a merrifield conceptualization resources writing review editing jennifer montaño conceptualization resources software writing review editing giovanni coco conceptualization supervision writing review editing funding acquisition fernando j méndez conceptualization methodology supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge all the groups that have generated and made available all the atmospheric storm surge and wave data used in this study atmospheric data from cfsr are available online at https climatedataguide ucar edu climate data climate forecast system reanaly sis cfsr the mjo data were downloaded from the australian bureau of meteorology at www bom gov au climate mjo the authors also thank katherine serafin for splitting the san diego tide gauge signal into its constitutive components and the cls space oceanographic division for producing the dac database they would also like to thank the coastal data information program cdip at the scripps institution of oceanography for making the buoy data at imperial beach available metocean solutions in nz for providing the waves at tairua beach as part of the shoreshop competition and ih cantabria for providing dow data for the laredo location this work would not have been possible without funding from the strategic environmental research and development program s grant dod serdp rc 2644 lc was also funded by a scholarship from the university of auckland gc was funded by a nz hazard platform grant 3710440 
23952,based on an eddy resolving regional ocean modeling system roms and the conditional nonlinear optimal perturbation cnop method we set up a nonlinear optimization system to investigate the effects of the fastest growing initial errors on the prediction of the kuroshio extension ke transition process from the stable to the unstable state the results reveal that the larger values of the cnop type fastest growing initial errors are mainly located upstream of the ke over the upper 500 m furthermore the dynamic mechanism of the error growth is explored in terms of error movement maintenance and development during the ke transition period the results indicate that the errors are transported eastward from the south of japan to the ke s first quasi stationary meander by linear advection the nonlinear advection slows down the moving speed of these errors leading to their long term existence in the first quasi stationary meander of the ke in this situation errors have enough time to absorb energy from the background field continuously through barotropic energy conversion ultimately errors grow rapidly and further induce the overestimation of the ke strength keywords kuroshio extension predictability initial error 1 introduction the kuroshio extension ke is a strong eastward flowing current originating from the eastern coast of japan in the western north pacific ocean mitsudera et al 2004 qiu and chen 2005 pierini 2006 qiu et al 2014 on decadal time scales it exhibits a well defined low frequency variability between a stable state and an unstable state when in the stable state the ke is characterized by two well developed quasi stationary meanders an enhanced southern recirculation gyre and a reduced eddy kinetic energy eke level in contrast barely discernible quasi stationary meanders a weaker southern recirculation gyre and an elevated eke are observed when the ke switches to the unstable state qiu and chen 2005 2010 taguchi et al 2007 qiu et al 2014 the ke state transition has been recently found to leave significant imprints on the atmospheric circulation through strong air sea interactions frankignoul et al 2011 masunaga et al 2016 révelard et al 2016 besides it also influences the modification of the oceanic water mass and the salinity variations on seasonal and decadal time scales qiu and chen 2011 nan et al 2015 kitamura et al 2016 geng et al 2018 these changes impact in turn local fisheries and marine ecosystems nishikawa and yasuda 2011 chiba et al 2013 given their importance significant research efforts have been invested in studying the dynamics and predictability of the ke decadal variability dynamic mechanisms with the potential to cause the ke state transition have been extensively investigated qiu and chen 2005 2010 demonstrated that the ke transition is caused by the westward propagating baroclinic rossby wave which is generated by the basin scale wind stress curl anomalies in the eastern north pacific ocean at different phases of the pacific decadal oscillation pdo or the north pacific gyre oscillation npgo pierini 2006 and pierini et al 2009 suggested that the intrinsic oceanic variability plays an essential role in determining the dynamic states of the ke based on a reduced gravity shallow water model moreover reconciling the above viewpoints taguchi et al 2007 and pierini 2014 indicated that the ke decadal variability is likely due to nonlinear intrinsic oceanic adjustment triggered by the westward propagating rossby wave these studies provide theoretical guidance for the simulation and prediction of the ke state transition in recent years studies on the predictability of the ke decadal variability have been focused on two aspects the estimate of predictability time limit and the influence of initial errors on the prediction results from the perspective of wind stress forcing in the eastern north pacific ocean related to pdo or npgo nonaka et al 2012 and qiu et al 2014 suggested that the predictive time limit of the ke state transition is about 3 6 years however nonaka et al 2016 demonstrated that the potential predictability of the ke state transition is significantly affected by the nonlinear intrinsic variability of the ocean in fact this intrinsic variability may influence the predictive results by promoting the rapid growth of an initial error in the numerical prediction hence some researchers explored the effects of the initial errors or perturbations on the ke state transition wang et al 2017 based on the singular vector sv method and a shallow water model found that the error growth for different transition phases of the ke is affected by different physical processes however the ke state transition is a nonlinear process to overcome the linear approximation of the sv method zhang et al 2017 used a nonlinear method conditional nonlinear optimal perturbation cnop mu et al 2003 wang et al 2020 and a shallow water model to explore the triggering mechanism of the ke transition from the unstable to the stable state from the viewpoint of the initial optimal precursors moreover wang et al 2017 indicated that when the ke transits from the stable to the unstable state its predictability is lower than that of the opposite ke transition process therefore in this study we employ the cnop method to investigate the predictability related to initial condition error the first kind predictability the model sensitivity of inaccurate initial conditions lorenz 1975 chu 1999 during the ke transition from the stable to the unstable state generally the purpose of such a predictability study is to analyze the reasons and mechanisms yielding the uncertainties of the prediction results and to explore the methods to reduce these uncertainties mu et al 2004 hence we study the error growth dynamics to explore the factors that cause uncertainties in the model solution which provides a theoretical guidance for the design of ocean observation strategies davies and didone 2013 to reduce these uncertainties to achieve the above objective an important premise is to reproduce the low frequency variability of the ke for this purpose we choose a realistic eddy resolving regional ocean model the regional ocean modeling system roms which includes complex topography and comprehensive physical processes and has previously been applied to simulate the kuroshio large meander and the upstream kuroshio liu et al 2018 zhang et al 2016 furthermore a key technical issue is to set up a nonlinear optimization system based on the cnop method to find the initial error that has the largest impact on the ke transition process to the best of our knowledge this is the first attempt to apply the predictability of the ke state transition a large dimension predictability research to the high resolution ocean model the reminder of this paper is organized as follows the configuration and simulation of roms as well as the cnop method and its application to the predictability study of the ke state transition are described in section 2 the results presented in section 3 include the structures impacts evolution and growth mechanism of the fastest growing initial errors finally in section 4 we draw the conclusions and discuss the results 2 model and method 2 1 model configuration roms is a free surface terrain following ocean model based on primitive equations widely used for regional simulation song and haidvogel 1994 haidvogel et al 2000 in this study roms is used to simulate the decadal variability of the ke and its adjoint model moore et al 2004 is adopted to search for the fastest growing initial errors affecting the ke state transition the model domain is set to a large part of the north pacific ocean basin 14º 57º n 117º e 125º w as shown in fig 1 it has a 1 10º horizontal resolution for the entire domain and 30 sigma vertical levels in this study we focus on the first kind predictability to address this problem we assume that the model is perfect and there are no errors in the atmospheric forcing and lateral boundary fields under these premises we conduct a hindcast experiment by using the time varying forcing and lateral boundary fields to simulate the ke decadal variability the simulation results provide more realistic initial fields in accordance with the model dynamics for the predictability study of the ke transition process for the hindcast experiment the atmospheric forcing fields include shortwave radiation flux net heat flux wind stress sea surface salinity sss sea surface temperature sst and the kinematic surface net heat flux sensitivity to sst dqdsst here the model sst and sss are restored to the monthly mean sst and sss of the forcing field with a restoring time scale of 30 days the dqdsst is used to correct the model surface net heat flux during the restoration process the sss sst and the latent heat flux sensible heat flux long wave radiation flux and short wave radiation flux used to calculate net heat flux are obtained from the national centers for environmental prediction ncep climate forecast system reanalysis cfsr https rda ucar edu datasets ds093 2 saha 2010 the wind stress is calculated using wind speed at 10 m from the national centers for environmental prediction national center for atmospheric research ncep ncar reanalysis 1 https www esrl noaa gov psd data gridded data ncep reanalysis html kalnay et al 1996 to compute the dqdsst we need the surface atmospheric temperature surface atmospheric density sea level specific humidity and the wind speed at 10 m all variables are taken from the ncep cfsr expect for the wind speed because of the time limitations of the necp cfsr data we choose the forcing fields datasets recorded from 1979 to 2008 furthermore a control experiment is conducted to investigate the predictability of the ke transition from the stable to the unstable state however the sst and sss restoration may affect the growth of the initial errors and further causes the final results not to reflect the real variations of these errors hence we switch off the restoring of the sst and sss in the control experiment as such the atmospheric forcing fields do not include sss sst and dqdsst but contain net freshwater flux from ncep cfsr all other variables are the same as those of the hindcast experiment the model adopts warm starts and the initial temperature salinity velocity and sea surface height ssh in january 1979 are taken from simple ocean data assimilation soda 2 2 4 dataset http iridl ldeo columbia edu sources carton giese soda carton and giese 2008 the variables of boundary conditions are the same as those of the initial field and their time range corresponds to that of the forcing field all the lateral boundaries are opened except for the north side with the above settings the model is integrated over 30 years 1979 2008 for the first 8 years we use the soda dataset to nudge similar to data assimilation temperature salinity and velocity to produce a dynamically stable initial field close to the observation for integration over the subsequent years here we analyze the model outputs of the last 20 years 2 2 model results and validation the roms simulated standard deviation of ssh and the climatological ssh averaged for 20 year model outputs are shown in fig 2a the roms simulation captures the main features of the ke system with the obvious variations of ssh the two quasi stationary meanders and the ke southern recirculation gyre compared with the corresponding satellite altimetry results aviso archiving validation and interpretation of satellite oceanographic altimetry http opendap aviso altimetry fr aviso altimetry 2009 averaged from 1993 to 2012 fig 2b the simulated ssh variations are weaker in addition the simulated mean ke meanders are limited in their meridional amplitude and their position is slightly westward which leads to a shorter ke length fig 2c and d present the variations of the 20 year mean surface eke for the roms hindcast and the observation result respectively significant changes in eke occur mainly in the ke upstream for both sets of data presented but the variation amplitude of the simulated eke is weaker than that of the observed although there are differences between the model and observations roms can capture the essential characteristics of the ke system under a time varying monthly forcing to further examine the model s ability to reproduce the ke bimodal state in detail we compute four indices usually used to represent the bimodal state of the ke jet qiu and chen 2005 2010 qiu et al 2014 based on the simulated results these indices include the upstream ke path length integrated from 141º to 153º e the ke strength represented by the ssh difference across the ke jet averaged from 140º to 165º e the intensity of the ke southern recirculation gyre and the kinetic energy integrated in the ke region 32º 38º n 140º 165º e over the upper 1000 m here the intensity of the ke southern recirculation gyre is defined by r t s s s h x y t d s where s represents the region south of the ke jet from 141º to 158º e within the ssh exceeding 0 4 m fig 3 shows the time series of these four indices the blue lines denote the ke stable periods when the indices are larger than their respective average except that the upstream ke path length is less than its average the upstream ke path length of the ke stable state is shorter than that of the unstable state the ke strength and the intensity of the ke southern recirculation gyre are stronger in the stable state of the ke and the kinetic energy is larger than that during the ke unstable state although there are some differences between the simulation and the aviso results fig 3 of qiu et al 2014 of the above indices the simulated time series of these four indices indicate the obvious characteristics of the ke bimodal state moreover we assign the average of the above four indices after normalization with the reverse sign for the upstream ke path length to the ke index fig 4a as similarly defined by qiu et al 2014 the positive and negative ke indices denote the ke stable blue lines in fig 4a and unstable states respectively the ke index average is zero the temporal changes of the ke index successfully capture the ke decadal modulations and the years corresponding to the two ke states blue lines in figs 3 and 4a basically coincide with the observation results documented by qiu and chen 2005 2010 and qiu et al 2014 hence the differences between simulation and observation obtained for the climatological ke system do not affect the low frequency variability of the ke to a large extent the results of the roms are valid laying the foundation for the predictability research of the ke state transition using this model 2 3 cnop method and its application the cnop method is employed to search for initial errors leading to the largest prediction error at time t mu et al 2003 wang et al 2020 it is a maximum problem and thus differs from the four dimensional variational method belonging to the minimum problem bannister 2001 here we briefly recall the cnop method the solution of the nonlinear model at time t can be written formally as the following 1 x t m t x 0 where x 0 is the initial state vector propagated by the nonlinear propagator m t to state vector x t at time t suppose that x 0 is the initial error then eq 1 becomes the following 2 x t x t m t x 0 x 0 where x t represents the nonlinear evolution of the initial error x 0 at time t to find the fastest growing initial error with the greatest impact on the forecast results at time t a nonlinear constraint optimization problem is defined as follows 3 j x 0 δ max x 0 a δ j x 0 max x 0 a δ m t x 0 x 0 m t x 0 b 2 where j x 0 denotes the objective function in region b x 0 a δ is the initial error constraint in region a and δ is a positive constraint radius x 0 δ is the fastest growing initial error that causes the largest nonlinear evolution at time t with a given δ and it is represented by the cnop regions a and b are marked in fig 1 to obtain the cnop we establish a nonlinear optimization system based on the spectral projected gradient 2 spg2 optimization algorithm birgin et al 2000 the flow chart for calculating cnop is shown in fig 5 for this algorithm we need the value of the objective function at time t and the gradient information of the objective function at the initial time these can be calculated with the roms nonlinear model and adjoint model respectively the initial condition of the adjoint model is the objective function gradient at time t considering that the kinetic energy is an important indicator to measure the ke state transition we define the objective function as the kinetic energy of errors at time t over the upper 1000 m in the region b 32º 38º n 141º 153º e in which the ke state transition occurs as such the objective function can be expressed by the following equation 4 j m t x 0 x 0 m t x 0 b 2 1 2 u t 2 v t 2 d x d y d z where u t and v t denote the zonal and meridional velocity errors respectively at time t furthermore we choose the sum of the quadratic errors nondimensionalized and normalized by the standard deviation as the initial constraint condition li et al 2014 liu et al 2018 this sum represents the errors of the kinetic and potential energy and includes all the possible errors affecting the ke transition process it can be written as follows 5 x 0 a u u s t d 2 v v s t d 2 t t s t d 2 s s s t d 2 η η s t d 2 where u v t s η and u s t d v s t d t s t d s s t d η s t d represent the errors and standard deviation of zonal velocity meridional velocity temperature salinity and ssh respectively within region a 25º 42º n 130º 170º e to reduce the optimization dimension and improve the optimization efficiency we do not superimpose the initial error on the entire model domain considering that the ke state transition is a short term process wang et al 2017 the errors from the eastern north pacific ocean do not have enough time to affect it during the period of the transition nonaka et al 2012 qiu et al 2014 therefore it is physically appropriate to superimpose the initial error only on the region a moreover the constraint radius δ is set to 1 1 0 8 based on the practical measurement accuracies of the used variables in addition this optimization system requires a suitable background state because we focus here mainly on the effects of the cnop type initial errors on the ke transition from the stable to the unstable state this type of transition process is selected as the background state fig 4a and b show the simulated time series of the ke index and filtered ke index tendency obtained with the hindcast experiment respectively the tendency changes exceed by their 1 5 times positive and negative standard deviations indicating a remarkable ke transition process accordingly the gray bands in fig 4 indicate the two prominent ke transition phases from the stable to the unstable state and the duration of these two obvious transition processes is approximately 300 days hence we set the optimization time to 300 days for convenience these two processes are designated by case 1 and case 2 hereafter note that the simulated ke transition years are in accord with the observed ones see the yearly ke paths in fig 2 presented by qiu et al 2014 moreover the results of the hindcast experiment at the initial time of case 1 and case 2 provide more realistic initial fields for the control experiment further to check whether roms can simulate the ke transition process in the control experiment we compare the ke index of the hindcast and control experiments fig 4a black and green lines respectively the variation trends of the ke index of these two experiments for cases 1 and 2 are similar with their correlation coefficients of 0 78 case 1 and 0 77 case 2 at a 95 confidence level this indicates that the control experiment can simulate the ke transition well hence we can utilize the simulated results of the control experiment as the background state 3 results 3 1 spatial structures and effects of initial errors based on the nonlinear optimization system described above we obtain the cnop type initial errors for each case the spatial distributions of the cnop type initial errors including the information of all possible initial errors i e zonal velocity meridional velocity temperature salinity and ssh errors integrated over the whole vertical layers according to the initial constraint norm in eq 5 are presented in fig 6a and b it can be seen that the large amplitude of the cnop is mainly located in the ke region 32º 38º n 141º 160º e especially around the ke s first quasi stationary meander and there is a relatively small amplitude of the cnop in the south of japan compared with that in the ke region in addition we find that the vertical structures of the cnop type initial errors shown in fig 6c and d are evident over the upper 500 m from the spatial and vertical distribution of the initial errors we infer that the initial errors in the ke region over the upper 500 m significantly influence the ke transition from the stable to the unstable state of course some small differences exist in the initial error patterns for the two cases this indicates that the spatial structures of the initial errors are related to a certain extent to the selected background states in addition owing to the nonlinearity of the ke system the gradients calculated by the adjoint model may not be strictly accurate although the results obtained by using the spg2 algorithm can converge to the cnop solution normally moreover for a high dimensional optimization problem associated with this predictability study it is difficult to ensure that the long term optimization result is optimal for the cnop calculation as it cannot be mathematically proven hence in the practical calculation process we test the optimality of each detected initial error through two groups of numerical experiments considering the huge computation load each group only takes 10 members for the first group exp1 we superimpose 10 random errors on the cnop type initial errors generating 10 new initial errors according to the magnitudes of the cnop type initial errors the random errors of velocity and temperature range from 0 to 0 1 and those of salinity and ssh range from 0 to 0 01 for the second group exp2 we obtain 10 new initial errors by making a difference between the variables of two adjacent output time points of the hindcast experiment output time interval is 15 days the amplitude of the 20 initial errors newly generated is the same as that of the cnop type initial errors we superimpose these initial errors on the initial field x 0 in eq 1 and then run the nonlinear model with new initial conditions we calculate their respective cost function values and for both cases we find that they are smaller than those caused by the cnop type initial errors fig 7 in addition during the calculation of the cnop type initial errors we find that different initial guesses lead to similar spatial structures of the final optimization results these indicate that the obtained cnop type initial errors in this study are robust furthermore to explore the effects of the cnop type initial errors on the prediction of the ke state transition we superimpose the initial errors on the initial fields and then integrate the nonlinear model over 300 days fig 8 compares the ssh distribution at the initial time day 0 and the end of the optimization time day 300 without and with the cnop type initial errors a larger smaller magnitude of the ssh south of the ke jet denotes a stronger weaker southern recirculation gyre which yields an enhanced weakened ke strength because of the larger smaller ssh difference across the ke jet particularly for case 1 the ke is in the stable state at the initial time with a strong southern recirculation gyre fig 8a after 300 days the ke becomes the unstable state with a weak recirculation gyre fig 8b however at this time the ke is accompanied by an enhanced southern recirculation gyre after superimposing the initial errors fig 8d the situation is similar for case 2 although the ke southern recirculation gyre at day 300 is slightly stronger than that in case 1 these imply that the cnop type initial errors lead to the strengthening of the ke at the end of the optimization time however in the reference field the ke state is unstable and relatively weak at the end of the optimization time as a result the strengthening of the ke caused by the initial errors means that the ke transition process from the stable to the unstable state is suppressed it should be mentioned that we did not find the initial errors promoting the ke state transition the possible reason is as follows the inhibition of initial errors to the ke state transition means that the ke state is relatively stable compared with that of the reference field the ke unstable state at the end of the optimization time in contrast the intensification of initial errors to the ke state transition means that the ke state is more unstable than that of the reference field at the end of the optimization time the differences between the ke relative stable and unstable states may be greater than those between the two similar unstable states of the ke because the optimization algorithm always tries to seek the initial errors causing the maximum of the objective function the calculated initial errors tend to result in the relative stable state of the ke at the end of the optimization time this means that the initial errors will inhibit the ke transition process from the stable to the unstable state 3 2 evolution processes of initial errors to explore how cnop type initial errors affect the ke transition we investigate their evolution processes according to previous studies qiu and chen 2010 pierini 2014 the ssh anomalies sshas the deviation from the climatological monthly ssh south of the ke jet play an important indicative role in the ke state especially near the region located south of the ke s first quasi stationary meander thus this study presents the ssh error sshe caused by the initial error fig 9a and b demonstrate the time series of the regional average sshe south of the ke s first quasi stationary meander 33º 35º n 141º 145º e the positive sshe develops rapidly near the first quasi stationary meander from day 200 to day 300 for both case 1 and case 2 fig 9c and d show the evolution processes of the sshe we find that the development of positive sshe is accompanied by its eastward movement from the southern region of japan ultimately this positive sshe moving to the ke s first quasi stationary meander causes the ke strength to be enhanced next we examine what leads to a positive sshe south of the ke s first quasi stationary meander during the error evolution period with the relative vorticity budget analysis fig 10 illustrates the evolution process of the relative vorticity errors showing that the negative vorticity errors come from the south of japan corresponding to the evolution of sshe in fact the errors in this region are caused by the initial errors naturally the question then is what leads to variations in the negative relative vorticity error to address this problem we calculate the following budget equation of the anomalous relative vorticity under a quasi geostrophic qg assumption 6 ξ t u g ξ u g ξ f 0 u a β v g u g ξ a b c d e f where ξ is the relative vorticity u g u g v g 0 is the geostrophic velocity vector u a u a v a w denotes the ageostrophic component f 0 is the average coriolis parameter in the ke region and β is the meridional derivative of the coriolis parameter the overbar represents the background state that is not affected by the initial errors and the prime denotes the error field caused by the evolution of the cnop type initial errors term a is the temporal change of the relative vorticity error terms b to e denote the linear processes representing the interaction between the error and the background field in the following manner term b denotes the linear advection of relative vorticity error by background geostrophic velocity term c denotes the linear advection of background relative vorticity by geostrophic velocity error term d represents the linear ageostrophic divergence and term e represents the beta effect term f is the nonlinear advection representing the contribution of nonlinear processes caused by the interaction among errors we compare the left and right hand sides of eq 6 and find that their spatial structures are similar although there are slight differences in their magnitudes figure not shown this indicates that the sum of the right hand side terms can approximately reflect the temporal changes of the relative vorticity error fig 11a and b illustrate the averaged spatial pattern of each term of eq 6 except the beta term whose contribution is negligible integrated over the upper 500 m during the rapid evolution stage day 200 to day 300 for both case 1 and case 2 the relative vorticity error black dashed lines is negative near the south of the ke s first quasi stationary meander during the rapid evolution stage of error and the tendency tden is negative on the eastern side of it but positive on its western side indicating that the negative relative vorticity error moves eastward moreover the linear ladv the sum of terms b and c in eq 6 and nonlinear advections nladv play a dominant role in the evolution of the anomalous relative vorticity and have almost opposite spatial patterns the linear advection is negative on the eastern side of the negative relative vorticity error and positive on its western side which implies a weakening of the relative vorticity error in the east and a strengthening in the west in contrast the nonlinear advection implies a strengthened relative vorticity error in the east and a weakened relative vorticity error in the west these findings indicate that the linear advection tends to move the negative relative vorticity error eastward while the nonlinear advection moves it westward hence the negative relative vorticity error near the south of the ke s first quasi stationary meander is mainly caused by the linear advection for a quantitative comparison we compute the spatial similarity index kim et al 2004 wang et al 2013 the similarity indices between the tendency of the relative vorticity error and the linear nonlinear advection are 0 60 0 002 and 0 59 0 07 for case 1 and case 2 respectively they are the average results of the whole error evolution stage day 0 to day 300 in addition we calculate the relative vorticity budget averaged in the region with noticeable vorticity variations case 1 34º 36º n 141 5º 145º e case 2 33º 36º n 142 5º 146º e during the error rapid evolution period and directly compare the contribution of each term fig 11c and d both results indicate that the effects of the linear advection are larger than those of the nonlinear advection which further explains the importance of the linear advection in the error movement simultaneously the opposing influence of the linear advection and the nonlinear advection slows down the moving speed of the negative relative vorticity errors corresponding to the positive sshe causing them to remain in the ke s first quasi stationary meander for a long time this underscores the importance of the nonlinear advection in maintaining the errors in a specific region 3 3 analysis of the error growth dynamic mechanism due to the offset effect of the negative and positive relative vorticity errors the vorticity budget analysis cannot directly reflect the development of initial errors near the ke s first quasi stationary meander hence to analyze the error growth mechanism we employ the eddy energetics analysis approach tsujino et al 2006 fujii et al 2008 based on a qg assumption the evolution equation of the total eddy energy is written as eq 7 of box i where te is the sum of eke and eddy potential energy epe representing the total eddy energy caused by the cnop type initial errors u g and u a are described in section 3 2 p is the hydrostatic pressure g is the gravitational acceleration ρ is the density ρ 0 is the reference state of ρ and ρ z is the vertical derivative of ρ in this equation the overbar indicates the mean field denoted by the background state and the prime is treated as the eddy field represented by the error field same as in section 3 2 term a is the change rate of the total eddy energy the total eddy energy is caused by errors and therefore we rename it total error energy hereafter the right hand side terms represent the factors causing the growth of error energy terms b and c denote the nonlocal processes np through advection and pressure work respectively term d denotes the barotropic conversion rate bt which represents the kinetic energy transfer between the mean background flow and eddies errors through barotropic instabilities the baroclinic conversion rate bc designed by term e represents the potential energy transfer through baroclinic instabilities we investigate the spatial patterns between the left and right hand sides of eq 7 and find that they are similar for each case figure not shown but their magnitudes are slightly different therefore the sum of the right hand side terms can approximately represent the variation tendency of the error energy the temporal changes of the error energy figure not shown near the ke s first quasi stationary meander are similar to those of the ssh errors shown in fig 9a and b and therefore the rapid growth periods of the error energy are also similar for both cases the average distributions of np bt and bc in the rapid growth period of error for case 1 are shown in the first line of fig 12 the contributions of np and bt to the growth of the total error energy are almost equivalent while that of bc can be neglected near the ke s first quasi stationary meander on the one hand the error obtains energy from the background flow due to the positive bt on the other hand the error energy is carried away from this region causing the negative np by calculating the average error energy budget in the surrounding region of the ke s first quasi stationary meander 33º 37º n 141 5º 147º e during the error rapid growth period we find that the absolute value of bt is greater than that of np shown in the rightmost column of fig 12 implying that the energy obtained by the error is larger than the energy transported out of this region thus the error proliferates because it gains energy from the background flow which causes the ke jet to extend along the direction of the total error energy growth as shown by the thin gray and thick black contours in fig 12 as discussed by section 3 2 the errors can maintain near the ke s first quasi stationary meander for a long term as such the errors have enough time to absorb the energy to grow the situation of case 2 is similar as shown by the second line of fig 12 to explore why bt is important near the first quasi stationary meander of the ke we investigate the variations of the horizontal velocity shear of the background field v e l o c i t y s h e a r u g x 2 u g y 2 v g x 2 v g y 2 fig 13 shows the average distribution of the horizontal velocity shear of the background state shaded in the rapid evolution period of errors for both cases the horizontal velocity shear is larger in the ke upstream region especially near the ke s first quasi stationary meander during the error rapid growth stage and it matches the obvious velocity error structure vectors this reveals that the relatively remarkable bt in the ke upstream region may be caused by the large velocity shear 4 conclusions and discussions in this study we have successfully reproduced the ke decadal variability using an eddy resolving ocean model roms based on the simulated results and the cnop method we have established a nonlinear optimization system using this system we found the initial errors affecting the ke state transition most the cnop type initial errors the spatial distribution of the initial errors indicates that the large amplitudes are mainly in the ke region over the upper 500 m then we investigated the effects of the initial errors on the prediction of the ke and found that they cause an overestimation of the ke strength during the ke transition period from the stable to the unstable state to find why the ke strength is overestimated we performed a diagnostic analysis of the relative vorticity anomaly the results show that the linear advection of the relative vorticity overcomes the inhibition of its nonlinear advection and thus it gradually transports the negative relative vorticity errors eastward to the ke s first quasi stationary meander reducing the relative vorticity and leading to a positive sshe in this region in addition the opposing influences of the linear and nonlinear advections slow down the moving speed of the negative relative vorticity errors corresponding to the positive sshe maintaining their location in the ke s first quasi stationary meander for a long term in this process the linear advection influences the movement of the relative vorticity errors while the nonlinear advection is conducive to their maintenance in addition we explored what affects the growth of the cnop type initial errors near the ke s first quasi stationary meander and found that the contributions of np to the growth of the total error energy are almost equivalent to those of bt while that of bc is negligible the impact of bt is greater than that of np implying that the error energy obtained from the background field is larger than the energy transported out of this region moreover the analysis of the relative vorticity indicated that the long term existence of the errors near the ke s first quasi stationary meander provides enough time for them to absorb energy from the background field continuously which results in the rapid growth of the errors in fact there is a connection between the advection of the relative vorticity and the barotropic energy conversion they are both related to the velocity advection and involve the interaction between errors and the gradient of the background field implying that the significance of these two processes is consistent to some extent simultaneously this also reflects the importance of the interaction between errors and the background field gradient during the ke transition from the stable to the unstable state to the best of our knowledge this study is the first time to investigate the three dimensional fastest growing initial errors of the ke transition process from the stable to the unstable state in fact the cnop type initial errors upstream of the ke highlight the importance of the initial conditions in this region for the ke state transition hence the location of the three dimensional structure of the cnop can serve as a guidance for determining in which area and at what depth sensitive area to implement adaptive observation carrying out additional observations over these areas may reduce initial errors and effectively improve the forecast skill of the ke transition process from the stable to the unstable state in addition the mechanism of the initial error development lays a theoretical foundation for actual measurements our results indicate that the error growth is caused by barotropic instability implying that the velocity errors are more important to the ke state transition process thus more attention should be paid to velocity errors in practical target observations moreover the magnitude of the initial errors is quite important for observation the average limits of the cnop type initial errors over the upper 500 m including the errors of zonal velocity meridional velocity temperature salinity and ssh are 0 5 ms 1 0 5 ms 1 0 3 º c 0 06 psu and 0 07 m respectively for two cases according to the statements of the ocean current data from japan oceanographic data center the observation precision of the ocean currents is 0 1 ms 1 the temperature and salinity in the argo profiles are accurate to 0 002 º c and 0 01 psu respectively the accuracy of the ssh measured by satellite altimetry is 0 02 0 03 m fu et al 1994 comparing the initial errors obtained by our calculations with the realistic observation precisions the magnitudes of the detected cnop type initial errors can satisfy the requirements of a realistic measurement accuracy hence the initial errors can be reduced by additional observations in addition due to the huge computational requirements we study only two cases and focus only on the predictability of the ke state transition from the stable to the unstable state in the future more cases will be discussed and an analogous predictability study for the opposite ke transition process will be performed credit authorship contribution statement yu geng conceptualization validation formal analysis investigation writing original draft visualization qiang wang writing review editing supervision mu mu methodology resources funding acquisition kun zhang software data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors appreciate two anonymous reviewers for their constructive comments and are thankful to the public platforms for providing the data listed in section 2 this study was supported by the qingdao national laboratory for marine science and technology qnlm2016orp0107 the fundamental research funds for the central universities b200201011 the national natural science foundation of china 41576015 41806013 41906003 the nsfc innovative group 41421005 strategic priority research program of chinese academy of sciences xda20060502 the national programme on global change and air sea interaction gasi ipovai 06 and the nsfc shandong joint fund for marine science research centers u1606402 it was also supported by the high performance computing center at the institute of oceanology chinese academy of sciences 
23952,based on an eddy resolving regional ocean modeling system roms and the conditional nonlinear optimal perturbation cnop method we set up a nonlinear optimization system to investigate the effects of the fastest growing initial errors on the prediction of the kuroshio extension ke transition process from the stable to the unstable state the results reveal that the larger values of the cnop type fastest growing initial errors are mainly located upstream of the ke over the upper 500 m furthermore the dynamic mechanism of the error growth is explored in terms of error movement maintenance and development during the ke transition period the results indicate that the errors are transported eastward from the south of japan to the ke s first quasi stationary meander by linear advection the nonlinear advection slows down the moving speed of these errors leading to their long term existence in the first quasi stationary meander of the ke in this situation errors have enough time to absorb energy from the background field continuously through barotropic energy conversion ultimately errors grow rapidly and further induce the overestimation of the ke strength keywords kuroshio extension predictability initial error 1 introduction the kuroshio extension ke is a strong eastward flowing current originating from the eastern coast of japan in the western north pacific ocean mitsudera et al 2004 qiu and chen 2005 pierini 2006 qiu et al 2014 on decadal time scales it exhibits a well defined low frequency variability between a stable state and an unstable state when in the stable state the ke is characterized by two well developed quasi stationary meanders an enhanced southern recirculation gyre and a reduced eddy kinetic energy eke level in contrast barely discernible quasi stationary meanders a weaker southern recirculation gyre and an elevated eke are observed when the ke switches to the unstable state qiu and chen 2005 2010 taguchi et al 2007 qiu et al 2014 the ke state transition has been recently found to leave significant imprints on the atmospheric circulation through strong air sea interactions frankignoul et al 2011 masunaga et al 2016 révelard et al 2016 besides it also influences the modification of the oceanic water mass and the salinity variations on seasonal and decadal time scales qiu and chen 2011 nan et al 2015 kitamura et al 2016 geng et al 2018 these changes impact in turn local fisheries and marine ecosystems nishikawa and yasuda 2011 chiba et al 2013 given their importance significant research efforts have been invested in studying the dynamics and predictability of the ke decadal variability dynamic mechanisms with the potential to cause the ke state transition have been extensively investigated qiu and chen 2005 2010 demonstrated that the ke transition is caused by the westward propagating baroclinic rossby wave which is generated by the basin scale wind stress curl anomalies in the eastern north pacific ocean at different phases of the pacific decadal oscillation pdo or the north pacific gyre oscillation npgo pierini 2006 and pierini et al 2009 suggested that the intrinsic oceanic variability plays an essential role in determining the dynamic states of the ke based on a reduced gravity shallow water model moreover reconciling the above viewpoints taguchi et al 2007 and pierini 2014 indicated that the ke decadal variability is likely due to nonlinear intrinsic oceanic adjustment triggered by the westward propagating rossby wave these studies provide theoretical guidance for the simulation and prediction of the ke state transition in recent years studies on the predictability of the ke decadal variability have been focused on two aspects the estimate of predictability time limit and the influence of initial errors on the prediction results from the perspective of wind stress forcing in the eastern north pacific ocean related to pdo or npgo nonaka et al 2012 and qiu et al 2014 suggested that the predictive time limit of the ke state transition is about 3 6 years however nonaka et al 2016 demonstrated that the potential predictability of the ke state transition is significantly affected by the nonlinear intrinsic variability of the ocean in fact this intrinsic variability may influence the predictive results by promoting the rapid growth of an initial error in the numerical prediction hence some researchers explored the effects of the initial errors or perturbations on the ke state transition wang et al 2017 based on the singular vector sv method and a shallow water model found that the error growth for different transition phases of the ke is affected by different physical processes however the ke state transition is a nonlinear process to overcome the linear approximation of the sv method zhang et al 2017 used a nonlinear method conditional nonlinear optimal perturbation cnop mu et al 2003 wang et al 2020 and a shallow water model to explore the triggering mechanism of the ke transition from the unstable to the stable state from the viewpoint of the initial optimal precursors moreover wang et al 2017 indicated that when the ke transits from the stable to the unstable state its predictability is lower than that of the opposite ke transition process therefore in this study we employ the cnop method to investigate the predictability related to initial condition error the first kind predictability the model sensitivity of inaccurate initial conditions lorenz 1975 chu 1999 during the ke transition from the stable to the unstable state generally the purpose of such a predictability study is to analyze the reasons and mechanisms yielding the uncertainties of the prediction results and to explore the methods to reduce these uncertainties mu et al 2004 hence we study the error growth dynamics to explore the factors that cause uncertainties in the model solution which provides a theoretical guidance for the design of ocean observation strategies davies and didone 2013 to reduce these uncertainties to achieve the above objective an important premise is to reproduce the low frequency variability of the ke for this purpose we choose a realistic eddy resolving regional ocean model the regional ocean modeling system roms which includes complex topography and comprehensive physical processes and has previously been applied to simulate the kuroshio large meander and the upstream kuroshio liu et al 2018 zhang et al 2016 furthermore a key technical issue is to set up a nonlinear optimization system based on the cnop method to find the initial error that has the largest impact on the ke transition process to the best of our knowledge this is the first attempt to apply the predictability of the ke state transition a large dimension predictability research to the high resolution ocean model the reminder of this paper is organized as follows the configuration and simulation of roms as well as the cnop method and its application to the predictability study of the ke state transition are described in section 2 the results presented in section 3 include the structures impacts evolution and growth mechanism of the fastest growing initial errors finally in section 4 we draw the conclusions and discuss the results 2 model and method 2 1 model configuration roms is a free surface terrain following ocean model based on primitive equations widely used for regional simulation song and haidvogel 1994 haidvogel et al 2000 in this study roms is used to simulate the decadal variability of the ke and its adjoint model moore et al 2004 is adopted to search for the fastest growing initial errors affecting the ke state transition the model domain is set to a large part of the north pacific ocean basin 14º 57º n 117º e 125º w as shown in fig 1 it has a 1 10º horizontal resolution for the entire domain and 30 sigma vertical levels in this study we focus on the first kind predictability to address this problem we assume that the model is perfect and there are no errors in the atmospheric forcing and lateral boundary fields under these premises we conduct a hindcast experiment by using the time varying forcing and lateral boundary fields to simulate the ke decadal variability the simulation results provide more realistic initial fields in accordance with the model dynamics for the predictability study of the ke transition process for the hindcast experiment the atmospheric forcing fields include shortwave radiation flux net heat flux wind stress sea surface salinity sss sea surface temperature sst and the kinematic surface net heat flux sensitivity to sst dqdsst here the model sst and sss are restored to the monthly mean sst and sss of the forcing field with a restoring time scale of 30 days the dqdsst is used to correct the model surface net heat flux during the restoration process the sss sst and the latent heat flux sensible heat flux long wave radiation flux and short wave radiation flux used to calculate net heat flux are obtained from the national centers for environmental prediction ncep climate forecast system reanalysis cfsr https rda ucar edu datasets ds093 2 saha 2010 the wind stress is calculated using wind speed at 10 m from the national centers for environmental prediction national center for atmospheric research ncep ncar reanalysis 1 https www esrl noaa gov psd data gridded data ncep reanalysis html kalnay et al 1996 to compute the dqdsst we need the surface atmospheric temperature surface atmospheric density sea level specific humidity and the wind speed at 10 m all variables are taken from the ncep cfsr expect for the wind speed because of the time limitations of the necp cfsr data we choose the forcing fields datasets recorded from 1979 to 2008 furthermore a control experiment is conducted to investigate the predictability of the ke transition from the stable to the unstable state however the sst and sss restoration may affect the growth of the initial errors and further causes the final results not to reflect the real variations of these errors hence we switch off the restoring of the sst and sss in the control experiment as such the atmospheric forcing fields do not include sss sst and dqdsst but contain net freshwater flux from ncep cfsr all other variables are the same as those of the hindcast experiment the model adopts warm starts and the initial temperature salinity velocity and sea surface height ssh in january 1979 are taken from simple ocean data assimilation soda 2 2 4 dataset http iridl ldeo columbia edu sources carton giese soda carton and giese 2008 the variables of boundary conditions are the same as those of the initial field and their time range corresponds to that of the forcing field all the lateral boundaries are opened except for the north side with the above settings the model is integrated over 30 years 1979 2008 for the first 8 years we use the soda dataset to nudge similar to data assimilation temperature salinity and velocity to produce a dynamically stable initial field close to the observation for integration over the subsequent years here we analyze the model outputs of the last 20 years 2 2 model results and validation the roms simulated standard deviation of ssh and the climatological ssh averaged for 20 year model outputs are shown in fig 2a the roms simulation captures the main features of the ke system with the obvious variations of ssh the two quasi stationary meanders and the ke southern recirculation gyre compared with the corresponding satellite altimetry results aviso archiving validation and interpretation of satellite oceanographic altimetry http opendap aviso altimetry fr aviso altimetry 2009 averaged from 1993 to 2012 fig 2b the simulated ssh variations are weaker in addition the simulated mean ke meanders are limited in their meridional amplitude and their position is slightly westward which leads to a shorter ke length fig 2c and d present the variations of the 20 year mean surface eke for the roms hindcast and the observation result respectively significant changes in eke occur mainly in the ke upstream for both sets of data presented but the variation amplitude of the simulated eke is weaker than that of the observed although there are differences between the model and observations roms can capture the essential characteristics of the ke system under a time varying monthly forcing to further examine the model s ability to reproduce the ke bimodal state in detail we compute four indices usually used to represent the bimodal state of the ke jet qiu and chen 2005 2010 qiu et al 2014 based on the simulated results these indices include the upstream ke path length integrated from 141º to 153º e the ke strength represented by the ssh difference across the ke jet averaged from 140º to 165º e the intensity of the ke southern recirculation gyre and the kinetic energy integrated in the ke region 32º 38º n 140º 165º e over the upper 1000 m here the intensity of the ke southern recirculation gyre is defined by r t s s s h x y t d s where s represents the region south of the ke jet from 141º to 158º e within the ssh exceeding 0 4 m fig 3 shows the time series of these four indices the blue lines denote the ke stable periods when the indices are larger than their respective average except that the upstream ke path length is less than its average the upstream ke path length of the ke stable state is shorter than that of the unstable state the ke strength and the intensity of the ke southern recirculation gyre are stronger in the stable state of the ke and the kinetic energy is larger than that during the ke unstable state although there are some differences between the simulation and the aviso results fig 3 of qiu et al 2014 of the above indices the simulated time series of these four indices indicate the obvious characteristics of the ke bimodal state moreover we assign the average of the above four indices after normalization with the reverse sign for the upstream ke path length to the ke index fig 4a as similarly defined by qiu et al 2014 the positive and negative ke indices denote the ke stable blue lines in fig 4a and unstable states respectively the ke index average is zero the temporal changes of the ke index successfully capture the ke decadal modulations and the years corresponding to the two ke states blue lines in figs 3 and 4a basically coincide with the observation results documented by qiu and chen 2005 2010 and qiu et al 2014 hence the differences between simulation and observation obtained for the climatological ke system do not affect the low frequency variability of the ke to a large extent the results of the roms are valid laying the foundation for the predictability research of the ke state transition using this model 2 3 cnop method and its application the cnop method is employed to search for initial errors leading to the largest prediction error at time t mu et al 2003 wang et al 2020 it is a maximum problem and thus differs from the four dimensional variational method belonging to the minimum problem bannister 2001 here we briefly recall the cnop method the solution of the nonlinear model at time t can be written formally as the following 1 x t m t x 0 where x 0 is the initial state vector propagated by the nonlinear propagator m t to state vector x t at time t suppose that x 0 is the initial error then eq 1 becomes the following 2 x t x t m t x 0 x 0 where x t represents the nonlinear evolution of the initial error x 0 at time t to find the fastest growing initial error with the greatest impact on the forecast results at time t a nonlinear constraint optimization problem is defined as follows 3 j x 0 δ max x 0 a δ j x 0 max x 0 a δ m t x 0 x 0 m t x 0 b 2 where j x 0 denotes the objective function in region b x 0 a δ is the initial error constraint in region a and δ is a positive constraint radius x 0 δ is the fastest growing initial error that causes the largest nonlinear evolution at time t with a given δ and it is represented by the cnop regions a and b are marked in fig 1 to obtain the cnop we establish a nonlinear optimization system based on the spectral projected gradient 2 spg2 optimization algorithm birgin et al 2000 the flow chart for calculating cnop is shown in fig 5 for this algorithm we need the value of the objective function at time t and the gradient information of the objective function at the initial time these can be calculated with the roms nonlinear model and adjoint model respectively the initial condition of the adjoint model is the objective function gradient at time t considering that the kinetic energy is an important indicator to measure the ke state transition we define the objective function as the kinetic energy of errors at time t over the upper 1000 m in the region b 32º 38º n 141º 153º e in which the ke state transition occurs as such the objective function can be expressed by the following equation 4 j m t x 0 x 0 m t x 0 b 2 1 2 u t 2 v t 2 d x d y d z where u t and v t denote the zonal and meridional velocity errors respectively at time t furthermore we choose the sum of the quadratic errors nondimensionalized and normalized by the standard deviation as the initial constraint condition li et al 2014 liu et al 2018 this sum represents the errors of the kinetic and potential energy and includes all the possible errors affecting the ke transition process it can be written as follows 5 x 0 a u u s t d 2 v v s t d 2 t t s t d 2 s s s t d 2 η η s t d 2 where u v t s η and u s t d v s t d t s t d s s t d η s t d represent the errors and standard deviation of zonal velocity meridional velocity temperature salinity and ssh respectively within region a 25º 42º n 130º 170º e to reduce the optimization dimension and improve the optimization efficiency we do not superimpose the initial error on the entire model domain considering that the ke state transition is a short term process wang et al 2017 the errors from the eastern north pacific ocean do not have enough time to affect it during the period of the transition nonaka et al 2012 qiu et al 2014 therefore it is physically appropriate to superimpose the initial error only on the region a moreover the constraint radius δ is set to 1 1 0 8 based on the practical measurement accuracies of the used variables in addition this optimization system requires a suitable background state because we focus here mainly on the effects of the cnop type initial errors on the ke transition from the stable to the unstable state this type of transition process is selected as the background state fig 4a and b show the simulated time series of the ke index and filtered ke index tendency obtained with the hindcast experiment respectively the tendency changes exceed by their 1 5 times positive and negative standard deviations indicating a remarkable ke transition process accordingly the gray bands in fig 4 indicate the two prominent ke transition phases from the stable to the unstable state and the duration of these two obvious transition processes is approximately 300 days hence we set the optimization time to 300 days for convenience these two processes are designated by case 1 and case 2 hereafter note that the simulated ke transition years are in accord with the observed ones see the yearly ke paths in fig 2 presented by qiu et al 2014 moreover the results of the hindcast experiment at the initial time of case 1 and case 2 provide more realistic initial fields for the control experiment further to check whether roms can simulate the ke transition process in the control experiment we compare the ke index of the hindcast and control experiments fig 4a black and green lines respectively the variation trends of the ke index of these two experiments for cases 1 and 2 are similar with their correlation coefficients of 0 78 case 1 and 0 77 case 2 at a 95 confidence level this indicates that the control experiment can simulate the ke transition well hence we can utilize the simulated results of the control experiment as the background state 3 results 3 1 spatial structures and effects of initial errors based on the nonlinear optimization system described above we obtain the cnop type initial errors for each case the spatial distributions of the cnop type initial errors including the information of all possible initial errors i e zonal velocity meridional velocity temperature salinity and ssh errors integrated over the whole vertical layers according to the initial constraint norm in eq 5 are presented in fig 6a and b it can be seen that the large amplitude of the cnop is mainly located in the ke region 32º 38º n 141º 160º e especially around the ke s first quasi stationary meander and there is a relatively small amplitude of the cnop in the south of japan compared with that in the ke region in addition we find that the vertical structures of the cnop type initial errors shown in fig 6c and d are evident over the upper 500 m from the spatial and vertical distribution of the initial errors we infer that the initial errors in the ke region over the upper 500 m significantly influence the ke transition from the stable to the unstable state of course some small differences exist in the initial error patterns for the two cases this indicates that the spatial structures of the initial errors are related to a certain extent to the selected background states in addition owing to the nonlinearity of the ke system the gradients calculated by the adjoint model may not be strictly accurate although the results obtained by using the spg2 algorithm can converge to the cnop solution normally moreover for a high dimensional optimization problem associated with this predictability study it is difficult to ensure that the long term optimization result is optimal for the cnop calculation as it cannot be mathematically proven hence in the practical calculation process we test the optimality of each detected initial error through two groups of numerical experiments considering the huge computation load each group only takes 10 members for the first group exp1 we superimpose 10 random errors on the cnop type initial errors generating 10 new initial errors according to the magnitudes of the cnop type initial errors the random errors of velocity and temperature range from 0 to 0 1 and those of salinity and ssh range from 0 to 0 01 for the second group exp2 we obtain 10 new initial errors by making a difference between the variables of two adjacent output time points of the hindcast experiment output time interval is 15 days the amplitude of the 20 initial errors newly generated is the same as that of the cnop type initial errors we superimpose these initial errors on the initial field x 0 in eq 1 and then run the nonlinear model with new initial conditions we calculate their respective cost function values and for both cases we find that they are smaller than those caused by the cnop type initial errors fig 7 in addition during the calculation of the cnop type initial errors we find that different initial guesses lead to similar spatial structures of the final optimization results these indicate that the obtained cnop type initial errors in this study are robust furthermore to explore the effects of the cnop type initial errors on the prediction of the ke state transition we superimpose the initial errors on the initial fields and then integrate the nonlinear model over 300 days fig 8 compares the ssh distribution at the initial time day 0 and the end of the optimization time day 300 without and with the cnop type initial errors a larger smaller magnitude of the ssh south of the ke jet denotes a stronger weaker southern recirculation gyre which yields an enhanced weakened ke strength because of the larger smaller ssh difference across the ke jet particularly for case 1 the ke is in the stable state at the initial time with a strong southern recirculation gyre fig 8a after 300 days the ke becomes the unstable state with a weak recirculation gyre fig 8b however at this time the ke is accompanied by an enhanced southern recirculation gyre after superimposing the initial errors fig 8d the situation is similar for case 2 although the ke southern recirculation gyre at day 300 is slightly stronger than that in case 1 these imply that the cnop type initial errors lead to the strengthening of the ke at the end of the optimization time however in the reference field the ke state is unstable and relatively weak at the end of the optimization time as a result the strengthening of the ke caused by the initial errors means that the ke transition process from the stable to the unstable state is suppressed it should be mentioned that we did not find the initial errors promoting the ke state transition the possible reason is as follows the inhibition of initial errors to the ke state transition means that the ke state is relatively stable compared with that of the reference field the ke unstable state at the end of the optimization time in contrast the intensification of initial errors to the ke state transition means that the ke state is more unstable than that of the reference field at the end of the optimization time the differences between the ke relative stable and unstable states may be greater than those between the two similar unstable states of the ke because the optimization algorithm always tries to seek the initial errors causing the maximum of the objective function the calculated initial errors tend to result in the relative stable state of the ke at the end of the optimization time this means that the initial errors will inhibit the ke transition process from the stable to the unstable state 3 2 evolution processes of initial errors to explore how cnop type initial errors affect the ke transition we investigate their evolution processes according to previous studies qiu and chen 2010 pierini 2014 the ssh anomalies sshas the deviation from the climatological monthly ssh south of the ke jet play an important indicative role in the ke state especially near the region located south of the ke s first quasi stationary meander thus this study presents the ssh error sshe caused by the initial error fig 9a and b demonstrate the time series of the regional average sshe south of the ke s first quasi stationary meander 33º 35º n 141º 145º e the positive sshe develops rapidly near the first quasi stationary meander from day 200 to day 300 for both case 1 and case 2 fig 9c and d show the evolution processes of the sshe we find that the development of positive sshe is accompanied by its eastward movement from the southern region of japan ultimately this positive sshe moving to the ke s first quasi stationary meander causes the ke strength to be enhanced next we examine what leads to a positive sshe south of the ke s first quasi stationary meander during the error evolution period with the relative vorticity budget analysis fig 10 illustrates the evolution process of the relative vorticity errors showing that the negative vorticity errors come from the south of japan corresponding to the evolution of sshe in fact the errors in this region are caused by the initial errors naturally the question then is what leads to variations in the negative relative vorticity error to address this problem we calculate the following budget equation of the anomalous relative vorticity under a quasi geostrophic qg assumption 6 ξ t u g ξ u g ξ f 0 u a β v g u g ξ a b c d e f where ξ is the relative vorticity u g u g v g 0 is the geostrophic velocity vector u a u a v a w denotes the ageostrophic component f 0 is the average coriolis parameter in the ke region and β is the meridional derivative of the coriolis parameter the overbar represents the background state that is not affected by the initial errors and the prime denotes the error field caused by the evolution of the cnop type initial errors term a is the temporal change of the relative vorticity error terms b to e denote the linear processes representing the interaction between the error and the background field in the following manner term b denotes the linear advection of relative vorticity error by background geostrophic velocity term c denotes the linear advection of background relative vorticity by geostrophic velocity error term d represents the linear ageostrophic divergence and term e represents the beta effect term f is the nonlinear advection representing the contribution of nonlinear processes caused by the interaction among errors we compare the left and right hand sides of eq 6 and find that their spatial structures are similar although there are slight differences in their magnitudes figure not shown this indicates that the sum of the right hand side terms can approximately reflect the temporal changes of the relative vorticity error fig 11a and b illustrate the averaged spatial pattern of each term of eq 6 except the beta term whose contribution is negligible integrated over the upper 500 m during the rapid evolution stage day 200 to day 300 for both case 1 and case 2 the relative vorticity error black dashed lines is negative near the south of the ke s first quasi stationary meander during the rapid evolution stage of error and the tendency tden is negative on the eastern side of it but positive on its western side indicating that the negative relative vorticity error moves eastward moreover the linear ladv the sum of terms b and c in eq 6 and nonlinear advections nladv play a dominant role in the evolution of the anomalous relative vorticity and have almost opposite spatial patterns the linear advection is negative on the eastern side of the negative relative vorticity error and positive on its western side which implies a weakening of the relative vorticity error in the east and a strengthening in the west in contrast the nonlinear advection implies a strengthened relative vorticity error in the east and a weakened relative vorticity error in the west these findings indicate that the linear advection tends to move the negative relative vorticity error eastward while the nonlinear advection moves it westward hence the negative relative vorticity error near the south of the ke s first quasi stationary meander is mainly caused by the linear advection for a quantitative comparison we compute the spatial similarity index kim et al 2004 wang et al 2013 the similarity indices between the tendency of the relative vorticity error and the linear nonlinear advection are 0 60 0 002 and 0 59 0 07 for case 1 and case 2 respectively they are the average results of the whole error evolution stage day 0 to day 300 in addition we calculate the relative vorticity budget averaged in the region with noticeable vorticity variations case 1 34º 36º n 141 5º 145º e case 2 33º 36º n 142 5º 146º e during the error rapid evolution period and directly compare the contribution of each term fig 11c and d both results indicate that the effects of the linear advection are larger than those of the nonlinear advection which further explains the importance of the linear advection in the error movement simultaneously the opposing influence of the linear advection and the nonlinear advection slows down the moving speed of the negative relative vorticity errors corresponding to the positive sshe causing them to remain in the ke s first quasi stationary meander for a long time this underscores the importance of the nonlinear advection in maintaining the errors in a specific region 3 3 analysis of the error growth dynamic mechanism due to the offset effect of the negative and positive relative vorticity errors the vorticity budget analysis cannot directly reflect the development of initial errors near the ke s first quasi stationary meander hence to analyze the error growth mechanism we employ the eddy energetics analysis approach tsujino et al 2006 fujii et al 2008 based on a qg assumption the evolution equation of the total eddy energy is written as eq 7 of box i where te is the sum of eke and eddy potential energy epe representing the total eddy energy caused by the cnop type initial errors u g and u a are described in section 3 2 p is the hydrostatic pressure g is the gravitational acceleration ρ is the density ρ 0 is the reference state of ρ and ρ z is the vertical derivative of ρ in this equation the overbar indicates the mean field denoted by the background state and the prime is treated as the eddy field represented by the error field same as in section 3 2 term a is the change rate of the total eddy energy the total eddy energy is caused by errors and therefore we rename it total error energy hereafter the right hand side terms represent the factors causing the growth of error energy terms b and c denote the nonlocal processes np through advection and pressure work respectively term d denotes the barotropic conversion rate bt which represents the kinetic energy transfer between the mean background flow and eddies errors through barotropic instabilities the baroclinic conversion rate bc designed by term e represents the potential energy transfer through baroclinic instabilities we investigate the spatial patterns between the left and right hand sides of eq 7 and find that they are similar for each case figure not shown but their magnitudes are slightly different therefore the sum of the right hand side terms can approximately represent the variation tendency of the error energy the temporal changes of the error energy figure not shown near the ke s first quasi stationary meander are similar to those of the ssh errors shown in fig 9a and b and therefore the rapid growth periods of the error energy are also similar for both cases the average distributions of np bt and bc in the rapid growth period of error for case 1 are shown in the first line of fig 12 the contributions of np and bt to the growth of the total error energy are almost equivalent while that of bc can be neglected near the ke s first quasi stationary meander on the one hand the error obtains energy from the background flow due to the positive bt on the other hand the error energy is carried away from this region causing the negative np by calculating the average error energy budget in the surrounding region of the ke s first quasi stationary meander 33º 37º n 141 5º 147º e during the error rapid growth period we find that the absolute value of bt is greater than that of np shown in the rightmost column of fig 12 implying that the energy obtained by the error is larger than the energy transported out of this region thus the error proliferates because it gains energy from the background flow which causes the ke jet to extend along the direction of the total error energy growth as shown by the thin gray and thick black contours in fig 12 as discussed by section 3 2 the errors can maintain near the ke s first quasi stationary meander for a long term as such the errors have enough time to absorb the energy to grow the situation of case 2 is similar as shown by the second line of fig 12 to explore why bt is important near the first quasi stationary meander of the ke we investigate the variations of the horizontal velocity shear of the background field v e l o c i t y s h e a r u g x 2 u g y 2 v g x 2 v g y 2 fig 13 shows the average distribution of the horizontal velocity shear of the background state shaded in the rapid evolution period of errors for both cases the horizontal velocity shear is larger in the ke upstream region especially near the ke s first quasi stationary meander during the error rapid growth stage and it matches the obvious velocity error structure vectors this reveals that the relatively remarkable bt in the ke upstream region may be caused by the large velocity shear 4 conclusions and discussions in this study we have successfully reproduced the ke decadal variability using an eddy resolving ocean model roms based on the simulated results and the cnop method we have established a nonlinear optimization system using this system we found the initial errors affecting the ke state transition most the cnop type initial errors the spatial distribution of the initial errors indicates that the large amplitudes are mainly in the ke region over the upper 500 m then we investigated the effects of the initial errors on the prediction of the ke and found that they cause an overestimation of the ke strength during the ke transition period from the stable to the unstable state to find why the ke strength is overestimated we performed a diagnostic analysis of the relative vorticity anomaly the results show that the linear advection of the relative vorticity overcomes the inhibition of its nonlinear advection and thus it gradually transports the negative relative vorticity errors eastward to the ke s first quasi stationary meander reducing the relative vorticity and leading to a positive sshe in this region in addition the opposing influences of the linear and nonlinear advections slow down the moving speed of the negative relative vorticity errors corresponding to the positive sshe maintaining their location in the ke s first quasi stationary meander for a long term in this process the linear advection influences the movement of the relative vorticity errors while the nonlinear advection is conducive to their maintenance in addition we explored what affects the growth of the cnop type initial errors near the ke s first quasi stationary meander and found that the contributions of np to the growth of the total error energy are almost equivalent to those of bt while that of bc is negligible the impact of bt is greater than that of np implying that the error energy obtained from the background field is larger than the energy transported out of this region moreover the analysis of the relative vorticity indicated that the long term existence of the errors near the ke s first quasi stationary meander provides enough time for them to absorb energy from the background field continuously which results in the rapid growth of the errors in fact there is a connection between the advection of the relative vorticity and the barotropic energy conversion they are both related to the velocity advection and involve the interaction between errors and the gradient of the background field implying that the significance of these two processes is consistent to some extent simultaneously this also reflects the importance of the interaction between errors and the background field gradient during the ke transition from the stable to the unstable state to the best of our knowledge this study is the first time to investigate the three dimensional fastest growing initial errors of the ke transition process from the stable to the unstable state in fact the cnop type initial errors upstream of the ke highlight the importance of the initial conditions in this region for the ke state transition hence the location of the three dimensional structure of the cnop can serve as a guidance for determining in which area and at what depth sensitive area to implement adaptive observation carrying out additional observations over these areas may reduce initial errors and effectively improve the forecast skill of the ke transition process from the stable to the unstable state in addition the mechanism of the initial error development lays a theoretical foundation for actual measurements our results indicate that the error growth is caused by barotropic instability implying that the velocity errors are more important to the ke state transition process thus more attention should be paid to velocity errors in practical target observations moreover the magnitude of the initial errors is quite important for observation the average limits of the cnop type initial errors over the upper 500 m including the errors of zonal velocity meridional velocity temperature salinity and ssh are 0 5 ms 1 0 5 ms 1 0 3 º c 0 06 psu and 0 07 m respectively for two cases according to the statements of the ocean current data from japan oceanographic data center the observation precision of the ocean currents is 0 1 ms 1 the temperature and salinity in the argo profiles are accurate to 0 002 º c and 0 01 psu respectively the accuracy of the ssh measured by satellite altimetry is 0 02 0 03 m fu et al 1994 comparing the initial errors obtained by our calculations with the realistic observation precisions the magnitudes of the detected cnop type initial errors can satisfy the requirements of a realistic measurement accuracy hence the initial errors can be reduced by additional observations in addition due to the huge computational requirements we study only two cases and focus only on the predictability of the ke state transition from the stable to the unstable state in the future more cases will be discussed and an analogous predictability study for the opposite ke transition process will be performed credit authorship contribution statement yu geng conceptualization validation formal analysis investigation writing original draft visualization qiang wang writing review editing supervision mu mu methodology resources funding acquisition kun zhang software data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors appreciate two anonymous reviewers for their constructive comments and are thankful to the public platforms for providing the data listed in section 2 this study was supported by the qingdao national laboratory for marine science and technology qnlm2016orp0107 the fundamental research funds for the central universities b200201011 the national natural science foundation of china 41576015 41806013 41906003 the nsfc innovative group 41421005 strategic priority research program of chinese academy of sciences xda20060502 the national programme on global change and air sea interaction gasi ipovai 06 and the nsfc shandong joint fund for marine science research centers u1606402 it was also supported by the high performance computing center at the institute of oceanology chinese academy of sciences 
23953,this study makes progress towards a data driven parameterization for mesoscale oceanic eddies to demonstrate the concept and reveal accompanying caveats we aimed at replacing a computationally expensive standard high resolution ocean model with its inexpensive low resolution analogue augmented by the parameterization we considered eddy resolving and non eddy resolving double gyre ocean circulation models characterized by drastically different solutions due to the nonlinear mesoscale eddy effects the key step of the proposed approach is to extract from the high resolution reference solution its eddy field varying in space and time and then to use this information to improve the low resolution analogue model by interactively coupling both the continuously supplied history of the eddy field and the explicitly modeled low resolution large scale flow we obtained the additional eddy forcing term which modified the low resolution model and significantly augmented its solutions this eddy forcing term represents the action of the eddy field its coupling with the large scale flow and is a key dynamical constraint imposed on the augmentation procedure although the augmentation drastically improved the low resolution circulation patterns it did not recover the robust intrinsic large scale low frequency variability lfv which is an important feature of the high resolution solution this is by itself an important negative result that has significant implication for any data driven eddy parameterization especially given the fact that we used the most complete information about the space time history of the eddy fields note when we supplied the reference true eddy forcing rather than just the eddy field the lfv was recovered this suggests that the lfv is crucially dependent on the details of the space time eddy forcing large scale flow correlations which are not fully respected by the proposed augmentation procedure in order to overcome the deficiency and recover the lfv we statistically filtered the augmented low resolution model solution by projecting it onto the leading empirical orthogonal functions eofs of the large scale component of the high resolution reference solution this operation allowed us to remove spurious effects associated with higher eofs we tested and confirmed that without using the data driven eddy information this filtering alone cannot augment the low resolution solution but in conjunction with the eddy information it produced desirable outcome moreover as a natural step towards parameterization we took advantage of data driven stochastic inverse modeling to obtain inexpensive emulators of the eddy field and showed generally promising results of augmenting the coarse resolution model with the obtained emulators our results showed that obtaining the lfv characteristics for the eddy parameterization which is already capable of reproducing the large scale flow pattern should become a standard parameterization requirement but it can be challenging to meet keywords ocean dynamics mesoscale eddies eddy forcing parameterizations 1 introduction numerical model solutions of complex oceanic flows are highly sensitive to the spatial grid resolution shevchenko and berloff 2015 shevchenko et al 2016 if the resolution is too coarse for representing mesoscale eddy dynamics the resulting errors can be accumulated on large scales which are nominally well resolved even with dynamically coarse grids on the one hand this problem is now well understood in the ocean modeling community marshall et al 2012 bachman et al 2017 on the other hand resolving all the dynamically important scales is an insurmountable task and many parameterizations aiming to circumvent this have been proposed and implemented gent and mcwilliams 1990 frederiksen 1999 frederiksen et al 2012 porta mana and zanna 2014 berloff 2015 2016 zanna et al 2017 berloff 2018 mak et al 2018 ryzhov et al 2019 however there is still no unified framework because different approaches are designed to account for different processes and also each parameterization accounts for the effects of a certain range of scales progress with parameterizations is hampered because the ocean circulation does not have spectral gaps between different ranges of scales however many theoretical insights rely on simple conceptual models with clear scale separation e g the lorentz toy model majda et al 1999 fatkullin and vanden eijnden 2004 kravtsov et al 2005 crommelin and vanden eijnden 2008 arnold et al 2013 chorin and lu 2015 furthermore different scales are nonlinearly tangled and accounting for this by understanding their interactions is difficult bachman et al 2017 but ultimately needed the above mentioned two aspects make the problem of flow scale decomposition for the purposes of parameterizations open and important for now the main constraint for a flow decomposition is rather intuitive and vague given the resolution of a coarse grid model we assume that the unrepresented and dynamically distorted scales range from the kolmogorov scale to about 10 intervals of the computational grid and the scales larger than the grid interval are increasingly better accounted for by the model dynamics more specifically in this paper we consider the classical wind driven midlatitude ocean circulation model featuring two large scale counter rotating gyres with the western boundary currents and with their intense eastward jet extension that separates the gyres our focus is on the eastward jet region where the solutions of the model most critically depend on the spatial grid resolution shevchenko and berloff 2015 with an inadequate resolution misrepresentation of the mesoscale eddy dynamics results in an underdeveloped and even absent eastward jet extension whereas with a proper resolution the eastward jet reappears as a pronounced meandering and vortex shedding large scale feature characterized by vigorous eddy dynamics and intensive eddy large scale interactions note that the flow decomposition into the large and small scale i e mesoscale eddy components is not unique because of both the absence of the spectral gap and the highly nonlinear dynamics this complicates the analyses and parameterizations of the eddy effects our goal is to improve the analogue coarse resolution double gyre model by feeding it with information obtained from solutions of the high resolution model which is treated as the reference truth or the observed data ideally this data driven approach should enable us to reproduce in the coarse resolution model the main characteristics of the high resolution reference solution a the large scale circulation pattern specifically the eastward jet extension with its adjacent recirculation zones and b its intrinsic large scale low frequency variability lfv as we show in this paper the latter characteristic proves more elusive to rectify even if the augmentation makes use of the full eddy information to be precise one should aim at comparing the augmented coarse resolution solution with the large scale component of the high resolution solution which is obtained by statistical filtering nevertheless we focus on rectifying the large scale circulation patterns and lfv which are interconnected that are clearly transparent in the full high resolution solution as well so we use it for the comparison recently ryzhov et al 2019 introduced a novel approach for augmenting the coarse resolution analogue model with data inferred from the high resolution truth it involves the following main steps i running the high resolution model saving the solution data and verifying that the analogue low resolution model significantly misrepresents certain key features of the large scale circulation ii decomposing the high resolution data into some large scale and small scale eddy fields iii producing the eddy forcing term which is based on the decomposed fields and provides an important dynamical constraint in order to exert extra forcing and augment the low resolution model in a dynamically consistent way overall an advantage of this approach is in combining its data driven nature with the transparent dynamical constraint and this is strengthened by significant flexibility of its practical implementations in this paper our goal is to extend the approach of ryzhov et al 2019 by significantly reducing and simplifying the information supplied from the high resolution reference truth now instead of augmenting the model with the true eddy forcing history coarse grained on the low resolution grid we supply only the true eddy field and its statistical emulation by a space time stochastic process in a separate experiment this means that the eddy forcing term is now interactively and continuously calculated online from the supplied eddy field history and the dynamical low resolution solution which is treated as the prognostic large scale circulation the approach is based on the implicit assumption that the low resolution model if it is properly augmented is adequate for representing the large scale circulation patterns and the lfv 2 double gyre model 2 1 governing equations we use the same model configuration as in ryzhov et al 2019 the model has been extensively tested both in eddy permitting and eddy resolving regimes marshall et al 2012 maddison et al 2015 shevchenko and berloff 2015 shevchenko et al 2016 ying et al 2019 a brief description is as follows the quasi geostrophic qg potential vorticity pv evolution in 3 stacked isopycnal layers i 1 3 from top to bottom with densities ρ i ρ 1 1000 ρ 2 1001 498 ρ 3 1001 62 kg m 3 and heights h i h 1 250 h 2 750 h 3 3000 m is given by 1 q i t j ψ i q i β ψ i x w x y ρ i h i δ 1 i γ δ ψ i δ 3 i ν δ 2 ψ i where q i is the pv anomaly ψ i is the streamfunction j is the jacobian operator δ i j is the kronecker delta δ is the horizontal laplacian β 2 1 0 11 m 1 s 1 is the planetary vorticity gradient ν is the eddy viscosity varies for different spatial resolutions used in the study γ 4 1 0 8 s 1 is the bottom friction parameter the basin is north south oriented square l x y l where 2 l 3840 km the upper ocean layer is forced by the stationary asymmetric wind stress curl 2 w x y π τ 0 a l sin π l y l b x y b x π τ 0 l a sin π y b x l b x y b x where the asymmetry tilt and wind stress magnitude parameters are a 0 9 b 0 2 and τ 0 0 08 n m 2 respectively the pv anomalies and streamfunctions are related through 3 q 1 δ ψ 1 s 1 ψ 2 ψ 1 q 2 δ ψ 2 s 21 ψ 1 ψ 2 s 22 ψ 3 ψ 2 q 3 δ ψ 3 s 3 ψ 2 ψ 3 where the stratification parameters s 1 s 21 s 22 s 3 are chosen to yield the first and second baroclinic rossby deformation radii of 40 and 23 km respectively the boundary conditions are no flow through and partial slip with the partial slip length scale equal to 120 km the mass is conserved in each layer the model is solved using the high resolution cabaret method that features a second order non dissipative and low dispersive conservative advection scheme karabasov et al 2009 given an adequately fine spatial resolution the model is capable of resolving the eddies that maintain the well developed eastward jet extension of the western boundary current otherwise the eastward jet extension is under predicted or even absent because the backscatter process of the energy transfer from the eddies to the large sale flow is under resolved by the model jansen and held 2014 jansen et al 2015 shevchenko and berloff 2016 berloff 2018 2 2 differences of flow structures in eddy resolving and eddy permitting regimes we consider two spatial grid resolutions for simulating the eddy permitting low resolution and eddy resolving high resolution flow regimes 129 129 and 513 513 respectively for resolving the western boundary layer berloff and mcwilliams 1999 the low resolution configuration is run with the viscosity ν 50 m 2 s 1 whilst the high resolution one has ν 2 m 2 s 1 in both cases the model is first spun up for 100 years until a statistically equilibrated state is achieved then its daily output is saved for another 90 years for further analyses the differences in the resulting flows are well documented shevchenko and berloff 2015 ryzhov et al 2019 so here we only note that the low resolution model does not induce a proper eastward jet extension fig 1a whereas the high resolution one features a well pronounced eddy driven eastward jet with the adjacent recirculation zones fig 1b throughout the paper we make use of the standard deviation instead of the time mean when address the problem of rectifying the large scale circulation patterns the standard deviation accentuates more saliently the differences also easily seen in time mean patterns not only the spatial patterns but also the temporal variabilities of the reference solutions are different to reveal details of the latter we used the data adaptive harmonic decomposition dahd method chekroun and kondrashov 2017 kondrashov et al 2018a which characterizes a complex and multiscale spatio temporal variability by extracting spatial data adaptive harmonic modes dahms such that each one of them oscillates at a single temporal frequency and is spatially orthogonal to all other modes at that frequency see appendix a for details the dahd has been successfully applied to characterize variabilities in different geophysical datasets including ocean circulation kondrashov et al 2018a ryzhov et al 2019 kondrashov et al 2020 sea ice kondrashov et al 2018c b and space physics kondrashov and chekroun 2018 here we applied the dahd to the upper ocean pv anomaly fields of the reference solutions to make our analysis computationallytractable first these fields were compressed using the standard principal component analysis pca preisendorfer 1988 to retain the leading d 2000 empirical orthogonal function eof modes these modes capture 98 and 95 of the variance in the low and high resolution solutions respectively next the original pv anomaly fields were projected onto the retained eofs to obtain the corresponding principal components pcs these d 2000 pcs were used as inputs for the dahd frequency domain formulation which is tailored for analysis of high dimensional datasets chekroun and kondrashov 2017 ryzhov et al 2019 and based on the singular value decomposition svd of the d d symmetrized complex cross spectral matrix s f 4 s p q ρ p q f if q p ρ q p f if q p where 1 p q d and ρ p q f is the fourier transform of the double sided cross correlation coefficients ρ p q m estimated for all pairs of the channels pcs p and q and for the time lag m up to its maximum m 1 i e m 1 m m 1 each singular value σ k f of s f is associated with a pair of negative positive eigenvalues λ k f λ k f obtained by using the standard dahd time domain formulation and an eigen decomposition of a matrix formed of the elements ρ p q m kondrashov et al 2018a ryzhov et al 2019 kondrashov et al 2020 5 λ k f λ k f σ k f 1 k d the dahd power spectrum is obtained by plotting eigenvalues λ f which represent energy conveyed by associated dahms the frequency f is equally spaced with the nyquist interval 0 0 5 across the m values 6 f 0 5 ℓ 1 m 1 ℓ 1 m the adequate spectral resolution in the low frequency part isachieved by considering 30 k days long pcs sub sampled every 5 days thus we have n 6000 samples and use the largest possible embedding window m n 2 3000 for the maximum spectral resolution in the frequency domain despite the overall similarity of the dahd spectra shown in fig 2 and characterized by the bands of higher values separated by the gaps from the broadly distributed bands of lower values as well as by the power law behaviors in the high frequency range the low resolution solution spectrum has significantly smaller magnitudes which indicate the reduced eddy activity in the upper band there are two λ values at each frequency each of them corresponding to a negative positive pair see eq 5 the observed gap in the spectrum can be interpreted as a dominance of a particular physical mechanism of energy distribution and transfer across all the temporal frequencies however the exact interpretation of the spectra is significantly hindered by the nonlinear character of the underlying physical interactions here we use the spectra to diagnose the lfv and its profound effect on the spectrum the striking difference is the pronounced lfv in the high resolution solution see the blue dots in fig 2b at the period 17 years and its complete absence in the low resolution solution fig 2a this interdecadal lfv was studied elsewhere berloff and mcwilliams 1999 berloff et al 2007 shevchenko et al 2016 and here we just note that the quality of an augmented low resolution model can be tested by the model s capability to simulate this lfv 2 3 low frequency variability as an indicator of properly resolved small scales as we pointed out in the previous section one of the most remarkable dynamical features which differentiate the low and high resolution solutions is the lfv in the latter the lfv manifests itself as the total energy modulation with the period 17 years berloff and mcwilliams 1999 kondrashov and berloff 2015 a peculiar characteristic of the lfv is that it appears only if the double gyre model resolves the eddies and hence activates the essential eddy backscatter mechanism berloff et al 2007 shevchenko and berloff 2016 the backscatter here means that the energy from the small scales is transferred to the large scales and thus impacts the large scale circulation if the spatial resolution is too coarse even in eddy permitting regimes the small scales are not resolved and in turn the large scales are also under saturated which introduces many inconsistencies in the flow when comparing solutions corresponding to differing spatial resolutions ryzhov et al 2019 demonstrated that the low resolution model is in principle capable of inducing the lfv provided that it is augmented with the eddy forcing history provided by the high resolution data our goal now is to reduce the amount of the information inferred from the high resolution data but still be able to capture the lfv and induce it in the augmented low resolution model thus instead of using the complete high resolution data for estimating the true eddy forcing and using it to augment the low resolution model we intend to use only the true eddy component of the flow and to calculate the augmenting eddy forcing interactively by using the large scale flow predicted by the augmented low resolution model 3 scale decomposition of the high resolution solution the high resolution solution which is treated as the truth should be decomposed into a combination of large scale and small scale eddy components the former one should be adequately captured by an augmented low resolution model whilst the latter one may remain largely unresolved however we know that the true eddy forcing adequately augments the low resolution model and this is a necessary condition for our next steps an issue of significant concern is that the large scale eddy flow decomposition which is central to the proposed augmentation scenarios is neither unique nor clearly constrained by dynamical or statistical arguments for now various methods assume hasselmann 1988 von storch et al 1995 schmid 2010 li and von storch 2013 dijkstra 2013 2018 viebahn et al 2019 agarwal et al 2020 that the implemented flow decomposition i e scale separation is practically meaningful and then build upon this assumption our work is fully within this framework a formal scale decomposition for an arbitrary 2d time dependent field ξ in our case ξ stands for the layer wise streamfunctions ψ i and pv anomalies q i reads 7 ξ x y t ξ x y t ξ x y t where the overbar and prime indicate the large scale and eddy components respectively with this in mind we decomposed the high resolution streamfunctions ψ i by the moving average square filter of size w and the corresponding pv anomalies are obtained by differentiation akin eq 3 we justify our choice of w by focusing on mesoscale eddies which are scaled by the first baroclinic rossby deformation radius but we also admit that the problem contains many length scales and they vary geographically making the flow decomposition a difficult and open problem the problem stems from the fact that for linear flows when all the active scales are well separated in the fourier spectra the filter size should linearly depend on the ratio between the fine and coarse resolution grids however in our case there is no separation between the active scales and the filter size is chosen based on the expected dynamical features we would like to filter out assuming the coarse resolution model being unable to resolve them in our case these features are mesoscale eddies with length scales of order of the first baroclinic rossby deformation radius 10 100 km preliminary analyses ryzhov et al 2019 suggest that the filter size of w 21 of high resolution grid intervals 150 km in physical units is adequate but we also tested w 41 as a tribute to the unavoidable sensitivity analysis the eddy fields calculated on the high resolution spatial grid 513 513 were coarse grained to be fed into the low resolution 129 129 model by averaging over four adjacent grid cells in each spatial direction guided by the fact that the lfv is eddy driven we substituted 7 into the governing equation 1 and for each layer obtained 8 q i t j ψ i q i f i ψ i q i ψ i q i h i ψ i q i l i ψ i q i where the operator h i contains all terms involving only the large scale components the linear operator l i contains the eddy tendency term and all linear terms involving the eddy components and the remaining term 9 f i j ψ i q i j ψ i q i j ψ i q i is the eddy forcing berloff 2005 due to nonlinear coupling of the large scale and eddy components the linear eddy term l i can be neglected since its contribution to the eastward jet as we checked is about 2 of that of the eddy forcing ryzhov et al 2019 established that the eddy forcing term when properly preprocessed with respect to the low resolution dynamics can be effectively added into the low resolution model to improve significantly the mean flow and transient spectrally treated characteristics of its solutions in this work our goal is to reduce the amount of the high resolution information by feeding the eddies rather than the eddy forcing information which depends on both the eddies and large scales into the augmented model 4 feeding the eddy field into the low resolution model with only the eddies being fed to the augmented model the external information is subtler which makes it harder for the low resolution model to resolve desired dynamics resembling the fine resolution reference solution such that the eastward extension of the jet is noticeably rectified and the low frequency variability is present at the same time gauging the possibility of reducing the amount of data necessary for successful parameterization and errors introduced due to the incompleteness of the data is practically important the governing equations for the augmented low resolution model are thus 10 q i t j ψ i q i f i ψ i q i ψ i q i h i ψ i q i where the small scale eddy fields ψ i q i are taken from the high resolution data and the prognostic low resolution large scale variables ψ i q i are continuously updated online during numerical integration of the model we used all 90 years of the daily output to extract the eddy fields and then linearly interpolated them in time in between the data records an important issue of determining the minimal length of the eddy history for the quality augmentation of the low resolution model is left outside the scope of the paper and will be addressed elsewhere we assessed the quality of the augmented low resolution solution by looking into the simulated eastward jet region focusing on its large scale circulation patterns evinced by the standard deviation in time and lfv the augmented model eastward jet has improved but is still substantially different from the reference truth as can be seen by comparing figs 3a and 1a similarly large discrepancies are seen in the augmented model dahd spectrum fig 3b which completely lacks the lfv the interactive eddy forcing fig 4a can be significantly less efficient because it is noticeably weaker than the true eddy forcing fig 4c we checked this by considering the more energetic eddy field extracted with the larger filter size w 41 fig 4b but although the resulting eddy forcing is as intensive as the true one the augmented model is still incapable of generating the lfv as implied by the dahd spectrum fig 3d from this we conclude that feeding even the most complete eddy fields into the model is still not sufficient for augmenting the solution so one has to use additional information from the high resolution data to induce the lfv it has been already established ryzhov et al 2019 that the true off line eddy forcing fig 4b generates the lfv in the augmented solution therefore we know that one way or another the model can be successfully augmented with the right amount of the extra information one way to add this information is by interactively projecting the augmented solution onto the leading true large scale eofs and this can be viewed as a weak statistical constraint imposed by the filtering the corresponding set of eofs are obtained through the standard singular value decomposition such that 11 q h r i pc i eof i where q h r i is the large scale true pv anomaly in the i th layer and in the matrix form rearranged so that the rows correspond to the spatial degrees of freedom whilst the columns represent their time evolutions pc i u i s i eof i v i where u i s i v i are the left eigenvector diagonal singular value and the right eigenvector matrices respectively is matrix transpose projection of the on line augmented pv anomaly q i onto some n eofs eof n i takes the form 12 q n i q i eof n i eof n i and the updated field q i is used on the next time step of the model eq 10 there are two key parameters at the projection step the number n of eofs and the time interval t p r o j between successive projections these parameters are chosen empirically for optimizing both the results and computational costs we found by sensitivity experiments that the number of the eofs should be relatively large and 2000 out of 12 9 2 16641 total eofs are good enough and t p r o j should not be much longer than 100 model days used here as the benchmark value with these parameters the augmented model recovered not only more than 95 of the lfv spectral power but also the correct frequencies we varied the number of the eofs and obtained qualitatively similar results within the 500 2000 range and the lower values degrade the solution since the eof projections are made infrequently the filtering process is computationally inexpensive the additionally filtered model solutions now exhibit the lfv as diagnosed by dahd spectra shown in fig 5a for w 21 and fig 5b for w 41 it is worth noting that even in the solution augmented with weaker eddies w 21 the lfv is also reproduced albeit it is not as energetic as with the stronger eddies w 41 the eastward jet extension is also reproduced similarly to the case without large scale filtering see fig 3 in addition to the detailed dahd spectral space time diagnostic of pv anomaly field it is also useful to consider the manifestation of lfv in the total potential energy which is a global characteristic of the solution fig 6 shows the fourier spectral analysis of the potential energy time series by the standard multitaper method percival and walden 1993 which reveals broadband lfv peaks at frequency 0 06 year 1 about 17 years period both for the reference high resolution and augmented low resolution solutions whilst the reference low resolution solution features no lfv with a mostly flat spectrum due to the projection the augmented solution acquires oversaturated high frequencies near the lfv peak this may be dealt with by carefully selecting the projection basis of the filtering procedure so to filter out spurious small scale effects and is beyond the scope of the current study as we aimed at imbuing the coarse resolution solution with the correct lfv finally we would like to emphasize that feeding the eddies to induce the augmenting eddy forcing in the low resolution model eq 9 is absolutely necessary for generating the lfv and we verified this by turning it off if the filtering based on the eof projection procedure is applied alone it does not augment the solution thus confirming that the main component of the parameterization is the eddy forcing 5 statistical emulation of the eddy field here we developed data driven statistical emulators of the true eddy field for feeding them into the low resolution model instead of the original high resolution eddy fields the number of statistical emulation methods has recently surged including stochastic approaches in climate science penland and matrosova 2001 strounine et al 2010 franzke et al 2015 kondrashov et al 2015 chen et al 2016 palmer 2019 seleznev et al 2019 foster et al 2020 as well as other machine learning deep learning methods developed for fluid dynamics applications brunton et al 2020 bolton and zanna 2019 the detailed analysis of emulated eddy fields is beyond the scope of this study and in the context of assessing the skill of our emulators we focus solely on one of the central problems in climate ocean model simulations namely the correct rectification of the eddy field s impact on the large scale circulation thus we aimed for the solution of the low resolution model when augmented by an emulated eddy field to be able to reproduce the long term statistics of the high resolution reference solution we utilized the same skill measures as for the true eddy field explored in previous section these are the geometrical shape of the large scale circulation patterns as well as the manifestation of the lfv we used a 30 000 day long high resolution dataset of the eddy stream function ψ ˆ for the three layers combined the dataset is then coarse grained onto the low spatial resolution 129 129 and further compressed by the pca we retained the leading 1000 pcs that account for 98 of the variability as a basic and most straightforward emulator we considered a linear stochastic regression model kravtsov et al 2005 2006 kondrashov et al 2005 2015 in the following discrete form 13 ξ t 1 ξ t a ξ t r t 0 where t is the time index in days ξ is a vector of pcs and a is a matrix of the regression coefficients while eq 13 can include additional model layers of hidden variables obtained in a sequential regression procedure it is not necessary here since the regression residual r t 0 is well approximated by a spatially correlated white noise r t 0 σ w where w is a wiener process and σ is the cholesky decomposition of the correlation matrix of the residuals from the model fitting the emulated pcs are obtained by initializing the model from the first data point of the training interval and by running it for 30000 days the eddy field is reconstructed in space from the emulated pcs by using the eof basis and then it is fed into the low resolution model in our augmentation procedure while this basic emulator of the eddy field yields a fairly reasonable geometrical structure of the jet extension in the augmented solution fig 7a it does not induce the lfv as evident by the flat spectral density curve of the full potential energy fig 7b which is also similar to the non augmented low resolution solution a closer analysis shows that the lack of the lfv in the augmented solution is related to the spectral content of the emulated eddy field in which energy at low frequencies is underestimated in comparison to the true eddy field in turn because the lfv in the true eddy field is considerably weaker than in the true reference solution it is challenging to capture it by an emulator based on pca pcs which typically mix different temporal scales the dahd method section 2 2 and appendix a provides a novel emulation alternative as it combines identification of frequency ranked modes and their efficient modeling it extracts pairs of data adaptive harmonic modes dahms that form an orthonormal set of spatial patterns oscillating harmonically in time and thus represent global monochromatic space time filters projection of the dataset ontodahms yields pairs of narrowband time series of data adaptive harmonic coefficients dahcs which are modulated in amplitude but do not mix temporal scales chekroun and kondrashov 2017 showed that the stuart landau sl stochastic oscillator a nonlinear oscillating system near a hopf bifurcation and driven by an additive noise is best suited to model amplitude modulations and frequency for the narrowband and in phase quadrature time series of a dahc pair ζ t f ζ t f associated with a given spectral pair λ f λ f see section 2 2 and appendix b here written in a compact form with a complex number notation 14 z t 1 f z t f μ f i γ f z t f 1 i β f z t f 2 z t f ϵ t where z t f ζ t f i ζ t f μ f γ f and β f are real parameters and ϵ t is an additive noise furthermore multiple sl oscillators associated with the same non zero frequency are linearly coupled and synchronized across frequencies by the pairwise correlated white noise while the model parameters are estimated by a regression with constraints see appendix b for numerical details the original dataset with its multiple time scales can be modeled in a computationally efficient manner since the contribution of each temporal frequency is simulated in parallel kondrashov et al 2018a developed a stochastic dahd emulator for the lfv in the model considered and here we extended these results to the eddies we used the leading d 100 pcs of the eddy streamfunction capturing 70 of the variance and applied the dahd with the embedding window of m 100 days then we fit the model of coupled d 100 stochastic oscillators for the dahcs and obtained their emulations for the m 100 frequencies after emulated dahcs were back transformed into the space time eddy field by using dahms and eofs and combined across all the emulated frequencies we fed the outcome into the augmented model the geometrical shape of the augmented solution is again reproduced fairly well and it is very similar to fig 7a not shown for brevity furthermore since the lfv is now better captured in the emulated eddy field compare to the high resolution truth it is also induced in the augmented solution fig 7b albeit it is less energetic then when the true eddy field is used see fig 6 6 conclusions in this paper we focused on improving solutions of an eddy permitting low resolution model by augmenting it with the information from the reference high resolution model solution which was treated as the observed truth our approach can be viewed as a basis for developing data driven parameterizations for the mesoscale oceanic eddies and their effects and in perspective for other types of turbulent fluid motions ultimately the parameterization should involve statistical emulations of the key unresolved or under resolved flow features we adopted a systematic approach towards such a parameterization framework this paper is the second one in the series after ryzhov et al 2019 for the ocean circulation model we considered the classical wind driven double gyres in the quasigeostrophic approximation with 3 active isopycnal layers and in an idealized closed midlatitude basin configuration solutions of the double gyre model are notoriously sensitive to the spatial grid resolution which is typical for the general ocean circulation models two prominent flow features which are crucially dependent on the resolution are in the focus of our study 1 the eastward jet extension of the western boundary currents with its adjacent recirculation zones and 2 the intrinsic large scale low frequency interdecadal variability of the gyres that is most pronounced in the eastward jet region both of these features are essentially mesoscale eddy driven therefore for their dynamical representation in the model the eddies have to be either properly resolved which is computationally expensive or adequately parameterized in terms of a simpler model in the high resolution reference solution both of the key features are well represented whereas the low resolution reference solution lacks any of them motivation for including 1 is straightforward because any eddy parameterization is first of all tested for its ability to simulate the large scale climatological fields motivation for including 2 is to test the ability of the parameterization to simulate intrinsic climate variabilities similar to the relatively well understood interdecadal variability featured in our model our hope is that testing mesoscale eddy parameterization skills will eventually include climate variability signals as the standard test beds our model augmentation procedure involves the following main steps first the high resolution true solution is decomposed into large scale and small scale eddy flow components by simple moving average filtering in space this flow decomposition is neither unique nor obviously constrained by dynamical or statistical arguments here we only assumed that the filter width should be about scaled with the first baroclinic rossby deformation radius since our study targets mesoscale eddies in the prequel study ryzhov et al 2019 the decomposed flow components were used to find the history of the eddy forcing which is just part of the advection operator that involves the eddy field then this history was coarse grained and applied to augment the low resolution model with many analyses and sensitivity studies attached to this statement and reported in the paper in the present study we extended the approach by supplying the primary eddy fields instead of the eddy forcing which is a higher level and subtler information moreover we tested the augmentation procedure skills in terms of the challenging reproduction of the lfv the eddy field component was interactively coupled with the corresponding low resolution model solution which was treated as the simulated large scale flow component via the on line eddy forcing operator which can be viewed as an additional dynamical constraint imposed on the augmentation procedure we found that the augmentation significantly improved representation of the eastward jet extension but the lfv was still missing the immediate hypothesis was that this was because the eddies are too weak hence the interactive eddy forcing was too weak to generate the lfv we tested this hypothesis by increasing the filter size used to extract the eddies and the resulting new eddy forcing turned out to be of the same intensity as the true eddy forcing however this further improved the modeled eastward jet but did not generate the lfv from this we concluded that the lfv was crucially dependent on the correlations between the large scale flow and the eddy forcing which were not fully respected by the augmentation procedure we also realized that the eddy history alone was not sufficient and some additional information had to be supplied as part of the augmentation we do not yet have the ultimate answer on what this information should be but in order to make progress we decided to supply some large scale flow information in terms of interactive weak filtering of the simulated large scale flow towards the observed truth this idea was implemented as a statistical filtration interactively projecting the simulated transient flow anomalies onto the leading empirical orthogonal functions eofs of the reference high resolution true flow this approach worked well and we experimentally found the optimal number of the eofs and the optimal frequency of the applied filtering procedure so that the lfv was almost fully recovered since the filtering can be applied infrequently about every 100 days in our case rather than continuously which is also possible its computational cost is nearly negligible however the exact amount of information needed from the high resolution truth for a correct rectification of the lfv remains unknown and its assessment should be addressed elsewhere we hypothesized that this information should contain correct correlations between the eddy and large scale fields we also demonstrated that the filtering was of secondary importance relative to the supplied eddy forcing because when the latter was switched off the filtering alone was not capable of augmenting the solution to any acceptable level finally we developed a statistical emulation of the eddy field as spatio temporal stochastic process and used it in our augmented procedure results showed that the frequency ranked data adaptive harmonic decomposition dahd emulator reproduces the lfv substantially better than the pca based linear stochastic model an agenda for further research stemming from this paper is to build on and improve statistical emulators for the eddy field as well as to consider extending the proposed approach beyond the relatively simple quasigeostrophic approximation to comprehensive general circulation models constraining the large scale eddy flow decomposition and making it consistent with the low resolution ocean model is also very important finally adding new criteria e g higher order statistical moments and spatio temporal correlations for assessing eddy parameterization skills should not be too far away declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank dr j maddison and one anonymous reviewer for constructive comments that helped improve this manuscript this research was supported by the national science foundation nsf usa grants o c e 1658357 and the natural environment research council nerc grant n e r 011567 1 pavel berloff also gratefully acknowledges funding by nerc uk grant no n e t 002220 1 and leverhulme trust uk grant no r p g 2019 024 and the moscow center for fundamental and applied mathematics supported by the agreement 075 15 2019 1624 with the ministry of education and science of the russian federation dahd analysis was supported by the russian science foundation grant no 18 12 00231 we would like to acknowledge the high performance computing support from cheyenne doi 10 5065 d 6 r x 99 h x provided by ncar s computational and information systems laboratory sponsored by the nsf usa the dahd toolbox is available at http research atmos ucla edu tcd dkondras software html appendix a data adaptive harmonic decomposition dahd here we present a brief summary of the dahd frequency domain implementation and stochastic emulation methodology following chekroun and kondrashov 2017 kondrashov and chekroun 2018 kondrashov et al 2018a b and tailored to high dimensional datasets we consider a multivariate time series x t x 1 t x d t formed with d spatial channels and t 1 n time points sampled evenly double sided unbiased cross correlation coefficients ρ p q m are estimated for all the pairs of channels p and q and time lag m up to a maximum m 1 15 ρ p q m 1 n m t 1 n m x p t m x q t 0 m m 1 ρ q p m m 0 where m is the embedding window and each of ρ p q m sequences is of length m 2 m 1 the dahd numerical algorithm computes its spectral elements λ j w j j 1 d 2 m 1 by utilizing a d d symmetrized complex cross spectral matrix s f built from the fourier transforms of the cross correlation sequences see eq 4 the data adaptive harmonic modes dahms represent collection of spatio temporal patterns w j e 1 j e d j oscillating with different but single frequency f in time embedded space 1 m m 16 e k j m b k j cos 2 π f m θ k j 1 k d where the amplitudes b k j and phases θ k j are data adaptive f takes distinct m values that are equally spaced in nyquist interval 0 0 5 17 f ℓ 1 m 1 ℓ 1 m 1 2 and λ j informs on energy conveyed by w j in particular for each f 0 there are 2 d positive negative eigenelements which are necessarily paired as λ k f λ k f k 1 d while the phases for the associated dahm pair w k f w k f satisfy θ k θ k π 2 i e these modes are shifted by one fourth of the period and are thus always in exact phase quadrature similar to the sine and cosine pair in the fourier analysis but in a data adaptive and global in space fashion there are also d non paired spectral elements λ k w k associated with the frequency f 0 the fourier transforms of the dahms are computed as eigenvectors of the matrix s f s f chekroun and kondrashov 2017 theorem v 1 and eq 74 18 s f s f w k f λ k 2 w k f and spatiotemporal patterns of w k f w k f are obtained then by the inverse fourier transform a projection of x onto given w j yields the time series of the dahd expansion coefficients dahcs 19 ζ j t m 1 m k 1 d x k t m 1 e k j m where 1 t n m 1 the time series of a given dahc pair ζ k t ζ k t associated with the modes w k f w k f at the frequency f 0 are narrowband nearly in phase quadrature and heavily modulated in amplitude appendix b frequency ranked stochastic emulators the collective behavior of the d pairs at the frequency f 0 see appendix a is simulated by a system of linearly coupled stuart landau stochastic oscillators 20 d ζ k d t β k f ζ k α k f ζ k σ k f ζ k ζ k 2 ζ k 2 i k d a i k f ζ i i k d b i k f ζ i ϵ k d ζ k d t α k f ζ k β k f ζ k σ k f ζ k ζ k 2 ζ k 2 i k d c i k f ζ i i k d d i k f ζ i ϵ k where 1 k d the model parameters are estimated by a pairwise multiple linear regression with linear constraints on α k f and β k f to ensure antisymmetry for the linear coupling within a given pair as well as equal and positive values σ k f 0 to ensure numerical stability the stochastic forcing in eq 20 is informed by regression residuals from the model fitting namely ϵ t ϵ t σ d w where σ is the 2 d 2 d cholesky decomposition of the correlation matrix of the residuals and d w is a 2 d valued wiener process the linear stochastic emulator eq 13 is used to model the time series of the dahcs associated with f 0 which are not paired any subset of dahcs can be convolved with its corresponding set of dahms to produce a partial or full reconstruction of the original dataset thus the following j th reconstructed component rc at time t and for channel k is defined as 21 r k j t 1 m t m l t u t ζ j t m 1 e k j m 1 m m where l t u t is a lower upper bound in 1 m that depends on time and the normalization factor m t equals m except near the ends of the time series the sum of all the rcs across all the frequencies recovers the original time series and stochastically emulated dahcs are back transformed to the phase space of the original dataset by using eq 21 
23953,this study makes progress towards a data driven parameterization for mesoscale oceanic eddies to demonstrate the concept and reveal accompanying caveats we aimed at replacing a computationally expensive standard high resolution ocean model with its inexpensive low resolution analogue augmented by the parameterization we considered eddy resolving and non eddy resolving double gyre ocean circulation models characterized by drastically different solutions due to the nonlinear mesoscale eddy effects the key step of the proposed approach is to extract from the high resolution reference solution its eddy field varying in space and time and then to use this information to improve the low resolution analogue model by interactively coupling both the continuously supplied history of the eddy field and the explicitly modeled low resolution large scale flow we obtained the additional eddy forcing term which modified the low resolution model and significantly augmented its solutions this eddy forcing term represents the action of the eddy field its coupling with the large scale flow and is a key dynamical constraint imposed on the augmentation procedure although the augmentation drastically improved the low resolution circulation patterns it did not recover the robust intrinsic large scale low frequency variability lfv which is an important feature of the high resolution solution this is by itself an important negative result that has significant implication for any data driven eddy parameterization especially given the fact that we used the most complete information about the space time history of the eddy fields note when we supplied the reference true eddy forcing rather than just the eddy field the lfv was recovered this suggests that the lfv is crucially dependent on the details of the space time eddy forcing large scale flow correlations which are not fully respected by the proposed augmentation procedure in order to overcome the deficiency and recover the lfv we statistically filtered the augmented low resolution model solution by projecting it onto the leading empirical orthogonal functions eofs of the large scale component of the high resolution reference solution this operation allowed us to remove spurious effects associated with higher eofs we tested and confirmed that without using the data driven eddy information this filtering alone cannot augment the low resolution solution but in conjunction with the eddy information it produced desirable outcome moreover as a natural step towards parameterization we took advantage of data driven stochastic inverse modeling to obtain inexpensive emulators of the eddy field and showed generally promising results of augmenting the coarse resolution model with the obtained emulators our results showed that obtaining the lfv characteristics for the eddy parameterization which is already capable of reproducing the large scale flow pattern should become a standard parameterization requirement but it can be challenging to meet keywords ocean dynamics mesoscale eddies eddy forcing parameterizations 1 introduction numerical model solutions of complex oceanic flows are highly sensitive to the spatial grid resolution shevchenko and berloff 2015 shevchenko et al 2016 if the resolution is too coarse for representing mesoscale eddy dynamics the resulting errors can be accumulated on large scales which are nominally well resolved even with dynamically coarse grids on the one hand this problem is now well understood in the ocean modeling community marshall et al 2012 bachman et al 2017 on the other hand resolving all the dynamically important scales is an insurmountable task and many parameterizations aiming to circumvent this have been proposed and implemented gent and mcwilliams 1990 frederiksen 1999 frederiksen et al 2012 porta mana and zanna 2014 berloff 2015 2016 zanna et al 2017 berloff 2018 mak et al 2018 ryzhov et al 2019 however there is still no unified framework because different approaches are designed to account for different processes and also each parameterization accounts for the effects of a certain range of scales progress with parameterizations is hampered because the ocean circulation does not have spectral gaps between different ranges of scales however many theoretical insights rely on simple conceptual models with clear scale separation e g the lorentz toy model majda et al 1999 fatkullin and vanden eijnden 2004 kravtsov et al 2005 crommelin and vanden eijnden 2008 arnold et al 2013 chorin and lu 2015 furthermore different scales are nonlinearly tangled and accounting for this by understanding their interactions is difficult bachman et al 2017 but ultimately needed the above mentioned two aspects make the problem of flow scale decomposition for the purposes of parameterizations open and important for now the main constraint for a flow decomposition is rather intuitive and vague given the resolution of a coarse grid model we assume that the unrepresented and dynamically distorted scales range from the kolmogorov scale to about 10 intervals of the computational grid and the scales larger than the grid interval are increasingly better accounted for by the model dynamics more specifically in this paper we consider the classical wind driven midlatitude ocean circulation model featuring two large scale counter rotating gyres with the western boundary currents and with their intense eastward jet extension that separates the gyres our focus is on the eastward jet region where the solutions of the model most critically depend on the spatial grid resolution shevchenko and berloff 2015 with an inadequate resolution misrepresentation of the mesoscale eddy dynamics results in an underdeveloped and even absent eastward jet extension whereas with a proper resolution the eastward jet reappears as a pronounced meandering and vortex shedding large scale feature characterized by vigorous eddy dynamics and intensive eddy large scale interactions note that the flow decomposition into the large and small scale i e mesoscale eddy components is not unique because of both the absence of the spectral gap and the highly nonlinear dynamics this complicates the analyses and parameterizations of the eddy effects our goal is to improve the analogue coarse resolution double gyre model by feeding it with information obtained from solutions of the high resolution model which is treated as the reference truth or the observed data ideally this data driven approach should enable us to reproduce in the coarse resolution model the main characteristics of the high resolution reference solution a the large scale circulation pattern specifically the eastward jet extension with its adjacent recirculation zones and b its intrinsic large scale low frequency variability lfv as we show in this paper the latter characteristic proves more elusive to rectify even if the augmentation makes use of the full eddy information to be precise one should aim at comparing the augmented coarse resolution solution with the large scale component of the high resolution solution which is obtained by statistical filtering nevertheless we focus on rectifying the large scale circulation patterns and lfv which are interconnected that are clearly transparent in the full high resolution solution as well so we use it for the comparison recently ryzhov et al 2019 introduced a novel approach for augmenting the coarse resolution analogue model with data inferred from the high resolution truth it involves the following main steps i running the high resolution model saving the solution data and verifying that the analogue low resolution model significantly misrepresents certain key features of the large scale circulation ii decomposing the high resolution data into some large scale and small scale eddy fields iii producing the eddy forcing term which is based on the decomposed fields and provides an important dynamical constraint in order to exert extra forcing and augment the low resolution model in a dynamically consistent way overall an advantage of this approach is in combining its data driven nature with the transparent dynamical constraint and this is strengthened by significant flexibility of its practical implementations in this paper our goal is to extend the approach of ryzhov et al 2019 by significantly reducing and simplifying the information supplied from the high resolution reference truth now instead of augmenting the model with the true eddy forcing history coarse grained on the low resolution grid we supply only the true eddy field and its statistical emulation by a space time stochastic process in a separate experiment this means that the eddy forcing term is now interactively and continuously calculated online from the supplied eddy field history and the dynamical low resolution solution which is treated as the prognostic large scale circulation the approach is based on the implicit assumption that the low resolution model if it is properly augmented is adequate for representing the large scale circulation patterns and the lfv 2 double gyre model 2 1 governing equations we use the same model configuration as in ryzhov et al 2019 the model has been extensively tested both in eddy permitting and eddy resolving regimes marshall et al 2012 maddison et al 2015 shevchenko and berloff 2015 shevchenko et al 2016 ying et al 2019 a brief description is as follows the quasi geostrophic qg potential vorticity pv evolution in 3 stacked isopycnal layers i 1 3 from top to bottom with densities ρ i ρ 1 1000 ρ 2 1001 498 ρ 3 1001 62 kg m 3 and heights h i h 1 250 h 2 750 h 3 3000 m is given by 1 q i t j ψ i q i β ψ i x w x y ρ i h i δ 1 i γ δ ψ i δ 3 i ν δ 2 ψ i where q i is the pv anomaly ψ i is the streamfunction j is the jacobian operator δ i j is the kronecker delta δ is the horizontal laplacian β 2 1 0 11 m 1 s 1 is the planetary vorticity gradient ν is the eddy viscosity varies for different spatial resolutions used in the study γ 4 1 0 8 s 1 is the bottom friction parameter the basin is north south oriented square l x y l where 2 l 3840 km the upper ocean layer is forced by the stationary asymmetric wind stress curl 2 w x y π τ 0 a l sin π l y l b x y b x π τ 0 l a sin π y b x l b x y b x where the asymmetry tilt and wind stress magnitude parameters are a 0 9 b 0 2 and τ 0 0 08 n m 2 respectively the pv anomalies and streamfunctions are related through 3 q 1 δ ψ 1 s 1 ψ 2 ψ 1 q 2 δ ψ 2 s 21 ψ 1 ψ 2 s 22 ψ 3 ψ 2 q 3 δ ψ 3 s 3 ψ 2 ψ 3 where the stratification parameters s 1 s 21 s 22 s 3 are chosen to yield the first and second baroclinic rossby deformation radii of 40 and 23 km respectively the boundary conditions are no flow through and partial slip with the partial slip length scale equal to 120 km the mass is conserved in each layer the model is solved using the high resolution cabaret method that features a second order non dissipative and low dispersive conservative advection scheme karabasov et al 2009 given an adequately fine spatial resolution the model is capable of resolving the eddies that maintain the well developed eastward jet extension of the western boundary current otherwise the eastward jet extension is under predicted or even absent because the backscatter process of the energy transfer from the eddies to the large sale flow is under resolved by the model jansen and held 2014 jansen et al 2015 shevchenko and berloff 2016 berloff 2018 2 2 differences of flow structures in eddy resolving and eddy permitting regimes we consider two spatial grid resolutions for simulating the eddy permitting low resolution and eddy resolving high resolution flow regimes 129 129 and 513 513 respectively for resolving the western boundary layer berloff and mcwilliams 1999 the low resolution configuration is run with the viscosity ν 50 m 2 s 1 whilst the high resolution one has ν 2 m 2 s 1 in both cases the model is first spun up for 100 years until a statistically equilibrated state is achieved then its daily output is saved for another 90 years for further analyses the differences in the resulting flows are well documented shevchenko and berloff 2015 ryzhov et al 2019 so here we only note that the low resolution model does not induce a proper eastward jet extension fig 1a whereas the high resolution one features a well pronounced eddy driven eastward jet with the adjacent recirculation zones fig 1b throughout the paper we make use of the standard deviation instead of the time mean when address the problem of rectifying the large scale circulation patterns the standard deviation accentuates more saliently the differences also easily seen in time mean patterns not only the spatial patterns but also the temporal variabilities of the reference solutions are different to reveal details of the latter we used the data adaptive harmonic decomposition dahd method chekroun and kondrashov 2017 kondrashov et al 2018a which characterizes a complex and multiscale spatio temporal variability by extracting spatial data adaptive harmonic modes dahms such that each one of them oscillates at a single temporal frequency and is spatially orthogonal to all other modes at that frequency see appendix a for details the dahd has been successfully applied to characterize variabilities in different geophysical datasets including ocean circulation kondrashov et al 2018a ryzhov et al 2019 kondrashov et al 2020 sea ice kondrashov et al 2018c b and space physics kondrashov and chekroun 2018 here we applied the dahd to the upper ocean pv anomaly fields of the reference solutions to make our analysis computationallytractable first these fields were compressed using the standard principal component analysis pca preisendorfer 1988 to retain the leading d 2000 empirical orthogonal function eof modes these modes capture 98 and 95 of the variance in the low and high resolution solutions respectively next the original pv anomaly fields were projected onto the retained eofs to obtain the corresponding principal components pcs these d 2000 pcs were used as inputs for the dahd frequency domain formulation which is tailored for analysis of high dimensional datasets chekroun and kondrashov 2017 ryzhov et al 2019 and based on the singular value decomposition svd of the d d symmetrized complex cross spectral matrix s f 4 s p q ρ p q f if q p ρ q p f if q p where 1 p q d and ρ p q f is the fourier transform of the double sided cross correlation coefficients ρ p q m estimated for all pairs of the channels pcs p and q and for the time lag m up to its maximum m 1 i e m 1 m m 1 each singular value σ k f of s f is associated with a pair of negative positive eigenvalues λ k f λ k f obtained by using the standard dahd time domain formulation and an eigen decomposition of a matrix formed of the elements ρ p q m kondrashov et al 2018a ryzhov et al 2019 kondrashov et al 2020 5 λ k f λ k f σ k f 1 k d the dahd power spectrum is obtained by plotting eigenvalues λ f which represent energy conveyed by associated dahms the frequency f is equally spaced with the nyquist interval 0 0 5 across the m values 6 f 0 5 ℓ 1 m 1 ℓ 1 m the adequate spectral resolution in the low frequency part isachieved by considering 30 k days long pcs sub sampled every 5 days thus we have n 6000 samples and use the largest possible embedding window m n 2 3000 for the maximum spectral resolution in the frequency domain despite the overall similarity of the dahd spectra shown in fig 2 and characterized by the bands of higher values separated by the gaps from the broadly distributed bands of lower values as well as by the power law behaviors in the high frequency range the low resolution solution spectrum has significantly smaller magnitudes which indicate the reduced eddy activity in the upper band there are two λ values at each frequency each of them corresponding to a negative positive pair see eq 5 the observed gap in the spectrum can be interpreted as a dominance of a particular physical mechanism of energy distribution and transfer across all the temporal frequencies however the exact interpretation of the spectra is significantly hindered by the nonlinear character of the underlying physical interactions here we use the spectra to diagnose the lfv and its profound effect on the spectrum the striking difference is the pronounced lfv in the high resolution solution see the blue dots in fig 2b at the period 17 years and its complete absence in the low resolution solution fig 2a this interdecadal lfv was studied elsewhere berloff and mcwilliams 1999 berloff et al 2007 shevchenko et al 2016 and here we just note that the quality of an augmented low resolution model can be tested by the model s capability to simulate this lfv 2 3 low frequency variability as an indicator of properly resolved small scales as we pointed out in the previous section one of the most remarkable dynamical features which differentiate the low and high resolution solutions is the lfv in the latter the lfv manifests itself as the total energy modulation with the period 17 years berloff and mcwilliams 1999 kondrashov and berloff 2015 a peculiar characteristic of the lfv is that it appears only if the double gyre model resolves the eddies and hence activates the essential eddy backscatter mechanism berloff et al 2007 shevchenko and berloff 2016 the backscatter here means that the energy from the small scales is transferred to the large scales and thus impacts the large scale circulation if the spatial resolution is too coarse even in eddy permitting regimes the small scales are not resolved and in turn the large scales are also under saturated which introduces many inconsistencies in the flow when comparing solutions corresponding to differing spatial resolutions ryzhov et al 2019 demonstrated that the low resolution model is in principle capable of inducing the lfv provided that it is augmented with the eddy forcing history provided by the high resolution data our goal now is to reduce the amount of the information inferred from the high resolution data but still be able to capture the lfv and induce it in the augmented low resolution model thus instead of using the complete high resolution data for estimating the true eddy forcing and using it to augment the low resolution model we intend to use only the true eddy component of the flow and to calculate the augmenting eddy forcing interactively by using the large scale flow predicted by the augmented low resolution model 3 scale decomposition of the high resolution solution the high resolution solution which is treated as the truth should be decomposed into a combination of large scale and small scale eddy components the former one should be adequately captured by an augmented low resolution model whilst the latter one may remain largely unresolved however we know that the true eddy forcing adequately augments the low resolution model and this is a necessary condition for our next steps an issue of significant concern is that the large scale eddy flow decomposition which is central to the proposed augmentation scenarios is neither unique nor clearly constrained by dynamical or statistical arguments for now various methods assume hasselmann 1988 von storch et al 1995 schmid 2010 li and von storch 2013 dijkstra 2013 2018 viebahn et al 2019 agarwal et al 2020 that the implemented flow decomposition i e scale separation is practically meaningful and then build upon this assumption our work is fully within this framework a formal scale decomposition for an arbitrary 2d time dependent field ξ in our case ξ stands for the layer wise streamfunctions ψ i and pv anomalies q i reads 7 ξ x y t ξ x y t ξ x y t where the overbar and prime indicate the large scale and eddy components respectively with this in mind we decomposed the high resolution streamfunctions ψ i by the moving average square filter of size w and the corresponding pv anomalies are obtained by differentiation akin eq 3 we justify our choice of w by focusing on mesoscale eddies which are scaled by the first baroclinic rossby deformation radius but we also admit that the problem contains many length scales and they vary geographically making the flow decomposition a difficult and open problem the problem stems from the fact that for linear flows when all the active scales are well separated in the fourier spectra the filter size should linearly depend on the ratio between the fine and coarse resolution grids however in our case there is no separation between the active scales and the filter size is chosen based on the expected dynamical features we would like to filter out assuming the coarse resolution model being unable to resolve them in our case these features are mesoscale eddies with length scales of order of the first baroclinic rossby deformation radius 10 100 km preliminary analyses ryzhov et al 2019 suggest that the filter size of w 21 of high resolution grid intervals 150 km in physical units is adequate but we also tested w 41 as a tribute to the unavoidable sensitivity analysis the eddy fields calculated on the high resolution spatial grid 513 513 were coarse grained to be fed into the low resolution 129 129 model by averaging over four adjacent grid cells in each spatial direction guided by the fact that the lfv is eddy driven we substituted 7 into the governing equation 1 and for each layer obtained 8 q i t j ψ i q i f i ψ i q i ψ i q i h i ψ i q i l i ψ i q i where the operator h i contains all terms involving only the large scale components the linear operator l i contains the eddy tendency term and all linear terms involving the eddy components and the remaining term 9 f i j ψ i q i j ψ i q i j ψ i q i is the eddy forcing berloff 2005 due to nonlinear coupling of the large scale and eddy components the linear eddy term l i can be neglected since its contribution to the eastward jet as we checked is about 2 of that of the eddy forcing ryzhov et al 2019 established that the eddy forcing term when properly preprocessed with respect to the low resolution dynamics can be effectively added into the low resolution model to improve significantly the mean flow and transient spectrally treated characteristics of its solutions in this work our goal is to reduce the amount of the high resolution information by feeding the eddies rather than the eddy forcing information which depends on both the eddies and large scales into the augmented model 4 feeding the eddy field into the low resolution model with only the eddies being fed to the augmented model the external information is subtler which makes it harder for the low resolution model to resolve desired dynamics resembling the fine resolution reference solution such that the eastward extension of the jet is noticeably rectified and the low frequency variability is present at the same time gauging the possibility of reducing the amount of data necessary for successful parameterization and errors introduced due to the incompleteness of the data is practically important the governing equations for the augmented low resolution model are thus 10 q i t j ψ i q i f i ψ i q i ψ i q i h i ψ i q i where the small scale eddy fields ψ i q i are taken from the high resolution data and the prognostic low resolution large scale variables ψ i q i are continuously updated online during numerical integration of the model we used all 90 years of the daily output to extract the eddy fields and then linearly interpolated them in time in between the data records an important issue of determining the minimal length of the eddy history for the quality augmentation of the low resolution model is left outside the scope of the paper and will be addressed elsewhere we assessed the quality of the augmented low resolution solution by looking into the simulated eastward jet region focusing on its large scale circulation patterns evinced by the standard deviation in time and lfv the augmented model eastward jet has improved but is still substantially different from the reference truth as can be seen by comparing figs 3a and 1a similarly large discrepancies are seen in the augmented model dahd spectrum fig 3b which completely lacks the lfv the interactive eddy forcing fig 4a can be significantly less efficient because it is noticeably weaker than the true eddy forcing fig 4c we checked this by considering the more energetic eddy field extracted with the larger filter size w 41 fig 4b but although the resulting eddy forcing is as intensive as the true one the augmented model is still incapable of generating the lfv as implied by the dahd spectrum fig 3d from this we conclude that feeding even the most complete eddy fields into the model is still not sufficient for augmenting the solution so one has to use additional information from the high resolution data to induce the lfv it has been already established ryzhov et al 2019 that the true off line eddy forcing fig 4b generates the lfv in the augmented solution therefore we know that one way or another the model can be successfully augmented with the right amount of the extra information one way to add this information is by interactively projecting the augmented solution onto the leading true large scale eofs and this can be viewed as a weak statistical constraint imposed by the filtering the corresponding set of eofs are obtained through the standard singular value decomposition such that 11 q h r i pc i eof i where q h r i is the large scale true pv anomaly in the i th layer and in the matrix form rearranged so that the rows correspond to the spatial degrees of freedom whilst the columns represent their time evolutions pc i u i s i eof i v i where u i s i v i are the left eigenvector diagonal singular value and the right eigenvector matrices respectively is matrix transpose projection of the on line augmented pv anomaly q i onto some n eofs eof n i takes the form 12 q n i q i eof n i eof n i and the updated field q i is used on the next time step of the model eq 10 there are two key parameters at the projection step the number n of eofs and the time interval t p r o j between successive projections these parameters are chosen empirically for optimizing both the results and computational costs we found by sensitivity experiments that the number of the eofs should be relatively large and 2000 out of 12 9 2 16641 total eofs are good enough and t p r o j should not be much longer than 100 model days used here as the benchmark value with these parameters the augmented model recovered not only more than 95 of the lfv spectral power but also the correct frequencies we varied the number of the eofs and obtained qualitatively similar results within the 500 2000 range and the lower values degrade the solution since the eof projections are made infrequently the filtering process is computationally inexpensive the additionally filtered model solutions now exhibit the lfv as diagnosed by dahd spectra shown in fig 5a for w 21 and fig 5b for w 41 it is worth noting that even in the solution augmented with weaker eddies w 21 the lfv is also reproduced albeit it is not as energetic as with the stronger eddies w 41 the eastward jet extension is also reproduced similarly to the case without large scale filtering see fig 3 in addition to the detailed dahd spectral space time diagnostic of pv anomaly field it is also useful to consider the manifestation of lfv in the total potential energy which is a global characteristic of the solution fig 6 shows the fourier spectral analysis of the potential energy time series by the standard multitaper method percival and walden 1993 which reveals broadband lfv peaks at frequency 0 06 year 1 about 17 years period both for the reference high resolution and augmented low resolution solutions whilst the reference low resolution solution features no lfv with a mostly flat spectrum due to the projection the augmented solution acquires oversaturated high frequencies near the lfv peak this may be dealt with by carefully selecting the projection basis of the filtering procedure so to filter out spurious small scale effects and is beyond the scope of the current study as we aimed at imbuing the coarse resolution solution with the correct lfv finally we would like to emphasize that feeding the eddies to induce the augmenting eddy forcing in the low resolution model eq 9 is absolutely necessary for generating the lfv and we verified this by turning it off if the filtering based on the eof projection procedure is applied alone it does not augment the solution thus confirming that the main component of the parameterization is the eddy forcing 5 statistical emulation of the eddy field here we developed data driven statistical emulators of the true eddy field for feeding them into the low resolution model instead of the original high resolution eddy fields the number of statistical emulation methods has recently surged including stochastic approaches in climate science penland and matrosova 2001 strounine et al 2010 franzke et al 2015 kondrashov et al 2015 chen et al 2016 palmer 2019 seleznev et al 2019 foster et al 2020 as well as other machine learning deep learning methods developed for fluid dynamics applications brunton et al 2020 bolton and zanna 2019 the detailed analysis of emulated eddy fields is beyond the scope of this study and in the context of assessing the skill of our emulators we focus solely on one of the central problems in climate ocean model simulations namely the correct rectification of the eddy field s impact on the large scale circulation thus we aimed for the solution of the low resolution model when augmented by an emulated eddy field to be able to reproduce the long term statistics of the high resolution reference solution we utilized the same skill measures as for the true eddy field explored in previous section these are the geometrical shape of the large scale circulation patterns as well as the manifestation of the lfv we used a 30 000 day long high resolution dataset of the eddy stream function ψ ˆ for the three layers combined the dataset is then coarse grained onto the low spatial resolution 129 129 and further compressed by the pca we retained the leading 1000 pcs that account for 98 of the variability as a basic and most straightforward emulator we considered a linear stochastic regression model kravtsov et al 2005 2006 kondrashov et al 2005 2015 in the following discrete form 13 ξ t 1 ξ t a ξ t r t 0 where t is the time index in days ξ is a vector of pcs and a is a matrix of the regression coefficients while eq 13 can include additional model layers of hidden variables obtained in a sequential regression procedure it is not necessary here since the regression residual r t 0 is well approximated by a spatially correlated white noise r t 0 σ w where w is a wiener process and σ is the cholesky decomposition of the correlation matrix of the residuals from the model fitting the emulated pcs are obtained by initializing the model from the first data point of the training interval and by running it for 30000 days the eddy field is reconstructed in space from the emulated pcs by using the eof basis and then it is fed into the low resolution model in our augmentation procedure while this basic emulator of the eddy field yields a fairly reasonable geometrical structure of the jet extension in the augmented solution fig 7a it does not induce the lfv as evident by the flat spectral density curve of the full potential energy fig 7b which is also similar to the non augmented low resolution solution a closer analysis shows that the lack of the lfv in the augmented solution is related to the spectral content of the emulated eddy field in which energy at low frequencies is underestimated in comparison to the true eddy field in turn because the lfv in the true eddy field is considerably weaker than in the true reference solution it is challenging to capture it by an emulator based on pca pcs which typically mix different temporal scales the dahd method section 2 2 and appendix a provides a novel emulation alternative as it combines identification of frequency ranked modes and their efficient modeling it extracts pairs of data adaptive harmonic modes dahms that form an orthonormal set of spatial patterns oscillating harmonically in time and thus represent global monochromatic space time filters projection of the dataset ontodahms yields pairs of narrowband time series of data adaptive harmonic coefficients dahcs which are modulated in amplitude but do not mix temporal scales chekroun and kondrashov 2017 showed that the stuart landau sl stochastic oscillator a nonlinear oscillating system near a hopf bifurcation and driven by an additive noise is best suited to model amplitude modulations and frequency for the narrowband and in phase quadrature time series of a dahc pair ζ t f ζ t f associated with a given spectral pair λ f λ f see section 2 2 and appendix b here written in a compact form with a complex number notation 14 z t 1 f z t f μ f i γ f z t f 1 i β f z t f 2 z t f ϵ t where z t f ζ t f i ζ t f μ f γ f and β f are real parameters and ϵ t is an additive noise furthermore multiple sl oscillators associated with the same non zero frequency are linearly coupled and synchronized across frequencies by the pairwise correlated white noise while the model parameters are estimated by a regression with constraints see appendix b for numerical details the original dataset with its multiple time scales can be modeled in a computationally efficient manner since the contribution of each temporal frequency is simulated in parallel kondrashov et al 2018a developed a stochastic dahd emulator for the lfv in the model considered and here we extended these results to the eddies we used the leading d 100 pcs of the eddy streamfunction capturing 70 of the variance and applied the dahd with the embedding window of m 100 days then we fit the model of coupled d 100 stochastic oscillators for the dahcs and obtained their emulations for the m 100 frequencies after emulated dahcs were back transformed into the space time eddy field by using dahms and eofs and combined across all the emulated frequencies we fed the outcome into the augmented model the geometrical shape of the augmented solution is again reproduced fairly well and it is very similar to fig 7a not shown for brevity furthermore since the lfv is now better captured in the emulated eddy field compare to the high resolution truth it is also induced in the augmented solution fig 7b albeit it is less energetic then when the true eddy field is used see fig 6 6 conclusions in this paper we focused on improving solutions of an eddy permitting low resolution model by augmenting it with the information from the reference high resolution model solution which was treated as the observed truth our approach can be viewed as a basis for developing data driven parameterizations for the mesoscale oceanic eddies and their effects and in perspective for other types of turbulent fluid motions ultimately the parameterization should involve statistical emulations of the key unresolved or under resolved flow features we adopted a systematic approach towards such a parameterization framework this paper is the second one in the series after ryzhov et al 2019 for the ocean circulation model we considered the classical wind driven double gyres in the quasigeostrophic approximation with 3 active isopycnal layers and in an idealized closed midlatitude basin configuration solutions of the double gyre model are notoriously sensitive to the spatial grid resolution which is typical for the general ocean circulation models two prominent flow features which are crucially dependent on the resolution are in the focus of our study 1 the eastward jet extension of the western boundary currents with its adjacent recirculation zones and 2 the intrinsic large scale low frequency interdecadal variability of the gyres that is most pronounced in the eastward jet region both of these features are essentially mesoscale eddy driven therefore for their dynamical representation in the model the eddies have to be either properly resolved which is computationally expensive or adequately parameterized in terms of a simpler model in the high resolution reference solution both of the key features are well represented whereas the low resolution reference solution lacks any of them motivation for including 1 is straightforward because any eddy parameterization is first of all tested for its ability to simulate the large scale climatological fields motivation for including 2 is to test the ability of the parameterization to simulate intrinsic climate variabilities similar to the relatively well understood interdecadal variability featured in our model our hope is that testing mesoscale eddy parameterization skills will eventually include climate variability signals as the standard test beds our model augmentation procedure involves the following main steps first the high resolution true solution is decomposed into large scale and small scale eddy flow components by simple moving average filtering in space this flow decomposition is neither unique nor obviously constrained by dynamical or statistical arguments here we only assumed that the filter width should be about scaled with the first baroclinic rossby deformation radius since our study targets mesoscale eddies in the prequel study ryzhov et al 2019 the decomposed flow components were used to find the history of the eddy forcing which is just part of the advection operator that involves the eddy field then this history was coarse grained and applied to augment the low resolution model with many analyses and sensitivity studies attached to this statement and reported in the paper in the present study we extended the approach by supplying the primary eddy fields instead of the eddy forcing which is a higher level and subtler information moreover we tested the augmentation procedure skills in terms of the challenging reproduction of the lfv the eddy field component was interactively coupled with the corresponding low resolution model solution which was treated as the simulated large scale flow component via the on line eddy forcing operator which can be viewed as an additional dynamical constraint imposed on the augmentation procedure we found that the augmentation significantly improved representation of the eastward jet extension but the lfv was still missing the immediate hypothesis was that this was because the eddies are too weak hence the interactive eddy forcing was too weak to generate the lfv we tested this hypothesis by increasing the filter size used to extract the eddies and the resulting new eddy forcing turned out to be of the same intensity as the true eddy forcing however this further improved the modeled eastward jet but did not generate the lfv from this we concluded that the lfv was crucially dependent on the correlations between the large scale flow and the eddy forcing which were not fully respected by the augmentation procedure we also realized that the eddy history alone was not sufficient and some additional information had to be supplied as part of the augmentation we do not yet have the ultimate answer on what this information should be but in order to make progress we decided to supply some large scale flow information in terms of interactive weak filtering of the simulated large scale flow towards the observed truth this idea was implemented as a statistical filtration interactively projecting the simulated transient flow anomalies onto the leading empirical orthogonal functions eofs of the reference high resolution true flow this approach worked well and we experimentally found the optimal number of the eofs and the optimal frequency of the applied filtering procedure so that the lfv was almost fully recovered since the filtering can be applied infrequently about every 100 days in our case rather than continuously which is also possible its computational cost is nearly negligible however the exact amount of information needed from the high resolution truth for a correct rectification of the lfv remains unknown and its assessment should be addressed elsewhere we hypothesized that this information should contain correct correlations between the eddy and large scale fields we also demonstrated that the filtering was of secondary importance relative to the supplied eddy forcing because when the latter was switched off the filtering alone was not capable of augmenting the solution to any acceptable level finally we developed a statistical emulation of the eddy field as spatio temporal stochastic process and used it in our augmented procedure results showed that the frequency ranked data adaptive harmonic decomposition dahd emulator reproduces the lfv substantially better than the pca based linear stochastic model an agenda for further research stemming from this paper is to build on and improve statistical emulators for the eddy field as well as to consider extending the proposed approach beyond the relatively simple quasigeostrophic approximation to comprehensive general circulation models constraining the large scale eddy flow decomposition and making it consistent with the low resolution ocean model is also very important finally adding new criteria e g higher order statistical moments and spatio temporal correlations for assessing eddy parameterization skills should not be too far away declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank dr j maddison and one anonymous reviewer for constructive comments that helped improve this manuscript this research was supported by the national science foundation nsf usa grants o c e 1658357 and the natural environment research council nerc grant n e r 011567 1 pavel berloff also gratefully acknowledges funding by nerc uk grant no n e t 002220 1 and leverhulme trust uk grant no r p g 2019 024 and the moscow center for fundamental and applied mathematics supported by the agreement 075 15 2019 1624 with the ministry of education and science of the russian federation dahd analysis was supported by the russian science foundation grant no 18 12 00231 we would like to acknowledge the high performance computing support from cheyenne doi 10 5065 d 6 r x 99 h x provided by ncar s computational and information systems laboratory sponsored by the nsf usa the dahd toolbox is available at http research atmos ucla edu tcd dkondras software html appendix a data adaptive harmonic decomposition dahd here we present a brief summary of the dahd frequency domain implementation and stochastic emulation methodology following chekroun and kondrashov 2017 kondrashov and chekroun 2018 kondrashov et al 2018a b and tailored to high dimensional datasets we consider a multivariate time series x t x 1 t x d t formed with d spatial channels and t 1 n time points sampled evenly double sided unbiased cross correlation coefficients ρ p q m are estimated for all the pairs of channels p and q and time lag m up to a maximum m 1 15 ρ p q m 1 n m t 1 n m x p t m x q t 0 m m 1 ρ q p m m 0 where m is the embedding window and each of ρ p q m sequences is of length m 2 m 1 the dahd numerical algorithm computes its spectral elements λ j w j j 1 d 2 m 1 by utilizing a d d symmetrized complex cross spectral matrix s f built from the fourier transforms of the cross correlation sequences see eq 4 the data adaptive harmonic modes dahms represent collection of spatio temporal patterns w j e 1 j e d j oscillating with different but single frequency f in time embedded space 1 m m 16 e k j m b k j cos 2 π f m θ k j 1 k d where the amplitudes b k j and phases θ k j are data adaptive f takes distinct m values that are equally spaced in nyquist interval 0 0 5 17 f ℓ 1 m 1 ℓ 1 m 1 2 and λ j informs on energy conveyed by w j in particular for each f 0 there are 2 d positive negative eigenelements which are necessarily paired as λ k f λ k f k 1 d while the phases for the associated dahm pair w k f w k f satisfy θ k θ k π 2 i e these modes are shifted by one fourth of the period and are thus always in exact phase quadrature similar to the sine and cosine pair in the fourier analysis but in a data adaptive and global in space fashion there are also d non paired spectral elements λ k w k associated with the frequency f 0 the fourier transforms of the dahms are computed as eigenvectors of the matrix s f s f chekroun and kondrashov 2017 theorem v 1 and eq 74 18 s f s f w k f λ k 2 w k f and spatiotemporal patterns of w k f w k f are obtained then by the inverse fourier transform a projection of x onto given w j yields the time series of the dahd expansion coefficients dahcs 19 ζ j t m 1 m k 1 d x k t m 1 e k j m where 1 t n m 1 the time series of a given dahc pair ζ k t ζ k t associated with the modes w k f w k f at the frequency f 0 are narrowband nearly in phase quadrature and heavily modulated in amplitude appendix b frequency ranked stochastic emulators the collective behavior of the d pairs at the frequency f 0 see appendix a is simulated by a system of linearly coupled stuart landau stochastic oscillators 20 d ζ k d t β k f ζ k α k f ζ k σ k f ζ k ζ k 2 ζ k 2 i k d a i k f ζ i i k d b i k f ζ i ϵ k d ζ k d t α k f ζ k β k f ζ k σ k f ζ k ζ k 2 ζ k 2 i k d c i k f ζ i i k d d i k f ζ i ϵ k where 1 k d the model parameters are estimated by a pairwise multiple linear regression with linear constraints on α k f and β k f to ensure antisymmetry for the linear coupling within a given pair as well as equal and positive values σ k f 0 to ensure numerical stability the stochastic forcing in eq 20 is informed by regression residuals from the model fitting namely ϵ t ϵ t σ d w where σ is the 2 d 2 d cholesky decomposition of the correlation matrix of the residuals and d w is a 2 d valued wiener process the linear stochastic emulator eq 13 is used to model the time series of the dahcs associated with f 0 which are not paired any subset of dahcs can be convolved with its corresponding set of dahms to produce a partial or full reconstruction of the original dataset thus the following j th reconstructed component rc at time t and for channel k is defined as 21 r k j t 1 m t m l t u t ζ j t m 1 e k j m 1 m m where l t u t is a lower upper bound in 1 m that depends on time and the normalization factor m t equals m except near the ends of the time series the sum of all the rcs across all the frequencies recovers the original time series and stochastically emulated dahcs are back transformed to the phase space of the original dataset by using eq 21 
23954,downscaling broadscale ocean model information to resolve the fine scale swash zone dynamics has a number of applications such as improved resolution of coastal flood hazard drivers modeling of sediment transport and seabed morphological evolution a new method is presented which enables wave averaged models for the nearshore circulation to include short wave induced swash zone dynamics that evolve at the wave group scale i e averaged over the short waves such dynamics which cannot be described by construction through wave averaged models play a fundamental role in nearshore hydrodynamics and morphodynamics the method is based on the implementation of a set of shoreline boundary conditions sbcs in wave averaged models the chosen set of sbcs allows for proper computation of the short wave properties at a mean shoreline x l taken as the envelope of the actual shoreline the suitability of the approach is assessed through implementation of the sbcs into the regional ocean modeling system roms coupled to a spectral wave model inwave for ig waves and swan for wind waves as the aim is to assess the viability of the approach the sbcs are implemented only through a one way coupling to roms i e roms forcing the sbcs four different test cases with constant periodic and bichromatic offshore forcing are run to assess the model performances the main results of the analysis are a the proposed sbcs can well reproduce the shoreline motion and swash zone dynamics in there for all chosen tests rmse and bias less than 20 up to a cross shore resolution of 4 0m l 0 3 or l 0 5 and b implementation of the sbcs allows roms to accurately simulate the swash zone flows even at a resolution 40 times coarser than that needed by roms with its own wet dry routine to properly describe the same flows the latter result clearly demonstrates the major computational advantage of using the proposed sbcs we also show that most of the swash zone dynamics is due to the mean flow i e incoming riemann variable and the local at x l wave height however especially in the case of bichromatic waves the swash zone water volume content also seems to play a crucial role keywords nearshore swash zone shoreline boundary conditions 1 introduction the fundamental challenges connected with preserving the nearshore environment require an ever increasing and improving modeling capability this in turn entails tackling a number of issues fundamental is the bridging of the large number of scales characteristics of the nearshore dynamics from those of turbulence to those of tides and morphology the fundamental forcing of the nearshore dynamics are wind waves or short waves evolving at typical periods of o 10 s hence a single model that aims at properly describing the entire spectrum of nearshore dynamics must be highly a complete to deal with all fundamental mechanisms b flexible to have all such mechanisms be suitably represented and tuned and c powerful to compute in reasonable times the mentioned mechanisms for example when simulating extreme events such as coastal inundation hazard and potential changes in the coming century e g lewis et al 2019 not all processes can be resolved sufficiently for an efficient numerical solution simulation therefore basin scale ocean models are typically used to derive the drivers of coastal hazard which are then used to simulate the inundation hazard of such an event e g lewis et al 2011 therefore assumptions are used in these models to describe small scale dynamics through lumped descriptions averaging is the main simplifying mathematical tool a o brocchini 2013 over turbulence e g rans models les models over the vertical e g nswe models boussinesq type models over the wind or short waves i e wave averaged models when circulation models like roms shchepetkin and mcwilliams 2005 2009 haidvogel et al 2008 shorecirc haas and warner 2009 etc are coupled with phase averaged wave models like swan booij et al 1999 the wave contribution within such models is taken into account in a wave averaged sense thus we here refer to such coupled wave circulation models as wave averaged models as a result of their accuracy flexibility and power we focus on the wave averaged models which presently are the best candidates for the above mentioned scopes with the aim of improving their performances therefore we aim to parameterize a sub scale process into the roms model to resolve the impact of individual waves on water flows at the coast when coupled with either swan or inwave warner et al 2018 in particular our long range goal is to enable such models to accurately reproduce the swash zone sz dynamics i e those induced by the uprush and backwash of short waves over the beach face i e achieved once a full coupling is obtained proper modeling of wave interaction in the swash zone is fundamental for a proper description of the infragravity ig wave content within the surf zone such waves can be generated both in the surf zone and the swash zone ig wave generation in the surf zone occurs via the breakpoint oscillation mechanism of symonds et al 1982 and by the release of bound long waves e g biésel 1952 longuet higgins and stewart 1962 1964 however specific to the swash zone and of primary importance for the present contribution is the generation of ig waves resulting from the wave catch up inside the swash zone in particular larger and faster waves that catch up smaller and slower waves induce a significant frequency downshifting of the emerging signal see mase 1995 watson et al 1995 bertin et al 2018 similar interactions occurring within the swash zone are part of the swash swash interaction see baldock et al 1997 swash swash interaction occurs between incident waves and the runup or backwash of preceding waves the interaction generates a range of scales of new motion from mean flows swash setup to ig waves backwash bores and hydraulic jumps and turbulence e g brocchini and baldock 2008 however swash swash interaction is suitably described only if the actual shoreline motion is reproduced i e at the wave resolving level this cannot be achieved by wave averaged models because of their construction which allows calculation of the shoreline motion only at the wave averaged level in brief wave averaged models reproduce a shoreline that only mimics the motion of the wave resolved shoreline placing a zero depth condition at a wall that moves with the mean flow an example of the significant difference of the ig waves generated and radiated out to sea by a fixed rigid wall and by a wave resolved swash zone is provided in fig 1 this shows that artificially using a wall left panel in fig 1 to represent the shoreline boundary leads to errors in both the form and the intensity of waves radiated out to sea compared to the what happens in nature right panel in fig 1 the problem of prescribing proper shoreline boundary conditions sbcs within two dimensional horizontal 2dh wave averaged simulations has been tackled by brocchini and co authors who proposed brocchini and peregrine 1996 brocchini and bellotti 2002 antuono et al 2007 and validated bellotti et al 2003 a set of sbcs suited for implementation into wave averaged models this work will apply the above mentioned sbcs into a modeling framework to allow downscaling of water flows at typical ocean model output frequencies to much finer scales which could be applied to a number of studies such as coastal flood risk estimation modeling of sediment transport and seabed morphological evolution sbcs are computed into the regional ocean modeling system roms which coupled with a wave model swan or inwave accounts for short waves at a wave averaged level roms is a free surface terrain following primitive equation ocean model widely used by the scientific community for a diverse range of coastal applications e g warner et al 2005 2008 roms performances have also been favorably assessed with respect to a classical wave averaged model for the nearshore circulation like shorecirc the choice of the models i e roms coupled with swan inwave is dictated by the reliability and extensive use within the coastal ocean community of the mentioned circulation model however implementation of the proposed sbcs is similarly viable within other circulation models that account for the short wave dynamics at a wave averaged level in other words while the detailed implementation procedure obviously varies as a function of the specific coding of each circulation model the overall implementation procedure is completely general section 2 provides a brief illustration of flow models and sbcs the implementation of such sbcs as a predictive tool to be one way coupled to roms i e roms forcing the sbcs is described in section 3 section 4 provides a number of sample test cases that show how the proposed sbcs can actually reproduce the shoreline motion at reduced computational costs in section 5 we summarize the main results and suggest how the proposed sbcs can be implemented for fully coupled simulations section 6 closes the paper providing what we think are the main conclusions of our analyses 2 models this section briefly describes the different components of the modeling system and the theoretical model for the calculation of the sbcs that were added the circulation model roms has been coupled either with the wave model swan or with the wave model and wave group driver inwave within the coupled ocean atmosphere wave sediment transport modeling system coawst warner et al 2010 in order to model the surfzone dynamics 2 1 roms the ocean model roms belongs to the general class of free surface terrain following numerical models that solve the three dimensional reynolds averaged navier stokes equations rans using the hydrostatic and boussinesq approximations the hydrostatic primitive equations for momentum are solved using a split explicit time stepping scheme which requires special treatment and coupling between barotropic fast and baroclinic slow modes the free surface computed from the nswe introduces waves that propagate at a speed g h which imposes a restrictive time step thus these equations are integrated in a barotropic mode using a short time step the model contains also a wetting and drying scheme to describe the evolution of the wave averaged shoreline motion warner et al 2013 the wet dry algorithm compares the total depth of water at the cell center if the total depth is below a user defined threshold value d c r i t then water flux is prevented from leaving that cell from all of its faces warner et al 2013 roms provides a flexible structure that allows multiple choices for many of the model components in this paper the vortex force terms are based on kumar et al 2012 while radiation stress terms are based on mellor 2008 with the bottom boundary layer bbl represented by a simple drag coefficient expression using a quadratic bottom friction 2 2 swan the model simulating waves nearshore swan is a phase averaged third generation spectral wave model specifically designed for shallow waters that solves the action balance equation in either stationary or non stationary mode holthuijsen 2010 the action density i e the wave energy density e divided by the relative frequency n e σ is used as fundamental flow variable because the action density is conserved in the presence of currents swan simulates wind wave generation and propagation in coastal waters and includes the processes of refraction diffraction shoaling wave wave interactions and dissipation due to whitecapping wave breaking and bottom friction specific formulations for wind input bottom stress whitecapping wave wave interactions etc are described in detail in booij et al 2004 2 3 inwave inwave is a wave model and wave group driver that allows for simulating infragravity wave frequencies with periods between 30s and 5min wave groups can be imposed parametrically as done here or statistically created from swan frequency directional spectra the time series of the envelope of wave action density is imposed at the boundary as is an associated bound wave inwave propagates refracts and dissipates the wave action density and roms resolves the hydrodynamics associated with the bound wave the waves are driven at the boundary by integrating the two dimensional spectra from swan to compute a time series of wave action density envelope inwave transports the wave action density and it is resolved on the roms ocean model grid and interacts with the ocean currents on the infragravity time scale 2 4 sbcs a detailed analysis of different definitions of mean shoreline in brocchini and peregrine 1996 showed that such a mean interface cannot be uniquely defined however it was shown that flow properties can be unambiguously defined within the wet region when the envelope of the rundown positions is taken as a boundary between the wet and dry regions over a beach a set of equations built on the basis of an integral model for the swash zone were proposed to provide sbcs at the wet dry interface with this purpose the nswe equations 2 4 and 4 1 of brocchini and peregrine 1996 are integrated over the swash zone width that is between the lower x l and the upper x h boundary of the swash zone in this context x l is the envelope of the rundown positions while x h corresponds to the maximum runup see fig 2 these equations are written in a reference system where the still water level is z 0 with x and y representing the cross shore and the long shore directions respectively this introduces both local flow properties calculated at x l and integral flow properties of the whole swash zone such as 1 v x l x h d d x p x l x h u d d x ϒ x l x h τ d x which are respectively the volume of water in the swash zone v the momentum of water in the swash zone p and the seabed friction force inside the swash zone ϒ here u u v is the depth averaged horizontal velocity d the total water depth and τ the seabed shear stress each flow property i e u and d which we can call f is split into a long period f and a short period component which may differ inside and outside the swash zone by a reynolds type decomposition as follows 2 f x t f f if x is outside the sz f f ˆ if x is inside the sz the decomposition is made assuming the swash motion is almost entirely assigned to short wave contribution x l being driven by the long period motions sbcs for one dimensional horizontal 1dh flow propagation written in terms of mean flow velocity u are shown in brocchini and bellotti 2002 as follows 3 d x l d t u 1 d d v ˆ d t 4 d d t p x ˆ d x l d t v ˆ g α v ˆ ϒ x u 2 d x l d t 2 d 2 u d d x l d t u 2 g 2 d 2 q d s here represents averaging of the short waves and ˆ indicate short wave fluctuations outside and inside the swash zone respectively α is the beach slope g the acceleration due to gravity and some integral see eq 1 and local 5a d u mean water depth and the mean water velocity at x l 5b q u d short wave mass flux 5c s u 2 d u 2 d g 2 d 2 radiation stress flow variables are used the proposed sbcs provide contribution due to short waves through averages of products of short wave properties these are represented by the averages of the integral properties of the swash zone such as v ˆ ϒ x and p x ˆ and by the short wave correlations at the lower boundary x l of the swash zone i e q and s all these terms which evolve at the wave group level provide explicit extra forcing by the short waves to solve this set of equations eqs 3 and 4 for x l d x l and u x l a third equation is needed brocchini and bellotti 2002 brocchini and bellotti 2002 pointed out that this must provide information on the processes in the domain seaward of the swash zone and proposed the positive incoming riemann variable r u 2 g d to be the carrier of such information r propagates from the interior of the domain towards the shoreline along positive characteristics of the nswe mean flow özkan haller and kirby 1999 brocchini and bellotti 2002 6 d r d t g α 1 d d s d x along d x d t u g d u c with c g d the addition of this third equation to a simplified version of eqs 3 and 4 see brocchini and bellotti 2002 leads to the following set of equations providing suitable sbcs for nearshore circulation wave averaged solvers 7a d x l d t r 2 g h 4 c v α d h d t 7b d x l h 2 7c u x l r 2 g h where h is the wave height and c v is a known coefficient relating the volume of water in the sz i e that associated with the short waves the proposed sbcs are such that 1 the shoreline motion is forced by a the low frequency contribution carried by r along positive characteristics from the offshore region to the shoreline and b the short wave contribution which after the simplifications given by brocchini and bellotti 2002 is represented by the rate of change of the water volume in the swash zone proportional to d h d t through the coefficient c v and evolving at a wave averaged or group wave scale 2 the water depth at the mean shoreline x l is about half of the local wave height as demonstrated through simple geometrical arguments by bellotti et al 2003 3 the local cross shore velocity depends on the local shallow water velocity and on r when included in wave averaged models this set of equation are sbcs that work as a porous wall where momentum and mass exchange conditions are imposed and allow proper description of the wave wave interactions within the swash zone e g mase 1995 3 sbcs implementation in view of the fundamental importance of the positive riemann function on the calculation of eqs 7 roms has been partially modified in order to use roms and swan or inwave variables interpolated along the positive characteristics c 8 d x d t u c to solve equations 6 and 7 the 2 d roms kernel adopts a predictor corrector scheme to solve the depth averaged continuity and momentum equations however for the present initial implementation of the sbcs we decided to evolve our solution only at the corrector step among others u d and the vertically integrated wave averaged cross shore radiation stress s are roms variables required for the sbcs computation the former two are updated at each barotropic time level while the latter is updated only at baroclinic time level for this reason the sbcs solution is evolved only at the first barotropic time level corrector of each baroclinic time level 3 0 1 interpolation of the flow properties along the characteristic curves in the horizontal x y plane the state variables are arranged in a way equivalent to the well known arakawa c grid in particular the variable d is located at the center of the cell grid the so called r h o points while u and v variables are located at the center of the cell sides the so called u points and v points see figure 1 of haidvogel et al 2008 or fig 3 for the 1 d case with few exceptions a centered second order finite difference approximation is adopted in the horizontal haidvogel et al 2008 when x l is calculated from eq 7a it can fall within five possible different wet dry areas see fig 3 that depend on the location of the wet dry interface on the computational grid for all the cases a two points lagrangian interpolator has been used 1 x l is completely surrounded by wet points fig 3 case a both rho points h and d and u points u and s seaward and landward of x l are wet flow properties at x l are calculated by interpolating their values known at the grid nodes rho points and u points seaward and landward of x l surrounding x l 2 x l is surrounded by wet rho points while the u point seaward of x l is wet and the u point landward of x l is dry fig 3 case b in this case h and d at x l are retrieved as for the former case for u and s a different approach is needed since no information for these properties is available at the u point landward of x l here we first calculate the instantaneous shoreline position x s see brocchini and peregrine 1996 for the calculation of x s then u and s are calculated at x l interpolating their values from the u point seaward x l and x s where obviously all the flow properties and thus u and s are equal to zero 3 x l is surrounded by wet u points while the rho point seaward of x l is wet and the rho point landward of x l is dry fig 3 case c in this case u and s at x l are calculated by interpolating there the values of u and s from the u points surrounding x l for h and d this is not possible since no information on such properties is available at the rho point landward of x l further in this case we first calculate x s then h and d are estimated at x l by interpolating there the values of h and d from the rho point seaward of x l and x s 4 the rho point and the u point seaward of x l are wet and the rho point and the u point landward of x l are dry fig 3 case d all the flow properties and wave parameters are calculated at x l by interpolating their values from the rho point and the u point seaward of x l and x s 5 x l is completely surrounded by dry points fig 3 case e here the last wet point of the computational domain is first found and used as x l and then c is calculated once the flow properties at x l are known the next point along c can be calculated however there is still the chance that such a point falls within one of the above cases thus the same procedure is then repeated for every point along c hereafter called x c 3 1 characteristic curves and propagation of the positive riemann variable the last wet point of the computational domain is taken to coincide with x l fig 4 dotted red line for a few time steps in this work from 10 to 1000 depending on the specific baroclinic time step of the simulation along characteristic curves fig 4 black lines space and time are linked by characteristic speed see eq 8 at time step n starting from x l eq 8 is integrated backward in time either for an imposed time span or for a time large enough to reach the breaking point see fig 4 for an illustration of this we use the breaking point as the reference location for the backward integration because it is there that the most fundamental nearshore dynamics e g wave breaking turbulence generation wave energy dissipation start once c is known the r variable is propagated along c starting from the point most offshore of c up to the shoreline solving equation 6 the integration of eq 6 is achieved using a finite different numerical scheme as simple as 9 r i 1 r i g α δ t u c i 1 s 11 i 1 s 11 i d i u c i 1 3 2 shoreline evolution when r at the shoreline is known together with the wave height from the wave driver eq 7a can be solved hence at the lowest order of approximation the new position of the shoreline x l n 1 fig 4 red solid line can be obtained from 10 x l n 1 x l n d x l d t δ t 3 3 look ahead to a two way coupling the idea underlying the ongoing two way coupling is to have our sbcs time step with the existing roms wet dry scheme in brief with reference to a cross shore section of the computational domain from the off shore to the shore with grid cells indexed from 1 to i at a generic time step n the new sbcs are solved to provide the position of the mean shoreline at time step n 1 hence at time step n if x l lies in an area deemed to be wet then variables needed for the calculation of the sbcs flow properties will be interpolated at x l for a subsequent update through the sbcs alternatively in case of x l lying in an area determined to be dry the variable will be linearly extrapolated to x l see also lynett et al 2002 for a similar procedure to compute the variables needed by the sbcs to update the flow the solution will be stepped forward at time n 1 by i computing directly the new position of x l through the sbcs and ii updating the roms flow properties at wet nodes close to x l with a direct stencil directly bearing on x l at time n and indirect stencil based on the wet node closest to x l at time n influence of the sbcs 3 4 evaluation of the model performance two statistics namely the mean bias bias and root mean square error rmse normalized by the range of the reference data were used to evaluate the performance of the sbcs and are defined as 11 b i a s i 1 n m t i m r e f i n max m r e f min m r e f 12 r m s e i 1 n m t i m r e f i 2 n max m r e f min m r e f where n is the number of time record m t is the solution to be tested and m r e f is the reference solution 4 applications four test cases are used to assess the sbcs capability in representing the shoreline and the swash zone properties the hydrodynamic model roms and the wave model the wave driver swan for test case 1 and test case 2 or the inwave model for test case 0 and test case 3 are run into the coawst modeling system in two different configurations i alone romsswan sl or romsinwave sl hereinafter or ii in conjunction with the newly implemented sbcs solution romsswan sbcs or romsinwave sbcs to be tested solutions obtained using romsswan inwave sl with a cross shore resolution of 0 05m or 0 1m i e between about 120 and 217 nodes per wavelength are used as reference solution romsswan sl ref or romsinwave sl ref hereinafter the wave induced momentum has been taken into account by either the vortex force test case 0 and test case 3 or the radiation stress formalisms test case 1 and test case 2 test case 0 was performed to assess the quality of the reference solution romsinwave sl ref used in all other benchmarking tests and also provide evaluation of the semi analytical solution romsinwave sbcs with this purpose the experimental condition of the case study called mr 10 from padilla and alsina 2018 has been reproduced the shoreline position computed with romsswan inwave sl is based on the wet dry algorithm warner et al 2013 and is here represented by the wet point that is farthest from the initial state in the cross shore direction test cases 1 and 2 use phase averaged wave spectra approaching a mildly sloping 1 80 planar beach with normal incidence the wave forcing has been simulated by swan specifying 36 directional bins and 11 frequency bins between 0 04s 1 and 1 0s 1 test case 1 is forced by a constant offshore significant wave height h s 0 hereinafter while test case 2 makes use of a sinusoidally varying h s 0 for test case 3 the inwave model was used for the propagation of bichromatic waves and the imposition of the associated bound wave approaching a mildly sloping 1 80 planar beach with a normal incidence fig 5 illustrates snapshots of the mean water level η at the maximum runup for the three cases of interest after steady state condition has been reached the results are shown along a cross shore section placed in the middle of the domain results are shown with the exception of fig 5 and fig 7 in dimensionless form using either the offshore input wave period t 0 test cases 1 and 2 or the offshore mean primary period t m t 0 1 t 0 2 2 test case 3 for the time scale and the offshore maximum wavelength l 0 and the offshore depth d 0 for horizontal and vertical length scales respectively all the parameters for each test case are listed in table 1 while the imposed offshore input waves in terms of significant wave height h s 0 or water level are shown in fig 6 the romsswan inwave sbcs performance in reproducing the mean shoreline is quantitatively assessed using statistical indices like bias eq 11 and the rmse eq 12 which are given in table 2 for the tests performed 4 1 test case 0 validation of the reference solution we here provide an assessment of the value of our reference solution romsinwave sl ref and a first evaluation through laboratory data of the proposed romsinwave sbcs solution in this regard we reproduce numerically roms coupled with inwave the same experimental condition of one of the case studies mr 10 proposed by padilla and alsina 2018 a bichromatic wave with initial amplitudes of the primary components a 1 a 2 0 015 m and primary frequencies f 1 0 637 hz and f 2 0 563 hz are imposed at the offshore boundary of a gentle sloping beach 1 100 with a maximum depth of 0 5m fig 7a represents the original shoreline from padilla and alsina 2018 while in fig 7 b the input bichromatic wave in terms of water level is shown the resulting shoreline computed by romsinwave sl ref l 0 200 0 05m and the ones calculated with romsinwave sbcs at resolutions l 0 200 0 05m and l 0 7 1 5m are compared with the experimental shoreline averaged over one second in fig 7c and fig 7d respectively romsinwave sl ref represents well the experiment of padilla and alsina 2018 thus confirming that romsinwave sl ref is a good benchmark for the case studies that follow fig 7 c b i a s 1 72 and r m s e 7 16 further romsinwave sbss well reproduced the mean shoreline at the two resolutions used see table 2 4 2 test case 1 constant forcing for this test case a constant incoming wave with h s 0 of approximately d 0 6 fig 6a and a l 0 170 cross shore resolution have been used for both romsswan sl ref and romsswan sbcs steady state conditions are reached after 2 min of simulation i e after about 24 t 0 when the cross shore profiles of the significant wave height h s fig 8 top panel and η fig 8 bottom panel have become constant for the romsswan sl ref simulation the h s begins to decrease near x 3 l 0 due to wave breaking dissipation not surprising for a wave averaged model forcing the simulation with a constant h s 0 leads to a constant wave setup shoreward when the waves start breaking the wave height decreases rapidly fig 8 top panel and becomes close to 0 as they approach the shoreline this means that most of the wave energy is already lost before it can reach the shoreline with no energy left for the swash motion fig 9 represents the evolution of the shoreline x l respectively computed with romsswan sl ref and romsswan sbcs the great similarity between the two shorelines is evident and it is also confirmed by the statistic with the negative bias indicating a slight underestimation of the mean shoreline by romsswan sbcs see table 2 after an initial large run up and subsequent run down due to the cold start of the simulation the mean shoreline reaches a steady state as the consequence of the permanent wave setup the shoreline computed by romsswan sbcs is obtained through integration of eq 7a with the three terms of the right hand side shown in fig 10 the initial run up and run down are mainly driven by the first term that is r whereas the subsequent shoreline steady state seems to be the results of the balance between the first and the second terms in this test case the third term which corresponds to the time derivative of the swash zone water volume can be regarded as negligible thus the shoreline oscillation is completely associated with the mean flow and the local onshore velocity 4 3 test case 2 simple periodic forcing in this simulation the wave forcing consists of a wave train with a periodic h s 0 given by 13 h s 0 h m a x sin π t t r t where t t r is the wave train period and t is time for this test case h m a x d 0 6 and t t r 30 t 0 see fig 6 central panel this forcing allows for the generation of a signal evolving at the wave groups scale that leads to an oscillation of the mean shoreline using grids with different cross shore resolutions ranging from about 120 to 1 5 nodes per l 0 we tested various coarse cross shore resolutions to check the limit at which x l can be well reproduced with both romsswan sl and romsswan sbcs moving toward coarser resolutions we expect to lose the shoreline signal when romsswan sl is used rather while running romsswan sbcs we expect to still be able to reproduce x l the steady state in a statistical sense condition has been reached after 1 min of simulation i e after 15 t 0 this becomes evident observing the cross shore profile of the h s fig 11 top panel and η fig 11 bottom panel where after one minute of spin up the same pattern is repeated with period t t r the mean shorelines computed at several cross shore resolutions with both romsswan sl and romsswan sbcs are shown in fig 12 the two solutions obtained are consistent with the incoming wave forcing fig 6 central panel having its same period at really high cross shore resolution fig 12a both romsswan sl and romsswan sbcs reproduce x l in a similar fashion however using romsswan sl with an increasingly coarse cross shore resolution leads to a rapid loss of the shoreline signal the same is not true for the shoreline calculated with romsswan sbcs which is well resolved up to 8 0m about half l 0 fig 12f cross shore resolutions when x l starts to be distorted it is also to be noted that moving toward coarser resolutions the shoreline calculated by romsswan sl moves landward while that computed by romsswan sbcs oscillates over the same spatial range of the well resolved case this appears more evident when observing fig 13 where the mean shorelines computed with romsswan sbcs at the different cross shore resolutions are compared with that obtained with the well resolved romsswan sl ref l 0 120 0 1m this visual analysis is also quantitatively supported by the statistics with good values of rmse up to the 8 0m l 0 3 cross shore resolution where the rmse becomes higher than 20 the decreasing negative bias going toward coarser resolutions indicates an increasing underestimation of the shoreline computed by romsswan sbcs see table 2 the shorelines computed with the romsswan sl and romsswan sbcs have an excursion of about l 0 3 such long wave runup induced by a fairly low wave height maximum wave height about d 0 6 is explained by the very low friction quadratic bottom drag coefficient of 0 005 and very mild slope used the x l and the three terms of equation 7a are shown in fig 14 in the top panel and the bottom panel respectively the first and second terms of equation 7a are still the shoreline main forcing as demonstrated by the rundown starting approximately when the two terms are equal however in this case the third term is not negligible this is especially true during the runup where the third term may be about one third of the second term and a fifth of the first term r is known all along the characteristic curves c see fig 23 for an example of characteristic curves by integration of eq 6 once r is calculated u and d can be easily retrieved by 14a d r u 2 4 g 14b u r 2 g d this set of equations is used to calculate d and u from the sbcs up to the shoreline x l where instead they are calculated from eqs 7b and 7c solving this set of equations along the characteristic curves calculated during the computation gives a cloud of points in the space time domain where u and d are known which can be both represented by contour maps and if needed interpolated onto a regular grid contour maps of near shoreline depth d and cross shore velocity u are represented by fig 15 and fig 16 respectively for both figures the top panel is the solution calculated by romsswan sl ref then moving toward the bottom we find d and u calculated by romsswan sbcs with resolutions l 0 12 l 0 3 and l 0 2 the near shoreline d and u are well represented by the sbcs up to a resolution of l 0 3 with a moderate underestimation of the cross shore velocity during the backwash phase 4 4 test case 3 bichromatic waves here bichromatic waves are imposed at the offshore boundary for this test case the envelope of the wave action density and the associated bound wave are generated by inwave the wave action density is propagated by inwave and the bound wave is propagated by roms the bichromatic wave emerges from the superimposition of two cosinusoidal monochromatic waves one with amplitude 0 2m and frequency f 1 0 168 hz and the other with amplitude 0 1m and frequency f 2 0 2016 hz with a mean primary period t m 5 46 s the superposition leads to an emerging wave period of t g 29 7 s this specific simulation has been run to highlight the model capability to represent a modulated swash zone this is important in view of the proper evolution of low frequency modes that dominate swash zone induced by bichromatic waves baldock et al 1997 madsen et al 1997 brocchini and bellotti 2002 dong et al 2009 it is exactly the weakness of wave averaging models to reproduce low frequency waves emerging from the swash zone dynamics that makes this test particularly significant further in this case we used grids with different cross shore resolutions ranging from about 217 to 2 7 nodes per l 0 to check the capability of romsinwave sbcs to reproduce x l the quasi steady state condition i e periodicity in modulated swash zone properties has been reached in less than 2 min i e after 20 t 0 see fig 18 thus also in this case after a short spin up period the same pattern see figs 17 18 for the main flow properties is periodically repeated in agreement with the modulated forcing wave see fig 6 right panel the mean shorelines computed at several cross shore resolutions with both romsinwave sl and romsinwave sbcs are shown in fig 18 while fig 19 illustrates the mean shorelines computed with both romsinwave sl ref l 0 217 0 1m and romsinwave sbcs again the proposed sbcs were able to reproduce x l see figs 18 19 and table 2 it was possible to properly represent x l up to an 8 0m about l 0 2 7 cross shore resolution for which the x l computed with romsinwave sbcs is underestimated when compared with the one of romsinwave sl ref see fig 19c the rmse for this test case was less than 20 even when the coarsest grid was used again the bias showed a negative trend with the only exception for the grid at 1 0m cross shore resolution where the bias was positive see table 2 the maximum flooding due to the bichromatic wave used for this simulation is larger than 0 6 l 0 the mean shoreline x l and the three terms of the right hand side of eq 7a are shown in the top panel and bottom panel of fig 20 respectively here the first of such terms is still dominant but the second and the third ones have comparable importance this underlines the importance of the swash zone water volume in determining the mean shoreline position x l in presence of low frequency waves contour maps of near shoreline depth d and cross shore velocity u are illustrated in fig 21 and fig 22 respectively for both figures the top panel is the solution calculated by romsinwave sl ref then moving toward the bottom we find d and u calculated by romsinwave sbcs with resolutions l 0 22 l 0 5 and l 0 2 7 further for this test case the near shoreline maps of d and u are well reproduced by the sbcs up to a resolution of l 0 2 7 where the flow properties are not well represented anymore like for test case 2 a moderate underestimation of the cross shore velocity during the backwash phase occurs the above results are really encouraging in view of the implementation of the sbcs in a predictive manner 5 discussion a new shoreline boundary condition has been incorporated into roms to resolve the cross shore position of the mean shoreline x l and the flow property there when using grids with cross shore resolution of 0 1m from 217 to 120 nodes per wavelength with a really small d c r i t it is possible to solve the equations of motion and the continuity equation in very shallow water i e up to the shoreline in this condition the model does not need any external boundary conditions to represent the swash zone since the fluid properties are calculated locally further the small d c r i t used is very close to 0 where the artificial shoreline conditions of rigid wall describe well the shoreline motion such simulations are extremely time consuming and computationally demanding and are thus often impractical however running roms in these conditions romsswan inwave sl ref allowed us to produce a benchmark solution to assess the value of the proposed sbcs solutions obtained with romsinwave sl ref were also positively assessed against the experimental data reproducing numerically one of the experiments of padilla and alsina 2018 the achieved results confirmed the solutions from romsswan inwave sl ref to be suitable benchmark to test the romsswan inwave sbcs model against in the proposed case studies we used three different wave forcings to assess the performances of the proposed sbcs forcing roms with waves of constant height test case 1 leads to a constant mean shoreline after an initial wave setup fig 9 here the wave setup is mainly due to the first term of the right hand side of eq 7a while the constant shoreline is the consequence of the mutual cancellation of the first and second terms see fig 10 the third term which represents the time derivative of the swash zone volume was close to zero for the wall simulation this together with the constant mean shoreline is in agreement with the symmetric swash due to a monochromatic wave sinusoidally varying h s 0 used in test case 2 allowed for the modulation of the mean shoreline here the time derivative of the swash zone volume is not zero see fig 14 which is in line with an asymmetric swash when the simulation was forced with a bichromatic wave as in test case 3 the flooded region was much larger than those of test case 1 and test case 2 because of the increased wave setup forced by the low frequency wave here the swash zone volume time derivative is even more crucial for the calculation of the mean shoreline x l thus pointing out the importance of a proper treatment of low frequency waves within the surf zone this is probably a consequence of the greater change in swash zone water volume due to the greater wave setup the characteristic curves calculated integrating equation 8 are shown in fig 23 it is interesting to note that during the run up the characteristic curves are steeper than during the run down this is especially true for test case 3 likely as a consequence of a greater water volume into the swash zone with the proposed sbcs we could reproduce both the shoreline and the flow properties in the swash zone close to those obtained by romsswan inwave sl ref achieved with 0 1m from l 0 217 to l 0 120 cross shore resolution but using a much coarser grid of 4 0m l 0 3 for the test case 2 and l 0 5 for the test case 3 at the coarsest resolution of about l 0 2 even the proposed sbcs cannot properly represent x l the spatial and time scales of the phenomena evolving in really shallow waters are small and need to be calculated at high time and spatial resolutions thus terms like the radiation stress which is a fundamental forcing of the sbcs may not be properly computed with too coarse resolutions and this also causes problems in the assessment of the shoreline motion in all our tests the sbcs described the shoreline motion well this makes romsswan inwave sbc a powerful tool to use for flooding simulations when the domain of interest is too large to use wave resolving models the time needed for the simulation run with romsswan inwave sl ref at l 0 120 or l 0 217 cross shore resolution is in the order of some hours while that carried out with romsswan inwave sbcs at a l 0 3 cross shore resolution only required a few minutes hence the great advantage in terms of computational costs of using the proposed sbcs is very evident see table 3 the simplified set of eqs 7 that describes the sbcs has been validated through a dedicated laboratory campaign characterized by complex swash zone dynamics also involving wave wave catch up i e bore merging within the swash zone see brocchini and bellotti 2002 thus the flow property at the predicted x l can account also for the process of bore merging in the swash zone this means that once fully coupled in wave averaged models the sbcs will provide forcing from a swash zone which accounts for all wave wave interaction dynamics once the c v parameter of eq 7 is properly set therefore the sbcs will allow the eventual generation of ig waves and for the proper modification and reflection out to sea of the incoming waves at the predicted mean shoreline 6 conclusions this work is the first step toward the inclusion of swash zone dynamics into nearshore wave averaged models here is a brief and schematic summary of what we believe are the main conclusions that can be drawn from our analyses 1 the sbcs deriving from an integral theoretical model of swash zone flows have been successfully calculated within the kernel of a wave averaging model for the first time using flow properties coming from the offshore region of the computational domain 2 most of the swash zone dynamics is due to the mean flow i e incoming riemann variable and to the local at x l wave height however also the swash zone water volume content also seems to play an important role for a proper calculation of the mean shoreline x l especially in the case of bichromatic waves 3 work is underway to achieve a two way coupling between the sbcs and roms however romsswan inwave sbcs is already a powerful tool for the study of coastal inundation at the wave averaged level 4 wave climate can be influenced by tidal dynamics with larger waves occurring during high water lewis et al 2019 in this perspective possible applications of the proposed methodology are within future coastal flood risk assessment methods to resolve future flood risk events due to waves and their interactions with ocean processes such as tides under different scenarios 5 finally the methodology here proposed can also be implemented into other coupled wave circulation modeling systems commonly used by the coastal community such as adcirc swan dietrich et al 2011 schism wwm schloen et al 2017 and fvcom swave mao and xia 2018 credit authorship contribution statement francesco memmola conceptualization methodology software formal analysis writing original draft writing review editing alessandro coluccelli software resources aniello russo resources writing review editing john c warner supervision writing review editing maurizio brocchini conceptualization methodology supervision writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments financial support from the onr global uk through the nicop morse research grant n62909 17 1 2148 and from the italian place project code ars01 00891 a national research programme funded by the italian ministry of university and research through pon r i 2014 2020 e fsc funds are gratefully acknowledged we wish to thank padilla and alsina 2018 for making their data available online 
23954,downscaling broadscale ocean model information to resolve the fine scale swash zone dynamics has a number of applications such as improved resolution of coastal flood hazard drivers modeling of sediment transport and seabed morphological evolution a new method is presented which enables wave averaged models for the nearshore circulation to include short wave induced swash zone dynamics that evolve at the wave group scale i e averaged over the short waves such dynamics which cannot be described by construction through wave averaged models play a fundamental role in nearshore hydrodynamics and morphodynamics the method is based on the implementation of a set of shoreline boundary conditions sbcs in wave averaged models the chosen set of sbcs allows for proper computation of the short wave properties at a mean shoreline x l taken as the envelope of the actual shoreline the suitability of the approach is assessed through implementation of the sbcs into the regional ocean modeling system roms coupled to a spectral wave model inwave for ig waves and swan for wind waves as the aim is to assess the viability of the approach the sbcs are implemented only through a one way coupling to roms i e roms forcing the sbcs four different test cases with constant periodic and bichromatic offshore forcing are run to assess the model performances the main results of the analysis are a the proposed sbcs can well reproduce the shoreline motion and swash zone dynamics in there for all chosen tests rmse and bias less than 20 up to a cross shore resolution of 4 0m l 0 3 or l 0 5 and b implementation of the sbcs allows roms to accurately simulate the swash zone flows even at a resolution 40 times coarser than that needed by roms with its own wet dry routine to properly describe the same flows the latter result clearly demonstrates the major computational advantage of using the proposed sbcs we also show that most of the swash zone dynamics is due to the mean flow i e incoming riemann variable and the local at x l wave height however especially in the case of bichromatic waves the swash zone water volume content also seems to play a crucial role keywords nearshore swash zone shoreline boundary conditions 1 introduction the fundamental challenges connected with preserving the nearshore environment require an ever increasing and improving modeling capability this in turn entails tackling a number of issues fundamental is the bridging of the large number of scales characteristics of the nearshore dynamics from those of turbulence to those of tides and morphology the fundamental forcing of the nearshore dynamics are wind waves or short waves evolving at typical periods of o 10 s hence a single model that aims at properly describing the entire spectrum of nearshore dynamics must be highly a complete to deal with all fundamental mechanisms b flexible to have all such mechanisms be suitably represented and tuned and c powerful to compute in reasonable times the mentioned mechanisms for example when simulating extreme events such as coastal inundation hazard and potential changes in the coming century e g lewis et al 2019 not all processes can be resolved sufficiently for an efficient numerical solution simulation therefore basin scale ocean models are typically used to derive the drivers of coastal hazard which are then used to simulate the inundation hazard of such an event e g lewis et al 2011 therefore assumptions are used in these models to describe small scale dynamics through lumped descriptions averaging is the main simplifying mathematical tool a o brocchini 2013 over turbulence e g rans models les models over the vertical e g nswe models boussinesq type models over the wind or short waves i e wave averaged models when circulation models like roms shchepetkin and mcwilliams 2005 2009 haidvogel et al 2008 shorecirc haas and warner 2009 etc are coupled with phase averaged wave models like swan booij et al 1999 the wave contribution within such models is taken into account in a wave averaged sense thus we here refer to such coupled wave circulation models as wave averaged models as a result of their accuracy flexibility and power we focus on the wave averaged models which presently are the best candidates for the above mentioned scopes with the aim of improving their performances therefore we aim to parameterize a sub scale process into the roms model to resolve the impact of individual waves on water flows at the coast when coupled with either swan or inwave warner et al 2018 in particular our long range goal is to enable such models to accurately reproduce the swash zone sz dynamics i e those induced by the uprush and backwash of short waves over the beach face i e achieved once a full coupling is obtained proper modeling of wave interaction in the swash zone is fundamental for a proper description of the infragravity ig wave content within the surf zone such waves can be generated both in the surf zone and the swash zone ig wave generation in the surf zone occurs via the breakpoint oscillation mechanism of symonds et al 1982 and by the release of bound long waves e g biésel 1952 longuet higgins and stewart 1962 1964 however specific to the swash zone and of primary importance for the present contribution is the generation of ig waves resulting from the wave catch up inside the swash zone in particular larger and faster waves that catch up smaller and slower waves induce a significant frequency downshifting of the emerging signal see mase 1995 watson et al 1995 bertin et al 2018 similar interactions occurring within the swash zone are part of the swash swash interaction see baldock et al 1997 swash swash interaction occurs between incident waves and the runup or backwash of preceding waves the interaction generates a range of scales of new motion from mean flows swash setup to ig waves backwash bores and hydraulic jumps and turbulence e g brocchini and baldock 2008 however swash swash interaction is suitably described only if the actual shoreline motion is reproduced i e at the wave resolving level this cannot be achieved by wave averaged models because of their construction which allows calculation of the shoreline motion only at the wave averaged level in brief wave averaged models reproduce a shoreline that only mimics the motion of the wave resolved shoreline placing a zero depth condition at a wall that moves with the mean flow an example of the significant difference of the ig waves generated and radiated out to sea by a fixed rigid wall and by a wave resolved swash zone is provided in fig 1 this shows that artificially using a wall left panel in fig 1 to represent the shoreline boundary leads to errors in both the form and the intensity of waves radiated out to sea compared to the what happens in nature right panel in fig 1 the problem of prescribing proper shoreline boundary conditions sbcs within two dimensional horizontal 2dh wave averaged simulations has been tackled by brocchini and co authors who proposed brocchini and peregrine 1996 brocchini and bellotti 2002 antuono et al 2007 and validated bellotti et al 2003 a set of sbcs suited for implementation into wave averaged models this work will apply the above mentioned sbcs into a modeling framework to allow downscaling of water flows at typical ocean model output frequencies to much finer scales which could be applied to a number of studies such as coastal flood risk estimation modeling of sediment transport and seabed morphological evolution sbcs are computed into the regional ocean modeling system roms which coupled with a wave model swan or inwave accounts for short waves at a wave averaged level roms is a free surface terrain following primitive equation ocean model widely used by the scientific community for a diverse range of coastal applications e g warner et al 2005 2008 roms performances have also been favorably assessed with respect to a classical wave averaged model for the nearshore circulation like shorecirc the choice of the models i e roms coupled with swan inwave is dictated by the reliability and extensive use within the coastal ocean community of the mentioned circulation model however implementation of the proposed sbcs is similarly viable within other circulation models that account for the short wave dynamics at a wave averaged level in other words while the detailed implementation procedure obviously varies as a function of the specific coding of each circulation model the overall implementation procedure is completely general section 2 provides a brief illustration of flow models and sbcs the implementation of such sbcs as a predictive tool to be one way coupled to roms i e roms forcing the sbcs is described in section 3 section 4 provides a number of sample test cases that show how the proposed sbcs can actually reproduce the shoreline motion at reduced computational costs in section 5 we summarize the main results and suggest how the proposed sbcs can be implemented for fully coupled simulations section 6 closes the paper providing what we think are the main conclusions of our analyses 2 models this section briefly describes the different components of the modeling system and the theoretical model for the calculation of the sbcs that were added the circulation model roms has been coupled either with the wave model swan or with the wave model and wave group driver inwave within the coupled ocean atmosphere wave sediment transport modeling system coawst warner et al 2010 in order to model the surfzone dynamics 2 1 roms the ocean model roms belongs to the general class of free surface terrain following numerical models that solve the three dimensional reynolds averaged navier stokes equations rans using the hydrostatic and boussinesq approximations the hydrostatic primitive equations for momentum are solved using a split explicit time stepping scheme which requires special treatment and coupling between barotropic fast and baroclinic slow modes the free surface computed from the nswe introduces waves that propagate at a speed g h which imposes a restrictive time step thus these equations are integrated in a barotropic mode using a short time step the model contains also a wetting and drying scheme to describe the evolution of the wave averaged shoreline motion warner et al 2013 the wet dry algorithm compares the total depth of water at the cell center if the total depth is below a user defined threshold value d c r i t then water flux is prevented from leaving that cell from all of its faces warner et al 2013 roms provides a flexible structure that allows multiple choices for many of the model components in this paper the vortex force terms are based on kumar et al 2012 while radiation stress terms are based on mellor 2008 with the bottom boundary layer bbl represented by a simple drag coefficient expression using a quadratic bottom friction 2 2 swan the model simulating waves nearshore swan is a phase averaged third generation spectral wave model specifically designed for shallow waters that solves the action balance equation in either stationary or non stationary mode holthuijsen 2010 the action density i e the wave energy density e divided by the relative frequency n e σ is used as fundamental flow variable because the action density is conserved in the presence of currents swan simulates wind wave generation and propagation in coastal waters and includes the processes of refraction diffraction shoaling wave wave interactions and dissipation due to whitecapping wave breaking and bottom friction specific formulations for wind input bottom stress whitecapping wave wave interactions etc are described in detail in booij et al 2004 2 3 inwave inwave is a wave model and wave group driver that allows for simulating infragravity wave frequencies with periods between 30s and 5min wave groups can be imposed parametrically as done here or statistically created from swan frequency directional spectra the time series of the envelope of wave action density is imposed at the boundary as is an associated bound wave inwave propagates refracts and dissipates the wave action density and roms resolves the hydrodynamics associated with the bound wave the waves are driven at the boundary by integrating the two dimensional spectra from swan to compute a time series of wave action density envelope inwave transports the wave action density and it is resolved on the roms ocean model grid and interacts with the ocean currents on the infragravity time scale 2 4 sbcs a detailed analysis of different definitions of mean shoreline in brocchini and peregrine 1996 showed that such a mean interface cannot be uniquely defined however it was shown that flow properties can be unambiguously defined within the wet region when the envelope of the rundown positions is taken as a boundary between the wet and dry regions over a beach a set of equations built on the basis of an integral model for the swash zone were proposed to provide sbcs at the wet dry interface with this purpose the nswe equations 2 4 and 4 1 of brocchini and peregrine 1996 are integrated over the swash zone width that is between the lower x l and the upper x h boundary of the swash zone in this context x l is the envelope of the rundown positions while x h corresponds to the maximum runup see fig 2 these equations are written in a reference system where the still water level is z 0 with x and y representing the cross shore and the long shore directions respectively this introduces both local flow properties calculated at x l and integral flow properties of the whole swash zone such as 1 v x l x h d d x p x l x h u d d x ϒ x l x h τ d x which are respectively the volume of water in the swash zone v the momentum of water in the swash zone p and the seabed friction force inside the swash zone ϒ here u u v is the depth averaged horizontal velocity d the total water depth and τ the seabed shear stress each flow property i e u and d which we can call f is split into a long period f and a short period component which may differ inside and outside the swash zone by a reynolds type decomposition as follows 2 f x t f f if x is outside the sz f f ˆ if x is inside the sz the decomposition is made assuming the swash motion is almost entirely assigned to short wave contribution x l being driven by the long period motions sbcs for one dimensional horizontal 1dh flow propagation written in terms of mean flow velocity u are shown in brocchini and bellotti 2002 as follows 3 d x l d t u 1 d d v ˆ d t 4 d d t p x ˆ d x l d t v ˆ g α v ˆ ϒ x u 2 d x l d t 2 d 2 u d d x l d t u 2 g 2 d 2 q d s here represents averaging of the short waves and ˆ indicate short wave fluctuations outside and inside the swash zone respectively α is the beach slope g the acceleration due to gravity and some integral see eq 1 and local 5a d u mean water depth and the mean water velocity at x l 5b q u d short wave mass flux 5c s u 2 d u 2 d g 2 d 2 radiation stress flow variables are used the proposed sbcs provide contribution due to short waves through averages of products of short wave properties these are represented by the averages of the integral properties of the swash zone such as v ˆ ϒ x and p x ˆ and by the short wave correlations at the lower boundary x l of the swash zone i e q and s all these terms which evolve at the wave group level provide explicit extra forcing by the short waves to solve this set of equations eqs 3 and 4 for x l d x l and u x l a third equation is needed brocchini and bellotti 2002 brocchini and bellotti 2002 pointed out that this must provide information on the processes in the domain seaward of the swash zone and proposed the positive incoming riemann variable r u 2 g d to be the carrier of such information r propagates from the interior of the domain towards the shoreline along positive characteristics of the nswe mean flow özkan haller and kirby 1999 brocchini and bellotti 2002 6 d r d t g α 1 d d s d x along d x d t u g d u c with c g d the addition of this third equation to a simplified version of eqs 3 and 4 see brocchini and bellotti 2002 leads to the following set of equations providing suitable sbcs for nearshore circulation wave averaged solvers 7a d x l d t r 2 g h 4 c v α d h d t 7b d x l h 2 7c u x l r 2 g h where h is the wave height and c v is a known coefficient relating the volume of water in the sz i e that associated with the short waves the proposed sbcs are such that 1 the shoreline motion is forced by a the low frequency contribution carried by r along positive characteristics from the offshore region to the shoreline and b the short wave contribution which after the simplifications given by brocchini and bellotti 2002 is represented by the rate of change of the water volume in the swash zone proportional to d h d t through the coefficient c v and evolving at a wave averaged or group wave scale 2 the water depth at the mean shoreline x l is about half of the local wave height as demonstrated through simple geometrical arguments by bellotti et al 2003 3 the local cross shore velocity depends on the local shallow water velocity and on r when included in wave averaged models this set of equation are sbcs that work as a porous wall where momentum and mass exchange conditions are imposed and allow proper description of the wave wave interactions within the swash zone e g mase 1995 3 sbcs implementation in view of the fundamental importance of the positive riemann function on the calculation of eqs 7 roms has been partially modified in order to use roms and swan or inwave variables interpolated along the positive characteristics c 8 d x d t u c to solve equations 6 and 7 the 2 d roms kernel adopts a predictor corrector scheme to solve the depth averaged continuity and momentum equations however for the present initial implementation of the sbcs we decided to evolve our solution only at the corrector step among others u d and the vertically integrated wave averaged cross shore radiation stress s are roms variables required for the sbcs computation the former two are updated at each barotropic time level while the latter is updated only at baroclinic time level for this reason the sbcs solution is evolved only at the first barotropic time level corrector of each baroclinic time level 3 0 1 interpolation of the flow properties along the characteristic curves in the horizontal x y plane the state variables are arranged in a way equivalent to the well known arakawa c grid in particular the variable d is located at the center of the cell grid the so called r h o points while u and v variables are located at the center of the cell sides the so called u points and v points see figure 1 of haidvogel et al 2008 or fig 3 for the 1 d case with few exceptions a centered second order finite difference approximation is adopted in the horizontal haidvogel et al 2008 when x l is calculated from eq 7a it can fall within five possible different wet dry areas see fig 3 that depend on the location of the wet dry interface on the computational grid for all the cases a two points lagrangian interpolator has been used 1 x l is completely surrounded by wet points fig 3 case a both rho points h and d and u points u and s seaward and landward of x l are wet flow properties at x l are calculated by interpolating their values known at the grid nodes rho points and u points seaward and landward of x l surrounding x l 2 x l is surrounded by wet rho points while the u point seaward of x l is wet and the u point landward of x l is dry fig 3 case b in this case h and d at x l are retrieved as for the former case for u and s a different approach is needed since no information for these properties is available at the u point landward of x l here we first calculate the instantaneous shoreline position x s see brocchini and peregrine 1996 for the calculation of x s then u and s are calculated at x l interpolating their values from the u point seaward x l and x s where obviously all the flow properties and thus u and s are equal to zero 3 x l is surrounded by wet u points while the rho point seaward of x l is wet and the rho point landward of x l is dry fig 3 case c in this case u and s at x l are calculated by interpolating there the values of u and s from the u points surrounding x l for h and d this is not possible since no information on such properties is available at the rho point landward of x l further in this case we first calculate x s then h and d are estimated at x l by interpolating there the values of h and d from the rho point seaward of x l and x s 4 the rho point and the u point seaward of x l are wet and the rho point and the u point landward of x l are dry fig 3 case d all the flow properties and wave parameters are calculated at x l by interpolating their values from the rho point and the u point seaward of x l and x s 5 x l is completely surrounded by dry points fig 3 case e here the last wet point of the computational domain is first found and used as x l and then c is calculated once the flow properties at x l are known the next point along c can be calculated however there is still the chance that such a point falls within one of the above cases thus the same procedure is then repeated for every point along c hereafter called x c 3 1 characteristic curves and propagation of the positive riemann variable the last wet point of the computational domain is taken to coincide with x l fig 4 dotted red line for a few time steps in this work from 10 to 1000 depending on the specific baroclinic time step of the simulation along characteristic curves fig 4 black lines space and time are linked by characteristic speed see eq 8 at time step n starting from x l eq 8 is integrated backward in time either for an imposed time span or for a time large enough to reach the breaking point see fig 4 for an illustration of this we use the breaking point as the reference location for the backward integration because it is there that the most fundamental nearshore dynamics e g wave breaking turbulence generation wave energy dissipation start once c is known the r variable is propagated along c starting from the point most offshore of c up to the shoreline solving equation 6 the integration of eq 6 is achieved using a finite different numerical scheme as simple as 9 r i 1 r i g α δ t u c i 1 s 11 i 1 s 11 i d i u c i 1 3 2 shoreline evolution when r at the shoreline is known together with the wave height from the wave driver eq 7a can be solved hence at the lowest order of approximation the new position of the shoreline x l n 1 fig 4 red solid line can be obtained from 10 x l n 1 x l n d x l d t δ t 3 3 look ahead to a two way coupling the idea underlying the ongoing two way coupling is to have our sbcs time step with the existing roms wet dry scheme in brief with reference to a cross shore section of the computational domain from the off shore to the shore with grid cells indexed from 1 to i at a generic time step n the new sbcs are solved to provide the position of the mean shoreline at time step n 1 hence at time step n if x l lies in an area deemed to be wet then variables needed for the calculation of the sbcs flow properties will be interpolated at x l for a subsequent update through the sbcs alternatively in case of x l lying in an area determined to be dry the variable will be linearly extrapolated to x l see also lynett et al 2002 for a similar procedure to compute the variables needed by the sbcs to update the flow the solution will be stepped forward at time n 1 by i computing directly the new position of x l through the sbcs and ii updating the roms flow properties at wet nodes close to x l with a direct stencil directly bearing on x l at time n and indirect stencil based on the wet node closest to x l at time n influence of the sbcs 3 4 evaluation of the model performance two statistics namely the mean bias bias and root mean square error rmse normalized by the range of the reference data were used to evaluate the performance of the sbcs and are defined as 11 b i a s i 1 n m t i m r e f i n max m r e f min m r e f 12 r m s e i 1 n m t i m r e f i 2 n max m r e f min m r e f where n is the number of time record m t is the solution to be tested and m r e f is the reference solution 4 applications four test cases are used to assess the sbcs capability in representing the shoreline and the swash zone properties the hydrodynamic model roms and the wave model the wave driver swan for test case 1 and test case 2 or the inwave model for test case 0 and test case 3 are run into the coawst modeling system in two different configurations i alone romsswan sl or romsinwave sl hereinafter or ii in conjunction with the newly implemented sbcs solution romsswan sbcs or romsinwave sbcs to be tested solutions obtained using romsswan inwave sl with a cross shore resolution of 0 05m or 0 1m i e between about 120 and 217 nodes per wavelength are used as reference solution romsswan sl ref or romsinwave sl ref hereinafter the wave induced momentum has been taken into account by either the vortex force test case 0 and test case 3 or the radiation stress formalisms test case 1 and test case 2 test case 0 was performed to assess the quality of the reference solution romsinwave sl ref used in all other benchmarking tests and also provide evaluation of the semi analytical solution romsinwave sbcs with this purpose the experimental condition of the case study called mr 10 from padilla and alsina 2018 has been reproduced the shoreline position computed with romsswan inwave sl is based on the wet dry algorithm warner et al 2013 and is here represented by the wet point that is farthest from the initial state in the cross shore direction test cases 1 and 2 use phase averaged wave spectra approaching a mildly sloping 1 80 planar beach with normal incidence the wave forcing has been simulated by swan specifying 36 directional bins and 11 frequency bins between 0 04s 1 and 1 0s 1 test case 1 is forced by a constant offshore significant wave height h s 0 hereinafter while test case 2 makes use of a sinusoidally varying h s 0 for test case 3 the inwave model was used for the propagation of bichromatic waves and the imposition of the associated bound wave approaching a mildly sloping 1 80 planar beach with a normal incidence fig 5 illustrates snapshots of the mean water level η at the maximum runup for the three cases of interest after steady state condition has been reached the results are shown along a cross shore section placed in the middle of the domain results are shown with the exception of fig 5 and fig 7 in dimensionless form using either the offshore input wave period t 0 test cases 1 and 2 or the offshore mean primary period t m t 0 1 t 0 2 2 test case 3 for the time scale and the offshore maximum wavelength l 0 and the offshore depth d 0 for horizontal and vertical length scales respectively all the parameters for each test case are listed in table 1 while the imposed offshore input waves in terms of significant wave height h s 0 or water level are shown in fig 6 the romsswan inwave sbcs performance in reproducing the mean shoreline is quantitatively assessed using statistical indices like bias eq 11 and the rmse eq 12 which are given in table 2 for the tests performed 4 1 test case 0 validation of the reference solution we here provide an assessment of the value of our reference solution romsinwave sl ref and a first evaluation through laboratory data of the proposed romsinwave sbcs solution in this regard we reproduce numerically roms coupled with inwave the same experimental condition of one of the case studies mr 10 proposed by padilla and alsina 2018 a bichromatic wave with initial amplitudes of the primary components a 1 a 2 0 015 m and primary frequencies f 1 0 637 hz and f 2 0 563 hz are imposed at the offshore boundary of a gentle sloping beach 1 100 with a maximum depth of 0 5m fig 7a represents the original shoreline from padilla and alsina 2018 while in fig 7 b the input bichromatic wave in terms of water level is shown the resulting shoreline computed by romsinwave sl ref l 0 200 0 05m and the ones calculated with romsinwave sbcs at resolutions l 0 200 0 05m and l 0 7 1 5m are compared with the experimental shoreline averaged over one second in fig 7c and fig 7d respectively romsinwave sl ref represents well the experiment of padilla and alsina 2018 thus confirming that romsinwave sl ref is a good benchmark for the case studies that follow fig 7 c b i a s 1 72 and r m s e 7 16 further romsinwave sbss well reproduced the mean shoreline at the two resolutions used see table 2 4 2 test case 1 constant forcing for this test case a constant incoming wave with h s 0 of approximately d 0 6 fig 6a and a l 0 170 cross shore resolution have been used for both romsswan sl ref and romsswan sbcs steady state conditions are reached after 2 min of simulation i e after about 24 t 0 when the cross shore profiles of the significant wave height h s fig 8 top panel and η fig 8 bottom panel have become constant for the romsswan sl ref simulation the h s begins to decrease near x 3 l 0 due to wave breaking dissipation not surprising for a wave averaged model forcing the simulation with a constant h s 0 leads to a constant wave setup shoreward when the waves start breaking the wave height decreases rapidly fig 8 top panel and becomes close to 0 as they approach the shoreline this means that most of the wave energy is already lost before it can reach the shoreline with no energy left for the swash motion fig 9 represents the evolution of the shoreline x l respectively computed with romsswan sl ref and romsswan sbcs the great similarity between the two shorelines is evident and it is also confirmed by the statistic with the negative bias indicating a slight underestimation of the mean shoreline by romsswan sbcs see table 2 after an initial large run up and subsequent run down due to the cold start of the simulation the mean shoreline reaches a steady state as the consequence of the permanent wave setup the shoreline computed by romsswan sbcs is obtained through integration of eq 7a with the three terms of the right hand side shown in fig 10 the initial run up and run down are mainly driven by the first term that is r whereas the subsequent shoreline steady state seems to be the results of the balance between the first and the second terms in this test case the third term which corresponds to the time derivative of the swash zone water volume can be regarded as negligible thus the shoreline oscillation is completely associated with the mean flow and the local onshore velocity 4 3 test case 2 simple periodic forcing in this simulation the wave forcing consists of a wave train with a periodic h s 0 given by 13 h s 0 h m a x sin π t t r t where t t r is the wave train period and t is time for this test case h m a x d 0 6 and t t r 30 t 0 see fig 6 central panel this forcing allows for the generation of a signal evolving at the wave groups scale that leads to an oscillation of the mean shoreline using grids with different cross shore resolutions ranging from about 120 to 1 5 nodes per l 0 we tested various coarse cross shore resolutions to check the limit at which x l can be well reproduced with both romsswan sl and romsswan sbcs moving toward coarser resolutions we expect to lose the shoreline signal when romsswan sl is used rather while running romsswan sbcs we expect to still be able to reproduce x l the steady state in a statistical sense condition has been reached after 1 min of simulation i e after 15 t 0 this becomes evident observing the cross shore profile of the h s fig 11 top panel and η fig 11 bottom panel where after one minute of spin up the same pattern is repeated with period t t r the mean shorelines computed at several cross shore resolutions with both romsswan sl and romsswan sbcs are shown in fig 12 the two solutions obtained are consistent with the incoming wave forcing fig 6 central panel having its same period at really high cross shore resolution fig 12a both romsswan sl and romsswan sbcs reproduce x l in a similar fashion however using romsswan sl with an increasingly coarse cross shore resolution leads to a rapid loss of the shoreline signal the same is not true for the shoreline calculated with romsswan sbcs which is well resolved up to 8 0m about half l 0 fig 12f cross shore resolutions when x l starts to be distorted it is also to be noted that moving toward coarser resolutions the shoreline calculated by romsswan sl moves landward while that computed by romsswan sbcs oscillates over the same spatial range of the well resolved case this appears more evident when observing fig 13 where the mean shorelines computed with romsswan sbcs at the different cross shore resolutions are compared with that obtained with the well resolved romsswan sl ref l 0 120 0 1m this visual analysis is also quantitatively supported by the statistics with good values of rmse up to the 8 0m l 0 3 cross shore resolution where the rmse becomes higher than 20 the decreasing negative bias going toward coarser resolutions indicates an increasing underestimation of the shoreline computed by romsswan sbcs see table 2 the shorelines computed with the romsswan sl and romsswan sbcs have an excursion of about l 0 3 such long wave runup induced by a fairly low wave height maximum wave height about d 0 6 is explained by the very low friction quadratic bottom drag coefficient of 0 005 and very mild slope used the x l and the three terms of equation 7a are shown in fig 14 in the top panel and the bottom panel respectively the first and second terms of equation 7a are still the shoreline main forcing as demonstrated by the rundown starting approximately when the two terms are equal however in this case the third term is not negligible this is especially true during the runup where the third term may be about one third of the second term and a fifth of the first term r is known all along the characteristic curves c see fig 23 for an example of characteristic curves by integration of eq 6 once r is calculated u and d can be easily retrieved by 14a d r u 2 4 g 14b u r 2 g d this set of equations is used to calculate d and u from the sbcs up to the shoreline x l where instead they are calculated from eqs 7b and 7c solving this set of equations along the characteristic curves calculated during the computation gives a cloud of points in the space time domain where u and d are known which can be both represented by contour maps and if needed interpolated onto a regular grid contour maps of near shoreline depth d and cross shore velocity u are represented by fig 15 and fig 16 respectively for both figures the top panel is the solution calculated by romsswan sl ref then moving toward the bottom we find d and u calculated by romsswan sbcs with resolutions l 0 12 l 0 3 and l 0 2 the near shoreline d and u are well represented by the sbcs up to a resolution of l 0 3 with a moderate underestimation of the cross shore velocity during the backwash phase 4 4 test case 3 bichromatic waves here bichromatic waves are imposed at the offshore boundary for this test case the envelope of the wave action density and the associated bound wave are generated by inwave the wave action density is propagated by inwave and the bound wave is propagated by roms the bichromatic wave emerges from the superimposition of two cosinusoidal monochromatic waves one with amplitude 0 2m and frequency f 1 0 168 hz and the other with amplitude 0 1m and frequency f 2 0 2016 hz with a mean primary period t m 5 46 s the superposition leads to an emerging wave period of t g 29 7 s this specific simulation has been run to highlight the model capability to represent a modulated swash zone this is important in view of the proper evolution of low frequency modes that dominate swash zone induced by bichromatic waves baldock et al 1997 madsen et al 1997 brocchini and bellotti 2002 dong et al 2009 it is exactly the weakness of wave averaging models to reproduce low frequency waves emerging from the swash zone dynamics that makes this test particularly significant further in this case we used grids with different cross shore resolutions ranging from about 217 to 2 7 nodes per l 0 to check the capability of romsinwave sbcs to reproduce x l the quasi steady state condition i e periodicity in modulated swash zone properties has been reached in less than 2 min i e after 20 t 0 see fig 18 thus also in this case after a short spin up period the same pattern see figs 17 18 for the main flow properties is periodically repeated in agreement with the modulated forcing wave see fig 6 right panel the mean shorelines computed at several cross shore resolutions with both romsinwave sl and romsinwave sbcs are shown in fig 18 while fig 19 illustrates the mean shorelines computed with both romsinwave sl ref l 0 217 0 1m and romsinwave sbcs again the proposed sbcs were able to reproduce x l see figs 18 19 and table 2 it was possible to properly represent x l up to an 8 0m about l 0 2 7 cross shore resolution for which the x l computed with romsinwave sbcs is underestimated when compared with the one of romsinwave sl ref see fig 19c the rmse for this test case was less than 20 even when the coarsest grid was used again the bias showed a negative trend with the only exception for the grid at 1 0m cross shore resolution where the bias was positive see table 2 the maximum flooding due to the bichromatic wave used for this simulation is larger than 0 6 l 0 the mean shoreline x l and the three terms of the right hand side of eq 7a are shown in the top panel and bottom panel of fig 20 respectively here the first of such terms is still dominant but the second and the third ones have comparable importance this underlines the importance of the swash zone water volume in determining the mean shoreline position x l in presence of low frequency waves contour maps of near shoreline depth d and cross shore velocity u are illustrated in fig 21 and fig 22 respectively for both figures the top panel is the solution calculated by romsinwave sl ref then moving toward the bottom we find d and u calculated by romsinwave sbcs with resolutions l 0 22 l 0 5 and l 0 2 7 further for this test case the near shoreline maps of d and u are well reproduced by the sbcs up to a resolution of l 0 2 7 where the flow properties are not well represented anymore like for test case 2 a moderate underestimation of the cross shore velocity during the backwash phase occurs the above results are really encouraging in view of the implementation of the sbcs in a predictive manner 5 discussion a new shoreline boundary condition has been incorporated into roms to resolve the cross shore position of the mean shoreline x l and the flow property there when using grids with cross shore resolution of 0 1m from 217 to 120 nodes per wavelength with a really small d c r i t it is possible to solve the equations of motion and the continuity equation in very shallow water i e up to the shoreline in this condition the model does not need any external boundary conditions to represent the swash zone since the fluid properties are calculated locally further the small d c r i t used is very close to 0 where the artificial shoreline conditions of rigid wall describe well the shoreline motion such simulations are extremely time consuming and computationally demanding and are thus often impractical however running roms in these conditions romsswan inwave sl ref allowed us to produce a benchmark solution to assess the value of the proposed sbcs solutions obtained with romsinwave sl ref were also positively assessed against the experimental data reproducing numerically one of the experiments of padilla and alsina 2018 the achieved results confirmed the solutions from romsswan inwave sl ref to be suitable benchmark to test the romsswan inwave sbcs model against in the proposed case studies we used three different wave forcings to assess the performances of the proposed sbcs forcing roms with waves of constant height test case 1 leads to a constant mean shoreline after an initial wave setup fig 9 here the wave setup is mainly due to the first term of the right hand side of eq 7a while the constant shoreline is the consequence of the mutual cancellation of the first and second terms see fig 10 the third term which represents the time derivative of the swash zone volume was close to zero for the wall simulation this together with the constant mean shoreline is in agreement with the symmetric swash due to a monochromatic wave sinusoidally varying h s 0 used in test case 2 allowed for the modulation of the mean shoreline here the time derivative of the swash zone volume is not zero see fig 14 which is in line with an asymmetric swash when the simulation was forced with a bichromatic wave as in test case 3 the flooded region was much larger than those of test case 1 and test case 2 because of the increased wave setup forced by the low frequency wave here the swash zone volume time derivative is even more crucial for the calculation of the mean shoreline x l thus pointing out the importance of a proper treatment of low frequency waves within the surf zone this is probably a consequence of the greater change in swash zone water volume due to the greater wave setup the characteristic curves calculated integrating equation 8 are shown in fig 23 it is interesting to note that during the run up the characteristic curves are steeper than during the run down this is especially true for test case 3 likely as a consequence of a greater water volume into the swash zone with the proposed sbcs we could reproduce both the shoreline and the flow properties in the swash zone close to those obtained by romsswan inwave sl ref achieved with 0 1m from l 0 217 to l 0 120 cross shore resolution but using a much coarser grid of 4 0m l 0 3 for the test case 2 and l 0 5 for the test case 3 at the coarsest resolution of about l 0 2 even the proposed sbcs cannot properly represent x l the spatial and time scales of the phenomena evolving in really shallow waters are small and need to be calculated at high time and spatial resolutions thus terms like the radiation stress which is a fundamental forcing of the sbcs may not be properly computed with too coarse resolutions and this also causes problems in the assessment of the shoreline motion in all our tests the sbcs described the shoreline motion well this makes romsswan inwave sbc a powerful tool to use for flooding simulations when the domain of interest is too large to use wave resolving models the time needed for the simulation run with romsswan inwave sl ref at l 0 120 or l 0 217 cross shore resolution is in the order of some hours while that carried out with romsswan inwave sbcs at a l 0 3 cross shore resolution only required a few minutes hence the great advantage in terms of computational costs of using the proposed sbcs is very evident see table 3 the simplified set of eqs 7 that describes the sbcs has been validated through a dedicated laboratory campaign characterized by complex swash zone dynamics also involving wave wave catch up i e bore merging within the swash zone see brocchini and bellotti 2002 thus the flow property at the predicted x l can account also for the process of bore merging in the swash zone this means that once fully coupled in wave averaged models the sbcs will provide forcing from a swash zone which accounts for all wave wave interaction dynamics once the c v parameter of eq 7 is properly set therefore the sbcs will allow the eventual generation of ig waves and for the proper modification and reflection out to sea of the incoming waves at the predicted mean shoreline 6 conclusions this work is the first step toward the inclusion of swash zone dynamics into nearshore wave averaged models here is a brief and schematic summary of what we believe are the main conclusions that can be drawn from our analyses 1 the sbcs deriving from an integral theoretical model of swash zone flows have been successfully calculated within the kernel of a wave averaging model for the first time using flow properties coming from the offshore region of the computational domain 2 most of the swash zone dynamics is due to the mean flow i e incoming riemann variable and to the local at x l wave height however also the swash zone water volume content also seems to play an important role for a proper calculation of the mean shoreline x l especially in the case of bichromatic waves 3 work is underway to achieve a two way coupling between the sbcs and roms however romsswan inwave sbcs is already a powerful tool for the study of coastal inundation at the wave averaged level 4 wave climate can be influenced by tidal dynamics with larger waves occurring during high water lewis et al 2019 in this perspective possible applications of the proposed methodology are within future coastal flood risk assessment methods to resolve future flood risk events due to waves and their interactions with ocean processes such as tides under different scenarios 5 finally the methodology here proposed can also be implemented into other coupled wave circulation modeling systems commonly used by the coastal community such as adcirc swan dietrich et al 2011 schism wwm schloen et al 2017 and fvcom swave mao and xia 2018 credit authorship contribution statement francesco memmola conceptualization methodology software formal analysis writing original draft writing review editing alessandro coluccelli software resources aniello russo resources writing review editing john c warner supervision writing review editing maurizio brocchini conceptualization methodology supervision writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments financial support from the onr global uk through the nicop morse research grant n62909 17 1 2148 and from the italian place project code ars01 00891 a national research programme funded by the italian ministry of university and research through pon r i 2014 2020 e fsc funds are gratefully acknowledged we wish to thank padilla and alsina 2018 for making their data available online 
