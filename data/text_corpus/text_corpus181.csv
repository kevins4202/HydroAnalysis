index,text
905,the long range dependence lrd is considered an inherent property of geophysical processes whose presence increases uncertainty here we examine the spatial behaviour of lrd in precipitation by regressing the hurst parameter estimate of mean annual precipitation instrumental data which span from 1916 2015 and cover a big area of the earth s surface on location characteristics of the instrumental data stations furthermore we apply the mann kendall test under the lrd assumption mkt lrd to reassess the significance of observed trends to summarize the results the lrd is spatially clustered it seems to depend mostly on the location of the stations while the predictive value of the regression model is good thus when investigating for lrd properties we recommend that the local characteristics should be considered the application of the mkt lrd suggests that no significant monotonic trend appears in global precipitation excluding the climate type d snow regions in which positive significant trends appear keywords hurst long range dependence mann kendall test precipitation random forests trend analysis 1 introduction the long range dependence lrd also known in hydrological science as the hurst phenomenon is a behaviour observed in geophysical processes in which wet years or dry years are clustered to respective long time periods koutsoyiannis 2002 a common practice for evaluating the presence of the lrd is to model the geophysical time series with the hurst kolmogorov process hkp and estimate its hurst parameter h koutsoyiannis 2003 tyralis and koutsoyiannis 2011 where high values of h indicate strong lrd the estimation of h is of great importance in engineering practice lins and cohn 2011 as indicated by koutsoyiannis 2006 koutsoyiannis and montanari 2007 and tyralis and koutsoyiannis 2014 the uncertainty increases substantially when lrd is present furthermore due to the increase in uncertainty observed trends in data even if they seem significant using classical statistical testing can be insignificant under the lrd assumption as shown by hamed 2008 most studies on the assessment of the magnitude of precipitation lrd using instrumental data are local e g liu et al 2012 munshi 2015 valle et al 2013 however some studies including fatichi et al 2012 and iliopoulou et al 2017 estimated the magnitude of the precipitation lrd from instrumental measurements in global spatial scale and argued for its weak existence although the evidence for its presence in annual precipitation records is inconclusive o connell et al 2015 similar global studies based on dissimilar datasets include kumar et al 2013 who estimated the h parameter of coupled model intercomparison project cmip5 twentieth century precipitation simulations sun et al 2014 who used reanalysis datasets and bunde et al 2013 who used instrumental measurements climate model simulations and precipitation reconstructions to infer the significance of lrd in precipitation the mann kendall test is frequently used in hydrology to evaluate the significance of trends however the mann kendall test under the lrd assumption mkt lrd hamed 2008 in which a possible presence of lrd is considered has been less frequently adopted a few local case studies in which the authors applied the mann kendall test considering the presence of lrd include the investigation of precipitation dinpashoh et al 2014 stream flows ehsanzadeh and adamowski 2010 khaliq et al 2009 kumar et al 2009 sagarika et al 2014 zamani et al 2017 and both fathian et al 2016 the analysis of point precipitation at a global setting is an important topic in hydrology for instance see de lima et al 2012 it can be supported by the analysis of precipitation instrumental data from stations that spatially cover the globe which has become a common subject in the recent literature and is supported by the increasing availability and accessibility of global data sets bierkens 2015 while it is an important constituent of global scale hydrology whose emergence was highlighted by eagleson 1986 1994 such studies include the analysis of extremes alexander et al 2006 asadieh and krakauer 2015 koutsoyiannis 2004 papalexiou and koutsoyiannis 2013 droughts nasrollahi et al 2015 analysis of trends van wijngaarden and syed 2015 the temporal concentration of precipitation monjo and martin vide 2016 and reconstruction of past precipitation smith et al 2012 although the instrumental data need some processing to be used they could be considered more reliable compared to climate simulations or reconstructions however the coverage of the earth s surface by rain gauges is not high while it decreases considerably when the analysis demands a sufficient long time period to obtain more reliable results new et al 2001 in such cases several alternative methods have been proposed including the use of satellite data kidd and huffman 2011 the spatial analysis of precipitation based on instrumental measurements can be applied in local case studies because the areas of interest are uniformly covered by the stations this is the case e g in blanchet et al 2009 who study the extreme statistics of snowfall villarini and smith 2010 who investigate flood peak distributions li et al 2011 who study precipitation trends and dyrrdal et al 2016 who analyse the extreme precipitation in this study we estimate the h parameter of the mean annual precipitation from instrumental data from a large part of the earth the database used in this study menne et al 2012a 2012b includes stations that cover the largest part of the inhabited earth surface however for statistical reasons we examine stations with data which span the hundred year period 1916 2015 and thus the coverage decreases considerably however we prefer to use this reduced dataset instead of reanalysis datasets because the artificial nature of the latter can alter considerably the results particularly when using reanalysis data from uncovered areas at early time periods the primary aim of our study is to investigate the relationship between h and locations features which has been suggested for further research in iliopoulou et al 2017 while fatichi et al 2012 did not identify the presence of a particular geographical pattern the results of sun et al 2014 and markonis and koutsoyiannis 2016 figure s3 indicate that h varies considerably with the location of the stations however they were obtained by reconstructions of past precipitation classical spatial statistical analysis cannot be applied because the coverage of the earth s surface by the examined stations is low and strongly non uniform in such cases machine learning methods are a useful alternative e g alobaidi et al 2015 leuenberger and kanevski 2015 as well as a complementary option kanevski and demyanov 2015 therefore to overcome the problem of non uniform coverage an alternative approach is to regress the h parameter estimates on spatial characteristics of the stations i e their coordinates however location characteristics of the stations such as their elevation and their köppen geiger climate class kottek et al 2006 may also be related to h the ability to include predictor variables beyond the coordinates is another advantage of machine learning methods compared to classical statistical spatial analysis methods to find all possible relationships we apply both linear regression models and random forests algorithms breiman 2001 the latter are classified as machine learning methods and they are particularly useful to model non linear relationships between the dependent and the predictor variables even when the latter are correlated furthermore they have been applied in spatial analyses with better results compared to other machine learning methods cracknell and reading 2014 a secondary aim of our study is to assess the significance of precipitation trends by applying the mkt lrd test along with an exploratory analysis in which we can present the relationship between the magnitude and significance of trends and the location characteristics morin 2011 also applied the mkt to the mean annual precipitation but using a gridded dataset and pre whitening to account for the presence of serial correlation additionally van wijngaarden and syed 2015 already examined the precipitation trends using nearly 1000 stations for the time period 1700 2013 they assessed the significance of the trends using the statistical t test at the 5 level and they concluded that some caution is warranted about claiming that large changes to global precipitation have occurred during the last 150 years zhang et al 2007 also examined the trends in global precipitation using instrumental measurements and compared them to projections of climate models with the aim to find how anthropogenic forcing influences the precipitation in most studies global precipitation and possible changes in the hydrological cycle are examined by extended use of general circulation models for instance allan et al 2014 allen and ingram 2002 gu and adler 2015 or gridded datasets for instance hartmann et al 2013 gu and adler 2015 morin 2011 climate models outputs are not used here because they contain large errors in precipitation simulations trenberth 2011 gridded datasets are not used here because they are constructed using observations of temporally varying spatial density which may lead to inaccurate estimates of parameters used in climate modelling beguería et al 2016 sun et al 2014 while they may also be based on climate models reanalysis datasets in particular the present study examines large areas thus temporal variations of the spatial density of the observations would be even higher instead here we use a set of stations with constant spatial density during the study period furthermore due to the long time period only instrumental measurements can provide sufficient data for the investigation of possible trends new et al 2001 there is also a discussion on the relationship between the precipitation and temperature changes with the aim to find constraints of the maximum precipitation change conditionally as a function of temperature change with contradictory results allan et al 2014 estimate an increase in precipitation approximately 2 3 k lambert et al 2008 estimated an increase approximately equal to 6 k while wentz et al 2007 estimated an increase equal to 7 k the assessment of the significance of trends under the assumption of lrd has been proposed for research by morin 2011 the significance of trends has been also examined using instrumental measurements in van wijngaarden and syed 2015 and using gridded datasets in hartmann et al 2013 however they did not use the lrd assumption there have been attempts to link the precipitation changes with the temperature however the statistical significance is a concept which cannot be incorporated in such framework a trend may be small but can be statistically significant and vice versa furthermore koutsoyiannis and montanari 2014 noticed that geophysical processes can be modelled using non stationary models if there is a well defined deterministic relationship constructed by deduction therefore changes in precipitation related to temperature could be useful in such setting i e if a deterministic function for the changes of the temperature could be also constructed by deduction in our opinion this is not possible see also koutsoyiannis 2010 on the other hand the assessment of the significance of trends can be incorporated in a stationary and completely stochastic framework because it can test whether the particular stochastic framework is suitable to model the geophysical process of interest 2 data we used daily precipitation data from the global historical climatology network ghcn menne et al 2012a b time periods of precipitation records for each station differ the length of the time series affects the bias and uncertainty related to the parameters estimation when the maximum likelihood estimator mle is used tyralis and koutsoyiannis 2011 see also section 3 1 therefore we preferred to use the common time period 1916 2015 while we discarded data out of this period even when the instrumental data were covering a longer time period the code and the datasets used in the present study are available online see appendix a 2 1 station and data selection the initial dataset included time series with missing or flagged i e data of low quality for reasons explained in menne et al 2012a values we processed the dataset according to the following briefly described sequence of actions a flagged values were considered as missing values b we used the values 0 34 and 0 83 to differentiate between the months months with a percentage of observations higher than 0 83 i e with more than 25 30 or 26 31 daily observations are considered good while months with a percentage of recorded values less than 0 34 i e equal or less than 10 30 and 10 31 daily observations are considered of poor quality the reason for the differentiation is that we first aggregate to the monthly time scale and then to the annual time scale thus even if all values in a month are missing we can fill the monthly value after the first aggregation as described in step c b1 missing values within months with observed values more than 83 were filled using linear interpolation b2 all values within months with observed values less than 34 were considered as missing b3 for the rest of the months the missing values were filled using linear interpolation and then these months were considered as missing the reason is explained in step d c missing months corresponding to steps b2 and b3 the latter after the substitution with missing values were filled using a seasonal kalman filter implemented in the r package zoo zeileis and grothendieck 2005 d mean monthly values for months in which both steps b3 and c i e months with missing values more than 34 and less than 83 were applied were calculated with the mean of monthly values of steps b3 and c e from the mean monthly values we obtained the mean annual values f finally we discarded annual time series if one of the following constraints was satisfied f1 two or more missing years f2 h 0 95 mean annual rainfall μ 3000 mm standard deviation of annual rainfall σ 750 mm coefficient of variation of annual rainfall c v 0 8 we set these constraints on the estimated parameters because a preliminary analysis showed that higher values were outliers f3 four or more years with less than 60 of observed daily values the estimated parameters of the annual time series of step f2 are described in section 3 1 the interested reader is referred to part 3 of the supplementary information for more details regarding the use of selection algorithms constraints for data inclusion and other details in part 4 of the supplementary information we present boxplots for every parameter of step f2 despite the removal of outliers based on the preliminary analysis in step f2 some outliers remained this will not constitute a problem in our analysis due to the robustness of the random forests see section 3 2 to outliers breiman 2001 one may argue that high values observed in step f2 may also be representative of the parameters population therefore they should not be removed e g see the textbooks of aggarwal 2017 barnett and lewis 1978 hawkins 1980 since the aim of our study is to investigate the relationship between h and location features and due to the low number of outliers found in step f2 their removal will not affect the analysis we present the locations of the subset of stations which remained after the initial procedure in fig 1 1535 stations remained most of which are located in australia europe and north america data for each station include its geographic coordinates i e elevation longitude and latitude we calculated the cartesian coordinates of stations under the assumption of a spherical earth using eqs 1 3 to model the proximity of stations which appear to be algebraically distant when considering their longitudes 1 x r cos lat cos lon 2 y r cos lat sin lon 3 z r sin lat r denotes the radius of the earth the x and y axes of the cartesian coordinate system define a plane which includes all points with zero latitude while the z axis is perpendicular to the plane e g for given lat 0 stations with longitudes 180 and 180 are coincident the coincidence can be reproduced by the transformations 1 3 2 1 1 grouping of stations to köppen geiger climate classes the stations are grouped based on the climate classification of köppen geiger kottek et al 2006 table 1 presents the classes whose combination gives the climatic types we grouped the stations according to the climate type of the nearest point of the grid provided by kottek et al 2006 we calculated distances between stations and grid points using the haversine half versed sine formula as implemented in the r package geosphere hijmans 2016a table 2 presents the 20 climate types of the stations twelve more climate types in kottek et al 2006 were not represented by the spatial distribution of the stations the model calibration presented in section 3 2 cannot be applied to the initial classification because some climate types include a low number of stations we regrouped the stations in the three groupings presented in table 2 grouping 1 included types with low number of stations together considering their main climate and precipitation type grouping 2 classified stations according to their main climate grouping 3 is similar to that of ragulina and reitan 2017 who regrouped the stations according to precipitation conditions 3 methods here we present a minimum theoretical background of the methods because they are established in the scientific literature 3 1 hurst kolmogorov process we modelled the annual time series of section 2 1 with the hkp let x t t 1 2 be a hkp the hkp is a three parameter normal stationary stochastic process in discrete time its parameters μ σ h are defined by eqs 4 6 tyralis and koutsoyiannis 2011 4 μ e x t 5 σ var x t 1 2 6 ρ k corr x t x t k k 1 2 h 2 k 1 2 h 2 k 2 h k 0 1 h 0 1 the parameter μ is the mean of the stochastic process and the parameter σ is its standard deviation the parameter η represents the magnitude of lrd i e the tendency of wet or dry years to be clustered in long time periods persistence while the autocorrelation function ρk increases with h high values of h denote strong long term persistence while when h 0 5 the resulting stochastic process is antipersistent but still stationary h 0 5 is equivalent to a stochastic process of independent variables the implementation of the maximum likelihood estimator in the r package hkprocess tyralis 2016 was applied for estimating μ σ and h furthermore we computed the maximum likelihood estimate of the coefficient of variation defined as 7 c v σ μ the maximum likelihood estimate of c v can be obtained from eq 7 after substitution of μ and σ with their maximum likelihood estimates due to the invariance properties of the mle from hereinafter μ σ and h will denote the estimates of the respective parameters we will not use hats as in step f 2 in section 2 1 the maximum likelihood estimator of the hkp parameters has excellent properties when compared to other estimators as shown in the simulation experiments in tyralis and koutsoyiannis 2011 while similar simulation experiments can be found in taqqu et al 1995 jeong et al 2007 and rea et al 2013 the latter three studies mostly implement estimators presented in tyralis et al 2011 3 2 model fitting and testing we regressed h on combinations of other available variables related to local characteristics of the stations i e their geographic coordinates cartesian coordinates elevation climate type μ and σ the use of geographic coordinates is more intuitive compared to cartesian coordinates thus we preferred to visualize the results using the former coordinate system the regression was applied using linear regression the random forests algorithm biau and scornet 2016 breiman 2001 as implemented in the r package randomforest liaw and wiener 2002 and the random forests based on conditional inference trees cforest algorithm strobl et al 2007 2008 as implemented in the r package party hothorn et al 2017 properties of linear models are well known however random forests are less used in hydrological sciences random forests can handle non linear interactions and highly correlated variables and have high predictive power furthermore random forest variable importance measures for variable selection purposes are available strobl et al 2008 therefore despite being black boxes they can still provide information about the relationship between the dependent and the predictor variables in this study we used the permutation importance which measures the mean increase of the prediction mean squared error on the out of bag portion of the data after permuting each predictor variable in the trees of the trained model more details can be found in the documentation of the importance function of the r package randomforest liaw and wiener 2002 yet the random forest importance variable measures are not reliable when the predictor variables vary in their scale of measurement or their number of categories strobl et al 2007 in such cases strobl et al 2007 propose the use of the cforest algorithm and its respective permutation importance measure which we also used in our study the three algorithms are applied using the r package caret kuhn 2008 kuhn et al 2017 we trained the three models on 80 of the sample and we tested their performance on the remaining 20 using the root mean squared error rmse mean absolute error mae mean absolute percentage error mape and pearson s r metrics h was the dependent variable while we used a combination of spatial and location variables as predictors the first testing of various combinations of predictor variables can be used initially to choose the most suitable for predicting h and reassess the selected combinations applying a computationally demanding 5 fold cross validation in the 5 fold cross validation the original sample is randomly divided into five equal sized subsamples the model is fitted in four subsamples and tested in the remaining one while the procedure is repeated five times consequently the randomness of the partitioning of the dataset in the 5 fold cross validation influences the results less compared to the simple cross validation the 5 fold cross validation was applied to four datasets sets i e the full dataset and three subsets including stations in australia europe and the usa respectively in the 5 fold cross validation we compared the performance of random forests for predicting h with the simulations of a truncated normal distribution fitted to the sample of hs and with a naïve approach in which the predicted value is equal to the median of h of the fitting set the maximum likelihood estimates of the parameters of the truncated normal distribution in each one of the 80 folds were used for the simulation of the other 20 the maximum likelihood estimates were obtained using the r package tmvtnorm wilhelm and manjunath 2015 the rmse pearson s r and the slope of the regression line between the predicted and testing values metrics were used for the comparison for more details on the application of the algorithms and the use of tuning parameters on the case of random forests and cforest through the r package caret the interested reader is referred to parts 4 8 of the supplementary information for more details regarding the use of metrics to assess the predictive performance of regression algorithms the reader is referred to alexander et al 2015 and gramatica and sangion 2016 3 3 mann kendall test under the long range dependence assumption the mkt lrd consists of three consecutive hypothesis tests namely o original mk test h hurst parameter test and m hamed 2008 let h 0 i denote the null hypothesis of each test and let h 1 i denote the alternative hypothesis where i o h m denotes the step of the mkt lrd the null hypotheses are as follows h 0 o no trend under the independence assumption h 0 h no significant lrd h 0 m no trend under lrd assumption the possible outcomes of the test are summarized by the following sequences h 0 o no significant trend h 1 o h 0 h significant trend exists h 1 o h 1 h h 0 m no significant trend h 1 o h 1 h h 1 m significant trend exists we used the test implementation in the r package hkprocess tyralis 2016 with a predefined significance level α 0 05 for all steps for more details on the algorithm and its implementation using the r package hkprocess the interested reader is referred to tegos et al 2017 furthermore we estimated the trends of the annual time series with the fitting of a linear model the estimated trends were set equal to the slope of the least squares line 3 4 global moran s i test the global moran s i test uses the moran s i statistic moran 1950 the moran s i statistic is a measure of spatial autocorrelation based on the location of variables and their observed values in the global moran s i test the null hypothesis is that the observed spatial pattern is a realization of a random spatial process the alternative hypotheses are that the spatial distribution of high values and or low values are spatially clustered or they are spatially dispersed the former holds for low p values and positive z score while the latter for low p values and negative z score more details on the global moran s test can be found in bivand et al 2013b pp 275 284 350 351 here we applied the global moran s i test by implementing the spdep r package bivand and piras 2015 bivand et al 2013a 3 5 kriging besides random forests we furthermore used ordinary kriging to predict h spatially for comparison and benchmarking reasons kriging is a stochastic method of spatial interpolation which uses normal processes to model the observed values for more details the reader may refer to bivand et al 2013b pp 232 233 kriging is less computationally intensive compared to the random forests we used the gstat r package to perform kriging pebesma 2004 gräler et al 2016 in addition we used exponential functions to model the autocovariance of the spatial model details on the autocovariance functions can be found in the documentation of the r package geor ribeiro jr and diggle 2016 4 methodology summary here we describe an outline of the method and the procedure of our analysis firstly we selected stations with precipitation data in the time period 1916 2015 we filled the missing data we computed the mean annual precipitation values and discarded some stations which did not satisfy the criteria set in section 2 1 then we grouped the stations in climate types see section 2 1 1 the record for each station includes its location in geographic and cartesian coordinates its elevation its climate type three groupings and mean annual precipitation time series we modelled the time series with hkp and we estimated the parameters μ σ h section 3 1 we regressed h on combinations of location parameters using linear regression random forests and the cforest algorithm the fitting of the algorithms was performed in the 80 of the 1535 stations while their performance was tested in the other 20 we compared the predictions of h between the random forests the simulation from a fitted truncated normal distribution and the naïve method in a 5 fold cross validation using the rmse the pearson s r and the slope of the regression line between the predicted and testing values we applied the 5 fold cross validation to the entire dataset as well as three subsets each one corresponding to a continent furthermore we computed variable importance measures with the application of random forests and the cforest to the full dataset section 3 2 the combination of the validations and the use of variable importance measures can provide reliable information despite the shortcomings of each method when used individually we applied the global moran s i test to the hs to find possible spatial patterns with the aim to assess the results of the regression model we furthermore compared the predictive performance of the random forests with ordinary kriging in which we used an exponential covariance function to model the spatial dependence of hs the comparison was performed using data from the contiguous part of the usa to find a lower bound of predictive performance we modelled the spatial pattern of the hs in the usa using a gaussian random field with an exponential covariance function while we attempted to preserve the spatial dependence of the hs to this end we simulated the spatial pattern 1000 times and we tested the predictive performance of ordinary kriging in predicting h in the 20 of the sample when fitted in the 80 of the sample finally we estimated the trend and its significance under the lrd assumption section 3 3 and we visualized the results coupled with location variables 5 long range dependence analysis in this section we present the results of the analysis for the h parameter 5 1 overview of h fig 2 is the histogram of hs the maximum likelihood estimated values are skewed to the right with skewness equal to 0 21 while the median value is equal to 0 56 for comparison reasons and as shown in a simulation study in part 9 of the supplementary information the median of the h estimates of 100 000 simulated time series of length equal to 100 and h 0 59 is equal to 0 56 furthermore 95 of the simulated experiment hs are in the interval 0 413 0 691 while 95 of the precipitation dataset hs are in the interval 0 402 0 733 a truncated normal distribution with support 0 1 seems to be a reasonable model for h figure 3 presents the correlations between some variables of interest the longitude is omitted while the inclusion of x and y coordinates as single variables would be meaningless we observe a high correlation between μ and σ and between the absolute latitude and c v h is not highly correlated with any of the variables in fig 3 to investigate the spatial properties of h we computed the spatial correlogram presented in fig 4 see part 9 the autocorrelation for distances equal to 250 km is approximately 0 1 while it increases to approximately 0 2 for distances equal to 100 km the low autocorrelations for distances higher than 250 km may decrease the accuracy of predictions of h based on spatial characteristics furthermore we calculated the global moran s i statistic moran 1950 using the global dataset as well as the respective datasets in the usa and australia part 9 of the supplementary information applying the hypothesis tests based on the global moran s i statistic we found p values almost equal to 0 and positive z scores implying that the spatial distribution of high or low values is spatially clustered which rejects the hypothesis that the underlying spatial process is random 5 2 visualization of h coupled with the predictor variables in this section we visualize h coupled with the predictor variables we present a full exploratory data analysis in the supplementary information while here we present some important figures for brevity fig 5 presents how h varies with the climate class of the station grouping 2 of table 2 is used as the predictor variable it seems that h does not significantly vary with grouping 2 while its values are near to the median value 0 56 computed in section 5 1 on the other hand grouping 1 see table 2 in fig 6 seems to be a better predictor because of the higher variation of h between different climate classes in fig 7 we observe the variation of h with the latitude higher h values are observed for positive latitude however no trend prevails while we do not observe any linear relationship between the two variables fig 7 also presents the relationship between h and longitude again we do not observe any clear linear relationship between the two variables furthermore h is not linearly related to the elevation of each station 5 3 model fitting and testing from the analysis in section 5 2 it is apparent that a linear regression model between h and the location variables could be a benchmark and be compared with the more complex random forests and the cforest algorithm we examined combinations of predictor variables as shown in table 3 combinations 1 11 and 17 26 include the dependence of h on the location of the stations combination 12 examines its dependence on variables which are features of the precipitation of the station while combinations 13 16 and 27 30 examine both location and precipitation features we built the models of table 3 using a stepwise regression method and in particular a forward selection approach i e we started with no variables and we tested the addition of each variable using criteria such as the rmse the mae the mape and pearson s r we fitted the models on 80 of the data and we tested their performance in predicting h on the other 20 in table 4 we present the testing results of each model combinations 1 and 3 16 for the cforest algorithm were omitted due to high computational load combined with the fact that they would not behave considerably different compared to the respective application of random forests random forests and the cforest had good performance while the performance of linear models was poor indicating a strong non linear relationship between the predictor variables and h the term linear here refers to the relationship e g between h and the elevation or the latitude and not to spatial relationships the cforest is more computationally intensive compared to the random forests firstly we examined the dependence of h on the elevation and the climate combinations 1 4 grouping 1 combination 2 was the best predictor with a similar performance for all methods then we examined the dependence of h on the cartesian coordinates combined with or without other variables combinations 5 11 13 16 the combination 5 i e x and y coordinates performed very good in random forests while the inclusion of the z coordinate the elevation and the climate type further improved the performance combination 11 which includes grouping 3 performed marginally better than combinations 9 and 10 which include groupings 1 and 2 respectively inclusion of μ and σ further improved the performance of the random forests combinations 13 16 secondly we performed a similar investigation using the geographic coordinates instead of the cartesian coordinates combinations 17 30 the longitude and latitude combinations 17 18 are not good predictors when we combine each one of them with grouping 1 combinations 19 20 the results are worse or similar with using grouping 1 as a single predictor the combination 21 i e longitude and latitude performed well while the inclusion of grouping 1 combination 22 weakened the regression model on the other hand the inclusion of the elevation combination 23 improved marginally the performance of the model climate type combinations 24 26 worsened the performance while inclusion of μ and σ combinations 27 30 further improved the performance of the random forests it is noteworthy that some results seem incoherent e g in the case of cartesian coordinates climate improves the random forests results combinations 8 11 while for the geographic coordinates combinations 23 26 it is the opposite this may be explained by the slight deviations induced by the inclusion of climate in this case the 5 fold cross validation presented in the following is a valid method to obtain a more reliable inference in fig 8 we present the predicted h from the application of the trained random forests for the combination 24 to the test set pearson s r indicates a good prediction while the range of predicted hs is smaller than the range of hs in the test set we observe the same behaviour for the cforest algorithm in fig 8 albeit pearson s r is somewhat lower when pearson s r is used to assess the predictive performance of regression models in the test set it is not a measure of correlation while r 2 does not explain variability in table 5 we present the results of a 5 fold cross validation for the prediction of h we compare the random forests in the combinations 2 9 17 22 24 27 28 of predictor variables with the truncated normal distribution and the naïve approach the combinations 2 9 17 22 24 can be used for the prediction of h at ungauged locations the combinations 27 and 28 which include μ and σ could be useful to predict h at an ungauged location if we could make assumptions about the μ and σ parameters based on experts experience this is also possible for cases with few years of observed data since the uncertainty in estimating μ and to a lesser extend σ is less sensitive to the sample size besides μ and σ are also representative of the climate at the given location thus they are equally useful at the stage of the analysis regarding the overall view the rmse of the random forests is lower than that of the truncated normal distribution and the naïve approach in most cases however it is notable albeit expected that the pearson s r and the slope are approximately 0 for the truncated normal distribution this highlights the importance of the higher predicting performance of the random forests in terms of pearson s r and the slope while in all cases the performance of the random forests is not perfect i e pearson s r is in the neighbourhood of 0 5 and the slope in the neighbourhood of 0 3 the improvement over the benchmark approaches is somewhat significant indeed the rmse is 10 15 lower furthermore we note that the variation of rmse pearson s r and the slope values is low for all 11 random forests cases meaning that the algorithm is stable with respect to the choice of the fitting sample there is a rather weak relationship between h and grouping 1 combination 2 while there is a rather moderate relationship between h and the longitude and latitude predictors combination 21 the inclusion of grouping 1 to the longitude and latitude predictors combination 22 did not improve the model compared to combination 21 however the inclusion of grouping 1 and the elevation combination 24 as predictor variables improved marginally the predictive performance of the fitted model a possible explanation is that all information about h is included in the geographic location of the stations knowing the climate class of the stations does not add any information to that obtained by their locations however the inclusion of μ and σ combination 28 which may represent the climate in the location better than the climate class further improved noticeably the performance of the prediction on the other hand the better performance of the combination 28 compared to combination 27 is possibly owed to that h and σ are not orthogonal tyralis and koutsoyiannis 2011 and therefore the properties of the estimates of the former depend on the latter we repeated the 5 fold cross validation for stations in the usa australia and europe parts 6 8 of the supplementary information and we present the results for the usa and australia for reasons of brevity in table 6 while the overall comparison pattern remains the same with table 5 there is a remarkable difference between the rmses of the random forests they are lower in the usa and higher in australia while the rmse of the truncated normal distribution is equal in the entire dataset and australia the inclusion of the predictor variables has a lesser effect in the case of australia for instance the rmse of combination 28 is lower by 0 11 0 087 0 076 in australia while 0 14 0 086 0 072 for the entire dataset we observe a similar behaviour in the usa 0 12 0 078 0 066 albeit the differences are less significant we attribute this behaviour to the combined information offered by entire dataset which can be effectively exploited by the random forests in fig 9 we present the variable importance for the combination 24 of predictor variables because it includes all predictor variables excluding μ and σ the location parameters combined are the most important for predicting h followed by the elevation and the climate classification on the other hand the cforest algorithm differs in that it estimates higher importance of the climate classification as presented in fig 9 bottom this is possibly owed to the better performance of the cforest algorithm when estimating categorical variables importance to find a lower bound for the errors we simulated the observed spatial pattern in the usa using gaussian random fields as analysed in sections 3 5 and 4 and we predicted h using ordinary kriging the results are presented in part 12 of the supplementary information we found a lower bound for the rmse equal to 0 059 which is an improvement in predictive performance equal to 24 furthermore we predicted h in the usa using kriging we performed a 5 fold cross validation presented in part 12 of the supplementary information the rmse was equal to 0 069 while r 0 48 and the slope was equal to 0 26 indicating a slighter better performance of the random forests to present an application of the prediction model in fig 10 we show the variation of h in the usa and australia obtained from the prediction of the random forests using the geographic coordinates as predictor variables the spatial clustering appearing in fig 10 is in accordance with the results of the hypothesis test based on the global moran s i statistic presented in section 5 1 in fig 11 we present the errors of the random forests models when they are fitted in the 80 of the sample using the geographic coordinates as predictor variables and predict the remaining 20 of the sample in a 5 fold cross validation the pattern of the errors seems to suggest a random spatial underlying process for the errors summarizing the results of section 5 3 one may interpret that the random forests could simply perform better than the naïve methods in cases in which the underlying process is random implying that the better performance of the random forests is not the consequence of specific spatial patterns to assess such arguments we reassigned uniformly and randomly to the stations the hs and obtained a completely random pattern part 11 of the supplementary information the 5 fold cross validation proved that the naïve methods had a better predictive performance compared to the random forests in the new dataset in fact the predictive performance of the random forests decreased considerably compared to the real dataset while the performance of the naïve methods did not change as expected 6 trend analysis in this section we present the analysis on the significance of the observed trends under the lrd assumption 6 1 overview of trend estimates figure 12 is the histogram of estimated trends from the dataset of the 1535 stations for the time period 1916 2015 the median value is equal to 0 36 mm year i e in the last 100 years we observed an increase in the annual precipitation of 36 mm for comparison with the mean precipitation values we note that the median annual precipitation for the 1535 stations is equal to 718 mm 6 2 visualization of trend estimates coupled with the location variables in this section we visualize the estimated trends as well as their significance coupled with location parameters the full exploratory analysis is presented in the supplementary information while here we present some important observations in fig 13 we present how the precipitation trend varies with the climate type in all five types of grouping 2 the estimated trend is positive while we observe a larger variation for climate type a fig 14 presents the variation of trends conditional on grouping 3 it seems that non significant differences are observed between different climate types notably as shown in fig 15 the mean annual precipitation seems to have been slightly increased in the northern hemisphere and slightly decreased in the southern hemisphere this slight increase in the northern hemisphere confirms the findings of van wijngaarden and syed 2015 fig 16 depicts the monotonicity and significance of trends for each main climate type after application of the mkt lrd with a predefined significance level α 0 05 for all steps to the mean annual precipitation time series the absolute number of stations with main climate type d and positive significant trend is considerably higher compared to the number of stations with significant negative trend however the main climate types b and c are characterized by mostly significant negative trends we cannot infer on stations with main climate types a and e because of the low number of stations the observed patterns are also shown in a different form in fig 16 we observe insignificant trends in approximately 50 of the stations for main climate types a b c and d however the percentage of stations with positive significant trends is higher than the percentage of negative significant trends for main climate type d snow and e polar while the opposite is true for main climate types a b and c all other climates 7 summary discussion and conclusions we examined the long range dependence properties of mean annual precipitation of 1535 stations for the time period 1916 2015 and we tested the trends under the assumption of long range dependence based on the maximum likelihood estimates of hurst parameter h which is a measure of long range dependence we found that the median value of h is equal to 0 56 this result is consistent with those of fatichi et al 2012 and iliopoulou et al 2017 regarding the lrd properties of the mean annual precipitation from instrumental measurements which cover large part of the earth s land surface fatichi et al 2012 estimated a median value h 0 597 using an estimator based on the periodogram taqqu et al 1995 iliopoulou et al 2017 estimated a mean h 0 58 in section 5 3 we showed that the patterns of lrd are spatially clustered while fatichi et al 2012 did not identify any geographical pattern a spatially clustered pattern was produced by predicting h using random forests furthermore we showed that the location of the station and the climate type are the most important predictor variables of h followed by the elevation of the station the order of importance of the three former variables depends on the algorithm the cforest algorithm estimates that the climate type is the most important while due to its simultaneous handling of continuous and categorical variables can be considered more reliable than the random forests in estimating the variable importance the combinations 6 and 21 of predictor variables which include respectively the cartesian coordinates and the geographic coordinates of the stations performs well in terms of the error metrics but most importantly their predictions had good correlation with the tested values this correlation cannot be achieved with fitting a distribution to the set of the h values therefore the truncated normal distribution should be used with caution when modelling h and only as a prior that needs updating in a bayesian setting conditional on the observed precipitation of the location the inclusion of the climate type and the elevation combinations 9 24 improved further albeit little the performance of the random forests however this marginal improvement means that the information obtained from the geographic location of the station already includes the information of the climate type overall the improvement from the truncated normal distribution to the inclusion of the geographic coordinates is 10 which is not negligible considering that the median value 2 rmse forms an approximately 95 confidence interval hunter and goodchild 1995 the improved confidence interval is narrower by 0 04 this improvement may sound negligible but it is not as pointed out by koutsoyiannis and montanari 2007 one could claim that the ability of the used algorithms to explain the spatial patterns of h is low the low predictive performance could be due to the uncertainty in estimating h or due to the weak spatial autocorrelation of hs here we proved that for the given spatial autocorrelation the best performance for a regular spatial pattern could not be more than 24 the improvement in the real dataset was equal to 12 which is sufficient in our opinion despite the uncertainty in the estimation of h we identified spatial patterns indicating that the influence of the errors in the estimation procedure was remedied by the large size of the sample we proved that naïve methods are better than random forests when predicting in cases of complete randomness therefore the better performance of the random forests here is due to the spatial distribution of hs and not due to their ability to predict better than naïve methods when complete randomness is present the overall result is that the random forest algorithm can predict well the lrd of the mean annual precipitation when the location characteristics are used as predictor variables while their performance is considerably better compared to the predictive ability of the simple distribution of h particularly in terms of the correlation between the predicted and the estimated values therefore the random forests can be used to predict h in locations without data or insufficient quantity of data and can serve as a substitute of spatial interpolation methods compared to spatial algorithms the random forests excel in combining information from distant locations through the common latitude climate type and elevation variables even if the spatial coverage is limited and non uniform ordinary kriging has similar performance with the random forests when using the geographic coordinates as predictor variables but it can t use other explanatory variables which proved to further slightly improve the predictive performance the hurst df rdata which is the outcome of part 4 of the supplementary information can be used by the interested reader to fit a model and predict h for other applications regarding the presence of trends in the mean annual precipitation for the time period 1916 2015 it seems that the magnitude and sign of trends depend on the latitude and climate type of the station the median of estimated trends was equal to 0 36 mm year however it varies with the climate types in grouping 3 and the latitude the mkt lrd indicates that positive significant trends have been observed for the main climate type d snow in the other climate types the percentage of stations with positive significant trends was approximately equal to that of negative significant trends while 50 of all stations do not exhibit significant trends at all a limitation of our study is that the random forests algorithm can predict values only if given values of the predictor variables are within the range of the fitting set thus the limited availability of data prohibits the generalization of the method to regions and köppen geiger climate classes which are not represented by the dataset however the random forests algorithm could provide information about the full conditional distribution of h e g see coulston et al 2016 meinshausen 2006 these probabilistic predictions could be more appropriate for determining an initial prior distribution for h in a bayesian setting compared e g to the uniform distribution in tyralis et al 2014 or to a fitted distribution in a sample of estimated h values which is independent of the location the random forests algorithm provides additional means to examine the effect of interaction between the predictor variables and h which could give some insights on the natural explanation of the long range dependence in precipitation the latter issue is of high importance in hydrological science to this end non linear transformations of the variables could be tested in addition to the exploratory data analysis presented here furthermore the same fitting and testing procedure can be applied to the estimated trends and their estimated significances to generalize the preliminary results of the trend analysis acknowledgments we thank dr yiannis markonis and miss georgia papacharalampous for comments on an earlier version of this paper two anonymous reviewers comments helped us improve the original version of the paper funding information the authors received no funding for this research which was performed for scientific curiosity appendix a statistical software and supplementary information the analyses and visualizations were performed in r programming language r core team 2017 we used the contributed r packages caret kuhn 2008 kuhn et al 2017 devtools wickham and chang 2017 fbasics rmetrics core team et al 2014 fgn mcleod and veenstra 2014 gdata warnes et al 2017 geor ribeiro jr and diggle 2016 geosphere hijmans 2016a ggplot2 wickham 2016 gstat pebesma 2004 gräler et al 2016 hkprocess tyralis 2016 hmisc harrell jr et al 2017 hydrotsm zambrano bigiarini 2017 knitr xie 2014 2015 2017 lubridate grolemund and wickham 2011 magrittr bache and wickham 2014 maps brownrigg et al 2017 matrix bates and maechler 2017 ncf bjornstad 2016 party hothorn et al 2017 plyr wickham 2011 randomforest liaw and wiener 2002 raster hijmans 2016b readr wickham et al 2017 reshape2 wickham 2007 scales wickham 2017 sp bivand et al 2013b pebesma and bivand 2005 spdep bivand and piras 2015 bivand et al 2013a tmvtnorm wilhelm and manjunath 2015 truncnorm trautmann et al 2014 xts ryan and ulrich 2017 zoo zeileis and grothendieck 2005 the code used for analysing the dataset is available online as supplementary information online at tyralis 2017 the supplementary information also contains the 12 html outcomes of the code named part 1 12 the data and information about the data in a readme txt file in the main folder the interested reader can use it to reproduce our analysis supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2017 11 010 appendix b supplementary materials image application 1 
905,the long range dependence lrd is considered an inherent property of geophysical processes whose presence increases uncertainty here we examine the spatial behaviour of lrd in precipitation by regressing the hurst parameter estimate of mean annual precipitation instrumental data which span from 1916 2015 and cover a big area of the earth s surface on location characteristics of the instrumental data stations furthermore we apply the mann kendall test under the lrd assumption mkt lrd to reassess the significance of observed trends to summarize the results the lrd is spatially clustered it seems to depend mostly on the location of the stations while the predictive value of the regression model is good thus when investigating for lrd properties we recommend that the local characteristics should be considered the application of the mkt lrd suggests that no significant monotonic trend appears in global precipitation excluding the climate type d snow regions in which positive significant trends appear keywords hurst long range dependence mann kendall test precipitation random forests trend analysis 1 introduction the long range dependence lrd also known in hydrological science as the hurst phenomenon is a behaviour observed in geophysical processes in which wet years or dry years are clustered to respective long time periods koutsoyiannis 2002 a common practice for evaluating the presence of the lrd is to model the geophysical time series with the hurst kolmogorov process hkp and estimate its hurst parameter h koutsoyiannis 2003 tyralis and koutsoyiannis 2011 where high values of h indicate strong lrd the estimation of h is of great importance in engineering practice lins and cohn 2011 as indicated by koutsoyiannis 2006 koutsoyiannis and montanari 2007 and tyralis and koutsoyiannis 2014 the uncertainty increases substantially when lrd is present furthermore due to the increase in uncertainty observed trends in data even if they seem significant using classical statistical testing can be insignificant under the lrd assumption as shown by hamed 2008 most studies on the assessment of the magnitude of precipitation lrd using instrumental data are local e g liu et al 2012 munshi 2015 valle et al 2013 however some studies including fatichi et al 2012 and iliopoulou et al 2017 estimated the magnitude of the precipitation lrd from instrumental measurements in global spatial scale and argued for its weak existence although the evidence for its presence in annual precipitation records is inconclusive o connell et al 2015 similar global studies based on dissimilar datasets include kumar et al 2013 who estimated the h parameter of coupled model intercomparison project cmip5 twentieth century precipitation simulations sun et al 2014 who used reanalysis datasets and bunde et al 2013 who used instrumental measurements climate model simulations and precipitation reconstructions to infer the significance of lrd in precipitation the mann kendall test is frequently used in hydrology to evaluate the significance of trends however the mann kendall test under the lrd assumption mkt lrd hamed 2008 in which a possible presence of lrd is considered has been less frequently adopted a few local case studies in which the authors applied the mann kendall test considering the presence of lrd include the investigation of precipitation dinpashoh et al 2014 stream flows ehsanzadeh and adamowski 2010 khaliq et al 2009 kumar et al 2009 sagarika et al 2014 zamani et al 2017 and both fathian et al 2016 the analysis of point precipitation at a global setting is an important topic in hydrology for instance see de lima et al 2012 it can be supported by the analysis of precipitation instrumental data from stations that spatially cover the globe which has become a common subject in the recent literature and is supported by the increasing availability and accessibility of global data sets bierkens 2015 while it is an important constituent of global scale hydrology whose emergence was highlighted by eagleson 1986 1994 such studies include the analysis of extremes alexander et al 2006 asadieh and krakauer 2015 koutsoyiannis 2004 papalexiou and koutsoyiannis 2013 droughts nasrollahi et al 2015 analysis of trends van wijngaarden and syed 2015 the temporal concentration of precipitation monjo and martin vide 2016 and reconstruction of past precipitation smith et al 2012 although the instrumental data need some processing to be used they could be considered more reliable compared to climate simulations or reconstructions however the coverage of the earth s surface by rain gauges is not high while it decreases considerably when the analysis demands a sufficient long time period to obtain more reliable results new et al 2001 in such cases several alternative methods have been proposed including the use of satellite data kidd and huffman 2011 the spatial analysis of precipitation based on instrumental measurements can be applied in local case studies because the areas of interest are uniformly covered by the stations this is the case e g in blanchet et al 2009 who study the extreme statistics of snowfall villarini and smith 2010 who investigate flood peak distributions li et al 2011 who study precipitation trends and dyrrdal et al 2016 who analyse the extreme precipitation in this study we estimate the h parameter of the mean annual precipitation from instrumental data from a large part of the earth the database used in this study menne et al 2012a 2012b includes stations that cover the largest part of the inhabited earth surface however for statistical reasons we examine stations with data which span the hundred year period 1916 2015 and thus the coverage decreases considerably however we prefer to use this reduced dataset instead of reanalysis datasets because the artificial nature of the latter can alter considerably the results particularly when using reanalysis data from uncovered areas at early time periods the primary aim of our study is to investigate the relationship between h and locations features which has been suggested for further research in iliopoulou et al 2017 while fatichi et al 2012 did not identify the presence of a particular geographical pattern the results of sun et al 2014 and markonis and koutsoyiannis 2016 figure s3 indicate that h varies considerably with the location of the stations however they were obtained by reconstructions of past precipitation classical spatial statistical analysis cannot be applied because the coverage of the earth s surface by the examined stations is low and strongly non uniform in such cases machine learning methods are a useful alternative e g alobaidi et al 2015 leuenberger and kanevski 2015 as well as a complementary option kanevski and demyanov 2015 therefore to overcome the problem of non uniform coverage an alternative approach is to regress the h parameter estimates on spatial characteristics of the stations i e their coordinates however location characteristics of the stations such as their elevation and their köppen geiger climate class kottek et al 2006 may also be related to h the ability to include predictor variables beyond the coordinates is another advantage of machine learning methods compared to classical statistical spatial analysis methods to find all possible relationships we apply both linear regression models and random forests algorithms breiman 2001 the latter are classified as machine learning methods and they are particularly useful to model non linear relationships between the dependent and the predictor variables even when the latter are correlated furthermore they have been applied in spatial analyses with better results compared to other machine learning methods cracknell and reading 2014 a secondary aim of our study is to assess the significance of precipitation trends by applying the mkt lrd test along with an exploratory analysis in which we can present the relationship between the magnitude and significance of trends and the location characteristics morin 2011 also applied the mkt to the mean annual precipitation but using a gridded dataset and pre whitening to account for the presence of serial correlation additionally van wijngaarden and syed 2015 already examined the precipitation trends using nearly 1000 stations for the time period 1700 2013 they assessed the significance of the trends using the statistical t test at the 5 level and they concluded that some caution is warranted about claiming that large changes to global precipitation have occurred during the last 150 years zhang et al 2007 also examined the trends in global precipitation using instrumental measurements and compared them to projections of climate models with the aim to find how anthropogenic forcing influences the precipitation in most studies global precipitation and possible changes in the hydrological cycle are examined by extended use of general circulation models for instance allan et al 2014 allen and ingram 2002 gu and adler 2015 or gridded datasets for instance hartmann et al 2013 gu and adler 2015 morin 2011 climate models outputs are not used here because they contain large errors in precipitation simulations trenberth 2011 gridded datasets are not used here because they are constructed using observations of temporally varying spatial density which may lead to inaccurate estimates of parameters used in climate modelling beguería et al 2016 sun et al 2014 while they may also be based on climate models reanalysis datasets in particular the present study examines large areas thus temporal variations of the spatial density of the observations would be even higher instead here we use a set of stations with constant spatial density during the study period furthermore due to the long time period only instrumental measurements can provide sufficient data for the investigation of possible trends new et al 2001 there is also a discussion on the relationship between the precipitation and temperature changes with the aim to find constraints of the maximum precipitation change conditionally as a function of temperature change with contradictory results allan et al 2014 estimate an increase in precipitation approximately 2 3 k lambert et al 2008 estimated an increase approximately equal to 6 k while wentz et al 2007 estimated an increase equal to 7 k the assessment of the significance of trends under the assumption of lrd has been proposed for research by morin 2011 the significance of trends has been also examined using instrumental measurements in van wijngaarden and syed 2015 and using gridded datasets in hartmann et al 2013 however they did not use the lrd assumption there have been attempts to link the precipitation changes with the temperature however the statistical significance is a concept which cannot be incorporated in such framework a trend may be small but can be statistically significant and vice versa furthermore koutsoyiannis and montanari 2014 noticed that geophysical processes can be modelled using non stationary models if there is a well defined deterministic relationship constructed by deduction therefore changes in precipitation related to temperature could be useful in such setting i e if a deterministic function for the changes of the temperature could be also constructed by deduction in our opinion this is not possible see also koutsoyiannis 2010 on the other hand the assessment of the significance of trends can be incorporated in a stationary and completely stochastic framework because it can test whether the particular stochastic framework is suitable to model the geophysical process of interest 2 data we used daily precipitation data from the global historical climatology network ghcn menne et al 2012a b time periods of precipitation records for each station differ the length of the time series affects the bias and uncertainty related to the parameters estimation when the maximum likelihood estimator mle is used tyralis and koutsoyiannis 2011 see also section 3 1 therefore we preferred to use the common time period 1916 2015 while we discarded data out of this period even when the instrumental data were covering a longer time period the code and the datasets used in the present study are available online see appendix a 2 1 station and data selection the initial dataset included time series with missing or flagged i e data of low quality for reasons explained in menne et al 2012a values we processed the dataset according to the following briefly described sequence of actions a flagged values were considered as missing values b we used the values 0 34 and 0 83 to differentiate between the months months with a percentage of observations higher than 0 83 i e with more than 25 30 or 26 31 daily observations are considered good while months with a percentage of recorded values less than 0 34 i e equal or less than 10 30 and 10 31 daily observations are considered of poor quality the reason for the differentiation is that we first aggregate to the monthly time scale and then to the annual time scale thus even if all values in a month are missing we can fill the monthly value after the first aggregation as described in step c b1 missing values within months with observed values more than 83 were filled using linear interpolation b2 all values within months with observed values less than 34 were considered as missing b3 for the rest of the months the missing values were filled using linear interpolation and then these months were considered as missing the reason is explained in step d c missing months corresponding to steps b2 and b3 the latter after the substitution with missing values were filled using a seasonal kalman filter implemented in the r package zoo zeileis and grothendieck 2005 d mean monthly values for months in which both steps b3 and c i e months with missing values more than 34 and less than 83 were applied were calculated with the mean of monthly values of steps b3 and c e from the mean monthly values we obtained the mean annual values f finally we discarded annual time series if one of the following constraints was satisfied f1 two or more missing years f2 h 0 95 mean annual rainfall μ 3000 mm standard deviation of annual rainfall σ 750 mm coefficient of variation of annual rainfall c v 0 8 we set these constraints on the estimated parameters because a preliminary analysis showed that higher values were outliers f3 four or more years with less than 60 of observed daily values the estimated parameters of the annual time series of step f2 are described in section 3 1 the interested reader is referred to part 3 of the supplementary information for more details regarding the use of selection algorithms constraints for data inclusion and other details in part 4 of the supplementary information we present boxplots for every parameter of step f2 despite the removal of outliers based on the preliminary analysis in step f2 some outliers remained this will not constitute a problem in our analysis due to the robustness of the random forests see section 3 2 to outliers breiman 2001 one may argue that high values observed in step f2 may also be representative of the parameters population therefore they should not be removed e g see the textbooks of aggarwal 2017 barnett and lewis 1978 hawkins 1980 since the aim of our study is to investigate the relationship between h and location features and due to the low number of outliers found in step f2 their removal will not affect the analysis we present the locations of the subset of stations which remained after the initial procedure in fig 1 1535 stations remained most of which are located in australia europe and north america data for each station include its geographic coordinates i e elevation longitude and latitude we calculated the cartesian coordinates of stations under the assumption of a spherical earth using eqs 1 3 to model the proximity of stations which appear to be algebraically distant when considering their longitudes 1 x r cos lat cos lon 2 y r cos lat sin lon 3 z r sin lat r denotes the radius of the earth the x and y axes of the cartesian coordinate system define a plane which includes all points with zero latitude while the z axis is perpendicular to the plane e g for given lat 0 stations with longitudes 180 and 180 are coincident the coincidence can be reproduced by the transformations 1 3 2 1 1 grouping of stations to köppen geiger climate classes the stations are grouped based on the climate classification of köppen geiger kottek et al 2006 table 1 presents the classes whose combination gives the climatic types we grouped the stations according to the climate type of the nearest point of the grid provided by kottek et al 2006 we calculated distances between stations and grid points using the haversine half versed sine formula as implemented in the r package geosphere hijmans 2016a table 2 presents the 20 climate types of the stations twelve more climate types in kottek et al 2006 were not represented by the spatial distribution of the stations the model calibration presented in section 3 2 cannot be applied to the initial classification because some climate types include a low number of stations we regrouped the stations in the three groupings presented in table 2 grouping 1 included types with low number of stations together considering their main climate and precipitation type grouping 2 classified stations according to their main climate grouping 3 is similar to that of ragulina and reitan 2017 who regrouped the stations according to precipitation conditions 3 methods here we present a minimum theoretical background of the methods because they are established in the scientific literature 3 1 hurst kolmogorov process we modelled the annual time series of section 2 1 with the hkp let x t t 1 2 be a hkp the hkp is a three parameter normal stationary stochastic process in discrete time its parameters μ σ h are defined by eqs 4 6 tyralis and koutsoyiannis 2011 4 μ e x t 5 σ var x t 1 2 6 ρ k corr x t x t k k 1 2 h 2 k 1 2 h 2 k 2 h k 0 1 h 0 1 the parameter μ is the mean of the stochastic process and the parameter σ is its standard deviation the parameter η represents the magnitude of lrd i e the tendency of wet or dry years to be clustered in long time periods persistence while the autocorrelation function ρk increases with h high values of h denote strong long term persistence while when h 0 5 the resulting stochastic process is antipersistent but still stationary h 0 5 is equivalent to a stochastic process of independent variables the implementation of the maximum likelihood estimator in the r package hkprocess tyralis 2016 was applied for estimating μ σ and h furthermore we computed the maximum likelihood estimate of the coefficient of variation defined as 7 c v σ μ the maximum likelihood estimate of c v can be obtained from eq 7 after substitution of μ and σ with their maximum likelihood estimates due to the invariance properties of the mle from hereinafter μ σ and h will denote the estimates of the respective parameters we will not use hats as in step f 2 in section 2 1 the maximum likelihood estimator of the hkp parameters has excellent properties when compared to other estimators as shown in the simulation experiments in tyralis and koutsoyiannis 2011 while similar simulation experiments can be found in taqqu et al 1995 jeong et al 2007 and rea et al 2013 the latter three studies mostly implement estimators presented in tyralis et al 2011 3 2 model fitting and testing we regressed h on combinations of other available variables related to local characteristics of the stations i e their geographic coordinates cartesian coordinates elevation climate type μ and σ the use of geographic coordinates is more intuitive compared to cartesian coordinates thus we preferred to visualize the results using the former coordinate system the regression was applied using linear regression the random forests algorithm biau and scornet 2016 breiman 2001 as implemented in the r package randomforest liaw and wiener 2002 and the random forests based on conditional inference trees cforest algorithm strobl et al 2007 2008 as implemented in the r package party hothorn et al 2017 properties of linear models are well known however random forests are less used in hydrological sciences random forests can handle non linear interactions and highly correlated variables and have high predictive power furthermore random forest variable importance measures for variable selection purposes are available strobl et al 2008 therefore despite being black boxes they can still provide information about the relationship between the dependent and the predictor variables in this study we used the permutation importance which measures the mean increase of the prediction mean squared error on the out of bag portion of the data after permuting each predictor variable in the trees of the trained model more details can be found in the documentation of the importance function of the r package randomforest liaw and wiener 2002 yet the random forest importance variable measures are not reliable when the predictor variables vary in their scale of measurement or their number of categories strobl et al 2007 in such cases strobl et al 2007 propose the use of the cforest algorithm and its respective permutation importance measure which we also used in our study the three algorithms are applied using the r package caret kuhn 2008 kuhn et al 2017 we trained the three models on 80 of the sample and we tested their performance on the remaining 20 using the root mean squared error rmse mean absolute error mae mean absolute percentage error mape and pearson s r metrics h was the dependent variable while we used a combination of spatial and location variables as predictors the first testing of various combinations of predictor variables can be used initially to choose the most suitable for predicting h and reassess the selected combinations applying a computationally demanding 5 fold cross validation in the 5 fold cross validation the original sample is randomly divided into five equal sized subsamples the model is fitted in four subsamples and tested in the remaining one while the procedure is repeated five times consequently the randomness of the partitioning of the dataset in the 5 fold cross validation influences the results less compared to the simple cross validation the 5 fold cross validation was applied to four datasets sets i e the full dataset and three subsets including stations in australia europe and the usa respectively in the 5 fold cross validation we compared the performance of random forests for predicting h with the simulations of a truncated normal distribution fitted to the sample of hs and with a naïve approach in which the predicted value is equal to the median of h of the fitting set the maximum likelihood estimates of the parameters of the truncated normal distribution in each one of the 80 folds were used for the simulation of the other 20 the maximum likelihood estimates were obtained using the r package tmvtnorm wilhelm and manjunath 2015 the rmse pearson s r and the slope of the regression line between the predicted and testing values metrics were used for the comparison for more details on the application of the algorithms and the use of tuning parameters on the case of random forests and cforest through the r package caret the interested reader is referred to parts 4 8 of the supplementary information for more details regarding the use of metrics to assess the predictive performance of regression algorithms the reader is referred to alexander et al 2015 and gramatica and sangion 2016 3 3 mann kendall test under the long range dependence assumption the mkt lrd consists of three consecutive hypothesis tests namely o original mk test h hurst parameter test and m hamed 2008 let h 0 i denote the null hypothesis of each test and let h 1 i denote the alternative hypothesis where i o h m denotes the step of the mkt lrd the null hypotheses are as follows h 0 o no trend under the independence assumption h 0 h no significant lrd h 0 m no trend under lrd assumption the possible outcomes of the test are summarized by the following sequences h 0 o no significant trend h 1 o h 0 h significant trend exists h 1 o h 1 h h 0 m no significant trend h 1 o h 1 h h 1 m significant trend exists we used the test implementation in the r package hkprocess tyralis 2016 with a predefined significance level α 0 05 for all steps for more details on the algorithm and its implementation using the r package hkprocess the interested reader is referred to tegos et al 2017 furthermore we estimated the trends of the annual time series with the fitting of a linear model the estimated trends were set equal to the slope of the least squares line 3 4 global moran s i test the global moran s i test uses the moran s i statistic moran 1950 the moran s i statistic is a measure of spatial autocorrelation based on the location of variables and their observed values in the global moran s i test the null hypothesis is that the observed spatial pattern is a realization of a random spatial process the alternative hypotheses are that the spatial distribution of high values and or low values are spatially clustered or they are spatially dispersed the former holds for low p values and positive z score while the latter for low p values and negative z score more details on the global moran s test can be found in bivand et al 2013b pp 275 284 350 351 here we applied the global moran s i test by implementing the spdep r package bivand and piras 2015 bivand et al 2013a 3 5 kriging besides random forests we furthermore used ordinary kriging to predict h spatially for comparison and benchmarking reasons kriging is a stochastic method of spatial interpolation which uses normal processes to model the observed values for more details the reader may refer to bivand et al 2013b pp 232 233 kriging is less computationally intensive compared to the random forests we used the gstat r package to perform kriging pebesma 2004 gräler et al 2016 in addition we used exponential functions to model the autocovariance of the spatial model details on the autocovariance functions can be found in the documentation of the r package geor ribeiro jr and diggle 2016 4 methodology summary here we describe an outline of the method and the procedure of our analysis firstly we selected stations with precipitation data in the time period 1916 2015 we filled the missing data we computed the mean annual precipitation values and discarded some stations which did not satisfy the criteria set in section 2 1 then we grouped the stations in climate types see section 2 1 1 the record for each station includes its location in geographic and cartesian coordinates its elevation its climate type three groupings and mean annual precipitation time series we modelled the time series with hkp and we estimated the parameters μ σ h section 3 1 we regressed h on combinations of location parameters using linear regression random forests and the cforest algorithm the fitting of the algorithms was performed in the 80 of the 1535 stations while their performance was tested in the other 20 we compared the predictions of h between the random forests the simulation from a fitted truncated normal distribution and the naïve method in a 5 fold cross validation using the rmse the pearson s r and the slope of the regression line between the predicted and testing values we applied the 5 fold cross validation to the entire dataset as well as three subsets each one corresponding to a continent furthermore we computed variable importance measures with the application of random forests and the cforest to the full dataset section 3 2 the combination of the validations and the use of variable importance measures can provide reliable information despite the shortcomings of each method when used individually we applied the global moran s i test to the hs to find possible spatial patterns with the aim to assess the results of the regression model we furthermore compared the predictive performance of the random forests with ordinary kriging in which we used an exponential covariance function to model the spatial dependence of hs the comparison was performed using data from the contiguous part of the usa to find a lower bound of predictive performance we modelled the spatial pattern of the hs in the usa using a gaussian random field with an exponential covariance function while we attempted to preserve the spatial dependence of the hs to this end we simulated the spatial pattern 1000 times and we tested the predictive performance of ordinary kriging in predicting h in the 20 of the sample when fitted in the 80 of the sample finally we estimated the trend and its significance under the lrd assumption section 3 3 and we visualized the results coupled with location variables 5 long range dependence analysis in this section we present the results of the analysis for the h parameter 5 1 overview of h fig 2 is the histogram of hs the maximum likelihood estimated values are skewed to the right with skewness equal to 0 21 while the median value is equal to 0 56 for comparison reasons and as shown in a simulation study in part 9 of the supplementary information the median of the h estimates of 100 000 simulated time series of length equal to 100 and h 0 59 is equal to 0 56 furthermore 95 of the simulated experiment hs are in the interval 0 413 0 691 while 95 of the precipitation dataset hs are in the interval 0 402 0 733 a truncated normal distribution with support 0 1 seems to be a reasonable model for h figure 3 presents the correlations between some variables of interest the longitude is omitted while the inclusion of x and y coordinates as single variables would be meaningless we observe a high correlation between μ and σ and between the absolute latitude and c v h is not highly correlated with any of the variables in fig 3 to investigate the spatial properties of h we computed the spatial correlogram presented in fig 4 see part 9 the autocorrelation for distances equal to 250 km is approximately 0 1 while it increases to approximately 0 2 for distances equal to 100 km the low autocorrelations for distances higher than 250 km may decrease the accuracy of predictions of h based on spatial characteristics furthermore we calculated the global moran s i statistic moran 1950 using the global dataset as well as the respective datasets in the usa and australia part 9 of the supplementary information applying the hypothesis tests based on the global moran s i statistic we found p values almost equal to 0 and positive z scores implying that the spatial distribution of high or low values is spatially clustered which rejects the hypothesis that the underlying spatial process is random 5 2 visualization of h coupled with the predictor variables in this section we visualize h coupled with the predictor variables we present a full exploratory data analysis in the supplementary information while here we present some important figures for brevity fig 5 presents how h varies with the climate class of the station grouping 2 of table 2 is used as the predictor variable it seems that h does not significantly vary with grouping 2 while its values are near to the median value 0 56 computed in section 5 1 on the other hand grouping 1 see table 2 in fig 6 seems to be a better predictor because of the higher variation of h between different climate classes in fig 7 we observe the variation of h with the latitude higher h values are observed for positive latitude however no trend prevails while we do not observe any linear relationship between the two variables fig 7 also presents the relationship between h and longitude again we do not observe any clear linear relationship between the two variables furthermore h is not linearly related to the elevation of each station 5 3 model fitting and testing from the analysis in section 5 2 it is apparent that a linear regression model between h and the location variables could be a benchmark and be compared with the more complex random forests and the cforest algorithm we examined combinations of predictor variables as shown in table 3 combinations 1 11 and 17 26 include the dependence of h on the location of the stations combination 12 examines its dependence on variables which are features of the precipitation of the station while combinations 13 16 and 27 30 examine both location and precipitation features we built the models of table 3 using a stepwise regression method and in particular a forward selection approach i e we started with no variables and we tested the addition of each variable using criteria such as the rmse the mae the mape and pearson s r we fitted the models on 80 of the data and we tested their performance in predicting h on the other 20 in table 4 we present the testing results of each model combinations 1 and 3 16 for the cforest algorithm were omitted due to high computational load combined with the fact that they would not behave considerably different compared to the respective application of random forests random forests and the cforest had good performance while the performance of linear models was poor indicating a strong non linear relationship between the predictor variables and h the term linear here refers to the relationship e g between h and the elevation or the latitude and not to spatial relationships the cforest is more computationally intensive compared to the random forests firstly we examined the dependence of h on the elevation and the climate combinations 1 4 grouping 1 combination 2 was the best predictor with a similar performance for all methods then we examined the dependence of h on the cartesian coordinates combined with or without other variables combinations 5 11 13 16 the combination 5 i e x and y coordinates performed very good in random forests while the inclusion of the z coordinate the elevation and the climate type further improved the performance combination 11 which includes grouping 3 performed marginally better than combinations 9 and 10 which include groupings 1 and 2 respectively inclusion of μ and σ further improved the performance of the random forests combinations 13 16 secondly we performed a similar investigation using the geographic coordinates instead of the cartesian coordinates combinations 17 30 the longitude and latitude combinations 17 18 are not good predictors when we combine each one of them with grouping 1 combinations 19 20 the results are worse or similar with using grouping 1 as a single predictor the combination 21 i e longitude and latitude performed well while the inclusion of grouping 1 combination 22 weakened the regression model on the other hand the inclusion of the elevation combination 23 improved marginally the performance of the model climate type combinations 24 26 worsened the performance while inclusion of μ and σ combinations 27 30 further improved the performance of the random forests it is noteworthy that some results seem incoherent e g in the case of cartesian coordinates climate improves the random forests results combinations 8 11 while for the geographic coordinates combinations 23 26 it is the opposite this may be explained by the slight deviations induced by the inclusion of climate in this case the 5 fold cross validation presented in the following is a valid method to obtain a more reliable inference in fig 8 we present the predicted h from the application of the trained random forests for the combination 24 to the test set pearson s r indicates a good prediction while the range of predicted hs is smaller than the range of hs in the test set we observe the same behaviour for the cforest algorithm in fig 8 albeit pearson s r is somewhat lower when pearson s r is used to assess the predictive performance of regression models in the test set it is not a measure of correlation while r 2 does not explain variability in table 5 we present the results of a 5 fold cross validation for the prediction of h we compare the random forests in the combinations 2 9 17 22 24 27 28 of predictor variables with the truncated normal distribution and the naïve approach the combinations 2 9 17 22 24 can be used for the prediction of h at ungauged locations the combinations 27 and 28 which include μ and σ could be useful to predict h at an ungauged location if we could make assumptions about the μ and σ parameters based on experts experience this is also possible for cases with few years of observed data since the uncertainty in estimating μ and to a lesser extend σ is less sensitive to the sample size besides μ and σ are also representative of the climate at the given location thus they are equally useful at the stage of the analysis regarding the overall view the rmse of the random forests is lower than that of the truncated normal distribution and the naïve approach in most cases however it is notable albeit expected that the pearson s r and the slope are approximately 0 for the truncated normal distribution this highlights the importance of the higher predicting performance of the random forests in terms of pearson s r and the slope while in all cases the performance of the random forests is not perfect i e pearson s r is in the neighbourhood of 0 5 and the slope in the neighbourhood of 0 3 the improvement over the benchmark approaches is somewhat significant indeed the rmse is 10 15 lower furthermore we note that the variation of rmse pearson s r and the slope values is low for all 11 random forests cases meaning that the algorithm is stable with respect to the choice of the fitting sample there is a rather weak relationship between h and grouping 1 combination 2 while there is a rather moderate relationship between h and the longitude and latitude predictors combination 21 the inclusion of grouping 1 to the longitude and latitude predictors combination 22 did not improve the model compared to combination 21 however the inclusion of grouping 1 and the elevation combination 24 as predictor variables improved marginally the predictive performance of the fitted model a possible explanation is that all information about h is included in the geographic location of the stations knowing the climate class of the stations does not add any information to that obtained by their locations however the inclusion of μ and σ combination 28 which may represent the climate in the location better than the climate class further improved noticeably the performance of the prediction on the other hand the better performance of the combination 28 compared to combination 27 is possibly owed to that h and σ are not orthogonal tyralis and koutsoyiannis 2011 and therefore the properties of the estimates of the former depend on the latter we repeated the 5 fold cross validation for stations in the usa australia and europe parts 6 8 of the supplementary information and we present the results for the usa and australia for reasons of brevity in table 6 while the overall comparison pattern remains the same with table 5 there is a remarkable difference between the rmses of the random forests they are lower in the usa and higher in australia while the rmse of the truncated normal distribution is equal in the entire dataset and australia the inclusion of the predictor variables has a lesser effect in the case of australia for instance the rmse of combination 28 is lower by 0 11 0 087 0 076 in australia while 0 14 0 086 0 072 for the entire dataset we observe a similar behaviour in the usa 0 12 0 078 0 066 albeit the differences are less significant we attribute this behaviour to the combined information offered by entire dataset which can be effectively exploited by the random forests in fig 9 we present the variable importance for the combination 24 of predictor variables because it includes all predictor variables excluding μ and σ the location parameters combined are the most important for predicting h followed by the elevation and the climate classification on the other hand the cforest algorithm differs in that it estimates higher importance of the climate classification as presented in fig 9 bottom this is possibly owed to the better performance of the cforest algorithm when estimating categorical variables importance to find a lower bound for the errors we simulated the observed spatial pattern in the usa using gaussian random fields as analysed in sections 3 5 and 4 and we predicted h using ordinary kriging the results are presented in part 12 of the supplementary information we found a lower bound for the rmse equal to 0 059 which is an improvement in predictive performance equal to 24 furthermore we predicted h in the usa using kriging we performed a 5 fold cross validation presented in part 12 of the supplementary information the rmse was equal to 0 069 while r 0 48 and the slope was equal to 0 26 indicating a slighter better performance of the random forests to present an application of the prediction model in fig 10 we show the variation of h in the usa and australia obtained from the prediction of the random forests using the geographic coordinates as predictor variables the spatial clustering appearing in fig 10 is in accordance with the results of the hypothesis test based on the global moran s i statistic presented in section 5 1 in fig 11 we present the errors of the random forests models when they are fitted in the 80 of the sample using the geographic coordinates as predictor variables and predict the remaining 20 of the sample in a 5 fold cross validation the pattern of the errors seems to suggest a random spatial underlying process for the errors summarizing the results of section 5 3 one may interpret that the random forests could simply perform better than the naïve methods in cases in which the underlying process is random implying that the better performance of the random forests is not the consequence of specific spatial patterns to assess such arguments we reassigned uniformly and randomly to the stations the hs and obtained a completely random pattern part 11 of the supplementary information the 5 fold cross validation proved that the naïve methods had a better predictive performance compared to the random forests in the new dataset in fact the predictive performance of the random forests decreased considerably compared to the real dataset while the performance of the naïve methods did not change as expected 6 trend analysis in this section we present the analysis on the significance of the observed trends under the lrd assumption 6 1 overview of trend estimates figure 12 is the histogram of estimated trends from the dataset of the 1535 stations for the time period 1916 2015 the median value is equal to 0 36 mm year i e in the last 100 years we observed an increase in the annual precipitation of 36 mm for comparison with the mean precipitation values we note that the median annual precipitation for the 1535 stations is equal to 718 mm 6 2 visualization of trend estimates coupled with the location variables in this section we visualize the estimated trends as well as their significance coupled with location parameters the full exploratory analysis is presented in the supplementary information while here we present some important observations in fig 13 we present how the precipitation trend varies with the climate type in all five types of grouping 2 the estimated trend is positive while we observe a larger variation for climate type a fig 14 presents the variation of trends conditional on grouping 3 it seems that non significant differences are observed between different climate types notably as shown in fig 15 the mean annual precipitation seems to have been slightly increased in the northern hemisphere and slightly decreased in the southern hemisphere this slight increase in the northern hemisphere confirms the findings of van wijngaarden and syed 2015 fig 16 depicts the monotonicity and significance of trends for each main climate type after application of the mkt lrd with a predefined significance level α 0 05 for all steps to the mean annual precipitation time series the absolute number of stations with main climate type d and positive significant trend is considerably higher compared to the number of stations with significant negative trend however the main climate types b and c are characterized by mostly significant negative trends we cannot infer on stations with main climate types a and e because of the low number of stations the observed patterns are also shown in a different form in fig 16 we observe insignificant trends in approximately 50 of the stations for main climate types a b c and d however the percentage of stations with positive significant trends is higher than the percentage of negative significant trends for main climate type d snow and e polar while the opposite is true for main climate types a b and c all other climates 7 summary discussion and conclusions we examined the long range dependence properties of mean annual precipitation of 1535 stations for the time period 1916 2015 and we tested the trends under the assumption of long range dependence based on the maximum likelihood estimates of hurst parameter h which is a measure of long range dependence we found that the median value of h is equal to 0 56 this result is consistent with those of fatichi et al 2012 and iliopoulou et al 2017 regarding the lrd properties of the mean annual precipitation from instrumental measurements which cover large part of the earth s land surface fatichi et al 2012 estimated a median value h 0 597 using an estimator based on the periodogram taqqu et al 1995 iliopoulou et al 2017 estimated a mean h 0 58 in section 5 3 we showed that the patterns of lrd are spatially clustered while fatichi et al 2012 did not identify any geographical pattern a spatially clustered pattern was produced by predicting h using random forests furthermore we showed that the location of the station and the climate type are the most important predictor variables of h followed by the elevation of the station the order of importance of the three former variables depends on the algorithm the cforest algorithm estimates that the climate type is the most important while due to its simultaneous handling of continuous and categorical variables can be considered more reliable than the random forests in estimating the variable importance the combinations 6 and 21 of predictor variables which include respectively the cartesian coordinates and the geographic coordinates of the stations performs well in terms of the error metrics but most importantly their predictions had good correlation with the tested values this correlation cannot be achieved with fitting a distribution to the set of the h values therefore the truncated normal distribution should be used with caution when modelling h and only as a prior that needs updating in a bayesian setting conditional on the observed precipitation of the location the inclusion of the climate type and the elevation combinations 9 24 improved further albeit little the performance of the random forests however this marginal improvement means that the information obtained from the geographic location of the station already includes the information of the climate type overall the improvement from the truncated normal distribution to the inclusion of the geographic coordinates is 10 which is not negligible considering that the median value 2 rmse forms an approximately 95 confidence interval hunter and goodchild 1995 the improved confidence interval is narrower by 0 04 this improvement may sound negligible but it is not as pointed out by koutsoyiannis and montanari 2007 one could claim that the ability of the used algorithms to explain the spatial patterns of h is low the low predictive performance could be due to the uncertainty in estimating h or due to the weak spatial autocorrelation of hs here we proved that for the given spatial autocorrelation the best performance for a regular spatial pattern could not be more than 24 the improvement in the real dataset was equal to 12 which is sufficient in our opinion despite the uncertainty in the estimation of h we identified spatial patterns indicating that the influence of the errors in the estimation procedure was remedied by the large size of the sample we proved that naïve methods are better than random forests when predicting in cases of complete randomness therefore the better performance of the random forests here is due to the spatial distribution of hs and not due to their ability to predict better than naïve methods when complete randomness is present the overall result is that the random forest algorithm can predict well the lrd of the mean annual precipitation when the location characteristics are used as predictor variables while their performance is considerably better compared to the predictive ability of the simple distribution of h particularly in terms of the correlation between the predicted and the estimated values therefore the random forests can be used to predict h in locations without data or insufficient quantity of data and can serve as a substitute of spatial interpolation methods compared to spatial algorithms the random forests excel in combining information from distant locations through the common latitude climate type and elevation variables even if the spatial coverage is limited and non uniform ordinary kriging has similar performance with the random forests when using the geographic coordinates as predictor variables but it can t use other explanatory variables which proved to further slightly improve the predictive performance the hurst df rdata which is the outcome of part 4 of the supplementary information can be used by the interested reader to fit a model and predict h for other applications regarding the presence of trends in the mean annual precipitation for the time period 1916 2015 it seems that the magnitude and sign of trends depend on the latitude and climate type of the station the median of estimated trends was equal to 0 36 mm year however it varies with the climate types in grouping 3 and the latitude the mkt lrd indicates that positive significant trends have been observed for the main climate type d snow in the other climate types the percentage of stations with positive significant trends was approximately equal to that of negative significant trends while 50 of all stations do not exhibit significant trends at all a limitation of our study is that the random forests algorithm can predict values only if given values of the predictor variables are within the range of the fitting set thus the limited availability of data prohibits the generalization of the method to regions and köppen geiger climate classes which are not represented by the dataset however the random forests algorithm could provide information about the full conditional distribution of h e g see coulston et al 2016 meinshausen 2006 these probabilistic predictions could be more appropriate for determining an initial prior distribution for h in a bayesian setting compared e g to the uniform distribution in tyralis et al 2014 or to a fitted distribution in a sample of estimated h values which is independent of the location the random forests algorithm provides additional means to examine the effect of interaction between the predictor variables and h which could give some insights on the natural explanation of the long range dependence in precipitation the latter issue is of high importance in hydrological science to this end non linear transformations of the variables could be tested in addition to the exploratory data analysis presented here furthermore the same fitting and testing procedure can be applied to the estimated trends and their estimated significances to generalize the preliminary results of the trend analysis acknowledgments we thank dr yiannis markonis and miss georgia papacharalampous for comments on an earlier version of this paper two anonymous reviewers comments helped us improve the original version of the paper funding information the authors received no funding for this research which was performed for scientific curiosity appendix a statistical software and supplementary information the analyses and visualizations were performed in r programming language r core team 2017 we used the contributed r packages caret kuhn 2008 kuhn et al 2017 devtools wickham and chang 2017 fbasics rmetrics core team et al 2014 fgn mcleod and veenstra 2014 gdata warnes et al 2017 geor ribeiro jr and diggle 2016 geosphere hijmans 2016a ggplot2 wickham 2016 gstat pebesma 2004 gräler et al 2016 hkprocess tyralis 2016 hmisc harrell jr et al 2017 hydrotsm zambrano bigiarini 2017 knitr xie 2014 2015 2017 lubridate grolemund and wickham 2011 magrittr bache and wickham 2014 maps brownrigg et al 2017 matrix bates and maechler 2017 ncf bjornstad 2016 party hothorn et al 2017 plyr wickham 2011 randomforest liaw and wiener 2002 raster hijmans 2016b readr wickham et al 2017 reshape2 wickham 2007 scales wickham 2017 sp bivand et al 2013b pebesma and bivand 2005 spdep bivand and piras 2015 bivand et al 2013a tmvtnorm wilhelm and manjunath 2015 truncnorm trautmann et al 2014 xts ryan and ulrich 2017 zoo zeileis and grothendieck 2005 the code used for analysing the dataset is available online as supplementary information online at tyralis 2017 the supplementary information also contains the 12 html outcomes of the code named part 1 12 the data and information about the data in a readme txt file in the main folder the interested reader can use it to reproduce our analysis supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2017 11 010 appendix b supplementary materials image application 1 
906,numerical models of permafrost evolution in porous media typically rely upon a smooth continuous relation between pore ice saturation and sub freezing temperature rather than the abrupt phase change that occurs in pure media soil scientists have known for decades that this function known as the soil freezing curve sfc is related to the soil water characteristic curve swcc for unfrozen soils due to the analogous capillary and sorptive effects experienced during both soil freezing and drying herein we demonstrate that other factors beyond the sfc swcc relationship can influence the potential range over which pore water phase change occurs in particular we provide a theoretical extension for the functional form of the sfc based upon the presence of spatial heterogeneity in both soil thermal conductivity and the freezing point depression of water we infer the functional form of the sfc from many abrupt interface 1 d numerical simulations of heterogeneous systems with prescribed statistical distributions of water and soil properties the proposed sfc paradigm extension has the appealing features that it 1 is determinable from measurable soil and water properties 2 collapses into an abrupt phase transition for homogeneous media 3 describes a wide range of heterogeneity within a single functional expression and 4 replicates the observed hysteretic behavior of freeze thaw cycles in soils keywords permafrost ice fractionation soil freezing function heterogeneity soil freezing curve 1 introduction ground ice influences the mechanical hydraulic and thermal properties of soil jamshidi et al 2015 qi et al 2006 tang and yan 2014 and thus permafrost thaw can cause soil instability kemper and rosenau 1986 oztas and fayetorbay 2003 and hydrologic and hydrogeologic changes wang et al 2009 thawing permafrost also acts as a positive climate change feedback by releasing sequestered carbon into the atmosphere kurganova et al 2007 consequently quantifying the influence of recent and future global warming on permafrost thaw is an important research topic for climate scientists hydrologists and geotechnical engineers hinzman et al 2005 schuur et al 2015 walvoord and kurylyk 2016 numerical models are often employed to calculate rates of permafrost thaw because analytical solutions to heat transfer problems involving phase change are limited by their simplifying assumptions kurylyk et al 2014 in numerical models the front tracking approach skrzypczak and wȩgrzyn skrzypczak 2012 precisely predicts the location of the phase change interface and thus it is popular in solidification problems in which the exact location of the interface between the solid and liquid phase is important the issue with this method is that it may be impossible to track multiple sharp interfaces of complex shape alternately in most porous media the phase change interface is not sharp the latter is generally the case for soil freezing and thawing because pore water phase change is a non isothermal process due to capillary and sorptive forces variable solute concentrations and soil heterogeneities painter et al 2016 thus the freeze thaw interface in soil exists as a partially frozen slushy zone in this case a continuum enthalpy approach is typically implemented to represent the phase change interface dall amico et al 2011 in which the transition between the fully frozen and fully thawed zones is simulated by considering a temperature ice saturation or alternatively a temperature liquid water saturation relation called the soil freezing curve sfc koopmans and miller 1966 here the stefan condition which states that the discontinuity in heat flux at the interface is equivalent to the rate of latent heat released or absorbed lunardini 1981 is implicitly satisfied also the sharp interface which is a source of singularity in the numerical solution is smoothed out and does not require any special accommodation this continuum approach is the one most commonly employed in numerical models of freeze thaw in porous media e g voller et al 1987 swaminathan and voller 1992 alexiades and solomon 1993 nedjar 2002 in part because it is the most numerically stable sfcs are either derived theoretically based on the analogy between soil water characteristic curves swccs and sfcs koopmans and miller 1966 or empirically developed from field or laboratory data using a simple mathematical expression such as a power or exponential function kurylyk and watanabe 2013 the analog between sfcs and swccs is predicated on an understanding that pore water is held by capillary and or sorptive forces during both soil freezing and soil drying and this tightly held water retards drying or freezing processes the primary variable in swcc derived sfcs is pressure the clausius clapeyron equation is used to express the equilibrium relationship between the pressure of ice and water and soil temperature the generalized form of the clausius clapeyron equation in terms of water and ice pressure can be written as ma et al 2015 1 1 ρ w d p w 1 ρ i d p i l f t d t in which p n m 2 is pressure t is the temperature at the freezing front in kelvin k and l f is the latent heat of fusion of pure water 334 000 j kg 1 ρ kg m 3 is density and subscripts w and i denote the water and ice phase respectively this equation is an approximation that is only valid at the freezing front when the temperature is close to the melting temperature of water eq 1 which represents an equilibrium relation between pressure and temperature has several variations in the literature as reviewed by kurylyk and watanabe 2013 this equation can be inserted into an existing swcc to indirectly develop an expression between pressure or temperature and the liquid saturation hansson et al 2004 the focus of the present study is independently derived sfcs for which the primary variable is temperature the main drawback of the independently derived sfcs is that they lack a rigorous theoretical justification for their range parameters for these sfcs can be obtained based on fitting experimental curves but there is uncertainty if the curves are transferable when pressures water contents or other conditions change often a differentiable continuous function is used with the slushy zone ranging from about 1 to 4 c e g mckenzie et al 2007 the purpose of this study to develop a theoretical understanding of how mechanisms other than sorptive or capillary processes may contribute to the temperature range over which water freezes in soils in particular this study examines how spatial variability in the freezing point temperature and the soil properties may widen the sfc interval the aqueous heterogeneity i e spatial variation in the freezing point temperature is related to solute concentration and water matric potential bao et al 2016 flerchinger et al 2006 and the soil heterogeneity is ascribed to natural spatial variation in thermal conductivity at scales finer than a representative elementary volume rev 2 slushy zone characterization the existence of a slushy zone in soil is generally understood to be caused by capillary and sorptive forces which impede the complete freezing of pore water this is physically analogous to the suction range over which soil dries koopmans and miller 1966 while we acknowledge that these processes are very important in creating a temperature range over which soil freezes and thaws we herein demonstrate that the extent of the slushy zone can also be partly ascribed to variations in both water purity and soil properties which impact the freezing point of water it is noteworthy that the observed freezing temperature range is mainly below and slightly above 0 c the freezing point for pure water williams and smith 1989 while this gradual transition from pure ice to liquid water has been observed in the field and laboratory experiments e g williams 1964 koopmans and miller 1966 spaans and baker 1996 quinton et al 2005 zhou et al 2014 and repeatedly used for continuum modeling of freeze thaw processes in porous media e g lunardini 1985 flerchinger et al 2006 mckenzie et al 2007 bense et al 2009 few attempts have been made to determine the theoretical factors which determine the extent and shape of the temperature ice fraction relation beyond the similarities between soil wetting drying and soil freezing thawing processes this research presents a novel procedure to investigate the temperature ice fraction relationship in which an rev of the slushy zone is considered as an average of several stochastically generated heterogeneous soil columns each column may consist of several layers of soil with different properties which are randomly distributed in space figs 1 and 2 these heterogeneities in the properties of the system can be represented by a distribution fig 2 and this distribution causes a distribution in the freeze thaw interface fig 1b which can in turn be represented as a gradually transitioning slushy zone fig 1c while treated as parallel 1 d systems the conceptual model could also describe independent intertwining pathways through the porous medium through which freezing or thawing progresses each soil water ice system might have various degrees of heterogeneity and the characteristic heterogeneity of the system leads to a different sfc for instance chemical heterogeneity of the pore liquid ionic content leads to a depression in the soil freezing point heterogeneity in the physical properties of soil affects both the hydraulic conductivity important for advective heat transfer and the average thermal properties the focus of this research is to assess the effects of heterogeneous thermal properties and liquid characteristics particularly soil thermal conductivity and freezing point on sfcs the stochastically generated 1 d freeze thaw progression models are simulated using the extended finite element xfem method khoei 2015 as described in more detail later in this approach the interface is driven by applying the stefan condition each realization generates a different location of the ice water interface leading to a spatial distribution of local temperature and ice saturation within the 1 d column as seen in fig 3 depending on the soil type there may be residual water saturation s w res at temperatures below the freezing point due to water being tightly held cannell and gardner 1959 miller 1990 spaans and baker 1996 watanabe and mizoguchi 2002 the slushy zone in fully saturated soil would consist of two extreme fronts i e the ice front where the ice saturation s i is equal to 1 s w res where s w res is the residual liquid water content and the liquid water front where the liquid water saturation s w is equal to 1 these two fronts are separated by the slushy zone the spatial extent of this transition zone is herein ignored rather the average temperature across all realizations is mapped to the average ice saturation across all realizations generating a corresponding sfc fig 3c thus a distribution of sharp interfaces is employed to arrive at a slushy zone or distributed sfc in this study the soil is fully saturated 2 1 the 1 d model the equations used in the one dimensional realizations are similar to the governing equations of the classic stefan problem which has been applied both analytically and numerically and appears in several works bernauer and herzog 2012 chessa et al 2002 lunardini 1981 thus each of the one dimensional realizations is a two phase stefan problem but is further characterized by varying properties along the problem domain ω although heat transfer in saturated soil can occur via conduction and advection in the current model only heat transfer via conduction is considered hence the energy conservation equation over ω applies 2 t ρ c t q 3 q k t in which q is the conductive heat flux density vector kg m 2 and ρ c and k are the bulk volumetric heat capacity and bulk isotropic thermal conductivity of the soil water ice media respectively these are calculated as the volumetrically weighted arithmetic mean of the soil constituent thermal properties 4 ρ c n s w ρ w c w s i ρ i c i 1 n ρ s c s 5 k n s w k w s i k i 1 n k s where n s ρ c and k are the porosity saturation density specific heat and thermal conductivity respectively the w i and s subscripts denote the parameters of water ice and soil particles the boundary and initial conditions for this problem are 6 t t z t on γ d 7 t z 0 t ini where t z t is the temperature over the dirichlet γd boundaries in addition another condition must be satisfied at the interface between fully frozen and fully thawed media in which the temperature should be equal to the freezing point of water t f 8 t int t f on γ int where t int t f and γ int are the interface temperature soil freezing temperature and interface boundary respectively eq 8 indicates that the temperature field is continuous however its gradient is discontinuous due to the latent heat released absorbed during porewater phase change and the different soil thermal properties below and above the interface caused by the dissimilar thermal properties of pore ice and pore water as previously noted the location of interface is tracked explicitly in this study hence an energy balance equation is required to allocate the interface position and control its rate of migration this equation is called the stefan condition which is applied on γ int and stipulates that the conductive heat flow discontinuity across the interface is equal to the released or consumed latent energy with progression of that interface i e 9 k t k t n ρ w l f v where ρ w is the water density kg m 3 l f is the latent heat of fusion of water 334 000 j kg 1 v is the speed of the interface progression m s 1 and and indicate positions immediately below and above the interface the interface calculations follow the level set method presented in salvatori and tosi 2009 2 1 1 xfem formulation due to the continuity of the base shape functions used in the classic finite element method fem fem is incapable of handling discontinuity of any kind within an element the sharp interface of ice water in soil imposes weak discontinuity in the temperature field resulting in discontinuity in its gradient field xfem is an enhancement to the classic fem to overcome its inability to capture discontinuities khoei 2015 this is accomplished by adding an enhanced field to the standard interpolation field this process is called enrichment according to the nature of the stefan problem the enriched field should be continuous over ω however the gradient of this field needs to be discontinuous the xfem approximation of the temperature field can be written as 10 t z t i n n std i z t std i t j m n enr i φ z φ z j e n r i c h e d s h a p e f u n c t i o n t enr j t where n denotes the shape functions vector t is the nodal temperature vector n and m are respectively the standard and enrichment sets the std subscript denotes an association with standard degrees of freedom enr subscript denotes values associated with the enriched degrees of freedom the signed distance function chessa et al 2002 is used as the enrichment function φ z which exhibits a discontinuity in the temperature gradient field and is expressed as 11 φ z z z sign z z in which z is the interface location the weak form and fully discretized equations are presented in appendix a this solution approach closely follows merle and dolbow 2002 the accuracy of the xfem for a homogeneous case was determined in this study results not shown via comparison with results reported by merle and dolbow 2002 and verified against the exact solution of the classic stefan problem solomon 1966 2 2 soil freezing curves sfcs the sfc is herein generated via averaging of the multiple realizations of 1 d freezing front propagation in both homogeneous and heterogeneous media care was taken to avoid artefacts in the resultant sfc that could be caused by boundary condition effects numerical discretization errors domain size issues and insufficient number of realizations i e the resultant model cannot be dependent upon the geometric configuration or numerical parameters of the model in order to investigate the effects of soil heterogeneity on the sfc the freezing zone is assumed to consist of several heterogeneous soil columns and each heterogeneous column is represented by a realization which will be analyzed by the xfem figs 1 and 2 it is noteworthy that these realizations are assumed to be representatives of realistic soils after extracting the water saturation and temperature graphs from each realization and averaging them along every fixed z cross section figs 3a and b the sfc can be derived fig 3c 3 numerical simulation and results in this paper two forms of heterogeneity are investigated independently 1 soil thermal conductivity and 2 soil freezing point depression first we examine the impacts of boundary conditions and explore the sensitivity of the sfc to the extrinsic parameters of the soil water ice system of interest we then aggregate and non dimensionalize model results to infer a general sfc that is a function of the standard deviation of media thermal conductivity only lastly we determine a sfc for the case where the media is homogeneous but the freezing point is treated as a random variable due to local variability of dissolved solute concentrations the domain of interest fig 4 which is consistent in all the simulations included in this research is a 10 m 1 d heterogeneous soil column discretized into 300 homogeneous elements in the case of the soil being physically heterogeneous the thermal conductivity of the elements is log normally distributed with mean μ and standard deviation σ these are related to the untransformed mean m and untransformed variance v by the following functions 12 μ log m 2 v m 2 13 σ log v m 2 1 the variability of the soil freezing point is represented using a log normal distribution the material properties and statistical parameters used in the simulations are given in tables 1 and 2 the soil statistical parameters and thermal properties incorporated in the simulations represent soil with a clay mineral constituent reported by williams and smith 1989 and can also be found in bonan 2008 a sensitivity analysis was run to choose an appropriate number of realizations such that the minimum difference between the standard deviation of the gaussian fitting function and the standard deviation of the raw outputted data was achieved the representative average model generated sfcs are here fit with a gaussian fitting function which is defined as 14 s w t 1 s w res e t α β 2 s w res in which s w is the average water saturation s res is the residual water saturation t is the average temperature and α and β are the fitting parameters calculated from a least squares fitting procedure note that s w and t indicate average or representative s w and t in an rev and therefore translates to s w and t in the conventional sfc s w f t this functional form has been used in the past but without justification of the choice for α and β e g mckenzie et al 2007 since the gaussian function is continuous and differentiable it can be readily implemented in continuum modelling of soil freeze thaw processes as most model formulations require a continuous and differentiable function for the soil freezing curve e g mckenzie et al 2007 how such a function can be implemented in conjunction with a sorptive or capillary based soil freezing curve is a question that will be investigated in future work 3 1 sensitivity analysis effect of boundary conditions on the sfc a set of sensitivity analyses was carried out to investigate the effect of boundary conditions geometry and solution time the effect of boundary conditions on the sfc is described in this section with the desirable outcome that the impacts of 1 d model boundary conditions is negligible in this analysis twelve cases of dirichlet boundary conditions are considered the thermal conductivity and freezing point depression are both considered as sources of heterogeneity as is evident in fig 5 a the sfc is mildly impacted by boundary conditions however by giving careful consideration to the trend followed by each case it can be interpreted as a sensitivity to the system temperature gradient temperature difference of the cold and warm boundaries hence the following normalization of the average temperature results in a narrow variation of sfc fig 5b 15 t t t f abs t 0 t ini in which t is the scaled temperature t is the average temperature and t 0 and t ini are the temperatures at the freezing boundary and the initial temperature of the soil domain respectively however fig 6 clearly illustrates that boundary conditions do not have a significant influence on the sfc if the source of heterogeneity is the soil freezing point depression recall here that the primary concern is describing the temperature range of the slushy transition not the precise shape of the curve it is noteworthy that the sfcs are not sensitive to the length of the solution time or medium if the medium is sufficiently large that the problem is effectively semi infinite 3 2 heterogeneity of the soil thermal conductivity field the soil thermal conductivity is considered as the only heterogeneous parameter of the soil thermal properties two cases are studied with each case consisting of twelve sets of realizations table 2 in the first case the thermal conductivity mean is varied between 1 32 4 52 w m 1 k 1 and the standard deviation is held constant 0 2 w m 1 k 1 however in the second case the standard deviation is varied between 0 3 1 7 w m 1 k 1 and the thermal conductivity mean is held constant 2 92 w m 1 k 1 these values were loosely based on the range of thermal properties depending on the porosity and type of soil grains e g bonan 2008 as is evident in figs 7 and 8 both the thermal conductivity mean and standard deviation influence the sfc by scaling the temperature using the following equation 16 t t t f m k s s d k s the sfcs collapse into a single characteristic curve additionally figs 7b and 8 b which depict the relationship between the normalized temperature and the water saturation indicate that the sfcs for a realistic range of the statistical parameters of permafrost thermal conductivity are similar 3 3 heterogeneity of the soil freezing point depression like in the previous section both the mean and standard deviation of the water freezing point are parameters of interest in the first case the water freezing point depression mean is varied between 0 42 0 c banin and anderson 1974 and the standard deviation is held constant 0 095 c in the second case the standard deviation of the freezing point depression is varied between 0 035 0 140 c and the mean is held constant 0 25 c as expected the freezing point depression heterogeneity plays an important role figs 9 b and 10 b show that by normalizing the temperature with the following equation 17 t t m t f s d t f the soil freezing curves become identical however due to log normally distributed freezing points of the realizations there are some artifacts in the extreme case such that the mean is almost zero representing pure water in this case the freezing function gets straightened to mimic a sharp interface and cannot be resolved by eq 17 which should only be considered valid for m t f 0 1 c 3 4 hysteresis in freeze thaw conditions after applying the procedure suggested in section 2 2 for deriving the sfc for both freezing and thawing conditions it was observed that the sfc obeys a hysteresis cycle during freeze thaw as is consistent with experimental observations koopmans and miller 1966 the hysteretic behavior of permafrost has been reported at many sites including a peat plateau in scotty creek canada quinton and baltzer 2013 and in riparian peatlands of the western boreal forest canada smerdon and mendoza 2010 two sets of realizations were run to capture the hysteretic behavior and each set represents one condition freeze or thaw for the freezing scenario a 10 m one dimensional initially unfrozen soil column is considered initial condition is 5 c subjected to a top surface freezing temperature of 5 c at the freezing boundary whereas for the thawing scenario the same domain is assumed initially frozen initial condition is 5 c and subjected to a top surface thawing temperature of 5 c as fig 11 shows the suggested process mimics the hysteretic behavior revealed in field data 3 5 comparison of heterogeneity based sfcs and existing sfcs as mentioned in section 1 the existence of a slushy zone in soil is generally understood to be due to sorptive and capillary processes here after testing the hypothesis that suggests another rationale for the existence of a slushy zone we infer that this temperature range for pore water phase change can be partly ascribed to soil heterogeneities comparing the laboratory results reported by koopmans and miller 1966 and others reviewed by kurylyk and watanabe 2013 to the results of this study it is evident that the heterogeneity induced temperature range for slushy zone which varies between 0 1 to 0 3 c is narrower than those previously reported in lab results e g about 2 c but dependent on grain size and distribution clearly sorptive and capillary processes appear to account for well over half the overall temperature range however the magnitude of heterogeneity effects are not inconsiderable in comparison and may be particularly important for modelling systems on coarse grids where sub cell heterogeneity will be present 4 conclusion in this study the effect of local sub grid sub rev heterogeneity of soil thermal conductivity and depression point temperature on the sfc were investigated using a stochastic conceptual approach it was found that heterogeneity in soil thermal conductivity and depression point may be used to directly estimate an explicit functional form of the sfc for saturated porous media in the absence of capillary or sorptive forces the range of temperatures over which the slushy zone was shown to exist in this study is narrower than those reported in laboratory studies or those commonly used in existing field scale permafrost models for fine grained soils even in highly heterogeneous media these discrepancies arise because capillary and sorptive processes can expand the temperature interval of freezing and thawing such processes have been studied for decades koopmans and miller 1966 and were not the focus of the present study the temperature range for the slushy zone generated solely via thermal conductivity heterogeneity is only on the order of 0 2 c for highly heterogeneous media and the maximum range due solely to heterogeneity in water quality is on the order of 0 3 c while these ranges are small they would be a significant portion of typical sfc ranges particularly for those observed in coarse grained soils further studies will investigate how the sfc range due to heterogeneities as investigated in this study interfaces with the sfc range due to capillary and sorptive forces but at present the ranges are hypothesized to be summative this theoretical study suggests that the slushy zone extent can be different than that determined only from the swcc sfc relationship and the results appears to be consistent with experimentally determined sfcs additionally this approach suggests that a knowledge of the temperature gradient and the statistical parameters of soil thermal properties could be applied to obtain an estimate of the shape and extent of the sfc i e parameters of eq 14 can be obtained the proposed approach for generating the sfc led to closed form relationships for the sfc function that were dependent on the degree of soil and water quality heterogeneity and collapse into the sharp front condition for pure i e non porous media while not yet extended to unsaturated systems where variability in water pressure will also play a significant role in controlling the sfc the approach provides a theoretical justification for extending our understanding of both the shape and extent of the sfc in heterogeneous media the approach has been shown to replicate the observed hysteretic ice saturation temperature relation under freezing and thawing conditions and may be used to help justifying the selection of an appropriate sfc function for both column and field scale numerical studies of ice evolution in porous media for practical application we conjecture that the relative influence of the individual terms are likely to be roughly additive i e the range in temperatures over which the freezing curve varies will be the sum of the ranges from heterogeneous media heterogeneous depression point and the traditional curve determined by analogy to swccs as a future extension to this work different sources of heterogeneity can be applied conjunctively moreover the effect of advective heat transfer in the characteristics of the slushy zone could be studied as mckenzie et al 2007 illustrated that advection accelerates soil s freeze thaw processes we expect that the inclusion of high rates of advection would likely compress the sfc temperature range due to heterogeneities as conductive processes would become less important however we would note that in ice saturated medium conduction is normally the dominant process williams and smith 1989 appendix a weak form and discretization of the equations in the present work the fem and finite difference method backward euler is employed for the space and time domain discretization respectively the weak formulation of the problem is calculated by multiplying an appropriate trial function δt which could also be chosen arbitrarily to the strong form of the problem s pde eq 3 and integrating it over ω 18 ω δ t k t t ρ c t d ω 0 after implementing gauss green theorem the weak form of the problem is calculated as 19 γ δ t k t n d γ n ω δ t k t d ω ω δ t t ρ c t d ω 0 let k t n q δ t n δ t t n t n b and 20 δ t t ω n t t ρ c n t t ρ c n t t 1 δ t d ω ω b t t k t b t d ω t γ n t t q t d γ 0 in which n is the shape function and it also contains enriched shape functions in fully or partially enriched elements finally the discretized equation is expressed as 21 c t t t c t 1 t t 1 δ t k t t t f t 0 where 22 c t ω n t t ρ c t n t d ω c t ω n t t ρ c t 1 n t 1 d ω k t ω b t t k t b t d ω f t ω n t t h b t d ω γ n t t q t d γ the interface temperature condition is applied by the penalty method salvatori and tosi 2009 while the iterative procedure presented by merle and dolbow 2002 is implemented for the level set update 
906,numerical models of permafrost evolution in porous media typically rely upon a smooth continuous relation between pore ice saturation and sub freezing temperature rather than the abrupt phase change that occurs in pure media soil scientists have known for decades that this function known as the soil freezing curve sfc is related to the soil water characteristic curve swcc for unfrozen soils due to the analogous capillary and sorptive effects experienced during both soil freezing and drying herein we demonstrate that other factors beyond the sfc swcc relationship can influence the potential range over which pore water phase change occurs in particular we provide a theoretical extension for the functional form of the sfc based upon the presence of spatial heterogeneity in both soil thermal conductivity and the freezing point depression of water we infer the functional form of the sfc from many abrupt interface 1 d numerical simulations of heterogeneous systems with prescribed statistical distributions of water and soil properties the proposed sfc paradigm extension has the appealing features that it 1 is determinable from measurable soil and water properties 2 collapses into an abrupt phase transition for homogeneous media 3 describes a wide range of heterogeneity within a single functional expression and 4 replicates the observed hysteretic behavior of freeze thaw cycles in soils keywords permafrost ice fractionation soil freezing function heterogeneity soil freezing curve 1 introduction ground ice influences the mechanical hydraulic and thermal properties of soil jamshidi et al 2015 qi et al 2006 tang and yan 2014 and thus permafrost thaw can cause soil instability kemper and rosenau 1986 oztas and fayetorbay 2003 and hydrologic and hydrogeologic changes wang et al 2009 thawing permafrost also acts as a positive climate change feedback by releasing sequestered carbon into the atmosphere kurganova et al 2007 consequently quantifying the influence of recent and future global warming on permafrost thaw is an important research topic for climate scientists hydrologists and geotechnical engineers hinzman et al 2005 schuur et al 2015 walvoord and kurylyk 2016 numerical models are often employed to calculate rates of permafrost thaw because analytical solutions to heat transfer problems involving phase change are limited by their simplifying assumptions kurylyk et al 2014 in numerical models the front tracking approach skrzypczak and wȩgrzyn skrzypczak 2012 precisely predicts the location of the phase change interface and thus it is popular in solidification problems in which the exact location of the interface between the solid and liquid phase is important the issue with this method is that it may be impossible to track multiple sharp interfaces of complex shape alternately in most porous media the phase change interface is not sharp the latter is generally the case for soil freezing and thawing because pore water phase change is a non isothermal process due to capillary and sorptive forces variable solute concentrations and soil heterogeneities painter et al 2016 thus the freeze thaw interface in soil exists as a partially frozen slushy zone in this case a continuum enthalpy approach is typically implemented to represent the phase change interface dall amico et al 2011 in which the transition between the fully frozen and fully thawed zones is simulated by considering a temperature ice saturation or alternatively a temperature liquid water saturation relation called the soil freezing curve sfc koopmans and miller 1966 here the stefan condition which states that the discontinuity in heat flux at the interface is equivalent to the rate of latent heat released or absorbed lunardini 1981 is implicitly satisfied also the sharp interface which is a source of singularity in the numerical solution is smoothed out and does not require any special accommodation this continuum approach is the one most commonly employed in numerical models of freeze thaw in porous media e g voller et al 1987 swaminathan and voller 1992 alexiades and solomon 1993 nedjar 2002 in part because it is the most numerically stable sfcs are either derived theoretically based on the analogy between soil water characteristic curves swccs and sfcs koopmans and miller 1966 or empirically developed from field or laboratory data using a simple mathematical expression such as a power or exponential function kurylyk and watanabe 2013 the analog between sfcs and swccs is predicated on an understanding that pore water is held by capillary and or sorptive forces during both soil freezing and soil drying and this tightly held water retards drying or freezing processes the primary variable in swcc derived sfcs is pressure the clausius clapeyron equation is used to express the equilibrium relationship between the pressure of ice and water and soil temperature the generalized form of the clausius clapeyron equation in terms of water and ice pressure can be written as ma et al 2015 1 1 ρ w d p w 1 ρ i d p i l f t d t in which p n m 2 is pressure t is the temperature at the freezing front in kelvin k and l f is the latent heat of fusion of pure water 334 000 j kg 1 ρ kg m 3 is density and subscripts w and i denote the water and ice phase respectively this equation is an approximation that is only valid at the freezing front when the temperature is close to the melting temperature of water eq 1 which represents an equilibrium relation between pressure and temperature has several variations in the literature as reviewed by kurylyk and watanabe 2013 this equation can be inserted into an existing swcc to indirectly develop an expression between pressure or temperature and the liquid saturation hansson et al 2004 the focus of the present study is independently derived sfcs for which the primary variable is temperature the main drawback of the independently derived sfcs is that they lack a rigorous theoretical justification for their range parameters for these sfcs can be obtained based on fitting experimental curves but there is uncertainty if the curves are transferable when pressures water contents or other conditions change often a differentiable continuous function is used with the slushy zone ranging from about 1 to 4 c e g mckenzie et al 2007 the purpose of this study to develop a theoretical understanding of how mechanisms other than sorptive or capillary processes may contribute to the temperature range over which water freezes in soils in particular this study examines how spatial variability in the freezing point temperature and the soil properties may widen the sfc interval the aqueous heterogeneity i e spatial variation in the freezing point temperature is related to solute concentration and water matric potential bao et al 2016 flerchinger et al 2006 and the soil heterogeneity is ascribed to natural spatial variation in thermal conductivity at scales finer than a representative elementary volume rev 2 slushy zone characterization the existence of a slushy zone in soil is generally understood to be caused by capillary and sorptive forces which impede the complete freezing of pore water this is physically analogous to the suction range over which soil dries koopmans and miller 1966 while we acknowledge that these processes are very important in creating a temperature range over which soil freezes and thaws we herein demonstrate that the extent of the slushy zone can also be partly ascribed to variations in both water purity and soil properties which impact the freezing point of water it is noteworthy that the observed freezing temperature range is mainly below and slightly above 0 c the freezing point for pure water williams and smith 1989 while this gradual transition from pure ice to liquid water has been observed in the field and laboratory experiments e g williams 1964 koopmans and miller 1966 spaans and baker 1996 quinton et al 2005 zhou et al 2014 and repeatedly used for continuum modeling of freeze thaw processes in porous media e g lunardini 1985 flerchinger et al 2006 mckenzie et al 2007 bense et al 2009 few attempts have been made to determine the theoretical factors which determine the extent and shape of the temperature ice fraction relation beyond the similarities between soil wetting drying and soil freezing thawing processes this research presents a novel procedure to investigate the temperature ice fraction relationship in which an rev of the slushy zone is considered as an average of several stochastically generated heterogeneous soil columns each column may consist of several layers of soil with different properties which are randomly distributed in space figs 1 and 2 these heterogeneities in the properties of the system can be represented by a distribution fig 2 and this distribution causes a distribution in the freeze thaw interface fig 1b which can in turn be represented as a gradually transitioning slushy zone fig 1c while treated as parallel 1 d systems the conceptual model could also describe independent intertwining pathways through the porous medium through which freezing or thawing progresses each soil water ice system might have various degrees of heterogeneity and the characteristic heterogeneity of the system leads to a different sfc for instance chemical heterogeneity of the pore liquid ionic content leads to a depression in the soil freezing point heterogeneity in the physical properties of soil affects both the hydraulic conductivity important for advective heat transfer and the average thermal properties the focus of this research is to assess the effects of heterogeneous thermal properties and liquid characteristics particularly soil thermal conductivity and freezing point on sfcs the stochastically generated 1 d freeze thaw progression models are simulated using the extended finite element xfem method khoei 2015 as described in more detail later in this approach the interface is driven by applying the stefan condition each realization generates a different location of the ice water interface leading to a spatial distribution of local temperature and ice saturation within the 1 d column as seen in fig 3 depending on the soil type there may be residual water saturation s w res at temperatures below the freezing point due to water being tightly held cannell and gardner 1959 miller 1990 spaans and baker 1996 watanabe and mizoguchi 2002 the slushy zone in fully saturated soil would consist of two extreme fronts i e the ice front where the ice saturation s i is equal to 1 s w res where s w res is the residual liquid water content and the liquid water front where the liquid water saturation s w is equal to 1 these two fronts are separated by the slushy zone the spatial extent of this transition zone is herein ignored rather the average temperature across all realizations is mapped to the average ice saturation across all realizations generating a corresponding sfc fig 3c thus a distribution of sharp interfaces is employed to arrive at a slushy zone or distributed sfc in this study the soil is fully saturated 2 1 the 1 d model the equations used in the one dimensional realizations are similar to the governing equations of the classic stefan problem which has been applied both analytically and numerically and appears in several works bernauer and herzog 2012 chessa et al 2002 lunardini 1981 thus each of the one dimensional realizations is a two phase stefan problem but is further characterized by varying properties along the problem domain ω although heat transfer in saturated soil can occur via conduction and advection in the current model only heat transfer via conduction is considered hence the energy conservation equation over ω applies 2 t ρ c t q 3 q k t in which q is the conductive heat flux density vector kg m 2 and ρ c and k are the bulk volumetric heat capacity and bulk isotropic thermal conductivity of the soil water ice media respectively these are calculated as the volumetrically weighted arithmetic mean of the soil constituent thermal properties 4 ρ c n s w ρ w c w s i ρ i c i 1 n ρ s c s 5 k n s w k w s i k i 1 n k s where n s ρ c and k are the porosity saturation density specific heat and thermal conductivity respectively the w i and s subscripts denote the parameters of water ice and soil particles the boundary and initial conditions for this problem are 6 t t z t on γ d 7 t z 0 t ini where t z t is the temperature over the dirichlet γd boundaries in addition another condition must be satisfied at the interface between fully frozen and fully thawed media in which the temperature should be equal to the freezing point of water t f 8 t int t f on γ int where t int t f and γ int are the interface temperature soil freezing temperature and interface boundary respectively eq 8 indicates that the temperature field is continuous however its gradient is discontinuous due to the latent heat released absorbed during porewater phase change and the different soil thermal properties below and above the interface caused by the dissimilar thermal properties of pore ice and pore water as previously noted the location of interface is tracked explicitly in this study hence an energy balance equation is required to allocate the interface position and control its rate of migration this equation is called the stefan condition which is applied on γ int and stipulates that the conductive heat flow discontinuity across the interface is equal to the released or consumed latent energy with progression of that interface i e 9 k t k t n ρ w l f v where ρ w is the water density kg m 3 l f is the latent heat of fusion of water 334 000 j kg 1 v is the speed of the interface progression m s 1 and and indicate positions immediately below and above the interface the interface calculations follow the level set method presented in salvatori and tosi 2009 2 1 1 xfem formulation due to the continuity of the base shape functions used in the classic finite element method fem fem is incapable of handling discontinuity of any kind within an element the sharp interface of ice water in soil imposes weak discontinuity in the temperature field resulting in discontinuity in its gradient field xfem is an enhancement to the classic fem to overcome its inability to capture discontinuities khoei 2015 this is accomplished by adding an enhanced field to the standard interpolation field this process is called enrichment according to the nature of the stefan problem the enriched field should be continuous over ω however the gradient of this field needs to be discontinuous the xfem approximation of the temperature field can be written as 10 t z t i n n std i z t std i t j m n enr i φ z φ z j e n r i c h e d s h a p e f u n c t i o n t enr j t where n denotes the shape functions vector t is the nodal temperature vector n and m are respectively the standard and enrichment sets the std subscript denotes an association with standard degrees of freedom enr subscript denotes values associated with the enriched degrees of freedom the signed distance function chessa et al 2002 is used as the enrichment function φ z which exhibits a discontinuity in the temperature gradient field and is expressed as 11 φ z z z sign z z in which z is the interface location the weak form and fully discretized equations are presented in appendix a this solution approach closely follows merle and dolbow 2002 the accuracy of the xfem for a homogeneous case was determined in this study results not shown via comparison with results reported by merle and dolbow 2002 and verified against the exact solution of the classic stefan problem solomon 1966 2 2 soil freezing curves sfcs the sfc is herein generated via averaging of the multiple realizations of 1 d freezing front propagation in both homogeneous and heterogeneous media care was taken to avoid artefacts in the resultant sfc that could be caused by boundary condition effects numerical discretization errors domain size issues and insufficient number of realizations i e the resultant model cannot be dependent upon the geometric configuration or numerical parameters of the model in order to investigate the effects of soil heterogeneity on the sfc the freezing zone is assumed to consist of several heterogeneous soil columns and each heterogeneous column is represented by a realization which will be analyzed by the xfem figs 1 and 2 it is noteworthy that these realizations are assumed to be representatives of realistic soils after extracting the water saturation and temperature graphs from each realization and averaging them along every fixed z cross section figs 3a and b the sfc can be derived fig 3c 3 numerical simulation and results in this paper two forms of heterogeneity are investigated independently 1 soil thermal conductivity and 2 soil freezing point depression first we examine the impacts of boundary conditions and explore the sensitivity of the sfc to the extrinsic parameters of the soil water ice system of interest we then aggregate and non dimensionalize model results to infer a general sfc that is a function of the standard deviation of media thermal conductivity only lastly we determine a sfc for the case where the media is homogeneous but the freezing point is treated as a random variable due to local variability of dissolved solute concentrations the domain of interest fig 4 which is consistent in all the simulations included in this research is a 10 m 1 d heterogeneous soil column discretized into 300 homogeneous elements in the case of the soil being physically heterogeneous the thermal conductivity of the elements is log normally distributed with mean μ and standard deviation σ these are related to the untransformed mean m and untransformed variance v by the following functions 12 μ log m 2 v m 2 13 σ log v m 2 1 the variability of the soil freezing point is represented using a log normal distribution the material properties and statistical parameters used in the simulations are given in tables 1 and 2 the soil statistical parameters and thermal properties incorporated in the simulations represent soil with a clay mineral constituent reported by williams and smith 1989 and can also be found in bonan 2008 a sensitivity analysis was run to choose an appropriate number of realizations such that the minimum difference between the standard deviation of the gaussian fitting function and the standard deviation of the raw outputted data was achieved the representative average model generated sfcs are here fit with a gaussian fitting function which is defined as 14 s w t 1 s w res e t α β 2 s w res in which s w is the average water saturation s res is the residual water saturation t is the average temperature and α and β are the fitting parameters calculated from a least squares fitting procedure note that s w and t indicate average or representative s w and t in an rev and therefore translates to s w and t in the conventional sfc s w f t this functional form has been used in the past but without justification of the choice for α and β e g mckenzie et al 2007 since the gaussian function is continuous and differentiable it can be readily implemented in continuum modelling of soil freeze thaw processes as most model formulations require a continuous and differentiable function for the soil freezing curve e g mckenzie et al 2007 how such a function can be implemented in conjunction with a sorptive or capillary based soil freezing curve is a question that will be investigated in future work 3 1 sensitivity analysis effect of boundary conditions on the sfc a set of sensitivity analyses was carried out to investigate the effect of boundary conditions geometry and solution time the effect of boundary conditions on the sfc is described in this section with the desirable outcome that the impacts of 1 d model boundary conditions is negligible in this analysis twelve cases of dirichlet boundary conditions are considered the thermal conductivity and freezing point depression are both considered as sources of heterogeneity as is evident in fig 5 a the sfc is mildly impacted by boundary conditions however by giving careful consideration to the trend followed by each case it can be interpreted as a sensitivity to the system temperature gradient temperature difference of the cold and warm boundaries hence the following normalization of the average temperature results in a narrow variation of sfc fig 5b 15 t t t f abs t 0 t ini in which t is the scaled temperature t is the average temperature and t 0 and t ini are the temperatures at the freezing boundary and the initial temperature of the soil domain respectively however fig 6 clearly illustrates that boundary conditions do not have a significant influence on the sfc if the source of heterogeneity is the soil freezing point depression recall here that the primary concern is describing the temperature range of the slushy transition not the precise shape of the curve it is noteworthy that the sfcs are not sensitive to the length of the solution time or medium if the medium is sufficiently large that the problem is effectively semi infinite 3 2 heterogeneity of the soil thermal conductivity field the soil thermal conductivity is considered as the only heterogeneous parameter of the soil thermal properties two cases are studied with each case consisting of twelve sets of realizations table 2 in the first case the thermal conductivity mean is varied between 1 32 4 52 w m 1 k 1 and the standard deviation is held constant 0 2 w m 1 k 1 however in the second case the standard deviation is varied between 0 3 1 7 w m 1 k 1 and the thermal conductivity mean is held constant 2 92 w m 1 k 1 these values were loosely based on the range of thermal properties depending on the porosity and type of soil grains e g bonan 2008 as is evident in figs 7 and 8 both the thermal conductivity mean and standard deviation influence the sfc by scaling the temperature using the following equation 16 t t t f m k s s d k s the sfcs collapse into a single characteristic curve additionally figs 7b and 8 b which depict the relationship between the normalized temperature and the water saturation indicate that the sfcs for a realistic range of the statistical parameters of permafrost thermal conductivity are similar 3 3 heterogeneity of the soil freezing point depression like in the previous section both the mean and standard deviation of the water freezing point are parameters of interest in the first case the water freezing point depression mean is varied between 0 42 0 c banin and anderson 1974 and the standard deviation is held constant 0 095 c in the second case the standard deviation of the freezing point depression is varied between 0 035 0 140 c and the mean is held constant 0 25 c as expected the freezing point depression heterogeneity plays an important role figs 9 b and 10 b show that by normalizing the temperature with the following equation 17 t t m t f s d t f the soil freezing curves become identical however due to log normally distributed freezing points of the realizations there are some artifacts in the extreme case such that the mean is almost zero representing pure water in this case the freezing function gets straightened to mimic a sharp interface and cannot be resolved by eq 17 which should only be considered valid for m t f 0 1 c 3 4 hysteresis in freeze thaw conditions after applying the procedure suggested in section 2 2 for deriving the sfc for both freezing and thawing conditions it was observed that the sfc obeys a hysteresis cycle during freeze thaw as is consistent with experimental observations koopmans and miller 1966 the hysteretic behavior of permafrost has been reported at many sites including a peat plateau in scotty creek canada quinton and baltzer 2013 and in riparian peatlands of the western boreal forest canada smerdon and mendoza 2010 two sets of realizations were run to capture the hysteretic behavior and each set represents one condition freeze or thaw for the freezing scenario a 10 m one dimensional initially unfrozen soil column is considered initial condition is 5 c subjected to a top surface freezing temperature of 5 c at the freezing boundary whereas for the thawing scenario the same domain is assumed initially frozen initial condition is 5 c and subjected to a top surface thawing temperature of 5 c as fig 11 shows the suggested process mimics the hysteretic behavior revealed in field data 3 5 comparison of heterogeneity based sfcs and existing sfcs as mentioned in section 1 the existence of a slushy zone in soil is generally understood to be due to sorptive and capillary processes here after testing the hypothesis that suggests another rationale for the existence of a slushy zone we infer that this temperature range for pore water phase change can be partly ascribed to soil heterogeneities comparing the laboratory results reported by koopmans and miller 1966 and others reviewed by kurylyk and watanabe 2013 to the results of this study it is evident that the heterogeneity induced temperature range for slushy zone which varies between 0 1 to 0 3 c is narrower than those previously reported in lab results e g about 2 c but dependent on grain size and distribution clearly sorptive and capillary processes appear to account for well over half the overall temperature range however the magnitude of heterogeneity effects are not inconsiderable in comparison and may be particularly important for modelling systems on coarse grids where sub cell heterogeneity will be present 4 conclusion in this study the effect of local sub grid sub rev heterogeneity of soil thermal conductivity and depression point temperature on the sfc were investigated using a stochastic conceptual approach it was found that heterogeneity in soil thermal conductivity and depression point may be used to directly estimate an explicit functional form of the sfc for saturated porous media in the absence of capillary or sorptive forces the range of temperatures over which the slushy zone was shown to exist in this study is narrower than those reported in laboratory studies or those commonly used in existing field scale permafrost models for fine grained soils even in highly heterogeneous media these discrepancies arise because capillary and sorptive processes can expand the temperature interval of freezing and thawing such processes have been studied for decades koopmans and miller 1966 and were not the focus of the present study the temperature range for the slushy zone generated solely via thermal conductivity heterogeneity is only on the order of 0 2 c for highly heterogeneous media and the maximum range due solely to heterogeneity in water quality is on the order of 0 3 c while these ranges are small they would be a significant portion of typical sfc ranges particularly for those observed in coarse grained soils further studies will investigate how the sfc range due to heterogeneities as investigated in this study interfaces with the sfc range due to capillary and sorptive forces but at present the ranges are hypothesized to be summative this theoretical study suggests that the slushy zone extent can be different than that determined only from the swcc sfc relationship and the results appears to be consistent with experimentally determined sfcs additionally this approach suggests that a knowledge of the temperature gradient and the statistical parameters of soil thermal properties could be applied to obtain an estimate of the shape and extent of the sfc i e parameters of eq 14 can be obtained the proposed approach for generating the sfc led to closed form relationships for the sfc function that were dependent on the degree of soil and water quality heterogeneity and collapse into the sharp front condition for pure i e non porous media while not yet extended to unsaturated systems where variability in water pressure will also play a significant role in controlling the sfc the approach provides a theoretical justification for extending our understanding of both the shape and extent of the sfc in heterogeneous media the approach has been shown to replicate the observed hysteretic ice saturation temperature relation under freezing and thawing conditions and may be used to help justifying the selection of an appropriate sfc function for both column and field scale numerical studies of ice evolution in porous media for practical application we conjecture that the relative influence of the individual terms are likely to be roughly additive i e the range in temperatures over which the freezing curve varies will be the sum of the ranges from heterogeneous media heterogeneous depression point and the traditional curve determined by analogy to swccs as a future extension to this work different sources of heterogeneity can be applied conjunctively moreover the effect of advective heat transfer in the characteristics of the slushy zone could be studied as mckenzie et al 2007 illustrated that advection accelerates soil s freeze thaw processes we expect that the inclusion of high rates of advection would likely compress the sfc temperature range due to heterogeneities as conductive processes would become less important however we would note that in ice saturated medium conduction is normally the dominant process williams and smith 1989 appendix a weak form and discretization of the equations in the present work the fem and finite difference method backward euler is employed for the space and time domain discretization respectively the weak formulation of the problem is calculated by multiplying an appropriate trial function δt which could also be chosen arbitrarily to the strong form of the problem s pde eq 3 and integrating it over ω 18 ω δ t k t t ρ c t d ω 0 after implementing gauss green theorem the weak form of the problem is calculated as 19 γ δ t k t n d γ n ω δ t k t d ω ω δ t t ρ c t d ω 0 let k t n q δ t n δ t t n t n b and 20 δ t t ω n t t ρ c n t t ρ c n t t 1 δ t d ω ω b t t k t b t d ω t γ n t t q t d γ 0 in which n is the shape function and it also contains enriched shape functions in fully or partially enriched elements finally the discretized equation is expressed as 21 c t t t c t 1 t t 1 δ t k t t t f t 0 where 22 c t ω n t t ρ c t n t d ω c t ω n t t ρ c t 1 n t 1 d ω k t ω b t t k t b t d ω f t ω n t t h b t d ω γ n t t q t d γ the interface temperature condition is applied by the penalty method salvatori and tosi 2009 while the iterative procedure presented by merle and dolbow 2002 is implemented for the level set update 
907,quantitative hydrogeophysical studies rely heavily on petrophysical relationships that link geophysical properties to hydrogeological properties and state variables coupled inversion studies are frequently based on the questionable assumption that these relationships are perfect i e no scatter using synthetic examples and crosshole ground penetrating radar gpr data from the south oyster bacterial transport site in virginia usa we investigate the impact of spatially correlated petrophysical uncertainty on inferred posterior porosity and hydraulic conductivity distributions and on bayes factors used in bayesian model selection our study shows that accounting for petrophysical uncertainty in the inversion i decreases bias of the inferred variance of hydrogeological subsurface properties ii provides more realistic uncertainty assessment and iii reduces the overconfidence in the ability of geophysical data to falsify conceptual hydrogeological models keywords petrophysical uncertainty hydrogeophysics bayesian model selection bayesian inversion evidence conceptual model 1 introduction a primary goal in hydrogeophysical studies is often to infer quantitative hydrogeological models from geophysical and any available hydrogeological data unfortunately petrophysical relationships describing links between geophysical properties and hydrogeological parameters and state variables are uncertain and the information content of hydrogeophysically inferred estimates is significantly affected by their predictive power we distinguish here between three types of uncertainty in petrophysical also called rock physics models 1 petrophysical model uncertainty refers to uncertainty about the most appropriate parametric form e g archie s law time propagation model wyllie s formula 2 petrophysical parameter uncertainty relates to uncertainty about the most appropriate parameter values e g cementation index saturation exponent and 3 petrophysical prediction uncertainty describes the scatter and bias around the calibrated petrophysical model e g dispersion around predictions based on archie s law these three types of uncertainty are clearly not independent of each other for instance petrophysical prediction uncertainty is described by the residuals between the actual prediction quantity e g porosity hydraulic conductivity and the predictions for a given petrophysical model and parameter values to date most focus in hydrogeophysical inversion has been on petrophysical parameter uncertainty e g kowalsky et al 2005 lochbühler et al 2014 with the petrophysical parameter values being inferred deterministically or probabilistically as a part of the inversion process however ignoring the other two types of uncertainty may lead to biased estimates and unrealistically low uncertainty estimates for instance brunetti et al 2017 suggest that ignoring petrophysical prediction uncertainty when using bayesian model selection to discriminate among conceptual hydrogeological models will likely lead to over confidence in the ability of geophysical data to falsify and discriminate between alternative conceptual hydrogeological models linde 2014 furthermore it also implies that ad hoc data weighting schemes are needed when jointly inverting geophysical and hydrogeological data e g lochbühler et al 2013 in which each data type was given an equal weight in the objective function one approach to partly circumvent these issues is to avoid the use of explicit petrophysical relationships altogether for instance this can be achieved using structural approaches to joint inversion haber and oldenburg 1997 the cross gradient method of gallardo and meju 2003 is a widely employed approach to penalize structural dissimilarity between any two parameter fields defined as the cross product of the spatial gradients of two parameter fields hydrogeophysical adaptations and applications of this method can be found in doetsch et al 2010 linde et al 2006 2008 lochbühler et al 2013 unfortunately minimizing the cross gradient function is an inappropriate approach when both hydrogeological properties and state variables vary e g doetsch et al 2010 linde et al 2006 among a multitude of cluster based approaches we highlight the works by sun and li 2016 2017 who develop a multidomain joint clustering inversion method that uses the fuzzy c means clustering technique to constrain the statistical behaviour of inverted physical property values in the parameter domain this approach overcomes the problem of determining a priori the appropriate petrophysical model as it is allowed to exhibit different forms in different regions of the model domain for time lapse applications vasco et al 2014 circumvent the use of an explicit petrophysical model by relating the time at which a significant change in geophysical data occurs to the time of a saturation and or pressure change within a reservoir or aquifer alternative approaches are presented by hermans et al 2016 and oware et al 2013 they link geophysical properties to hydrogeological parameters by physically based regularization operators or direct multivariate statistical models but unlike other methods they adopt an explicit petrophysical relationship to create a prior set of subsurface model realizations or training images this is done to ensure geologically realistic results explicit petrophysical relationships can be integrated in hydrogeophysical inversions using two types of work flows two step or sequential inversion approaches chen et al 2001 copty et al 1993 doyen 1988 2007 rubin et al 1992 and coupled inversion approaches hinnell et al 2010 kowalsky et al 2005 the two step inversion approach consists of two sequential steps first the geophysical properties e g electrical permittivity are inferred from geophysical data e g first arrival ground penetrating radar gpr travel times through deterministic or stochastic inversions second petrophysical relationships are used to classify and map the inferred geophysical properties into probability density functions mukerji et al 2001 or deterministic estimates of hydrogeological or reservoir properties this is achieved by different statistical techniques such as co kriging discriminant analysis neural networks and bayesian classification estimation in reservoir geophysics the two step inversion approach has been favored in conjunction with sophisticated statistical rock physics models for instance shahraeeni and curtis 2011 shahraeeni et al 2012 use neural networks to map inferred seismic wave impedances into posterior distributions of porosity clay content and water saturation grana and della rossa 2010 grana et al 2012 sample the posterior distribution of reservoir properties using the monte carlo method for a given seismic model they conceptualize petrophysical prediction uncertainty as gaussian random fields with zero mean and a covariance matrix estimated by comparing predictions with well log data in hydrogeophysics the bayesian two step approaches are also used for instance by chen et al 2001 2004 to estimate hydraulic conductivity conditioned to gpr velocity gpr attenuation and seismic velocity tomograms in hydrogeophysics the two step approach has been criticized as it can lead to inconsistent estimates apparent mass loss and spatially dependent bias day lewis et al 2005 the coupled inversion approach is often formulated within a bayesian framework in which hydrogeological properties are estimated by inversion of geophysical and possibly hydrogeological data a pioneering work on coupled inversion is bosch 1999 who develops a formal bayesian procedure referred to as lithological tomography or lithological inversion in this approach markov chain monte carlo mcmc is used to integrate geophysical data geological concepts and uncertain petrophysical relationships the coupled inversion approach is well suited to integrate multiple geophysical datasets and arbitrary petrophysical relationships also when confronted with non linear physics and non linear petrophysical relationships the coupled inversion approach is preferable to a two step inversion approach bosch 2004 most hydrogeophysical works based on coupled inversion approaches assume that the petrophysical relationship is perfect with known or unknown parameter values chen et al 2006 kowalsky et al 2005 lochbühler et al 2015 when petrophysical parameter values are unknown they are inverted for simultaneously with the hydrogeological properties of interest petrophysical prediction uncertainty has received less attention in coupled inversion in the rare circumstances it is included at all it is commonly conceptualized with a multivariate gaussian distribution with known mean and covariance matrix bosch 2004 2016 bosch et al 2009 chen and dickens 2009 the petrophysical prediction uncertainty is then typically sampled using the brute force monte carlo method by adding random multivariate gaussian realizations to the petrophysical model outputs at each iteration of the mcmc inversion in this study we address the following research questions using a coupled bayesian hydrogeophysical inversion approach 1 how can we efficiently incorporate petrophysical prediction uncertainty in mcmc inversions 2 what are the consequences of ignoring or making incorrect assumptions on petrophysical prediction uncertainty including its correlation structure on inferred posterior distributions of interest 3 can we reliably infer a geostatistical model of petrophysical prediction uncertainty within the inversion 4 what are the impacts of petrophysical uncertainty on bayesian model selection results after introducing the theory and method section 2 we start out by exploring the above mentioned research questions by means of porosity estimation using synthetic crosshole gpr travel time data and an explicit well known petrophysical relationship with known parameters section 3 we then present a field case study section 4 aiming at hydraulic conductivity estimation from gpr travel time and hydraulic conductivity flowmeter data measured at the south oyster bacterial transport site in virginia usa chen et al 2001 hubbard et al 2001 scheibe et al 2011 here we solely assume to know the parametric form of the petrophysical relationship and we infer for its petrophysical parameters i e the petrophysical parameter uncertainty is considered in addition to petrophysical prediction uncertainty 2 theory and method 2 1 bayesian inference and model selection we present below a short summary of bayesian inference and model selection given n measurements y y 1 y n and a d dimensional vector of model parameters θ θ 1 θ d bayes theorem defines the posterior probability density function pdf of the model parameters p θ y as 1 p θ y p θ l θ y p y the posterior pdf describes the state of knowledge about the model parameters given the observed data and prior knowledge the prior pdf p θ quantifies the initial state of knowledge about the model parameters before considering the observed data we consider a likelihood function l θ y that is gaussian in shape by imposing uncorrelated and normally distributed measurement errors with constant standard deviation σ y 2 l θ y 2 π σ y 2 n exp 1 2 h 1 n f h θ y h σ y 2 the larger the likelihood the lower is the data misfit between the simulated forward responses f θ and the data y the evidence p y evaluates the support provided by the observed data to a given model parametrization and prior pdf conceptual model η and it is defined as the multidimensional integral of the likelihood function over the prior distribution 3 p y η l θ η y p θ η d θ computing the evidence is challenging as in general the integral in eq 3 cannot be evaluated analytically and it must be approximated by numerical means the evidence is used to calculate bayes factors and is thus the cornerstone of bayesian model selection kass and raftery 1995 bayesian model selection jeffreys 1935 1939 aims at determining the competing conceptual model that is the most supported by the observed data while honoring the principle of occam s razor this implies that if multiple conceptual models fit the data nearly equally well then the simplest model e g with the least number of unknown parameters or the smallest prior parameter ranges is favored over more complex ones gull 1988 jefferys and berger 1992 jeffreys 1939 mackay 1992 conceptual models could refer to different spatial parametrizations of the subsurface e g multi gaussian fields with isotropy or vertical anisotropy or alternative petrophysical relationships bayes factors are simply the ratio of the evidences of two competing conceptual models η 1 and η 2 for instance the bayes factor of η 1 with respect to η 2 or b η 1 η 2 is defined as 4 b η 1 η 2 p y η 1 p y η 2 subsurface conceptual models with large bayes factors are preferred statistically and the conceptual model with the largest evidence is the one that best honors the data on average over the prior pdf this implies that there is no guarantee that the correct conceptual model will be favored if a simpler model allows for similar degrees of data misfit in this work we perform coupled bayesian hydrogeophysical inversion based on mcmc sampling robert and casella 2013 using the dream zs algorithm laloy and vrugt 2012 vrugt 2016 to estimate p θ y this multi chain method creates symmetric model proposals from an historical archive of past states and automatically tunes the scales and orientation of the proposal distribution on the fly to the target posterior distribution each proposal is accepted or rejected based on the metropolis acceptance ratio hastings 1970 metropolis et al 1953 if the proposal is accepted the chain moves to the new location otherwise the chain remains at its current location acceptance ratios between 15 and 40 usually indicate good performance of the mcmc simulation gelman et al 1996 the convergence to the target posterior distribution is monitored with the analysis of variance by gelman and rubin 1992 approximate convergence is declared when the variance between the different chains is lower than the variance within each single chain gilks et al 1995 for purposes of bayesian model selection we estimate the evidence with the gaussian mixture importance sampling approach recently developed by volpi et al 2017 this approach allows for four different sampling methods reciprocal importance sampling importance sampling and bridge sampling with geometric and optimal bridge following brunetti et al 2017 we rely on importance sampling from a gaussian mixture model that is fitted to the estimated posterior probability density function 2 2 mc and mcmc sampling of petrophysical prediction uncertainty as mentioned in section 1 in the rare cases when petrophysical prediction uncertainty is included in coupled inversion it is sampled through the brute force monte carlo mc method hammersley and handscomb 1964 while the inference of model parameters of interest is achieved through mcmc this method draws independent samples from the multivariate prior distribution of petrophysical prediction uncertainty and we refer to it as mc within mcmc in section 3 1 we will demonstrate that the mc within mcmc method turns out to be very inefficient because of acceptance rates that are prohibitively low as an alternative we make use of the dream zs proposal mechanism see details in laloy and vrugt 2012 vrugt 2016 to infer the petrophysical prediction uncertainty together with the other parameters by mcmc full mcmc in essence this implies that petrophysical prediction uncertainty is parameterized and treated in the same way as the other unknowns that are inferred in the mcmc inversion both the mc within mcmc and the full mcmc approaches should converge to the same result an alternative to such explicit treatments of petrophysical prediction uncertainty as nuisance parameters is to incorporate their effects in the likelihood function however efficient and theoretically consistent ways to achieve this for non linear problems remains an open research question see section 5 2 in linde et al 2017 2 3 petrophysical relationships and geophysical forward model we consider synthetic test cases for known and theoretically based petrophysical relationships for which petrophysical prediction uncertainty is comparatively low for the field study we consider an unknown empirically based and comparatively weak petrophysical relationship the synthetic example concerns predictions of the porosity field and the field study aims at predicting hydraulic conductivity these two types of problems were chosen to span typical applications as well as different strengths and types of petrophysical relationships the synthetic examples section 3 used in this study rely on the following petrophysical relationship to link gpr velocities v m s to porosities φ 5 v φ m c 2 ɛ w φ m 1 ɛ s 1 where ɛ w 81 and c 3 10 8 m s are the relative permittivity of water and the speed of light in vacuum respectively we assume the relative permittivity of the mineral grains εs equal to 5 and the cementation index m equal to 1 5 in order to incorporate the petrophysical prediction uncertainty eq 5 is computed in three steps the effective relative permittivities ε are first found for a given porosity model pride 1994 6 step 1 ɛ ɛ s φ m ɛ w φ m ɛ s then the petrophysical prediction errors δp describing the residual for each model cell are added 7 step 2 ɛ ɛ δ p and the corresponding gpr velocities are derived 8 step 3 v c 2 ɛ 1 in the context of the field study section 4 at the south oyster bacterial transport site we compare linear and quadratic petrophysical relationships to link the gpr velocities v m s to the natural logarithm of the hydraulic conductivities k log k log m h 9 step 1 v a 0 a 1 k or 10 step 1 v a 0 a 1 k a 2 k 2 where a 0 a 1 and a 2 are the polynomial coefficients we then add δp 11 step 2 v v δ p chen et al 2001 and hubbard et al 2001 demonstrate at the south oyster bacterial transport site that the gpr velocities inferred by linear tomographic inversion are correlated to the logarithm of hydraulic conductivities with a correlation coefficient of 0 68 this suggests that the true underlying correlation is equal or stronger than this value however we stress that any relationship between gpr velocity and hydraulic conductivity is site specific and typically weak the spatial model domain of interest covers an area of 7 2 m 7 2 m below the ground surface we consider multi gaussian models of porosity hydraulic conductivity and petrophysical prediction uncertainty over a regular 2d grid of size 180 180 we use the non linear 2d traveltime solver time 2d of podvin and lecomte 1991 to compute first arrival travel times from velocity fields obtained by applying the petrophysical relationships of eqs 5 8 and 9 11 to each porosity or hydraulic conductivity field 2 4 model parameterisation we generally describe the petrophysical prediction uncertainty δp the porosity φ and the log hydraulic conductivity k fields as multi gaussian random fields the only exception is the illustrative synthetic example of section 3 1 in which the φ and δp fields correspond to independent horizontal layers we parameterise our multi gaussian fields using the method by laloy et al 2015 this method generates stationary multi gaussian fields by employing circulant embedding of the covariance matrix to decrease the number of unknowns inferred during the inversion process the dimensionality is reduced by resampling two low dimensional vectors of standard normal random numbers to the original size of the model using the one dimensional fast fourier transform interpolation we refer to laloy et al 2015 for more details in our case we generate each vector with 50 dimensionality reduction dr variables i e 100 instead of 32 400 unknowns which substantially decrease the mcmc computational cost the multi gaussian model is described by the matérn variogram model and associated geostatistical parameters including the mean and the variance the integral scale along the major axis of anisotropy i the anisotropy angle φ the ratio of the integral scales along the minor and major axis of anisotropy r and the shape parameter of the matérn variogram model ν we jointly infer the geostatistical parameters and the dr variables describing the hydrogeological properties i e porosity or hydraulic conductivity with the corresponding parameters and variables characterizing the petrophysical prediction uncertainty 3 synthetic examples 3 1 toy example mc within mcmc versus full mcmc sampling historically see section 2 2 petrophysical prediction uncertainty has been addressed by drawing independent proposals of δp from the prior while parameters of interest have been inferred by mcmc mc within mcmc as an alternative petrophysical prediction uncertainty is here parameterized and inferred as any other parameter in the mcmc inversion full mcmc we consider a toy example to demonstrate the advantage of using an appropriate model proposal distribution to infer the petrophysical prediction uncertainty full mcmc when considering moderately large or large data sets with high signal to noise ratios the set up of this simple synthetic example consists of 10 gpr transmitters and 10 receivers placed at uniform depth intervals on the right and left side of the model domain respectively fig 1 a considering all possible transmitter receiver pairs yields 100 first arrival travel time data the true porosity field is characterized by four layers of equal thickness with values of 0 3 0 45 0 35 and 0 4 starting from the ground surface fig 1a we consider synthetic travel time data that are contaminated with uncorrelated and normally distributed measurement errors with standard deviation σ y equal to 0 5 ns i e typical of crosshole gpr and 2 ns respectively we consider a uniform prior distribution of porosity in the range 0 25 0 50 and the prior distribution of the petrophysical prediction uncertainty δp is gaussian with zero mean and standard deviation of 0 8 chosen according to the experimental study of roth et al 1990 the δp values are added following eq 7 and integrated in the inversion with the mc within mcmc and the full mcmc methods see section 2 2 the latter draws the parameters from the dream zs proposal distribution that gradually update δp we obtain appropriate acceptance rates of 20 with σ y 0 5 ns and 22 with σ y 2 0 ns when considering full mcmc table 1 for mc within mcmc the acceptance ratio is 0 002 when σ y 0 5 ns and 0 31 when σ y 2 0 ns convergence to the target distribution is consequently much faster for full mcmc than for mc within mcmc especially when σ y 0 5 ns i e 5 103 forward simulations needed instead of 9 5 106 table 1 that is the mcmc derived method allows for an almost 2000 fold decrease in sampling time with respect to the mc within mcmc method this ratio grows further when using smaller σ y and more data for the case of σ y 0 5 ns we compare the posterior mean porosity fields and associated standard deviations obtained when ignoring δp fig 1b and e when using the full mcmc fig 1c and f and the mc within mcmc estimated δp fig 1d and g the posterior mean porosity fields obtained in the three cases fig 1b d are very similar and agree very well with the true porosity field shown in fig 1a the incorporation of the petrophysical prediction uncertainty results in a standard deviation fig 1f and g that is ten times higher than for the case without petrophysical prediction uncertainty fig 1e these results suggest that petrophysical prediction uncertainty has a strong effect on the inferred model uncertainty and that the full mcmc approach is much more efficient than mc within mcmc in the following we will only present results obtained by the full mcmc approach and recommend it over mc within mcmc 3 2 the forward problem impact of petrophysical prediction uncertainty for a given study area geological facies and properties change in space e g porosity specific surface area tortuosity such that the optimal parameters describing any petrophysical relationship are likely to vary in space this implies that when relying on the common assumption of a stationary petrophysical relationship i e the parameter values are the same everywhere the petrophysical prediction uncertainty is likely to have a spatially correlated structure at a scale similar to the geological variability in this section we investigate the impact of spatially correlated petrophysical prediction uncertainty on data residuals by considering forward responses obtained with and without spatially correlated petrophysical errors in this section we do not perform any inversion but simply demonstrate the impact of the correlation scale of petrophysical prediction uncertainty we consider 841 synthetic crosshole gpr travel times that are related to the porosity field in fig 2 a the porosity field is described by a multi gaussian field with horizontal anisotropy with φ 90 mean φ 0 39 variance σ φ 2 2 10 4 integral scale i φ 1 5 m integral scales ratio r φ 0 13 and the shape parameter ν φ 0 5 that corresponds to an exponential variogram in the absence of any petrophysical prediction uncertainty we obtain the velocity field by applying eq 5 with known petrophysical parameters after calculating the corresponding forward response section 2 3 we add uncorrelated gaussian observational noise with σ y 0 5 ns which leads to a root mean square error rmse of 0 5 ns for the case of uncorrelated petrophysical prediction errors we apply eqs 6 8 and draw δp realizations from an uncorrelated gaussian distribution with σ δ p 0 8 on the resulting simulated travel time data we add the same observational noise realization this yields a rmse of 0 64 ns fig 2b a comparatively small increase in rmse compared with the previous case we then describe the petrophysical prediction uncertainty with zero mean isotropic r δ p 1 multi gaussian models with σ δ p 0 8 and ν δ p 0 5 to assess the impact of the spatial correlation of the petrophysical prediction uncertainty we draw δp realizations for isotropic multi gaussian distributions with increasing integral scales for the corresponding forward responses we observe a sharp increase of rmse with increasing integral scales fig 2b for example it is higher than 1 20 ns for an integral scale of 1 5 m the rmse reaches a plateau slightly above 1 36 ns when the integral scale approaches the size of the model domain 7 2 m these results suggests that uncorrelated petrophysical prediction uncertainty i e described by a nugget model will have a relatively weak impact on inversion results when considering finely discretized models however we suspect petrophysical prediction uncertainty to be spatially correlated and this correlation increase the effect on the observed data if these effects are ignored in the inversion one would expect negative impacts on the inversion results this is studied in the following section 3 3 the inverse problem impact of assumptions on petrophysical prediction uncertainty in this section we investigate the consequences of making incorrect assumptions about petrophysical prediction uncertainty when inferring posterior distributions and bayesian model selection we consider the same true porosity field fig 2a as in section 3 2 and 841 first arrival gpr travel time data contaminated with uncorrelated and normally distributed measurement errors with standard deviation σ y 0 5 ns in the mcmc inversions we infer multi gaussian porosity fields with horizontal anisotropy and dr φ φ σ φ 2 being unknown parameters drawn from the associated prior distributions listed in table 2 while all the other geostatistical parameters affecting the porosity structure are kept fixed the petrophysical prediction uncertainty if considered is described as a zero mean multi gaussian field with horizontal anisotropy and known geostatistical parameters i e only dr δ p variables are inferred in the inversion see table 2 as before the standard deviation σ δ p was set equal to 0 8 according to the experimental study of roth et al 1990 the addition of dr δ p leads to a decrease in the magnitude of the correlation coefficient from 1 to 0 81 between the true porosity and the true gpr velocity values we consider four cases δp is not present in the data i e it is not used to generate the synthetic data and it is not inferred in the mcmc inversion case 1 δp is inferred but it is not present in the data used for inversion case 2 δp is present in the data but not inferred case 3 δp is present in the data and inferred case 4 cases 1 and 4 represent situations where the assumptions are consistent with the field situation while cases 2 and 3 are based on inconsistent assumptions we suggest that case 3 represent the most common situation in the hydrogeophysics literature i e petrophysical prediction uncertainty exists but it is ignored all cases considered provide accurate estimates of the mean porosity fig 3 a but only the consistent cases case 1 and 4 give significant probability to the actual variance i e sill describing the porosity field fig 3b with as expected case 4 providing less precise estimates i e parameter uncertainty is higher for the inconsistent cases we find for case 2 that the standard deviation of the porosity field is greatly underestimated while it is overestimated in case 3 fig 3b we now consider the resulting mean porosity fields and the standard deviations for the consistent cases for case 1 we find a mean porosity field fig 4 a that is very close to the true field fig 2a the standard deviation is low fig 4e the scatter between the mean model and the true model follows the 1 1 trend line fig 4i and the correlation coefficient is high 0 9 for case 4 we find a slightly less precise mean model fig 4d which is reflected in the standard deviation being twice as large fig 4h nevertheless the corresponding scatter plot fig 4l indicates that there is no bias the scatter falls on the 1 1 trend line and the correlation coefficient is 0 75 we now turn our attention to the inconsistent cases when considering case 2 we find a less variable mean field fig 4b and standard deviations that are in between the two consistent cases fig 4f the correlation coefficient is high 0 88 but the estimates are biased as they do not follow the 1 1 trend line fig 4j for case 3 we find an overly variable mean field fig 4c rather small standard deviations fig 4g and a moderate correlation coefficient 0 75 with a scatter plot above the 1 1 trend line fig 4k these results suggest different outcomes first including a known petrophysical prediction uncertainty in the inversion leads to consistent estimates but a wider posterior distribution than if petrophysical prediction uncertainty is absent second the correlation coefficient with the true model is mainly determined by the petrophysical prediction uncertainty third the estimated petrophysical prediction uncertainty that does not exist in case 2 accounts for some of the variability due to porosity variations which leads to a too smooth mean porosity field lastly ignoring actual petrophysical prediction uncertainty in the inversion process case 3 the common case leads to overly variable fields in order to accommodate data variability caused by both porosity variations and petrophysical prediction uncertainty from these first inversion examples we conclude that ignoring petrophysical prediction uncertainty leads to overly confident parameter inference and that some of the estimated parameters might be biased we now focus our attention on bayesian model selection for each of the four cases we also use the data to infer porosity fields assuming erroneously a multi gaussian conceptual model with isotropy or vertical anisotropy we compute the evidence for each of these conceptual models the case of the true horizontal anisotropy and the incorrect cases of isotropy and vertical anisotropy by approximating the integral in eq 3 with the gaussian mixture importance sampling estimator section 2 1 for each case we use a total of 105 importance samples and repeat the evidence computation 10 times the mean evidences and associated ranges are presented in fig 5 we find that the ranking of the different conceptual models is the same for all cases as expected the multi gaussian model with horizontal anisotropy true conceptual model has the largest evidence followed by the isotropic model fig 5a the evidence values are the largest when no petrophysical prediction uncertainty is present in the data or in the inversion case 1 fig 5a when we include δp in the inversion the evidence estimates case 2 fig 5a decrease drastically with respect to case 1 for instance we find a 29 orders of magnitude decrease of the evidence estimates for the best model multi gaussian model with horizontal anisotropy when petrophysical prediction uncertainty is absent in the data cases 1 and 2 we find thus that bayesian model selection clearly indicates that the conceptual model with horizontal anisotropy and no petrophysical prediction uncertainty is superior the consistent case note that this is the case despite the fact that we find the highest log likelihoods for case 2 black dotted lines in fig 5b d the addition of 100 unnecessary degrees of freedom in case 2 leads to a much decreased ability to differentiate among the different geostatistical models the error bars of the evidence estimates overlap for case 2 and the bayes factors table 3 are much smaller than for case 1 which imply that it is much more difficult to judge which geostatistical model is preferred statistically we have seen above that the bayesian model selection clearly favours the consistent case 1 when comparing cases 1 and 2 unfortunately this is not the case when comparing cases 3 and 4 the consistent case 4 petrophysical prediction error in data and model parameterization has a much lower evidence fig 5a for the multi gaussian model with horizontal anisotropy and much lower bayes factors table 3 than the inconsistent case 3 petrophysical prediction errors in the data only the reason for this is that case 3 has similar log likelihoods i e data misfit as case 4 fig 5b but half as many model parameters the ability to fit the data so well with this inconsistent model is probably a consequence of the petrophysical prediction uncertainty having the same geostatistical model as the porosity field this implies that formal bayesian model selection will favour a lower dimensional model parameterization that fits the data well regardless of if it is the correct model or not this is a characteristic of bayesian model selection e g schöniger et al 2015b additional tests were performed not shown with conditioning to 17 porosity values along each borehole this decreased the evidence for case 3 somewhat and increased it for case 4 however case 3 was still strongly favored when calculating the corresponding bayes factor 3 4 inference of petrophysical prediction uncertainty we have shown section 3 3 that ignoring petrophysical prediction uncertainty in mcmc inversions leads to over confident parameter estimates and biased estimates of geostatistical properties e g the sill in practical field situations it is difficult to determine a priori the appropriate geostatistical model that governs petrophysical prediction uncertainty in this section we explore to which extent it is possible to infer for both δp and its underlying geostatistical model we consider the same overall setting as in sections 3 2 and 3 3 and the same true porosity field fig 2a here the true petrophysical prediction uncertainty is a zero mean isotropic multi gaussian field with σ δ p 0 8 i δ p 0 8 m r δ p 1 and ν δ p 0 5 we then infer for the mean and variance of the porosity field and for all the geostatistical parameters of δp described above and the corresponding dr δ p variables the corresponding prior distributions of these unknown parameters are listed in tables 2 and 4 the petrophysical relationship used is eq 5 and the petrophysical prediction uncertainty is accounted for following eq 7 the inferred posterior distributions of the mean fig 6 a and variance fig 6b of the porosity field are in general quite well recovered even if they show a slight tendency to underestimate the true values overall the geostatistical properties of the reference petrophysical prediction uncertainty field are captured in the sense that the corresponding true values are included in the posterior distributions fig 6c g however some of the parameters are poorly recovered for instance the inferred standard deviation of δp is centered on the value of 1 instead of 0 8 fig 6c and the inferred shape parameter of the matérn variogram peaks on a value that is half of the corresponding true value fig 6g the anisotropy angle is poorly estimated which is a consequence of the true δp field being isotropic fig 6e the integral scale along the major axis of anisotropy and the ratio of the integral scales peak on the true values but their posterior distributions are relatively wide fig 6d and f the dominant structures in the reference porosity field fig 7 a such as the low porosity zones at a depth of 0 5 m 4 m and 6 m are well represented by the posterior mean porosity field fig 7b the posterior standard deviations on the inferred porosity field span a range between 0 6 and 1 fig 7c we find that the inferred mean petrophysical prediction uncertainty field fig 7d and the true field fig 7e have a rather low correlation coefficient 0 55 the posterior standard deviations of δp span a range between 0 6 and 1 fig 7f these large uncertainties are also reflected in the δp posterior realizations fig 8 that appear to be rather isotropic but with integral scales that vary significantly overall the structural features of the gpr velocity field are well inferred even if their values span a wider range than the reference field fig 7g h in particular the high velocity zone in the bottom right corner of the model domain are enhanced and characterized by large uncertainties fig 7i we performed also a test with the petrophysical prediction uncertainty field conceptualized by a multi gaussian field with anisotropy at 45 not shown for this case we find a significant improvement in the ability to infer for the standard deviation angle of anisotropy and the shape parameter of δp these results suggest that δp is best resolved when its geostatistical properties are markedly different from the underlying porosity field however bayesian model selection between the two conceptual models that include and not include δp in the inversion still favours the case in which petrophysical prediction uncertainty errors are ignored not shown 4 field example 4 1 field site and available data we now focus our attention on field data from the south oyster bacterial transport site in virginia usa hubbard et al 2001 in section 3 we considered a well known and strong petrophysical relationship while here we consider a case of an unknown and only moderately strong petrophysical relationship a pulseekko 100 gpr system with a 100 mhz nominal frequency antenna was used and we consider 841 crosshole gpr first arrival travel time data between 29 transmitter and 29 receiver locations in boreholes s14 and m3 respectively a total of 95 hydraulic conductivity estimates along boreholes s14 t2 and m13 obtained from an electromagnetic flowmeter were used for point conditioning following the methodology outlined by laloy et al 2015 we use the gpr data to infer the underlying log hydraulic conductivity field k assuming a multi gaussian model with horizontal anisotropy its integral scales the anisotropy angle and the shape parameter of the matérn variogram are set based on previous investigations at the site chen et al 2001 hubbard et al 2001 these fixed parameters include i k 1 5 m φ k 90 r k 0 13 and ν k 0 5 the dimensionality reduction variables dr k the mean k and standard deviation σ k of the log hydraulic conductivity field are subject to inference and the corresponding prior ranges are listed in table 5 the prior range on σ k is set to include the 0 42 log m h standard deviation of the available flowmeter data the petrophysical prediction uncertainty is described by a zero mean multi gaussian field with prior distributions outlined in table 5 the upper bound on the prior range of σ δ p is chosen such that the resulting correlation coefficient between gpr velocities and log hydraulic conductivities is equal or stronger than 0 68 which corresponds to the value reported by chen et al 2001 and hubbard et al 2001 we also jointly infer the petrophysical parameters a 0 a 1 and a 2 in eqs 9 and 10 and the standard deviation of the measurement errors σ y table 5 the overall number of parameters subject to inference is 211 4 2 results at the south oyster bacterial transport site in section 3 we considered a synthetic example and a known petrophysical relationship in the present field example we only assume to know the parametric form of the petrophysical relationship and we estimate its petrophysical parameters we infer the underlying log hydraulic conductivity field and compare the results obtained by assuming three different petrophysical models a perfect linear petrophysical relationship eq 9 in which the petrophysical prediction uncertainty is ignored model 1 a linear petrophysical relationship with scatter δp taken into account by following eqs 9 and 11 model 2 and a quadratic petrophysical relationship with scatter δp accounted for as in eqs 10 and 11 model 3 after mcmc inversion we obtain similar posterior distributions of the mean log hydraulic conductivity when using a perfect linear 1 58 log m h and a scattered linear 1 57 log m h petrophysical relationship and a slightly lower value 1 68 log m h when using a scattered quadratic petrophysical relationship fig 9 a when ignoring δp the inferred standard deviation of the log hydraulic conductivity field peaks close to the upper bound black line fig 9b when using a scattered linear or quadratic petrophysical relationship the inferred posterior distribution of the standard deviation is truncated on the lower bound of the prior range green and blue lines fig 9b the highest inferred standard deviation of the measurement errors 0 56 ns is obtained when ignoring δp in the inversion black line fig 9c when considering the scattered linear or quadratic petrophysical relationship the corresponding estimates are 0 37 ns and 0 36 ns respectively fig 9c the parameters describing the three petrophysical relationships are well defined fig 9d f the inferred standard deviation of the petrophysical prediction uncertainty peak on the upper bound of the prior range fig 9g the other geostatistical parameters describing the δp field have similar posterior distributions regardless of if a linear green lines or a quadratic blue lines petrophysical relationship is used fig 9h k in particular we find that the petrophysical prediction uncertainty field is characterized by an integral scale along the major axis of anisotropy centered around 2 4 m fig 9h an almost horizontal anisotropy fig 9i and a ratio of the integral scales of 0 30 fig 9j the posterior distribution of the matérn shape parameter is truncated by the upper bound thereby suggesting a smooth field fig 9k in fig 10 a c we display the mean posterior hydraulic conductivity fields in linear scale the three fields show similar values close to the boreholes where flowmeter data are available but away from these locations the different petrophysical models lead to different subsurface structures and estimates e g within the first meter below the ground surface and between borehole t2 and m3 fig 10a c nevertheless all the three hydraulic conductivity mean models depict a low hydraulic conductivity zone at a depth of 1 2 m b s l and at 5 6 m b s l fig 10a c when the petrophysical prediction uncertainty is ignored the inferred hydraulic conductivity fig 10a and gpr velocity fig 10g fields are characterized by a high variability on average the standard deviations of the posterior hydraulic conductivity estimates are higher when petrophysical prediction uncertainty is accounted for fig 10d f we observe similarities between the corresponding posterior gpr mean velocities fig 10g i for instance they all show a low velocity zone within the first 2 m b s l at 3 m b s l and at 5 6 m b s l and a high velocity zone at 4 5 m b s l as expected the inferred velocity fields derived from scattered petrophysical relationships fig 10h and i are smoother than the case in which this uncertainty is ignored fig 10g the mean posterior fields of the petrophysical prediction uncertainty distributions fig 10k and l are very similar and correlated with the posterior velocity means the red lines in fig 11 a c depict the inferred mean petrophysical relationships and the scatter black dots around them represents the inferred mean petrophysical prediction uncertainty the gpr velocity range appears to be overestimated whether δ p is ignored fig 11a or accounted for together with a quadratic petrophysical model fig 11c while a scattered linear petrophysical relationship fig 11b provides a velocity range in agreement with previous studies brunetti et al 2017 chen et al 2001 hubbard et al 2001 linde et al 2008 linde and vrugt 2013 we now turn our attention to the bayesian model selection results we find that model 2 scattered linear relationship has the largest evidence value 260 20 in log10 units and model 1 δp are ignored has the lowest one 361 00 fig 12 the bayes factor for the best petrophysical model model 2 with respect to model 1 and model 3 is 10100 80 and 109 38 respectively these results confirm that the perfect petrophysical model model 1 is erroneous furthermore the results suggest that the use of a more complex petrophysical relationship is not necessarily favored even if predictions based on the quadratic petrophysical model model 3 fits the data slightly better than the linear petrophysical model model 2 fig 9c the highest evidence is found for model 2 this is a consequence of the trade off between parsimony and goodness of fit typical of the occam s razor principle on which bayesian model selection is based 5 discussion our coupled bayesian hydrogeophysical inversion approach with explicit inference of spatially correlated petrophysical prediction uncertainty leads to less bias e g in the inferred variance of the inferred hydrogeological property field more realistic uncertainty quantification and less over confident model selection compared to the common choice of ignoring this type of uncertainty even if our approach to infer petrophysical prediction uncertainty doubles the number of parameters in the inversion problem we observe dramatic gains in sampling efficiency compared to mc within mcmc e g bosch 1999 2016 moreover dream zs allows for parallel evaluation of the different markov chains and therefore enables feasible computational times even in high e g in our case more than 200 model dimensions our synthetic and field based case studies suggest that it is not always possible to independently constrain hydrogeological and petrophysical properties this trade off is particularly acute when the petrophysical prediction errors have similar geostatistical properties e g orientations and integral scales as the hydrogeological property field of interest fig 7 a manifestation of this trade off is given by the field application at the south oyster bacterial transport site for which it was necessary to constrain the standard deviation of petrophysical prediction uncertainty and the standard deviation of the logarithm of hydraulic conductivity without such constraints the inversion yields largely uncorrelated log hydraulic conductivity and gpr velocity fields results that are inconsistent with previous studies chen et al 2001 hubbard et al 2001 linde et al 2008 this suggests that a careful petrophysical analysis involving borehole data or literature reviews are needed to define constraining prior information when performing coupled hydrogeophysical inversion of field data in a previous study on bayesian hydrogeophysical inversion model selection that ignored petrophysical prediction uncertainty brunetti et al 2017 it was found that the typically large data sets encountered in geophysics and the assumption of small uncorrelated data errors gaussian likelihood lead to very strong confidence in the ability of geophysical data to discriminate between alternative conceptual hydrogeological models by including spatially correlated petrophysical prediction uncertainty we find for a synthetic example fig 5 that the magnitude of the bayes factor of the best conceptual model relative to the worse one decreases by 63 orders of magnitude nevertheless the comparison between case 3 petrophysical prediction errors ignored and case 4 petrophysical prediction errors accounted for in fig 5a and table 3 still indicates high bayes factors and a practically speaking unique ability of geophysical data to find the most appropriate conceptual hydrogeological model among a set of candidates in the future one should also account for the effect of modeling errors i e the discrepancy between actual physical responses and those simulated with simplified physics here a ray based approximation in the present study instead of a full solution of the maxwell s equations a number of promising approaches to address modeling errors are available brynjarsdóttir and o hagan 2014 hansen et al 2014 xu and valocchi 2015 accounting for modeling errors is an essential next step to achieve reliable bayesian hydrogeophysical model selection we anticipate that this will further decrease the range of bayes factors bayesian model selection at the south oyster bacterial transport site section 4 demonstrates clearly that the relationship between log hydraulic conductivity and gpr velocity is not a perfect relationship that is the petrophysical model with a scattered linear relationship has a much higher evidence than results obtained by assuming a perfect linear relationship however contrasting results were obtained in the synthetic example of section 3 3 that did not involve any hydrogeological point measurements in that case formal bayesian model selection erroneously favored a conceptual model that ignored petrophysical prediction uncertainty this happens because this conceptual model has fewer parameters and is still able to fit the data well albeit with a porosity model with biased variance at the south oyster bacterial transport site we condition all model proposals to point data flowmeter estimates of hydraulic conductivity and it is then not possible to propose a biased model close to the boreholes hence the scattered petrophysical relationship is preferred however even if the inclusion of point conditioning in the synthetic example not shown decreased the bayes factor the model selection still favored the wrong conceptual model in the synthetic example we considered boreholes at the left and right sides of the model domain and the relative petrophysical prediction uncertainty was much smaller than for the field example this could explain why the inconsistency between point data and gpr data is more evident for the field example which led the bayesian model selection to favour a model with petrophysical prediction uncertainty these findings suggest that mcmc inversion and model selection is not always able to identify the right model and that their outputs need to be treated with some caution the more prior information that is available e g on petrophysical prediction uncertainty in terms of variance and correlation scale the more reliable are the results indeed bayesian model selection is built on the principle of occam s razor and a problem specific and conceptual model specific level of informative data is needed to overcome this tendency to favour a simpler but erroneous conceptual model e g schöniger et al 2015a in this study we have made the choice to infer for petrophysical prediction uncertainty instead of accounting for its effects in the likelihood function for linear theory it is indeed possible to propagate the impact of multi gaussian petrophysical errors and add the corresponding covariance matrices to the data covariance matrix bosch 2004 2016 bosch et al 2009 chen and dickens 2009 this is not possible for non linear theory as the resulting impact of petrophysical uncertainty on the data leads to model dependent non gaussian distributions the corresponding problem formulation and ways to address this problem was recently discussed by linde et al 2017 in their section 5 2 in the future it would be interesting to compare these two approaches i e inferring for petrophysical uncertainty this study or accounting for the effect of petrophysical uncertainty in the likelihood function 6 conclusions we have demonstrated the importance of accounting for petrophysical prediction uncertainty in coupled hydrogeophysical inversion and highlighted the critical role played by its spatial correlation as mcmc inversions are primarily performed to enable accurate uncertainty quantification we suggest that petrophysical prediction uncertainty should be accounted for in future hydrogeophysical studies in this work we parameterize the petrophysical prediction uncertainty as a multi gaussian field that is inferred together with hydrogeological target properties to decrease model dimensionality future work should also focus on developing computationally efficient and accurate approaches to account for this uncertainty in the likelihood function inferring petrophysical prediction uncertainty with mcmc leads to dramatic performance gains compared to previous work in which it has been accounted for by monte carlo sampling in our examples we show that ignoring petrophysical prediction uncertainty and above all its spatial correlation causes bias in the inferred variance of the hydrogeological properties which implies overly variable fields accounting for this error source allows for consistent hydrogeological estimates and widens the estimated posterior distributions however the geostatistical model describing petrophysical prediction uncertainty is only partially recoverable by the inversion when performing bayesian model selection accounting for petrophysical prediction uncertainty reduces overconfidence in the ability of geophysical data to discriminate between conceptual hydrogeological models of the subsurface when considering geophysical data alone there is a risk that bayesian hydrogeophysical model selection will favour a model parameterization that ignores petrophysical prediction uncertainty provided that the resulting overly variable hydrogeological estimates can explain the geophysical data well this highlights the importance of including constraining prior information about petrophysical prediction uncertainty and the value of combining geophysical and hydrogeological data in the inversion acknowledgments this work was supported by the swiss national science foundation under grant number 200021 155924 we thank eric laloy for making the code available for building the multi gaussian model we thank jasper vrugt for making dream zs available and for providing an initial version of the code used for gaussian mixture importance sampling we are grateful to john peterson and susan hubbard who provided the crosshole gpr and flowmeter data from the south oyster bacterial transport site we also thank the editor paolo d odorico the associated editor harrie jan hendricks franssen anneli guthke and two anonymous reviewers for their constructive and insightful comments 
907,quantitative hydrogeophysical studies rely heavily on petrophysical relationships that link geophysical properties to hydrogeological properties and state variables coupled inversion studies are frequently based on the questionable assumption that these relationships are perfect i e no scatter using synthetic examples and crosshole ground penetrating radar gpr data from the south oyster bacterial transport site in virginia usa we investigate the impact of spatially correlated petrophysical uncertainty on inferred posterior porosity and hydraulic conductivity distributions and on bayes factors used in bayesian model selection our study shows that accounting for petrophysical uncertainty in the inversion i decreases bias of the inferred variance of hydrogeological subsurface properties ii provides more realistic uncertainty assessment and iii reduces the overconfidence in the ability of geophysical data to falsify conceptual hydrogeological models keywords petrophysical uncertainty hydrogeophysics bayesian model selection bayesian inversion evidence conceptual model 1 introduction a primary goal in hydrogeophysical studies is often to infer quantitative hydrogeological models from geophysical and any available hydrogeological data unfortunately petrophysical relationships describing links between geophysical properties and hydrogeological parameters and state variables are uncertain and the information content of hydrogeophysically inferred estimates is significantly affected by their predictive power we distinguish here between three types of uncertainty in petrophysical also called rock physics models 1 petrophysical model uncertainty refers to uncertainty about the most appropriate parametric form e g archie s law time propagation model wyllie s formula 2 petrophysical parameter uncertainty relates to uncertainty about the most appropriate parameter values e g cementation index saturation exponent and 3 petrophysical prediction uncertainty describes the scatter and bias around the calibrated petrophysical model e g dispersion around predictions based on archie s law these three types of uncertainty are clearly not independent of each other for instance petrophysical prediction uncertainty is described by the residuals between the actual prediction quantity e g porosity hydraulic conductivity and the predictions for a given petrophysical model and parameter values to date most focus in hydrogeophysical inversion has been on petrophysical parameter uncertainty e g kowalsky et al 2005 lochbühler et al 2014 with the petrophysical parameter values being inferred deterministically or probabilistically as a part of the inversion process however ignoring the other two types of uncertainty may lead to biased estimates and unrealistically low uncertainty estimates for instance brunetti et al 2017 suggest that ignoring petrophysical prediction uncertainty when using bayesian model selection to discriminate among conceptual hydrogeological models will likely lead to over confidence in the ability of geophysical data to falsify and discriminate between alternative conceptual hydrogeological models linde 2014 furthermore it also implies that ad hoc data weighting schemes are needed when jointly inverting geophysical and hydrogeological data e g lochbühler et al 2013 in which each data type was given an equal weight in the objective function one approach to partly circumvent these issues is to avoid the use of explicit petrophysical relationships altogether for instance this can be achieved using structural approaches to joint inversion haber and oldenburg 1997 the cross gradient method of gallardo and meju 2003 is a widely employed approach to penalize structural dissimilarity between any two parameter fields defined as the cross product of the spatial gradients of two parameter fields hydrogeophysical adaptations and applications of this method can be found in doetsch et al 2010 linde et al 2006 2008 lochbühler et al 2013 unfortunately minimizing the cross gradient function is an inappropriate approach when both hydrogeological properties and state variables vary e g doetsch et al 2010 linde et al 2006 among a multitude of cluster based approaches we highlight the works by sun and li 2016 2017 who develop a multidomain joint clustering inversion method that uses the fuzzy c means clustering technique to constrain the statistical behaviour of inverted physical property values in the parameter domain this approach overcomes the problem of determining a priori the appropriate petrophysical model as it is allowed to exhibit different forms in different regions of the model domain for time lapse applications vasco et al 2014 circumvent the use of an explicit petrophysical model by relating the time at which a significant change in geophysical data occurs to the time of a saturation and or pressure change within a reservoir or aquifer alternative approaches are presented by hermans et al 2016 and oware et al 2013 they link geophysical properties to hydrogeological parameters by physically based regularization operators or direct multivariate statistical models but unlike other methods they adopt an explicit petrophysical relationship to create a prior set of subsurface model realizations or training images this is done to ensure geologically realistic results explicit petrophysical relationships can be integrated in hydrogeophysical inversions using two types of work flows two step or sequential inversion approaches chen et al 2001 copty et al 1993 doyen 1988 2007 rubin et al 1992 and coupled inversion approaches hinnell et al 2010 kowalsky et al 2005 the two step inversion approach consists of two sequential steps first the geophysical properties e g electrical permittivity are inferred from geophysical data e g first arrival ground penetrating radar gpr travel times through deterministic or stochastic inversions second petrophysical relationships are used to classify and map the inferred geophysical properties into probability density functions mukerji et al 2001 or deterministic estimates of hydrogeological or reservoir properties this is achieved by different statistical techniques such as co kriging discriminant analysis neural networks and bayesian classification estimation in reservoir geophysics the two step inversion approach has been favored in conjunction with sophisticated statistical rock physics models for instance shahraeeni and curtis 2011 shahraeeni et al 2012 use neural networks to map inferred seismic wave impedances into posterior distributions of porosity clay content and water saturation grana and della rossa 2010 grana et al 2012 sample the posterior distribution of reservoir properties using the monte carlo method for a given seismic model they conceptualize petrophysical prediction uncertainty as gaussian random fields with zero mean and a covariance matrix estimated by comparing predictions with well log data in hydrogeophysics the bayesian two step approaches are also used for instance by chen et al 2001 2004 to estimate hydraulic conductivity conditioned to gpr velocity gpr attenuation and seismic velocity tomograms in hydrogeophysics the two step approach has been criticized as it can lead to inconsistent estimates apparent mass loss and spatially dependent bias day lewis et al 2005 the coupled inversion approach is often formulated within a bayesian framework in which hydrogeological properties are estimated by inversion of geophysical and possibly hydrogeological data a pioneering work on coupled inversion is bosch 1999 who develops a formal bayesian procedure referred to as lithological tomography or lithological inversion in this approach markov chain monte carlo mcmc is used to integrate geophysical data geological concepts and uncertain petrophysical relationships the coupled inversion approach is well suited to integrate multiple geophysical datasets and arbitrary petrophysical relationships also when confronted with non linear physics and non linear petrophysical relationships the coupled inversion approach is preferable to a two step inversion approach bosch 2004 most hydrogeophysical works based on coupled inversion approaches assume that the petrophysical relationship is perfect with known or unknown parameter values chen et al 2006 kowalsky et al 2005 lochbühler et al 2015 when petrophysical parameter values are unknown they are inverted for simultaneously with the hydrogeological properties of interest petrophysical prediction uncertainty has received less attention in coupled inversion in the rare circumstances it is included at all it is commonly conceptualized with a multivariate gaussian distribution with known mean and covariance matrix bosch 2004 2016 bosch et al 2009 chen and dickens 2009 the petrophysical prediction uncertainty is then typically sampled using the brute force monte carlo method by adding random multivariate gaussian realizations to the petrophysical model outputs at each iteration of the mcmc inversion in this study we address the following research questions using a coupled bayesian hydrogeophysical inversion approach 1 how can we efficiently incorporate petrophysical prediction uncertainty in mcmc inversions 2 what are the consequences of ignoring or making incorrect assumptions on petrophysical prediction uncertainty including its correlation structure on inferred posterior distributions of interest 3 can we reliably infer a geostatistical model of petrophysical prediction uncertainty within the inversion 4 what are the impacts of petrophysical uncertainty on bayesian model selection results after introducing the theory and method section 2 we start out by exploring the above mentioned research questions by means of porosity estimation using synthetic crosshole gpr travel time data and an explicit well known petrophysical relationship with known parameters section 3 we then present a field case study section 4 aiming at hydraulic conductivity estimation from gpr travel time and hydraulic conductivity flowmeter data measured at the south oyster bacterial transport site in virginia usa chen et al 2001 hubbard et al 2001 scheibe et al 2011 here we solely assume to know the parametric form of the petrophysical relationship and we infer for its petrophysical parameters i e the petrophysical parameter uncertainty is considered in addition to petrophysical prediction uncertainty 2 theory and method 2 1 bayesian inference and model selection we present below a short summary of bayesian inference and model selection given n measurements y y 1 y n and a d dimensional vector of model parameters θ θ 1 θ d bayes theorem defines the posterior probability density function pdf of the model parameters p θ y as 1 p θ y p θ l θ y p y the posterior pdf describes the state of knowledge about the model parameters given the observed data and prior knowledge the prior pdf p θ quantifies the initial state of knowledge about the model parameters before considering the observed data we consider a likelihood function l θ y that is gaussian in shape by imposing uncorrelated and normally distributed measurement errors with constant standard deviation σ y 2 l θ y 2 π σ y 2 n exp 1 2 h 1 n f h θ y h σ y 2 the larger the likelihood the lower is the data misfit between the simulated forward responses f θ and the data y the evidence p y evaluates the support provided by the observed data to a given model parametrization and prior pdf conceptual model η and it is defined as the multidimensional integral of the likelihood function over the prior distribution 3 p y η l θ η y p θ η d θ computing the evidence is challenging as in general the integral in eq 3 cannot be evaluated analytically and it must be approximated by numerical means the evidence is used to calculate bayes factors and is thus the cornerstone of bayesian model selection kass and raftery 1995 bayesian model selection jeffreys 1935 1939 aims at determining the competing conceptual model that is the most supported by the observed data while honoring the principle of occam s razor this implies that if multiple conceptual models fit the data nearly equally well then the simplest model e g with the least number of unknown parameters or the smallest prior parameter ranges is favored over more complex ones gull 1988 jefferys and berger 1992 jeffreys 1939 mackay 1992 conceptual models could refer to different spatial parametrizations of the subsurface e g multi gaussian fields with isotropy or vertical anisotropy or alternative petrophysical relationships bayes factors are simply the ratio of the evidences of two competing conceptual models η 1 and η 2 for instance the bayes factor of η 1 with respect to η 2 or b η 1 η 2 is defined as 4 b η 1 η 2 p y η 1 p y η 2 subsurface conceptual models with large bayes factors are preferred statistically and the conceptual model with the largest evidence is the one that best honors the data on average over the prior pdf this implies that there is no guarantee that the correct conceptual model will be favored if a simpler model allows for similar degrees of data misfit in this work we perform coupled bayesian hydrogeophysical inversion based on mcmc sampling robert and casella 2013 using the dream zs algorithm laloy and vrugt 2012 vrugt 2016 to estimate p θ y this multi chain method creates symmetric model proposals from an historical archive of past states and automatically tunes the scales and orientation of the proposal distribution on the fly to the target posterior distribution each proposal is accepted or rejected based on the metropolis acceptance ratio hastings 1970 metropolis et al 1953 if the proposal is accepted the chain moves to the new location otherwise the chain remains at its current location acceptance ratios between 15 and 40 usually indicate good performance of the mcmc simulation gelman et al 1996 the convergence to the target posterior distribution is monitored with the analysis of variance by gelman and rubin 1992 approximate convergence is declared when the variance between the different chains is lower than the variance within each single chain gilks et al 1995 for purposes of bayesian model selection we estimate the evidence with the gaussian mixture importance sampling approach recently developed by volpi et al 2017 this approach allows for four different sampling methods reciprocal importance sampling importance sampling and bridge sampling with geometric and optimal bridge following brunetti et al 2017 we rely on importance sampling from a gaussian mixture model that is fitted to the estimated posterior probability density function 2 2 mc and mcmc sampling of petrophysical prediction uncertainty as mentioned in section 1 in the rare cases when petrophysical prediction uncertainty is included in coupled inversion it is sampled through the brute force monte carlo mc method hammersley and handscomb 1964 while the inference of model parameters of interest is achieved through mcmc this method draws independent samples from the multivariate prior distribution of petrophysical prediction uncertainty and we refer to it as mc within mcmc in section 3 1 we will demonstrate that the mc within mcmc method turns out to be very inefficient because of acceptance rates that are prohibitively low as an alternative we make use of the dream zs proposal mechanism see details in laloy and vrugt 2012 vrugt 2016 to infer the petrophysical prediction uncertainty together with the other parameters by mcmc full mcmc in essence this implies that petrophysical prediction uncertainty is parameterized and treated in the same way as the other unknowns that are inferred in the mcmc inversion both the mc within mcmc and the full mcmc approaches should converge to the same result an alternative to such explicit treatments of petrophysical prediction uncertainty as nuisance parameters is to incorporate their effects in the likelihood function however efficient and theoretically consistent ways to achieve this for non linear problems remains an open research question see section 5 2 in linde et al 2017 2 3 petrophysical relationships and geophysical forward model we consider synthetic test cases for known and theoretically based petrophysical relationships for which petrophysical prediction uncertainty is comparatively low for the field study we consider an unknown empirically based and comparatively weak petrophysical relationship the synthetic example concerns predictions of the porosity field and the field study aims at predicting hydraulic conductivity these two types of problems were chosen to span typical applications as well as different strengths and types of petrophysical relationships the synthetic examples section 3 used in this study rely on the following petrophysical relationship to link gpr velocities v m s to porosities φ 5 v φ m c 2 ɛ w φ m 1 ɛ s 1 where ɛ w 81 and c 3 10 8 m s are the relative permittivity of water and the speed of light in vacuum respectively we assume the relative permittivity of the mineral grains εs equal to 5 and the cementation index m equal to 1 5 in order to incorporate the petrophysical prediction uncertainty eq 5 is computed in three steps the effective relative permittivities ε are first found for a given porosity model pride 1994 6 step 1 ɛ ɛ s φ m ɛ w φ m ɛ s then the petrophysical prediction errors δp describing the residual for each model cell are added 7 step 2 ɛ ɛ δ p and the corresponding gpr velocities are derived 8 step 3 v c 2 ɛ 1 in the context of the field study section 4 at the south oyster bacterial transport site we compare linear and quadratic petrophysical relationships to link the gpr velocities v m s to the natural logarithm of the hydraulic conductivities k log k log m h 9 step 1 v a 0 a 1 k or 10 step 1 v a 0 a 1 k a 2 k 2 where a 0 a 1 and a 2 are the polynomial coefficients we then add δp 11 step 2 v v δ p chen et al 2001 and hubbard et al 2001 demonstrate at the south oyster bacterial transport site that the gpr velocities inferred by linear tomographic inversion are correlated to the logarithm of hydraulic conductivities with a correlation coefficient of 0 68 this suggests that the true underlying correlation is equal or stronger than this value however we stress that any relationship between gpr velocity and hydraulic conductivity is site specific and typically weak the spatial model domain of interest covers an area of 7 2 m 7 2 m below the ground surface we consider multi gaussian models of porosity hydraulic conductivity and petrophysical prediction uncertainty over a regular 2d grid of size 180 180 we use the non linear 2d traveltime solver time 2d of podvin and lecomte 1991 to compute first arrival travel times from velocity fields obtained by applying the petrophysical relationships of eqs 5 8 and 9 11 to each porosity or hydraulic conductivity field 2 4 model parameterisation we generally describe the petrophysical prediction uncertainty δp the porosity φ and the log hydraulic conductivity k fields as multi gaussian random fields the only exception is the illustrative synthetic example of section 3 1 in which the φ and δp fields correspond to independent horizontal layers we parameterise our multi gaussian fields using the method by laloy et al 2015 this method generates stationary multi gaussian fields by employing circulant embedding of the covariance matrix to decrease the number of unknowns inferred during the inversion process the dimensionality is reduced by resampling two low dimensional vectors of standard normal random numbers to the original size of the model using the one dimensional fast fourier transform interpolation we refer to laloy et al 2015 for more details in our case we generate each vector with 50 dimensionality reduction dr variables i e 100 instead of 32 400 unknowns which substantially decrease the mcmc computational cost the multi gaussian model is described by the matérn variogram model and associated geostatistical parameters including the mean and the variance the integral scale along the major axis of anisotropy i the anisotropy angle φ the ratio of the integral scales along the minor and major axis of anisotropy r and the shape parameter of the matérn variogram model ν we jointly infer the geostatistical parameters and the dr variables describing the hydrogeological properties i e porosity or hydraulic conductivity with the corresponding parameters and variables characterizing the petrophysical prediction uncertainty 3 synthetic examples 3 1 toy example mc within mcmc versus full mcmc sampling historically see section 2 2 petrophysical prediction uncertainty has been addressed by drawing independent proposals of δp from the prior while parameters of interest have been inferred by mcmc mc within mcmc as an alternative petrophysical prediction uncertainty is here parameterized and inferred as any other parameter in the mcmc inversion full mcmc we consider a toy example to demonstrate the advantage of using an appropriate model proposal distribution to infer the petrophysical prediction uncertainty full mcmc when considering moderately large or large data sets with high signal to noise ratios the set up of this simple synthetic example consists of 10 gpr transmitters and 10 receivers placed at uniform depth intervals on the right and left side of the model domain respectively fig 1 a considering all possible transmitter receiver pairs yields 100 first arrival travel time data the true porosity field is characterized by four layers of equal thickness with values of 0 3 0 45 0 35 and 0 4 starting from the ground surface fig 1a we consider synthetic travel time data that are contaminated with uncorrelated and normally distributed measurement errors with standard deviation σ y equal to 0 5 ns i e typical of crosshole gpr and 2 ns respectively we consider a uniform prior distribution of porosity in the range 0 25 0 50 and the prior distribution of the petrophysical prediction uncertainty δp is gaussian with zero mean and standard deviation of 0 8 chosen according to the experimental study of roth et al 1990 the δp values are added following eq 7 and integrated in the inversion with the mc within mcmc and the full mcmc methods see section 2 2 the latter draws the parameters from the dream zs proposal distribution that gradually update δp we obtain appropriate acceptance rates of 20 with σ y 0 5 ns and 22 with σ y 2 0 ns when considering full mcmc table 1 for mc within mcmc the acceptance ratio is 0 002 when σ y 0 5 ns and 0 31 when σ y 2 0 ns convergence to the target distribution is consequently much faster for full mcmc than for mc within mcmc especially when σ y 0 5 ns i e 5 103 forward simulations needed instead of 9 5 106 table 1 that is the mcmc derived method allows for an almost 2000 fold decrease in sampling time with respect to the mc within mcmc method this ratio grows further when using smaller σ y and more data for the case of σ y 0 5 ns we compare the posterior mean porosity fields and associated standard deviations obtained when ignoring δp fig 1b and e when using the full mcmc fig 1c and f and the mc within mcmc estimated δp fig 1d and g the posterior mean porosity fields obtained in the three cases fig 1b d are very similar and agree very well with the true porosity field shown in fig 1a the incorporation of the petrophysical prediction uncertainty results in a standard deviation fig 1f and g that is ten times higher than for the case without petrophysical prediction uncertainty fig 1e these results suggest that petrophysical prediction uncertainty has a strong effect on the inferred model uncertainty and that the full mcmc approach is much more efficient than mc within mcmc in the following we will only present results obtained by the full mcmc approach and recommend it over mc within mcmc 3 2 the forward problem impact of petrophysical prediction uncertainty for a given study area geological facies and properties change in space e g porosity specific surface area tortuosity such that the optimal parameters describing any petrophysical relationship are likely to vary in space this implies that when relying on the common assumption of a stationary petrophysical relationship i e the parameter values are the same everywhere the petrophysical prediction uncertainty is likely to have a spatially correlated structure at a scale similar to the geological variability in this section we investigate the impact of spatially correlated petrophysical prediction uncertainty on data residuals by considering forward responses obtained with and without spatially correlated petrophysical errors in this section we do not perform any inversion but simply demonstrate the impact of the correlation scale of petrophysical prediction uncertainty we consider 841 synthetic crosshole gpr travel times that are related to the porosity field in fig 2 a the porosity field is described by a multi gaussian field with horizontal anisotropy with φ 90 mean φ 0 39 variance σ φ 2 2 10 4 integral scale i φ 1 5 m integral scales ratio r φ 0 13 and the shape parameter ν φ 0 5 that corresponds to an exponential variogram in the absence of any petrophysical prediction uncertainty we obtain the velocity field by applying eq 5 with known petrophysical parameters after calculating the corresponding forward response section 2 3 we add uncorrelated gaussian observational noise with σ y 0 5 ns which leads to a root mean square error rmse of 0 5 ns for the case of uncorrelated petrophysical prediction errors we apply eqs 6 8 and draw δp realizations from an uncorrelated gaussian distribution with σ δ p 0 8 on the resulting simulated travel time data we add the same observational noise realization this yields a rmse of 0 64 ns fig 2b a comparatively small increase in rmse compared with the previous case we then describe the petrophysical prediction uncertainty with zero mean isotropic r δ p 1 multi gaussian models with σ δ p 0 8 and ν δ p 0 5 to assess the impact of the spatial correlation of the petrophysical prediction uncertainty we draw δp realizations for isotropic multi gaussian distributions with increasing integral scales for the corresponding forward responses we observe a sharp increase of rmse with increasing integral scales fig 2b for example it is higher than 1 20 ns for an integral scale of 1 5 m the rmse reaches a plateau slightly above 1 36 ns when the integral scale approaches the size of the model domain 7 2 m these results suggests that uncorrelated petrophysical prediction uncertainty i e described by a nugget model will have a relatively weak impact on inversion results when considering finely discretized models however we suspect petrophysical prediction uncertainty to be spatially correlated and this correlation increase the effect on the observed data if these effects are ignored in the inversion one would expect negative impacts on the inversion results this is studied in the following section 3 3 the inverse problem impact of assumptions on petrophysical prediction uncertainty in this section we investigate the consequences of making incorrect assumptions about petrophysical prediction uncertainty when inferring posterior distributions and bayesian model selection we consider the same true porosity field fig 2a as in section 3 2 and 841 first arrival gpr travel time data contaminated with uncorrelated and normally distributed measurement errors with standard deviation σ y 0 5 ns in the mcmc inversions we infer multi gaussian porosity fields with horizontal anisotropy and dr φ φ σ φ 2 being unknown parameters drawn from the associated prior distributions listed in table 2 while all the other geostatistical parameters affecting the porosity structure are kept fixed the petrophysical prediction uncertainty if considered is described as a zero mean multi gaussian field with horizontal anisotropy and known geostatistical parameters i e only dr δ p variables are inferred in the inversion see table 2 as before the standard deviation σ δ p was set equal to 0 8 according to the experimental study of roth et al 1990 the addition of dr δ p leads to a decrease in the magnitude of the correlation coefficient from 1 to 0 81 between the true porosity and the true gpr velocity values we consider four cases δp is not present in the data i e it is not used to generate the synthetic data and it is not inferred in the mcmc inversion case 1 δp is inferred but it is not present in the data used for inversion case 2 δp is present in the data but not inferred case 3 δp is present in the data and inferred case 4 cases 1 and 4 represent situations where the assumptions are consistent with the field situation while cases 2 and 3 are based on inconsistent assumptions we suggest that case 3 represent the most common situation in the hydrogeophysics literature i e petrophysical prediction uncertainty exists but it is ignored all cases considered provide accurate estimates of the mean porosity fig 3 a but only the consistent cases case 1 and 4 give significant probability to the actual variance i e sill describing the porosity field fig 3b with as expected case 4 providing less precise estimates i e parameter uncertainty is higher for the inconsistent cases we find for case 2 that the standard deviation of the porosity field is greatly underestimated while it is overestimated in case 3 fig 3b we now consider the resulting mean porosity fields and the standard deviations for the consistent cases for case 1 we find a mean porosity field fig 4 a that is very close to the true field fig 2a the standard deviation is low fig 4e the scatter between the mean model and the true model follows the 1 1 trend line fig 4i and the correlation coefficient is high 0 9 for case 4 we find a slightly less precise mean model fig 4d which is reflected in the standard deviation being twice as large fig 4h nevertheless the corresponding scatter plot fig 4l indicates that there is no bias the scatter falls on the 1 1 trend line and the correlation coefficient is 0 75 we now turn our attention to the inconsistent cases when considering case 2 we find a less variable mean field fig 4b and standard deviations that are in between the two consistent cases fig 4f the correlation coefficient is high 0 88 but the estimates are biased as they do not follow the 1 1 trend line fig 4j for case 3 we find an overly variable mean field fig 4c rather small standard deviations fig 4g and a moderate correlation coefficient 0 75 with a scatter plot above the 1 1 trend line fig 4k these results suggest different outcomes first including a known petrophysical prediction uncertainty in the inversion leads to consistent estimates but a wider posterior distribution than if petrophysical prediction uncertainty is absent second the correlation coefficient with the true model is mainly determined by the petrophysical prediction uncertainty third the estimated petrophysical prediction uncertainty that does not exist in case 2 accounts for some of the variability due to porosity variations which leads to a too smooth mean porosity field lastly ignoring actual petrophysical prediction uncertainty in the inversion process case 3 the common case leads to overly variable fields in order to accommodate data variability caused by both porosity variations and petrophysical prediction uncertainty from these first inversion examples we conclude that ignoring petrophysical prediction uncertainty leads to overly confident parameter inference and that some of the estimated parameters might be biased we now focus our attention on bayesian model selection for each of the four cases we also use the data to infer porosity fields assuming erroneously a multi gaussian conceptual model with isotropy or vertical anisotropy we compute the evidence for each of these conceptual models the case of the true horizontal anisotropy and the incorrect cases of isotropy and vertical anisotropy by approximating the integral in eq 3 with the gaussian mixture importance sampling estimator section 2 1 for each case we use a total of 105 importance samples and repeat the evidence computation 10 times the mean evidences and associated ranges are presented in fig 5 we find that the ranking of the different conceptual models is the same for all cases as expected the multi gaussian model with horizontal anisotropy true conceptual model has the largest evidence followed by the isotropic model fig 5a the evidence values are the largest when no petrophysical prediction uncertainty is present in the data or in the inversion case 1 fig 5a when we include δp in the inversion the evidence estimates case 2 fig 5a decrease drastically with respect to case 1 for instance we find a 29 orders of magnitude decrease of the evidence estimates for the best model multi gaussian model with horizontal anisotropy when petrophysical prediction uncertainty is absent in the data cases 1 and 2 we find thus that bayesian model selection clearly indicates that the conceptual model with horizontal anisotropy and no petrophysical prediction uncertainty is superior the consistent case note that this is the case despite the fact that we find the highest log likelihoods for case 2 black dotted lines in fig 5b d the addition of 100 unnecessary degrees of freedom in case 2 leads to a much decreased ability to differentiate among the different geostatistical models the error bars of the evidence estimates overlap for case 2 and the bayes factors table 3 are much smaller than for case 1 which imply that it is much more difficult to judge which geostatistical model is preferred statistically we have seen above that the bayesian model selection clearly favours the consistent case 1 when comparing cases 1 and 2 unfortunately this is not the case when comparing cases 3 and 4 the consistent case 4 petrophysical prediction error in data and model parameterization has a much lower evidence fig 5a for the multi gaussian model with horizontal anisotropy and much lower bayes factors table 3 than the inconsistent case 3 petrophysical prediction errors in the data only the reason for this is that case 3 has similar log likelihoods i e data misfit as case 4 fig 5b but half as many model parameters the ability to fit the data so well with this inconsistent model is probably a consequence of the petrophysical prediction uncertainty having the same geostatistical model as the porosity field this implies that formal bayesian model selection will favour a lower dimensional model parameterization that fits the data well regardless of if it is the correct model or not this is a characteristic of bayesian model selection e g schöniger et al 2015b additional tests were performed not shown with conditioning to 17 porosity values along each borehole this decreased the evidence for case 3 somewhat and increased it for case 4 however case 3 was still strongly favored when calculating the corresponding bayes factor 3 4 inference of petrophysical prediction uncertainty we have shown section 3 3 that ignoring petrophysical prediction uncertainty in mcmc inversions leads to over confident parameter estimates and biased estimates of geostatistical properties e g the sill in practical field situations it is difficult to determine a priori the appropriate geostatistical model that governs petrophysical prediction uncertainty in this section we explore to which extent it is possible to infer for both δp and its underlying geostatistical model we consider the same overall setting as in sections 3 2 and 3 3 and the same true porosity field fig 2a here the true petrophysical prediction uncertainty is a zero mean isotropic multi gaussian field with σ δ p 0 8 i δ p 0 8 m r δ p 1 and ν δ p 0 5 we then infer for the mean and variance of the porosity field and for all the geostatistical parameters of δp described above and the corresponding dr δ p variables the corresponding prior distributions of these unknown parameters are listed in tables 2 and 4 the petrophysical relationship used is eq 5 and the petrophysical prediction uncertainty is accounted for following eq 7 the inferred posterior distributions of the mean fig 6 a and variance fig 6b of the porosity field are in general quite well recovered even if they show a slight tendency to underestimate the true values overall the geostatistical properties of the reference petrophysical prediction uncertainty field are captured in the sense that the corresponding true values are included in the posterior distributions fig 6c g however some of the parameters are poorly recovered for instance the inferred standard deviation of δp is centered on the value of 1 instead of 0 8 fig 6c and the inferred shape parameter of the matérn variogram peaks on a value that is half of the corresponding true value fig 6g the anisotropy angle is poorly estimated which is a consequence of the true δp field being isotropic fig 6e the integral scale along the major axis of anisotropy and the ratio of the integral scales peak on the true values but their posterior distributions are relatively wide fig 6d and f the dominant structures in the reference porosity field fig 7 a such as the low porosity zones at a depth of 0 5 m 4 m and 6 m are well represented by the posterior mean porosity field fig 7b the posterior standard deviations on the inferred porosity field span a range between 0 6 and 1 fig 7c we find that the inferred mean petrophysical prediction uncertainty field fig 7d and the true field fig 7e have a rather low correlation coefficient 0 55 the posterior standard deviations of δp span a range between 0 6 and 1 fig 7f these large uncertainties are also reflected in the δp posterior realizations fig 8 that appear to be rather isotropic but with integral scales that vary significantly overall the structural features of the gpr velocity field are well inferred even if their values span a wider range than the reference field fig 7g h in particular the high velocity zone in the bottom right corner of the model domain are enhanced and characterized by large uncertainties fig 7i we performed also a test with the petrophysical prediction uncertainty field conceptualized by a multi gaussian field with anisotropy at 45 not shown for this case we find a significant improvement in the ability to infer for the standard deviation angle of anisotropy and the shape parameter of δp these results suggest that δp is best resolved when its geostatistical properties are markedly different from the underlying porosity field however bayesian model selection between the two conceptual models that include and not include δp in the inversion still favours the case in which petrophysical prediction uncertainty errors are ignored not shown 4 field example 4 1 field site and available data we now focus our attention on field data from the south oyster bacterial transport site in virginia usa hubbard et al 2001 in section 3 we considered a well known and strong petrophysical relationship while here we consider a case of an unknown and only moderately strong petrophysical relationship a pulseekko 100 gpr system with a 100 mhz nominal frequency antenna was used and we consider 841 crosshole gpr first arrival travel time data between 29 transmitter and 29 receiver locations in boreholes s14 and m3 respectively a total of 95 hydraulic conductivity estimates along boreholes s14 t2 and m13 obtained from an electromagnetic flowmeter were used for point conditioning following the methodology outlined by laloy et al 2015 we use the gpr data to infer the underlying log hydraulic conductivity field k assuming a multi gaussian model with horizontal anisotropy its integral scales the anisotropy angle and the shape parameter of the matérn variogram are set based on previous investigations at the site chen et al 2001 hubbard et al 2001 these fixed parameters include i k 1 5 m φ k 90 r k 0 13 and ν k 0 5 the dimensionality reduction variables dr k the mean k and standard deviation σ k of the log hydraulic conductivity field are subject to inference and the corresponding prior ranges are listed in table 5 the prior range on σ k is set to include the 0 42 log m h standard deviation of the available flowmeter data the petrophysical prediction uncertainty is described by a zero mean multi gaussian field with prior distributions outlined in table 5 the upper bound on the prior range of σ δ p is chosen such that the resulting correlation coefficient between gpr velocities and log hydraulic conductivities is equal or stronger than 0 68 which corresponds to the value reported by chen et al 2001 and hubbard et al 2001 we also jointly infer the petrophysical parameters a 0 a 1 and a 2 in eqs 9 and 10 and the standard deviation of the measurement errors σ y table 5 the overall number of parameters subject to inference is 211 4 2 results at the south oyster bacterial transport site in section 3 we considered a synthetic example and a known petrophysical relationship in the present field example we only assume to know the parametric form of the petrophysical relationship and we estimate its petrophysical parameters we infer the underlying log hydraulic conductivity field and compare the results obtained by assuming three different petrophysical models a perfect linear petrophysical relationship eq 9 in which the petrophysical prediction uncertainty is ignored model 1 a linear petrophysical relationship with scatter δp taken into account by following eqs 9 and 11 model 2 and a quadratic petrophysical relationship with scatter δp accounted for as in eqs 10 and 11 model 3 after mcmc inversion we obtain similar posterior distributions of the mean log hydraulic conductivity when using a perfect linear 1 58 log m h and a scattered linear 1 57 log m h petrophysical relationship and a slightly lower value 1 68 log m h when using a scattered quadratic petrophysical relationship fig 9 a when ignoring δp the inferred standard deviation of the log hydraulic conductivity field peaks close to the upper bound black line fig 9b when using a scattered linear or quadratic petrophysical relationship the inferred posterior distribution of the standard deviation is truncated on the lower bound of the prior range green and blue lines fig 9b the highest inferred standard deviation of the measurement errors 0 56 ns is obtained when ignoring δp in the inversion black line fig 9c when considering the scattered linear or quadratic petrophysical relationship the corresponding estimates are 0 37 ns and 0 36 ns respectively fig 9c the parameters describing the three petrophysical relationships are well defined fig 9d f the inferred standard deviation of the petrophysical prediction uncertainty peak on the upper bound of the prior range fig 9g the other geostatistical parameters describing the δp field have similar posterior distributions regardless of if a linear green lines or a quadratic blue lines petrophysical relationship is used fig 9h k in particular we find that the petrophysical prediction uncertainty field is characterized by an integral scale along the major axis of anisotropy centered around 2 4 m fig 9h an almost horizontal anisotropy fig 9i and a ratio of the integral scales of 0 30 fig 9j the posterior distribution of the matérn shape parameter is truncated by the upper bound thereby suggesting a smooth field fig 9k in fig 10 a c we display the mean posterior hydraulic conductivity fields in linear scale the three fields show similar values close to the boreholes where flowmeter data are available but away from these locations the different petrophysical models lead to different subsurface structures and estimates e g within the first meter below the ground surface and between borehole t2 and m3 fig 10a c nevertheless all the three hydraulic conductivity mean models depict a low hydraulic conductivity zone at a depth of 1 2 m b s l and at 5 6 m b s l fig 10a c when the petrophysical prediction uncertainty is ignored the inferred hydraulic conductivity fig 10a and gpr velocity fig 10g fields are characterized by a high variability on average the standard deviations of the posterior hydraulic conductivity estimates are higher when petrophysical prediction uncertainty is accounted for fig 10d f we observe similarities between the corresponding posterior gpr mean velocities fig 10g i for instance they all show a low velocity zone within the first 2 m b s l at 3 m b s l and at 5 6 m b s l and a high velocity zone at 4 5 m b s l as expected the inferred velocity fields derived from scattered petrophysical relationships fig 10h and i are smoother than the case in which this uncertainty is ignored fig 10g the mean posterior fields of the petrophysical prediction uncertainty distributions fig 10k and l are very similar and correlated with the posterior velocity means the red lines in fig 11 a c depict the inferred mean petrophysical relationships and the scatter black dots around them represents the inferred mean petrophysical prediction uncertainty the gpr velocity range appears to be overestimated whether δ p is ignored fig 11a or accounted for together with a quadratic petrophysical model fig 11c while a scattered linear petrophysical relationship fig 11b provides a velocity range in agreement with previous studies brunetti et al 2017 chen et al 2001 hubbard et al 2001 linde et al 2008 linde and vrugt 2013 we now turn our attention to the bayesian model selection results we find that model 2 scattered linear relationship has the largest evidence value 260 20 in log10 units and model 1 δp are ignored has the lowest one 361 00 fig 12 the bayes factor for the best petrophysical model model 2 with respect to model 1 and model 3 is 10100 80 and 109 38 respectively these results confirm that the perfect petrophysical model model 1 is erroneous furthermore the results suggest that the use of a more complex petrophysical relationship is not necessarily favored even if predictions based on the quadratic petrophysical model model 3 fits the data slightly better than the linear petrophysical model model 2 fig 9c the highest evidence is found for model 2 this is a consequence of the trade off between parsimony and goodness of fit typical of the occam s razor principle on which bayesian model selection is based 5 discussion our coupled bayesian hydrogeophysical inversion approach with explicit inference of spatially correlated petrophysical prediction uncertainty leads to less bias e g in the inferred variance of the inferred hydrogeological property field more realistic uncertainty quantification and less over confident model selection compared to the common choice of ignoring this type of uncertainty even if our approach to infer petrophysical prediction uncertainty doubles the number of parameters in the inversion problem we observe dramatic gains in sampling efficiency compared to mc within mcmc e g bosch 1999 2016 moreover dream zs allows for parallel evaluation of the different markov chains and therefore enables feasible computational times even in high e g in our case more than 200 model dimensions our synthetic and field based case studies suggest that it is not always possible to independently constrain hydrogeological and petrophysical properties this trade off is particularly acute when the petrophysical prediction errors have similar geostatistical properties e g orientations and integral scales as the hydrogeological property field of interest fig 7 a manifestation of this trade off is given by the field application at the south oyster bacterial transport site for which it was necessary to constrain the standard deviation of petrophysical prediction uncertainty and the standard deviation of the logarithm of hydraulic conductivity without such constraints the inversion yields largely uncorrelated log hydraulic conductivity and gpr velocity fields results that are inconsistent with previous studies chen et al 2001 hubbard et al 2001 linde et al 2008 this suggests that a careful petrophysical analysis involving borehole data or literature reviews are needed to define constraining prior information when performing coupled hydrogeophysical inversion of field data in a previous study on bayesian hydrogeophysical inversion model selection that ignored petrophysical prediction uncertainty brunetti et al 2017 it was found that the typically large data sets encountered in geophysics and the assumption of small uncorrelated data errors gaussian likelihood lead to very strong confidence in the ability of geophysical data to discriminate between alternative conceptual hydrogeological models by including spatially correlated petrophysical prediction uncertainty we find for a synthetic example fig 5 that the magnitude of the bayes factor of the best conceptual model relative to the worse one decreases by 63 orders of magnitude nevertheless the comparison between case 3 petrophysical prediction errors ignored and case 4 petrophysical prediction errors accounted for in fig 5a and table 3 still indicates high bayes factors and a practically speaking unique ability of geophysical data to find the most appropriate conceptual hydrogeological model among a set of candidates in the future one should also account for the effect of modeling errors i e the discrepancy between actual physical responses and those simulated with simplified physics here a ray based approximation in the present study instead of a full solution of the maxwell s equations a number of promising approaches to address modeling errors are available brynjarsdóttir and o hagan 2014 hansen et al 2014 xu and valocchi 2015 accounting for modeling errors is an essential next step to achieve reliable bayesian hydrogeophysical model selection we anticipate that this will further decrease the range of bayes factors bayesian model selection at the south oyster bacterial transport site section 4 demonstrates clearly that the relationship between log hydraulic conductivity and gpr velocity is not a perfect relationship that is the petrophysical model with a scattered linear relationship has a much higher evidence than results obtained by assuming a perfect linear relationship however contrasting results were obtained in the synthetic example of section 3 3 that did not involve any hydrogeological point measurements in that case formal bayesian model selection erroneously favored a conceptual model that ignored petrophysical prediction uncertainty this happens because this conceptual model has fewer parameters and is still able to fit the data well albeit with a porosity model with biased variance at the south oyster bacterial transport site we condition all model proposals to point data flowmeter estimates of hydraulic conductivity and it is then not possible to propose a biased model close to the boreholes hence the scattered petrophysical relationship is preferred however even if the inclusion of point conditioning in the synthetic example not shown decreased the bayes factor the model selection still favored the wrong conceptual model in the synthetic example we considered boreholes at the left and right sides of the model domain and the relative petrophysical prediction uncertainty was much smaller than for the field example this could explain why the inconsistency between point data and gpr data is more evident for the field example which led the bayesian model selection to favour a model with petrophysical prediction uncertainty these findings suggest that mcmc inversion and model selection is not always able to identify the right model and that their outputs need to be treated with some caution the more prior information that is available e g on petrophysical prediction uncertainty in terms of variance and correlation scale the more reliable are the results indeed bayesian model selection is built on the principle of occam s razor and a problem specific and conceptual model specific level of informative data is needed to overcome this tendency to favour a simpler but erroneous conceptual model e g schöniger et al 2015a in this study we have made the choice to infer for petrophysical prediction uncertainty instead of accounting for its effects in the likelihood function for linear theory it is indeed possible to propagate the impact of multi gaussian petrophysical errors and add the corresponding covariance matrices to the data covariance matrix bosch 2004 2016 bosch et al 2009 chen and dickens 2009 this is not possible for non linear theory as the resulting impact of petrophysical uncertainty on the data leads to model dependent non gaussian distributions the corresponding problem formulation and ways to address this problem was recently discussed by linde et al 2017 in their section 5 2 in the future it would be interesting to compare these two approaches i e inferring for petrophysical uncertainty this study or accounting for the effect of petrophysical uncertainty in the likelihood function 6 conclusions we have demonstrated the importance of accounting for petrophysical prediction uncertainty in coupled hydrogeophysical inversion and highlighted the critical role played by its spatial correlation as mcmc inversions are primarily performed to enable accurate uncertainty quantification we suggest that petrophysical prediction uncertainty should be accounted for in future hydrogeophysical studies in this work we parameterize the petrophysical prediction uncertainty as a multi gaussian field that is inferred together with hydrogeological target properties to decrease model dimensionality future work should also focus on developing computationally efficient and accurate approaches to account for this uncertainty in the likelihood function inferring petrophysical prediction uncertainty with mcmc leads to dramatic performance gains compared to previous work in which it has been accounted for by monte carlo sampling in our examples we show that ignoring petrophysical prediction uncertainty and above all its spatial correlation causes bias in the inferred variance of the hydrogeological properties which implies overly variable fields accounting for this error source allows for consistent hydrogeological estimates and widens the estimated posterior distributions however the geostatistical model describing petrophysical prediction uncertainty is only partially recoverable by the inversion when performing bayesian model selection accounting for petrophysical prediction uncertainty reduces overconfidence in the ability of geophysical data to discriminate between conceptual hydrogeological models of the subsurface when considering geophysical data alone there is a risk that bayesian hydrogeophysical model selection will favour a model parameterization that ignores petrophysical prediction uncertainty provided that the resulting overly variable hydrogeological estimates can explain the geophysical data well this highlights the importance of including constraining prior information about petrophysical prediction uncertainty and the value of combining geophysical and hydrogeological data in the inversion acknowledgments this work was supported by the swiss national science foundation under grant number 200021 155924 we thank eric laloy for making the code available for building the multi gaussian model we thank jasper vrugt for making dream zs available and for providing an initial version of the code used for gaussian mixture importance sampling we are grateful to john peterson and susan hubbard who provided the crosshole gpr and flowmeter data from the south oyster bacterial transport site we also thank the editor paolo d odorico the associated editor harrie jan hendricks franssen anneli guthke and two anonymous reviewers for their constructive and insightful comments 
908,although high performance computers and advanced numerical methods have made the application of fully integrated surface and subsurface flow and transport models such as hydrogeosphere common place run times for large complex basin models can still be on the order of days to weeks thus limiting the usefulness of traditional workhorse algorithms for uncertainty quantification uq such as latin hypercube simulation lhs or monte carlo simulation mcs which generally require thousands of simulations to achieve an acceptable level of accuracy in this paper we investigate non intrusive polynomial chaos for uncertainty quantification which in contrast to random sampling methods e g lhs and mcs represents a model response of interest as a weighted sum of polynomials over the random inputs once a chaos expansion has been constructed approximating the mean covariance probability density function cumulative distribution function and other common statistics as well as local and global sensitivity measures is straightforward and computationally inexpensive thus making pce an attractive uq method for hydrologic models with long run times our polynomial chaos implementation was validated through comparison with analytical solutions as well as solutions obtained via lhs for simple numerical problems it was then used to quantify parametric uncertainty in a series of numerical problems with increasing complexity including a two dimensional fully saturated steady flow and transient transport problem with six uncertain parameters and one quantity of interest a one dimensional variably saturated column test involving transient flow and transport four uncertain parameters and two quantities of interest at 101 spatial locations and five different times each 1010 total and a three dimensional fully integrated surface and subsurface flow and transport problem for a small test catchment involving seven uncertain parameters and three quantities of interest at 241 different times each numerical experiments show that polynomial chaos is an effective and robust method for quantifying uncertainty in fully integrated hydrologic simulations which provides a rich set of features and is computationally efficient our approach has the potential for significant speedup over existing sampling based methods when the number of uncertain model parameters is modest 20 to our knowledge this is the first implementation of the algorithm in a comprehensive fully integrated physically based three dimensional hydrosystem model keywords hydrogeosphere integrated hydrologic modeling polynomial chaos expansion uncertainty quantification 1 introduction as a result of today s high performance computers sophisticated mathematical models are capable of incorporating many complex processes as is often the case the partial or incomplete knowledge of these processes and the input parameters required to describe them necessitates the analyst to make various assumptions and approximations and in doing so introduces uncertainty into the model the aim of uncertainty quantification is to estimate the variability in model responses propagated by model uncertainty consequently uncertainty quantification provides the modeler with a certain level of confidence regarding the predictions made by their model among the many different types of uncertainty present in a mathematical model we consider parametric uncertainty that is the uncertainty in model responses that arises from uncertainty in model input parameters in practice monte carlo simulation mcs metropolis and ulam 1949 or latin hypercube simulation lhs iman et al 1981a 1981b are the workhorse algorithms for uncertainty quantification these algorithms are robust and scale well to higher dimensions however they are slow to converge generally requiring thousands of simulations to obtain an acceptable level of accuracy in contrast to random sampling methods a polynomial chaos expansion represents a model response of interest as a weighted sum of polynomials over the random inputs a non intrusive formulation treats the underlying deterministic model as a black box and consequently does not require any modification of the deterministic model depending on the nature of the problem at hand polynomial chaos has the potential for significant speedup over more traditional random sampling methods moreover once an expansion has been constructed approximating the mean covariance probability density function cumulative distribution function and other common statistics as well as local and global sensitivity measures is straightforward and computationally inexpensive polynomial chaos has its roots in the homogeneous chaos introduced by wiener 1938 which uses hermite polynomials to model random processes involving normal random variables ghanem and spanos 1991 pioneered the very successful stochastic finite elements which combines a hermite polynomial basis with a finite element approach as a method to analyze uncertainty in solid mechanics their method was later generalized by xiu and karniadakis 2002 to the generalized polynomial chaos based on the correspondence between certain probability density functions and weight functions corresponding to orthogonal polynomials from the askey scheme askey and wilson 1985 it should be noted that a similar method referred to as probabilistic collocation was proposed much earlier by tatang et al 1997 and isukapalli et al 1998 however their approach defines the expansion coefficients through the solution of a least squares regression problem instead of via pseudo spectral projection onto an orthogonal polynomial basis a related method that employs a basis of lagrange interpolating polynomials referred to as stochastic collocation was proposed in mathelin and hussaini 2003 and has been further developed in xiu and hesthaven 2005 nobile et al 2008a 2008b and babuška et al 2007 generalizations and extensions of these methods have been proposed in the literature blatman and sudret 2010 foo et al 2008 wan and karniadakis 2005 2006 in this paper we investigate non intrusive polynomial chaos for quantification of uncertainty in fully integrated physics based hydrologic simulations using the hydrogeosphere hgs model aquanty inc 2015 as part of our implementation details we discuss an iterative refinement technique that incrementally improves a chaos expansion through optimal reuse of previous lower order expansions the effectiveness of polynomial chaos as a tool for uncertainty quantification has been demonstrated in many areas including computational fluid dynamics hosder et al 2007 2006 single and multiphase flow in heterogeneous media li and zhang 2007 2009 vehicle dynamics kewlani et al 2012 groundwater modeling deman et al 2016 loosely coupled surface subsurface modeling wu et al 2014 and seawater intrusion modeling rajabi et al 2015 riva et al 2015 however to the best of our knowledge this is the first time that it has been used to quantify the uncertainty in simulations generated by a globally implicit fully integrated surface and variably saturated subsurface flow and solute transport model our polynomial chaos implementation was validated by comparison with analytical solutions as well as solutions obtained via lhs for simple numerical problems it was then used to quantify the parametric uncertainty in a series of numerical problems with increasing complexity including a two dimensional 2d fully saturated steady state flow and transient transport problem with six uncertain parameters and one quantity of interest section 7 1 a one dimensional 1d variably saturated column test involving transient flow and transport with four uncertain parameters and two quantities of interest at 101 spatial locations and five different times each 1010 total section 7 2 and a three dimensional 3d fully integrated surface and subsurface flow and transport problem for a small catchment with seven uncertain parameters and three quantities of interest at 241 different times each section 7 3 the main contributions of this manuscript include the development of a model independent robust and user friendly uncertainty quantification code based on polynomial chaos for application to hydrologic simulations in particular we present a novel iterative refinement procedure for incrementally improving regression based polynomial chaos expansions through optimal reuse of previous lower order expansions the novel application of our code to a highly nonlinear fully integrated surface water groundwater problem in particular we demonstrate that pce can be applied to this complex model problem to efficiently compute time varying global sensitivity indices that provide insight into the model behavior as well as the physical system as a whole the remainder of this paper is organized as follows sections 2 and 3 introduce the notation and setup the mathematical framework in section 4 we provide a thorough overview of polynomial chaos including a brief discussion of the computation of statistics and sensitivity indices from an expansion section 5 discusses the details of the implementation and in particular highlights the iterative refinement procedure section 6 describes the hydrogeosphere model and section 7 describes the numerical testing and provides a discussion of the results concluding statements are presented in section 8 2 notation throughout this paper we adhere to the following notational conventions uppercase and lowercase letters denote scalar quantities and in some cases may be used to denote sets functions or operators bold lowercase letters always denote vectors e g x and bold uppercase letters always denote matrices e g a uppercase script letters always denote sets or spaces e g a d the set of nonnegative integers 0 1 2 is denoted by n the set of strictly positive integers 1 2 3 is denoted by n 1 and the set of real numbers is denoted by r the set of all n vectors with elements in some set b for some positive integer n is denoted by b n we use u a b to denote the uniform probability distribution on the interval a b logn μ σ to denote the lognormal distribution with location parameter μ and scale parameter σ and logu a b to denote the loguniform distribution on the interval a b we note that a random variable x logu a b if and only if ln x u ln a ln b 3 mathematical framework consider a mathematical model that depends on a finite collection of parameters ξ ξ 1 ξ d and let u x t be any real valued response of this model where x r n is the position variable and t 0 is the time variable suppose that the model parameters ξ are uncertain and that we would like to quantify the resulting uncertainty in u we refer to ξ as the parameters of interest and to u as a quantity of interest note that u may also depend on additional certain parameters that are fixed and are not considered by our analysis a natural way to approach the mathematical formulation of this problem is to adopt a probabilistic framework for the uncertain parameters treating them as random variables and recasting the deterministic function u as a function of these random variables in doing so the uncertainty in u may then be rigorously quantified through statistical measures such as its mean and variance the random variables ξ 1 ξ d are modeled as a d variate random vector ξ ξ 1 ξ d in a properly defined probability space ω f p where ω is the sample space f is the event space p is the probability measure and ξ ω r d we make the assumption that ξ has independent components we note that as discussed in eldred and burkardt 2009 this assumption is not absolutely necessary in theory correlated random variables may be transformed to uncorrelated standard random variables in which the independence assumption is valid e g standard normal random variables we regard u u x t ξ as a random process in which u x t ξ ω ω r is a random variable for each ordered pair x t we assume that ξ consists entirely of continuous random variables although this framework works equally well for discrete variables let p i z p ξ i z denote the probability density function pdf of each random variable ξi with support on the interval a i ξ i ω r for i 1 d furthermore let f i z p ξ i z denote the cumulative distribution function cdf of ξi with support on a i for i 1 d since ξ has independent components its joint pdf is defined in terms of the marginal pdfs by 1 p z p 1 z 1 p d z d which has support on 2 a a 1 a d r d let l 2 ω p denote the hilbert space of continuous real valued random variables on ω f p that have finite second moments with respect to the inner product l 2 defined by 3 x y l 2 ω x ω y ω p d ω we assume that u x t ξ ω belongs to l 2 ω p for each ordered pair x t and note that its expected value is defined by 4 e u ω u x t ξ ω p d ω a u x t z p z d z where the latter equivalent expression is more convenient for computation naturally higher order moments of u such as its variance may be defined analogously 4 polynomial chaos expansion polynomial chaos pc employs a basis of multivariate orthogonal polynomials to capture the functional relationship between a model response u and the random parameters ξ this relationship may be expressed by an infinite sum referred to as a polynomial chaos expansion pce of the form 5 u x t ξ i 1 c i x t φ i ξ where ci are the real valued expansion coefficients and φ i a r i n 1 is a sequence of orthogonal multivariate polynomials referred to as basis polynomials naturally in practice one must truncate the infinite sum to a finite number of terms and so we shall consider truncated polynomial chaos expansions of the form 6 u x t ξ u n x t ξ i 1 n c i x t φ i ξ in this work we consider only non intrusive pc formulations in which the underlying deterministic model is treated as a black box intrusive pc formulations such as the stochastic galerkin method ghanem and spanos 1991 require access to the governing equations of the underlying model and are not applicable in our case due to the complex nature of sophisticated 3d simulations it is evident from 6 that any polynomial chaos expansion consists of three parts the basis polynomials the truncation scheme or basis type and the expansion coefficients in the following sections we discuss how each of these components may be defined in addition we discuss how various statistics and sensitivity measures may be computed from a truncated expansion 4 1 basis polynomials the polynomial chaos basis polynomials φi are constructed as products of univariate orthogonal polynomials the optimal basis polynomials are obtained when the univariate orthogonal polynomials in dimension j are orthogonal with respect to the pdf of the random parameter ξj ideally each random parameter corresponds via its pdf to a family of orthogonal polynomials from the askey scheme of hypergeometric orthogonal polynomials askey and wilson 1985 table 1 presents some families of orthogonal polynomials that correspond to classical weight functions from the askey scheme it is important to note that each orthogonal polynomial corresponds to a probability density function in standard form up to scaling e g the hermite polynomials used by wiener in the homogeneous chaos correspond to the standard normal distribution if this is not the case then one may either generate the orthogonal polynomials numerically gautschi 1982 1994 or employ a nonlinear variable transformation to the most similar askey type for example a loguniform random variable could be transformed to a uniform random variable prior to constructing the pce for technical reasons related to the convergence of the polynomial chaos expansion see section 4 4 we generally prefer the variable transformation approach let φ k j k 0 denote the univariate orthogonal polynomials with respect to pj z for j 1 d we introduce a d dimensional multi index as a d variate vector i i 1 i 1 i d in which each component is a nonnegative integer i n d observe that there exists a one to one correspondence γ n d n 1 between the set of all possible multi indexes and the set of strictly positive integers thus to each expansion index i we may assign a unique expansion multi index i such that γ i i the ith basis polynomial φi ξ is then defined by 7 φ i ξ φ γ i ξ φ i 1 1 ξ 1 φ i 2 2 ξ 2 φ i d d ξ d owing to the product form of the joint probability density function 1 the basis polynomials satisfy the orthogonality condition 8 φ k ξ φ ℓ ξ l 2 0 k ℓ 0 k ℓ as discussed in section 4 4 under certain conditions on the random parameters ξ 1 ξ d and their cdfs the orthogonal polynomials φ i ξ i n 1 form a complete orthogonal basis of the hilbert space l 2 ω p in which the model response u x t ξ lives thus u can be expressed as an infinite weighted sum of these polynomials we note that if the constituent univariate polynomials are orthonormal then φ i ξ i n 1 forms a complete orthonormal basis of l 2 ω p exactly which basis polynomials are included in the truncated expansion depends on the truncation scheme which is discussed in section 4 2 4 2 expansion truncation in general a polynomial chaos expansion may be truncated to any positive number of terms n involving any combination of basis polynomials among the many possible truncation schemes the total order expansion is most commonly cited in the literature a total order pce employs a complete basis of polynomials up to a fixed total order specification that is the degree of the multivariate basis polynomials is restricted to not exceed a fixed integer p 0 in terms of the expansion multi index i n d all indices that satisfy the inequality i 1 i 1 i d p are kept for example the basis polynomials of a two dimensional second order expansion d 2 p 2 are φ 1 ξ φ 0 1 ξ 1 φ 0 2 ξ 2 φ 2 ξ φ 1 1 ξ 1 φ 0 2 ξ 2 φ 3 ξ φ 0 1 ξ 1 φ 1 2 ξ 2 φ 4 ξ φ 0 1 ξ 1 φ 2 2 ξ 2 φ 5 ξ φ 1 1 ξ 1 φ 1 2 ξ 2 φ 6 ξ φ 2 1 ξ 1 φ 0 2 ξ 2 the number of terms n in an expansion of total order p and dimension d is given by n k 0 p d k 1 k d p d p compared with an isotropic tensor product expansion of order p in which each element of the expansion multi index i ranges over 0 p and hence n 1 p d a total order expansion offers good monomial coverage with slower growth in n 4 3 expansion coefficients different flavors of polynomial chaos are due in part to the variety of ways in which the expansion coefficients ci may be defined we discuss two of the major approaches found in the literature namely pseudo spectral and linear regression pc 4 3 1 pseudo spectral polynomial chaos mathematically the expansion coefficients ci are defined through the projection of u x t ξ onto the polynomial basis φ i ξ i n 1 assuming for simplicity that the basis polynomials are orthonormal the expansion coefficients are given by 9 c i x t a φ i z u x t z p z d z for i 1 2 and the chaos expansion 5 forms a generalized fourier series of u unfortunately since u is known only through the solution of a complex numerical model these integrals in general cannot be evaluated analytically in pseudo spectral polynomial chaos the expansion coefficients are computed through numerical approximation of the integrals 9 among the available approximation methods which include monte carlo simulation quadrature and cubature we briefly describe the quadrature based method in more detail let q m f j 1 m a j f ξ j a j r ξ j a be an m point quadrature formula for d dimensional integrals of the form i f a f z p z d z then the expansion coefficients may be approximated by 10 c i x t q m φ i ξ u x t ξ j 1 m a j φ i ξ j u x t ξ j for i 1 2 if the expansion has been truncated to n terms then we arrive at the linear system 11 c φ t a u that defines the coefficients where 12 φ φ 1 ξ 1 φ n ξ 1 φ 1 ξ m φ n ξ m r m n a diag a 1 a m r m m and c u are m vectors of the approximate coefficients and model responses at the points ξ j respectively the standard approach to defining d dimensional quadrature rules is to construct them from 1d quadrature rules analogous to how the d variate orthogonal polynomials are constructed from univariate orthogonal polynomials due to their high precision and close relationship with orthogonal polynomials gauss quadrature rules gautschi 1968 golub and welsch 1969 are typical candidates for the 1d rules the simplest approach to forming a d dimensional rule is to combine the 1d rules through a tensor product construction however this approach suffers from the curse of dimensionality and is only practical when the dimension d is small a more successful approach that aims to lessen the curse of dimensionality while at the same time preserving a high level of accuracy is based on smolyak sparse grid rules smolyak 1963 smolyak sparse grid rules are nothing more than linear combinations of tensor product rules with the key distinction that only products with relatively few points are used their application to pce has been thoroughly investigated in the literature constantine et al 2012 eldred and burkardt 2009 nobile et al 2008a 2008b the real power of smolyak sparse grid rules is realized when the constituent 1d quadrature rules are nested since this greatly reduces the number of nodes in the resulting multidimensional rule nesting has the added benefit of allowing for significant point reuse as the order of the sparse grid rule increases 4 3 2 linear regression polynomial chaos suppose the chaos expansion has been truncated to n terms in the linear regression approach the coefficients c 1 c n are defined by minimizing 13 u n x t ξ j u x t ξ j for j 1 m in the l 2 sense for some set of points e ξ 1 ξ m a in terms of matrices and vectors we have 14 c arg min v r n u φ v 2 where u c and φ are defined in section 4 3 1 the points in e at which the response values are computed are referred to as collocation points thus the linear regression approach seeks the coefficients that best reproduce the response values over a set of collocation points naturally the approximation quality of the polynomial chaos expansion depends strongly on the distribution of collocation points in a a number of methods are possible for defining the collocation point set including point collocation the collocation points are defined on an unstructured grid obtained from random sampling within the pdf p z hosder et al 2007 probabilistic collocation the collocation points are defined as a subset of the tensor product grid constructed from p 1 point gauss quadrature rules isukapalli et al 1998 tatang et al 1997 tensor product grid collocation the collocation points are defined via a tensor product grid constructed from gauss quadrature rules sparse grid collocation the collocation points are defined via a smolyak sparse grid constructed from nested gauss quadrature rules the linear system defining the least squares problem in the linear regression approach may have any shape however it is commonly overdetermined that is m n as discussed by isukapalli et al 1998 an over sampling with m 2n collocation points is recommended to obtain a high quality pce the solution of 14 in the overdetermined case is given by 15 c φ u where φ is the moore penrose pseudoinverse of φ since the least squares problem may be ill conditioned its solution is computed via the singular value decomposition golub and van loan 1996 we note that it is only necessary to compute the singular value decomposition of φ once after which it can be stored in factorized form for future use if φ has full column rank then 15 is the unique minimal solution otherwise there exist infinitely many minimal solutions as discussed by sudret 2008 in order to obtain an accurate polynomial chaos approximation the least squares problem 14 should have a unique solution it is interesting to observe that under certain assumptions the pseudo spectral and linear regression approaches are equivalent in the following sense suppose that the maximum degree of any basis polynomial φi in the truncated expansion is p and that φ and a in 11 are constructed from a multidimensional quadrature rule with positive weights and polynomial precision at least 2p these assumptions are met for example by a total order expansion of order p that employs p 1 point gauss quadrature rules to construct a tensor product quadrature rule without restricting generality assume that the basis polynomials are orthonormal then φ t a φ i j a φ i z φ j z p z d z δ i j for all 1 i j n letting a 1 2 denote the square root of matrix a it follows that a 1 2 φ has orthonormal columns and left inverse φ t a 1 2 therefore the coefficient vector c φ t a u defined by pseudo spectral projection is the unique solution of the weighted least squares problem a 1 2 φ v a 1 2 u up to diagonal scaling by a 1 2 this least squares problem is identical to that which is solved by the linear regression approach this simple analysis further reveals that under the given assumptions the matrix φ has full column rank as desired it is worth noting that this equivalency may fail when using sparse grid quadrature rules since negative weights are possible even when the underlying 1d quadrature rules are positive however it remains true that φ has full column rank 4 4 convergence let u be a random variable belonging to l 2 ω p and let un denote its polynomial chaos expansion truncated to n terms in which the expansion coefficients are defined through spectral projection 9 suppose that each random variable ξ 1 ξ d possesses finite moments of all orders and that the cdf fj is uniquely defined by its moments for j 1 d then according to ernst et al 2012 the polynomials φ i ξ i n 1 form a complete orthogonal basis of l 2 ω p and un u as n in the mean square sense this result further implies that un converges to u in both probability and distribution and in particular the mean and variance of un converges to the mean and variance of u respectively however it should be noted that depending on the probability distributions involved higher order moments of un may fail to converge to the corresponding moments of u as demonstrated by field jr and grigoriu 2007 probability distributions that meet the assumptions given above include but are not limited to the normal uniform loguniform triangular gamma and beta distributions we note that since the lognormal distribution is not uniquely defined by its moments a polynomial chaos expansion in which at least one random parameter ξj follows a lognormal distribution may not converge to the correct quantity it is possible to remedy this issue via a variable transformation to a standard normal random variable however variable transformations should be used sparingly and only in certain circumstances because the additional nonlinearity introduced by the transformation may negatively affect the convergence of un the convergence rate of un depends strongly on the smoothness of u x t ξ with respect to the uncertain parameters ξ in particular if u or any of its derivatives are discontinuous or if steep gradients are present the rate of convergence may be diminished perhaps severely thus depending of the regularity of u it may be necessary to use a high order expansion necessitating many model simulations to obtain reasonable accuracy the poor convergence behavior of un in this case may be attributed in part to the fact that a global polynomial basis is ill suited for representing such functions 4 5 analytic statistics and probability distributions analytic expressions for the mean and covariance of a model response u may be readily computed with minimal expense from its truncated chaos expansion un without restricting generality suppose that the chaos expansion of u employs an orthonormal polynomial basis in which φ1 ξ 1 then by orthonormality of the basis polynomials 16 e u x t e u n x t u n x t ξ φ 1 ξ l 2 i 1 n c i x t φ i ξ φ 1 ξ l 2 c 1 x t by a similar argument we arrive at an expression for the covariance 17 cov u x 1 t 1 u x 2 t 2 cov u n x 1 t 1 u k x 2 t 2 e u n x 1 t 1 u k x 2 t 2 e u n x 1 t 1 e u k x 2 t 2 i 1 n j 1 k c i x 1 t 1 c j x 2 t 2 φ i ξ φ j ξ l 2 c 1 x 1 t 1 c 1 x 2 t 2 i 2 min n k c i x 1 t 1 c i x 2 t 2 it follows immediately from the expression for the covariance that 18 var u x t var u n x t i 2 n c i x t 2 formulas for the third and higher moments may also be derived however they are more difficult to use in practice as they involve multidimensional integrals additionally as u is assumed to be a second order process there is no guarantee that the third and higher moments of un converge to the corresponding moments of u as n the pdf or cdf of u may be approximated from its polynomial chaos expansion by random sampling of the truncated expansion un naturally statistics such as the median interquartile range and percentiles for example may be estimated from the random samples random sampling of un is in general much more efficient then random sampling of u since it involves computing linear combinations of multidimensional polynomials which are relatively cheap compared to simulations of a complex numerical model 4 6 local sensitivity analysis local sensitivity analysis seeks to quantify the change in a model response to local perturbations of the model parameters taken one at a time about a fixed point if the partial derivatives of the model response u with respect to ξ 1 ξ d exist at some point ξ 0 a then the first order local sensitivity coefficients sj may be defined by 19 s j x t u x t ξ ξ j ξ ξ 0 for j 1 d a typical value for ξ 0 is the mean of ξ the sensitivity coefficients sj may be approximated in terms of the polynomial chaos expansion by 20 s j x t u n x t ξ ξ j ξ ξ 0 i 1 n c i x t φ i ξ ξ j ξ ξ 0 for j 1 d since the basis polynomials are the product of univariate orthogonal polynomials 7 it follows that each partial derivative in the formula above reduces to the derivative of a single univariate orthogonal polynomial φ i ξ ξ j d φ i j j ξ j d ξ j k j φ i k k ξ k hence each local sensitivity coefficient may be computed with the same effort as evaluating the chaos expansion at a single point 4 7 global sensitivity analysis global sensitivity analysis seeks to quantify the change in a model response as a result of simultaneously varying subsets of the model parameters over their parameter spaces variance based decomposition is a type of global sensitivity analysis in which the variance of the model response is decomposed into fractions that can be attributed to subsets of the input parameters sobol 1993 it uses two primary measures the main effect index sj and the total effect index tj the main effect index sj measures the contribution to the model response variance arising solely from the jth model parameter ξj on the other hand the total effect index tj measures the contribution to the model response variance arising from the jth model parameter and all its interactions with the other model parameters in addition to the main and total effects indices of higher order interactions involving two or more model parameters may also be defined we use the notation s j 1 j k to denote the interaction between the model parameters ξ j 1 ξ j k where j 1 j k 1 d are distinct and 1 k d in general computing sj tj and higher order indices of a model response u requires the evaluation of multidimensional integrals which may be accomplished by methods based on random sampling saltelli et al 2010 on the other hand as illustrated by sudret 2008 ciriello et al 2013 and formaggia et al 2013 polynomial chaos expansion is particularly amenable to variance based global sensitivity analysis in which case the sensitivity indices can be expressed as analytic functions of the expansion coefficients to define s j 1 j k and tj in terms of the expansion coefficients we assume that the truncated polynomial chaos expansion of u is expressed in the equivalent form 21 u n x t ξ i i c i x t φ i ξ where i is the set of n multi indexes i n d that define the basis polynomials in the expansion now define the index set 22 s j 1 j k i i i i ℓ 0 ℓ j 1 j k i ℓ 0 ℓ 1 d j 1 j k then the sensitivity index s j 1 j k of the model response u may be approximated by 23 s j 1 j k 1 v i s j 1 j k i c i x t 2 where v is the total variance of u computed from 18 because the index set i can always be partitioned into nonempty sets of the form s j 1 j k i it follows that the sum of all such indices is equal to one we note that for the main effect indices sj the index set 22 simplifies to s j i i i i j 0 i k 0 k j the total effect index tj may be approximated from the polynomial chaos expansion by a similar equation 24 t j 1 v i t j i c i x t 2 t j i i i i j 0 there are 2 d 1 sensitivity indices of the form s j 1 j k so even for relatively small dimensions d the number of possibilities is large in practice this does not pose a problem since we are primarily interested in the main effects and total effects which comprise only 2d indices moreover since many of the higher order interaction indices will be near or equal to zero we may truncate any indices that fall below a given threshold 5 polynomial chaos implementation one of the difficulties that arises when using the polynomial chaos expansion for uncertainty quantification is the selection of the expansion order p ideally the expansion order should be large enough to obtain a reasonable level of accuracy but should not result in an undue number of costly simulations unfortunately estimating the optimal expansion order a priori is typically intractable for all but the simplest problems to remedy this issue we have developed an iterative refinement framework that attempts to efficiently construct a sufficiently accurate chaos expansion starting from an initial low order approximation the steps of our algorithm see algorithm 1 are as follows starting from an initial low order expansion with a user specified expansion order typically p 1 to start during each refinement step the expansion order is incremented the chaos expansion is reconstructed requiring new model simulations and a convergence criterion is checked the refinement process attempts to minimize the number of model simulations by reusing the results of previous refinement steps reuse is accomplished through the use of nested quadrature rules within a smolyak sparse grid framework when reuse is optimal iterative refinement can iterate through expansion orders 1 p in roughly the same amount of work that it would take to construct an order p expansion the convergence criteria are based on the relative and absolute changes in the mean and standard deviation from one refinement step to the next and on the magnitude of the leave one out cross validation error loocv additional criteria based on the maximum number of simulations the maximum expansion order and the maximum number of refinement steps may also be imposed as discussed by blatman and sudret 2010 the loocv is a relatively robust error estimate that can be computed efficiently for linear regression polynomial chaos expansions we use the loocv to quantify the predicative accuracy of the polynomial chaos expansion un relative to the underlying deterministic model response u in principle if the loocv is sufficiently small then our estimates of the relative error in the mean and standard deviation on which we base our convergence criterion should be trustworthy our polynomial chaos implementation is assembled together with sampling methods such as monte carlo latin hypercube and quasi monte carlo simulation to provide a comprehensive uncertainty quantification tool due to the non intrusive nature of pce and the sampling methods our tool is effectively model independent and hence is amenable to models other than hgs it is implemented in c and provides failure tracking of model simulations as well as a restart feature probability distributions commonly associated with surface water and groundwater modeling are supported including the normal uniform lognormal loguniform and triangular distributions model simulations which are typically the most computationally demanding aspect of uncertainty quantification have been parallelized with openmp openmp architecture review board 2002 additionally our tool is hosted on algocompute merrick 2017 a web based cloud computing tool that employs the microsoft azure cloud computing platform our polynomial chaos implementation has been validated by comparison with analytical solutions as well as empirical solutions obtained by lhs for a variety of simple model problems and its performance has been measured against lhs for the purposes of this paper we discuss more complex problems which are described in section 7 6 hgs numerical model hydrogeosphere hgs is a fully integrated surface and variably saturated subsurface flow model including contaminant and heat transport evapotranspiration and land surface processes aquanty inc 2015 davison et al 2015 hwang et al 2015 sudicky et al 2008 hgs implements an implicit control volume finite element method and utilizes openmp for its parallelization hwang et al 2014 park et al 2009 the two dimensional surface is draped directly over the three dimensional subsurface domain and is modeled by the depth integrated saint venant equation ϕ 0 h 0 t d 0 k 0 d 0 z 0 d 0 γ 0 q 0 where ϕ 0 is the surface domain porosity h 0 is the hydraulic head m d 0 is the surface water depth m t is time s γ0 is the surface water volumetric exchange between domains s 1 and q 0 is surface domain sources and sinks m s 1 the surface conductance m s 1 k 0 is calculated by the manning chezy or darcy weisbach equations the surface domain is coupled to the subsurface domain by either the common node approach both the surface and subsurface domain share one node or the dual node approach the surface and subsurface are directly linked between domains the variably saturated subsurface domain implements richards equation s w s s ψ t ϕ s t k k r ψ z γ q where s is the water saturation ss is the specific storage m 1 ϕ is the porosity ψ is the pressure head m k is hydraulic conductivity tensor m s 1 z is the elevation m γ is the volumetric water exchange flux between domains s 1 and q represents the sources and sinks s 1 the relative conductivity kr is determined by genuchten 1980 brooks and corey 1964 or through a lookup table contaminant transport in the subsurface is modeled by the three dimensional advection dispersion equation 25 ϕ s c t q c ϕ s d c where c is the solute concentration g m3 q is the flux m s 1 and d is the hydrodynamic dispersion tensor m2 s 1 the two dimensional surface domain implements a modified version of 25 by removing the z component 7 numerical tests our polynomial chaos implementation was tested through a series of numerical problems of increasing complexity both in terms of the physical problem and in the application of our uncertainty quantification tool all test problems were conducted using linear regression polynomial chaos with a total order basis sparse grid collocation and iterative refinement in conjunction with the physics based fully integrated hydrologic model hydrogeosphere aquanty inc 2015 the test problems include a 2d fully saturated steady flow and transient transport problem with six uncertain parameters and one quantity of interest a 1d variably saturated column with transient flow and transient transport four uncertain parameters and two quantities of interest at 101 spatial locations and five times each 1010 total and a 3d fully integrated surface and variably saturated subsurface flow and solute transport problem for a small catchment with seven uncertain parameters and three quantities of interest at 241 times each 7 1 2d fully saturated steady flow with transient transport 7 1 1 problem description this problem simulates the movement of a dissolved conservative contaminant from a deep underground source to a near surface environmental receptor fig 1 the problem consists of a 2d layered bedrock system in which hydraulic conductivity and porosity within each layer are uncertain and decrease with depth parameter ranges and distributions are shown in table 2 the flow system is assumed to be at steady state and is driven by a recharge zone and a discharge zone the quantity of interest is the concentration at the observation point near the discharge zone after one million years 7 1 2 model setup the 2d model domain is unit thickness in the y direction and measures 1500 m in the x direction and 100 m in z direction numerical elements measure 2 m 2 m in the x and z directions and the mesh contains a total of 76 602 nodes no flow boundaries are assigned to all outer boundaries except in the areas labeled as recharge and discharge zones constant head boundaries are assigned in these regions to generate a regional flow field h recharge 100 m and h discharge 99 m the contaminant source is represented by a constant concentration boundary condition of unit concentration the solute is conservative non reactive has a free solution diffusion coefficient of 10 9 m2 s 1 and has longitudinal and transverse dispersivity values of 5 m and 0 5 m respectively 7 1 3 results the mean and standard deviation of the concentration at the observation point after one million years are determined using a pce and lhs the predicted mean and standard deviation are compared against their empirically determined true values which are obtained via 10 000 lhs simulations the relative error of the mean and standard deviation are shown in fig 2 in this case pce converges significantly faster than lhs which suggests that pce has the potential for considerable speedup over more traditional sampling based methods fig 3 shows the pdf in subplot a and the cdf in subplot b of the concentration computed from an order 4 pce 737 simulations compared to the empirical true case 10 000 latin hypercube simulations with more than 13 times fewer model simulations the chaos expansion is able to accurately reproduce the empirically determined pdf and cdf of the concentration we conclude this section by examining how closely the pce constructed through iterative refinement approximates the full hgs model table 3 presents the mean absolute error mae root mean square error rmse nash sutcliffe model efficiency coefficient nse and the percentage bias pbias of the concentration obtained by comparing the order four pce against hgs simulation results at 100 points randomly distributed throughout the parameter space via latin hypercube sampling in addition we present the leave one out cross validation error loocv computed over the set of collocation points 7 2 1d transient flow and transport in a 1d unsaturated column 7 2 1 problem description this problem simulates the constant infiltration of a unit concentration dissolved solute into a 5 m long dry soil column with free drainage outlet at the bottom concentration and water saturation values are observed at 5 cm intervals along the column at five separate times 7 2 2 model setup the numerical model consists of a 5 m long column divided vertically into 500 elements each element is 1 cm thick the elements measure unit thickness in the x and y directions 1 m and contains 2002 nodes and 500 elements as shown in table 4 four parameters are assumed to be uncertain for this problem hydraulic parameters are based on literature values for borden sand sudicky 1986 for hydraulic conductivity and berg and gillham 2010 for mean van genuchten parameters uncertainty was introduced into the van genuchten parameters by perturbing them by approximately 15 25 around their mean values the impact of the uncertainty in the van genuchten alpha and beta on the pressure saturation relationship is shown in fig 4 initially the model is near residual saturation and has a constant flux boundary condition of 8 64 mm day on top additionally the top boundary condition contains a conservative non reactive tracer chloride with a free solution diffusion coefficient of 10 9 m2 s 1 for the duration of the simulation 25 days uncertainty in saturation and concentration is quantified using an order four pce at 101 spatial locations every 5 cm along the column and five different times for each quantity of interest 1010 pce estimates 7 2 3 results spatial and temporal solute concentration and saturation distributions are shown in fig 5 respectively fig 5 a shows the mean solute concentration distribution bounded by one standard deviation as determined by pce at 3 7 15 21 and 25 days fig 5 b shows the mean saturation distribution bounded by one standard deviation as determined by pce at 1 3 7 11 and 13 days 7 3 3d fully integrated surface and variably saturated subsurface flow and transport in a small catchment this demonstration case involves a rainfall runoff field experiment conducted by abdul and gillham 1989 at the borden research site this field experiment has become a standard problem for benchmarking fully integrated surface and subsurface flow models aquanty inc 2015 kollet et al 2017 vanderkwaak 1999 7 3 1 problem description the simulation problem is a fully integrated surface and variably saturated subsurface water flow and solute transport problem the small test catchment measures approximately 80 m long by 15 m wide fig 6 the rainfall runoff experiment consisted of applying precipitation with a conservative bromide tracer to the catchment at a rate of 2 cm h for 50 min outflow rates and bromide concentrations were measured at the outlet of the model as shown in table 5 seven parameters were assumed to be uncertain the uncertainty range in the hydraulic conductivity was based on literature values sudicky 1986 and the ranges for the remaining parameters in table 5 were obtained by a pest doherty 1994 calibration the quantities of interest include water flux out of the catchment bromide mass flux out of the catchment and hydraulic head at a point in the model recorded at 241 output times 7 3 2 model setup the numerical model simulates both surface subsurface flow and transport in a fully integrated manner using hgs aquanty inc 2015 the finite element mesh contains approximately 22 000 nodes and 40 000 elements similar to previous model designs by vanderkwaak 1999 and kollet et al 2017 the subsurface is homogeneous and the surface domain is divided into two zones the main channel and the surrounding area precipitation with a unit concentration of bromide tracer is applied to the upper surface of the model at a rate 2 cm h for 50 min after which the simulation continues for an additional 50 min with no precipitation water and bromide mass flux out of the domain and the hydraulic head at an observation point fig 6 are recorded every 25 s 7 3 3 results temporally varying global sensitivity indices are computed using an order two pce requiring a total of 98 model simulations fig 7 and for comparison an order four pce requiring a total of 1329 model simulations fig 8 in each figure subplots a b and c show the median and interquartile ranges for water flux discharge bromide mass discharge and hydraulic head respectively these statistics were computed from 10 000 random samples of the chaos expansion subplots d e and f show the most significant main effect global sensitivity indices for these same quantities of interest it is encouraging to observe that with less than one tenth the number of model simulations the order two expansion results are nearly identical to the order four expansion results in particular the order two expansion is able to capture the main features of the time varying sensitivity indices as shown in figs 7 and 8 time varying sensitivity coefficients can provide considerable insight into how the most sensitive system parameters may change depending on model state for example during the early portion of the simulation when rain is being applied to the domain hydraulic head is most sensitive to rill storage however when the simulated rain stops hydraulic head becomes most sensitive to porosity 8 conclusions although high performance computers and advanced numerical methods have made the application of fully integrated surface and subsurface flow and transport models such as hydrogeosphere common place run times for large complex basin models can still be on the order of days to weeks traditional workhorse algorithms for uncertainty quantification such as lhs or mcs typically require thousands of simulations to achieve an acceptable level of accuracy thus making their computational cost prohibitive for such applications in this paper we demonstrated the performance gains that are possible by applying polynomial chaos to uncertainty quantification of integrated hydrologic models an added benefit of our approach is that once a chaos expansion has been constructed approximating the mean covariance pdf cdf and other common statistics as well as local and global sensitivity measures is straightforward and computationally inexpensive our numerical experiments show that polynomial chaos is an effective and robust method for quantifying uncertainty in hydrologic simulations that provides a rich set of features and is computationally efficient with the potential for significant speedup over existing sampling methods when the number of uncertain model parameters is modest we note that while the test cases presented in this manuscript employ hydrogeosphere our non intrusive implementation is model independent allowing it to be applied to any deterministic model provided the model inputs and outputs are formatted appropriately in particular the following key points were discussed in this study the development and application of a non intrusive polynomial chaos expansion tool for the quantification of uncertainty in deterministic hydrologic models such as hydrogeosphere in particular our implementation contains an iterative refinement framework that incrementally improves a chaos expansion through the optimal reuse of previous lower order expansions for increased efficiency the pce tool is parallelized with openmp openmp architecture review board 2002 thus allowing many concurrent simulations to be performed on a single desktop computer additionally the pce tool is hosted on algocompute merrick 2017 a web based cloud computing tool that employs the microsoft azure cloud computing platform in the first test case section 7 1 convergence with respect to increasing expansion order hence increasing simulation count of the pce mean and standard deviation was compared to lhs for a simple 2d fully saturated steady state flow and transient transport problem it was observed that pce converged to a relative error of less than 10 2 significantly faster than lhs requiring almost 7 times fewer model simulations furthermore the pdf and cdf obtained from random sampling of the fourth order pce 10 000 samples were shown to be essentially identical to the pdf and cdf obtained from lhs with 10 000 model simulations the second test case section 7 2 which consisted of two quantities of interest at 101 spatially distributed observation locations demonstrated the ability of the pce tool to handle a large number of different observation locations in contrast to the second test case the third test case section 7 3 is highly transiently resolved with 241 output times for three different observation locations this small watershed case demonstrated the capability of pce for time varying global sensitivity analysis which can provide significant insight into the relative parameter importance under changing hydrologic conditions future work will focus primarily on validation involving multiple sources of uncertainty including both surface and subsurface processes the application of our tool to nuclear waste management flood risk analysis precipitation large basin models and other areas of interest is planned further refinement of the sparse grid collocation iterative refinement method is anticipated and adaptive grid refinement guided by global sensitivity analysis is being investigated grid refinement with respect to the deterministic model time is also being investigated acknowledgements the authors would like to thank alberto guadagnini and the two anonymous reviewers whose constructive comments improved the final version of this manuscript funding this research was supported by an industrial r d fellowship awarded to killian miller by the natural sciences and engineering research council of canada nserc grant no 6037 2014 469139 and an industrial talentedge fellowship awarded to jason davison by the ontario centres of excellence oce grant no 26691 
908,although high performance computers and advanced numerical methods have made the application of fully integrated surface and subsurface flow and transport models such as hydrogeosphere common place run times for large complex basin models can still be on the order of days to weeks thus limiting the usefulness of traditional workhorse algorithms for uncertainty quantification uq such as latin hypercube simulation lhs or monte carlo simulation mcs which generally require thousands of simulations to achieve an acceptable level of accuracy in this paper we investigate non intrusive polynomial chaos for uncertainty quantification which in contrast to random sampling methods e g lhs and mcs represents a model response of interest as a weighted sum of polynomials over the random inputs once a chaos expansion has been constructed approximating the mean covariance probability density function cumulative distribution function and other common statistics as well as local and global sensitivity measures is straightforward and computationally inexpensive thus making pce an attractive uq method for hydrologic models with long run times our polynomial chaos implementation was validated through comparison with analytical solutions as well as solutions obtained via lhs for simple numerical problems it was then used to quantify parametric uncertainty in a series of numerical problems with increasing complexity including a two dimensional fully saturated steady flow and transient transport problem with six uncertain parameters and one quantity of interest a one dimensional variably saturated column test involving transient flow and transport four uncertain parameters and two quantities of interest at 101 spatial locations and five different times each 1010 total and a three dimensional fully integrated surface and subsurface flow and transport problem for a small test catchment involving seven uncertain parameters and three quantities of interest at 241 different times each numerical experiments show that polynomial chaos is an effective and robust method for quantifying uncertainty in fully integrated hydrologic simulations which provides a rich set of features and is computationally efficient our approach has the potential for significant speedup over existing sampling based methods when the number of uncertain model parameters is modest 20 to our knowledge this is the first implementation of the algorithm in a comprehensive fully integrated physically based three dimensional hydrosystem model keywords hydrogeosphere integrated hydrologic modeling polynomial chaos expansion uncertainty quantification 1 introduction as a result of today s high performance computers sophisticated mathematical models are capable of incorporating many complex processes as is often the case the partial or incomplete knowledge of these processes and the input parameters required to describe them necessitates the analyst to make various assumptions and approximations and in doing so introduces uncertainty into the model the aim of uncertainty quantification is to estimate the variability in model responses propagated by model uncertainty consequently uncertainty quantification provides the modeler with a certain level of confidence regarding the predictions made by their model among the many different types of uncertainty present in a mathematical model we consider parametric uncertainty that is the uncertainty in model responses that arises from uncertainty in model input parameters in practice monte carlo simulation mcs metropolis and ulam 1949 or latin hypercube simulation lhs iman et al 1981a 1981b are the workhorse algorithms for uncertainty quantification these algorithms are robust and scale well to higher dimensions however they are slow to converge generally requiring thousands of simulations to obtain an acceptable level of accuracy in contrast to random sampling methods a polynomial chaos expansion represents a model response of interest as a weighted sum of polynomials over the random inputs a non intrusive formulation treats the underlying deterministic model as a black box and consequently does not require any modification of the deterministic model depending on the nature of the problem at hand polynomial chaos has the potential for significant speedup over more traditional random sampling methods moreover once an expansion has been constructed approximating the mean covariance probability density function cumulative distribution function and other common statistics as well as local and global sensitivity measures is straightforward and computationally inexpensive polynomial chaos has its roots in the homogeneous chaos introduced by wiener 1938 which uses hermite polynomials to model random processes involving normal random variables ghanem and spanos 1991 pioneered the very successful stochastic finite elements which combines a hermite polynomial basis with a finite element approach as a method to analyze uncertainty in solid mechanics their method was later generalized by xiu and karniadakis 2002 to the generalized polynomial chaos based on the correspondence between certain probability density functions and weight functions corresponding to orthogonal polynomials from the askey scheme askey and wilson 1985 it should be noted that a similar method referred to as probabilistic collocation was proposed much earlier by tatang et al 1997 and isukapalli et al 1998 however their approach defines the expansion coefficients through the solution of a least squares regression problem instead of via pseudo spectral projection onto an orthogonal polynomial basis a related method that employs a basis of lagrange interpolating polynomials referred to as stochastic collocation was proposed in mathelin and hussaini 2003 and has been further developed in xiu and hesthaven 2005 nobile et al 2008a 2008b and babuška et al 2007 generalizations and extensions of these methods have been proposed in the literature blatman and sudret 2010 foo et al 2008 wan and karniadakis 2005 2006 in this paper we investigate non intrusive polynomial chaos for quantification of uncertainty in fully integrated physics based hydrologic simulations using the hydrogeosphere hgs model aquanty inc 2015 as part of our implementation details we discuss an iterative refinement technique that incrementally improves a chaos expansion through optimal reuse of previous lower order expansions the effectiveness of polynomial chaos as a tool for uncertainty quantification has been demonstrated in many areas including computational fluid dynamics hosder et al 2007 2006 single and multiphase flow in heterogeneous media li and zhang 2007 2009 vehicle dynamics kewlani et al 2012 groundwater modeling deman et al 2016 loosely coupled surface subsurface modeling wu et al 2014 and seawater intrusion modeling rajabi et al 2015 riva et al 2015 however to the best of our knowledge this is the first time that it has been used to quantify the uncertainty in simulations generated by a globally implicit fully integrated surface and variably saturated subsurface flow and solute transport model our polynomial chaos implementation was validated by comparison with analytical solutions as well as solutions obtained via lhs for simple numerical problems it was then used to quantify the parametric uncertainty in a series of numerical problems with increasing complexity including a two dimensional 2d fully saturated steady state flow and transient transport problem with six uncertain parameters and one quantity of interest section 7 1 a one dimensional 1d variably saturated column test involving transient flow and transport with four uncertain parameters and two quantities of interest at 101 spatial locations and five different times each 1010 total section 7 2 and a three dimensional 3d fully integrated surface and subsurface flow and transport problem for a small catchment with seven uncertain parameters and three quantities of interest at 241 different times each section 7 3 the main contributions of this manuscript include the development of a model independent robust and user friendly uncertainty quantification code based on polynomial chaos for application to hydrologic simulations in particular we present a novel iterative refinement procedure for incrementally improving regression based polynomial chaos expansions through optimal reuse of previous lower order expansions the novel application of our code to a highly nonlinear fully integrated surface water groundwater problem in particular we demonstrate that pce can be applied to this complex model problem to efficiently compute time varying global sensitivity indices that provide insight into the model behavior as well as the physical system as a whole the remainder of this paper is organized as follows sections 2 and 3 introduce the notation and setup the mathematical framework in section 4 we provide a thorough overview of polynomial chaos including a brief discussion of the computation of statistics and sensitivity indices from an expansion section 5 discusses the details of the implementation and in particular highlights the iterative refinement procedure section 6 describes the hydrogeosphere model and section 7 describes the numerical testing and provides a discussion of the results concluding statements are presented in section 8 2 notation throughout this paper we adhere to the following notational conventions uppercase and lowercase letters denote scalar quantities and in some cases may be used to denote sets functions or operators bold lowercase letters always denote vectors e g x and bold uppercase letters always denote matrices e g a uppercase script letters always denote sets or spaces e g a d the set of nonnegative integers 0 1 2 is denoted by n the set of strictly positive integers 1 2 3 is denoted by n 1 and the set of real numbers is denoted by r the set of all n vectors with elements in some set b for some positive integer n is denoted by b n we use u a b to denote the uniform probability distribution on the interval a b logn μ σ to denote the lognormal distribution with location parameter μ and scale parameter σ and logu a b to denote the loguniform distribution on the interval a b we note that a random variable x logu a b if and only if ln x u ln a ln b 3 mathematical framework consider a mathematical model that depends on a finite collection of parameters ξ ξ 1 ξ d and let u x t be any real valued response of this model where x r n is the position variable and t 0 is the time variable suppose that the model parameters ξ are uncertain and that we would like to quantify the resulting uncertainty in u we refer to ξ as the parameters of interest and to u as a quantity of interest note that u may also depend on additional certain parameters that are fixed and are not considered by our analysis a natural way to approach the mathematical formulation of this problem is to adopt a probabilistic framework for the uncertain parameters treating them as random variables and recasting the deterministic function u as a function of these random variables in doing so the uncertainty in u may then be rigorously quantified through statistical measures such as its mean and variance the random variables ξ 1 ξ d are modeled as a d variate random vector ξ ξ 1 ξ d in a properly defined probability space ω f p where ω is the sample space f is the event space p is the probability measure and ξ ω r d we make the assumption that ξ has independent components we note that as discussed in eldred and burkardt 2009 this assumption is not absolutely necessary in theory correlated random variables may be transformed to uncorrelated standard random variables in which the independence assumption is valid e g standard normal random variables we regard u u x t ξ as a random process in which u x t ξ ω ω r is a random variable for each ordered pair x t we assume that ξ consists entirely of continuous random variables although this framework works equally well for discrete variables let p i z p ξ i z denote the probability density function pdf of each random variable ξi with support on the interval a i ξ i ω r for i 1 d furthermore let f i z p ξ i z denote the cumulative distribution function cdf of ξi with support on a i for i 1 d since ξ has independent components its joint pdf is defined in terms of the marginal pdfs by 1 p z p 1 z 1 p d z d which has support on 2 a a 1 a d r d let l 2 ω p denote the hilbert space of continuous real valued random variables on ω f p that have finite second moments with respect to the inner product l 2 defined by 3 x y l 2 ω x ω y ω p d ω we assume that u x t ξ ω belongs to l 2 ω p for each ordered pair x t and note that its expected value is defined by 4 e u ω u x t ξ ω p d ω a u x t z p z d z where the latter equivalent expression is more convenient for computation naturally higher order moments of u such as its variance may be defined analogously 4 polynomial chaos expansion polynomial chaos pc employs a basis of multivariate orthogonal polynomials to capture the functional relationship between a model response u and the random parameters ξ this relationship may be expressed by an infinite sum referred to as a polynomial chaos expansion pce of the form 5 u x t ξ i 1 c i x t φ i ξ where ci are the real valued expansion coefficients and φ i a r i n 1 is a sequence of orthogonal multivariate polynomials referred to as basis polynomials naturally in practice one must truncate the infinite sum to a finite number of terms and so we shall consider truncated polynomial chaos expansions of the form 6 u x t ξ u n x t ξ i 1 n c i x t φ i ξ in this work we consider only non intrusive pc formulations in which the underlying deterministic model is treated as a black box intrusive pc formulations such as the stochastic galerkin method ghanem and spanos 1991 require access to the governing equations of the underlying model and are not applicable in our case due to the complex nature of sophisticated 3d simulations it is evident from 6 that any polynomial chaos expansion consists of three parts the basis polynomials the truncation scheme or basis type and the expansion coefficients in the following sections we discuss how each of these components may be defined in addition we discuss how various statistics and sensitivity measures may be computed from a truncated expansion 4 1 basis polynomials the polynomial chaos basis polynomials φi are constructed as products of univariate orthogonal polynomials the optimal basis polynomials are obtained when the univariate orthogonal polynomials in dimension j are orthogonal with respect to the pdf of the random parameter ξj ideally each random parameter corresponds via its pdf to a family of orthogonal polynomials from the askey scheme of hypergeometric orthogonal polynomials askey and wilson 1985 table 1 presents some families of orthogonal polynomials that correspond to classical weight functions from the askey scheme it is important to note that each orthogonal polynomial corresponds to a probability density function in standard form up to scaling e g the hermite polynomials used by wiener in the homogeneous chaos correspond to the standard normal distribution if this is not the case then one may either generate the orthogonal polynomials numerically gautschi 1982 1994 or employ a nonlinear variable transformation to the most similar askey type for example a loguniform random variable could be transformed to a uniform random variable prior to constructing the pce for technical reasons related to the convergence of the polynomial chaos expansion see section 4 4 we generally prefer the variable transformation approach let φ k j k 0 denote the univariate orthogonal polynomials with respect to pj z for j 1 d we introduce a d dimensional multi index as a d variate vector i i 1 i 1 i d in which each component is a nonnegative integer i n d observe that there exists a one to one correspondence γ n d n 1 between the set of all possible multi indexes and the set of strictly positive integers thus to each expansion index i we may assign a unique expansion multi index i such that γ i i the ith basis polynomial φi ξ is then defined by 7 φ i ξ φ γ i ξ φ i 1 1 ξ 1 φ i 2 2 ξ 2 φ i d d ξ d owing to the product form of the joint probability density function 1 the basis polynomials satisfy the orthogonality condition 8 φ k ξ φ ℓ ξ l 2 0 k ℓ 0 k ℓ as discussed in section 4 4 under certain conditions on the random parameters ξ 1 ξ d and their cdfs the orthogonal polynomials φ i ξ i n 1 form a complete orthogonal basis of the hilbert space l 2 ω p in which the model response u x t ξ lives thus u can be expressed as an infinite weighted sum of these polynomials we note that if the constituent univariate polynomials are orthonormal then φ i ξ i n 1 forms a complete orthonormal basis of l 2 ω p exactly which basis polynomials are included in the truncated expansion depends on the truncation scheme which is discussed in section 4 2 4 2 expansion truncation in general a polynomial chaos expansion may be truncated to any positive number of terms n involving any combination of basis polynomials among the many possible truncation schemes the total order expansion is most commonly cited in the literature a total order pce employs a complete basis of polynomials up to a fixed total order specification that is the degree of the multivariate basis polynomials is restricted to not exceed a fixed integer p 0 in terms of the expansion multi index i n d all indices that satisfy the inequality i 1 i 1 i d p are kept for example the basis polynomials of a two dimensional second order expansion d 2 p 2 are φ 1 ξ φ 0 1 ξ 1 φ 0 2 ξ 2 φ 2 ξ φ 1 1 ξ 1 φ 0 2 ξ 2 φ 3 ξ φ 0 1 ξ 1 φ 1 2 ξ 2 φ 4 ξ φ 0 1 ξ 1 φ 2 2 ξ 2 φ 5 ξ φ 1 1 ξ 1 φ 1 2 ξ 2 φ 6 ξ φ 2 1 ξ 1 φ 0 2 ξ 2 the number of terms n in an expansion of total order p and dimension d is given by n k 0 p d k 1 k d p d p compared with an isotropic tensor product expansion of order p in which each element of the expansion multi index i ranges over 0 p and hence n 1 p d a total order expansion offers good monomial coverage with slower growth in n 4 3 expansion coefficients different flavors of polynomial chaos are due in part to the variety of ways in which the expansion coefficients ci may be defined we discuss two of the major approaches found in the literature namely pseudo spectral and linear regression pc 4 3 1 pseudo spectral polynomial chaos mathematically the expansion coefficients ci are defined through the projection of u x t ξ onto the polynomial basis φ i ξ i n 1 assuming for simplicity that the basis polynomials are orthonormal the expansion coefficients are given by 9 c i x t a φ i z u x t z p z d z for i 1 2 and the chaos expansion 5 forms a generalized fourier series of u unfortunately since u is known only through the solution of a complex numerical model these integrals in general cannot be evaluated analytically in pseudo spectral polynomial chaos the expansion coefficients are computed through numerical approximation of the integrals 9 among the available approximation methods which include monte carlo simulation quadrature and cubature we briefly describe the quadrature based method in more detail let q m f j 1 m a j f ξ j a j r ξ j a be an m point quadrature formula for d dimensional integrals of the form i f a f z p z d z then the expansion coefficients may be approximated by 10 c i x t q m φ i ξ u x t ξ j 1 m a j φ i ξ j u x t ξ j for i 1 2 if the expansion has been truncated to n terms then we arrive at the linear system 11 c φ t a u that defines the coefficients where 12 φ φ 1 ξ 1 φ n ξ 1 φ 1 ξ m φ n ξ m r m n a diag a 1 a m r m m and c u are m vectors of the approximate coefficients and model responses at the points ξ j respectively the standard approach to defining d dimensional quadrature rules is to construct them from 1d quadrature rules analogous to how the d variate orthogonal polynomials are constructed from univariate orthogonal polynomials due to their high precision and close relationship with orthogonal polynomials gauss quadrature rules gautschi 1968 golub and welsch 1969 are typical candidates for the 1d rules the simplest approach to forming a d dimensional rule is to combine the 1d rules through a tensor product construction however this approach suffers from the curse of dimensionality and is only practical when the dimension d is small a more successful approach that aims to lessen the curse of dimensionality while at the same time preserving a high level of accuracy is based on smolyak sparse grid rules smolyak 1963 smolyak sparse grid rules are nothing more than linear combinations of tensor product rules with the key distinction that only products with relatively few points are used their application to pce has been thoroughly investigated in the literature constantine et al 2012 eldred and burkardt 2009 nobile et al 2008a 2008b the real power of smolyak sparse grid rules is realized when the constituent 1d quadrature rules are nested since this greatly reduces the number of nodes in the resulting multidimensional rule nesting has the added benefit of allowing for significant point reuse as the order of the sparse grid rule increases 4 3 2 linear regression polynomial chaos suppose the chaos expansion has been truncated to n terms in the linear regression approach the coefficients c 1 c n are defined by minimizing 13 u n x t ξ j u x t ξ j for j 1 m in the l 2 sense for some set of points e ξ 1 ξ m a in terms of matrices and vectors we have 14 c arg min v r n u φ v 2 where u c and φ are defined in section 4 3 1 the points in e at which the response values are computed are referred to as collocation points thus the linear regression approach seeks the coefficients that best reproduce the response values over a set of collocation points naturally the approximation quality of the polynomial chaos expansion depends strongly on the distribution of collocation points in a a number of methods are possible for defining the collocation point set including point collocation the collocation points are defined on an unstructured grid obtained from random sampling within the pdf p z hosder et al 2007 probabilistic collocation the collocation points are defined as a subset of the tensor product grid constructed from p 1 point gauss quadrature rules isukapalli et al 1998 tatang et al 1997 tensor product grid collocation the collocation points are defined via a tensor product grid constructed from gauss quadrature rules sparse grid collocation the collocation points are defined via a smolyak sparse grid constructed from nested gauss quadrature rules the linear system defining the least squares problem in the linear regression approach may have any shape however it is commonly overdetermined that is m n as discussed by isukapalli et al 1998 an over sampling with m 2n collocation points is recommended to obtain a high quality pce the solution of 14 in the overdetermined case is given by 15 c φ u where φ is the moore penrose pseudoinverse of φ since the least squares problem may be ill conditioned its solution is computed via the singular value decomposition golub and van loan 1996 we note that it is only necessary to compute the singular value decomposition of φ once after which it can be stored in factorized form for future use if φ has full column rank then 15 is the unique minimal solution otherwise there exist infinitely many minimal solutions as discussed by sudret 2008 in order to obtain an accurate polynomial chaos approximation the least squares problem 14 should have a unique solution it is interesting to observe that under certain assumptions the pseudo spectral and linear regression approaches are equivalent in the following sense suppose that the maximum degree of any basis polynomial φi in the truncated expansion is p and that φ and a in 11 are constructed from a multidimensional quadrature rule with positive weights and polynomial precision at least 2p these assumptions are met for example by a total order expansion of order p that employs p 1 point gauss quadrature rules to construct a tensor product quadrature rule without restricting generality assume that the basis polynomials are orthonormal then φ t a φ i j a φ i z φ j z p z d z δ i j for all 1 i j n letting a 1 2 denote the square root of matrix a it follows that a 1 2 φ has orthonormal columns and left inverse φ t a 1 2 therefore the coefficient vector c φ t a u defined by pseudo spectral projection is the unique solution of the weighted least squares problem a 1 2 φ v a 1 2 u up to diagonal scaling by a 1 2 this least squares problem is identical to that which is solved by the linear regression approach this simple analysis further reveals that under the given assumptions the matrix φ has full column rank as desired it is worth noting that this equivalency may fail when using sparse grid quadrature rules since negative weights are possible even when the underlying 1d quadrature rules are positive however it remains true that φ has full column rank 4 4 convergence let u be a random variable belonging to l 2 ω p and let un denote its polynomial chaos expansion truncated to n terms in which the expansion coefficients are defined through spectral projection 9 suppose that each random variable ξ 1 ξ d possesses finite moments of all orders and that the cdf fj is uniquely defined by its moments for j 1 d then according to ernst et al 2012 the polynomials φ i ξ i n 1 form a complete orthogonal basis of l 2 ω p and un u as n in the mean square sense this result further implies that un converges to u in both probability and distribution and in particular the mean and variance of un converges to the mean and variance of u respectively however it should be noted that depending on the probability distributions involved higher order moments of un may fail to converge to the corresponding moments of u as demonstrated by field jr and grigoriu 2007 probability distributions that meet the assumptions given above include but are not limited to the normal uniform loguniform triangular gamma and beta distributions we note that since the lognormal distribution is not uniquely defined by its moments a polynomial chaos expansion in which at least one random parameter ξj follows a lognormal distribution may not converge to the correct quantity it is possible to remedy this issue via a variable transformation to a standard normal random variable however variable transformations should be used sparingly and only in certain circumstances because the additional nonlinearity introduced by the transformation may negatively affect the convergence of un the convergence rate of un depends strongly on the smoothness of u x t ξ with respect to the uncertain parameters ξ in particular if u or any of its derivatives are discontinuous or if steep gradients are present the rate of convergence may be diminished perhaps severely thus depending of the regularity of u it may be necessary to use a high order expansion necessitating many model simulations to obtain reasonable accuracy the poor convergence behavior of un in this case may be attributed in part to the fact that a global polynomial basis is ill suited for representing such functions 4 5 analytic statistics and probability distributions analytic expressions for the mean and covariance of a model response u may be readily computed with minimal expense from its truncated chaos expansion un without restricting generality suppose that the chaos expansion of u employs an orthonormal polynomial basis in which φ1 ξ 1 then by orthonormality of the basis polynomials 16 e u x t e u n x t u n x t ξ φ 1 ξ l 2 i 1 n c i x t φ i ξ φ 1 ξ l 2 c 1 x t by a similar argument we arrive at an expression for the covariance 17 cov u x 1 t 1 u x 2 t 2 cov u n x 1 t 1 u k x 2 t 2 e u n x 1 t 1 u k x 2 t 2 e u n x 1 t 1 e u k x 2 t 2 i 1 n j 1 k c i x 1 t 1 c j x 2 t 2 φ i ξ φ j ξ l 2 c 1 x 1 t 1 c 1 x 2 t 2 i 2 min n k c i x 1 t 1 c i x 2 t 2 it follows immediately from the expression for the covariance that 18 var u x t var u n x t i 2 n c i x t 2 formulas for the third and higher moments may also be derived however they are more difficult to use in practice as they involve multidimensional integrals additionally as u is assumed to be a second order process there is no guarantee that the third and higher moments of un converge to the corresponding moments of u as n the pdf or cdf of u may be approximated from its polynomial chaos expansion by random sampling of the truncated expansion un naturally statistics such as the median interquartile range and percentiles for example may be estimated from the random samples random sampling of un is in general much more efficient then random sampling of u since it involves computing linear combinations of multidimensional polynomials which are relatively cheap compared to simulations of a complex numerical model 4 6 local sensitivity analysis local sensitivity analysis seeks to quantify the change in a model response to local perturbations of the model parameters taken one at a time about a fixed point if the partial derivatives of the model response u with respect to ξ 1 ξ d exist at some point ξ 0 a then the first order local sensitivity coefficients sj may be defined by 19 s j x t u x t ξ ξ j ξ ξ 0 for j 1 d a typical value for ξ 0 is the mean of ξ the sensitivity coefficients sj may be approximated in terms of the polynomial chaos expansion by 20 s j x t u n x t ξ ξ j ξ ξ 0 i 1 n c i x t φ i ξ ξ j ξ ξ 0 for j 1 d since the basis polynomials are the product of univariate orthogonal polynomials 7 it follows that each partial derivative in the formula above reduces to the derivative of a single univariate orthogonal polynomial φ i ξ ξ j d φ i j j ξ j d ξ j k j φ i k k ξ k hence each local sensitivity coefficient may be computed with the same effort as evaluating the chaos expansion at a single point 4 7 global sensitivity analysis global sensitivity analysis seeks to quantify the change in a model response as a result of simultaneously varying subsets of the model parameters over their parameter spaces variance based decomposition is a type of global sensitivity analysis in which the variance of the model response is decomposed into fractions that can be attributed to subsets of the input parameters sobol 1993 it uses two primary measures the main effect index sj and the total effect index tj the main effect index sj measures the contribution to the model response variance arising solely from the jth model parameter ξj on the other hand the total effect index tj measures the contribution to the model response variance arising from the jth model parameter and all its interactions with the other model parameters in addition to the main and total effects indices of higher order interactions involving two or more model parameters may also be defined we use the notation s j 1 j k to denote the interaction between the model parameters ξ j 1 ξ j k where j 1 j k 1 d are distinct and 1 k d in general computing sj tj and higher order indices of a model response u requires the evaluation of multidimensional integrals which may be accomplished by methods based on random sampling saltelli et al 2010 on the other hand as illustrated by sudret 2008 ciriello et al 2013 and formaggia et al 2013 polynomial chaos expansion is particularly amenable to variance based global sensitivity analysis in which case the sensitivity indices can be expressed as analytic functions of the expansion coefficients to define s j 1 j k and tj in terms of the expansion coefficients we assume that the truncated polynomial chaos expansion of u is expressed in the equivalent form 21 u n x t ξ i i c i x t φ i ξ where i is the set of n multi indexes i n d that define the basis polynomials in the expansion now define the index set 22 s j 1 j k i i i i ℓ 0 ℓ j 1 j k i ℓ 0 ℓ 1 d j 1 j k then the sensitivity index s j 1 j k of the model response u may be approximated by 23 s j 1 j k 1 v i s j 1 j k i c i x t 2 where v is the total variance of u computed from 18 because the index set i can always be partitioned into nonempty sets of the form s j 1 j k i it follows that the sum of all such indices is equal to one we note that for the main effect indices sj the index set 22 simplifies to s j i i i i j 0 i k 0 k j the total effect index tj may be approximated from the polynomial chaos expansion by a similar equation 24 t j 1 v i t j i c i x t 2 t j i i i i j 0 there are 2 d 1 sensitivity indices of the form s j 1 j k so even for relatively small dimensions d the number of possibilities is large in practice this does not pose a problem since we are primarily interested in the main effects and total effects which comprise only 2d indices moreover since many of the higher order interaction indices will be near or equal to zero we may truncate any indices that fall below a given threshold 5 polynomial chaos implementation one of the difficulties that arises when using the polynomial chaos expansion for uncertainty quantification is the selection of the expansion order p ideally the expansion order should be large enough to obtain a reasonable level of accuracy but should not result in an undue number of costly simulations unfortunately estimating the optimal expansion order a priori is typically intractable for all but the simplest problems to remedy this issue we have developed an iterative refinement framework that attempts to efficiently construct a sufficiently accurate chaos expansion starting from an initial low order approximation the steps of our algorithm see algorithm 1 are as follows starting from an initial low order expansion with a user specified expansion order typically p 1 to start during each refinement step the expansion order is incremented the chaos expansion is reconstructed requiring new model simulations and a convergence criterion is checked the refinement process attempts to minimize the number of model simulations by reusing the results of previous refinement steps reuse is accomplished through the use of nested quadrature rules within a smolyak sparse grid framework when reuse is optimal iterative refinement can iterate through expansion orders 1 p in roughly the same amount of work that it would take to construct an order p expansion the convergence criteria are based on the relative and absolute changes in the mean and standard deviation from one refinement step to the next and on the magnitude of the leave one out cross validation error loocv additional criteria based on the maximum number of simulations the maximum expansion order and the maximum number of refinement steps may also be imposed as discussed by blatman and sudret 2010 the loocv is a relatively robust error estimate that can be computed efficiently for linear regression polynomial chaos expansions we use the loocv to quantify the predicative accuracy of the polynomial chaos expansion un relative to the underlying deterministic model response u in principle if the loocv is sufficiently small then our estimates of the relative error in the mean and standard deviation on which we base our convergence criterion should be trustworthy our polynomial chaos implementation is assembled together with sampling methods such as monte carlo latin hypercube and quasi monte carlo simulation to provide a comprehensive uncertainty quantification tool due to the non intrusive nature of pce and the sampling methods our tool is effectively model independent and hence is amenable to models other than hgs it is implemented in c and provides failure tracking of model simulations as well as a restart feature probability distributions commonly associated with surface water and groundwater modeling are supported including the normal uniform lognormal loguniform and triangular distributions model simulations which are typically the most computationally demanding aspect of uncertainty quantification have been parallelized with openmp openmp architecture review board 2002 additionally our tool is hosted on algocompute merrick 2017 a web based cloud computing tool that employs the microsoft azure cloud computing platform our polynomial chaos implementation has been validated by comparison with analytical solutions as well as empirical solutions obtained by lhs for a variety of simple model problems and its performance has been measured against lhs for the purposes of this paper we discuss more complex problems which are described in section 7 6 hgs numerical model hydrogeosphere hgs is a fully integrated surface and variably saturated subsurface flow model including contaminant and heat transport evapotranspiration and land surface processes aquanty inc 2015 davison et al 2015 hwang et al 2015 sudicky et al 2008 hgs implements an implicit control volume finite element method and utilizes openmp for its parallelization hwang et al 2014 park et al 2009 the two dimensional surface is draped directly over the three dimensional subsurface domain and is modeled by the depth integrated saint venant equation ϕ 0 h 0 t d 0 k 0 d 0 z 0 d 0 γ 0 q 0 where ϕ 0 is the surface domain porosity h 0 is the hydraulic head m d 0 is the surface water depth m t is time s γ0 is the surface water volumetric exchange between domains s 1 and q 0 is surface domain sources and sinks m s 1 the surface conductance m s 1 k 0 is calculated by the manning chezy or darcy weisbach equations the surface domain is coupled to the subsurface domain by either the common node approach both the surface and subsurface domain share one node or the dual node approach the surface and subsurface are directly linked between domains the variably saturated subsurface domain implements richards equation s w s s ψ t ϕ s t k k r ψ z γ q where s is the water saturation ss is the specific storage m 1 ϕ is the porosity ψ is the pressure head m k is hydraulic conductivity tensor m s 1 z is the elevation m γ is the volumetric water exchange flux between domains s 1 and q represents the sources and sinks s 1 the relative conductivity kr is determined by genuchten 1980 brooks and corey 1964 or through a lookup table contaminant transport in the subsurface is modeled by the three dimensional advection dispersion equation 25 ϕ s c t q c ϕ s d c where c is the solute concentration g m3 q is the flux m s 1 and d is the hydrodynamic dispersion tensor m2 s 1 the two dimensional surface domain implements a modified version of 25 by removing the z component 7 numerical tests our polynomial chaos implementation was tested through a series of numerical problems of increasing complexity both in terms of the physical problem and in the application of our uncertainty quantification tool all test problems were conducted using linear regression polynomial chaos with a total order basis sparse grid collocation and iterative refinement in conjunction with the physics based fully integrated hydrologic model hydrogeosphere aquanty inc 2015 the test problems include a 2d fully saturated steady flow and transient transport problem with six uncertain parameters and one quantity of interest a 1d variably saturated column with transient flow and transient transport four uncertain parameters and two quantities of interest at 101 spatial locations and five times each 1010 total and a 3d fully integrated surface and variably saturated subsurface flow and solute transport problem for a small catchment with seven uncertain parameters and three quantities of interest at 241 times each 7 1 2d fully saturated steady flow with transient transport 7 1 1 problem description this problem simulates the movement of a dissolved conservative contaminant from a deep underground source to a near surface environmental receptor fig 1 the problem consists of a 2d layered bedrock system in which hydraulic conductivity and porosity within each layer are uncertain and decrease with depth parameter ranges and distributions are shown in table 2 the flow system is assumed to be at steady state and is driven by a recharge zone and a discharge zone the quantity of interest is the concentration at the observation point near the discharge zone after one million years 7 1 2 model setup the 2d model domain is unit thickness in the y direction and measures 1500 m in the x direction and 100 m in z direction numerical elements measure 2 m 2 m in the x and z directions and the mesh contains a total of 76 602 nodes no flow boundaries are assigned to all outer boundaries except in the areas labeled as recharge and discharge zones constant head boundaries are assigned in these regions to generate a regional flow field h recharge 100 m and h discharge 99 m the contaminant source is represented by a constant concentration boundary condition of unit concentration the solute is conservative non reactive has a free solution diffusion coefficient of 10 9 m2 s 1 and has longitudinal and transverse dispersivity values of 5 m and 0 5 m respectively 7 1 3 results the mean and standard deviation of the concentration at the observation point after one million years are determined using a pce and lhs the predicted mean and standard deviation are compared against their empirically determined true values which are obtained via 10 000 lhs simulations the relative error of the mean and standard deviation are shown in fig 2 in this case pce converges significantly faster than lhs which suggests that pce has the potential for considerable speedup over more traditional sampling based methods fig 3 shows the pdf in subplot a and the cdf in subplot b of the concentration computed from an order 4 pce 737 simulations compared to the empirical true case 10 000 latin hypercube simulations with more than 13 times fewer model simulations the chaos expansion is able to accurately reproduce the empirically determined pdf and cdf of the concentration we conclude this section by examining how closely the pce constructed through iterative refinement approximates the full hgs model table 3 presents the mean absolute error mae root mean square error rmse nash sutcliffe model efficiency coefficient nse and the percentage bias pbias of the concentration obtained by comparing the order four pce against hgs simulation results at 100 points randomly distributed throughout the parameter space via latin hypercube sampling in addition we present the leave one out cross validation error loocv computed over the set of collocation points 7 2 1d transient flow and transport in a 1d unsaturated column 7 2 1 problem description this problem simulates the constant infiltration of a unit concentration dissolved solute into a 5 m long dry soil column with free drainage outlet at the bottom concentration and water saturation values are observed at 5 cm intervals along the column at five separate times 7 2 2 model setup the numerical model consists of a 5 m long column divided vertically into 500 elements each element is 1 cm thick the elements measure unit thickness in the x and y directions 1 m and contains 2002 nodes and 500 elements as shown in table 4 four parameters are assumed to be uncertain for this problem hydraulic parameters are based on literature values for borden sand sudicky 1986 for hydraulic conductivity and berg and gillham 2010 for mean van genuchten parameters uncertainty was introduced into the van genuchten parameters by perturbing them by approximately 15 25 around their mean values the impact of the uncertainty in the van genuchten alpha and beta on the pressure saturation relationship is shown in fig 4 initially the model is near residual saturation and has a constant flux boundary condition of 8 64 mm day on top additionally the top boundary condition contains a conservative non reactive tracer chloride with a free solution diffusion coefficient of 10 9 m2 s 1 for the duration of the simulation 25 days uncertainty in saturation and concentration is quantified using an order four pce at 101 spatial locations every 5 cm along the column and five different times for each quantity of interest 1010 pce estimates 7 2 3 results spatial and temporal solute concentration and saturation distributions are shown in fig 5 respectively fig 5 a shows the mean solute concentration distribution bounded by one standard deviation as determined by pce at 3 7 15 21 and 25 days fig 5 b shows the mean saturation distribution bounded by one standard deviation as determined by pce at 1 3 7 11 and 13 days 7 3 3d fully integrated surface and variably saturated subsurface flow and transport in a small catchment this demonstration case involves a rainfall runoff field experiment conducted by abdul and gillham 1989 at the borden research site this field experiment has become a standard problem for benchmarking fully integrated surface and subsurface flow models aquanty inc 2015 kollet et al 2017 vanderkwaak 1999 7 3 1 problem description the simulation problem is a fully integrated surface and variably saturated subsurface water flow and solute transport problem the small test catchment measures approximately 80 m long by 15 m wide fig 6 the rainfall runoff experiment consisted of applying precipitation with a conservative bromide tracer to the catchment at a rate of 2 cm h for 50 min outflow rates and bromide concentrations were measured at the outlet of the model as shown in table 5 seven parameters were assumed to be uncertain the uncertainty range in the hydraulic conductivity was based on literature values sudicky 1986 and the ranges for the remaining parameters in table 5 were obtained by a pest doherty 1994 calibration the quantities of interest include water flux out of the catchment bromide mass flux out of the catchment and hydraulic head at a point in the model recorded at 241 output times 7 3 2 model setup the numerical model simulates both surface subsurface flow and transport in a fully integrated manner using hgs aquanty inc 2015 the finite element mesh contains approximately 22 000 nodes and 40 000 elements similar to previous model designs by vanderkwaak 1999 and kollet et al 2017 the subsurface is homogeneous and the surface domain is divided into two zones the main channel and the surrounding area precipitation with a unit concentration of bromide tracer is applied to the upper surface of the model at a rate 2 cm h for 50 min after which the simulation continues for an additional 50 min with no precipitation water and bromide mass flux out of the domain and the hydraulic head at an observation point fig 6 are recorded every 25 s 7 3 3 results temporally varying global sensitivity indices are computed using an order two pce requiring a total of 98 model simulations fig 7 and for comparison an order four pce requiring a total of 1329 model simulations fig 8 in each figure subplots a b and c show the median and interquartile ranges for water flux discharge bromide mass discharge and hydraulic head respectively these statistics were computed from 10 000 random samples of the chaos expansion subplots d e and f show the most significant main effect global sensitivity indices for these same quantities of interest it is encouraging to observe that with less than one tenth the number of model simulations the order two expansion results are nearly identical to the order four expansion results in particular the order two expansion is able to capture the main features of the time varying sensitivity indices as shown in figs 7 and 8 time varying sensitivity coefficients can provide considerable insight into how the most sensitive system parameters may change depending on model state for example during the early portion of the simulation when rain is being applied to the domain hydraulic head is most sensitive to rill storage however when the simulated rain stops hydraulic head becomes most sensitive to porosity 8 conclusions although high performance computers and advanced numerical methods have made the application of fully integrated surface and subsurface flow and transport models such as hydrogeosphere common place run times for large complex basin models can still be on the order of days to weeks traditional workhorse algorithms for uncertainty quantification such as lhs or mcs typically require thousands of simulations to achieve an acceptable level of accuracy thus making their computational cost prohibitive for such applications in this paper we demonstrated the performance gains that are possible by applying polynomial chaos to uncertainty quantification of integrated hydrologic models an added benefit of our approach is that once a chaos expansion has been constructed approximating the mean covariance pdf cdf and other common statistics as well as local and global sensitivity measures is straightforward and computationally inexpensive our numerical experiments show that polynomial chaos is an effective and robust method for quantifying uncertainty in hydrologic simulations that provides a rich set of features and is computationally efficient with the potential for significant speedup over existing sampling methods when the number of uncertain model parameters is modest we note that while the test cases presented in this manuscript employ hydrogeosphere our non intrusive implementation is model independent allowing it to be applied to any deterministic model provided the model inputs and outputs are formatted appropriately in particular the following key points were discussed in this study the development and application of a non intrusive polynomial chaos expansion tool for the quantification of uncertainty in deterministic hydrologic models such as hydrogeosphere in particular our implementation contains an iterative refinement framework that incrementally improves a chaos expansion through the optimal reuse of previous lower order expansions for increased efficiency the pce tool is parallelized with openmp openmp architecture review board 2002 thus allowing many concurrent simulations to be performed on a single desktop computer additionally the pce tool is hosted on algocompute merrick 2017 a web based cloud computing tool that employs the microsoft azure cloud computing platform in the first test case section 7 1 convergence with respect to increasing expansion order hence increasing simulation count of the pce mean and standard deviation was compared to lhs for a simple 2d fully saturated steady state flow and transient transport problem it was observed that pce converged to a relative error of less than 10 2 significantly faster than lhs requiring almost 7 times fewer model simulations furthermore the pdf and cdf obtained from random sampling of the fourth order pce 10 000 samples were shown to be essentially identical to the pdf and cdf obtained from lhs with 10 000 model simulations the second test case section 7 2 which consisted of two quantities of interest at 101 spatially distributed observation locations demonstrated the ability of the pce tool to handle a large number of different observation locations in contrast to the second test case the third test case section 7 3 is highly transiently resolved with 241 output times for three different observation locations this small watershed case demonstrated the capability of pce for time varying global sensitivity analysis which can provide significant insight into the relative parameter importance under changing hydrologic conditions future work will focus primarily on validation involving multiple sources of uncertainty including both surface and subsurface processes the application of our tool to nuclear waste management flood risk analysis precipitation large basin models and other areas of interest is planned further refinement of the sparse grid collocation iterative refinement method is anticipated and adaptive grid refinement guided by global sensitivity analysis is being investigated grid refinement with respect to the deterministic model time is also being investigated acknowledgements the authors would like to thank alberto guadagnini and the two anonymous reviewers whose constructive comments improved the final version of this manuscript funding this research was supported by an industrial r d fellowship awarded to killian miller by the natural sciences and engineering research council of canada nserc grant no 6037 2014 469139 and an industrial talentedge fellowship awarded to jason davison by the ontario centres of excellence oce grant no 26691 
909,this study utilizes a qualitative approach and a two phase numerical model to investigate net sediment transport caused by velocity skewness beneath oscillatory sheet flow and current the qualitative approach is derived based on the pseudo laminar approximation of boundary layer velocity and exponential approximation of concentration the two phase model can obtain well the instantaneous erosion depth sediment flux boundary layer thickness and sediment transport rate it can especially illustrate the difference between positive and negative flow stages caused by velocity skewness which is considerably important in determining the net boundary layer flow and sediment transport direction the two phase model also explains the effect of sediment diameter and phase lag to sediment transport by comparing the instantaneous type formulas to better illustrate velocity skewness effect in previous studies about sheet flow transport in pure velocity skewed flows net sediment transport is only attributed to the phase lag effect in the present study with the qualitative approach and two phase model phase lag effect is shown important but not sufficient for the net sediment transport beneath pure velocity skewed flow and current while the asymmetric wave boundary layer development between positive and negative flow stages also contributes to the sediment transport keywords boundary layer thickness qualitative approach sediment transport two phase model velocity skewness 1 introduction nearshore oscillatory flows induced by waves are usually velocity skewed with peaked narrow crest and flat wide trough in wave propagation and shoaling the interaction between sediment and fluid and the collision between sediment particles are obviously strong in the sheet flow layer with large shields parameter θ 0 8 1 0 and high sediment volumetric concentration s 0 08 0 65 an additional current is usually imposed to the velocity skewed oscillatory sheet flow and produces more complex boundary layer flow and sediment transport process accurate prediction of the proposed net sediment transport rates is important in coastal engineering however the relevant micro mechanism is not clearly understood accompanied with probable large boundary layer flow and net transport rate that are considerably essential for environmental and morphological studies thus more studies for boundary layer flow and net sediment transport in velocity skewed oscillatory sheet flows are needed the sediment transport measurements of oscillatory sheet flow referring to velocity skewness over the last quarter century include the pure velocity skewed flows dibajnia and watanabe 1992 ribberink and al salem 1994 ahmed and sato 2003 o donoghue and wright 2004a b dong et al 2013 mixed velocity skewed and acceleration skewed flows ruessink et al 2011 silva et al 2011 lanckriet et al 2014 van der zanden et al 2015 and velocity skewed flows combined with current dong et al 2013 ribberink and al salem 1995 dibajnia 1991 the net boundary layer flow is negative in pure velocity skewed flows o donoghue and wright 2004b ribberink and al salem 1995 however relevant net sediment transport caused by high velocity skewness is usually classified according to the influence of the phase lag without net boundary layer flow negative net sediment transport rate is observed and thought generated when the phase lag is considerably clear with small sediment diameter short period or large velocity amplitude dibajnia and watanabe 1992 o donoghue and wright 2004a b dong et al 2013 positive net sediment transport rate is observed and thought generated when the phase lag effect is small on the basis of different phase lag effects ribberink and al salem 1995 found correlation between net sediment transport rate and velocity skewness hassan and ribberink 2010 enhanced the correlation and added sediment size and velocity amplitude influence phase lag parameter is introduced dibajnia and watanabe 1992 dohmen janssen 1999 dohmen janssen et al 2002 representing the time ratio between sediment falling down in sheet flow layer and the wave period it was first adopted in the semi unsteady type empirical model dibajnia and watanabe 1992 for net sediment transport the phase lag theory of sediment movement in velocity skewed oscillatory sheet flow has been established by the widely used model dibajnia and watanabe 1992 however the result accuracy predicted by such theory remains insufficient for a wide range of flow conditions more complex parameters around phase lag are considered in certain ways ahmed and sato 2003 dong et al 2013 dohmen janssen et al 2002 silva et al 2006 van der a et al 2013 and difficulties exist in determining appropriate model parameters however some key factors are still not considered such as the asymmetric development of boundary layer thickness between positive and negative flow stages and the net boundary layer flow caused by velocity skewness the most observable area of net boundary layer flow is near the initial bed where sediment concentration is high in the asymmetric oscillatory flows ruessink et al 2009 and progressing waves kranenburg et al 2012 and when an additional current is exerted holmedal and myrhaug 2006 yuan and madsen 2015 holmedal et al 2013 nevertheless the contribution of net boundary layer flow in the total net sediment transport over a mobile bed in velocity skewed flow remains unclear as well as the combined action of net boundary layer flow and phase lag in the last decade two phase models liu and sato 2006 li et al 2008 chen et al 2011 lee et al 2016 have successfully predicted sediment transport in velocity skewed oscillatory sheet flows which can capture the complex fluid particle and inter particle interactions however the generation of net sediment transport by velocity skewness is still mainly attributed to the phase lag effect the present study develops a qualitative approach and applies the two phase turbulent numerical model chen et al 2011 for boundary layer flow and net sediment transport caused by velocity skewness in oscillatory sheet flow sediment transport to improve the theory and the relevant micro mechanisms about velocity skewness the qualitative approach is an application of nielsen s nielsen 1992 nielsen and guard 2010 pseudo laminar approximation of boundary layer velocity and the exponential approximation of concentration chen et al 2013 in oscillatory sheet flow the importance of key factors in oscillatory sheet flow e g velocity skewness related phase lag and wave boundary layer thickness can be clearly shown or illustrated using the qualitative approach the instantaneous variables of sediment transport such as velocity concentration mobile bed level and flux profile can be easily obtained to enhance the knowledge of velocity skewness using the two phase numerical model in view of the relevant net sediment transport rate near the initial bed induced by net boundary layer flow the present study attempts to illustrate the contribution of net boundary layer flow on the total net sediment transport over mobile bed in velocity skewed flow and the combined action of the phase lag and boundary layer flow net sediment transport rate contributed by the phase lag and the net boundary layer flow caused by velocity skewness are intensively studied using the qualitative approach and two phase model 2 qualitative approach the velocity in a turbulent wave boundary layer can be approximated as follows nielsen 1992 nielsen and guard 2010 1 v b w y t v w 1 exp 4 6 1 α i y δ δ b where the boldfaced notation v represents a complex velocity y is the vertical coordinate with origin located at the initial undisturbed bed t is the time α is the phase lead parameter at the immobile bed surface i is the imaginary unit δ is the erosion depth δb is the wave boundary layer thickness subscript w denote a wave part and subscript b denotes the boundary layer the exponential law approximation of the concentration profile considering mass conservation chen et al 2013 is applied in the present study that is 2 s y t s m exp 1 y δ where subscript m denotes the maximum value and sm 0 6 the complex sediment transport rate is calculated as follows 3 δ s y t v b w y t d y δ s m exp 1 y δ v w 1 exp 4 6 1 α i y δ δ b d y s m v w 0 exp y δ 1 exp 4 6 1 α i y δ b d y s m δ v w 1 δ b 4 6 δ 1 α i δ b α 0 is widely accepted as classical instantaneous formulas nielsen 1992 ribberink 1998 nielsen 2006 because the phase lead is actually minimal in practice in present study we define uw re v w and take the real part of eq 3 with α 0 as instantaneous transport rate q t as shown as follows 4 q t 4 6 s m δ u w 4 6 δ b δ in eq 4 the effect of the phase lag is contained in δ and a large δb corresponds to a small q for the same flows periodic q t is usually approximated by q qm sign uw uw uwm n generally n increases with the decrement of phase residual effect if the phase residual is extremely large that periodic δ is almost constant eq 4 consequently becomes q t 4 6smuw δ m 4 6 δb δ m corresponding to q qm uw uwm and n 1 if the phase residual is extremely small to be neglected δ cuw 2 asano 1992 zala flores and sleath 1998 where c is a parameter thus eq 4 becomes q t 4 6smc 2 uw 5 δb 4 6cuw 2 at the flow reversal it is almost q qm uw uwm 5 corresponding to n 5 meanwhile n corresponds to 3 5 because uw 2 exists in the denominator eq 4 is not valid for a progressive wave or a wave current condition because of eq 1 for a progressive wave the instantaneous form of net boundary layer flow caused by wave averaged stress should be added for a wave current condition the instantaneous form of net boundary layer flow and interaction between wave and current should be calibrated progressive wave is not investigated in the present study because of the limitation in qualitative approach 3 net sediment transport rate validation eq 4 cannot be directly used for the illustration of net sediment transport generation in present study because the qualitative δ and δb including phase lag and acceleration are lacking thus the two phase model chen et al 2011 is applied to cover the limitation and supply particular results for illustration in the qualitative approach table 1 shows the computed 162 data beneath pure velocity skewed flow and current where uwm is the maximum wave velocity and d is the sediment diameter the free stream velocity is u t uw t u where u is the current velocity the current pressure gradient is iterated such that the averaged velocity at the edge of the boundary layer equals u holmedal and myrhaug 2006 table 1 only refers to the pure velocity skewed uw t to isolate the effect of velocity skewness uw t for cnoidal wave is approximated by abreu et al 2010 and the second order stokes wave is given by 5 u w t u 1 cos σ t 0 21 t u 2 cos 2 σ t 0 21 t where σ 2π t is the angular frequency t is the wave period u 1 and u 2 are the first and second harmonic amplitudes respectively and a phase shift of 0 21t makes uw 0 0 fig 1 shows uw t and a t duw t dt of eq 5 where the positive and negative directions denote onshore and offshore respectively and subscript c and t denote wave crest and wave trough respectively fig 2 shows the net sediment transport rate q validation where the angle brackets denote the periodic average of and of c denote oscillatory flow and oscillatory flow combined with current respectively exp and comp denote experiments and computations respectively most data are around the solid line representing the accurate prediction 4 results and discussions considering that the net transport rates are predicted reasonably well we must determine whether the underlying processes are reasonably and actually predicted we mainly center on the contribution of phase lag and boundary layer flow caused by velocity skewness based on second order stokes flow experiments o donoghue and wright 2004a b with t 5 s 7 5 s d 0 13 mm 0 46 mm uwm 1 5 m s specific gravity 2 65 and r ut uc 0 9 1 5 0 6 in the following sediment flux is ϕ y t ub y t s y t 4 1 phase lag effect phase lag can be classified into three namely 1 phase shift between erosion depth and free stream velocity 2 phase residual of sediment entrained during the positive half period but transported after flow reversal and 3 phase lead α at the immobile bed surface in eq 1 phase shift and phase residual are evident in the instantaneous bottom and top of the sheet flow layer in fig 3 the bottom of the sheet flow layer is defined at s 0 99sm li et al 2008 the top of the sheet flow layer is defined by dohmen janssen dohmen janssen 1999 at s 0 08 because the averaged sediment spacing is about one diameter the phase lag effect is obvious with large um short t and small d dibajnia and watanabe 1992 o donoghue and wright 2004a b dohmen janssen et al 2002 van der a et al 2010 in fig 3 the phase shift i e the minimal δ moment falls behind t t 0 for fine d 0 13 mm cases i e fa5010 and fa7515 is about 0 08t t which is larger than 0 05t t 0 01t t for d 0 27 mm and 0 46 mm i e ma5010 ca7515 the phase residual for fine d 0 13 mm is also larger than that of d 0 27 mm and d 0 46 mm δ represents the entrained sediment amount δ at negative flow stage is close to that at flow crest in fa5010 and fa7515 due to large phase residual whereas δ at the negative flow is obviously smaller than that at flow crest in ma5010 ca7515 the two phase result agrees with the experiment well in the δ magnitude the phase shift and phase residual tendency if the phase residual is zero the minimal δ at flow reversal is zero and δ d can be proposed linearly proportional to θ asano 1992 zala flores and sleath 1998 in oscillatory sheet flows such linear relation cannot be reused in fa5010 and fa7515 with large phase residual fig 4 shows the experimental exp and computed comp period averaged ϕ following fig 3 the maximum ϕ suggests that the sheet flow transport mainly occurs near the initial bed where the net boundary layer flow is most evident in the experiment ϕ of fine d 0 13 mm fig 4 near the bed is clearly negative whereas the value of medium d 0 27 mm and coarse d 0 46 mm cases i e ma and ca are generally positive negative ϕ of the fine d 0 13 mm is attributed to large phase lag effect in the previous studies o donoghue and wright 2004b liu and sato 2006 li et al 2008 van der a et al 2010 i e a large amount of sediment entrained in the onshore duration remains suspended with low falling velocity when flow reverses and it is thought to be carried away by the offshore flow to contribute negative ϕ when t increases from 5 s to 7 5 s negative ϕ slightly decreases due to decreased phase lag positive ϕ of the d 0 27 mm and d 0 46 mm is attributed to small phase lag effect when the phase lag effect is small in the flow with large d the entrained sediment amount is proportional to u 2 and the multiplication to u is the instantaneous total flux u 3 thus the averaged total flux is proportional to velocity skewness u 3 ribberink and chen 1993 above the initial bed the data for the medium d 0 27 mm and coarse d 0 46 mm cases are scattered because of concentration measurement uncertainties high concentration measurement is a challenge and conservation of sediment concentration is not considered in the initial data malarkey et al 2009 however the flux profile predicted by the proposed two phase numerical model almost passes the center of the data and has the same shape as that fitted in o donoghue and wright 2004b over all the two phase numerical result agrees reasonably well with the experimental data thus the integration of averaged flux agrees well with net sediment transport rate data fig 2 the abovementioned phase lead effect can be seen in figs 5 6 which explain the instantaneous ϕ of fa5010 and ma5010 the four time sections correspond to the flow reversal t t 0 0 t t 0 42 flow crest t t 0 21 and flow trough t t 0 71 in fig 1 evident ϕ exists near the flow reversal at t t 0 0 and t t 0 42 figs 5 6 due to the bottom shear stress and velocity phase lead to free stream velocity maximum ϕ at t t 0 0 and t t 0 42 in fig 5 is larger than that in fig 6 because the phase residual for d 0 13 mm is larger than d 0 27 mm for the fine fa5010 fig 5 with large phase residual the immobile bed level is almost constant at y 5 mm in the wave period because the sediment picked up near flow crest can barely fall down even if velocity decreases for the medium ma5010 fig 6 with small phase residual the immobile bed level at flow crest is obviously larger than the other time because most sediment picked up at flow crest fall down at flow reversal generally the two phase result agrees with the conclusion that sheet flow transport mainly occurs near the initial bed however the agreement to experimental fa5010 is not as good as that of ma5010 the two phase computation is reasonable considering the concentration measurement uncertainties and the concentration conservation thus the two phase modeling transport rate as the integration of ϕ is very close to experimental data fig 2 4 2 net boundary layer flow beneath second order stokes flow net mean wave boundary layer flow is caused by asymmetric turbulent wave boundary layer yuan and madsen 2015 van der a et al 2011 fig 7 shows the instantaneous wave boundary layer thickness δ commonly defined as the distance from the immobile bed surface to the elevation at which the velocity defect equals 0 01uwm the commonly defined δ is positively correlated to δb and it equals to δb used in eq 1 at the flow crest and flow trough because the wave boundary velocity is damped oscillation propagating upwards there are two values after flow reversal van der a et al 2011 the top one is developed in the previous half period and the bottom one is new developed in the current half period the bottom δ grows as the increment of flow velocity and disappears near next flow peak in agreement with the measurement van der a et al 2011 the wave boundary layer developments of positive and negative flow stages are different due to the velocity skewness and δb near the flow crest is larger than that near the flow trough this is reasonable because the onshore shear stress and roughness are much larger than that at offshore wilson et al 1995 and δb is proportional to roughness fredsøe and deigaard 1992 van rijn 1993 we define ub y t as the boundary layer flow velocity asymmetric development of δb caused by velocity skewness leads to net boundary layer flow ub which is shown in fig 8 based on y δ and y respectively if δb is constant ub re v bw based on y δ in eq 1 is 0 at every location in a pure velocity skewed flow δb near the flow crest is larger than that near the flow trough comparing to a constant δb case in fig 8 ub re v bw in eq 1 based on y δ is relatively reduced near the flow crest and relatively increased near the flow trough to lead to negative ub negative ub based on y δ is observed by ref o donoghue and wright 2004b and illustrated by asymmetric turbulence development effect yuan and madsen 2015 van der a et al 2011 and predicted by the two phase model negative ub is also obtained by the two phase model based on y coordinate in fig 8 but it is unable to penetrate fully the sheet flow layer with obviously positive value in the sheet flow layer bottom this phenomenon can be illustrated by the mobile bed effect ruessink et al 2011 ribberink et al 2008 that the lowest level mobilized by a strong flow crest is not mobilized during the negative flow stage with weak flow strength fig 3 the phase residual effect decreases when d increases from 0 13 mm to 0 27 mm leading to an increment of the lowest level difference between flow crest and flow trough and the bottom positive ub based on y fig 8 2 ub caused by velocity skewness corresponds to net sediment flux ϕc ub s the two phase model computed ϕc is also shown in fig 8 together with wave related ϕw ub s and total flux ϕt ϕc ϕw where ub ub ub and s s s for fine fa5010 in fig 8 1 negative ϕc almost penetrates the whole sheet flow layer corresponding to ub ϕt is almost negative and penetrates fully the sheet flow layer closely coinciding with ϕc which means negative ϕt is mainly caused by ub this result is different from the previous conclusion that negative ϕt is caused by large phase residual o donoghue and wright 2004b liu and sato 2006 li et al 2008 van der a et al 2010 extremely large phase residual almost makes constant erosion depth and concentration i e δ δ s s δ 0 and s 0 thus ϕt ub s ub s ub s ϕc which denotes ub or ϕc determining ϕt in the large phase residual case for ma5010 the phase residual is smaller than that of d 0 13 mm case and the mobile bed effect is larger than that of d 0 13 mm case following ub ϕc is mainly positive near the bottom of sheet flow layer and is negative above the initial bed fig 8 2 the bottom positive ϕc is larger than the upper negative ϕc because s below the initial bed level is larger than that above the level ϕw is obviously large above the initial bed because variation of ub is larger above the initial bed than below the bed and s is obvious when phase residual is small generally ϕt is dominated by ub or ϕc in the bottom of sheet flow layer and by ϕw above the initial bed for ma5010 negative ub caused by the difference of δb between positive and negative flows plays a minor role in ϕt in this small phase residual case because it is unable to penetrate fully the sheet flow layer the effect of velocity skewness is the contribution of the phase lag and asymmetric development of the boundary layer 4 3 net boundary layer flow beneath second order stokes flow and current boundary layer fully penetrates the water tunnel once a current is exerted ub beneath second order stokes flow and current based on y δ and y are shown in fig 9 where serial u 0 1 0 05 0 02 0 02 0 05 0 1 m s are exerted to previous fa5010 and ma5010 u 0 m s at y 0 2 m experimental data for current cases are not available o donoghue and wright 2004a b so fa5010 and ma5010 u 0 m s are plotted again in fig 9 when u is weak 0 02 m s and 0 02 m s the present ub profile based on y δ are similar to the experimental ribberink and al salem 1995 and numerical holmedal and myrhaug 2006 weak current studies over a fixed bed when u increases to u 0 1 m s wavy distribution is obtained in ub profile based on y δ fig 9 1 2 due to the combination and interaction of asymmetric oscillatory boundary layer flow and exerted current ub next to y δ 0 is consistently negative when u is negative fig 9 1 2 but it is positive and different from fig 8 when u increases to 0 1 m s positive ub based on y also exists at the bottom of the sheet flow layer due to the mobile bed effect fig 9 3 4 for positive and negative u and its strength of 0 27 mm case is larger than 0 13 mm case due to small phase residual to analyze the wavy ub near y δ 0 in fig 9 we simply define a current component as ubc y δ u κ log y δ ks 1 where u is the period averaged current friction velocity obtained by ubc 0 2 u κ 0 4 is the karman constant ks δ is the period averaged roughness over mobile bed given by half of the flow layer thickness wilson et al 1995 which almost equals to the erosion depth according to chen et al 2013 in agreement with fig 3 in addition we define ubw ub ubc as an asymmetric wave boundary flow component fig 10 shows ubw ubc and ub beneath second order stokes flow and the current u 0 1 m s based on y δ positive ub overshoot next to y δ 0 for u 0 1 m s fig 10 1 2 is caused by the strong current component ubc ubw profile in fig 10 has the same shape as ub profile based on y δ in fig 8 the extreme ubw in fig 10 1 2 is stronger than the extreme ub based on y δ in fig 8 because the turbulent boundary layer difference between positive and negative flow stages are enhanced by u 0 1 m s considering the asymmetric turbulence development effect yuan and madsen 2015 van der a et al 2011 positive u increases the turbulence bottom shear stress and roughness in the positive flow stage and reduces their strength in the negative flow stage u 0 1 m s is opposite to 0 1 m s and the turbulent boundary layer asymmetry is reduced so the extreme ubw in fig 10 3 4 is weaken than the extreme ub in fig 8 to illustrate the net sediment transport with a current fig 11 shows ϕ beneath second order stokes flow and the current u 0 1 m s for d 0 13 mm and 0 27 mm u 0 1 m s generates considerably positive ub based on y fig 11 1 2 and increases the flow asymmetry r ut uc 0 8 1 6 0 5 for 0 13 mm case fig 11 1 uc 1 6 m s uwm 1 5 m s gives a larger phase residual than that of fig 8 1 so ϕt is also decided by ub or ϕc and positively penetrates the full flow layer for 0 27 mm case with a small phase residual ϕt is still dominated by ϕc in the flow layer bottom and by ϕw above the initial bed as ma5010 fig 8 2 u 0 1 m s reduces the flow asymmetry r ut uc 1 0 1 4 0 71 and mobile bed effect fig 11 3 4 so the corresponding negative ub almost penetrates fully the flow layer based on y for 0 13 mm case in fig 11 3 ϕt is negative and still decided by negative ϕc due to a relatively large phase residual for 0 27 mm case in fig 11 4 the bottom positive ub decreases as the decrement of mobile bed effect comparing to fig 8 2 ϕc dominates ϕt in the bottom of sheet flow layer and ϕw is still very important above the initial bed with relatively obvious s due to small phase residual 4 4 instantaneous sediment transport rate the generation of the net sediment transport has already been given fig 12 shows q t based on figs 3 4 where the right column is the comparison with sign uw uw uwm n and left column is the comparison with instantaneous formulas of 1 ribberink 1998 without acceleration skewness and 2 nielsen 2006 and gonzalez rodriguez and madsen 2007 considering acceleration skewness sign uw is neglected for n 2 3 to shorten the legend the q t magnitude decreases with the increment of d due to the decrement of the suspended sediment transport from fa5010 to ca5010 fig 12 1 3 5 corresponding to the decrement of erosion depth in fig 3 the large phase residual of fa5010 makes δ in the negative flow stage close to the positive flow stage and q qm near the flow trough t t 0 5 0 8 is relatively close to that near the flow crest t t 0 15 0 25 the instantaneous formulas cannot be used for fa5010 without suspended sediment and phase residual phase residual is important but cannot sufficiently generate negative ϕt based on the discussions in fig 8 δb during the negative flow is smaller than that during the positive flow fig 7 1 thereby leading to a relatively large ub in the negative flow in eq 1 thus ub is negative o donoghue and wright 2004b in fig 8 and negative ϕt is generated based on the nearly constant δ this process is in agreement with fig 12 2 with a comparison of uw uwm n supposing a constant δb n 1 is used with extremely large phase residual in eq 4 when periodic δ is constant thus q qm 0 t q q m d t 0 t u w u w m d t 0 and negative q will not appear however as represented by uw 4 6 δb δ in eq 4 negative ub near the flow trough t t 0 5 0 8 in the case of fa5010 is relatively enlarged because of the small δb in eq 1 which results in q qm uw uwm near the flow trough t t 0 5 0 8 in eq 4 fig 12 2 thus q is negative that is q qm 0 t q q m d t 0 t u w u w m d t 0 which agrees with the negative ϕt in fig 8 1 in the other cases the instantaneous formulas can be adequately used because the suspension amount and phase residual are considerably smaller than the fa5010 case thereby leading to a small amplitude of q variation and a significantly stronger q at the positive flow crest than those at the negative flow trough because uwc uwt for the coarse ca5010 with small phase residual the maximum δ near the flow peak t t 0 15 0 25 is significantly greater than that near the flow trough t t 0 5 0 8 with almost δ d θ q qm sign uw uw uwm n corresponds to n 3 5 according to small phase residual case in eq 4 and a relatively small uw corresponds to a relatively large n the asymmetric boundary layer relatively enlarges the negative ub near the flow trough t t 0 5 0 8 and makes n in fig 12 6 close to 3 which is close to the instantaneous formulas without phase residual the experimental data of ca5010 agree with the two phase model fig 12 5 that is in accordance with n 3 fig 12 6 relatively enlarged negative ub near the flow trough t t 0 5 0 8 only corresponds to small δ agreeing with fig 8 2 that the negative ub plays a minor role in the total sediment transport with small phase residual a considerably stronger flow crest q than that in the flow trough is generated as shown in fig 12 5 6 and leads to positive q that is 0 t q q m d t 0 t u w u w m 3 d t 0 the approximation exponent of medium ma5010 is between the fine and coarse cases in fig 12 3 4 and q is also positive generally the two phase model agrees with experiment considerably well whereas instantaneous formulas cannot correctly predict the tendency with d or obviously negative q due to large phase residual and asymmetric boundary layer development q t qm beneath the second order stokes flow and the current u 0 1 m s are compared with sign u u um n in fig 13 for d 0 13 mm cases in fig 13 1 2 q qm are still close to u um with large phase residual in a velocity skewed flow combined with a current the asymmetric turbulent wave boundary flow effect still exists fig 10 and relatively enhances the negative ub and q near the flow trough t t 0 5 0 8 and it is represented by uw 4 6 δb δ in eq 4 therefore q qm near the flow trough t t 0 5 0 8 in fig 13 1 2 are lower than u um as that in fig 12 2 negative q can be easily attributed to q qm 0 t u u m d t 0 in fig 13 2 which agrees with the negative ϕt in fig 11 3 for d 0 46 mm cases in fig 13 5 6 q qm are still close to u um 3 with minimal phase residual as that in fig 12 6 and in agreement with ribberink and chen 1993 and eq 4 without phase residual for d 0 27 mm cases in fig 13 3 4 an acceptable exponent n is approximately 2 which is also between 0 13 mm and 0 46 mm cases as that in fig 12 positive q can be simply attributed to q qm 0 t u u m u u m d t 0 in fig 13 3 which agrees with the positive ϕt in fig 11 2 5 conclusions net sediment transport beneath pure velocity skewed oscillatory sheet flow and current is studied with a new derived qualitative approach and a two phase model the qualitative approach is obtained based on the pseudo laminar approximation of boundary layer velocity and exponential approximation of concentration considering mass conservation the proposed qualitative approach shows the importance of wave boundary layer thickness in sediment transport the exponent for instantaneous sediment transport rate against exponential function of velocity i e q qm sign u u um n can generally be unified and summarized to decrease with the increment of phase residual the two phase model well predicts net sediment transport beneath pure velocity skewed oscillatory sheet flow and current and is better than formulas for instantaneous sediment transport rate the effects of phase lag and boundary layer flow for sediment transport in pure velocity skewed oscillatory sheet flow combing with current are studied the wave boundary layer developments of positive and negative flow stages are different due to velocity skewness and the wave boundary layer thickness near the flow crest is larger than that near the flow trough in pure velocity skewed flow the asymmetric wave boundary layer leads to negative net wave boundary layer flow component due to velocity skewness an exerted positive current enhances the turbulent wave boundary layer asymmetry to enlarge the negative net wave boundary layer flow component and a negative current reduces the turbulent wave boundary layer asymmetry to reduce the net wave boundary layer flow component in a large phase residual case the periodic erosion depth and concentration distribution are almost constant to lead to q qm u um and the total net boundary layer flow almost decides the net sediment transport in a small phase residual case the net sediment transport is dominated by the total net boundary layer flow in the sheet flow layer bottom and by the wave related transport above the initial bed where periodic concentration and velocity variations are obvious in this case the instantaneous sediment transport can be approximated by q qm u um 3 which is close to existing instantaneous formulas without phase residual in conclusion for sediment transport beneath pure velocity skewed oscillatory sheet flow and current the effect of velocity skewness is the contributions of phase lag and asymmetric wave boundary layer development between positive and negative flows acknowledgments the project is supported by national natural science foundation of china grant no 51609244 and 51779258 and national science technology support plan of china grant no 2015bad20b01 
909,this study utilizes a qualitative approach and a two phase numerical model to investigate net sediment transport caused by velocity skewness beneath oscillatory sheet flow and current the qualitative approach is derived based on the pseudo laminar approximation of boundary layer velocity and exponential approximation of concentration the two phase model can obtain well the instantaneous erosion depth sediment flux boundary layer thickness and sediment transport rate it can especially illustrate the difference between positive and negative flow stages caused by velocity skewness which is considerably important in determining the net boundary layer flow and sediment transport direction the two phase model also explains the effect of sediment diameter and phase lag to sediment transport by comparing the instantaneous type formulas to better illustrate velocity skewness effect in previous studies about sheet flow transport in pure velocity skewed flows net sediment transport is only attributed to the phase lag effect in the present study with the qualitative approach and two phase model phase lag effect is shown important but not sufficient for the net sediment transport beneath pure velocity skewed flow and current while the asymmetric wave boundary layer development between positive and negative flow stages also contributes to the sediment transport keywords boundary layer thickness qualitative approach sediment transport two phase model velocity skewness 1 introduction nearshore oscillatory flows induced by waves are usually velocity skewed with peaked narrow crest and flat wide trough in wave propagation and shoaling the interaction between sediment and fluid and the collision between sediment particles are obviously strong in the sheet flow layer with large shields parameter θ 0 8 1 0 and high sediment volumetric concentration s 0 08 0 65 an additional current is usually imposed to the velocity skewed oscillatory sheet flow and produces more complex boundary layer flow and sediment transport process accurate prediction of the proposed net sediment transport rates is important in coastal engineering however the relevant micro mechanism is not clearly understood accompanied with probable large boundary layer flow and net transport rate that are considerably essential for environmental and morphological studies thus more studies for boundary layer flow and net sediment transport in velocity skewed oscillatory sheet flows are needed the sediment transport measurements of oscillatory sheet flow referring to velocity skewness over the last quarter century include the pure velocity skewed flows dibajnia and watanabe 1992 ribberink and al salem 1994 ahmed and sato 2003 o donoghue and wright 2004a b dong et al 2013 mixed velocity skewed and acceleration skewed flows ruessink et al 2011 silva et al 2011 lanckriet et al 2014 van der zanden et al 2015 and velocity skewed flows combined with current dong et al 2013 ribberink and al salem 1995 dibajnia 1991 the net boundary layer flow is negative in pure velocity skewed flows o donoghue and wright 2004b ribberink and al salem 1995 however relevant net sediment transport caused by high velocity skewness is usually classified according to the influence of the phase lag without net boundary layer flow negative net sediment transport rate is observed and thought generated when the phase lag is considerably clear with small sediment diameter short period or large velocity amplitude dibajnia and watanabe 1992 o donoghue and wright 2004a b dong et al 2013 positive net sediment transport rate is observed and thought generated when the phase lag effect is small on the basis of different phase lag effects ribberink and al salem 1995 found correlation between net sediment transport rate and velocity skewness hassan and ribberink 2010 enhanced the correlation and added sediment size and velocity amplitude influence phase lag parameter is introduced dibajnia and watanabe 1992 dohmen janssen 1999 dohmen janssen et al 2002 representing the time ratio between sediment falling down in sheet flow layer and the wave period it was first adopted in the semi unsteady type empirical model dibajnia and watanabe 1992 for net sediment transport the phase lag theory of sediment movement in velocity skewed oscillatory sheet flow has been established by the widely used model dibajnia and watanabe 1992 however the result accuracy predicted by such theory remains insufficient for a wide range of flow conditions more complex parameters around phase lag are considered in certain ways ahmed and sato 2003 dong et al 2013 dohmen janssen et al 2002 silva et al 2006 van der a et al 2013 and difficulties exist in determining appropriate model parameters however some key factors are still not considered such as the asymmetric development of boundary layer thickness between positive and negative flow stages and the net boundary layer flow caused by velocity skewness the most observable area of net boundary layer flow is near the initial bed where sediment concentration is high in the asymmetric oscillatory flows ruessink et al 2009 and progressing waves kranenburg et al 2012 and when an additional current is exerted holmedal and myrhaug 2006 yuan and madsen 2015 holmedal et al 2013 nevertheless the contribution of net boundary layer flow in the total net sediment transport over a mobile bed in velocity skewed flow remains unclear as well as the combined action of net boundary layer flow and phase lag in the last decade two phase models liu and sato 2006 li et al 2008 chen et al 2011 lee et al 2016 have successfully predicted sediment transport in velocity skewed oscillatory sheet flows which can capture the complex fluid particle and inter particle interactions however the generation of net sediment transport by velocity skewness is still mainly attributed to the phase lag effect the present study develops a qualitative approach and applies the two phase turbulent numerical model chen et al 2011 for boundary layer flow and net sediment transport caused by velocity skewness in oscillatory sheet flow sediment transport to improve the theory and the relevant micro mechanisms about velocity skewness the qualitative approach is an application of nielsen s nielsen 1992 nielsen and guard 2010 pseudo laminar approximation of boundary layer velocity and the exponential approximation of concentration chen et al 2013 in oscillatory sheet flow the importance of key factors in oscillatory sheet flow e g velocity skewness related phase lag and wave boundary layer thickness can be clearly shown or illustrated using the qualitative approach the instantaneous variables of sediment transport such as velocity concentration mobile bed level and flux profile can be easily obtained to enhance the knowledge of velocity skewness using the two phase numerical model in view of the relevant net sediment transport rate near the initial bed induced by net boundary layer flow the present study attempts to illustrate the contribution of net boundary layer flow on the total net sediment transport over mobile bed in velocity skewed flow and the combined action of the phase lag and boundary layer flow net sediment transport rate contributed by the phase lag and the net boundary layer flow caused by velocity skewness are intensively studied using the qualitative approach and two phase model 2 qualitative approach the velocity in a turbulent wave boundary layer can be approximated as follows nielsen 1992 nielsen and guard 2010 1 v b w y t v w 1 exp 4 6 1 α i y δ δ b where the boldfaced notation v represents a complex velocity y is the vertical coordinate with origin located at the initial undisturbed bed t is the time α is the phase lead parameter at the immobile bed surface i is the imaginary unit δ is the erosion depth δb is the wave boundary layer thickness subscript w denote a wave part and subscript b denotes the boundary layer the exponential law approximation of the concentration profile considering mass conservation chen et al 2013 is applied in the present study that is 2 s y t s m exp 1 y δ where subscript m denotes the maximum value and sm 0 6 the complex sediment transport rate is calculated as follows 3 δ s y t v b w y t d y δ s m exp 1 y δ v w 1 exp 4 6 1 α i y δ δ b d y s m v w 0 exp y δ 1 exp 4 6 1 α i y δ b d y s m δ v w 1 δ b 4 6 δ 1 α i δ b α 0 is widely accepted as classical instantaneous formulas nielsen 1992 ribberink 1998 nielsen 2006 because the phase lead is actually minimal in practice in present study we define uw re v w and take the real part of eq 3 with α 0 as instantaneous transport rate q t as shown as follows 4 q t 4 6 s m δ u w 4 6 δ b δ in eq 4 the effect of the phase lag is contained in δ and a large δb corresponds to a small q for the same flows periodic q t is usually approximated by q qm sign uw uw uwm n generally n increases with the decrement of phase residual effect if the phase residual is extremely large that periodic δ is almost constant eq 4 consequently becomes q t 4 6smuw δ m 4 6 δb δ m corresponding to q qm uw uwm and n 1 if the phase residual is extremely small to be neglected δ cuw 2 asano 1992 zala flores and sleath 1998 where c is a parameter thus eq 4 becomes q t 4 6smc 2 uw 5 δb 4 6cuw 2 at the flow reversal it is almost q qm uw uwm 5 corresponding to n 5 meanwhile n corresponds to 3 5 because uw 2 exists in the denominator eq 4 is not valid for a progressive wave or a wave current condition because of eq 1 for a progressive wave the instantaneous form of net boundary layer flow caused by wave averaged stress should be added for a wave current condition the instantaneous form of net boundary layer flow and interaction between wave and current should be calibrated progressive wave is not investigated in the present study because of the limitation in qualitative approach 3 net sediment transport rate validation eq 4 cannot be directly used for the illustration of net sediment transport generation in present study because the qualitative δ and δb including phase lag and acceleration are lacking thus the two phase model chen et al 2011 is applied to cover the limitation and supply particular results for illustration in the qualitative approach table 1 shows the computed 162 data beneath pure velocity skewed flow and current where uwm is the maximum wave velocity and d is the sediment diameter the free stream velocity is u t uw t u where u is the current velocity the current pressure gradient is iterated such that the averaged velocity at the edge of the boundary layer equals u holmedal and myrhaug 2006 table 1 only refers to the pure velocity skewed uw t to isolate the effect of velocity skewness uw t for cnoidal wave is approximated by abreu et al 2010 and the second order stokes wave is given by 5 u w t u 1 cos σ t 0 21 t u 2 cos 2 σ t 0 21 t where σ 2π t is the angular frequency t is the wave period u 1 and u 2 are the first and second harmonic amplitudes respectively and a phase shift of 0 21t makes uw 0 0 fig 1 shows uw t and a t duw t dt of eq 5 where the positive and negative directions denote onshore and offshore respectively and subscript c and t denote wave crest and wave trough respectively fig 2 shows the net sediment transport rate q validation where the angle brackets denote the periodic average of and of c denote oscillatory flow and oscillatory flow combined with current respectively exp and comp denote experiments and computations respectively most data are around the solid line representing the accurate prediction 4 results and discussions considering that the net transport rates are predicted reasonably well we must determine whether the underlying processes are reasonably and actually predicted we mainly center on the contribution of phase lag and boundary layer flow caused by velocity skewness based on second order stokes flow experiments o donoghue and wright 2004a b with t 5 s 7 5 s d 0 13 mm 0 46 mm uwm 1 5 m s specific gravity 2 65 and r ut uc 0 9 1 5 0 6 in the following sediment flux is ϕ y t ub y t s y t 4 1 phase lag effect phase lag can be classified into three namely 1 phase shift between erosion depth and free stream velocity 2 phase residual of sediment entrained during the positive half period but transported after flow reversal and 3 phase lead α at the immobile bed surface in eq 1 phase shift and phase residual are evident in the instantaneous bottom and top of the sheet flow layer in fig 3 the bottom of the sheet flow layer is defined at s 0 99sm li et al 2008 the top of the sheet flow layer is defined by dohmen janssen dohmen janssen 1999 at s 0 08 because the averaged sediment spacing is about one diameter the phase lag effect is obvious with large um short t and small d dibajnia and watanabe 1992 o donoghue and wright 2004a b dohmen janssen et al 2002 van der a et al 2010 in fig 3 the phase shift i e the minimal δ moment falls behind t t 0 for fine d 0 13 mm cases i e fa5010 and fa7515 is about 0 08t t which is larger than 0 05t t 0 01t t for d 0 27 mm and 0 46 mm i e ma5010 ca7515 the phase residual for fine d 0 13 mm is also larger than that of d 0 27 mm and d 0 46 mm δ represents the entrained sediment amount δ at negative flow stage is close to that at flow crest in fa5010 and fa7515 due to large phase residual whereas δ at the negative flow is obviously smaller than that at flow crest in ma5010 ca7515 the two phase result agrees with the experiment well in the δ magnitude the phase shift and phase residual tendency if the phase residual is zero the minimal δ at flow reversal is zero and δ d can be proposed linearly proportional to θ asano 1992 zala flores and sleath 1998 in oscillatory sheet flows such linear relation cannot be reused in fa5010 and fa7515 with large phase residual fig 4 shows the experimental exp and computed comp period averaged ϕ following fig 3 the maximum ϕ suggests that the sheet flow transport mainly occurs near the initial bed where the net boundary layer flow is most evident in the experiment ϕ of fine d 0 13 mm fig 4 near the bed is clearly negative whereas the value of medium d 0 27 mm and coarse d 0 46 mm cases i e ma and ca are generally positive negative ϕ of the fine d 0 13 mm is attributed to large phase lag effect in the previous studies o donoghue and wright 2004b liu and sato 2006 li et al 2008 van der a et al 2010 i e a large amount of sediment entrained in the onshore duration remains suspended with low falling velocity when flow reverses and it is thought to be carried away by the offshore flow to contribute negative ϕ when t increases from 5 s to 7 5 s negative ϕ slightly decreases due to decreased phase lag positive ϕ of the d 0 27 mm and d 0 46 mm is attributed to small phase lag effect when the phase lag effect is small in the flow with large d the entrained sediment amount is proportional to u 2 and the multiplication to u is the instantaneous total flux u 3 thus the averaged total flux is proportional to velocity skewness u 3 ribberink and chen 1993 above the initial bed the data for the medium d 0 27 mm and coarse d 0 46 mm cases are scattered because of concentration measurement uncertainties high concentration measurement is a challenge and conservation of sediment concentration is not considered in the initial data malarkey et al 2009 however the flux profile predicted by the proposed two phase numerical model almost passes the center of the data and has the same shape as that fitted in o donoghue and wright 2004b over all the two phase numerical result agrees reasonably well with the experimental data thus the integration of averaged flux agrees well with net sediment transport rate data fig 2 the abovementioned phase lead effect can be seen in figs 5 6 which explain the instantaneous ϕ of fa5010 and ma5010 the four time sections correspond to the flow reversal t t 0 0 t t 0 42 flow crest t t 0 21 and flow trough t t 0 71 in fig 1 evident ϕ exists near the flow reversal at t t 0 0 and t t 0 42 figs 5 6 due to the bottom shear stress and velocity phase lead to free stream velocity maximum ϕ at t t 0 0 and t t 0 42 in fig 5 is larger than that in fig 6 because the phase residual for d 0 13 mm is larger than d 0 27 mm for the fine fa5010 fig 5 with large phase residual the immobile bed level is almost constant at y 5 mm in the wave period because the sediment picked up near flow crest can barely fall down even if velocity decreases for the medium ma5010 fig 6 with small phase residual the immobile bed level at flow crest is obviously larger than the other time because most sediment picked up at flow crest fall down at flow reversal generally the two phase result agrees with the conclusion that sheet flow transport mainly occurs near the initial bed however the agreement to experimental fa5010 is not as good as that of ma5010 the two phase computation is reasonable considering the concentration measurement uncertainties and the concentration conservation thus the two phase modeling transport rate as the integration of ϕ is very close to experimental data fig 2 4 2 net boundary layer flow beneath second order stokes flow net mean wave boundary layer flow is caused by asymmetric turbulent wave boundary layer yuan and madsen 2015 van der a et al 2011 fig 7 shows the instantaneous wave boundary layer thickness δ commonly defined as the distance from the immobile bed surface to the elevation at which the velocity defect equals 0 01uwm the commonly defined δ is positively correlated to δb and it equals to δb used in eq 1 at the flow crest and flow trough because the wave boundary velocity is damped oscillation propagating upwards there are two values after flow reversal van der a et al 2011 the top one is developed in the previous half period and the bottom one is new developed in the current half period the bottom δ grows as the increment of flow velocity and disappears near next flow peak in agreement with the measurement van der a et al 2011 the wave boundary layer developments of positive and negative flow stages are different due to the velocity skewness and δb near the flow crest is larger than that near the flow trough this is reasonable because the onshore shear stress and roughness are much larger than that at offshore wilson et al 1995 and δb is proportional to roughness fredsøe and deigaard 1992 van rijn 1993 we define ub y t as the boundary layer flow velocity asymmetric development of δb caused by velocity skewness leads to net boundary layer flow ub which is shown in fig 8 based on y δ and y respectively if δb is constant ub re v bw based on y δ in eq 1 is 0 at every location in a pure velocity skewed flow δb near the flow crest is larger than that near the flow trough comparing to a constant δb case in fig 8 ub re v bw in eq 1 based on y δ is relatively reduced near the flow crest and relatively increased near the flow trough to lead to negative ub negative ub based on y δ is observed by ref o donoghue and wright 2004b and illustrated by asymmetric turbulence development effect yuan and madsen 2015 van der a et al 2011 and predicted by the two phase model negative ub is also obtained by the two phase model based on y coordinate in fig 8 but it is unable to penetrate fully the sheet flow layer with obviously positive value in the sheet flow layer bottom this phenomenon can be illustrated by the mobile bed effect ruessink et al 2011 ribberink et al 2008 that the lowest level mobilized by a strong flow crest is not mobilized during the negative flow stage with weak flow strength fig 3 the phase residual effect decreases when d increases from 0 13 mm to 0 27 mm leading to an increment of the lowest level difference between flow crest and flow trough and the bottom positive ub based on y fig 8 2 ub caused by velocity skewness corresponds to net sediment flux ϕc ub s the two phase model computed ϕc is also shown in fig 8 together with wave related ϕw ub s and total flux ϕt ϕc ϕw where ub ub ub and s s s for fine fa5010 in fig 8 1 negative ϕc almost penetrates the whole sheet flow layer corresponding to ub ϕt is almost negative and penetrates fully the sheet flow layer closely coinciding with ϕc which means negative ϕt is mainly caused by ub this result is different from the previous conclusion that negative ϕt is caused by large phase residual o donoghue and wright 2004b liu and sato 2006 li et al 2008 van der a et al 2010 extremely large phase residual almost makes constant erosion depth and concentration i e δ δ s s δ 0 and s 0 thus ϕt ub s ub s ub s ϕc which denotes ub or ϕc determining ϕt in the large phase residual case for ma5010 the phase residual is smaller than that of d 0 13 mm case and the mobile bed effect is larger than that of d 0 13 mm case following ub ϕc is mainly positive near the bottom of sheet flow layer and is negative above the initial bed fig 8 2 the bottom positive ϕc is larger than the upper negative ϕc because s below the initial bed level is larger than that above the level ϕw is obviously large above the initial bed because variation of ub is larger above the initial bed than below the bed and s is obvious when phase residual is small generally ϕt is dominated by ub or ϕc in the bottom of sheet flow layer and by ϕw above the initial bed for ma5010 negative ub caused by the difference of δb between positive and negative flows plays a minor role in ϕt in this small phase residual case because it is unable to penetrate fully the sheet flow layer the effect of velocity skewness is the contribution of the phase lag and asymmetric development of the boundary layer 4 3 net boundary layer flow beneath second order stokes flow and current boundary layer fully penetrates the water tunnel once a current is exerted ub beneath second order stokes flow and current based on y δ and y are shown in fig 9 where serial u 0 1 0 05 0 02 0 02 0 05 0 1 m s are exerted to previous fa5010 and ma5010 u 0 m s at y 0 2 m experimental data for current cases are not available o donoghue and wright 2004a b so fa5010 and ma5010 u 0 m s are plotted again in fig 9 when u is weak 0 02 m s and 0 02 m s the present ub profile based on y δ are similar to the experimental ribberink and al salem 1995 and numerical holmedal and myrhaug 2006 weak current studies over a fixed bed when u increases to u 0 1 m s wavy distribution is obtained in ub profile based on y δ fig 9 1 2 due to the combination and interaction of asymmetric oscillatory boundary layer flow and exerted current ub next to y δ 0 is consistently negative when u is negative fig 9 1 2 but it is positive and different from fig 8 when u increases to 0 1 m s positive ub based on y also exists at the bottom of the sheet flow layer due to the mobile bed effect fig 9 3 4 for positive and negative u and its strength of 0 27 mm case is larger than 0 13 mm case due to small phase residual to analyze the wavy ub near y δ 0 in fig 9 we simply define a current component as ubc y δ u κ log y δ ks 1 where u is the period averaged current friction velocity obtained by ubc 0 2 u κ 0 4 is the karman constant ks δ is the period averaged roughness over mobile bed given by half of the flow layer thickness wilson et al 1995 which almost equals to the erosion depth according to chen et al 2013 in agreement with fig 3 in addition we define ubw ub ubc as an asymmetric wave boundary flow component fig 10 shows ubw ubc and ub beneath second order stokes flow and the current u 0 1 m s based on y δ positive ub overshoot next to y δ 0 for u 0 1 m s fig 10 1 2 is caused by the strong current component ubc ubw profile in fig 10 has the same shape as ub profile based on y δ in fig 8 the extreme ubw in fig 10 1 2 is stronger than the extreme ub based on y δ in fig 8 because the turbulent boundary layer difference between positive and negative flow stages are enhanced by u 0 1 m s considering the asymmetric turbulence development effect yuan and madsen 2015 van der a et al 2011 positive u increases the turbulence bottom shear stress and roughness in the positive flow stage and reduces their strength in the negative flow stage u 0 1 m s is opposite to 0 1 m s and the turbulent boundary layer asymmetry is reduced so the extreme ubw in fig 10 3 4 is weaken than the extreme ub in fig 8 to illustrate the net sediment transport with a current fig 11 shows ϕ beneath second order stokes flow and the current u 0 1 m s for d 0 13 mm and 0 27 mm u 0 1 m s generates considerably positive ub based on y fig 11 1 2 and increases the flow asymmetry r ut uc 0 8 1 6 0 5 for 0 13 mm case fig 11 1 uc 1 6 m s uwm 1 5 m s gives a larger phase residual than that of fig 8 1 so ϕt is also decided by ub or ϕc and positively penetrates the full flow layer for 0 27 mm case with a small phase residual ϕt is still dominated by ϕc in the flow layer bottom and by ϕw above the initial bed as ma5010 fig 8 2 u 0 1 m s reduces the flow asymmetry r ut uc 1 0 1 4 0 71 and mobile bed effect fig 11 3 4 so the corresponding negative ub almost penetrates fully the flow layer based on y for 0 13 mm case in fig 11 3 ϕt is negative and still decided by negative ϕc due to a relatively large phase residual for 0 27 mm case in fig 11 4 the bottom positive ub decreases as the decrement of mobile bed effect comparing to fig 8 2 ϕc dominates ϕt in the bottom of sheet flow layer and ϕw is still very important above the initial bed with relatively obvious s due to small phase residual 4 4 instantaneous sediment transport rate the generation of the net sediment transport has already been given fig 12 shows q t based on figs 3 4 where the right column is the comparison with sign uw uw uwm n and left column is the comparison with instantaneous formulas of 1 ribberink 1998 without acceleration skewness and 2 nielsen 2006 and gonzalez rodriguez and madsen 2007 considering acceleration skewness sign uw is neglected for n 2 3 to shorten the legend the q t magnitude decreases with the increment of d due to the decrement of the suspended sediment transport from fa5010 to ca5010 fig 12 1 3 5 corresponding to the decrement of erosion depth in fig 3 the large phase residual of fa5010 makes δ in the negative flow stage close to the positive flow stage and q qm near the flow trough t t 0 5 0 8 is relatively close to that near the flow crest t t 0 15 0 25 the instantaneous formulas cannot be used for fa5010 without suspended sediment and phase residual phase residual is important but cannot sufficiently generate negative ϕt based on the discussions in fig 8 δb during the negative flow is smaller than that during the positive flow fig 7 1 thereby leading to a relatively large ub in the negative flow in eq 1 thus ub is negative o donoghue and wright 2004b in fig 8 and negative ϕt is generated based on the nearly constant δ this process is in agreement with fig 12 2 with a comparison of uw uwm n supposing a constant δb n 1 is used with extremely large phase residual in eq 4 when periodic δ is constant thus q qm 0 t q q m d t 0 t u w u w m d t 0 and negative q will not appear however as represented by uw 4 6 δb δ in eq 4 negative ub near the flow trough t t 0 5 0 8 in the case of fa5010 is relatively enlarged because of the small δb in eq 1 which results in q qm uw uwm near the flow trough t t 0 5 0 8 in eq 4 fig 12 2 thus q is negative that is q qm 0 t q q m d t 0 t u w u w m d t 0 which agrees with the negative ϕt in fig 8 1 in the other cases the instantaneous formulas can be adequately used because the suspension amount and phase residual are considerably smaller than the fa5010 case thereby leading to a small amplitude of q variation and a significantly stronger q at the positive flow crest than those at the negative flow trough because uwc uwt for the coarse ca5010 with small phase residual the maximum δ near the flow peak t t 0 15 0 25 is significantly greater than that near the flow trough t t 0 5 0 8 with almost δ d θ q qm sign uw uw uwm n corresponds to n 3 5 according to small phase residual case in eq 4 and a relatively small uw corresponds to a relatively large n the asymmetric boundary layer relatively enlarges the negative ub near the flow trough t t 0 5 0 8 and makes n in fig 12 6 close to 3 which is close to the instantaneous formulas without phase residual the experimental data of ca5010 agree with the two phase model fig 12 5 that is in accordance with n 3 fig 12 6 relatively enlarged negative ub near the flow trough t t 0 5 0 8 only corresponds to small δ agreeing with fig 8 2 that the negative ub plays a minor role in the total sediment transport with small phase residual a considerably stronger flow crest q than that in the flow trough is generated as shown in fig 12 5 6 and leads to positive q that is 0 t q q m d t 0 t u w u w m 3 d t 0 the approximation exponent of medium ma5010 is between the fine and coarse cases in fig 12 3 4 and q is also positive generally the two phase model agrees with experiment considerably well whereas instantaneous formulas cannot correctly predict the tendency with d or obviously negative q due to large phase residual and asymmetric boundary layer development q t qm beneath the second order stokes flow and the current u 0 1 m s are compared with sign u u um n in fig 13 for d 0 13 mm cases in fig 13 1 2 q qm are still close to u um with large phase residual in a velocity skewed flow combined with a current the asymmetric turbulent wave boundary flow effect still exists fig 10 and relatively enhances the negative ub and q near the flow trough t t 0 5 0 8 and it is represented by uw 4 6 δb δ in eq 4 therefore q qm near the flow trough t t 0 5 0 8 in fig 13 1 2 are lower than u um as that in fig 12 2 negative q can be easily attributed to q qm 0 t u u m d t 0 in fig 13 2 which agrees with the negative ϕt in fig 11 3 for d 0 46 mm cases in fig 13 5 6 q qm are still close to u um 3 with minimal phase residual as that in fig 12 6 and in agreement with ribberink and chen 1993 and eq 4 without phase residual for d 0 27 mm cases in fig 13 3 4 an acceptable exponent n is approximately 2 which is also between 0 13 mm and 0 46 mm cases as that in fig 12 positive q can be simply attributed to q qm 0 t u u m u u m d t 0 in fig 13 3 which agrees with the positive ϕt in fig 11 2 5 conclusions net sediment transport beneath pure velocity skewed oscillatory sheet flow and current is studied with a new derived qualitative approach and a two phase model the qualitative approach is obtained based on the pseudo laminar approximation of boundary layer velocity and exponential approximation of concentration considering mass conservation the proposed qualitative approach shows the importance of wave boundary layer thickness in sediment transport the exponent for instantaneous sediment transport rate against exponential function of velocity i e q qm sign u u um n can generally be unified and summarized to decrease with the increment of phase residual the two phase model well predicts net sediment transport beneath pure velocity skewed oscillatory sheet flow and current and is better than formulas for instantaneous sediment transport rate the effects of phase lag and boundary layer flow for sediment transport in pure velocity skewed oscillatory sheet flow combing with current are studied the wave boundary layer developments of positive and negative flow stages are different due to velocity skewness and the wave boundary layer thickness near the flow crest is larger than that near the flow trough in pure velocity skewed flow the asymmetric wave boundary layer leads to negative net wave boundary layer flow component due to velocity skewness an exerted positive current enhances the turbulent wave boundary layer asymmetry to enlarge the negative net wave boundary layer flow component and a negative current reduces the turbulent wave boundary layer asymmetry to reduce the net wave boundary layer flow component in a large phase residual case the periodic erosion depth and concentration distribution are almost constant to lead to q qm u um and the total net boundary layer flow almost decides the net sediment transport in a small phase residual case the net sediment transport is dominated by the total net boundary layer flow in the sheet flow layer bottom and by the wave related transport above the initial bed where periodic concentration and velocity variations are obvious in this case the instantaneous sediment transport can be approximated by q qm u um 3 which is close to existing instantaneous formulas without phase residual in conclusion for sediment transport beneath pure velocity skewed oscillatory sheet flow and current the effect of velocity skewness is the contributions of phase lag and asymmetric wave boundary layer development between positive and negative flows acknowledgments the project is supported by national natural science foundation of china grant no 51609244 and 51779258 and national science technology support plan of china grant no 2015bad20b01 
