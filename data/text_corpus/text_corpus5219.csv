index,text
26095,karstification is considered as one of the most common reasons for interbasin groundwater flow igf igf in some karst areas could be significant such that it must be accounted for in hydrologic modeling in this study the soil and water assessment tool swat was modified to explicitly account for igf in karst areas the modified model uses two conceptual models to simulate hydrologic processes in karst and non karst regions the modified model was applied in the karst dominated region in the southwest harz mountains germany multisite streamflow data and satellite derived actual evapotranspiration eta were used for model calibration results show that 1 the modified model can be satisfactorily calibrated and validated for streamflow and eta 2 the model performance for eta and streamflow at some gauging stations are highly correlated and 3 the use of satellite derived eta does not affect the model performance keywords modified swat karst interbasin groundwater flow calibration satellite derived evapotranspiration software availability name of software swat igf developer and contact address van tam nguyen nguyen iww uni hannover de institute of hydrology and water resources management leibniz universität hannover appelstraße 9a 30167 hannover germany year available 2019 availability and cost the source code is freely available at http doi org 10 5281 zenodo 3574312 language fortran 1 introduction the term karst refers to a region with distinct landscape features e g sinking streams sinkholes and springs and underground features e g underground conduits and caves in some karst regions the karst landscape features could be absent or subtle but their aquifers could be heavily karstified ford and williams 2007 karst aquifers are developed as a result of dissolution of karstifiable rocks e g limestone dolomite gypsum and rock salt the so called karstification ford and williams 2007 bögli 1980 howard 1963 karst aquifers account for about 10 to 15 of the continental area and karst groundwater is one of the sources of drinking water for approximately a quarter of the world s population ford and williams 2007 however karst groundwater is particularly vulnerable to contamination due to their distinct hydrogeologic characteristics goldscheider 2005 doerfliger et al 1999 drew and hötzl 1999 therefore understanding the hydrogeologic characteristics of karst aquifers plays an important role in water resources management in karst regions hydrogeologic characteristics of karst aquifers are different from other aquifers bakalowicz 2005 karst aquifers often exhibit a duality of recharge infiltration porosity flow and storage goldscheider and drew 2007 white 2002 gun 1986 karst aquifers also show a high degree of spatial heterogeneity in hydraulic properties bonacci et al 2006 especially the surface drainage basin in karst aquifers usually do not coincide with the groundwater basin spangler 2001 dar et al 2014 karstification is considered as one of the most common causes of interbasin groundwater flow igf le moine et al 2007 water recharged to karst aquifers could flow through an underground conduit system spanning over several basins and emerge at springs located at distant sites e g anderson et al 2006 belcher et al 2009 le moine et al 2008 it should be noted that igf could also occur in porous aquifer in form of regional groundwater flow tóth 1963 nguyen and dietrich 2018 danapour et al 2019 however in this study we focus on igf in karst areas the term igf in this study could be also understood as regional groundwater flow across surface topographic divides igf in karst areas could significantly alter the water budget of a basin e g anderson et al 2006 le moine et al 2008 considering the aforementioned facts igf in karst areas should be accounted for in hydrological modeling especially in the context of transboundary or interbasin groundwater management various models have been used to simulate igf in karst aquifers with varying model complexity ranging from physically based distributed to conceptual lumped models physically based distributed models simulate groundwater flow based on hydraulic head gradient therefore groundwater could flow across topographic divide units which are normally considered as isolated groundwater units in surface hydrology conceptual models can simulate igf by allowing the simulation or routing of groundwater flow between topographical basins some models of these types are the modular three dimensional finite difference ground water flow model modflow scanlon et al 2003 the modified wetspa model liu et al 2005 the modified soil and water assessment tool swat arnold et al 1998 nerantzaki et al 2015 malagó et al 2016 palanisamy and workman 2014 modèle du génie rural à 4 paramètres journalier gr4j perrin et al 2003 le moine et al 2007 2008 the tank model anaya and wanakule 1993 and the multi cell aquifer model rozos and koutsoyiannis 2006 barrett and charbeneau 1997 swat is one of the most widely used models to simulate the effect of land use agricultural management practices and climate change on water and chemical yields in non karst areas arnold and fohrer 2005 gassman et al 2007 krysanova and white 2007 molina navarro et al 2017 therefore the modified swat versions which account for igf in karst areas could potentially help to explore these effects in karst regions the aforementioned modified swat models the so calledkarstswat palanisamy and workman 2014 and kswat nerantzaki et al 2015 malagó et al 2016 simulate igf in karst regions the karstswat model was specifically developed for watersheds dominated by sinkholes and springflow which is mainly fed by the water from sinkholes palanisamy and workman 2014 the kswat model combines the adapted swat model fig 3 malagó et al 2016 and the karst flow model nikolaidis et al 2013 the adapted swat model assumes that all water entering the soil profile is karst groundwater recharge fig 3 malagó et al 2016 however part of the infiltrated water could contribute to the streamflow as lateral flow and baseflow if the underlying aquifer of a subbasin is not entirely a karst aquifer e g palanisamy and workman 2014 the adapted swat model does not differentiate between concentrated recharge and diffuse recharge the karst flow model is the two linear storage reservoir model which receives the recharge simulated from the adapted swat model or from the original swat model nikolaidis et al 2013 and routes it to spring outflows from the two reservoirs of the karst flow model represent flow from wide conduits and narrow fractures kourgialas et al 2010 malagó et al 2016 because of the lumped feature of deep recharge from the adapted swat model the kswat model does not explicitly differentiate between 1 the diffuse recharge and concentrated recharge 2 between matrix storage and conduit storage this is important because these recharges and storages are different in term of travel time and storage in addition to the aforementioned disadvantages the recharge area of the karst aquifer in the karstswat and kswat models follows the subbasin delineation of swat in addition to the model development parameter identification in karst regions is also subject to higher uncertainty compared to other regions brenner et al 2018 hartmann et al 2017 2013 this is because the karst aquifer is highly heterogeneous and the upper flux actual evapotranspiration eta and the lower flux karst groundwater recharge are usually unknown in order to develop a robust model and to minimize the parameter uncertainty especially in karst regions multi variable calibration is suggested eta is one of the main components of the hydrologic cycle about 60 of the annual precipitation on the global land surface returns to the atmosphere as evapotranspiration jung et al 2010 oki and kanae 2006 considering the aforementioned facts observed eta should be used for calibrating the model however a direct observation of eta is very scarce in non karst areas many studies have used satellite derived eta for model calibration e g rajib et al 2018 franco and bonumá 2017 vervoort et al 2014 rientjes et al 2013 droogers et al 2010 zhang et al 2009 muthuwatta et al 2009 immerzeel and droogers 2008 in these studies satellite derived eta was either used as an independent calibration data set or as input data results showed that the model performance for streamflow could decrease when constraining model calibration with satellite derived eta as an additional variable vervoort et al 2014 however the above mentioned studies showed that using satellite derived eta in combination with observed streamflow for calibrating a hydrologic model could 1 better reproduce the catchment s water balance 2 reduce the parameter uncertainty 3 increase the model robustness and 4 detect the structural model issues in karst areas the use of satellite derived eta as an additional calibration variable has not been given enough attention in this study we developed a conceptual model which is able to 1 simulate surface and subsurface flows in both karst and non karst areas 2 apply for a region where the karst aquifer boundaries do not coincide with the surface subbasin boundaries and 3 represent different recharges diffuse recharge and concentrated recharge and storages matrix storage and conduit storage in karst areas the proposed concept was implemented into the swat model the modified swat model was tested in the karst dominated area in lower saxony germany the effects of using satellite derived eta for model calibration on the model performance was examined in detail the moderate resolution imaging spectroradiometer mod16 eta mu et al 2013 was used for the model calibration 2 methodology 2 1 the original swat model in swat a basin can be divided into subbasins which are further divided into hydrologic response units hrus hrus are created by lumping all areas having the same combination of land use soil type and slope within a subbasin the hru concept is computationally efficient while incorporating the aforementioned landscape properties swat simulates two phases of the hydrologic cycle the land phase and the routing phase the land phase includes hru related processes such as surface processes e g evapotranspiration surface runoff vegetation related processes and subsurface processes e g percolation lateral flow groundwater recharge return flow fig 1a the routing phase includes stream related processes e g flood routing nutrient transport and reservoir routing in swat groundwater recharge is partitioned into shallow and deep aquifer recharge recharge into the shallow aquifer ultimately returns to stream as baseflow while recharge into the deep aquifer is considered as a loss swat is not capable of simulating groundwater flow between hrus or subbasins due to the non spatial characteristic of the hru concept a more detailed description of the swat model is given by neitsch et al 2011 2 2 the modified swat model for igf in this section after a summary of the general hydrogeologic characteristics of karst areas the modified swat for karst areas is presented the modified swat model for modeling igf hereafter referred to as the swat igf model is comprised of two conceptual models the original conceptual model of swat is applied for non karst areas fig 1a while modified conceptual model of swat is applied for karst areas fig 1b the two conceptual models were combined into a single program resulting in a single executable file an aquifer classification map is used as an additional criterion for the delineation of hrus e g fig 3c this aquifer classification map contains information about the aquifer type and the extended recharge area of each spring then the swat igf will assign the appropriate conceptual model for the karst and non karst hrus automatically e g fig 3c and recharge from the extended karst area will be routed to the corresponding spring the user needs to assign the amount of recharge to each spring in case multiple springs are fed by the same recharge area recharge into the karst aquifer could either be classified as 1 autogenic or allogenic recharge or 2 concentrated or diffuse recharge gun 1986 taylor and greene 2008 ford and williams 2007 autogenic recharge originates from precipitation falling on the karst areas while allogenic recharge originates from runoff on non karst areas concentrated recharge can occur via sinkholes losing streams closed depressions and well developed fissures diffuse recharge is areal recharge through the unsaturated soil zone recharge into the karst aquifer is often drained by a well developed solution conduit system and discharged via one or several springs flow in the conduit is often fast and turbulent while flow in the rock matrix is slow and laminar white 2002 hartmann et al 2014a however the majority of karst groundwater is stored in the rock matrix due to the fast flow and small storage of the conduit system compared to that of the rock matrix the response of discharge to recharge from the conduit system is often faster than that from the matrix storage in this study the swat igf is proposed for the cases where 1 the recharge area and discharge points springs are located in different subbasins and 2 the discharge points are located in one subbasin further modifications could be done for other cases a two reservoir model is proposed to represent the duality of and storage and discharge of the karst area fig 1b the first reservoir hereinafter referred to as the matrix storage reservoir represents groundwater storage in the rock matrix the matrix storage reservoir receives diffuse recharge from the overlaying zone the second reservoir hereinafter referred to as the conduit storage reservoir represents groundwater storage in the conduit system the conduit storage reservoir receives 1 concentrated recharge from closed depressions infiltration losses from streams fractures and dolines and 2 diffuse discharge from the matrix storage reservoir it should be noted that there could be flow from the conduit storage reservoir to the matrix reservoir e g screaton et al 2004 however it is not explicitly considered in this study we consider flow from the matrix to the conduit as net flow which already takes into account flow from the conduit to the rock matrix diffuse recharge from the bottom of the soil profile to the matrix storage reservoir on day i taking into account the delay time in the unsaturated zone is calculated using the exponential decay weighting function venetis 1969 sangrey et al 1984 1 w r d i 1 e 1 δ g w β w s e e p i e 1 δ g w w r d i 1 where w r d i and w r d i 1 mm h 2 o is the amount of diffuse recharge to the matrix reservoir on day i and i 1 respectively δ g w days is the delay time for infiltrated water to reach the matrix storage reservoir β is the recharge separation factor ranging from 0 to 1 w s e e p mm h 2 o is the total amount of water exiting the bottom of the soil profile on day i outflow from the matrix storage reservoir is simulated using the linear storage discharge relationship e g nikolaidis et al 2013 neitsch et al 2011 2 q m a t r i x i e α m a t r i x δ t q m a t r i x i 1 1 e α m a t r i x δ t j 1 n h r u s w r d i j a j 1 0 3 where q m a t r i x i and q m a t r i x i 1 m 3 h 2 o are the outflows from the matrix storage reservoirs on day i and i 1 respectively α m a t r i x 1 day is the recession constant of the matrix storage reservoir respectively δ t is the time step δ t 1 day w r d i j mm h 2 o and a j m 2 are the diffuse recharge and area of the hydrologic response unit j respectively 10 3 is the unit conversion factor from mm h 2 o to m h 2 o nhrus is the number of hrus in the recharge area concentrated recharge from closed depressions fractures and sinkholes to the conduit storage reservoir on day i w r c i mm h 2 o is calculated as follows 3 w r c i 1 β w s e e p i the total amount of recharge to the conduit storage reservoir on day i w r c o n d u i t i m 3 h 2 o is expressed as follows 4 w r c o n d u i t i j 1 n h r u s w r c i j a j 1 0 3 r t t l c i q m a t r i x i where r t t l c i m 3 h 2 o is the mount of recharge from losing streams on day i outflow from the conduit storage reservoir is simulated using the linear storage discharge relationship 5 q c o n d u i t i e α c o n d u i t δ t q c o n d u i t i 1 1 e α c o n d u i t δ t w r c o n d u i t i where q c o n d u i t i and q c o n d u i t i 1 m 3 h 2 o are outflows from the conduit storage reservoir on day i and i 1 respectively α c o n d u i t 1 day is the recession constant of the conduit storage reservoir the total runoff of a basin where the springs are located q r i v e r i m 3 h 2 o is calculated as follows 6 q r i v e r i q c o n d u i t i q d i r e c t i where q d i r e c t i m 3 h 2 o is the direct runoff the sum of surface runoff and lateral flow from the basin where the spring is located it should be noted that the conduit and matrix reservoirs proposed in this study correspond to the upper and lower reservoirs of the karst flow model nikolaidis et al 2013 respectively the conduit and the matrix reservoirs are arranged in series while the upper and lower reservoirs are arranged in parallel the lower reservoir receives recharge from the upper reservoir while the conduit receives recharge from the matrix reservoir springflow in the karst flow model is directly fed by the upper and lower reservoirs while it is only directly fed by the conduit reservoir in the swat igf model outflows from both reservoirs in both models are simulated using a linear storage discharge relationship 3 case study 3 1 study area and data the study area is located in the southwest harz mountains non karst area and the southern harz rim karst dominated area in northern germany with a drainage basin of about 384 km 2 fig 2 the study area has two outlets located at the rhume spring and lindau gauging stations the study area receives inflow from the odertalsperre reservoir the digital elevation model dem obtained from the niedersächsische landesbetrieb für wasserwirtschaft küsten und naturschutz nlwkn shows that the elevation of the study area varies from 142 m to 929 m above mean sea level a m s l land use land cover lulc map was taken from the copernicus land monitoring service the soil map bük 200 and soil profile data were obtained from the bundesanstalt für geowissenschaften und rohstoffe bgr fig 3 initial soil hydraulic conductivity and soil available water content were derived by using the pedotransfer functions tables wessolek et al 2009 the dominant land use land cover classes are forest and agricultural accounting for about 55 and 31 of the study area respectively the most dominant soil type in the southwest harz mountains is spodic cambisols from acid igneous and metamorphic rocks covering 46 of the study area in the southern harz rim most of the soils were developed from gypsum with low water holding capacity schnug et al 2004 observed groundwater level data at three wells located within and nearby the pöhlder becken were collected from the nlwkn fig 2 daily weather data precipitation wind speed temperature solar radiation and relative humidity from 1997 2010 were obtained from deutscher wetterdienst dwd weather data from observed stations were interpolated for all subbasins using the inverse distance weighing idw method the study area has an average annual precipitation of 1242 mm yr with high spatial variability the annual precipitation is up to 1619 mm yr in the southwest harz mountains whereas that in the southern harz rim is 862 mm year temperature in the study area decreases with an increase in elevation daily observed streamflow and reservoir outflow were obtained from the nlwkn and the harzwasserwerke hww the mod16 eta at 8 day time step and 1 km 2 spatial resolution was downloaded using the modistools tuck et al 2014 3 2 geology the study area consists of two distinct geologic areas the southwest harz mountains and the southern harz rim grimmelmann 1992 the harz mountains were part of the european variscan fold belt formed by the collision of the africa baltica laurentia and other microplates in the early paleozoic era tait et al 1997 haggett 2002 the harz mountains were later eroded and a large part of it was inundated by the zechstein sea haggett 2002 koster 2005 under hot and dry climatic conditions of the late permian period a large amount of evaporites was formed in the inundated area after several evaporation cycles taylor 1998 schnug et al 2004 kramm and wedepohl 1991 böttcher 1999 tucker 1991 after other geologic processes the underlying geology of the southern harz mountains nowadays mainly consists of palaeozoic greywacke shale and conglomerate fig 4 while in the southern harz rim the permian zechstein dolomite gypsum anhydrite was exposed to the surface and subjected to the karstification process voigt et al 2008 schnug et al 2004 böttcher 1999 paul and vladi 2001 there is a 2 to 6 km wide strip of exposed permian zechstein in the southern harz rim with various karst features such as sinking streams sink holes caves and springs liersch 1987 the karst area in this region is subjected to a continuous karstification process about 7092 tons of sulfur bound to gypsum are washed from this karst dominated area each year schnug and haneklaus 1998 herrmann 1969 geological cross sections in the area show that the permian zechstein rocks are exposed to the surface near the southern harz rim and overlaid by non karstifiable rocks in the south at the oder and sieber rivers it was overlaid by a quaternary fluvial deposit layer originated from the harz mountains fig 4 detailed geologic maps and geologic cross sections of the study area can be found in herrmann 1969 grimmelmann 1992 liersch 1987 and voigt et al 2008 and nibis kartenserver http nibis lbeg de cardomap3 th 647 3 3 hydrogeology the main rhume spring outlet is located in a nw se trending fault where flow in the underground conduit of the zechstein deposits is blocked by a low permeability lower buntsandstein stratum herrmann 1969 lamoreaux and tanner 2001 besides the main outlet with a diameter of about 20 m there are about 360 small outlets located nearby herrmann 1969 they altogether release an average discharge of about 2 2 m 3 s via a small stream with a minimum of 1 5 m 3 s during low flow periods this indicates that there could be a relatively big subsurface matrix storage in the area compared to the rhume spring subbasin the sum of discharge from the main rhume spring outlet and its neighboring outlets is hereafter referred to as the rhume spring discharge many studies have been conducted to explain the origin of the water from the rhume spring discharge since early 20th century thürnau 1913 conducted tracer tests with uranine and found that the infiltrated tracers from the area in the southern harz rim which were later known as the pöhlder becken reappears at the rhume spring fig 2 thürnau 1913 was also able to determine the main losing streams fig 4 in the pöhlder becken as well as the travel time of tracers from the infiltration points to the rhume spring haase et al 1970 analyzed the water balance in the study area and found that there are significant infiltration losses in the sieber and oder rivers in 1981 another tracer tests with about 12 kg of uranine were carried out at sinkholes near herzberg liersch 1987 the injected tracers were detected at the rhume spring about 78 hours after the injection and were almost undetectable after 25 days from this experiment a flow path of about 7500 m and a horizontal groundwater flow velocity of over 100 m h were estimated lamoreaux and tanner 2001 a three reservoir storage model was proposed to explain the breakthrough curve of tracer concentration at the rhume spring liersch 1987 rienäcker 1987 found that the time lag between peak discharges of the sieber at hattorf gauging station of the oder at scharzfeld gauging station and the rhume spring varies between 24 to 72 hours depending on the existing groundwater reservoir storage level results from various geophysical and tracer experiments showed that infiltrated water from the pöhlder becken hereinafter referred to as the recharge area of the rhume spring fig 2 and transmission losses of the rivers located in this area are the main sources of the rhume spring discharge goldmann 1986 liersch 1987 the recharge area of the rhume spring receives allogenic recharge from upstream subbasins via a connected river network in the area in addition it also receives groundwater inflow from southwest harz mountains however the estimated amount is negligible 0 03 m 3 s grimmelmann 1992 the estimated contribution of flow from the rhume basin with an area of 8 km 2 is about 4 of the rhume spring discharge about 96 of the rhume spring discharge is from igf of which about 60 originates from the infiltration loss of the oder and sieber rivers goldmann 1986 liersch 1987 lamoreaux and tanner 2001 therefore the original swat igf should be used instead of the original swat to explain 96 of the flow volume at the rhume spring 4 model setup calibration and validation 4 1 model setup the study area was divided into 26 subbasins and 1094 hrus based on lulc soil dem and aquifer map fig 3 the thresholds for defining hrus were set to zero to include all of the basin landscape the swat igf model uses the conceptual model presented in fig 1a for the southwest harz mountains and the conceptual model presented in fig 1b for the southern harz rim fig 3c infiltration losses w s e e p and river transmission losses r t t l c from the karst area located outside the recharge area of the rhume spring were considered as losses from the hydrologic system the model was set to run for the period of 14 years from 1997 to 2010 with 3 years of warm up 1997 1999 6 years of calibration 2000 2005 and 5 years of validation 2006 2010 at a daily time step in order to have a comparable result with mod16 eta the penman monteith method monteith 1965 allen 1986 allen et al 1989 which was used for deriving mod16 eta was used for calculating evapotranspiration in swat igf 4 2 calibration and validation strategy in this study the sequential uncertainty fitting sufi 2 in the swat calibration and uncertainty programs swat cup was used for parameter sensitivity model calibration validation and uncertainty analysis abbaspour 2013 abbaspour et al 2007 2004 the selected parameters and their initial ranges table 1 were chosen based on local expertise and literature review arnold et al 2012 white and chaubey 2005 lam et al 2012 maier and dietrich 2016 uniyal et al 2017 nguyen and dietrich 2018 rajib et al 2018 global sensitivity analysis was used to identify the important influencing factors and to reduce the number of parameters for model calibration sufi 2 uses multiple regression and t test to identify the relative sensitivity of each parameter within this approach a higher absolute value of t stat and a smaller p value indicate a higher sensitivity of the parameter abbaspour et al 2018 several multi criteria objective functions were proposed and tested the following form of the multi criteria objective function was found to be appropriate for this study 7 o f m a x w 1 i 1 5 n s e q i w 2 n s e q l i n d a u w 3 n s e q r h u m e s p r i n g w 4 n s e e t a 5 w 1 w 2 w 3 w 4 where o f is the multi criteria objective function n s e q i is the nash sutcliffe efficiency eq 8 nash and sutcliffe 1970 for streamflow at five streamgauging stations inside the catchment hattorf scharzfeld herzberg kupferhütte and pionierbrücke n s e q l i n d a u n s e q r h u m e s p r i n g and n s e e t a are the nse for streamflow at the catchment outlets rhume spring and lindau gauging stations and the nse for eta respectively w is the weight for sensitivity analysis the weights in the objective function were assigned as follows w 1 1 w 2 5 w 3 5 w 4 5 therefore the model performances for streamflow at the lindau rhume spring five aforementioned gauging stations inside the catchment and for et are considered equally important in the objective function three calibrations scenarios were carried out with an increase in the number of calibrated variables from calibration scenarios s1 to s3 table 2 if a variable is not calibrated its corresponding weight in the objective function is set to zero table 2 the objective of these calibration scenarios is to examine the effects of using multi site streamflow and mod16 eta for model calibration on the model performance for model calibration 1000 parameter sets were generated using latin hybercube sampling these parameter sets were used for all three calibration scenarios although only the nse was considered in the objective function the kling gupta efficiency kge gupta et al 2009 and percent bias pbias was also calculated for the best simulation as follows 8 n s e 1 i 1 n x i o b s x i s i m 2 i 1 n x i o b s x o b s 2 9 k g e 1 r 1 2 α 1 2 β 1 2 10 p b i a s 100 i 1 n x i o b s x i s i m i 1 n x i o b s where x i o b s and x i s i m are the observed and simulated values respectively at time step i x o b s is the mean of observed values n is the number of simulated values r is the linear regression coefficient between observed and simulated values α β is the ratio of standard deviation mean of observed over standard deviation mean of simulated values in sufi 2 parameter uncertainty which is represented as a uniform distribution integrates all types of uncertainties e g uncertainty in input data model concept model parameter and measured variables all of these uncertainties ultimately propagate into the model output uncertainty which is expressed by the 95 prediction uncertainty band 95ppu the p factor the percentage of measured data bracketed by the 95ppu band and r factor the average thickness of the 95ppu band divided by the standard deviation of the measured data are used to characterize the 95ppu band abbaspour et al 2018 5 results and discussion 5 1 sensitivity analysis and best calibrated parameter set table 1 shows the results of global sensitivity analysis for 21 model parameters parameter sensitivity ranking was based on the values of t stat and p value it is seen that cn2 is the most sensitive parameter this indicates that streamflow karst groundwater recharge and evapotranspiration are strongly affected by the surface runoff generation process the parameter ch k2 riverbed hydraulic conductivity is listed among the most sensitive parameters this is because river transmission losses in the karst area could infiltrate into the conduit network and formulate interbasin groundwater flow ultimately affect the catchment water balance the high sensitivity ranking of esco is because this parameter controls the amount of evaporation from the soil it is seen that the parameter which controls the amount of deep groundwater recharge rchrg dp was found insignificant this is because this parameter only exists in the conceptual model for the non karst area the non karst area in this case is the harz mountains with high topographic gradient in this area the runoff coefficient is expected to be high therefore the amount of deep groundwater recharge is expected to be minor compared to surface runoff the newly introduced parameters for the karst area β α c o n d u i t α m a t r i x are not identified as sensitive parameters this could be due to the fact that these parameters only affect the rhume spring discharge which plays a minor role in the objective function eq 7 and table 2 however one at time sensitivity analysis shows that these parameters significantly affect the dynamic of the simulated rhume spring hydrograph and they should be taken into account for a successful model calibration based on the result of sensitivity analyses and the process based evaluation as aforementioned the seven most sensitive parameters and the three parameters of the karst model were selected for model calibration the best parameter values obtained from automatic calibration were shown in table 3 5 2 the role of using mod16 eta and multi site streamflow data and for model calibration calibration results show that the calibration scenarios s2 and s3 have the same best parameter values table 3 and the same number of behavioral simulations 71 behavioral simulations with a behavioral threshold of 0 5 as a result the model performance statistics between the calibration scenarios s2 and s3 are identical table 4 this indicates that using mod16 eta for model calibration does not effect the model performance in this case study a detailed examination of the results shows that simulated eta from the calibration scenario s2 fits well with mod16 eta despite mod16 eta was not used for model calibration fig 5 and table 4 in addition the model performance for eta tends to be improved with improvement of the model performance for streamflow at the lindau scharzfeld and kupferhütte gauging stations this was shown by a strong positive correlation r 0 78 between nse e t a and nse q at these gauging stations in the calibration scenario s2 fig 6 as a result the best model performance for streamflow in these gauging stations is likely to be among the best model performances for eta and the use of mod16 eta for model calibration might not have any effect or only minor effects on the model performance the results indicate that if there is a strong positive correlation in the model performances between two different variables in a multi variable calibration one variable can be dropped out of the objective function without having much influence on the model performance for multi site calibration the selected stream gauges should be located in different rivers unless there are some major changes in the river segment the aforementioned results however should be considered along with the weights used in the objective function table 2 it should be noted that differences between the calibration results of scenarios s2 and s3 occur if the weight for nse e t a accounts for more than 70 of all weights in the objective function w 4 0 7 5 w 1 w 2 w 3 w 4 it means that improving the model performance for eta is the main objective which is not the objective in this study it is seen from table 4 that the model performance for streamflow at the rhume spring was reduced from nse 0 75 scenario s1 to nse 0 69 scenario s2 when streamflow data at additional stream gauges were used for model calibration however the model prediction uncertainty was reduced and the model robustness was increased this is shown by a decrease in the r factor from 1 10 to 1 01 and a decrease in the difference of nse between the calibration and validation periods from 0 27 to 0 07 table 4 in the calibration scenario s2 the model performance for streamflow at all gauging stations except at the rhume spring and for eta are improved compared to that in the calibration scenario s1 the results indicate that in a karst dominated region multi gauge calibration should be done in order to have a better model performance therefore only results from the calibration scenario s2 were discussed in detail in the remaining sections 5 3 simulated streamflow fig 7a g presents the observed and simulated streamflow hydrographs and their respective flow duration curves during the calibration period with the best calibrated parameters it is seen that the swat igf tends to underestimate high flows fig 7a g and low flows fig 7d e and f the underestimation of high flows and low flows is inherited from the original swat e g uniyal et al 2017 nguyen and dietrich 2018 nguyen et al 2018 this could be a reason for the small p factor observed from the model calibration outputs table 4 the good fit between simulated low flows at the lindau and sharzfeld gauging stations with observed data fig 7a g is due to the effect of using observed outflow from the oder dam odertalsperre fig 2 as input data to the model at the hattorf gauging station low flows were overestimated by the model fig 7b this is due to a non linear relationship between discharge and transmission losses of the sieber river which cannot be represented in the current swat igf model in this river transmission losses are reported to be higher more than 70 of the river discharge with smaller discharges thürnau 1913 at the rhume spring gauging station the observed flow duration curve is well reproduced by the model and the 95ppu band covers most of the observed values p factor 96 simulated results show that runoff generated from the rhume spring basin accounts for about 4 of the rhume spring discharge whereas the remainder 96 is from igf the results match well with the ones reported by goldmann 1986 simulated results from the swat igf also show that annual transmission losses from the sieber and oder river systems contribute about 59 of the rhume spring discharge which is similar to the previously estimated value of 60 lamoreaux and tanner 2001 due to a significant contribution of igf to the rhume spring as aforementioned the original swat model failed to simulate flow at this gauging station fig 7g it should be noted that simulated streamflows in the karst area lindau hattorf and scharzfeld gauging stations from the original swat could be better than the swat igf this is because parameters of the swat igf model in the karst region are further constrained to match the simulated streamflow at the rhume spring with observed data therefore we did not compare the simulated streamflow from the original swat and the swat igf at these gauging stations in the validation period 2006 2010 similar results were also observed fig 8a g 5 4 simulated karst groundwater storage variation fig 9a c shows 1 the variations of simulated karst groundwater storage the total groundwater storage in the matrix and conduit storage reservoirs in the recharge area of the rhume spring and 2 changes in the observed groundwater levels in three wells fig 2 it is expected that changes in the groundwater levels reflect the variations in karst groundwater storage in three wells it is seen that the annual variations in the simulated karst groundwater storage agree well with the observed groundwater levels especially with well 1 a high correlation coefficient r 0 93 between the simulated groundwater storage and the observed groundwater levels was found fig 9a at wells 2 and 3 fig 9b c lower correlation coefficients r 0 73 and r 0 47 respectively were found the simulated karst groundwater storage varies from 35 to 67 million m 3 with an average value of about 48 million m 3 6 conclusions and recommendations interbasin groundwater flow igf especially in karst areas could significantly alter the water budget of a region in this study the original swat model was modified for simulating igf in karst areas resulting in the swat igf model a two linear reservoir model was proposed to represent the duality of recharge infiltration storage and discharge in the karst area the study area is located in a karst dominated region in the southwest harz mountains germany the model was successfully calibrated at the rhume spring and at multiple sites for streamflow and for eta by using mod16 eta calibration results show that multi site calibration is necessary to achieve a good model performance simulated eta from the swat igf model matches well with mod16 eta despite mod16 eta was not used for model calibration the use of mod16 eta as additional calibration variable does not affect the model performance this is because the model performance for eta tends to be improved with an improvement of the model performance for streamflow at some gauging stations the conclusion regarding the use of mod16 eta for model calibration however should not be generalized to other satellite remote sensing products and to studies in other areas the swat igf model was demonstrated as a robust model by further validating the model outputs with other data the swat igf is also highly flexible it could be applied in both karst and non karst areas where the surface subbasin boundaries do not coincide with the subsurface subbasin boundaries the model uses a parsimonious approach for modeling igf in karst systems while explicitly representing the duality of recharge discharge and storage in karst regions the swat igf introduced in this study however has not been developed for modeling solute transport different solute transport models could be incorporated into the swat igf model due to its flexible structure for example future studies could apply a well mixed model for modeling solute transport in the conduit because flow in the conduit storage is fast and turbulent for solute transport in the soil matrix the catchment scale formulation of transport based on travel time distributions appears to be a promising tool botter et al 2011 benettin et al 2013 this concept could be used to simulate 1 the delay between input and output solute concentration signals and 2 different selection schemes for outflow from the rock matrix in addition the recharge separation factor β was assumed to be constant regardless of the rainfall event characteristics future studies could use different recharge separation factors depending on different rainfall event characteristics hartmann et al 2014b acknowledgments this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors we thank the editor and three anonymous reviewers for their constructive comments which helped to improve the quality of the manuscript significantly we also thank the dwd hww bgr and nlwkn for providing data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
26095,karstification is considered as one of the most common reasons for interbasin groundwater flow igf igf in some karst areas could be significant such that it must be accounted for in hydrologic modeling in this study the soil and water assessment tool swat was modified to explicitly account for igf in karst areas the modified model uses two conceptual models to simulate hydrologic processes in karst and non karst regions the modified model was applied in the karst dominated region in the southwest harz mountains germany multisite streamflow data and satellite derived actual evapotranspiration eta were used for model calibration results show that 1 the modified model can be satisfactorily calibrated and validated for streamflow and eta 2 the model performance for eta and streamflow at some gauging stations are highly correlated and 3 the use of satellite derived eta does not affect the model performance keywords modified swat karst interbasin groundwater flow calibration satellite derived evapotranspiration software availability name of software swat igf developer and contact address van tam nguyen nguyen iww uni hannover de institute of hydrology and water resources management leibniz universität hannover appelstraße 9a 30167 hannover germany year available 2019 availability and cost the source code is freely available at http doi org 10 5281 zenodo 3574312 language fortran 1 introduction the term karst refers to a region with distinct landscape features e g sinking streams sinkholes and springs and underground features e g underground conduits and caves in some karst regions the karst landscape features could be absent or subtle but their aquifers could be heavily karstified ford and williams 2007 karst aquifers are developed as a result of dissolution of karstifiable rocks e g limestone dolomite gypsum and rock salt the so called karstification ford and williams 2007 bögli 1980 howard 1963 karst aquifers account for about 10 to 15 of the continental area and karst groundwater is one of the sources of drinking water for approximately a quarter of the world s population ford and williams 2007 however karst groundwater is particularly vulnerable to contamination due to their distinct hydrogeologic characteristics goldscheider 2005 doerfliger et al 1999 drew and hötzl 1999 therefore understanding the hydrogeologic characteristics of karst aquifers plays an important role in water resources management in karst regions hydrogeologic characteristics of karst aquifers are different from other aquifers bakalowicz 2005 karst aquifers often exhibit a duality of recharge infiltration porosity flow and storage goldscheider and drew 2007 white 2002 gun 1986 karst aquifers also show a high degree of spatial heterogeneity in hydraulic properties bonacci et al 2006 especially the surface drainage basin in karst aquifers usually do not coincide with the groundwater basin spangler 2001 dar et al 2014 karstification is considered as one of the most common causes of interbasin groundwater flow igf le moine et al 2007 water recharged to karst aquifers could flow through an underground conduit system spanning over several basins and emerge at springs located at distant sites e g anderson et al 2006 belcher et al 2009 le moine et al 2008 it should be noted that igf could also occur in porous aquifer in form of regional groundwater flow tóth 1963 nguyen and dietrich 2018 danapour et al 2019 however in this study we focus on igf in karst areas the term igf in this study could be also understood as regional groundwater flow across surface topographic divides igf in karst areas could significantly alter the water budget of a basin e g anderson et al 2006 le moine et al 2008 considering the aforementioned facts igf in karst areas should be accounted for in hydrological modeling especially in the context of transboundary or interbasin groundwater management various models have been used to simulate igf in karst aquifers with varying model complexity ranging from physically based distributed to conceptual lumped models physically based distributed models simulate groundwater flow based on hydraulic head gradient therefore groundwater could flow across topographic divide units which are normally considered as isolated groundwater units in surface hydrology conceptual models can simulate igf by allowing the simulation or routing of groundwater flow between topographical basins some models of these types are the modular three dimensional finite difference ground water flow model modflow scanlon et al 2003 the modified wetspa model liu et al 2005 the modified soil and water assessment tool swat arnold et al 1998 nerantzaki et al 2015 malagó et al 2016 palanisamy and workman 2014 modèle du génie rural à 4 paramètres journalier gr4j perrin et al 2003 le moine et al 2007 2008 the tank model anaya and wanakule 1993 and the multi cell aquifer model rozos and koutsoyiannis 2006 barrett and charbeneau 1997 swat is one of the most widely used models to simulate the effect of land use agricultural management practices and climate change on water and chemical yields in non karst areas arnold and fohrer 2005 gassman et al 2007 krysanova and white 2007 molina navarro et al 2017 therefore the modified swat versions which account for igf in karst areas could potentially help to explore these effects in karst regions the aforementioned modified swat models the so calledkarstswat palanisamy and workman 2014 and kswat nerantzaki et al 2015 malagó et al 2016 simulate igf in karst regions the karstswat model was specifically developed for watersheds dominated by sinkholes and springflow which is mainly fed by the water from sinkholes palanisamy and workman 2014 the kswat model combines the adapted swat model fig 3 malagó et al 2016 and the karst flow model nikolaidis et al 2013 the adapted swat model assumes that all water entering the soil profile is karst groundwater recharge fig 3 malagó et al 2016 however part of the infiltrated water could contribute to the streamflow as lateral flow and baseflow if the underlying aquifer of a subbasin is not entirely a karst aquifer e g palanisamy and workman 2014 the adapted swat model does not differentiate between concentrated recharge and diffuse recharge the karst flow model is the two linear storage reservoir model which receives the recharge simulated from the adapted swat model or from the original swat model nikolaidis et al 2013 and routes it to spring outflows from the two reservoirs of the karst flow model represent flow from wide conduits and narrow fractures kourgialas et al 2010 malagó et al 2016 because of the lumped feature of deep recharge from the adapted swat model the kswat model does not explicitly differentiate between 1 the diffuse recharge and concentrated recharge 2 between matrix storage and conduit storage this is important because these recharges and storages are different in term of travel time and storage in addition to the aforementioned disadvantages the recharge area of the karst aquifer in the karstswat and kswat models follows the subbasin delineation of swat in addition to the model development parameter identification in karst regions is also subject to higher uncertainty compared to other regions brenner et al 2018 hartmann et al 2017 2013 this is because the karst aquifer is highly heterogeneous and the upper flux actual evapotranspiration eta and the lower flux karst groundwater recharge are usually unknown in order to develop a robust model and to minimize the parameter uncertainty especially in karst regions multi variable calibration is suggested eta is one of the main components of the hydrologic cycle about 60 of the annual precipitation on the global land surface returns to the atmosphere as evapotranspiration jung et al 2010 oki and kanae 2006 considering the aforementioned facts observed eta should be used for calibrating the model however a direct observation of eta is very scarce in non karst areas many studies have used satellite derived eta for model calibration e g rajib et al 2018 franco and bonumá 2017 vervoort et al 2014 rientjes et al 2013 droogers et al 2010 zhang et al 2009 muthuwatta et al 2009 immerzeel and droogers 2008 in these studies satellite derived eta was either used as an independent calibration data set or as input data results showed that the model performance for streamflow could decrease when constraining model calibration with satellite derived eta as an additional variable vervoort et al 2014 however the above mentioned studies showed that using satellite derived eta in combination with observed streamflow for calibrating a hydrologic model could 1 better reproduce the catchment s water balance 2 reduce the parameter uncertainty 3 increase the model robustness and 4 detect the structural model issues in karst areas the use of satellite derived eta as an additional calibration variable has not been given enough attention in this study we developed a conceptual model which is able to 1 simulate surface and subsurface flows in both karst and non karst areas 2 apply for a region where the karst aquifer boundaries do not coincide with the surface subbasin boundaries and 3 represent different recharges diffuse recharge and concentrated recharge and storages matrix storage and conduit storage in karst areas the proposed concept was implemented into the swat model the modified swat model was tested in the karst dominated area in lower saxony germany the effects of using satellite derived eta for model calibration on the model performance was examined in detail the moderate resolution imaging spectroradiometer mod16 eta mu et al 2013 was used for the model calibration 2 methodology 2 1 the original swat model in swat a basin can be divided into subbasins which are further divided into hydrologic response units hrus hrus are created by lumping all areas having the same combination of land use soil type and slope within a subbasin the hru concept is computationally efficient while incorporating the aforementioned landscape properties swat simulates two phases of the hydrologic cycle the land phase and the routing phase the land phase includes hru related processes such as surface processes e g evapotranspiration surface runoff vegetation related processes and subsurface processes e g percolation lateral flow groundwater recharge return flow fig 1a the routing phase includes stream related processes e g flood routing nutrient transport and reservoir routing in swat groundwater recharge is partitioned into shallow and deep aquifer recharge recharge into the shallow aquifer ultimately returns to stream as baseflow while recharge into the deep aquifer is considered as a loss swat is not capable of simulating groundwater flow between hrus or subbasins due to the non spatial characteristic of the hru concept a more detailed description of the swat model is given by neitsch et al 2011 2 2 the modified swat model for igf in this section after a summary of the general hydrogeologic characteristics of karst areas the modified swat for karst areas is presented the modified swat model for modeling igf hereafter referred to as the swat igf model is comprised of two conceptual models the original conceptual model of swat is applied for non karst areas fig 1a while modified conceptual model of swat is applied for karst areas fig 1b the two conceptual models were combined into a single program resulting in a single executable file an aquifer classification map is used as an additional criterion for the delineation of hrus e g fig 3c this aquifer classification map contains information about the aquifer type and the extended recharge area of each spring then the swat igf will assign the appropriate conceptual model for the karst and non karst hrus automatically e g fig 3c and recharge from the extended karst area will be routed to the corresponding spring the user needs to assign the amount of recharge to each spring in case multiple springs are fed by the same recharge area recharge into the karst aquifer could either be classified as 1 autogenic or allogenic recharge or 2 concentrated or diffuse recharge gun 1986 taylor and greene 2008 ford and williams 2007 autogenic recharge originates from precipitation falling on the karst areas while allogenic recharge originates from runoff on non karst areas concentrated recharge can occur via sinkholes losing streams closed depressions and well developed fissures diffuse recharge is areal recharge through the unsaturated soil zone recharge into the karst aquifer is often drained by a well developed solution conduit system and discharged via one or several springs flow in the conduit is often fast and turbulent while flow in the rock matrix is slow and laminar white 2002 hartmann et al 2014a however the majority of karst groundwater is stored in the rock matrix due to the fast flow and small storage of the conduit system compared to that of the rock matrix the response of discharge to recharge from the conduit system is often faster than that from the matrix storage in this study the swat igf is proposed for the cases where 1 the recharge area and discharge points springs are located in different subbasins and 2 the discharge points are located in one subbasin further modifications could be done for other cases a two reservoir model is proposed to represent the duality of and storage and discharge of the karst area fig 1b the first reservoir hereinafter referred to as the matrix storage reservoir represents groundwater storage in the rock matrix the matrix storage reservoir receives diffuse recharge from the overlaying zone the second reservoir hereinafter referred to as the conduit storage reservoir represents groundwater storage in the conduit system the conduit storage reservoir receives 1 concentrated recharge from closed depressions infiltration losses from streams fractures and dolines and 2 diffuse discharge from the matrix storage reservoir it should be noted that there could be flow from the conduit storage reservoir to the matrix reservoir e g screaton et al 2004 however it is not explicitly considered in this study we consider flow from the matrix to the conduit as net flow which already takes into account flow from the conduit to the rock matrix diffuse recharge from the bottom of the soil profile to the matrix storage reservoir on day i taking into account the delay time in the unsaturated zone is calculated using the exponential decay weighting function venetis 1969 sangrey et al 1984 1 w r d i 1 e 1 δ g w β w s e e p i e 1 δ g w w r d i 1 where w r d i and w r d i 1 mm h 2 o is the amount of diffuse recharge to the matrix reservoir on day i and i 1 respectively δ g w days is the delay time for infiltrated water to reach the matrix storage reservoir β is the recharge separation factor ranging from 0 to 1 w s e e p mm h 2 o is the total amount of water exiting the bottom of the soil profile on day i outflow from the matrix storage reservoir is simulated using the linear storage discharge relationship e g nikolaidis et al 2013 neitsch et al 2011 2 q m a t r i x i e α m a t r i x δ t q m a t r i x i 1 1 e α m a t r i x δ t j 1 n h r u s w r d i j a j 1 0 3 where q m a t r i x i and q m a t r i x i 1 m 3 h 2 o are the outflows from the matrix storage reservoirs on day i and i 1 respectively α m a t r i x 1 day is the recession constant of the matrix storage reservoir respectively δ t is the time step δ t 1 day w r d i j mm h 2 o and a j m 2 are the diffuse recharge and area of the hydrologic response unit j respectively 10 3 is the unit conversion factor from mm h 2 o to m h 2 o nhrus is the number of hrus in the recharge area concentrated recharge from closed depressions fractures and sinkholes to the conduit storage reservoir on day i w r c i mm h 2 o is calculated as follows 3 w r c i 1 β w s e e p i the total amount of recharge to the conduit storage reservoir on day i w r c o n d u i t i m 3 h 2 o is expressed as follows 4 w r c o n d u i t i j 1 n h r u s w r c i j a j 1 0 3 r t t l c i q m a t r i x i where r t t l c i m 3 h 2 o is the mount of recharge from losing streams on day i outflow from the conduit storage reservoir is simulated using the linear storage discharge relationship 5 q c o n d u i t i e α c o n d u i t δ t q c o n d u i t i 1 1 e α c o n d u i t δ t w r c o n d u i t i where q c o n d u i t i and q c o n d u i t i 1 m 3 h 2 o are outflows from the conduit storage reservoir on day i and i 1 respectively α c o n d u i t 1 day is the recession constant of the conduit storage reservoir the total runoff of a basin where the springs are located q r i v e r i m 3 h 2 o is calculated as follows 6 q r i v e r i q c o n d u i t i q d i r e c t i where q d i r e c t i m 3 h 2 o is the direct runoff the sum of surface runoff and lateral flow from the basin where the spring is located it should be noted that the conduit and matrix reservoirs proposed in this study correspond to the upper and lower reservoirs of the karst flow model nikolaidis et al 2013 respectively the conduit and the matrix reservoirs are arranged in series while the upper and lower reservoirs are arranged in parallel the lower reservoir receives recharge from the upper reservoir while the conduit receives recharge from the matrix reservoir springflow in the karst flow model is directly fed by the upper and lower reservoirs while it is only directly fed by the conduit reservoir in the swat igf model outflows from both reservoirs in both models are simulated using a linear storage discharge relationship 3 case study 3 1 study area and data the study area is located in the southwest harz mountains non karst area and the southern harz rim karst dominated area in northern germany with a drainage basin of about 384 km 2 fig 2 the study area has two outlets located at the rhume spring and lindau gauging stations the study area receives inflow from the odertalsperre reservoir the digital elevation model dem obtained from the niedersächsische landesbetrieb für wasserwirtschaft küsten und naturschutz nlwkn shows that the elevation of the study area varies from 142 m to 929 m above mean sea level a m s l land use land cover lulc map was taken from the copernicus land monitoring service the soil map bük 200 and soil profile data were obtained from the bundesanstalt für geowissenschaften und rohstoffe bgr fig 3 initial soil hydraulic conductivity and soil available water content were derived by using the pedotransfer functions tables wessolek et al 2009 the dominant land use land cover classes are forest and agricultural accounting for about 55 and 31 of the study area respectively the most dominant soil type in the southwest harz mountains is spodic cambisols from acid igneous and metamorphic rocks covering 46 of the study area in the southern harz rim most of the soils were developed from gypsum with low water holding capacity schnug et al 2004 observed groundwater level data at three wells located within and nearby the pöhlder becken were collected from the nlwkn fig 2 daily weather data precipitation wind speed temperature solar radiation and relative humidity from 1997 2010 were obtained from deutscher wetterdienst dwd weather data from observed stations were interpolated for all subbasins using the inverse distance weighing idw method the study area has an average annual precipitation of 1242 mm yr with high spatial variability the annual precipitation is up to 1619 mm yr in the southwest harz mountains whereas that in the southern harz rim is 862 mm year temperature in the study area decreases with an increase in elevation daily observed streamflow and reservoir outflow were obtained from the nlwkn and the harzwasserwerke hww the mod16 eta at 8 day time step and 1 km 2 spatial resolution was downloaded using the modistools tuck et al 2014 3 2 geology the study area consists of two distinct geologic areas the southwest harz mountains and the southern harz rim grimmelmann 1992 the harz mountains were part of the european variscan fold belt formed by the collision of the africa baltica laurentia and other microplates in the early paleozoic era tait et al 1997 haggett 2002 the harz mountains were later eroded and a large part of it was inundated by the zechstein sea haggett 2002 koster 2005 under hot and dry climatic conditions of the late permian period a large amount of evaporites was formed in the inundated area after several evaporation cycles taylor 1998 schnug et al 2004 kramm and wedepohl 1991 böttcher 1999 tucker 1991 after other geologic processes the underlying geology of the southern harz mountains nowadays mainly consists of palaeozoic greywacke shale and conglomerate fig 4 while in the southern harz rim the permian zechstein dolomite gypsum anhydrite was exposed to the surface and subjected to the karstification process voigt et al 2008 schnug et al 2004 böttcher 1999 paul and vladi 2001 there is a 2 to 6 km wide strip of exposed permian zechstein in the southern harz rim with various karst features such as sinking streams sink holes caves and springs liersch 1987 the karst area in this region is subjected to a continuous karstification process about 7092 tons of sulfur bound to gypsum are washed from this karst dominated area each year schnug and haneklaus 1998 herrmann 1969 geological cross sections in the area show that the permian zechstein rocks are exposed to the surface near the southern harz rim and overlaid by non karstifiable rocks in the south at the oder and sieber rivers it was overlaid by a quaternary fluvial deposit layer originated from the harz mountains fig 4 detailed geologic maps and geologic cross sections of the study area can be found in herrmann 1969 grimmelmann 1992 liersch 1987 and voigt et al 2008 and nibis kartenserver http nibis lbeg de cardomap3 th 647 3 3 hydrogeology the main rhume spring outlet is located in a nw se trending fault where flow in the underground conduit of the zechstein deposits is blocked by a low permeability lower buntsandstein stratum herrmann 1969 lamoreaux and tanner 2001 besides the main outlet with a diameter of about 20 m there are about 360 small outlets located nearby herrmann 1969 they altogether release an average discharge of about 2 2 m 3 s via a small stream with a minimum of 1 5 m 3 s during low flow periods this indicates that there could be a relatively big subsurface matrix storage in the area compared to the rhume spring subbasin the sum of discharge from the main rhume spring outlet and its neighboring outlets is hereafter referred to as the rhume spring discharge many studies have been conducted to explain the origin of the water from the rhume spring discharge since early 20th century thürnau 1913 conducted tracer tests with uranine and found that the infiltrated tracers from the area in the southern harz rim which were later known as the pöhlder becken reappears at the rhume spring fig 2 thürnau 1913 was also able to determine the main losing streams fig 4 in the pöhlder becken as well as the travel time of tracers from the infiltration points to the rhume spring haase et al 1970 analyzed the water balance in the study area and found that there are significant infiltration losses in the sieber and oder rivers in 1981 another tracer tests with about 12 kg of uranine were carried out at sinkholes near herzberg liersch 1987 the injected tracers were detected at the rhume spring about 78 hours after the injection and were almost undetectable after 25 days from this experiment a flow path of about 7500 m and a horizontal groundwater flow velocity of over 100 m h were estimated lamoreaux and tanner 2001 a three reservoir storage model was proposed to explain the breakthrough curve of tracer concentration at the rhume spring liersch 1987 rienäcker 1987 found that the time lag between peak discharges of the sieber at hattorf gauging station of the oder at scharzfeld gauging station and the rhume spring varies between 24 to 72 hours depending on the existing groundwater reservoir storage level results from various geophysical and tracer experiments showed that infiltrated water from the pöhlder becken hereinafter referred to as the recharge area of the rhume spring fig 2 and transmission losses of the rivers located in this area are the main sources of the rhume spring discharge goldmann 1986 liersch 1987 the recharge area of the rhume spring receives allogenic recharge from upstream subbasins via a connected river network in the area in addition it also receives groundwater inflow from southwest harz mountains however the estimated amount is negligible 0 03 m 3 s grimmelmann 1992 the estimated contribution of flow from the rhume basin with an area of 8 km 2 is about 4 of the rhume spring discharge about 96 of the rhume spring discharge is from igf of which about 60 originates from the infiltration loss of the oder and sieber rivers goldmann 1986 liersch 1987 lamoreaux and tanner 2001 therefore the original swat igf should be used instead of the original swat to explain 96 of the flow volume at the rhume spring 4 model setup calibration and validation 4 1 model setup the study area was divided into 26 subbasins and 1094 hrus based on lulc soil dem and aquifer map fig 3 the thresholds for defining hrus were set to zero to include all of the basin landscape the swat igf model uses the conceptual model presented in fig 1a for the southwest harz mountains and the conceptual model presented in fig 1b for the southern harz rim fig 3c infiltration losses w s e e p and river transmission losses r t t l c from the karst area located outside the recharge area of the rhume spring were considered as losses from the hydrologic system the model was set to run for the period of 14 years from 1997 to 2010 with 3 years of warm up 1997 1999 6 years of calibration 2000 2005 and 5 years of validation 2006 2010 at a daily time step in order to have a comparable result with mod16 eta the penman monteith method monteith 1965 allen 1986 allen et al 1989 which was used for deriving mod16 eta was used for calculating evapotranspiration in swat igf 4 2 calibration and validation strategy in this study the sequential uncertainty fitting sufi 2 in the swat calibration and uncertainty programs swat cup was used for parameter sensitivity model calibration validation and uncertainty analysis abbaspour 2013 abbaspour et al 2007 2004 the selected parameters and their initial ranges table 1 were chosen based on local expertise and literature review arnold et al 2012 white and chaubey 2005 lam et al 2012 maier and dietrich 2016 uniyal et al 2017 nguyen and dietrich 2018 rajib et al 2018 global sensitivity analysis was used to identify the important influencing factors and to reduce the number of parameters for model calibration sufi 2 uses multiple regression and t test to identify the relative sensitivity of each parameter within this approach a higher absolute value of t stat and a smaller p value indicate a higher sensitivity of the parameter abbaspour et al 2018 several multi criteria objective functions were proposed and tested the following form of the multi criteria objective function was found to be appropriate for this study 7 o f m a x w 1 i 1 5 n s e q i w 2 n s e q l i n d a u w 3 n s e q r h u m e s p r i n g w 4 n s e e t a 5 w 1 w 2 w 3 w 4 where o f is the multi criteria objective function n s e q i is the nash sutcliffe efficiency eq 8 nash and sutcliffe 1970 for streamflow at five streamgauging stations inside the catchment hattorf scharzfeld herzberg kupferhütte and pionierbrücke n s e q l i n d a u n s e q r h u m e s p r i n g and n s e e t a are the nse for streamflow at the catchment outlets rhume spring and lindau gauging stations and the nse for eta respectively w is the weight for sensitivity analysis the weights in the objective function were assigned as follows w 1 1 w 2 5 w 3 5 w 4 5 therefore the model performances for streamflow at the lindau rhume spring five aforementioned gauging stations inside the catchment and for et are considered equally important in the objective function three calibrations scenarios were carried out with an increase in the number of calibrated variables from calibration scenarios s1 to s3 table 2 if a variable is not calibrated its corresponding weight in the objective function is set to zero table 2 the objective of these calibration scenarios is to examine the effects of using multi site streamflow and mod16 eta for model calibration on the model performance for model calibration 1000 parameter sets were generated using latin hybercube sampling these parameter sets were used for all three calibration scenarios although only the nse was considered in the objective function the kling gupta efficiency kge gupta et al 2009 and percent bias pbias was also calculated for the best simulation as follows 8 n s e 1 i 1 n x i o b s x i s i m 2 i 1 n x i o b s x o b s 2 9 k g e 1 r 1 2 α 1 2 β 1 2 10 p b i a s 100 i 1 n x i o b s x i s i m i 1 n x i o b s where x i o b s and x i s i m are the observed and simulated values respectively at time step i x o b s is the mean of observed values n is the number of simulated values r is the linear regression coefficient between observed and simulated values α β is the ratio of standard deviation mean of observed over standard deviation mean of simulated values in sufi 2 parameter uncertainty which is represented as a uniform distribution integrates all types of uncertainties e g uncertainty in input data model concept model parameter and measured variables all of these uncertainties ultimately propagate into the model output uncertainty which is expressed by the 95 prediction uncertainty band 95ppu the p factor the percentage of measured data bracketed by the 95ppu band and r factor the average thickness of the 95ppu band divided by the standard deviation of the measured data are used to characterize the 95ppu band abbaspour et al 2018 5 results and discussion 5 1 sensitivity analysis and best calibrated parameter set table 1 shows the results of global sensitivity analysis for 21 model parameters parameter sensitivity ranking was based on the values of t stat and p value it is seen that cn2 is the most sensitive parameter this indicates that streamflow karst groundwater recharge and evapotranspiration are strongly affected by the surface runoff generation process the parameter ch k2 riverbed hydraulic conductivity is listed among the most sensitive parameters this is because river transmission losses in the karst area could infiltrate into the conduit network and formulate interbasin groundwater flow ultimately affect the catchment water balance the high sensitivity ranking of esco is because this parameter controls the amount of evaporation from the soil it is seen that the parameter which controls the amount of deep groundwater recharge rchrg dp was found insignificant this is because this parameter only exists in the conceptual model for the non karst area the non karst area in this case is the harz mountains with high topographic gradient in this area the runoff coefficient is expected to be high therefore the amount of deep groundwater recharge is expected to be minor compared to surface runoff the newly introduced parameters for the karst area β α c o n d u i t α m a t r i x are not identified as sensitive parameters this could be due to the fact that these parameters only affect the rhume spring discharge which plays a minor role in the objective function eq 7 and table 2 however one at time sensitivity analysis shows that these parameters significantly affect the dynamic of the simulated rhume spring hydrograph and they should be taken into account for a successful model calibration based on the result of sensitivity analyses and the process based evaluation as aforementioned the seven most sensitive parameters and the three parameters of the karst model were selected for model calibration the best parameter values obtained from automatic calibration were shown in table 3 5 2 the role of using mod16 eta and multi site streamflow data and for model calibration calibration results show that the calibration scenarios s2 and s3 have the same best parameter values table 3 and the same number of behavioral simulations 71 behavioral simulations with a behavioral threshold of 0 5 as a result the model performance statistics between the calibration scenarios s2 and s3 are identical table 4 this indicates that using mod16 eta for model calibration does not effect the model performance in this case study a detailed examination of the results shows that simulated eta from the calibration scenario s2 fits well with mod16 eta despite mod16 eta was not used for model calibration fig 5 and table 4 in addition the model performance for eta tends to be improved with improvement of the model performance for streamflow at the lindau scharzfeld and kupferhütte gauging stations this was shown by a strong positive correlation r 0 78 between nse e t a and nse q at these gauging stations in the calibration scenario s2 fig 6 as a result the best model performance for streamflow in these gauging stations is likely to be among the best model performances for eta and the use of mod16 eta for model calibration might not have any effect or only minor effects on the model performance the results indicate that if there is a strong positive correlation in the model performances between two different variables in a multi variable calibration one variable can be dropped out of the objective function without having much influence on the model performance for multi site calibration the selected stream gauges should be located in different rivers unless there are some major changes in the river segment the aforementioned results however should be considered along with the weights used in the objective function table 2 it should be noted that differences between the calibration results of scenarios s2 and s3 occur if the weight for nse e t a accounts for more than 70 of all weights in the objective function w 4 0 7 5 w 1 w 2 w 3 w 4 it means that improving the model performance for eta is the main objective which is not the objective in this study it is seen from table 4 that the model performance for streamflow at the rhume spring was reduced from nse 0 75 scenario s1 to nse 0 69 scenario s2 when streamflow data at additional stream gauges were used for model calibration however the model prediction uncertainty was reduced and the model robustness was increased this is shown by a decrease in the r factor from 1 10 to 1 01 and a decrease in the difference of nse between the calibration and validation periods from 0 27 to 0 07 table 4 in the calibration scenario s2 the model performance for streamflow at all gauging stations except at the rhume spring and for eta are improved compared to that in the calibration scenario s1 the results indicate that in a karst dominated region multi gauge calibration should be done in order to have a better model performance therefore only results from the calibration scenario s2 were discussed in detail in the remaining sections 5 3 simulated streamflow fig 7a g presents the observed and simulated streamflow hydrographs and their respective flow duration curves during the calibration period with the best calibrated parameters it is seen that the swat igf tends to underestimate high flows fig 7a g and low flows fig 7d e and f the underestimation of high flows and low flows is inherited from the original swat e g uniyal et al 2017 nguyen and dietrich 2018 nguyen et al 2018 this could be a reason for the small p factor observed from the model calibration outputs table 4 the good fit between simulated low flows at the lindau and sharzfeld gauging stations with observed data fig 7a g is due to the effect of using observed outflow from the oder dam odertalsperre fig 2 as input data to the model at the hattorf gauging station low flows were overestimated by the model fig 7b this is due to a non linear relationship between discharge and transmission losses of the sieber river which cannot be represented in the current swat igf model in this river transmission losses are reported to be higher more than 70 of the river discharge with smaller discharges thürnau 1913 at the rhume spring gauging station the observed flow duration curve is well reproduced by the model and the 95ppu band covers most of the observed values p factor 96 simulated results show that runoff generated from the rhume spring basin accounts for about 4 of the rhume spring discharge whereas the remainder 96 is from igf the results match well with the ones reported by goldmann 1986 simulated results from the swat igf also show that annual transmission losses from the sieber and oder river systems contribute about 59 of the rhume spring discharge which is similar to the previously estimated value of 60 lamoreaux and tanner 2001 due to a significant contribution of igf to the rhume spring as aforementioned the original swat model failed to simulate flow at this gauging station fig 7g it should be noted that simulated streamflows in the karst area lindau hattorf and scharzfeld gauging stations from the original swat could be better than the swat igf this is because parameters of the swat igf model in the karst region are further constrained to match the simulated streamflow at the rhume spring with observed data therefore we did not compare the simulated streamflow from the original swat and the swat igf at these gauging stations in the validation period 2006 2010 similar results were also observed fig 8a g 5 4 simulated karst groundwater storage variation fig 9a c shows 1 the variations of simulated karst groundwater storage the total groundwater storage in the matrix and conduit storage reservoirs in the recharge area of the rhume spring and 2 changes in the observed groundwater levels in three wells fig 2 it is expected that changes in the groundwater levels reflect the variations in karst groundwater storage in three wells it is seen that the annual variations in the simulated karst groundwater storage agree well with the observed groundwater levels especially with well 1 a high correlation coefficient r 0 93 between the simulated groundwater storage and the observed groundwater levels was found fig 9a at wells 2 and 3 fig 9b c lower correlation coefficients r 0 73 and r 0 47 respectively were found the simulated karst groundwater storage varies from 35 to 67 million m 3 with an average value of about 48 million m 3 6 conclusions and recommendations interbasin groundwater flow igf especially in karst areas could significantly alter the water budget of a region in this study the original swat model was modified for simulating igf in karst areas resulting in the swat igf model a two linear reservoir model was proposed to represent the duality of recharge infiltration storage and discharge in the karst area the study area is located in a karst dominated region in the southwest harz mountains germany the model was successfully calibrated at the rhume spring and at multiple sites for streamflow and for eta by using mod16 eta calibration results show that multi site calibration is necessary to achieve a good model performance simulated eta from the swat igf model matches well with mod16 eta despite mod16 eta was not used for model calibration the use of mod16 eta as additional calibration variable does not affect the model performance this is because the model performance for eta tends to be improved with an improvement of the model performance for streamflow at some gauging stations the conclusion regarding the use of mod16 eta for model calibration however should not be generalized to other satellite remote sensing products and to studies in other areas the swat igf model was demonstrated as a robust model by further validating the model outputs with other data the swat igf is also highly flexible it could be applied in both karst and non karst areas where the surface subbasin boundaries do not coincide with the subsurface subbasin boundaries the model uses a parsimonious approach for modeling igf in karst systems while explicitly representing the duality of recharge discharge and storage in karst regions the swat igf introduced in this study however has not been developed for modeling solute transport different solute transport models could be incorporated into the swat igf model due to its flexible structure for example future studies could apply a well mixed model for modeling solute transport in the conduit because flow in the conduit storage is fast and turbulent for solute transport in the soil matrix the catchment scale formulation of transport based on travel time distributions appears to be a promising tool botter et al 2011 benettin et al 2013 this concept could be used to simulate 1 the delay between input and output solute concentration signals and 2 different selection schemes for outflow from the rock matrix in addition the recharge separation factor β was assumed to be constant regardless of the rainfall event characteristics future studies could use different recharge separation factors depending on different rainfall event characteristics hartmann et al 2014b acknowledgments this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors we thank the editor and three anonymous reviewers for their constructive comments which helped to improve the quality of the manuscript significantly we also thank the dwd hww bgr and nlwkn for providing data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
26096,the dynacof model was designed to model coffee agroforestry systems and study the trade offs to e g optimize the system facing climate changes the model simulates net primary productivity npp growth yield mortality energy and water balance of coffee agroforestry systems according to shade tree species and management several plot scale ecosystem services are simulated by the model such as production canopy cooling effect or potential c sequestration dynacof uses metamodels derived from a detailed 3d process based model maespa to account for complex spatial effects while running fast it also includes a coffee flower bud and fruit cohort module to better distribute fruit carbon demand over the year a key feature to obtain a realistic competition between sinks the model was parameterized and evaluated using a highly comprehensive database on a coffee agroforestry experimental site in costa rica the fluxes simulated by the model were close to the measurements over a 5 year period nrmse 26 27 for gross primary productivity 28 22 for actual evapo transpiration 53 91 for sensible heat flux and 15 26 for net radiation and dynacof satisfactorily simulated the yield npp mortality and carbon stock for each coffee organ type over a 35 year rotation graphical abstract image 1 keywords crop model coffea arabica maespa gpp erythrina poeppigiana plant to plot scale 1 software and data availability the dynacof model was developed as an r package r core team 2019 and as a julia package bezanson et al 2017 full documentation is available on its dedicated website https vezy github io dynacof the code is open source gnu gplv3 license and available on github repositories r https github com vezy dynacof julia https github com vezy dynacof jl and archived on zenodo https doi org 10 5281 zenodo 1256816 the input data used for this study is available as example data from this repository and the data for model evaluation is available from the fluxnet website http www europe fluxdata eu home site details id cr aqc 2 introduction the key role of crop models is to help understand and predict the links between crop development and climate soil management facilitation and competition between species crop models can provide insights into the main emerging agricultural challenges such as food security sustainability how to enhance ecosystem services and how to cope with the possible negative effects of climate changes spiertz 2012 there is an increasing need to address these issues at global scale to identify the different solutions available makowski et al 2014 especially when the products are exchanged on the global market like wheat maize soybean cocoa or coffee perennial plantations are difficult to study because their relatively long growing cycle extends the period necessary for data acquisition and because the heterogeneity of the canopy sometimes significantly increases the intra plot light and micro meteorological anisotropy such as for temperature vapor pressure or aerodynamic conditions luedeling et al 2014 2016 agroforestry systems afs are probably the most complex perennial agroecosystems malézieux et al 2009 because they have the most heterogeneous vertical and or horizontal canopies and these affect all ecosystem fluxes charbonnier et al 2013 vezy et al 2018 yet afs have the potential to enhance ecosystem services jose 2009 lin 2010 taugourdeau et al 2014 such as carbon sequestration jose and bardhan 2012 oelbermann et al 2004 and to mitigate climate pressure on crops lin 2007 in costa rica coffee arabica is mostly grown under afs management because it is assumed to improve coffee bean quality and to expand the cropping area to sub optimal low altitude and warmer areas muschler 2001 but such assumptions depend mostly on altitude local climate and postharvest processing worku et al 2018 modeling the energy water and carbon balance of these agroecosystems could provide insights into their functioning and allow stakeholders to test the in silico trends of new management practices e g density pruning thinning date or intensity or species arrangement on given outputs such as yield or other ecosystem services however several factors make these systems challenging to model first there are many options for shade management with highly heterogeneous canopies ranging from free growing low density shade trees like cordia alliodora to high density heavily managed low trees or plants such as banana or pollarded erythrina poeppigiana trees van oijen et al 2010a second the coffee reproductive phenology is a complex process that lasts for about two years camargo and camargo 2001 with competition between reproductive and vegetative compartments charbonnier et al 2017 and bienniality at the plant scale schnabel et al 2018 blossoming is mostly synchronized in sub tropical regions but can also be highly asynchronous in equatorial regions which impacts the distribution of fruit carbon demand and in turn carbon allocation to other organs rodríguez et al 2011 third coffee plants are often pruned every five to six years to sustain high levels of production on young resprouts it is also assumed that the reserve compartment plays a major role in bean production with biennial sprout dynamics cannell 1985 fourth there are very few comprehensive datasets that can be used to calibrate and test multi objective models for ecosystem services energy carbon and water balance aboveground and belowground biomass npp fruit yield and more model development implies identifying prioritizing and balancing the most important processes and the scale at which the model should simulate them in coffee systems we assume that i absorbed light ii light use efficiency lue iii within canopy temperature iv water and v nutrient uptake are among the most important primary processes because they regulate carbon assimilation respiration evapotranspiration vegetative growth flowering and fruit development the next most important processes may be vi shade tree and coffee leaf phenology that regulate light absorption canopy temperature and transpiration vii carbon partitioning to compute net primary productivity npp mortality litterfall and organ biomass and viii a detailed phenology of the reproductive organs comprising all stages from the appearance of cohorts of buds flowers and fruits until harvest or the overripe stage several models have already been developed to simulate coffee grown in full sun or agroforestry systems rodríguez et al 2011 proposed a model to simulate coffee in monoculture only from branch to whole plant scales the model was calibrated from planting to five years old the strength of this model lies in the fine phenology and physiological processes of the modeled coffee plant using branch level cohorts of flowers and fruits over the entire two year reproductive cycle indeed cohorts are required to realistically distribute the demand for carbon of the fruits over the course of the season and not all at once this model was successfully calibrated for colombian and brazilian sites two regions with contrasting climate and flower phenology subtropical and equatorial however this model was not designed for large plots long rotations or agroforestry coffee light absorption is computed using the beer lambert law using a constant coefficient of extinction absorbed light is converted into photosynthesis using constant light use efficiency and coffee pruning shade trees canopy temperature water and energy balance are not implemented in the model another model was developed by van oijen et al 2010 b this is a 1d plant average plot scale model for coffee grown in agroforestry systems simplifying the intra plot microclimate into either below shade or in full sun one clear advantage of this model is its ability to compute several ecosystem services and to incorporate various types of shade tree management and species and a thorough parameterization approach the model is simple fast and can be run under changing climates it was recently applied in east africa under climate change scenarios by rahn et al 2018 the main limitations of the model are i its light transmission module does not consider light distribution as a continuum under shade trees as described in charbonnier et al 2013 ii its formalism of lue which is not influenced by the intra plot light variability even though charbonnier et al 2017 found it to be greatly impacted iii its lack of a reserve compartment and of a cohort module and again iv the absence of energy balance and temperature of the canopy to drive the plant respiration reproductive development two other models have also been applied to coffee in an agroforestry system using 3d light interception modules in dauzat et al 2001 where only a sample of a few coffee plants were simulated and using the maespa model to simulate the whole system since maespa was recently demonstrated to accurately predict light distribution canopy temperature and water and energy balance in such systems charbonnier et al 2013 2017 vezy et al 2018 the model can readily compute all variables that are potentially influenced by the complex canopy structure however its relatively high computation time still limits its application for full rotations of coffee under afs and the model does not simulate growth and yield we argue that a proper combination of the inherent strengths of the above described models could provide significant improvements and extend application domains it would involve combining cohorts and reserves at the plant scale variable canopy temperature and intra plot microclimate and lue while allowing a reasonable level of abstraction to insure rapid simulations multiple plots crop rotations management etc in this study we built surrogate models i e metamodels of maespa for the spatial dependent variables and integrated them into a simpler growth and yield model to avoid expensive computation and development time these metamodels are simple instantaneous equations that efficiently compute a given output of a complex model in other words it is a reduction of a complex model intended to emulate the behavior of complex interactions between variables e g spatial heterogeneity into one empirical equation becoming an input for the next crop model metamodels are generally used to better understand the processes at stake in a model and to assess model sensitivity and uncertainty christina et al 2016 faivre et al 2013 for the purpose of optimization razavi et al 2012 or to make faster and reasonably accurate predictions for a given variable that is usually computed by a time consuming model but with fewer simulation errors compared to simpler models marie et al 2014 metamodels are often used as an efficient and simple tool to combine models at different time and or space scales without running the finer scale model iteratively consequently we designed dynacof to incorporate a plant scale reproductive phenology formalism inspired by rodríguez et al 2011 but dependent on canopy temperature with different sub modules to adapt coffee and shade tree management density and tree species as in van oijen et al 2010 b and metamodels calibrated from maespa simulations for spatially dependent variables such as diffuse and direct light extinction coefficients light use efficiency leaf water potential transpiration and sensible fluxes vezy et al 2018 regarding model parameterization calibration and evaluation for coffee modelling several strategies are proposed in the literature most of which depend on the availability of field data rodríguez et al 2011 assumed that the main factor that influences yield variability is latitude because of its impact on phenology so they tested their model on an equatorial site and on a sub tropical site with two distinct set of parameters for each situation the former model was evaluated against field data on biomass leaves branches berries stem and roots and total plant nitrogen content gathered at three distinct sites van oijen et al 2010 b and later rahn et al 2018 who further developed the caf2007 model proposed a calibration based on an extensive screening of the literature and a sensitivity analysis but no model evaluation against field data at this stage alternatively we propose a multiple objective strategy of evaluation in this study relying on a large range of state and flux variables measured at the same time by the end of the crop rotation including eddy covariance fluxes coffee and shade tree biomass measured at organ scale necromass yield npp water balance and energy balance and finally farm registers to describe management during a complete rotation charbonnier et al 2013 2017 defrenet 2016 gómez delgado et al 2011 taugourdeau et al 2014 vezy et al 2018 consequently the aims of the present study are to i develop metamodels for spatially dependent variables based on maespa which has already been calibrated and validated on coffee agroforestry systems ii develop a new plot scale 1d 5 layer shade tree coffee and 3 soil layers average plant ecophysiological process based model for coffee crops grown under agroforestry or in full sun while combining the advantages and strengths of three previously published models nutrients are considered non limiting in this first version which is realistic for many field conditions iii parameterize and evaluate the model using a multi objective approach applied to an extensive dataset from a long term observatory including the energy and water balance gpp yield npp and carbon mass per organ 3 materials and methods 3 1 site description the research site is located on the aquiares coffee farm 6 6 km2 between 9 56 8 and 9 56 35 n and 83 44 39 and 83 43 35 w itself located in the central caribbean area of costa rica the climate is tropical humid with no marked dry season 3014 mm mean annual precipitation and 19 5 c mean annual temperature during the 1979 to 2009 period the elevation of the research site ranges between 1020 and 1280 m a s l and the mean slope is 11 31 the vegetation consists of coffee plants coffea arabica l var caturra planted below erythrina poeppigiana a leguminous shade tree both were planted in 1979 shade trees were originally planted at a density of approximately 250 trees ha 1 and pollarded twice a year to optimize the light transmitted to the coffee layer they were thinned to about 7 4 trees ha 1 in 2000 and left growing freely until the end of the study the shade trees do not follow any particular planting pattern the aquiares farm is intensively managed with several applications of fertilizer per year 214 44 kg n ha 1 yr 1 and a regular selective pruning of the coffee shoots a practice often used by farmers to avoid a drop in production due to exhaustion the farm complies with the rainforest alliance for its pest and weed management weeds are scarce between 1995 and 2016 the aquiares farm reported average yields of green coffee around 1333 336 kg green coffee ha 1 yr 1 3 2 comprehensive database of measurements coffee flux is a collaborative research observatory https www umr ecosols fr recherche projets 53 coffee flux monitored continuously since 2009 and located on the aquiares coffee farm this research site has been intensively studied and described in detail notably for hydrology and eddy covariance by gómez delgado et al 2011 lai by taugourdeau et al 2014 light budget by charbonnier et al 2013 belowground biomass and npp by defrenet 2016 ecosystem biomass npp and lue by charbonnier et al 2017 and energy balance water balance and surface temperature by vezy et al 2018 the measurements used in this study to evaluate the model included half hourly n e e net ecosystem exchange h sensible heat flux l e latent heat flux and r n net radiation measured using an eddy covariance tower the online reddyproc tool wutzler et al 2018 was used to assess gross primary productivity gpp following the lasslop et al 2010 model option based on daily hyperbolic curves to estimate photosynthesis this model option was chosen because the coffee flux site is mountainous prone to night time advection the flux measurements were integrated at a daily time scale for comparison with dynacof outputs the lai of the shade tree l a i t r e e was also measured using a lai2000 li cor ne usa and the coffee lai l a i c o f f e e using a normalized difference vegetation index ndvi sensor positioned 25 m above the ground at an angle of 15 to the vertical with 45 view angle and converted to lai according to charbonnier et al 2013 the light transmittance by shade trees was also measured using the lai2000 the carbon mass gc m 2 of the shade tree stem and branches and the coffee resprout wood fine roots stump and coarse roots were all measured during two consecutive years 2011 2012 and 2012 2013 using dimension measurements and site specific allometric relationships the npp of each compartment of the shade tree and coffee plant was also computed from the biomass increment and from litter production further details about the methods used for measurement are available in charbonnier et al 2013 and charbonnier et al 2017 3 3 maespa model and metamodel conception 3 3 1 description of maespa maespa is a 3d explicit process based model duursma and medlyn 2012 medlyn 2004 wang and jarvis 1990 used to simulate forest energy water and carbon fluxes at the scale of the individual tree the light interception module canopy temperature water and energy balance of the model have already been calibrated used and validated on the same agroforestry system charbonnier et al 2013 vezy et al 2018 maespa is particularly well suited to simulate agroforestry system fluxes because it describes the forest at voxel scale which is a homogeneous representation of a sub part of the tree crown it can manage several tree species with their own position on the plot their overall structure crown height width etc and their physical and physiological parameters thus maespa computes a fine estimation of the light interception energy water and carbon fluxes of each plant in the forest and of the soil while taking the spatial heterogeneity of the canopy into account however maespa lacks a carbon allocation module or growth process and requires computationally intensive simulations e g a week of computation on the shared montpellier bioinformatic biodiversity mbb computing cluster platform to complete a distributed simulation of a 0 2 ha afs plot including 4176 coffea arabica sprouts and 14 erythrina poeppigiana shade trees over 1 year at 30 min time scale consequently maespa was used here mainly to compute metamodels to simulate spatial dependent variables 3 3 2 metamodels the main process affected by canopy complexity is probably the photosynthetically active radiation absorbed a p a r by the canopy charbonnier et al 2013 heterogeneous canopies like those of shade trees in afs tend to violate the assumption of a constant value for diffuse k d i f and direct k d i r light extinction coefficients because of non uniform spatial distribution of leaf area and because the leaf area density and foliage aggregation can change over time sampson and smith 1993 sinoquet et al 2007 furthermore a comparison between coffee plants grown in monoculture and under an agroforestry system showed that canopy complexity also affected canopy temperature water and energy partitioning light interception transpiration and stomatal conductance vezy et al 2018 metamodels were computed from maespa simulation outputs for diffuse k d i f and direct k d i r shade tree light extinction coefficients coffee and shade tree light use efficiency l u e g c m j 1 transpiration t r m m d 1 plant sensible heat flux h m j m 2 d 1 and soil net radiation r n s o i l m j m 2 d 1 metamodels are used to reproduce as far as possible the link between a set of input variables and the desired output variable as if it had been computed by the maespa model maespa simulations were also used to find the values of some parameters such as the coffee layer light extinction coefficients that had low variability throughout the simulation and hence were assumed constant in dynacof the partitioning parameter s o i l l e p see appendix a 2 used to compute soil sensible and latent heat flux from the soil net radiation in dynacof was also parameterized using maespa outputs metamodels were fitted using multilinear regressions selected according to a trade off between the number of explanatory variables their genericity and range of application and their accuracy obtained using different statistics ef modelling efficiency r2 r squared nrmse normalized root mean squared error and bias maespa was used to simulate one year of the coffee agroforestry system from the coffee flux site in the aquiares farm in order to calibrate the metamodels the parameterization and description of these maespa simulations are reported in vezy et al 2018 the outputs considered are converted from half hourly values at shade tree and coffee resprout scale to daily plot scale values for each plant layer during the same year fig 1 the metamodels were trained on 80 of maespa simulation outputs aggregated at daily time scale and evaluated over the 20 remaining validation data to compute out of sample statistics both training and testing partitions were sampled based on the dependent variable subgroup percentiles using the createdatapartition function in the caret r package kuhn 2019 the input variables of maespa used as predictors for metamodels were either related to climate or to plot average plant structure climate variables included daily air temperature vapor pressure deficit par diffuse and direct light fractions wind speed and air pressure the atmospheric co2 concentration can also be used for the l u e metamodel but were not useful for the present study input variables concerning shade tree structure for the metamodels were leaf area m l e a v e s 2 t r e e 1 crown height m trunk height m crown radius m trunk diameter m and all derivatives such as the leaf area index lai m l e a v e s 2 m s o i l 2 leaf area density lad m l e a v e s 2 m c r o w n 3 tree density t r e e s h a 1 or crown projection m c r o w n 2 m s o i l 2 all being plot averages the lai was measured continuously in the plot while the other structural data were measured only once during the 2011 campaign the variability and the interactions between the predictors were checked to insure the range of values was similar in the maespa simulation dataset and in the application dataset used for dynacof the metamodels are not mandatory in dynacof and can be replaced by any equation or fixed value hence dynacof was also run using standard equations instead of the metamodels to assess their relative contribution to the modeling performance for the plot scale net radiation rn latent le and sensible h heat flux and gross primary productivity gpp for this purpose the fao recommended plot scale equation from allen et al 1998 was chosen as a reference for comparison with the current computation for rn in dynacof le was computed using the penman monteith equation allen et al 1998 h was computed as the difference between rn and le and gpp was computed using a constant light use efficiency l u e and constant light extinction coefficients k d i f and k d i r for both the coffee and the shade tree layers the average values computed from maespa simulations were used to compute the constant l u e and k s coefficients on this modeling experiment 3 4 description of the dynacof model 3 4 1 introductory description dynacof which stands for dynamic agroforestry coffee crop model is a daily plot scale crop model murthy 2004 with two layers of vegetation shade trees and coffee plants and three soil layers aimed at simulating the growth and yield of coffee plantations under various shade tree species and management options considering the spatial heterogeneity of the shade tree canopy the coffee layer can be simulated either in monoculture or in agroforestry systems each layer is simulated sequentially at a daily time step variables with high intra plot variability i e light absorption lue transpiration plant sensible heat flux and soil net radiation are all computed using metamodels from maespa the model accounts for potential competition for light acquisition and water availability between plant and soil layers nutrients are considered non limiting in this first version which is realistic for many field conditions in costa rica water competition is simulated virtually from the day to day fluctuations in water content in each shared soil layer that can be reduced by drainage and evapotranspiration or increased by precipitation through throughfall this simple formalism should be sufficient given the absence of water limitations in the application concerned vezy et al 2018 but it can also reproduce the competition between plants under more constrained conditions 3 4 2 light interception and photosynthesis the diffuse a p a r d i f i m j m 2 d 1 and direct a p a r d i r i m j m 2 d 1 daily absorbed photosynthetic active radiation of each plant layer are computed using the beer lambert s law of light extinction 1 a p a r d i f i p a r d i f i 1 e k d i f l a i i 2 a p a r d i r i p a r d i r i 1 e k d i r l a i i with p a r d i f i and p a r d i r i the diffuse or direct daily photosynthetically active radiation m j m 2 d 1 reaching the layer on day i k d i f and k d i r the light extinction coefficient of the layer for the diffuse and direct light respectively and l a i the leaf area index m l e a f 2 m s o i l 2 of the layer both stand scale light extinction coefficients of the shade tree layer are computed using metamodels from maespa while the extinction coefficients of coffee were approximated to be constant after maespa simulations only a slight variability could not be explained using stand scale factors p a r d i f and p a r d i r are computed as the incoming p a r minus the p a r absorbed by the upper layer if any neglecting the p a r reflected back by the canopy the gross primary productivity g p p g c m 2 d 1 of each layer is then computed from the sum of diffuse and direct a p a r multiplied by the light use efficiency l u e g c m j 1 which is derived from a maespa metamodel 3 g p p i a p a r d i f i a p a r d i r i l u e i 3 4 3 carbon supply a whole plant daily gross carbohydrate budget gcb is computed from the daily g p p available reserves and plant maintenance respiration as follows 4 g c b i g p p i k r e s c m r e i 1 r m i the carbon available from reserves for day i is computed as a fraction k r e s of the carbon mass of the reserves from the previous day c m r e i 1 g c m 2 r m is the maintenance respiration g c m 2 d 1 see eq 6 the plant daily carbon supply is then computed as 5 s u p p l y i g c b i i f g c b i 0 0 i f g c b i 0 if g c b i is negative it means that the carbon available from g p p and reserves was not sufficient to support the maintenance respiration so s u p p l y i is equal to 0 and the missing carbon is considered as mortality imputed equally to all organs see eq 23 3 4 4 maintenance respiration the maintenance respiration requirement r m is computed as the sum of maintenance respiration of all plant organs litton et al 2007 the maintenance respiration of each organ r m j is computed using its carbon mass c m g c m 2 from the previous day following a q10 formalism as reported in dufrêne et al 2005 6 r m j i p a j c m j i 1 c j m r n q 10 j t j t m r 10 where j is the organ i the day p a j 0 1 the living fraction of the organ c m j i 1 g c m 2 the carbon mass of the organ from the previous day n c j g n g c 1 the organ nitrogen content m r n g c g n 1 d 1 the respiration rate per nitrogen unit q 10 j dimensionless the response of organ respiration to temperature t j c the temperature of the organ and t m r c the base temperature for maintenance respiration the leaf temperature of the plant layer t c that is computed in eq 44 is used as a proxy for t j except for the roots where the soil temperature t s o i l is used instead 3 4 5 carbon allocation to organs the allocation of carbohydrates to the organs is limited by either the s u p p l y i or by the organ demand for carbon the s u p p l y i is distributed to the different organs following a hierarchical allocation scheme lacointe 2000 for coffee charbonnier et al 2017 reported that allocation to the woody compartments remained quite steady whatever the fruit load so dynacof supplies this compartment with priority in contrast carbon allocation to fruits appeared to prevail over allocation to leaves charbonnier et al 2017 gutierrez et al 1998 so dynacof supplies carbon assimilates to fruits just after woody compartments and the remaining carbon is allocated to leaves and fine roots and eventually reserves the demand from the woody compartments is considered to always be proportional to the s u p p l y i the demand from fine roots and leaves is considered constant whereas the fruits demand is computed see section 3 4 11 if the demand for assimilates by the fruits is sufficiently high the carbon allocated to fruits potentially represents all the remaining global carbon s u p p l y i after allocation to the woody compartments leaving nothing for the leaves and the fine roots in practice the carbon s u p p l y i is first partitioned between shoot wood c a s h o o t i and stump and coarse roots wood c a s c r i using constant coefficients λ s h o o t and λ s c r 7 c a s h o o t i λ s h o o t s u p p l y i 8 c a s c r i λ s c r s u p p l y i λ s h o o t λ s c r 1 carbon allocation to the coffee fruits is computed as the minimum between the total fruit demand see eq 38 and the remaining carbon supply 9 c a f r u i t i min d e f r u i t i s u p p l y i c a s h o o t i c a s c r i the remaining carbon supply if any is then shared between leaves and fine roots following a coefficient of allocation λ r j for the remaining carbon 10 s u p p l y l e a f i λ r l e a f s u p p l y i c a s h o o t i c a s c r i c a f r u i t i 11 s u p p l y f r o o t i λ r f r o o t s u p p l y i c a s h o o t i c a s c r i c a f r u i t i 12 with λ r l e a f λ r f r o o t 1 and the allocation to both organs is the minimum between their respective supply and demand 13 c a l e a f i min d e l e a f i s u p p l y l e a f i 14 c a f r o o t i min d e f r o o t s u p p l y f r o o t i demand by the fine roots is a constant parameter but demand by the leaves is computed as 15 d e l e a f i d e l m s t o c k i n g i 10 000 where d e l m is the maximum leaf carbon demand g c p l a n t 1 d 1 per coffee plant and s t o c k i n g i is the coffee planting density on day i p l a n t h a 1 finally if both leaf and fine root carbon demands are satisfied the remaining carbon is stored in the reserves for future use 16 c a r e i o f f e r i c a s h o o t i c a s c r i c a f r u i t i c a l e a f i c a f r o o t i 3 4 6 growth respiration growth respiration is the energy cost in carbohydrates to make new biomass from c a on day i it is computed using a construction cost coefficient ε j g c p r o d u c e d g c a l l o c a t e d 1 applied to the carbon allocated to the organ j 17 r g j i c a j i ε j 3 4 7 net primary productivity maintenance respiration is already accounted for before any carbon allocation see 3 4 3 therefore the net primary productivity of each organ j for day i n p p j i g c m 2 d 1 is computed using the difference between the carbon allocated to the organ and the growth respiration 18 n p p j i c a j i r g j i 19 j s h o o t s c r f r u i t l e a f f r o o t the net primary productivity of the plant is the sum on j of the n p p j i of the layer considered and the total net primary productivity of the stand is the sum of all n p p j i in all the layers 3 4 8 mortality mortality is the sum of the natural mortality turnover rate m n a t of pruning m p r u n diseases and of the lack of carbon m c when the carbohydrate budget g c b i is negative the natural mortality of an organ j m n a t j i g c m 2 d 1 is proportional to its carbon mass from the previous day c m j i 1 g c m 2 and is computed using a lifespan parameter l i f e s p a n j d 20 m n a t j i c m j i 1 l i f e s p a n j the annual pruning of coffee plants affects the shoot wood and the leaves and it is assumed that leaf loss is accompanied by fine root loss pruning also affects the branches of the shade trees for the leaves and shoots pruning is adjusted using a pruning intensity coefficient p i as follows it is considered that two separate p i s are needed for the leaves and the shoots because the distribution of wood and leaf biomass may be heterogeneous between resprouts of different ages in the coffee plant as reported in charbonnier et al 2017 21 m p r u n j i p i j c m j i 1 mortality due to pruning of the fine roots is related to the mortality of pruned leaves using a constant parameter m f r o o t 22 m p r u n f r o o t i m p r u n l e a f i m f r o o t leaf disease mortality is only implemented for coffee using a module to compute the american leaf spot als according to avelino et al 2007 mortality due to lack of carbohydrates to meet the maintenance respiration requirements m c i is computed as 23 m c j i g c b i c m j i 1 c m t o t i 1 i f g c b i 0 0 i f g c b i 0 where c m j i 1 is the carbon mass of the organ j from the day before i see eq 24 and c m t o t i 1 is the total carbon mass of the leaves shoots fine roots stump and coarse roots of the previous day finally the total mortality of each organ m j i is computed as the sum of all mortalities natural pruning leaf disease for coffee only and lack of carbohydrates 3 4 9 carbon and dry mass of organs the carbon mass of an organ is incremented daily by adding its n p p j i and removing its mortality 24 c m j i c m j i 1 n p p j i m j i the organ dry mass is computed using the carbon mass and the carbon content of each organ c c j g c g d m 1 3 4 10 branch nodes the plagiotropic branches of a coffee plant present several nodes of decreasing age from the orthotropic branch to the tip several unproductive old nodes close to the orthotropic branch productive nodes from the previous year that potentially bear flower buds and then fruits newly formed nodes that bear leaves the coffee reproductive phenology is based on a 2 year cycle camargo and camargo 2001 for a given year n the new flush of nodes bearing new leaves will bear flower buds by the end of year n while the leaves from year n 1 born by the fruiting nodes n 1 are shed given that each new node bears two leaves one expects some proportionality between node number and leaf area for the year n gutierrez et al 1998 such proportionality is used here to upscale the equations of rodríguez et al 2011 from the branch to the whole plant the model computes the total number of newly formed nodes g n i on the coffee plant for year n i e the number of green wood nodes that will potentially bear flower buds by the end of the first year of the reproductive cycle 25 g n i l a i i r n l c n where r n l is the number of nodes per l a i unit at 20 c assuming that this parameter is dependent on the growth temperature c n is an empirical temperature dependant correction coefficient that is a function of the mean temperature during the vegetative growth period t g p c this relationship is derived from drinnan and menzel 1995 26 c n 0 4194773 0 2631364 t g p 0 0226364 t g p 2 0 0005455 t g p 3 note that this coefficient is from one of the only datasets in the literature that links coffee reproductive and vegetative growth to the ambient temperature 3 4 11 fruit development the reproduction module is mostly inspired by rodríguez et al 2011 but upscaled to the whole plant e g in eq 25 two main development processes are computed in the model the flower bud cohorts of year n and the fruit cohorts of the following year the bud itself has two stages of development while the fruit has five buds can be initiated only during the bud initialization period bip from day d b i p 1 to d b i p 2 the buds appear on branch nodes in daily cohorts every day within that time window the d b i p 1 date is computed from the cumulative sum of degree days s d d t f f b after the date of end of vegetative development d v g 2 the degree days are computed as follows 27 d d i t c a n i t m i n i f t c a n i t m i n 0 i f t c a n i t m i n where d d i are the degree days c of day i t c a n i the mean daily coffee canopy temperature t m i n the minimum i e base temperature and t m a x the maximum temperature for physiological activity one originality here is that the degree days are computed using the canopy temperature of each layer rather than measured air temperature because the microclimate close to the plant may differ depending on the local conditions e g the cooling effect of shade trees d b i p 1 is computed as 28 s d d t f f b i d v g 2 d d d i i f s d d t f f b f t f f b d b i p 1 d i f s d d t f f b f t f f b where f t f f b is the threshold value in degree days that triggers the start of the bud development window it is considered that bud cohorts stop initializing when the first bud cohort initiated on the plant enters the fruit stage in other words buds can be initiated every day as cohorts until the first fruit appears on the coffee plant this day is denoted d b i p 2 the number of buds initiated daily on a given cohort b u d s 1 from d b i p 1 to d b i p 2 depends on several factors the incoming radiation r a d m j m 2 d 1 the canopy degree day computed following eq 27 and the number of green nodes to support the buds g n i this computation follows eq 12 from rodríguez et al 2011 here adapted to the whole plant 29 b u d s 1 i a b u d b b u d r a d i g n i d d i where a b u d and b b u d are parameters each bud initiated on day i is considered to belong to the bud cohort of day i there are as many cohorts as days between d b i p 1 and d b i p 2 once initiated bud cohorts develop for f b u d s 1 degree days until they become dormant on day d b u d s 1 30 s d d b s 1 i i n i t d d d i i f s d d b s 1 f b u d s 1 d b u d s 1 d i f s d d b s 1 f b u d s 1 where i n i t is the day of initialization of the cohort to potentially break dormancy the bud cohort has to experience a minimum amount of cumulated rainfall f r a i n after d b u d s 1 31 s r a i n i d b u d s 1 d r a i n i i f s r a i n f r a i n d r a i n d i f s r a i n f r a i n however the bud cohort can cumulate a maximum of f b u d s 2 degree days during its lifetime if it does not break its dormancy before the day when this threshold is reached d b u d s 2 it is considered desiccated d b u d s 2 is computed as 32 s d d b s 2 i d b u d s 1 d d d i i f s d d b s 2 f b u d s 2 d b u d s 2 d i f s d d b s 2 f b u d s 2 the time window when buds from a cohort can potentially break their dormancy w f r u i t s 1 is then found by solving eq 31 and eq 32 33 w f r u i t s 1 d r a i n d b u d s 2 i f s r a i n f r a i n a n d d r a i n d b u d s 2 the time window is shared by all the buds in the same cohort but all the buds will not necessarily break their dormancy on the same day the number of buds in a given cohort that break their dormancy on a given day within w f r u i t s 1 depends on the combination of two factors the mean diurnal air temperature within the coffee canopy during bud growth c b c and the leaf water potential ψ l e a f m p a of the coffee plant these conditions reflect the need for a drier period followed by an intense rainfall event for optimal dormancy break rodríguez et al 2011 the number of buds breaking dormancy b u d b r e a k on a given day is computed as 34 b u d b r e a k i b u d s 2 i p b r e a k i c b where b u d s 2 i is the number of stage 2 buds in the cohort that have not yet broken dormancy c b is a temperature dependant correction coefficient and p b r e a k i is the rate at which they break dormancy which is related to the leaf water potential ψ l e a f m p a using two parameters a p and b p as follows 35 p b r e a k i 1 1 e a p b p ψ l e a f i c b is implemented to include the effect of the average canopy temperature on the number of buds initiated during growth of the coffee plant it is computed from a monotone hermite spline fritsch and carlson 1980 fitted on the rather unique drinnan and menzel 1995 dataset relating temperature and blossoming more details are available on the help page of the eponym function of the r package of dynacof https vezy github io dynacof reference cb html when buds do break dormancy they enter the fruit stage by forming a flower stage 1 fruit then the fruits develop and finally become mature stage 4 then overripe stage 5 when they fall onto the ground each bud breaking dormancy on day i forms a new cohort of fruits meaning that fruits forming a fruit cohort can originate from several different bud cohorts the stage 1 fruits of the cohort then enter the carbon allocation scheme to undergo the successive stages of maturation under optimal conditions the carbon mass of the fruit increases following a logistic growth over the growing period until it becomes overripe this logistic growth during physiological development can be modeled as 36 l 1 e f d d i n f s d d f r u i t s 1 where f d d i n f is the inflexion value of the logistic growth in degree days s is the steepness of the logistic growth and s d d f r u i t is the number of degree days accumulated by the fruit since its flowering date the carbon demand of a cohort c on day i is computed as the sum of the carbon mass incremented by all fruits in the cohort under optimal conditions it is computed using the derivative of eq 36 as follows 37 d e c f r u i t s 1 d e o p t l i f s d d f r u i t f o v e r 0 i f s d d f r u i t f o v e r where f r u i t s 1 is the number of fruits in the cohort d e o p t is the total carbon demand of a coffee fruit thoughout its development under optimal conditions including growth respiration and f o v e r degree days the physiological age for fruits to become overripe and no longer require carbohydrates because they fall to the ground thereby the total plant scale fruit carbon demand is computed as the sum of the demand of all the cohorts growing on the coffee plant 38 d e f r u i t i c 1 n d e c i the fruit demand for carbon can be considered as a genetic growth potential with optimal fruit growth when there are no limitations to the supply it is a sink strength that depends on the number of fruits and the degree days and is independent of the carbon supply consequently the allocation of carbon to fruits is constrained either by fruit demand or by the carbon supply as in eq 9 the fruit mass is then computed as in eq 24 the fruits that become overripe i e are not harvested are removed from the coffee plant and considered as a mortality coffee bean quality is also computed using the fruit sucrose content of each fruit cohort c based on the number of days after flowering following the model of pezzopane et al 2012 39 s m i c 1 n c m c i c c f r u i t s y 0 s a 100 1 i s x 0 s b 40 s m o p t i c 1 n c m c i c c f r u i t s y 0 s a 100 41 m a t i s m i s m o p t i where s m i is the sucrose mass of all the cohorts c on day i and s m o p t i is the optimal sucrose mass of the coffee bean i e the sucrose mass of the bean if it was fully mature c m c i is the carbon mass of the cth cohort on day i c c f r u i t is the carbon content of the fruit s y 0 is the sucrose content at the beginning of the fruit development s x 0 is the day at which the maturation is at the inflexion point and s a and s b are two maturation parameters harvest maturity is then simply the global fruit maturity on the day of harvest harvesting takes place once each growing season it is triggered on the date when total cumulated fruit mortality has been higher than fruit growth for more than ten days this simple method ensures that harvesting occurs when the fruit stock is at its maximum to optimize yield 3 4 12 shade tree allometry in addition to the common allocation scheme each shade tree species has its own set of allometric relationships any kind of allometry can be implemented and can then be used as input for the metamodels or as an informative model output in this study allometric relationships were used to compute the tree crown radius and height from the tree branch dry mass and the tree density the crown radius and height were then used to compute the lad leaf area density m l e a v e s 2 m c r o w n 3 which is an input for the metamodels for the light extinction coefficients shade tree height is mandatory to compute the aerodynamic conductance and was computed in this study using an allometric relationship with the tree stem dry mass the tree diameter at breast height was also computed from stem dry mass 3 4 13 temperature the temperature of the foliage and the air inside the canopy is computed using the formalism proposed by van de griend and van boxel 1989 using the diffusivities for momentum transport and potential energy flow inside and in between the inertial sublayer the roughness sublayer the air space of each canopy layers the foliage and the soil the temperature of the air inside the canopy is computed as 42 t a i r c a n i l t a i r i h i l ρ i c p g b u l k i with i the day l the canopy layer t a i r the air temperature measured above the canopy h the sensible heat flux m j m 2 d 1 ρ the air density k g m 3 c p the specific heat of air for constant pressure m j k 1 k g 1 and g b u l k the aerodynamic conductance for heat above the canopy m s 1 if the coffee plants are grown in monoculture eq 42 is applied directly to the coffee layer if they are grown under shade trees eq 42 is used for the shade tree and the air temperature inside the coffee canopy is computed as follows 43 t a i r c a n i c o f t a i r c a n i t r e e h i c o f ρ i c p g i n t e r l a y i with t a i r c a n i t r e e the temperature of the air inside the shade tree canopy computed using eq 42 and g i n t e r l a y the aerodynamic conductance at the interface between both canopy layers m s 1 the leaf temperature t c a n of each layer is then computed as 44 t c a n i l t a i r c a n i l h i l ρ i c p g b h i with t a i r c a n the air temperature inside the given canopy layer and g b h the leaf boundary layer conductance for heat m s 1 the temperature at the soil surface is similarly computed as 45 t s o i l i t a i r c a n i c o f h i s o i l ρ i c p g s o i l c a n with g s o i l c a n the canopy to soil aerodynamic conductance m s 1 3 4 14 soil water and energy the soil water balance module is inspired by the biljou model granier et al 1999 which has already been parameterized for this coffee agroforestry system gómez delgado et al 2011 and is given in appendix a it has three layers 0 1 25 m 1 25 1 75 m and 1 75 3 75 m respectively to cover the whole root profile of the coffee plants defrenet 2016 the net radiation of the soil is computed using a maespa metamodel and the partitioning between latent and sensible fluxes was parameterized using the average partitioning from the outputs of the maespa simulations vezy et al 2018 the shade tree and coffee transpiration t r and sensible heat h are simulated using maespa metamodels the net radiation of the shade tree and the coffee layers are computed as the sum of the latent l e and the sensible h heat fluxes of each layer the stand net radiation r n is then computed by summing the net radiation of the shade tree the coffee and the soil 3 4 15 inputs and outputs all parameters needed for a dynacof simulation are stored in specific input files for the shade tree the coffee the soil the site and the meteorology a set of example data is included into the package so any user can start a simulation without any data this set of example file is also archived as a git repository on github com https github com vezy dynacof inputs for convenience the main input values used for this study are listed on the paragraph 3 5 the input variables for the meteorology file should provide at least the maximum and minimum air temperature of the day c the rad or par mj m 2 d 1 and the relative humidity or vapor pressure deficit hpa the full list of mandatory and optional inputs the user can provide are available from the documentation https vezy github io dynacof reference meteorology html the model has 245 output variables mainly for energy balance water balance carbon assimilation and allocation respiration plant growth yield and mortality for each organ type the full list is available from the documentation https vezy github io dynacof reference dynacof html 3 5 model parameterization using a multi objective approach the maespa model was parameterized according to vezy et al 2018 and was run on a 0 2 ha sub plot of 4176 coffee resprouts 3 resprouts per coffee plant in average and 14 shade trees at a half hourly time step throughout the year 2011 the shade trees are not planted along a regular planting pattern and they present large crowns increasing the heterogeneity of the light distribution in the plot the simulated sub plot was reduced to the minimum area possible to decrease computation time while encompassing coffee plants that are always in full sun or only partly under shade in the morning or during the afternoon metamodels were then built from daily plot scale aggregations of maespa outputs and integrated in dynacof fig 1 the metamodels were built using linear regressions with maespa input variables as predictors the maespa dataset created from the simulations of the year 2011 was taken as a representative sample of most of the conditions of the growing cycle with yearly climate variations a highly variable shade tree lai due to almost total leaf fall and highly variable coffee plant structure with resprouts ranging from 0 to 5 years old dynacof was run from january 1979 to the end of december 2016 at a daily time step the climate inputs to the model came from the coffee flux project between 2009 and 2016 and were computed between 1979 and 2008 using the method and data described in hidalgo et al 2017 the values and sources of the parameters used in dynacof are listed in table 1 for climate and coffee in table 2 for the shade tree species and in table 3 for the soil given the large number of parameters and the scarce data on coffee and shade trees some parameters were not measured in this study and were not found in the literature either for example the coffee carbon allocation coefficients λ because allocation coefficients have multiple repercussions on other variables notably on respiration through organ mass and on light interception through lai a multi objective parameterization was needed as computation time was a limiting factor a manual multi objective parameter tuning was preferred to algorithmic optimization the values of the parameters were found by starting from expert a priori and tuned manually to minimize the simulation error for npp and biomass for each compartment using the first year of measurements reported in charbonnier et al 2017 at a stand age of 33 year after planting always keeping the values within a plausible range in any case the outputs of the model were evaluated on the second year of measurements which was not used for model parameterization 4 results 4 1 metamodels the metamodels for shade tree k d i f and k d i r are presented in table 4 and were computed using the shade tree l a d l a d t r e e m 2 m 3 as the sole predictor the metamodel for l u e g c m j 1 was made using climate inputs because it was found to depend more on the environment than on the plant structure the other metamodels for plant transpiration t r m m sensible heat fluxes h m j m 2 and soil net radiation are also presented in table 4 their performance was also assessed using four different statistics which showed that despite being simple models the metamodel predictions were in agreement with the validation sub sample of the outputs of maespa in the simulated year 2011 table 4 indeed all metamodels gave high r2 on the data on which they were trained and high modelling efficiency rather low nrmse and low bias when applied to new data except for k d i r which failed to capture the high day to day variability but still followed the overall trend although h t r e e did present satisfactory statistics overall its nrmse was high mainly due to the low and sometimes negative sensible fluxes of the shade tree which yielded a low average value 4 2 gpp and energy fluxes the modeled gpp water and energy balance from dynacof were compared to the whole period of measurements using data gathered during the long term coffee flux eddy covariance monitoring overall the model outputs for these variables computed in 2011 through metamodels were close to those computed directly from maespa nrmse gpp 15 4 rn 7 2 etr 23 9 indeed plant transpiration plant sensible heat fluxes and soil net radiation were computed using maespa metamodels and the parameter for soil energy partitioning into sensible and latent heat i e soil evaporation was also determined using the average value from the maespa simulations the simulated net radiation was close to measured values with relatively high modeling efficiency 0 88 fig 2 rn and a low bias of 0 24 mj m 2 d 1 that was reflected in the cumulated energy in fig 3 a the high point density along the identity function indicated that modeled evapotranspiration fig 2 aet was relatively close to the measurement in average even though the simulations presented a relatively high error for a few measurements in dark blue in the figure yet the cumulated aet from dynacof fig 3b showed good consistency compared to cumulated measurements indicating that simulated values converge towards the measured value asymptotically the simulated sensible heat fluxes were in agreement with measured fluxes except for a quasi systematic bias of 1 48 mj m 2 d 1 fig 2 h the simulated gpp was close to the observed gpp in general fig 2 gpp with a low positive bias of 0 38 gc m 2 d 1 and an nrmse of 26 27 however modelling efficiency remained low 0 14 mainly due to the high dispersion of the residuals especially for high gpp values yet when considering cumulated values of gpp over the whole period the simulated gpp remained within the range of the measurement error fig 3c we also stress here that gpp remains a data model product derived from net ecosystem carbon flux measurements but subject to several partitioning and modeling assumptions overall dynacof predictions of gpp and energy fluxes were in agreement with their respective measurements over the whole period figs 2 and 3 confirming that the model performs reasonably well beyond the data on which the metamodels were calibrated i e data from 2011 dynacof outputs were also compared to standard equations allen et al 1998 landsberg et al 2001 that are widely used for crops or plantations with more homogeneous canopies i e constant lue and ks parameters for gpp rn computed using an average albedo le computed using the penman monteith equation as expected the error was higher when the net radiation the sensible and latent heat flux and the gpp were simulated using this approach compared to a simulation using metamodels from maespa table 5 and fig c 1 hence the choice of metamodels in dynacof was justified 4 2 1 growth and yield overall the multi objective calibration of dynacof yielded realistic results for most compartments that were documented by field measurements despite the high initial planting density the lai of the shade trees remained relatively low during the first period when the shade trees were pruned i e from planting to thinning when it dropped to a particularly low value of 0 02 m2 m 2 on average in the first year then resumed during the second period the lai of the shade trees subsequently increased to reach a maximum of ca 0 66 m2 m 2 in the last year of the simulation fig 4 the leaves of e poeppigiana shed naturally between january and february in aquiares and then recover rapidly until may taugourdeau et al 2014 the observed phenology was matched by the model with a simulated range and dynamic of lai values close to the observations made in the same plot averaged over the whole measurement period taugourdeau et al 2014 despite a low density of 7 3 trees ha 1 after thinning in 2000 the simulated shade trees intercepted up to 18 4 of the light at maximum lai in 2016 which was consistent with the values measured in the same plot charbonnier et al 2013 the simulated dry mass of tree stem and branches represented on average 47 2 and 4 4 of the total plot shade tree carbon mass while pruned and 27 2 and 27 4 of the total carbon mass respectively at the end of the cycle when left to grow freely the annual stem mass growth was close to linear under pruning management and became close to exponential when trees were not pruned after thinning with the stem npp increasing almost five times in the five last years of the simulation compared to the five years preceding thinning table 6 branch mass grew much more slowly due to higher mortality when pruned but grew as fast as the stem when the tree was left to grow freely as seen in fig 4 both stem and branch mass simulations were in the range of the observations reported by charbonnier et al 2017 in the same plot dynamic simulations of the carbon mass of the coffee organs are plotted for a full growing cycle from 1979 to 2016 in fig 5 coffee leaf carbon mass fig 5a increased rapidly until reaching its maximum value of 211 gc m 2 at three years old onset of fruiting coffee pruning started 5 yap and leaf carbon mass then fluctuated between 58 and 174 gc m 2 until the end of the simulation corresponding to lai values between 1 4 and 4 1 m2 m 2 the lai dynamics expressed a yearly minimum by april after the drier season and just after pruning and a second minimum in september during bean filling very similar to field observations reported by taugourdeau et al 2014 interestingly this realistic phenology was not prescribed into the model but appeared after the introduction of fruit cohorts inducing a strong but progressive competition between sinks here with the leaves throughout the fruiting season resprout wood fig 5b grew rapidly from 0 to 4 yap before the onset of the pruning cycle which occurred every year for all resprouts aged more than 5 resprout wood growth then decreased to reach a stable value of ca 397 gc m 2 at around 13 yap with only intra annual fluctuations due to pruning the perennial compartment represented by both the stump and the coarse roots grew progressively throughout the crop cycle because it was not subject to pruning and has a very long lifespan it reached a maximum value of 1769 gc m 2 or 37 3 tdm ha 1 in the end of 2016 fig 5c the coffee fruit compartment fig 5d started to yield from the third yap with a carbon mass of between 71 and 134 gc m 2 at harvest and an average modeled green bean production of 1382 205 kg ha 1 year 1 the simulated carbon mass of the fruits was close to values measured by charbonnier et al 2017 in 2012 and 2013 the fine roots fig 5e grew rapidly in the early growth stages i e until the third yap when the combined effects of pruning and natural mortality maintained their carbon mass at a relatively stable level from one year to the next with values of ca 134 gc m 2 the reserves compartment fluctuated seasonally always in opposition with the fruit carbon growth the simulated carbon reserve was close to the reserve measured by cambou 2012 in the same plot fig 5f because the model was parameterized using this data the modeled coffee carbon allocation by organs showed that plant reserves represented the compartment with by far the highest carbon flow capturing on average 40 4 of the yearly plant carbon supply during the five last years of the simulation with a daily maximum allocation of 63 1 and a minimum of 0 during fruit production table 6 this compartment also had a high turnover rate because reserves were re distributed back to the carbon supply pool making it an effective carbon buffer for the coffee plant from one season to another leaves and branches were the organs with the highest npp representing 21 5 and 11 9 of the total yearly npp respectively during the last five years of the simulation fine roots represented 9 4 stump and coarse roots 7 3 and fruits 9 5 of total npp the high productivity of the branches was related to their high carbon demand that was often met by the supply as well as for the stump and coarse roots the simulated npp was reasonably consistent with measurements for coffee table 5 for the shade trees no direct measurements of the whole rooting system was available therefore the comparison is not proposed here nevertheless when cumulated into biomass the results for both species were realistic figs 4 and 5 5 discussion 5 1 metamodels simulating complex processes in crop models using metamodels is a promising way to reduce computation intensity and avoid numerous equations that are often an important part of the development and application effort another advantage is that physiological data e g photosynthetic parameters are often sampled at a fine scale but can be used to parameterize a precise model in this case maespa at a sub hourly time step and then be upscaled to field scale and to daily scale in the present study metamodels helped dynacof consider fine scale processes explicitly such as spatial anisotropy instead of only using a parameter e g a percentage of canopy cover which significantly improved the model simulations replacing some metamodels by simple standard plot scale models leads to a dramatic increase in simulation errors as shown in the present study table 5 evidence that our metamodeling approach was appropriate the quality of the predictions of a metamodel relies first upon the ability of the original model to correctly simulate the processes involved in the system and secondly on the ability of the metamodel to reproduce the outputs of the original model the first point was investigated in a previous work vezy et al 2018 where maespa successfully simulated the energy balance and the evapotranspiration in the same experimental plot the second point was addressed by choosing metamodels according to the best possible balance between the model complexity the number of explanatory variables and out of sample prediction quality marie et al 2014 found that despite being slower to compute neural networks and multi linear regressions with two or three level of interactions yielded higher r2 than multi linear regressions with no interactions like those used in our study however four out of ten metamodels in our study gave r2 higher than 0 90 with low nrmse which is considered highly accurate three gave r2 higher than 0 80 which is considered accurate villa vialaneix et al 2012 and only one metamodel could be considered not sufficiently accurate with a r2 of 0 65 for example the lad was used alone to predict the light extinction coefficient of the shade tree layer which was then used to compute its light absorption from its lai the diffuse extinction coefficient was predicted with high accuracy with an ef and a r2 both equal to 0 95 the lad is a useful proxy for within crown foliage aggregation which was determined along with the lai as the most important characteristic to model light penetration by sampson and smith 1993 however other structural variables from the shade trees can be used as predictors to improve the metamodel accuracy this was not the case in our study because shade trees structural data were measured only once for the year 2011 so only one value was given as input to maespa the metamodels for l u e yielded r2 and nrmse similar to those found in christina et al 2016 r2 of 0 87 compared to 0 87 and 0 94 for shade tree and coffee respectively in our study and an rmse of 0 20 gc mj 1 compared to 0 24 and 0 10 shade tree and coffee resp in our study yet despite being generally effective at reproducing a complex model output applying metamodels to new conditions requires caution as they can produce unexpected results outside their training values especially if they use non linear equations or when there is covariance between predictors this was of particular concern in the present study because metamodels were fitted using a one year long simulation of maespa only 2011 to overcome these problems the 2011 data was checked to present a broad range of values for the target variables some conditions were still not met in the training sample such as the period before the year 2000 when shade trees were pruned twice a year while the metamodels were trained on the system with free growing trees yet the 2011 training period included some days with very low e poeppigiana lai i e 0 04 m2 m 2 because e poeppigiana loses all its leaves once a year which helped the metamodel simulate a plausible range of transmittance values under low lai pruning conditions likewise the cumulated evapotranspiration and energy balance were satisfactorily predicted compared to measurements even outside their training period although both computations depended to a great extent upon metamodels the metamodel for the coffee lue predicted an increase in values with a reduction in incoming radiation on the plant layer which is in agreement with previous results reported in charbonnier et al 2017 as a result dynacof simulations of rn gpp and aet were close to those produced by maespa reported in vezy et al 2018 and more importantly close to the eddy covariance measurements from the long term coffee flux monitoring from 2009 to the end of 2016 figs 2 and 3 the agreement between simulated and measured values was particularly strong when cumulative fluxes were compared which confirmed the high degree of consistency throughout the measurement period mostly due to a low bias from the metamodels hence metamodels proved to be powerful tools to overcome the long term trade off between speed accuracy genericity and fast development of growth and yield models able to simulate whole crop rotations 5 2 growth and yield model outputs from dynacof the evaluation of a crop model is often challenging due to the lack of data for parameterization and validation yet our model was subjected to multi objective evaluation using numerous observations from the same experimental field or from the literature nevertheless it should be noted that the model was mostly parameterized using values in the literature and better results would be expected using measurements or an optimization algorithm van oijen et al 2005 however the model satisfactorily predicted most processes at plot scale with little or no discrepancy the total autotrophic respiration represented 55 of the gpp which is close to the 57 reported by litton et al 2007 in a review of results from a wide range of forest ecosystems the simulated lai for coffee was in agreement with the measured lai and the mean simulated leaf dry mass from 2011 132 2 g c m 2 was in agreement with the measured values reported in charbonnier et al 2017 taugourdeau et al 2014 and siles et al 2010 with values of 140 5 143 7 and ranging from 102 to 176 g c m 2 respectively the seasonal behavior of leaf biomass revealed a drop at the end of the drier season corresponding to natural leaf shedding followed by pruning then a rapid increase at the beginning of the rainy season with a secondary minimum when fruit dry mass was high interestingly the simulations well mimicked the seasonal observations reported by taugourdeau et al 2014 and the simulation was close to measured values the seasonality of the lai was represented using two main drivers pruning the leaves once a year and the introduction of fruit cohorts the first is a forced process but the second is the result of successive computations that allows a smooth distribution of the demand for carbon required for grain filling during the period of reproductive development depending on the developmental stage of each cohort however to date the interannual variability in leaf area is barely perceivable in the simulations compared to the field conditions we assume that some processes driving this variability still need to be incorporated in the model for example a dynamic leaf life span using cohorts of leaves more leaf diseases or nitrogen effects indeed american leaf spot als was already included in the model following avelino et al 2007 but coffee leaf rust is the predominant disease affecting coffee plants in this region which is not yet included due to the absence of a published model linking disease severity and leaf loss total npp is the consequence of the carbon assimilation its allocation and the respiration of each specific organ comparing the measured and simulated total npp is an integrative evaluation of the model and the total coffee npp simulated by dynacof was in agreement with the npp measured by charbonnier et al 2017 with an overestimation of ca 3 6 for both years 2011 2012 and 2012 2013 however the most important but most challenging integrative process to simulate is fruit yield because its allocation follows a complex scheme spread out over two years with numerous development stages camargo and camargo 2001 which was modeled in dynacof using a formalism inspired by rodríguez et al 2011 dynacof predicted a green bean yield of 1382 205 kg ha 1 year 1 which was within the range of values observed by campanha et al 2004 van der vossen et al 2015 and of the average yield in central america söndahl et al 2005 furthermore the aquiares farm reported average yields of green beans of around 1333 336 kg ha 1 year 1 between 1995 and 2014 for fields close to the experimental plot confirming that dynacof yield predictions were consistent however this comparison is only indicative and should be interpreted with caution because the conversion from whole fruit dry mass into processed green beans was made using a simple parameter fts see table 1 that may change depending on several factors and because dynacof only simulates potential yield in the absence of fruit diseases or predators furthermore the dataset used to fit the effect of temperature on the buds could overestimate the negative effect because drinnan and menzel 1995 used coffee plants grown in relatively small pots 10 l which can restrict root growth and then negatively affect the physiology of the coffee plant ronchi et al 2006 damatta et al 2019 found similar results on unpublished data though confirming this negative effect is not only coming from the restricted root growth moreover the model was also compared to in situ measurements reported in gc m 2 by charbonnier et al 2017 and predictions were found to be close to measured values even following the same pattern of yearly variability which is particularly hard to achieve considering the number of formalisms in use and the potential cumulated error from one process to another the shift in the shade tree management from pollarded to free growing appeared to have little impact on fruit production or maturity at harvest this apparent stability came from the low density of the shade trees which still transmitted at least 81 6 of the light during the mature state according to dynacof compared to 86 reported in charbonnier et al 2013 charbonnier et al 2017 reported that the higher lue simulated by maespa for coffee plants under higher shade could compensate for most of the reduction in incident par maintaining npp at a nearly constant level as long as shade remains at low values in dynacof gpp was reduced only by 2 6 for a reduction of 7 9 in apar thanks to an increase of 5 8 in the lue between the two periods of shade tree management i e from low lai in pruned shade trees to higher lai in free growing shade trees another strength of the model is the prediction of canopy temperature under shade as a driver for plant biology and water and energy balances thanks to a full soil module inspired from the biljou model and to the maespa metamodels indeed predictions of the cumulated aet and net radiation were very close to the nearly continuous measurements made from 2009 to 2015 given the model gave satisfactorily results for a wide range of processes using our multiple objective strategy of parameterization and evaluation i e different sets of variables measured in the same site at the same time it can provide other information that cannot be discovered from the data only and help researchers identify emergent properties in the system for example coffee lai was strongly affected by pruning once a year and during the period in between by natural mortality and by the high fruit demand at the time of grain filling which was also observed by charbonnier et al 2017 another observation made using the model outputs is that except for stump and coarse roots which are the only perennial compartments biomass increased rapidly in the early stages of the plantation until it reached its maximum value for the whole rotation after which biomass growth started to decrease with pruning and found a new lower equilibrium between growth and mortality the model also reproduced to some extent the biennial fruit production that was reported by cannell 1985 finally although the measured variables were acquired mostly at the end of the rotation the model ran for the full rotation and the simulated variables reached the values measured by the end of the rotation thereby revealing dynamics that remained hidden during the unknown part of the rotation e g biomass reaching a specific equilibrium according to the pruning intensity overall it is clear that the model is able to compute several long term ecosystem services and paves the way for the analysis of possible trade offs between them depending on different management options and climate although the model now needs to be applied in different soil climate and shade management conditions to evaluate its true genericity the process based equations implemented in most modules already imply relative genericity for example the flowering events and timing are based on the model from rodríguez et al 2011 which reproduced satisfactorily these processes on contrasted sites however the user should keep in mind that metamodels are not generic and should be updated when applied to new conditions e g different soil or planting design the overall genericity of dynacof should allow its application to other climate conditions such as under climate changes other locations with different soils and climates and other shade management systems such as coffee grown in full sun or under cordia alliodora banana plants eucalyptus sp or any species tree density and management such as pruning or thinning in specific situations it would be advisable to develop specific modules for coupling the nutrient cycles with the carbon and water cycles already available in the current version of dynacof therefore the current version is assumed to simulate the potential outputs only for situations without any nutrient stresses drought is already present in the model through leaf water potential and affects the reproductive phenology but should be tested and refined for other vegetative limitations 5 3 model adaptation dynacof can be easily adapted by users to other sites other shade trees other coffee varieties or managements by parameterizing the model accordingly adapting the model to other sites require mainly updating the site parameters e g latitude longitude elevation the soil properties e g field capacity pore fraction and the meteorology if the coffee cultivar is changed the parameters linked to the coffee should be checked and updated if necessary e g specific leaf area allocation coefficients base temperature to change the management of the shade tree or the species the user has to update the shade tree parameter file the metamodels should be updated by the user whenever the plants structure or functioning is out of the range they were trained on for example the lue metamodel should be fitted using the atmospheric co2 concentration and air temperature to integrate their effect for simulations under climate changes they also can be replaced by any plot scale equations because they are written as input code the model can then be calibrated and evaluated using field data lai for both shade tree and coffee is a key variable to measure because it is the result and has an impact on many processes the biomass increment of each organ can be measured to evaluate the plant npp for the coffee key variables from the reproductive development are useful to evaluate the simulated yield e g time of initiation and number of buds and flowers and of course yield itself the soil water content can be measured to evaluate the water balance module and the leaf and soil temperature for the energy balance module dynacof can also be modified easily by developers because it was designed as modular as possible with each module called in sequence at daily time step developers can replace any module while still leveraging the others easily for example the coffee module could be replaced by a module simulating any other plant annual or perennial e g cocoa wheat sugar cane rice new modules can also be added such as for nitrogen cycle this module could compute the nitrogen uptake allocation and content for all organs from each plant species and the mineralization processes from decaying organic matter which already exist in the model and microbial biomass in the soil with eventual mineral fertilization input by management e g urea synthetic fertilizers irrigation or natural processes e g precipitations 6 conclusion dynacof dynamic agroforestry coffee crop model was developed to simulate the effects of the environment the soil the species of shade tree and the management practices on coffee growth and yield the shade management module can be set to any shade type and density under full sun or agroforestry systems applying pruning or thinning at any age if required the model can be used for full rotations at a daily time step for any surface area from plot to landscape or even to a region if properly distributed under current past or future climate conditions as long as the metamodels built from maespa model simulations are updated to the target conditions the model was parameterized and evaluated using a comprehensive and unique dataset for energy and water balance biomass and npp from an experimental site in the aquiares farm in costa rica a substantial advantage of dynacof being a tree average plot model is the possibility to parameterize it using plot averages or totals which are more frequently available from farms e g yield pruning intensity coffee quality etc because data remain scarce especially under agroforestry management two other important features of the model are the simulation of the canopy temperature instead of air temperature to control the plant growth according to the shade level and the use of cohorts of flowers and fruits to consider grouped flowering in sub tropical conditions and distributed flowering in equatorial climates the model is implemented as an r and julia packages for easy sharing and collaboration and can be easily modified by adding new modules to compute pest attacks nutrient cycling soil organic matter decomposition or soil respiration the methodology can be further generalized for any type of shade or climate by using different maespa simulation sets to train the metamodels dynacof was built using a set of generic modules e g functions for aerodynamic conductance that can be used in other models to simulate any type of agroforestry systems or intercropped systems in conclusion dynacof concentrates considerable ecophysiological knowledge on coffee and is an efficient tool to evaluate and optimize coffee crop yield ecosystem services and their trade offs in response to climate conditions and management scenarios it was also designed to predict the impacts of climate change on coffee yield and the potential of changes in management to mitigate such effects declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project was funded by agence nationale de la recherche macacc projet anr 13 agro 0005 viabilité et adaptation des ecosystèmes productifs territoires et ressources face aux changements globaux agrobiosphere 2013 programme cirad centre de coopération internationale en recherche agronomique pour le développement and inra institut national de la recherche agronomique the authors are grateful for the support of the aquiares farm https aquiares com and for the long term coffee agroforestry trial the soere f ore t which is supported annually by ecofor allenvi and the french national research infrastructure anaee f http www anaee france fr fr the cirad ird safse project france and the pcp platform of catie the coffee flux observatory was supported and managed by cirad researchers we are grateful to the staff in costa rica in particular alvaro barquero alejandra barquero jenny barquero alexis perez guillermo ramirez rafael acuna manuel jara alonso barquero for their technical and field support the project analyzes largely benefited from the montpellier bioinformatics biodiversity mbb computing cluster platform which is a joint initiative of laboratories grouped in the cemeb labex mediterranean center for environment and biodiversity as part of the program investissements d avenir anr 10 labx 0004 appendix e supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix e supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104609 appendix a soil water balance model the soil water balance model is largely derived from the biljou model granier et al 1999 it is a bucket model meaning that the soil is represented by the depth dimension only itself divided into layers of given thickness 1 water balance the water flow is managed sequentially as in any other bucket model 1 1 interception and leaf evaporation during a rainfall event water can be either intercepted by the plants canopy or directly reach the soil surface the water intercepted by the plants is either stored on the canopy surface or flow along the branches and trunks to finally reach the soil the water that is stored on the canopy surface is then gradually evaporated back to the atmosphere the first step for computing the water balance in the model is to compute a maximum potential rainfall interception using the total stand lai and an interception parameter as follows a 1 i n t e r c m a x i n t e r c s l o p e l a i p l o t any water intercepted by the canopy when the canopy water retention is already at full capacity i e at i n t e r c m a x mm is considered as throughfall water the daily evaporation of the water stored in the canopy bucket is computed using the penman monteith equation as found in allen et al 1998 with an infinite stomatal conductance and a set of aerodynamic conductances 1 2 surface runoff water reaching the soil surface enter the surface layer w s u r f a c e r e s mm any water entering this layer when it is already full is considered as excess surface runoff which is added to the superficial runoff itself computed using a parameter a 2 s u p e r f i c i a l r u n o f f k b w s u r f a c e r e s 1 3 infiltration water from the surface layer infiltrates the first soil layer up to the first layer infiltration capacity w1 which is computed as follows a 3 i n f i l c a p a f o i f w 1 w m 1 f o w 1 w m 1 f o f c w f 1 w m 1 i f w 1 w m 1 a n d w 1 w f f c i f w 1 w m 1 a n d w 1 w f with w 1 the first layer water content mm w m 1 the minimum water content of the first layer w f 1 the field capacity of the first layer and f o and f c the maximum and minimum infiltration capacity respectively mm day 1 then w 1 is updated by adding the water that infiltrated from the surface bucket to its previous water content if w 1 exceeds w f 1 after this operation the excess water drains into the second layer this last operation is repeated for the two following layers w 2 and w 3 with their own field capacity w f 2 and w f 3 1 4 soil surface evaporation the soil surface evaporation mm is computed using the soil net radiation coming from a metamodel of maespa and a partitioning parameter s o i l l e p a 4 e s o i l r n s o i l s o i l l e p λ with λ the latent heat of vaporization mj kgh2o 1 1 5 root water uptake the water that is absorbed by the plants roots r o o t w a t e r e x t r a c t l for a given layer l is computed from the total stand transpiration t t o t a l the total root fraction in the layer r o o t f r a c t i o n l and the extractable water from the layer e w l as follows a 5 e w l w l w m l a 6 r o o t w a t e r e x t r a c t l min t t o t a l r o o t f r a c t i o n l e w l each layer s water content is then updated by removing to it the root water uptake 1 6 water potential the soil water potential ψ s o i l m p a is computed using the equation from campbell 1974 as follows a 7 ψ s o i l ψ e w t o t 1000 θ s b with ψ e mpa the air entry water potential w t o t the total soil water content mm and θ s the saturated water content m3 m 3 and the leaf water potential ψ l e a f m p a is computed as follows a 8 ψ l e a f l ψ s o i l t l m h 20 k t o t where t l is the plant transpiration m m m h 20 the molar mass of water and k t o t the soil to leaf hydraulic conductance m o l m 2 s 1 m p a 1 2 energy balance the soil energy balance is computed using the soil net radiation from the metamodels of maespa and the partitioning coefficient as in eq a 4 for both the latent and sensible fluxes the soil heat storage is neglected because its variability is low at daily time scale and because it tends to return at equilibrium after several days appendix b shade tree allometric equations several allometric relationships are used in the shade tree growth module only the shade tree height is mandatory for the other parts of the model because it is used to compute the canopy boundary layer conductance any other allometric equation can be added to the model via the tree parameter file and can be used for user custom metamodels or as informative output 1 dbh the diameter at breast height dbh m of the erythrina poeppigiana shade tree is computed following the equation from rojas garcía et al 2015 a 9 d b h d m s t e m c c w o o d 10 s t o c k i n g 0 5 0 625 2 height tree height m is computed following the equation from van oijen et al 2010 a 10 h e i g h t 0 46 d m s t e m 1000 s t o c k i n g 0 5 if the shade tree is pruned it breaks the relationship between the tree stem dry mass and its height as shown in eq a 10 instead the tree height is computed using the trunk height as follows a 11 h t r u n k 3 1 e 0 2 a g e a 12 h t o t h c r o w n h t r u n k the underlying hypothesis behind eq a 11 is that e poeppigiana are pruned by stakeholders so the trunk height do not exceed 3 m high 3 crown dimensions the crown radius m is computed as follows a 13 r c r o w n p c r o w n π with p c r o w n the crown projection itself computed as a 14 p c r o w n 8 d m b r a n c h 1000 s t o c k i n g 0 45 with d m b r a n c h the shade tree branch dry mass in g m 2 the crown height h c r o w n is taken as equal to the crown radius as shown in table 2 from charbonnier et al 2013 4 lad the leaf area density lad is computed as follows a 15 l a d l a t r e e r c r o w n 2 h c r o w n 2 π 4 3 with l a t r e e the shade tree leaf area appendix c comparing dynacof with and without metamodels to assess the importance of using metamodels from maespa to integrate fine scale processes to dynacof we compared dynacof outputs for gpp rn le and h with and without metamodels the gpp without metamodels was simulated using constant values for the light use efficiency and constant values for the light interception coefficients of the beer lambert s equation for both the coffee and the shade tree layers the parameterization was made using the average values from the maespa simulations the net radiation without metamodels was simulated using the equation from allen et al 1998 at plot scale using the albedo computed by maespa the latent heat flux without metamodels was computed using the penman monteith equation at stand scale penmon function from dynacof https vezy github io dynacof reference penmon html the sensible heat flux without metamodels was computed as the difference between the net radiation and the latent heat flux both computed without metamodels fig c 1 net radiation rn latent le and sensible h heat flux and gross primary productivity gpp simulations using dynacof with metamodels from maespa red or stand scale equations blue compared to field measurements fig c 1 figure c 1 shows that gpp simulation was improved when using metamodels and that net radiation was also improved using metamodels but mainly because the stand scale reference equation had a systematic positive bias compared to measurements this bias could potentially be corrected knowing it a priori the sensible heat flux were also systematically more biased without metamodels the latent heat flux simulated without metamodel was close to the measurements for low values 5 mj m 2 d 1 but was increasingly overestimated with higher values appendix d further plots 1 light interception gpp and ra the contribution of the shade tree and the coffee plants to lai apar gpp and ra are shown in fig d 1 the highest coffee contribution to the plot scale gpp can be explained by its higher lai and hence higher light interception compared to the shade tree the shade tree thinning in 2000 has a high impact on these variables during the first three years fig d 1 comparison of dynamic outputs from dynacof between shade tree and coffee for leaf area index lai absorbed photosynthetically active radiation apar gross primary productivity gpp and autotrophic respiration ra starting from 1979 to 01 01 until 2016 12 31 fig d 1 2 water balance dynacof simulates a broad range of informative variables related to the water balance of the system fig d 2 fig d 2 dynacof outputs related to the water balance at daily time scale and plot scale for the period starting from 1979 to 01 01 until 2016 12 31 θ is the volumetric water content for the full soil profile and ψleaf the leaf water potential fig d 2 3 temperatures dynacof also simulates several temperatures and aerodynamic conductances in the system to better represent the microclimate experienced by the plants dynacof uses the air temperature measured above the canopy as input and computes the temperature of the air inside the shade tree canopy and inside the coffee canopy and uses them to compute the leaf temperature of the shade tree and the coffee and the soil temperature these computations are dependent from other computations such as the wind extinction the sensible heat flux of each layer plants and soil and a sequence of aerodynamic conductances the temperatures simulated by the model are shown in fig d 3 fig d 3 input tair and simulated temperatures the simulated temperatures are presented as the difference between the simulated temperature and the input tair for easier assessment fig d 3 4 litter the litter is simulated by the model as a mortality of biomass the coffee leaf and resprout wood litter at plot scale is mainly impacted by the pruning effect each year fig d 4 as expected the fruit litter follow a seasonal variation related to the fruit production the shade tree leaf litter is mainly impacted by the pruning effect before 2000 pruning management and then by the natural seasonal variation the simulated litterfall from the coffee i e mortality from leaves resprout wood fruits is close to the observations made by charbonnier et al 2017 with values of 482 gc m 2 year 1 compared to 450 gc m 2 year 1 from their measurements the simulated tree litterfall i e leaves branches wood is also close to the observations with a value of 96 gc m 2 year 1 and 110 gc m 2 year 1 respectively high values of tree litterfall are explained by tree pruning before 2000 fig d 4 simulated litter fluxes for the compartments with the highest contribution for the coffee leaf resprout wood fruits and the shade tree leaf fig d 4 
26096,the dynacof model was designed to model coffee agroforestry systems and study the trade offs to e g optimize the system facing climate changes the model simulates net primary productivity npp growth yield mortality energy and water balance of coffee agroforestry systems according to shade tree species and management several plot scale ecosystem services are simulated by the model such as production canopy cooling effect or potential c sequestration dynacof uses metamodels derived from a detailed 3d process based model maespa to account for complex spatial effects while running fast it also includes a coffee flower bud and fruit cohort module to better distribute fruit carbon demand over the year a key feature to obtain a realistic competition between sinks the model was parameterized and evaluated using a highly comprehensive database on a coffee agroforestry experimental site in costa rica the fluxes simulated by the model were close to the measurements over a 5 year period nrmse 26 27 for gross primary productivity 28 22 for actual evapo transpiration 53 91 for sensible heat flux and 15 26 for net radiation and dynacof satisfactorily simulated the yield npp mortality and carbon stock for each coffee organ type over a 35 year rotation graphical abstract image 1 keywords crop model coffea arabica maespa gpp erythrina poeppigiana plant to plot scale 1 software and data availability the dynacof model was developed as an r package r core team 2019 and as a julia package bezanson et al 2017 full documentation is available on its dedicated website https vezy github io dynacof the code is open source gnu gplv3 license and available on github repositories r https github com vezy dynacof julia https github com vezy dynacof jl and archived on zenodo https doi org 10 5281 zenodo 1256816 the input data used for this study is available as example data from this repository and the data for model evaluation is available from the fluxnet website http www europe fluxdata eu home site details id cr aqc 2 introduction the key role of crop models is to help understand and predict the links between crop development and climate soil management facilitation and competition between species crop models can provide insights into the main emerging agricultural challenges such as food security sustainability how to enhance ecosystem services and how to cope with the possible negative effects of climate changes spiertz 2012 there is an increasing need to address these issues at global scale to identify the different solutions available makowski et al 2014 especially when the products are exchanged on the global market like wheat maize soybean cocoa or coffee perennial plantations are difficult to study because their relatively long growing cycle extends the period necessary for data acquisition and because the heterogeneity of the canopy sometimes significantly increases the intra plot light and micro meteorological anisotropy such as for temperature vapor pressure or aerodynamic conditions luedeling et al 2014 2016 agroforestry systems afs are probably the most complex perennial agroecosystems malézieux et al 2009 because they have the most heterogeneous vertical and or horizontal canopies and these affect all ecosystem fluxes charbonnier et al 2013 vezy et al 2018 yet afs have the potential to enhance ecosystem services jose 2009 lin 2010 taugourdeau et al 2014 such as carbon sequestration jose and bardhan 2012 oelbermann et al 2004 and to mitigate climate pressure on crops lin 2007 in costa rica coffee arabica is mostly grown under afs management because it is assumed to improve coffee bean quality and to expand the cropping area to sub optimal low altitude and warmer areas muschler 2001 but such assumptions depend mostly on altitude local climate and postharvest processing worku et al 2018 modeling the energy water and carbon balance of these agroecosystems could provide insights into their functioning and allow stakeholders to test the in silico trends of new management practices e g density pruning thinning date or intensity or species arrangement on given outputs such as yield or other ecosystem services however several factors make these systems challenging to model first there are many options for shade management with highly heterogeneous canopies ranging from free growing low density shade trees like cordia alliodora to high density heavily managed low trees or plants such as banana or pollarded erythrina poeppigiana trees van oijen et al 2010a second the coffee reproductive phenology is a complex process that lasts for about two years camargo and camargo 2001 with competition between reproductive and vegetative compartments charbonnier et al 2017 and bienniality at the plant scale schnabel et al 2018 blossoming is mostly synchronized in sub tropical regions but can also be highly asynchronous in equatorial regions which impacts the distribution of fruit carbon demand and in turn carbon allocation to other organs rodríguez et al 2011 third coffee plants are often pruned every five to six years to sustain high levels of production on young resprouts it is also assumed that the reserve compartment plays a major role in bean production with biennial sprout dynamics cannell 1985 fourth there are very few comprehensive datasets that can be used to calibrate and test multi objective models for ecosystem services energy carbon and water balance aboveground and belowground biomass npp fruit yield and more model development implies identifying prioritizing and balancing the most important processes and the scale at which the model should simulate them in coffee systems we assume that i absorbed light ii light use efficiency lue iii within canopy temperature iv water and v nutrient uptake are among the most important primary processes because they regulate carbon assimilation respiration evapotranspiration vegetative growth flowering and fruit development the next most important processes may be vi shade tree and coffee leaf phenology that regulate light absorption canopy temperature and transpiration vii carbon partitioning to compute net primary productivity npp mortality litterfall and organ biomass and viii a detailed phenology of the reproductive organs comprising all stages from the appearance of cohorts of buds flowers and fruits until harvest or the overripe stage several models have already been developed to simulate coffee grown in full sun or agroforestry systems rodríguez et al 2011 proposed a model to simulate coffee in monoculture only from branch to whole plant scales the model was calibrated from planting to five years old the strength of this model lies in the fine phenology and physiological processes of the modeled coffee plant using branch level cohorts of flowers and fruits over the entire two year reproductive cycle indeed cohorts are required to realistically distribute the demand for carbon of the fruits over the course of the season and not all at once this model was successfully calibrated for colombian and brazilian sites two regions with contrasting climate and flower phenology subtropical and equatorial however this model was not designed for large plots long rotations or agroforestry coffee light absorption is computed using the beer lambert law using a constant coefficient of extinction absorbed light is converted into photosynthesis using constant light use efficiency and coffee pruning shade trees canopy temperature water and energy balance are not implemented in the model another model was developed by van oijen et al 2010 b this is a 1d plant average plot scale model for coffee grown in agroforestry systems simplifying the intra plot microclimate into either below shade or in full sun one clear advantage of this model is its ability to compute several ecosystem services and to incorporate various types of shade tree management and species and a thorough parameterization approach the model is simple fast and can be run under changing climates it was recently applied in east africa under climate change scenarios by rahn et al 2018 the main limitations of the model are i its light transmission module does not consider light distribution as a continuum under shade trees as described in charbonnier et al 2013 ii its formalism of lue which is not influenced by the intra plot light variability even though charbonnier et al 2017 found it to be greatly impacted iii its lack of a reserve compartment and of a cohort module and again iv the absence of energy balance and temperature of the canopy to drive the plant respiration reproductive development two other models have also been applied to coffee in an agroforestry system using 3d light interception modules in dauzat et al 2001 where only a sample of a few coffee plants were simulated and using the maespa model to simulate the whole system since maespa was recently demonstrated to accurately predict light distribution canopy temperature and water and energy balance in such systems charbonnier et al 2013 2017 vezy et al 2018 the model can readily compute all variables that are potentially influenced by the complex canopy structure however its relatively high computation time still limits its application for full rotations of coffee under afs and the model does not simulate growth and yield we argue that a proper combination of the inherent strengths of the above described models could provide significant improvements and extend application domains it would involve combining cohorts and reserves at the plant scale variable canopy temperature and intra plot microclimate and lue while allowing a reasonable level of abstraction to insure rapid simulations multiple plots crop rotations management etc in this study we built surrogate models i e metamodels of maespa for the spatial dependent variables and integrated them into a simpler growth and yield model to avoid expensive computation and development time these metamodels are simple instantaneous equations that efficiently compute a given output of a complex model in other words it is a reduction of a complex model intended to emulate the behavior of complex interactions between variables e g spatial heterogeneity into one empirical equation becoming an input for the next crop model metamodels are generally used to better understand the processes at stake in a model and to assess model sensitivity and uncertainty christina et al 2016 faivre et al 2013 for the purpose of optimization razavi et al 2012 or to make faster and reasonably accurate predictions for a given variable that is usually computed by a time consuming model but with fewer simulation errors compared to simpler models marie et al 2014 metamodels are often used as an efficient and simple tool to combine models at different time and or space scales without running the finer scale model iteratively consequently we designed dynacof to incorporate a plant scale reproductive phenology formalism inspired by rodríguez et al 2011 but dependent on canopy temperature with different sub modules to adapt coffee and shade tree management density and tree species as in van oijen et al 2010 b and metamodels calibrated from maespa simulations for spatially dependent variables such as diffuse and direct light extinction coefficients light use efficiency leaf water potential transpiration and sensible fluxes vezy et al 2018 regarding model parameterization calibration and evaluation for coffee modelling several strategies are proposed in the literature most of which depend on the availability of field data rodríguez et al 2011 assumed that the main factor that influences yield variability is latitude because of its impact on phenology so they tested their model on an equatorial site and on a sub tropical site with two distinct set of parameters for each situation the former model was evaluated against field data on biomass leaves branches berries stem and roots and total plant nitrogen content gathered at three distinct sites van oijen et al 2010 b and later rahn et al 2018 who further developed the caf2007 model proposed a calibration based on an extensive screening of the literature and a sensitivity analysis but no model evaluation against field data at this stage alternatively we propose a multiple objective strategy of evaluation in this study relying on a large range of state and flux variables measured at the same time by the end of the crop rotation including eddy covariance fluxes coffee and shade tree biomass measured at organ scale necromass yield npp water balance and energy balance and finally farm registers to describe management during a complete rotation charbonnier et al 2013 2017 defrenet 2016 gómez delgado et al 2011 taugourdeau et al 2014 vezy et al 2018 consequently the aims of the present study are to i develop metamodels for spatially dependent variables based on maespa which has already been calibrated and validated on coffee agroforestry systems ii develop a new plot scale 1d 5 layer shade tree coffee and 3 soil layers average plant ecophysiological process based model for coffee crops grown under agroforestry or in full sun while combining the advantages and strengths of three previously published models nutrients are considered non limiting in this first version which is realistic for many field conditions iii parameterize and evaluate the model using a multi objective approach applied to an extensive dataset from a long term observatory including the energy and water balance gpp yield npp and carbon mass per organ 3 materials and methods 3 1 site description the research site is located on the aquiares coffee farm 6 6 km2 between 9 56 8 and 9 56 35 n and 83 44 39 and 83 43 35 w itself located in the central caribbean area of costa rica the climate is tropical humid with no marked dry season 3014 mm mean annual precipitation and 19 5 c mean annual temperature during the 1979 to 2009 period the elevation of the research site ranges between 1020 and 1280 m a s l and the mean slope is 11 31 the vegetation consists of coffee plants coffea arabica l var caturra planted below erythrina poeppigiana a leguminous shade tree both were planted in 1979 shade trees were originally planted at a density of approximately 250 trees ha 1 and pollarded twice a year to optimize the light transmitted to the coffee layer they were thinned to about 7 4 trees ha 1 in 2000 and left growing freely until the end of the study the shade trees do not follow any particular planting pattern the aquiares farm is intensively managed with several applications of fertilizer per year 214 44 kg n ha 1 yr 1 and a regular selective pruning of the coffee shoots a practice often used by farmers to avoid a drop in production due to exhaustion the farm complies with the rainforest alliance for its pest and weed management weeds are scarce between 1995 and 2016 the aquiares farm reported average yields of green coffee around 1333 336 kg green coffee ha 1 yr 1 3 2 comprehensive database of measurements coffee flux is a collaborative research observatory https www umr ecosols fr recherche projets 53 coffee flux monitored continuously since 2009 and located on the aquiares coffee farm this research site has been intensively studied and described in detail notably for hydrology and eddy covariance by gómez delgado et al 2011 lai by taugourdeau et al 2014 light budget by charbonnier et al 2013 belowground biomass and npp by defrenet 2016 ecosystem biomass npp and lue by charbonnier et al 2017 and energy balance water balance and surface temperature by vezy et al 2018 the measurements used in this study to evaluate the model included half hourly n e e net ecosystem exchange h sensible heat flux l e latent heat flux and r n net radiation measured using an eddy covariance tower the online reddyproc tool wutzler et al 2018 was used to assess gross primary productivity gpp following the lasslop et al 2010 model option based on daily hyperbolic curves to estimate photosynthesis this model option was chosen because the coffee flux site is mountainous prone to night time advection the flux measurements were integrated at a daily time scale for comparison with dynacof outputs the lai of the shade tree l a i t r e e was also measured using a lai2000 li cor ne usa and the coffee lai l a i c o f f e e using a normalized difference vegetation index ndvi sensor positioned 25 m above the ground at an angle of 15 to the vertical with 45 view angle and converted to lai according to charbonnier et al 2013 the light transmittance by shade trees was also measured using the lai2000 the carbon mass gc m 2 of the shade tree stem and branches and the coffee resprout wood fine roots stump and coarse roots were all measured during two consecutive years 2011 2012 and 2012 2013 using dimension measurements and site specific allometric relationships the npp of each compartment of the shade tree and coffee plant was also computed from the biomass increment and from litter production further details about the methods used for measurement are available in charbonnier et al 2013 and charbonnier et al 2017 3 3 maespa model and metamodel conception 3 3 1 description of maespa maespa is a 3d explicit process based model duursma and medlyn 2012 medlyn 2004 wang and jarvis 1990 used to simulate forest energy water and carbon fluxes at the scale of the individual tree the light interception module canopy temperature water and energy balance of the model have already been calibrated used and validated on the same agroforestry system charbonnier et al 2013 vezy et al 2018 maespa is particularly well suited to simulate agroforestry system fluxes because it describes the forest at voxel scale which is a homogeneous representation of a sub part of the tree crown it can manage several tree species with their own position on the plot their overall structure crown height width etc and their physical and physiological parameters thus maespa computes a fine estimation of the light interception energy water and carbon fluxes of each plant in the forest and of the soil while taking the spatial heterogeneity of the canopy into account however maespa lacks a carbon allocation module or growth process and requires computationally intensive simulations e g a week of computation on the shared montpellier bioinformatic biodiversity mbb computing cluster platform to complete a distributed simulation of a 0 2 ha afs plot including 4176 coffea arabica sprouts and 14 erythrina poeppigiana shade trees over 1 year at 30 min time scale consequently maespa was used here mainly to compute metamodels to simulate spatial dependent variables 3 3 2 metamodels the main process affected by canopy complexity is probably the photosynthetically active radiation absorbed a p a r by the canopy charbonnier et al 2013 heterogeneous canopies like those of shade trees in afs tend to violate the assumption of a constant value for diffuse k d i f and direct k d i r light extinction coefficients because of non uniform spatial distribution of leaf area and because the leaf area density and foliage aggregation can change over time sampson and smith 1993 sinoquet et al 2007 furthermore a comparison between coffee plants grown in monoculture and under an agroforestry system showed that canopy complexity also affected canopy temperature water and energy partitioning light interception transpiration and stomatal conductance vezy et al 2018 metamodels were computed from maespa simulation outputs for diffuse k d i f and direct k d i r shade tree light extinction coefficients coffee and shade tree light use efficiency l u e g c m j 1 transpiration t r m m d 1 plant sensible heat flux h m j m 2 d 1 and soil net radiation r n s o i l m j m 2 d 1 metamodels are used to reproduce as far as possible the link between a set of input variables and the desired output variable as if it had been computed by the maespa model maespa simulations were also used to find the values of some parameters such as the coffee layer light extinction coefficients that had low variability throughout the simulation and hence were assumed constant in dynacof the partitioning parameter s o i l l e p see appendix a 2 used to compute soil sensible and latent heat flux from the soil net radiation in dynacof was also parameterized using maespa outputs metamodels were fitted using multilinear regressions selected according to a trade off between the number of explanatory variables their genericity and range of application and their accuracy obtained using different statistics ef modelling efficiency r2 r squared nrmse normalized root mean squared error and bias maespa was used to simulate one year of the coffee agroforestry system from the coffee flux site in the aquiares farm in order to calibrate the metamodels the parameterization and description of these maespa simulations are reported in vezy et al 2018 the outputs considered are converted from half hourly values at shade tree and coffee resprout scale to daily plot scale values for each plant layer during the same year fig 1 the metamodels were trained on 80 of maespa simulation outputs aggregated at daily time scale and evaluated over the 20 remaining validation data to compute out of sample statistics both training and testing partitions were sampled based on the dependent variable subgroup percentiles using the createdatapartition function in the caret r package kuhn 2019 the input variables of maespa used as predictors for metamodels were either related to climate or to plot average plant structure climate variables included daily air temperature vapor pressure deficit par diffuse and direct light fractions wind speed and air pressure the atmospheric co2 concentration can also be used for the l u e metamodel but were not useful for the present study input variables concerning shade tree structure for the metamodels were leaf area m l e a v e s 2 t r e e 1 crown height m trunk height m crown radius m trunk diameter m and all derivatives such as the leaf area index lai m l e a v e s 2 m s o i l 2 leaf area density lad m l e a v e s 2 m c r o w n 3 tree density t r e e s h a 1 or crown projection m c r o w n 2 m s o i l 2 all being plot averages the lai was measured continuously in the plot while the other structural data were measured only once during the 2011 campaign the variability and the interactions between the predictors were checked to insure the range of values was similar in the maespa simulation dataset and in the application dataset used for dynacof the metamodels are not mandatory in dynacof and can be replaced by any equation or fixed value hence dynacof was also run using standard equations instead of the metamodels to assess their relative contribution to the modeling performance for the plot scale net radiation rn latent le and sensible h heat flux and gross primary productivity gpp for this purpose the fao recommended plot scale equation from allen et al 1998 was chosen as a reference for comparison with the current computation for rn in dynacof le was computed using the penman monteith equation allen et al 1998 h was computed as the difference between rn and le and gpp was computed using a constant light use efficiency l u e and constant light extinction coefficients k d i f and k d i r for both the coffee and the shade tree layers the average values computed from maespa simulations were used to compute the constant l u e and k s coefficients on this modeling experiment 3 4 description of the dynacof model 3 4 1 introductory description dynacof which stands for dynamic agroforestry coffee crop model is a daily plot scale crop model murthy 2004 with two layers of vegetation shade trees and coffee plants and three soil layers aimed at simulating the growth and yield of coffee plantations under various shade tree species and management options considering the spatial heterogeneity of the shade tree canopy the coffee layer can be simulated either in monoculture or in agroforestry systems each layer is simulated sequentially at a daily time step variables with high intra plot variability i e light absorption lue transpiration plant sensible heat flux and soil net radiation are all computed using metamodels from maespa the model accounts for potential competition for light acquisition and water availability between plant and soil layers nutrients are considered non limiting in this first version which is realistic for many field conditions in costa rica water competition is simulated virtually from the day to day fluctuations in water content in each shared soil layer that can be reduced by drainage and evapotranspiration or increased by precipitation through throughfall this simple formalism should be sufficient given the absence of water limitations in the application concerned vezy et al 2018 but it can also reproduce the competition between plants under more constrained conditions 3 4 2 light interception and photosynthesis the diffuse a p a r d i f i m j m 2 d 1 and direct a p a r d i r i m j m 2 d 1 daily absorbed photosynthetic active radiation of each plant layer are computed using the beer lambert s law of light extinction 1 a p a r d i f i p a r d i f i 1 e k d i f l a i i 2 a p a r d i r i p a r d i r i 1 e k d i r l a i i with p a r d i f i and p a r d i r i the diffuse or direct daily photosynthetically active radiation m j m 2 d 1 reaching the layer on day i k d i f and k d i r the light extinction coefficient of the layer for the diffuse and direct light respectively and l a i the leaf area index m l e a f 2 m s o i l 2 of the layer both stand scale light extinction coefficients of the shade tree layer are computed using metamodels from maespa while the extinction coefficients of coffee were approximated to be constant after maespa simulations only a slight variability could not be explained using stand scale factors p a r d i f and p a r d i r are computed as the incoming p a r minus the p a r absorbed by the upper layer if any neglecting the p a r reflected back by the canopy the gross primary productivity g p p g c m 2 d 1 of each layer is then computed from the sum of diffuse and direct a p a r multiplied by the light use efficiency l u e g c m j 1 which is derived from a maespa metamodel 3 g p p i a p a r d i f i a p a r d i r i l u e i 3 4 3 carbon supply a whole plant daily gross carbohydrate budget gcb is computed from the daily g p p available reserves and plant maintenance respiration as follows 4 g c b i g p p i k r e s c m r e i 1 r m i the carbon available from reserves for day i is computed as a fraction k r e s of the carbon mass of the reserves from the previous day c m r e i 1 g c m 2 r m is the maintenance respiration g c m 2 d 1 see eq 6 the plant daily carbon supply is then computed as 5 s u p p l y i g c b i i f g c b i 0 0 i f g c b i 0 if g c b i is negative it means that the carbon available from g p p and reserves was not sufficient to support the maintenance respiration so s u p p l y i is equal to 0 and the missing carbon is considered as mortality imputed equally to all organs see eq 23 3 4 4 maintenance respiration the maintenance respiration requirement r m is computed as the sum of maintenance respiration of all plant organs litton et al 2007 the maintenance respiration of each organ r m j is computed using its carbon mass c m g c m 2 from the previous day following a q10 formalism as reported in dufrêne et al 2005 6 r m j i p a j c m j i 1 c j m r n q 10 j t j t m r 10 where j is the organ i the day p a j 0 1 the living fraction of the organ c m j i 1 g c m 2 the carbon mass of the organ from the previous day n c j g n g c 1 the organ nitrogen content m r n g c g n 1 d 1 the respiration rate per nitrogen unit q 10 j dimensionless the response of organ respiration to temperature t j c the temperature of the organ and t m r c the base temperature for maintenance respiration the leaf temperature of the plant layer t c that is computed in eq 44 is used as a proxy for t j except for the roots where the soil temperature t s o i l is used instead 3 4 5 carbon allocation to organs the allocation of carbohydrates to the organs is limited by either the s u p p l y i or by the organ demand for carbon the s u p p l y i is distributed to the different organs following a hierarchical allocation scheme lacointe 2000 for coffee charbonnier et al 2017 reported that allocation to the woody compartments remained quite steady whatever the fruit load so dynacof supplies this compartment with priority in contrast carbon allocation to fruits appeared to prevail over allocation to leaves charbonnier et al 2017 gutierrez et al 1998 so dynacof supplies carbon assimilates to fruits just after woody compartments and the remaining carbon is allocated to leaves and fine roots and eventually reserves the demand from the woody compartments is considered to always be proportional to the s u p p l y i the demand from fine roots and leaves is considered constant whereas the fruits demand is computed see section 3 4 11 if the demand for assimilates by the fruits is sufficiently high the carbon allocated to fruits potentially represents all the remaining global carbon s u p p l y i after allocation to the woody compartments leaving nothing for the leaves and the fine roots in practice the carbon s u p p l y i is first partitioned between shoot wood c a s h o o t i and stump and coarse roots wood c a s c r i using constant coefficients λ s h o o t and λ s c r 7 c a s h o o t i λ s h o o t s u p p l y i 8 c a s c r i λ s c r s u p p l y i λ s h o o t λ s c r 1 carbon allocation to the coffee fruits is computed as the minimum between the total fruit demand see eq 38 and the remaining carbon supply 9 c a f r u i t i min d e f r u i t i s u p p l y i c a s h o o t i c a s c r i the remaining carbon supply if any is then shared between leaves and fine roots following a coefficient of allocation λ r j for the remaining carbon 10 s u p p l y l e a f i λ r l e a f s u p p l y i c a s h o o t i c a s c r i c a f r u i t i 11 s u p p l y f r o o t i λ r f r o o t s u p p l y i c a s h o o t i c a s c r i c a f r u i t i 12 with λ r l e a f λ r f r o o t 1 and the allocation to both organs is the minimum between their respective supply and demand 13 c a l e a f i min d e l e a f i s u p p l y l e a f i 14 c a f r o o t i min d e f r o o t s u p p l y f r o o t i demand by the fine roots is a constant parameter but demand by the leaves is computed as 15 d e l e a f i d e l m s t o c k i n g i 10 000 where d e l m is the maximum leaf carbon demand g c p l a n t 1 d 1 per coffee plant and s t o c k i n g i is the coffee planting density on day i p l a n t h a 1 finally if both leaf and fine root carbon demands are satisfied the remaining carbon is stored in the reserves for future use 16 c a r e i o f f e r i c a s h o o t i c a s c r i c a f r u i t i c a l e a f i c a f r o o t i 3 4 6 growth respiration growth respiration is the energy cost in carbohydrates to make new biomass from c a on day i it is computed using a construction cost coefficient ε j g c p r o d u c e d g c a l l o c a t e d 1 applied to the carbon allocated to the organ j 17 r g j i c a j i ε j 3 4 7 net primary productivity maintenance respiration is already accounted for before any carbon allocation see 3 4 3 therefore the net primary productivity of each organ j for day i n p p j i g c m 2 d 1 is computed using the difference between the carbon allocated to the organ and the growth respiration 18 n p p j i c a j i r g j i 19 j s h o o t s c r f r u i t l e a f f r o o t the net primary productivity of the plant is the sum on j of the n p p j i of the layer considered and the total net primary productivity of the stand is the sum of all n p p j i in all the layers 3 4 8 mortality mortality is the sum of the natural mortality turnover rate m n a t of pruning m p r u n diseases and of the lack of carbon m c when the carbohydrate budget g c b i is negative the natural mortality of an organ j m n a t j i g c m 2 d 1 is proportional to its carbon mass from the previous day c m j i 1 g c m 2 and is computed using a lifespan parameter l i f e s p a n j d 20 m n a t j i c m j i 1 l i f e s p a n j the annual pruning of coffee plants affects the shoot wood and the leaves and it is assumed that leaf loss is accompanied by fine root loss pruning also affects the branches of the shade trees for the leaves and shoots pruning is adjusted using a pruning intensity coefficient p i as follows it is considered that two separate p i s are needed for the leaves and the shoots because the distribution of wood and leaf biomass may be heterogeneous between resprouts of different ages in the coffee plant as reported in charbonnier et al 2017 21 m p r u n j i p i j c m j i 1 mortality due to pruning of the fine roots is related to the mortality of pruned leaves using a constant parameter m f r o o t 22 m p r u n f r o o t i m p r u n l e a f i m f r o o t leaf disease mortality is only implemented for coffee using a module to compute the american leaf spot als according to avelino et al 2007 mortality due to lack of carbohydrates to meet the maintenance respiration requirements m c i is computed as 23 m c j i g c b i c m j i 1 c m t o t i 1 i f g c b i 0 0 i f g c b i 0 where c m j i 1 is the carbon mass of the organ j from the day before i see eq 24 and c m t o t i 1 is the total carbon mass of the leaves shoots fine roots stump and coarse roots of the previous day finally the total mortality of each organ m j i is computed as the sum of all mortalities natural pruning leaf disease for coffee only and lack of carbohydrates 3 4 9 carbon and dry mass of organs the carbon mass of an organ is incremented daily by adding its n p p j i and removing its mortality 24 c m j i c m j i 1 n p p j i m j i the organ dry mass is computed using the carbon mass and the carbon content of each organ c c j g c g d m 1 3 4 10 branch nodes the plagiotropic branches of a coffee plant present several nodes of decreasing age from the orthotropic branch to the tip several unproductive old nodes close to the orthotropic branch productive nodes from the previous year that potentially bear flower buds and then fruits newly formed nodes that bear leaves the coffee reproductive phenology is based on a 2 year cycle camargo and camargo 2001 for a given year n the new flush of nodes bearing new leaves will bear flower buds by the end of year n while the leaves from year n 1 born by the fruiting nodes n 1 are shed given that each new node bears two leaves one expects some proportionality between node number and leaf area for the year n gutierrez et al 1998 such proportionality is used here to upscale the equations of rodríguez et al 2011 from the branch to the whole plant the model computes the total number of newly formed nodes g n i on the coffee plant for year n i e the number of green wood nodes that will potentially bear flower buds by the end of the first year of the reproductive cycle 25 g n i l a i i r n l c n where r n l is the number of nodes per l a i unit at 20 c assuming that this parameter is dependent on the growth temperature c n is an empirical temperature dependant correction coefficient that is a function of the mean temperature during the vegetative growth period t g p c this relationship is derived from drinnan and menzel 1995 26 c n 0 4194773 0 2631364 t g p 0 0226364 t g p 2 0 0005455 t g p 3 note that this coefficient is from one of the only datasets in the literature that links coffee reproductive and vegetative growth to the ambient temperature 3 4 11 fruit development the reproduction module is mostly inspired by rodríguez et al 2011 but upscaled to the whole plant e g in eq 25 two main development processes are computed in the model the flower bud cohorts of year n and the fruit cohorts of the following year the bud itself has two stages of development while the fruit has five buds can be initiated only during the bud initialization period bip from day d b i p 1 to d b i p 2 the buds appear on branch nodes in daily cohorts every day within that time window the d b i p 1 date is computed from the cumulative sum of degree days s d d t f f b after the date of end of vegetative development d v g 2 the degree days are computed as follows 27 d d i t c a n i t m i n i f t c a n i t m i n 0 i f t c a n i t m i n where d d i are the degree days c of day i t c a n i the mean daily coffee canopy temperature t m i n the minimum i e base temperature and t m a x the maximum temperature for physiological activity one originality here is that the degree days are computed using the canopy temperature of each layer rather than measured air temperature because the microclimate close to the plant may differ depending on the local conditions e g the cooling effect of shade trees d b i p 1 is computed as 28 s d d t f f b i d v g 2 d d d i i f s d d t f f b f t f f b d b i p 1 d i f s d d t f f b f t f f b where f t f f b is the threshold value in degree days that triggers the start of the bud development window it is considered that bud cohorts stop initializing when the first bud cohort initiated on the plant enters the fruit stage in other words buds can be initiated every day as cohorts until the first fruit appears on the coffee plant this day is denoted d b i p 2 the number of buds initiated daily on a given cohort b u d s 1 from d b i p 1 to d b i p 2 depends on several factors the incoming radiation r a d m j m 2 d 1 the canopy degree day computed following eq 27 and the number of green nodes to support the buds g n i this computation follows eq 12 from rodríguez et al 2011 here adapted to the whole plant 29 b u d s 1 i a b u d b b u d r a d i g n i d d i where a b u d and b b u d are parameters each bud initiated on day i is considered to belong to the bud cohort of day i there are as many cohorts as days between d b i p 1 and d b i p 2 once initiated bud cohorts develop for f b u d s 1 degree days until they become dormant on day d b u d s 1 30 s d d b s 1 i i n i t d d d i i f s d d b s 1 f b u d s 1 d b u d s 1 d i f s d d b s 1 f b u d s 1 where i n i t is the day of initialization of the cohort to potentially break dormancy the bud cohort has to experience a minimum amount of cumulated rainfall f r a i n after d b u d s 1 31 s r a i n i d b u d s 1 d r a i n i i f s r a i n f r a i n d r a i n d i f s r a i n f r a i n however the bud cohort can cumulate a maximum of f b u d s 2 degree days during its lifetime if it does not break its dormancy before the day when this threshold is reached d b u d s 2 it is considered desiccated d b u d s 2 is computed as 32 s d d b s 2 i d b u d s 1 d d d i i f s d d b s 2 f b u d s 2 d b u d s 2 d i f s d d b s 2 f b u d s 2 the time window when buds from a cohort can potentially break their dormancy w f r u i t s 1 is then found by solving eq 31 and eq 32 33 w f r u i t s 1 d r a i n d b u d s 2 i f s r a i n f r a i n a n d d r a i n d b u d s 2 the time window is shared by all the buds in the same cohort but all the buds will not necessarily break their dormancy on the same day the number of buds in a given cohort that break their dormancy on a given day within w f r u i t s 1 depends on the combination of two factors the mean diurnal air temperature within the coffee canopy during bud growth c b c and the leaf water potential ψ l e a f m p a of the coffee plant these conditions reflect the need for a drier period followed by an intense rainfall event for optimal dormancy break rodríguez et al 2011 the number of buds breaking dormancy b u d b r e a k on a given day is computed as 34 b u d b r e a k i b u d s 2 i p b r e a k i c b where b u d s 2 i is the number of stage 2 buds in the cohort that have not yet broken dormancy c b is a temperature dependant correction coefficient and p b r e a k i is the rate at which they break dormancy which is related to the leaf water potential ψ l e a f m p a using two parameters a p and b p as follows 35 p b r e a k i 1 1 e a p b p ψ l e a f i c b is implemented to include the effect of the average canopy temperature on the number of buds initiated during growth of the coffee plant it is computed from a monotone hermite spline fritsch and carlson 1980 fitted on the rather unique drinnan and menzel 1995 dataset relating temperature and blossoming more details are available on the help page of the eponym function of the r package of dynacof https vezy github io dynacof reference cb html when buds do break dormancy they enter the fruit stage by forming a flower stage 1 fruit then the fruits develop and finally become mature stage 4 then overripe stage 5 when they fall onto the ground each bud breaking dormancy on day i forms a new cohort of fruits meaning that fruits forming a fruit cohort can originate from several different bud cohorts the stage 1 fruits of the cohort then enter the carbon allocation scheme to undergo the successive stages of maturation under optimal conditions the carbon mass of the fruit increases following a logistic growth over the growing period until it becomes overripe this logistic growth during physiological development can be modeled as 36 l 1 e f d d i n f s d d f r u i t s 1 where f d d i n f is the inflexion value of the logistic growth in degree days s is the steepness of the logistic growth and s d d f r u i t is the number of degree days accumulated by the fruit since its flowering date the carbon demand of a cohort c on day i is computed as the sum of the carbon mass incremented by all fruits in the cohort under optimal conditions it is computed using the derivative of eq 36 as follows 37 d e c f r u i t s 1 d e o p t l i f s d d f r u i t f o v e r 0 i f s d d f r u i t f o v e r where f r u i t s 1 is the number of fruits in the cohort d e o p t is the total carbon demand of a coffee fruit thoughout its development under optimal conditions including growth respiration and f o v e r degree days the physiological age for fruits to become overripe and no longer require carbohydrates because they fall to the ground thereby the total plant scale fruit carbon demand is computed as the sum of the demand of all the cohorts growing on the coffee plant 38 d e f r u i t i c 1 n d e c i the fruit demand for carbon can be considered as a genetic growth potential with optimal fruit growth when there are no limitations to the supply it is a sink strength that depends on the number of fruits and the degree days and is independent of the carbon supply consequently the allocation of carbon to fruits is constrained either by fruit demand or by the carbon supply as in eq 9 the fruit mass is then computed as in eq 24 the fruits that become overripe i e are not harvested are removed from the coffee plant and considered as a mortality coffee bean quality is also computed using the fruit sucrose content of each fruit cohort c based on the number of days after flowering following the model of pezzopane et al 2012 39 s m i c 1 n c m c i c c f r u i t s y 0 s a 100 1 i s x 0 s b 40 s m o p t i c 1 n c m c i c c f r u i t s y 0 s a 100 41 m a t i s m i s m o p t i where s m i is the sucrose mass of all the cohorts c on day i and s m o p t i is the optimal sucrose mass of the coffee bean i e the sucrose mass of the bean if it was fully mature c m c i is the carbon mass of the cth cohort on day i c c f r u i t is the carbon content of the fruit s y 0 is the sucrose content at the beginning of the fruit development s x 0 is the day at which the maturation is at the inflexion point and s a and s b are two maturation parameters harvest maturity is then simply the global fruit maturity on the day of harvest harvesting takes place once each growing season it is triggered on the date when total cumulated fruit mortality has been higher than fruit growth for more than ten days this simple method ensures that harvesting occurs when the fruit stock is at its maximum to optimize yield 3 4 12 shade tree allometry in addition to the common allocation scheme each shade tree species has its own set of allometric relationships any kind of allometry can be implemented and can then be used as input for the metamodels or as an informative model output in this study allometric relationships were used to compute the tree crown radius and height from the tree branch dry mass and the tree density the crown radius and height were then used to compute the lad leaf area density m l e a v e s 2 m c r o w n 3 which is an input for the metamodels for the light extinction coefficients shade tree height is mandatory to compute the aerodynamic conductance and was computed in this study using an allometric relationship with the tree stem dry mass the tree diameter at breast height was also computed from stem dry mass 3 4 13 temperature the temperature of the foliage and the air inside the canopy is computed using the formalism proposed by van de griend and van boxel 1989 using the diffusivities for momentum transport and potential energy flow inside and in between the inertial sublayer the roughness sublayer the air space of each canopy layers the foliage and the soil the temperature of the air inside the canopy is computed as 42 t a i r c a n i l t a i r i h i l ρ i c p g b u l k i with i the day l the canopy layer t a i r the air temperature measured above the canopy h the sensible heat flux m j m 2 d 1 ρ the air density k g m 3 c p the specific heat of air for constant pressure m j k 1 k g 1 and g b u l k the aerodynamic conductance for heat above the canopy m s 1 if the coffee plants are grown in monoculture eq 42 is applied directly to the coffee layer if they are grown under shade trees eq 42 is used for the shade tree and the air temperature inside the coffee canopy is computed as follows 43 t a i r c a n i c o f t a i r c a n i t r e e h i c o f ρ i c p g i n t e r l a y i with t a i r c a n i t r e e the temperature of the air inside the shade tree canopy computed using eq 42 and g i n t e r l a y the aerodynamic conductance at the interface between both canopy layers m s 1 the leaf temperature t c a n of each layer is then computed as 44 t c a n i l t a i r c a n i l h i l ρ i c p g b h i with t a i r c a n the air temperature inside the given canopy layer and g b h the leaf boundary layer conductance for heat m s 1 the temperature at the soil surface is similarly computed as 45 t s o i l i t a i r c a n i c o f h i s o i l ρ i c p g s o i l c a n with g s o i l c a n the canopy to soil aerodynamic conductance m s 1 3 4 14 soil water and energy the soil water balance module is inspired by the biljou model granier et al 1999 which has already been parameterized for this coffee agroforestry system gómez delgado et al 2011 and is given in appendix a it has three layers 0 1 25 m 1 25 1 75 m and 1 75 3 75 m respectively to cover the whole root profile of the coffee plants defrenet 2016 the net radiation of the soil is computed using a maespa metamodel and the partitioning between latent and sensible fluxes was parameterized using the average partitioning from the outputs of the maespa simulations vezy et al 2018 the shade tree and coffee transpiration t r and sensible heat h are simulated using maespa metamodels the net radiation of the shade tree and the coffee layers are computed as the sum of the latent l e and the sensible h heat fluxes of each layer the stand net radiation r n is then computed by summing the net radiation of the shade tree the coffee and the soil 3 4 15 inputs and outputs all parameters needed for a dynacof simulation are stored in specific input files for the shade tree the coffee the soil the site and the meteorology a set of example data is included into the package so any user can start a simulation without any data this set of example file is also archived as a git repository on github com https github com vezy dynacof inputs for convenience the main input values used for this study are listed on the paragraph 3 5 the input variables for the meteorology file should provide at least the maximum and minimum air temperature of the day c the rad or par mj m 2 d 1 and the relative humidity or vapor pressure deficit hpa the full list of mandatory and optional inputs the user can provide are available from the documentation https vezy github io dynacof reference meteorology html the model has 245 output variables mainly for energy balance water balance carbon assimilation and allocation respiration plant growth yield and mortality for each organ type the full list is available from the documentation https vezy github io dynacof reference dynacof html 3 5 model parameterization using a multi objective approach the maespa model was parameterized according to vezy et al 2018 and was run on a 0 2 ha sub plot of 4176 coffee resprouts 3 resprouts per coffee plant in average and 14 shade trees at a half hourly time step throughout the year 2011 the shade trees are not planted along a regular planting pattern and they present large crowns increasing the heterogeneity of the light distribution in the plot the simulated sub plot was reduced to the minimum area possible to decrease computation time while encompassing coffee plants that are always in full sun or only partly under shade in the morning or during the afternoon metamodels were then built from daily plot scale aggregations of maespa outputs and integrated in dynacof fig 1 the metamodels were built using linear regressions with maespa input variables as predictors the maespa dataset created from the simulations of the year 2011 was taken as a representative sample of most of the conditions of the growing cycle with yearly climate variations a highly variable shade tree lai due to almost total leaf fall and highly variable coffee plant structure with resprouts ranging from 0 to 5 years old dynacof was run from january 1979 to the end of december 2016 at a daily time step the climate inputs to the model came from the coffee flux project between 2009 and 2016 and were computed between 1979 and 2008 using the method and data described in hidalgo et al 2017 the values and sources of the parameters used in dynacof are listed in table 1 for climate and coffee in table 2 for the shade tree species and in table 3 for the soil given the large number of parameters and the scarce data on coffee and shade trees some parameters were not measured in this study and were not found in the literature either for example the coffee carbon allocation coefficients λ because allocation coefficients have multiple repercussions on other variables notably on respiration through organ mass and on light interception through lai a multi objective parameterization was needed as computation time was a limiting factor a manual multi objective parameter tuning was preferred to algorithmic optimization the values of the parameters were found by starting from expert a priori and tuned manually to minimize the simulation error for npp and biomass for each compartment using the first year of measurements reported in charbonnier et al 2017 at a stand age of 33 year after planting always keeping the values within a plausible range in any case the outputs of the model were evaluated on the second year of measurements which was not used for model parameterization 4 results 4 1 metamodels the metamodels for shade tree k d i f and k d i r are presented in table 4 and were computed using the shade tree l a d l a d t r e e m 2 m 3 as the sole predictor the metamodel for l u e g c m j 1 was made using climate inputs because it was found to depend more on the environment than on the plant structure the other metamodels for plant transpiration t r m m sensible heat fluxes h m j m 2 and soil net radiation are also presented in table 4 their performance was also assessed using four different statistics which showed that despite being simple models the metamodel predictions were in agreement with the validation sub sample of the outputs of maespa in the simulated year 2011 table 4 indeed all metamodels gave high r2 on the data on which they were trained and high modelling efficiency rather low nrmse and low bias when applied to new data except for k d i r which failed to capture the high day to day variability but still followed the overall trend although h t r e e did present satisfactory statistics overall its nrmse was high mainly due to the low and sometimes negative sensible fluxes of the shade tree which yielded a low average value 4 2 gpp and energy fluxes the modeled gpp water and energy balance from dynacof were compared to the whole period of measurements using data gathered during the long term coffee flux eddy covariance monitoring overall the model outputs for these variables computed in 2011 through metamodels were close to those computed directly from maespa nrmse gpp 15 4 rn 7 2 etr 23 9 indeed plant transpiration plant sensible heat fluxes and soil net radiation were computed using maespa metamodels and the parameter for soil energy partitioning into sensible and latent heat i e soil evaporation was also determined using the average value from the maespa simulations the simulated net radiation was close to measured values with relatively high modeling efficiency 0 88 fig 2 rn and a low bias of 0 24 mj m 2 d 1 that was reflected in the cumulated energy in fig 3 a the high point density along the identity function indicated that modeled evapotranspiration fig 2 aet was relatively close to the measurement in average even though the simulations presented a relatively high error for a few measurements in dark blue in the figure yet the cumulated aet from dynacof fig 3b showed good consistency compared to cumulated measurements indicating that simulated values converge towards the measured value asymptotically the simulated sensible heat fluxes were in agreement with measured fluxes except for a quasi systematic bias of 1 48 mj m 2 d 1 fig 2 h the simulated gpp was close to the observed gpp in general fig 2 gpp with a low positive bias of 0 38 gc m 2 d 1 and an nrmse of 26 27 however modelling efficiency remained low 0 14 mainly due to the high dispersion of the residuals especially for high gpp values yet when considering cumulated values of gpp over the whole period the simulated gpp remained within the range of the measurement error fig 3c we also stress here that gpp remains a data model product derived from net ecosystem carbon flux measurements but subject to several partitioning and modeling assumptions overall dynacof predictions of gpp and energy fluxes were in agreement with their respective measurements over the whole period figs 2 and 3 confirming that the model performs reasonably well beyond the data on which the metamodels were calibrated i e data from 2011 dynacof outputs were also compared to standard equations allen et al 1998 landsberg et al 2001 that are widely used for crops or plantations with more homogeneous canopies i e constant lue and ks parameters for gpp rn computed using an average albedo le computed using the penman monteith equation as expected the error was higher when the net radiation the sensible and latent heat flux and the gpp were simulated using this approach compared to a simulation using metamodels from maespa table 5 and fig c 1 hence the choice of metamodels in dynacof was justified 4 2 1 growth and yield overall the multi objective calibration of dynacof yielded realistic results for most compartments that were documented by field measurements despite the high initial planting density the lai of the shade trees remained relatively low during the first period when the shade trees were pruned i e from planting to thinning when it dropped to a particularly low value of 0 02 m2 m 2 on average in the first year then resumed during the second period the lai of the shade trees subsequently increased to reach a maximum of ca 0 66 m2 m 2 in the last year of the simulation fig 4 the leaves of e poeppigiana shed naturally between january and february in aquiares and then recover rapidly until may taugourdeau et al 2014 the observed phenology was matched by the model with a simulated range and dynamic of lai values close to the observations made in the same plot averaged over the whole measurement period taugourdeau et al 2014 despite a low density of 7 3 trees ha 1 after thinning in 2000 the simulated shade trees intercepted up to 18 4 of the light at maximum lai in 2016 which was consistent with the values measured in the same plot charbonnier et al 2013 the simulated dry mass of tree stem and branches represented on average 47 2 and 4 4 of the total plot shade tree carbon mass while pruned and 27 2 and 27 4 of the total carbon mass respectively at the end of the cycle when left to grow freely the annual stem mass growth was close to linear under pruning management and became close to exponential when trees were not pruned after thinning with the stem npp increasing almost five times in the five last years of the simulation compared to the five years preceding thinning table 6 branch mass grew much more slowly due to higher mortality when pruned but grew as fast as the stem when the tree was left to grow freely as seen in fig 4 both stem and branch mass simulations were in the range of the observations reported by charbonnier et al 2017 in the same plot dynamic simulations of the carbon mass of the coffee organs are plotted for a full growing cycle from 1979 to 2016 in fig 5 coffee leaf carbon mass fig 5a increased rapidly until reaching its maximum value of 211 gc m 2 at three years old onset of fruiting coffee pruning started 5 yap and leaf carbon mass then fluctuated between 58 and 174 gc m 2 until the end of the simulation corresponding to lai values between 1 4 and 4 1 m2 m 2 the lai dynamics expressed a yearly minimum by april after the drier season and just after pruning and a second minimum in september during bean filling very similar to field observations reported by taugourdeau et al 2014 interestingly this realistic phenology was not prescribed into the model but appeared after the introduction of fruit cohorts inducing a strong but progressive competition between sinks here with the leaves throughout the fruiting season resprout wood fig 5b grew rapidly from 0 to 4 yap before the onset of the pruning cycle which occurred every year for all resprouts aged more than 5 resprout wood growth then decreased to reach a stable value of ca 397 gc m 2 at around 13 yap with only intra annual fluctuations due to pruning the perennial compartment represented by both the stump and the coarse roots grew progressively throughout the crop cycle because it was not subject to pruning and has a very long lifespan it reached a maximum value of 1769 gc m 2 or 37 3 tdm ha 1 in the end of 2016 fig 5c the coffee fruit compartment fig 5d started to yield from the third yap with a carbon mass of between 71 and 134 gc m 2 at harvest and an average modeled green bean production of 1382 205 kg ha 1 year 1 the simulated carbon mass of the fruits was close to values measured by charbonnier et al 2017 in 2012 and 2013 the fine roots fig 5e grew rapidly in the early growth stages i e until the third yap when the combined effects of pruning and natural mortality maintained their carbon mass at a relatively stable level from one year to the next with values of ca 134 gc m 2 the reserves compartment fluctuated seasonally always in opposition with the fruit carbon growth the simulated carbon reserve was close to the reserve measured by cambou 2012 in the same plot fig 5f because the model was parameterized using this data the modeled coffee carbon allocation by organs showed that plant reserves represented the compartment with by far the highest carbon flow capturing on average 40 4 of the yearly plant carbon supply during the five last years of the simulation with a daily maximum allocation of 63 1 and a minimum of 0 during fruit production table 6 this compartment also had a high turnover rate because reserves were re distributed back to the carbon supply pool making it an effective carbon buffer for the coffee plant from one season to another leaves and branches were the organs with the highest npp representing 21 5 and 11 9 of the total yearly npp respectively during the last five years of the simulation fine roots represented 9 4 stump and coarse roots 7 3 and fruits 9 5 of total npp the high productivity of the branches was related to their high carbon demand that was often met by the supply as well as for the stump and coarse roots the simulated npp was reasonably consistent with measurements for coffee table 5 for the shade trees no direct measurements of the whole rooting system was available therefore the comparison is not proposed here nevertheless when cumulated into biomass the results for both species were realistic figs 4 and 5 5 discussion 5 1 metamodels simulating complex processes in crop models using metamodels is a promising way to reduce computation intensity and avoid numerous equations that are often an important part of the development and application effort another advantage is that physiological data e g photosynthetic parameters are often sampled at a fine scale but can be used to parameterize a precise model in this case maespa at a sub hourly time step and then be upscaled to field scale and to daily scale in the present study metamodels helped dynacof consider fine scale processes explicitly such as spatial anisotropy instead of only using a parameter e g a percentage of canopy cover which significantly improved the model simulations replacing some metamodels by simple standard plot scale models leads to a dramatic increase in simulation errors as shown in the present study table 5 evidence that our metamodeling approach was appropriate the quality of the predictions of a metamodel relies first upon the ability of the original model to correctly simulate the processes involved in the system and secondly on the ability of the metamodel to reproduce the outputs of the original model the first point was investigated in a previous work vezy et al 2018 where maespa successfully simulated the energy balance and the evapotranspiration in the same experimental plot the second point was addressed by choosing metamodels according to the best possible balance between the model complexity the number of explanatory variables and out of sample prediction quality marie et al 2014 found that despite being slower to compute neural networks and multi linear regressions with two or three level of interactions yielded higher r2 than multi linear regressions with no interactions like those used in our study however four out of ten metamodels in our study gave r2 higher than 0 90 with low nrmse which is considered highly accurate three gave r2 higher than 0 80 which is considered accurate villa vialaneix et al 2012 and only one metamodel could be considered not sufficiently accurate with a r2 of 0 65 for example the lad was used alone to predict the light extinction coefficient of the shade tree layer which was then used to compute its light absorption from its lai the diffuse extinction coefficient was predicted with high accuracy with an ef and a r2 both equal to 0 95 the lad is a useful proxy for within crown foliage aggregation which was determined along with the lai as the most important characteristic to model light penetration by sampson and smith 1993 however other structural variables from the shade trees can be used as predictors to improve the metamodel accuracy this was not the case in our study because shade trees structural data were measured only once for the year 2011 so only one value was given as input to maespa the metamodels for l u e yielded r2 and nrmse similar to those found in christina et al 2016 r2 of 0 87 compared to 0 87 and 0 94 for shade tree and coffee respectively in our study and an rmse of 0 20 gc mj 1 compared to 0 24 and 0 10 shade tree and coffee resp in our study yet despite being generally effective at reproducing a complex model output applying metamodels to new conditions requires caution as they can produce unexpected results outside their training values especially if they use non linear equations or when there is covariance between predictors this was of particular concern in the present study because metamodels were fitted using a one year long simulation of maespa only 2011 to overcome these problems the 2011 data was checked to present a broad range of values for the target variables some conditions were still not met in the training sample such as the period before the year 2000 when shade trees were pruned twice a year while the metamodels were trained on the system with free growing trees yet the 2011 training period included some days with very low e poeppigiana lai i e 0 04 m2 m 2 because e poeppigiana loses all its leaves once a year which helped the metamodel simulate a plausible range of transmittance values under low lai pruning conditions likewise the cumulated evapotranspiration and energy balance were satisfactorily predicted compared to measurements even outside their training period although both computations depended to a great extent upon metamodels the metamodel for the coffee lue predicted an increase in values with a reduction in incoming radiation on the plant layer which is in agreement with previous results reported in charbonnier et al 2017 as a result dynacof simulations of rn gpp and aet were close to those produced by maespa reported in vezy et al 2018 and more importantly close to the eddy covariance measurements from the long term coffee flux monitoring from 2009 to the end of 2016 figs 2 and 3 the agreement between simulated and measured values was particularly strong when cumulative fluxes were compared which confirmed the high degree of consistency throughout the measurement period mostly due to a low bias from the metamodels hence metamodels proved to be powerful tools to overcome the long term trade off between speed accuracy genericity and fast development of growth and yield models able to simulate whole crop rotations 5 2 growth and yield model outputs from dynacof the evaluation of a crop model is often challenging due to the lack of data for parameterization and validation yet our model was subjected to multi objective evaluation using numerous observations from the same experimental field or from the literature nevertheless it should be noted that the model was mostly parameterized using values in the literature and better results would be expected using measurements or an optimization algorithm van oijen et al 2005 however the model satisfactorily predicted most processes at plot scale with little or no discrepancy the total autotrophic respiration represented 55 of the gpp which is close to the 57 reported by litton et al 2007 in a review of results from a wide range of forest ecosystems the simulated lai for coffee was in agreement with the measured lai and the mean simulated leaf dry mass from 2011 132 2 g c m 2 was in agreement with the measured values reported in charbonnier et al 2017 taugourdeau et al 2014 and siles et al 2010 with values of 140 5 143 7 and ranging from 102 to 176 g c m 2 respectively the seasonal behavior of leaf biomass revealed a drop at the end of the drier season corresponding to natural leaf shedding followed by pruning then a rapid increase at the beginning of the rainy season with a secondary minimum when fruit dry mass was high interestingly the simulations well mimicked the seasonal observations reported by taugourdeau et al 2014 and the simulation was close to measured values the seasonality of the lai was represented using two main drivers pruning the leaves once a year and the introduction of fruit cohorts the first is a forced process but the second is the result of successive computations that allows a smooth distribution of the demand for carbon required for grain filling during the period of reproductive development depending on the developmental stage of each cohort however to date the interannual variability in leaf area is barely perceivable in the simulations compared to the field conditions we assume that some processes driving this variability still need to be incorporated in the model for example a dynamic leaf life span using cohorts of leaves more leaf diseases or nitrogen effects indeed american leaf spot als was already included in the model following avelino et al 2007 but coffee leaf rust is the predominant disease affecting coffee plants in this region which is not yet included due to the absence of a published model linking disease severity and leaf loss total npp is the consequence of the carbon assimilation its allocation and the respiration of each specific organ comparing the measured and simulated total npp is an integrative evaluation of the model and the total coffee npp simulated by dynacof was in agreement with the npp measured by charbonnier et al 2017 with an overestimation of ca 3 6 for both years 2011 2012 and 2012 2013 however the most important but most challenging integrative process to simulate is fruit yield because its allocation follows a complex scheme spread out over two years with numerous development stages camargo and camargo 2001 which was modeled in dynacof using a formalism inspired by rodríguez et al 2011 dynacof predicted a green bean yield of 1382 205 kg ha 1 year 1 which was within the range of values observed by campanha et al 2004 van der vossen et al 2015 and of the average yield in central america söndahl et al 2005 furthermore the aquiares farm reported average yields of green beans of around 1333 336 kg ha 1 year 1 between 1995 and 2014 for fields close to the experimental plot confirming that dynacof yield predictions were consistent however this comparison is only indicative and should be interpreted with caution because the conversion from whole fruit dry mass into processed green beans was made using a simple parameter fts see table 1 that may change depending on several factors and because dynacof only simulates potential yield in the absence of fruit diseases or predators furthermore the dataset used to fit the effect of temperature on the buds could overestimate the negative effect because drinnan and menzel 1995 used coffee plants grown in relatively small pots 10 l which can restrict root growth and then negatively affect the physiology of the coffee plant ronchi et al 2006 damatta et al 2019 found similar results on unpublished data though confirming this negative effect is not only coming from the restricted root growth moreover the model was also compared to in situ measurements reported in gc m 2 by charbonnier et al 2017 and predictions were found to be close to measured values even following the same pattern of yearly variability which is particularly hard to achieve considering the number of formalisms in use and the potential cumulated error from one process to another the shift in the shade tree management from pollarded to free growing appeared to have little impact on fruit production or maturity at harvest this apparent stability came from the low density of the shade trees which still transmitted at least 81 6 of the light during the mature state according to dynacof compared to 86 reported in charbonnier et al 2013 charbonnier et al 2017 reported that the higher lue simulated by maespa for coffee plants under higher shade could compensate for most of the reduction in incident par maintaining npp at a nearly constant level as long as shade remains at low values in dynacof gpp was reduced only by 2 6 for a reduction of 7 9 in apar thanks to an increase of 5 8 in the lue between the two periods of shade tree management i e from low lai in pruned shade trees to higher lai in free growing shade trees another strength of the model is the prediction of canopy temperature under shade as a driver for plant biology and water and energy balances thanks to a full soil module inspired from the biljou model and to the maespa metamodels indeed predictions of the cumulated aet and net radiation were very close to the nearly continuous measurements made from 2009 to 2015 given the model gave satisfactorily results for a wide range of processes using our multiple objective strategy of parameterization and evaluation i e different sets of variables measured in the same site at the same time it can provide other information that cannot be discovered from the data only and help researchers identify emergent properties in the system for example coffee lai was strongly affected by pruning once a year and during the period in between by natural mortality and by the high fruit demand at the time of grain filling which was also observed by charbonnier et al 2017 another observation made using the model outputs is that except for stump and coarse roots which are the only perennial compartments biomass increased rapidly in the early stages of the plantation until it reached its maximum value for the whole rotation after which biomass growth started to decrease with pruning and found a new lower equilibrium between growth and mortality the model also reproduced to some extent the biennial fruit production that was reported by cannell 1985 finally although the measured variables were acquired mostly at the end of the rotation the model ran for the full rotation and the simulated variables reached the values measured by the end of the rotation thereby revealing dynamics that remained hidden during the unknown part of the rotation e g biomass reaching a specific equilibrium according to the pruning intensity overall it is clear that the model is able to compute several long term ecosystem services and paves the way for the analysis of possible trade offs between them depending on different management options and climate although the model now needs to be applied in different soil climate and shade management conditions to evaluate its true genericity the process based equations implemented in most modules already imply relative genericity for example the flowering events and timing are based on the model from rodríguez et al 2011 which reproduced satisfactorily these processes on contrasted sites however the user should keep in mind that metamodels are not generic and should be updated when applied to new conditions e g different soil or planting design the overall genericity of dynacof should allow its application to other climate conditions such as under climate changes other locations with different soils and climates and other shade management systems such as coffee grown in full sun or under cordia alliodora banana plants eucalyptus sp or any species tree density and management such as pruning or thinning in specific situations it would be advisable to develop specific modules for coupling the nutrient cycles with the carbon and water cycles already available in the current version of dynacof therefore the current version is assumed to simulate the potential outputs only for situations without any nutrient stresses drought is already present in the model through leaf water potential and affects the reproductive phenology but should be tested and refined for other vegetative limitations 5 3 model adaptation dynacof can be easily adapted by users to other sites other shade trees other coffee varieties or managements by parameterizing the model accordingly adapting the model to other sites require mainly updating the site parameters e g latitude longitude elevation the soil properties e g field capacity pore fraction and the meteorology if the coffee cultivar is changed the parameters linked to the coffee should be checked and updated if necessary e g specific leaf area allocation coefficients base temperature to change the management of the shade tree or the species the user has to update the shade tree parameter file the metamodels should be updated by the user whenever the plants structure or functioning is out of the range they were trained on for example the lue metamodel should be fitted using the atmospheric co2 concentration and air temperature to integrate their effect for simulations under climate changes they also can be replaced by any plot scale equations because they are written as input code the model can then be calibrated and evaluated using field data lai for both shade tree and coffee is a key variable to measure because it is the result and has an impact on many processes the biomass increment of each organ can be measured to evaluate the plant npp for the coffee key variables from the reproductive development are useful to evaluate the simulated yield e g time of initiation and number of buds and flowers and of course yield itself the soil water content can be measured to evaluate the water balance module and the leaf and soil temperature for the energy balance module dynacof can also be modified easily by developers because it was designed as modular as possible with each module called in sequence at daily time step developers can replace any module while still leveraging the others easily for example the coffee module could be replaced by a module simulating any other plant annual or perennial e g cocoa wheat sugar cane rice new modules can also be added such as for nitrogen cycle this module could compute the nitrogen uptake allocation and content for all organs from each plant species and the mineralization processes from decaying organic matter which already exist in the model and microbial biomass in the soil with eventual mineral fertilization input by management e g urea synthetic fertilizers irrigation or natural processes e g precipitations 6 conclusion dynacof dynamic agroforestry coffee crop model was developed to simulate the effects of the environment the soil the species of shade tree and the management practices on coffee growth and yield the shade management module can be set to any shade type and density under full sun or agroforestry systems applying pruning or thinning at any age if required the model can be used for full rotations at a daily time step for any surface area from plot to landscape or even to a region if properly distributed under current past or future climate conditions as long as the metamodels built from maespa model simulations are updated to the target conditions the model was parameterized and evaluated using a comprehensive and unique dataset for energy and water balance biomass and npp from an experimental site in the aquiares farm in costa rica a substantial advantage of dynacof being a tree average plot model is the possibility to parameterize it using plot averages or totals which are more frequently available from farms e g yield pruning intensity coffee quality etc because data remain scarce especially under agroforestry management two other important features of the model are the simulation of the canopy temperature instead of air temperature to control the plant growth according to the shade level and the use of cohorts of flowers and fruits to consider grouped flowering in sub tropical conditions and distributed flowering in equatorial climates the model is implemented as an r and julia packages for easy sharing and collaboration and can be easily modified by adding new modules to compute pest attacks nutrient cycling soil organic matter decomposition or soil respiration the methodology can be further generalized for any type of shade or climate by using different maespa simulation sets to train the metamodels dynacof was built using a set of generic modules e g functions for aerodynamic conductance that can be used in other models to simulate any type of agroforestry systems or intercropped systems in conclusion dynacof concentrates considerable ecophysiological knowledge on coffee and is an efficient tool to evaluate and optimize coffee crop yield ecosystem services and their trade offs in response to climate conditions and management scenarios it was also designed to predict the impacts of climate change on coffee yield and the potential of changes in management to mitigate such effects declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project was funded by agence nationale de la recherche macacc projet anr 13 agro 0005 viabilité et adaptation des ecosystèmes productifs territoires et ressources face aux changements globaux agrobiosphere 2013 programme cirad centre de coopération internationale en recherche agronomique pour le développement and inra institut national de la recherche agronomique the authors are grateful for the support of the aquiares farm https aquiares com and for the long term coffee agroforestry trial the soere f ore t which is supported annually by ecofor allenvi and the french national research infrastructure anaee f http www anaee france fr fr the cirad ird safse project france and the pcp platform of catie the coffee flux observatory was supported and managed by cirad researchers we are grateful to the staff in costa rica in particular alvaro barquero alejandra barquero jenny barquero alexis perez guillermo ramirez rafael acuna manuel jara alonso barquero for their technical and field support the project analyzes largely benefited from the montpellier bioinformatics biodiversity mbb computing cluster platform which is a joint initiative of laboratories grouped in the cemeb labex mediterranean center for environment and biodiversity as part of the program investissements d avenir anr 10 labx 0004 appendix e supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix e supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104609 appendix a soil water balance model the soil water balance model is largely derived from the biljou model granier et al 1999 it is a bucket model meaning that the soil is represented by the depth dimension only itself divided into layers of given thickness 1 water balance the water flow is managed sequentially as in any other bucket model 1 1 interception and leaf evaporation during a rainfall event water can be either intercepted by the plants canopy or directly reach the soil surface the water intercepted by the plants is either stored on the canopy surface or flow along the branches and trunks to finally reach the soil the water that is stored on the canopy surface is then gradually evaporated back to the atmosphere the first step for computing the water balance in the model is to compute a maximum potential rainfall interception using the total stand lai and an interception parameter as follows a 1 i n t e r c m a x i n t e r c s l o p e l a i p l o t any water intercepted by the canopy when the canopy water retention is already at full capacity i e at i n t e r c m a x mm is considered as throughfall water the daily evaporation of the water stored in the canopy bucket is computed using the penman monteith equation as found in allen et al 1998 with an infinite stomatal conductance and a set of aerodynamic conductances 1 2 surface runoff water reaching the soil surface enter the surface layer w s u r f a c e r e s mm any water entering this layer when it is already full is considered as excess surface runoff which is added to the superficial runoff itself computed using a parameter a 2 s u p e r f i c i a l r u n o f f k b w s u r f a c e r e s 1 3 infiltration water from the surface layer infiltrates the first soil layer up to the first layer infiltration capacity w1 which is computed as follows a 3 i n f i l c a p a f o i f w 1 w m 1 f o w 1 w m 1 f o f c w f 1 w m 1 i f w 1 w m 1 a n d w 1 w f f c i f w 1 w m 1 a n d w 1 w f with w 1 the first layer water content mm w m 1 the minimum water content of the first layer w f 1 the field capacity of the first layer and f o and f c the maximum and minimum infiltration capacity respectively mm day 1 then w 1 is updated by adding the water that infiltrated from the surface bucket to its previous water content if w 1 exceeds w f 1 after this operation the excess water drains into the second layer this last operation is repeated for the two following layers w 2 and w 3 with their own field capacity w f 2 and w f 3 1 4 soil surface evaporation the soil surface evaporation mm is computed using the soil net radiation coming from a metamodel of maespa and a partitioning parameter s o i l l e p a 4 e s o i l r n s o i l s o i l l e p λ with λ the latent heat of vaporization mj kgh2o 1 1 5 root water uptake the water that is absorbed by the plants roots r o o t w a t e r e x t r a c t l for a given layer l is computed from the total stand transpiration t t o t a l the total root fraction in the layer r o o t f r a c t i o n l and the extractable water from the layer e w l as follows a 5 e w l w l w m l a 6 r o o t w a t e r e x t r a c t l min t t o t a l r o o t f r a c t i o n l e w l each layer s water content is then updated by removing to it the root water uptake 1 6 water potential the soil water potential ψ s o i l m p a is computed using the equation from campbell 1974 as follows a 7 ψ s o i l ψ e w t o t 1000 θ s b with ψ e mpa the air entry water potential w t o t the total soil water content mm and θ s the saturated water content m3 m 3 and the leaf water potential ψ l e a f m p a is computed as follows a 8 ψ l e a f l ψ s o i l t l m h 20 k t o t where t l is the plant transpiration m m m h 20 the molar mass of water and k t o t the soil to leaf hydraulic conductance m o l m 2 s 1 m p a 1 2 energy balance the soil energy balance is computed using the soil net radiation from the metamodels of maespa and the partitioning coefficient as in eq a 4 for both the latent and sensible fluxes the soil heat storage is neglected because its variability is low at daily time scale and because it tends to return at equilibrium after several days appendix b shade tree allometric equations several allometric relationships are used in the shade tree growth module only the shade tree height is mandatory for the other parts of the model because it is used to compute the canopy boundary layer conductance any other allometric equation can be added to the model via the tree parameter file and can be used for user custom metamodels or as informative output 1 dbh the diameter at breast height dbh m of the erythrina poeppigiana shade tree is computed following the equation from rojas garcía et al 2015 a 9 d b h d m s t e m c c w o o d 10 s t o c k i n g 0 5 0 625 2 height tree height m is computed following the equation from van oijen et al 2010 a 10 h e i g h t 0 46 d m s t e m 1000 s t o c k i n g 0 5 if the shade tree is pruned it breaks the relationship between the tree stem dry mass and its height as shown in eq a 10 instead the tree height is computed using the trunk height as follows a 11 h t r u n k 3 1 e 0 2 a g e a 12 h t o t h c r o w n h t r u n k the underlying hypothesis behind eq a 11 is that e poeppigiana are pruned by stakeholders so the trunk height do not exceed 3 m high 3 crown dimensions the crown radius m is computed as follows a 13 r c r o w n p c r o w n π with p c r o w n the crown projection itself computed as a 14 p c r o w n 8 d m b r a n c h 1000 s t o c k i n g 0 45 with d m b r a n c h the shade tree branch dry mass in g m 2 the crown height h c r o w n is taken as equal to the crown radius as shown in table 2 from charbonnier et al 2013 4 lad the leaf area density lad is computed as follows a 15 l a d l a t r e e r c r o w n 2 h c r o w n 2 π 4 3 with l a t r e e the shade tree leaf area appendix c comparing dynacof with and without metamodels to assess the importance of using metamodels from maespa to integrate fine scale processes to dynacof we compared dynacof outputs for gpp rn le and h with and without metamodels the gpp without metamodels was simulated using constant values for the light use efficiency and constant values for the light interception coefficients of the beer lambert s equation for both the coffee and the shade tree layers the parameterization was made using the average values from the maespa simulations the net radiation without metamodels was simulated using the equation from allen et al 1998 at plot scale using the albedo computed by maespa the latent heat flux without metamodels was computed using the penman monteith equation at stand scale penmon function from dynacof https vezy github io dynacof reference penmon html the sensible heat flux without metamodels was computed as the difference between the net radiation and the latent heat flux both computed without metamodels fig c 1 net radiation rn latent le and sensible h heat flux and gross primary productivity gpp simulations using dynacof with metamodels from maespa red or stand scale equations blue compared to field measurements fig c 1 figure c 1 shows that gpp simulation was improved when using metamodels and that net radiation was also improved using metamodels but mainly because the stand scale reference equation had a systematic positive bias compared to measurements this bias could potentially be corrected knowing it a priori the sensible heat flux were also systematically more biased without metamodels the latent heat flux simulated without metamodel was close to the measurements for low values 5 mj m 2 d 1 but was increasingly overestimated with higher values appendix d further plots 1 light interception gpp and ra the contribution of the shade tree and the coffee plants to lai apar gpp and ra are shown in fig d 1 the highest coffee contribution to the plot scale gpp can be explained by its higher lai and hence higher light interception compared to the shade tree the shade tree thinning in 2000 has a high impact on these variables during the first three years fig d 1 comparison of dynamic outputs from dynacof between shade tree and coffee for leaf area index lai absorbed photosynthetically active radiation apar gross primary productivity gpp and autotrophic respiration ra starting from 1979 to 01 01 until 2016 12 31 fig d 1 2 water balance dynacof simulates a broad range of informative variables related to the water balance of the system fig d 2 fig d 2 dynacof outputs related to the water balance at daily time scale and plot scale for the period starting from 1979 to 01 01 until 2016 12 31 θ is the volumetric water content for the full soil profile and ψleaf the leaf water potential fig d 2 3 temperatures dynacof also simulates several temperatures and aerodynamic conductances in the system to better represent the microclimate experienced by the plants dynacof uses the air temperature measured above the canopy as input and computes the temperature of the air inside the shade tree canopy and inside the coffee canopy and uses them to compute the leaf temperature of the shade tree and the coffee and the soil temperature these computations are dependent from other computations such as the wind extinction the sensible heat flux of each layer plants and soil and a sequence of aerodynamic conductances the temperatures simulated by the model are shown in fig d 3 fig d 3 input tair and simulated temperatures the simulated temperatures are presented as the difference between the simulated temperature and the input tair for easier assessment fig d 3 4 litter the litter is simulated by the model as a mortality of biomass the coffee leaf and resprout wood litter at plot scale is mainly impacted by the pruning effect each year fig d 4 as expected the fruit litter follow a seasonal variation related to the fruit production the shade tree leaf litter is mainly impacted by the pruning effect before 2000 pruning management and then by the natural seasonal variation the simulated litterfall from the coffee i e mortality from leaves resprout wood fruits is close to the observations made by charbonnier et al 2017 with values of 482 gc m 2 year 1 compared to 450 gc m 2 year 1 from their measurements the simulated tree litterfall i e leaves branches wood is also close to the observations with a value of 96 gc m 2 year 1 and 110 gc m 2 year 1 respectively high values of tree litterfall are explained by tree pruning before 2000 fig d 4 simulated litter fluxes for the compartments with the highest contribution for the coffee leaf resprout wood fruits and the shade tree leaf fig d 4 
26097,decision making in the context of complex human natural systems requires a transition towards robust model based inferences which are effective despite uncertainties of human and climate driven change supporting robust decision making needs a sequence of interactive methodological choices for setting the problem context framing the decision problem evaluating possible solutions and making recommendations these methodological choices are influenced by a variety of human factors originating from cognitive behavioural and mental frameworks of stakeholders we review a broad array of methodological constructs to better emphasise the choices that are most appropriate given different levels of knowledge consideration of these methodological constructs clarifies how problems can be perceived and framed in rival decision support paths emerging from the cumulative effects of individual methodological choices and the challenging human factors that shape decision making under deep uncertainty we conclude that the careful consideration of rival decision support paths can enhance the confidence in decision recommendations and illuminate sensitivities to the methodological choices keywords decision analysis deep uncertainty scenario discovery exploratory modelling robustness cognitive biases and heuristics 1 introduction the debate on global environmental and societal change has shifted from problems to solutions in recent years schellnhuber et al 2011 a globally prominent example is the un 2030 agenda for sustainable development with its urgent call to action for achieving 17 sustainable development goals sdgs un 2015 aiming at multiple environmental and socio economic aspects such as decarbonising energy systems conserving biodiversity reducing poverty and improving gender equality to achieve such diverse goals decision makers need to make robust decisions insensitive to well characterised as well as deep uncertainties ben haim 2006 lempert et al 2003 that are characteristics of complex and coupled human natural systems deep uncertainty emerges from the limited and contested knowledge among stakeholders about interacting human natural systems and their boundaries the state e g the likelihood of key drivers of these systems e g population growth water availability energy demand and the relative importance of the outcomes e g trade offs between goals for economic development and climate actions khatami et al 2019 kwakkel et al 2016a singh et al 2015 the shift towards robust decisions requires careful accounting of candidate costs or regrets associated with highly precautionary or risk averse decision alternatives dessai and hulme 2007 lincke and hinkel 2018 small and xian 2018 weaver et al 2013 we aim to develop a unified framework that can inform robust inferences in complex human natural systems by classifying sequencing and evaluating alternative framings of decision support we specifically focus on model based decision support under different degrees of uncertainty from well characterised to deep uncertainty where a model i e conceptual or quantitative of the system of interest is used for the evaluation of assumptions and hypotheses walker 2000 decision support in this context requires a sequence of interactive methodological choices about how to set the problem context e g deep or well defined uncertainty how to frame the problem e g limited scenario narratives or many quantitative scenarios how to evaluate candidate decisions e g model simulation or qualitative evaluation and how to drive robust inferences and provide decision recommendations tsoukiàs 2008 alternative combinations of the variety of choices made by analysts can lead to fundamentally different outcomes for the systems of focus see kwakkel 2017 decision support is also a subjective and social process involving various stakeholders who directly or indirectly influence the methodological choices underlying any analysis voinov et al 2014 within this subjective process methodological choices can be influenced deliberately e g through self interest or unintentionally e g through cognitive bias or ignorance argyris and schön 1978 lahtinen and hämäläinen 2016 mayer et al 2017 for example in new projects analysts often tend to favour methods and approaches with which they are familiar and have used previously potentially in unrelated contexts lahtinen et al 2017 these traits can influence the evaluation of robust decisions by making one particular methodological choice e g deterministic simulation to be selected over other possibly more effective ones e g a robustness analysis which considers deep uncertainty the risks of biased methodological choices are in sub optimal or even misleading decision implications for the project kasprzyk et al 2009 lempert and collins 2007 oddo et al 2017 singh et al 2015 several studies have reviewed and classified common methodological choices in robustness focused decision support frameworks dittrich et al 2016 herman et al 2015 kwakkel and haasnoot 2019 maier et al 2016 however they are limited in addressing how researchers and practitioners should a make the choices that are most appropriate given different levels of knowledge b become critically aware of the human factors that influence their methodological choices c evaluate the combined consequences of these choices and factors on planning outcomes and d take measures to tackle their negative consequences we clarify sequences and interdependencies of methodological choices under the influence of critical human factors e g biases beliefs heuristics values from psychology and cognitive sciences we articulate how alternative candidate options for undertaking decision support processes termed decision support paths emerge from the cumulative effects of choices made at different stages of the process termed decision forks this enables us to analyse how alternative decision support paths may lead to a diversity of final inferences that are sensitive to the path taken i e path dependency we also go beyond existing taxonomies in the literature which mostly focus on choices of analytical components e g how to generate decisions and scenarios and review new methodological steps for framing and evaluating the decision support processes themselves the articulation of methodological choices at decision forks the resulting decision support paths their path dependency and their influential human factors contributes to policy discussions around assessing global environmental and societal sustainability and change in three ways first it can pave the way for the exploration and the deliberative improvement of robust decision making by illuminating the presence of alternative sometimes less well understood rival paths to be considered quinn et al 2017 saltelli and giampietro 2017 wirtz and nowak 2017 recognising and identifying rival paths leads to awareness about the variety of potential alternative ways of informing decision making depending on the context without being locked into one dominant but not necessarily relevant implementation of decision support second the transparent articulation of available methodological choices and potential paths helps to demystify the decision support process and to facilitate a better understanding of the overall effects of methodological choices on final inferences carefully documenting decision support paths is critical for ensuring that decision recommendations are supported by credible evidence and therefore are defensible cash et al 2003 cooke 1991 third discussion of the impact of influential human factors helps researchers and practitioners to be critically aware of the effects of mental and behavioural aspects on the decision support process and their range of consequences for conclusions it also signifies the importance of recognition evaluation and management of influential human factors in research projects to improve the effectiveness of outcomes section 2 presents the four conceptual bases of the current work section 3 explains methodological choices and human factors and their potential influence throughout the decision support process section 4 discusses frontier challenges opportunities and recommendations for future decision support innovations 2 conceptual basis this section briefly introduces the four conceptual pillars for this article we use concepts drawn from a constructive decision aiding to set out a generic description of the decision support process and different forks at which choices need to be made b a taxonomy of robustness frameworks to articulate methodological choices at decision forks c the path perspective to demonstrate the way in which particular methodological choices can lead to specific implementations of decision support d human factors to discuss the ways that the decision making process and its final results can be biased 2 1 constructive decision aiding we adapted the concepts of constructive decision aiding from tsoukiàs 2008 creating the four stages represented in fig 1 1 representation of the problem context 2 problem framing 3 evaluation and implementation 4 decision recommendations and monitoring these stages can be implemented with different approaches simulation optimisation approaches have been used widely to inform the decision support process using model based analysis see amaran et al 2016 for a review in the context of robustness analysis these approaches are connected to monte carlo simulation reliability engineering and evolutionary optimisation beyer and sendhoff 2007 maier et al 2014 nicklow et al 2010 reed et al 2013 within model based decision support exploratory modelling emerged in the early 1990s to enhance decision making under deep uncertainty bankes 1993 hodges 1991 hodges and dewar 1992 lempert et al 2003 exploratory modelling enables robust decision making by exploring the implications of varying decision assumptions within a decision space and scenarios within an uncertainty space in terms of performance or robustness measures within an outcome space using a series of computational experiments bankes 1993 multiple so called robustness frameworks of which some are capable of adopting exploratory modelling also emerged in support of robust decision making among them are multi objective robust decision making mo rdm kasprzyk et al 2013 lempert et al 2003 info gap decision theory ben haim 2006 dynamic adaptive policy pathways haasnoot et al 2013 kwakkel et al 2015 and decision scaling brown et al 2012 amongst others see walker et al 2013 and kwakkel and haasnoot 2019 for a review of these frameworks robustness frameworks adopt two contrasting approaches to decision support herman et al 2015 tsoukiàs 2008 a priori analyses that generate and order or rank candidate decisions using modelled abstractions of decision maker preferences e g aggregation schemes which weight incommensurate measures or expectation operators such as maximising expected utility this is the dominant means by which traditional risk and reliability studies have typically been formulated and solved using single objective optimisation techniques e g see chankong and y 1983 haimes 1998 a posteriori analyses that relate exploratory modelling where candidate decisions key uncertainties their conflicting objectives and assumed preferences are discovered from a diverse search through data gathering computational exploration and multi objective optimisation this approach is mainly driven by the mantra generate then choose within the broader information context coello et al 2007 cohon and marks 1975 herman et al 2015 johnson and geldner 2019 simpson et al 2016 robustness frameworks with a posteriori analysis can cope with deep uncertainty by avoiding prior assumptions to allow for unforeseen conditions weaver et al 2013 and by reducing possible system behaviours based on stakeholder preferences but only after broad assessment of trade offs and assumptions herman et al 2015 2 2 the taxonomy of robustness frameworks robustness frameworks can be supported by simulation models to investigate the implications of decisions over scenarios in terms of performance robustness measures see fig 2 based on methodological choices made in robustness frameworks the mechanics of decision support can be implemented in several ways herman et al 2015 and kwakkel and haasnoot 2019 proposed the following taxonomy of robustness frameworks generation of decisions i e alternatives candidate decisions can be pre specified standardised alternatives e g put forward by experts e g liu et al 2008 or can be generated through computational search such as a systematic sampling e g latin hypercube sampling or optimisation kasprzyk et al 2013 over the decision space generation of scenarios i e states of the world scenarios can be specified as pre specified standardised scenarios e g defined by experts e g moallemi et al 2017b or can be generated through exploratory modelling over the uncertainty space lamontagne et al 2018 measurement of performance i e robustness different measures can be used to represent the performance of candidate decisions under uncertainty see the recent reviews e g giuliani and castelletti 2016 mcphail et al 2018 among them are descriptive statistical measures i e representing ranges of outcomes statistically satisficing measures i e fulfilling minimum performance requirements and regret measures i e minimising the gap between the performance of a selected decision with that of the best decision in a prevailing scenario vulnerability analysis i e robustness controls decision support can use different ways to isolate scenarios responsible for a specific behaviour of interest such as an undesirable performance among them are subspace partitioning methods such as scenario discovery bryant and lempert 2010 and sensitivity analysis methods such as factor mapping which identifies the sensitivity of uncertain factors leading to a specific behaviour guivarch et al 2016 and factor prioritisation which ranks uncertain factors in order of their sensitivity saltelli et al 2008 2 3 the path perspective the emerging idea of the path perspective originated in behavioural operations research hämäläinen and alaja 2008 hämäläinen et al 2013 landry et al 1983 morris 1967 walker 2009 and behavioural decision theory payne et al 1992 simon 1956 1957 with roots in the psychological literature payne et al 1999 lahtinen and hämäläinen 2016 defined a path as the sequence of choices made through the life cycle of the deliberative processes that support a given application of decision support paths reflect different ways that decision support can be implemented e g optimisation or design of experiments and the variety of conclusions that can be obtained e g what candidate decisions fulfil objectives or how sensitive they are to scenarios it is noteworthy that this particular conceptualisation of path is different from other uses of the term for example the sequence of policies and actions in dynamic adaptive policy pathways haasnoot et al 2013 or the concept of sustainability pathways for achieving the sdgs gao and bryan 2017 despite the difference there is a commonality between the meaning of the path concept in these contexts and our use which refers to open ended sequences of options that could lead to multiple alternative outcomes a path then in this context is similar to the chain of reasoning that shapes ideas about a problem in a decision pathway gregory et al 1997 pathway choices are made at decision forks by stakeholders see section 3 1 1 for the definition of stakeholders where they decide how the decision support process is designed and implemented lahtinen et al 2017 for example how to formulate the problem and what methods to choose to evaluate the candidate actions choices at different decision forks are interdependent in a sense that those made at one fork may create lock in effects in particular choices or eliminate certain possibilities at subsequent forks this makes final decision recommendations highly sensitive to the methodological path taken or path dependent lahtinen and hämäläinen 2016 other studies have acknowledged the challenges posed by path dependency arguing how sequences of individual methodological choices influence the final results for example herman et al 2015 and mcphail et al 2018 discussed ways that the choice of performance robustness measures influenced final conclusions moallemi et al 2018b discussed how sequences of methods create different decision insights and islam and pruyt 2016 and moallemi et al 2018d discussed how computational experiment design influences the final results 2 4 influences on human decision making decision support processes are affected by multiple human factors e g intuition heuristics biases or behavioural contexts there is an extensive theoretical body of literature that has a long history of addressing these influential factors glynn 2015 hämäläinen 2015 montibeller and von winterfeldt 2015 payne et al 1992 simon 1956 tversky and kahneman 1974 we classify these human factors conceptually based on their origin e g mental framework behaviour as follows beliefs values heuristics and cognitive frameworks can be highly influential on people s judgement in the decision making process hämäläinen 2015 payne et al 1992 simon 1957 tversky and kahneman 1974 stakeholders can believe in and value some observations and alternatives more than others these personal preferences beliefs and values can lead to different types of biases in practice relevant examples include cognitive and motivational biases montibeller and von winterfeldt 2015 confirmation bias nickerson 1998 and groupthink bias janis 1972 all of which can negatively or positively affect some choices to the detriment of others lahtinen et al 2017 motivational effects such as expressed or hidden preferences can be a source of bias in decision making they often happen when stakeholder strategic or self interest motives lead to unintentional subconscious or deliberate conscious choices of some alternatives and can lead to ignorance misjudgement of other potential alternatives lahtinen et al 2017 montibeller and von winterfeldt 2015 mental model and focused thinking approach influence human decisions too bessette et al 2017 lahtinen et al 2017 for example focused thinking is where stakeholders deliberately focus on particular issues or perspectives e g focusing on bio physical aspects more than socio economic causes in integrated water resources modelling see zare et al 2017b likewise narrow thinking which refers to disregarding actively or unintentionally alternatives is another source of influence also relevant to this category are cases when the problem solver s mental model is biased towards previous methods used in the past which exemplifies when you have a hammer everything looks like a nail see hämäläinen 2015 system of problem solving is another source of influence in decision making which occurs when the strong impact of the starting point and the structure of the decision support process shape and influence the future choices of stakeholders lahtinen and hämäläinen 2016 the system of problem solving is composed of those involved as stakeholders the assumptions that they have and stakeholder choices with regard to project schedule management of interactions and technical details i e choice of models experimental runs etc 3 rival decision support paths rival decision support paths can emerge from the succession of methodological choices at different decision forks their path dependency and the effects of influential human factors e g biases values fig 3 shows an overview of these decision forks with their potential methodological choices the interdependencies between decision forks create cumulative impacts on the methodological design and on the final planning outcomes path dependency for example limited stakeholder knowledge of future scenarios fork in section 3 1 2 can justify searching over a broad uncertainty space and exploring many scenarios for the consideration of future unknown events fork in section 3 2 2 while the impacts of all decision forks on the quality of final outcomes are significant the forks themselves can vary significantly in how they influence the design of the methodology for example the level of knowledge available for defining decisions and scenarios section 3 1 2 can strongly influence analytical goals section 3 1 3 how decisions and scenarios are formulated sections 3 2 1 and 3 2 2 and the required computational experiments section 3 3 3 alternatively some other forks may represent modest variations in methodology while effecting substantial impacts on final outcomes for example the choices of the measure of robustness section 3 2 3 do not fundamentally change the way that decision making process is designed but it may significantly impact the final planning outcomes see mcphail et al 2018 the significance i e strength of these interdependencies depend on the context of the study fig 4 presents an illustrative but not necessarily enumerative illustration of influential forward interdependencies that could be shared among different decision support contexts this illustration can guide more detailed assessments of these interdependencies and their cumulative impacts in the context of a specific decision support application while decision forks are introduced in a sequence they remain highly interactive in implementation and choices made at decision forks can be modified with backward interactions for example the choice of performance measures can inform the initial choice of candidate decisions herman et al 2015 and problem formulation depends on the intended implementation of decision support and vice versa woodruff et al 2013 this is related to constructive decision aiding tsoukiàs 2008 and negotiated design selection woodruff et al 2013 where decision support stages are integrated in a process based on learning how to best formulate and implement the decision problem 3 1 representation of the problem context 3 1 1 analysing stakeholders stakeholder analysis reed et al 2009 aims to identify stakeholders to categorise them and to examine relationships between them it is important to map who is involved in decision support and who knows what i e identification which individual stakeholders are more influential i e categorisation and how stakeholders influence each other s decisions i e relationships different groups of stakeholders including researchers decision makers clients advocacy groups power groups and communities can be involved in the decision support process they can be identified with a range of methods such as focus groups snowball sampling amongst others reed et al 2009 stakeholders can be categorised analytically based on their level of influence lindenberg and crosby 1981 and their and legitimacy mitchell et al 1997 or in a reconstructive manner for example using q methodology barry and proops 1999 in an empirical analysis of stakeholder perceptions reed et al 2009 different types of relationships among stakeholders can be also identified stakeholder interdependencies e g actor linkage metrics see biggs and matsaert 1999 the patterns of communication and influence between stakeholders e g social network analysis see prell et al 2009 and the content of information circulated between stakeholders e g knowledge mapping see wexler 2001 narrow thinking ermer et al 2006 and deliberate ignorance hertwig and engel 2016 are two examples of human factors which can lead to underestimating the role of some groups of stakeholders or individuals within each group in decision support they can result in a lack of awareness of alternative views in setting the stage for decision support leading to an unbalanced representation of the decision problem for example decision support dominated by theoretical researchers can overlook practical constraints e g institutional barriers or legitimacy in making recommendations stakeholder identification is also dependent upon those responsible and how they view the rationale behind stakeholder participation for example the values and heuristics of the analyst responsible for stakeholder identification may determine who is involved and who is not e g the exclusion of environmental advocacy groups by power groups and their level of involvement e g using stakeholders to inform consult involve collaborate or empower the decision problem bryson 2004 3 1 2 clarifying current knowledge and assumptions stakeholder assumptions and information availability e g trend data documents previous plans which is crucial for identifying those interventions that most effectively enhance the system performance can lead to different knowledge of candidate decisions a priori expert specification of candidate decisions is effective when the current knowledge is significant feedback and evidence on performance is readily available and their preferences for identifying key e g worst best and mid level decisions are well defined kahneman and klein 2009 shanteau 1992 this implies that candidate decisions can be assumed and ordered or ranked a priori based on the aggregation and weighting of stakeholder preferences the existing knowledge of candidate decisions however becomes inadequate when there are unknown or highly ambiguous consequences conflicting trade offs across values or measures of performance and significant disagreements among stakeholder preferences about the selection of candidate decisions this necessitates the identification of decisions a posteriori after analysing many potential possibilities of decisions see e g basdekas 2014 brill et al 1990 ferringer et al 2009 smith et al 2019 available information and stakeholder assumptions about driving forces of the future can lead to different levels of knowledge about future scenarios döll and romero lankao 2017 zare et al 2019 the knowledge of scenarios is subject to ranges of uncertainties related to natural variability aleatory and or to lack of knowledge and ambiguity epistemic about the future der kiureghian and ditlevsen 2009 robustness frameworks focus on the epistemic form of uncertainty which can exist in model structure inputs and outcomes walker et al 2003 stakeholder knowledge of future scenarios is reliable when stakeholders are confident about how the future unfolds i e a set of deterministic scenarios or they can approximate probability distributions of different regions of the uncertainty space a priori i e known likelihood of future events this corresponds to the first and second level of uncertainty in walker et al 2003 where a standard sensitivity analysis or statistical and probabilistic uncertainty quantification can be effective for dealing with uncertainty knowledge of scenarios can also be reliable when a set of known scenarios can be specified but likelihood cannot be assigned this corresponds to walker et al 2003 s third level of uncertainty where traditional scenario approaches are effective however stakeholder knowledge of scenarios becomes inadequate when a lack information or consensus exists for identifying scenarios ranking them or for associating a probability distribution to them this reflects deep uncertainty lempert et al 2003 where selecting the key scenarios of the futures a posteriori after analysing many plausible scenarios becomes an effective tool a higher level of this condition is ignorance where the knowledge of plausible scenarios and their probabilities is contested stirling 2010 fig 5 shows different degrees of stakeholder knowledge in relation to decision and uncertainty spaces the system of problem solving defined as the stakeholders identified and involved in decision support can influence the current knowledge and initial assumptions an unbalanced representation of stakeholders can present limited pictures of futures that only rely on dominant stakeholder imaginations if the initial decision support process also lacks open and transparent communication about the decision problem among groups of stakeholders the complexities and uncertainties of the decision problem can be oversimplified e g deep uncertainty can be mistaken as well characterised scenarios iwanaga et al 2018 the mental model biases of stakeholders to previous methods can also negatively influence assumptions about the problem at hand for example linear thinking can simplify non linear feedback effects characteristic of complex problems and lead to a blinkered representation of decision impact on system performance 3 1 3 defining analytical goal s what a decision support process aims to deliver or the analytical goal is another choice to make which shapes the specification of the methodological path taken the range of potential analytical goals for deliberative decision support can include standard trade off analysis to evaluate the performance of a set of well defined candidate decisions under given well characterised scenarios e g turan et al 2018 sensitivity analysis in decision making to analyse the sensitivity of outcomes to the set of well defined candidate decisions over many scenarios e g dessai and hulme 2007 pye et al 2017 stress testing also known as adaptation analysis and scenario discovery bryant and lempert 2010 groves and lempert 2007 to analyse when a set of well defined candidate decisions fails to achieve objectives and when adaptation is needed e g lempert et al 2013 ray et al 2018 worst case scenario discovery to identify the worst possible outcomes of a set of decisions perfect storm and scenarios leading to those outcomes e g halim et al 2015 multi objective optimisation to search for candidate decisions that can make good trade offs between multiple objectives under reference scenarios e g sleptchenko et al 2019 in press reed et al 2013 kasprzyk et al 2009 kollat and reed 2006 multi objective robust optimisation to search for candidate decisions that can robustly fulfil multiple objectives over many scenarios e g moallemi et al 2018a quinn et al 2017 trindade et al 2017 watson and kasprzyk 2017 quinn et al 2018 kwakkel et al 2015 the analytical goal is normally defined based on the type of insights decision makers need to inform decision making however stakeholder bias towards deliberately concentrating on a single issue and ignoring the broader picture hertwig and engel 2016 may lead to narrowly framed analytical goal s trade off analysis of a set of pre specified natural resource management actions where there are several other potentially better performing actions which could be identified through a multi objective optimisation is an example of narrowly framed analytical goals specification of analytical goals can also be heavily influenced by available knowledge and assumptions as discussed in section 3 1 2 i e backward interdependency fig 6 for example when stakeholder knowledge of candidate decisions is contested an analytical goal such as multi objective robust optimisation which can search over a broad decision space and generate many potential decision alternatives can be more effective in another example where there is limited or conflicting information about future scenarios analytical goals such as stress testing and worst case scenario discovery which can explore future uncertainties may be more effective choice of analytical goal may also lead to dependencies in future decision forks i e forward interdependency see section 3 3 2 3 2 problem framing 3 2 1 formulating decisions stakeholder knowledge of decisions in the previous decision fork enables the formulation of candidate decisions see fig 7 when decisions are well defined and our current knowledge is reliable candidate decisions are often formulated as a pre specified list proposed by stakeholders a priori before any computational analysis this is commonly used in the rdm framework lempert et al 2013 decision scaling brown et al 2012 and info gap analysis ben haim 2006 in the case of a priori identification of candidate decisions decision support relies on a classical discrete choice among the identified decisions based on their robustness e g a trade off analysis or stress testing see brown et al 2012 bryant and lempert 2010 moody and brown 2013 however when the knowledge of decisions is inadequate candidate decisions are often generated through a search heuristic over the decision space such as the approach used in the mordm frameworks kasprzyk et al 2013 this enumerates many decisions through computational exploration e g latin hypercube sampling matrosov et al 2013 or multi objective optimisation coello et al 2007 hadka and reed 2013 reed et al 2013 candidate decisions are only selected a posteriori for example based on stakeholder preferences candidate decisions can also be formulated in an iterative stress and test process where an initial pre specified list of candidate decisions is modified for example after a search heuristic and trade off analysis see e g eker and van daalen 2015 to enhance their robustness specifying and prioritising decisions a priori can be heavily affected by biases beliefs heuristics and values of stakeholders these human factors can limit the exploration of the decision space and lead to sub optimal decisions one illustration of these human factors is rooted in narrow thinking where stakeholders deliberately limit the consideration of alternative decisions with the tacit assumption that their expertise is sufficient by itself to attain high levels of system performance narrow thinking can overlook candidate decisions that are not known or do not seem to be effective in advance even though they could potentially deliver higher performance compared to pre specified candidate decisions this is a form of cognitive discounting glynn et al 2017 which focuses on some obvious decisions rather than to consider their out of the box alternatives another influential human factor is where the decision maker intentionally disregards those decisions that do not contribute to or contradict their self interest despite being potentially effective for enhancing system performance e g see khazaei et al 2019 madani and lund 2011 such influences can be related to expressed preferences and hidden motives of stakeholders in decision support for example zare et al 2017a showed that a lack of alternatives to proposed infrastructure heavy solutions e g dam construction for water shortage resulted either from narrow thinking or hidden motives which obscured the fundamental causes e g water use pattern agricultural expansion institutional failures led to short term mitigation of the problem symptoms and increased the side effects gleick 2002 kasprzyk et al 2009 olmstead 2010 3 2 2 framing future scenarios there are two main aspects of framing future scenarios i e states of the world over which decisions are to be evaluated the first is the identification of influential but uncertain factors shaping future scenarios e g exogenous factors xˮ in the rand s xlrm framework see lempert et al 2003 via participatory processes with stakeholders the second is the means by which uncertain factors are selected for representation in the model based quantitative assessment depending on the knowledge of scenarios they can be generated in one of two main ways see fig 7 pre specified standardised and deterministic frames of the future e g liu et al 2008 moallemi et al 2017b or globally sampled stochastic analysis e g herman et al 2015 ranger and niehörster 2012 trindade et al 2017 the former represents the uncertainty space based on a set of scenario specifications and model inputs set a priori by stakeholders the latter generates scenarios through computational search sampling or optimisation over a well characterised probability distribution brown et al 2012 moody and brown 2013 zeff et al 2014 or over deep uncertainty without clear information or consensus on priors about the uncertainty space bankes 1993 eker and kwakkel 2018 groves et al 2019 hall et al 2012 kwakkel 2017 lamontagne et al 2018 2019 moallemi and köhler 2019 trindade et al 2017 narrow thinking can favour imaginable scenarios of stakeholders underestimate or disregard the possibility of less considered surprises and shocks and cause the unintentional dismissal of some uncertainties an example is the unintentional exclusion of socio political uncertainties in integrated water resources assessment and modelling when bio physical aspects dominate stakeholder concerns zare et al 2017b 3 2 3 selecting robustness measures this decision fork is to specify a set of measures in the outcome space which represent the robustness of decisions over possible scenarios for quantitative analysis or qualitative measures when the measures are not easily quantified measures can be selected based on the decision context and stakeholder risk attitudes such as avoiding regret maximising performance and or minimising variance amongst other criteria giuliani and castelletti 2016 mcphail et al 2018 among these measures are the absolute value of model outcomes represented with descriptive statistics e g means and variances moallemi and köhler 2019 voudouris et al 2014 satisficing measures bryant and lempert 2010 halim et al 2015 lempert et al 2013 and regret measures hall et al 2012 kasprzyk et al 2013 lempert and collins 2007 popper et al 2009 focusing deliberately on specific robustness measures can misrepresent multiple dimensions of the system performance for example risk aversion bias glynn et al 2017 will favour measures e g regret measures which are more sensitive to potential loses than potential gains from a decision expressed preferences and hidden motives can also lead to interpreting the results in the ways that confirm stakeholder beliefs expectations or hypotheses see e g madani and khatami 2015 montibeller and von winterfeldt 2015 nickerson 1998 3 3 evaluation and implementation 3 3 1 developing or selecting an evaluation model this decision fork is about the development or selection of models to evaluate candidate decisions in what if scenarios maier et al 2016 through running batch simulations the development or selection of such models can include choices in multiple categories conceptual framing stakeholder participation methods in model development for participatory modelling and the modelling approach to be used multiple choices can be made for each category for example conceptual framing can be performed using causal loop diagrams see e g moallemi et al 2017a mind maps see e g davies 2011 and decision trees see robinson 2008 stakeholder participatory modelling methods can include focus groups workshops and role playing games amongst others see voinov 2017 simulation models can also be developed using system dynamics approach papachristos 2018 or agent based modelling hansen et al 2019 problem framing see section 3 2 can influence model specification e g system boundary abstraction and aggregation levels this interdependency can lead to the development of fit for purpose models haasnoot et al 2014 that portray the systems of interest at a sufficient level of aggregation to serve the specific problem framing a fit for purpose model can be agile and efficient in performing a large number of simulation runs with a minimum possible computational cost moallemi et al 2018c the presence of multiple rival problem frames quinn et al 2017 may lead to the development of multiple models this multiplicity of models can be reflected in alternative conceptual framing model structures and parameterisations see e g pruyt and kwakkel 2014 however it is often restrained by the fidelity and flexibility of the base model the fidelity of model representation and its flexibility e g computational platforms versioning extensibility data demands hardware constraints and software development strategy can cause fundamental issues for developing multiple models the researchers and stakeholders who become involved in the decision support process can influence model development selection at this decision fork one example of the influence of human factors is the expressed preference or hidden motives towards using locally developed tools or methods voinov et al 2018 methods and approaches used in previous projects can shape views among the problem solving team and provide justification for the use of established methods in new projects this can also be related to a confirmation bias nickerson 1998 where the aim is to prove the locally developed tools credibility salience and legitimacy cash et al 2003 an unbalanced representation of stakeholders system of problem solving can reinforce this human factor people prefer to use their own methods toolboxes and previous models than those of other peer groups even if they only have limited experience with them this exemplifies the not invented here syndrome webb 2010 where modellers are biased against other groups tools and prefer what they already have or know 3 3 2 implementing problem frames this decision fork is about the ways that models are used to analyse the performance of candidate decisions against key measures over scenarios this can involve ad hoc standardised simulation systematic sampling optimisation or a combination these choices have interdependencies with stakeholder knowledge section 3 1 2 analytical goals section 3 1 3 and framing decisions and scenarios sections 3 2 1 and 3 2 2 fig 8 when there are limited pre specified candidate decisions and scenarios stakeholders can set up the model for decisions under each scenario and run multiple simulations manually which we call ad hoc simulation when one or both of decision space and uncertainty space are characterised through computational search the model should be systematically set up and run for a large number of candidate decisions and scenarios under this condition systematic sampling or open exploration can be used to take quasi random samples e g with monte carlo sampling from the decision space and or the uncertainty space to explore different possibilities in the outcome space optimisation e g multi objective evolutionary optimisation reed et al 2013 can also be used to search over the decision space and or the uncertainty space to find a particular behaviour in the outcome space there are examples of existing robustness frameworks such as mordm kasprzyk et al 2013 which were enhanced by a new set of methods or by rearranging the ways that original methods interact with each other one example is watson and kasprzyk 2017 where they ran moea optimisation under multiple problem formulations obtained from scenario discovery model based decision analysis is being increasingly implemented using a combination of problem frames rather than a single problem frame kasprzyk et al 2013 kwakkel et al 2015 2016b moallemi et al 2018b quinn et al 2018 this necessitates choices to be made regarding how to combine the implementation of multiple problem frames moallemi et al 2019a these choices can be related to the direction of interaction level of the integration overlap between problem frames the type of information to be passed between problem frames and condition frequency and points of interactions morgan et al 2017 these choices can be articulated into six patterns see fig 9 for combining problem frames mingers and brocklesby 1997 moallemi et al 2018b morgan et al 2017 choices of mixed problem frames depend on choices made in previous decision forks as an example the limited knowledge of decisions and a deep uncertainty around future scenarios section 3 1 2 can lead to a search over both decision and uncertainty spaces and the selection of candidate decisions and scenarios a posteriori sections 3 2 1 and 3 2 2 this mix of problem framings can justify integrating the systematic sampling of uncertainties into an optimisation process where the robustness of many generated decisions is assessed over many scenarios in each iteration of optimisation similar to the mordm framework researchers and other stakeholders who become involved early in the decision support process can influence problem frame mixing section 3 1 1 for example cognitive frameworks and heuristic rules of thumb held by researchers glynn 2015 can lead to a process thinking rather than a systems thinking approach in the implementation of decision support where the focus is on mainly linear interactions between the methods steps rather than complex interactions see e g lempert et al 2003 3 3 3 designing the execution of computational experiments researchers make choices regarding the execution of computational experiments for the problem frame and model that they have developed this can include choices regarding sampling and optimisation algorithms e g monte carlo sampling full factorial sampling multi objective evolutionary optimisation algorithms moeas see reed et al 2013 the parametrisation of sampling and optimisation e g specifying sufficient monte carlo sample size to generate the diversity of model behaviour and setting the search algorithm parameters that control the implementation of optimisation algorithms tools and software e g the exploratory modelling em workbench kwakkel 2017 the mordm library hadka et al 2015 and analytical toolkits for modelling and simulation such as anylogic and vensim communication and visualisation e g graphical interfaces visual aids see keim et al 2010 reed and kollat 2013 woodruff et al 2013 choices for the execution of experiments depend on choices made at earlier decision forks for example depending on the complexity of the model developed its computational platform and data availability section 3 3 1 experiments may be executed differently in terms of number of experiments sampling method etc to make trade offs between computational costs the diversity of solutions and confidence in final conclusions kwakkel et al 2016b human factors can also influence choices for the execution of experiments one example is the expressed preference or hidden motives towards using the locally developed tools and methods or those previously used in a project 3 4 decision recommendations and monitoring 3 4 1 specifying decision implications researchers specify the implications of model results in the form of robust inferences for decision making depending on choices made at previous decision forks e g analytical goal problem framing the decision support process can lead to different inferences including the following comparing the fulfilment of objectives for a set of candidate decisions and making trade offs between their performance e g lempert et al 2013 understanding which candidate decision s leverage system performance most or under which scenario s the system performs the best e g dessai and van der sluijs 2007 adapting a decision to maintain its effectiveness in improving system performance e g hamarat et al 2014 herman et al 2014 kwakkel et al 2016b lempert and groves 2010 enhancing the robustness of a set of decisions through estimating the worst possible outcomes under extreme conditions and identifying scenarios leading to them e g halim et al 2015 finding decisions that can fulfil multiple objectives under a selected set of reference scenarios e g kasprzyk et al 2012 kollat and reed 2006 reed et al 2013 finding decisions actions or policy pathways that can fulfil multiple objectives robustly under future uncertainties e g eker and kwakkel 2018 kwakkel et al 2015 moallemi et al 2018a quinn et al 2017 quinn et al 2018 trindade et al 2017 watson and kasprzyk 2017 the decision inferences can be heavily influenced by stakeholder values and heuristics their expressed strategic motives and their hidden preferences this means that any given set of results can have different emergent medium term and long term implications if interpreted by different groups of stakeholders stakeholder influence is not necessarily negative as decision insights need to be shaped and informed by stakeholder preferences and values to be practical and implementable in real world cases however such strong human factors can sometimes lead to a negative confirmation bias glynn et al 2017 nickerson 1998 when a powerful group of stakeholders imposes its own pre existing assumptions on decision insights and when stakeholders close down the diversity of outcomes to obtain their own preferred conclusion leach et al 2010 stakeholder influence in shaping decision inferences can raise multiple questions how are competing problem frames compared from the perspectives of stakeholders who can express preferences in decision recommendations how are stakeholder conflicts resolved and asymmetries of power accounted for herman et al 2015 hermans et al 2014 woodruff et al 2013 3 4 2 monitoring system design and contingency planning the design of a monitoring system and contingency planning to secure successful decision support implementation are the two last decision forks the monitoring system is designed to observe the dynamic environment and to identify and signal ongoing changes through signposts and triggers that have critical influence on the success of decisions haasnoot et al 2018 discussed the design of the monitoring system in response to three questions as follows different answers to these questions would lead to different design of the monitoring system what changes need to be known to make the right decision at the right time various developments such as socio economic changes and stakeholder preferences that can trigger new decisions or invalidate the current knowledge and assumptions can constitute what must be known for monitoring the real world environment what signposts with what critical levels triggers need to be monitored to detect material change there are three dimensions to clarify the selection of signposts and triggers one is the identification of the qualitative and quantitative signposts as a parameter i e the variable of interest which is tracked the signpost in this sense can include ranges of parameters related to causes e g sea level rise and effects of the changes e g damage costs haasnoot et al 2018 the second is the selection of a best estimate for this signpost parameter raso et al 2019 that can be observed values of real data e g the worst 10th percentile of average rainfall to measure precipitation from different sources of measurements e g observation simulation extrapolation the third is the identification of critical values of the signposts that is the estimate value which triggers a new decision setting critical values on signposts is challenging garschagen and solecki 2017 in particular for qualitative signposts as they need to be technically sound and socially legitimate how should this information be analysed to produce useful information for decision support this relates to the selection of criteria for assessing the suitability of signposts and their critical values haasnoot et al 2018 have suggested three main criteria to choose among signposts salience including the relevance of a selected signpost its measurability observability its timeliness indicating a change over time and its reliability false and missed signals credibility including scientific and technical soundness and the level of agreement among users as well as how they are embedded into the exiting institutional contexts and legitimacy or the acceptability and fairness of signposts to the users the unexpected changes that are signalled through the monitoring system are responded to through adaptation or contingency planning kiparsky et al 2012 kwakkel et al 2010 preston et al 2011 wise et al 2014 contingency planning aims to enhance the robustness of decisions by addressing their vulnerabilities in fulfilling analytical goals under uncertainty hermans et al 2017 e g constructing sea walls to tackle extreme storm surge events in flood risk management the way that contingency planning is undertaken can lead to other choices for stakeholders such as different ways for identifying vulnerabilities i e uncertainties most responsible for the failure of decisions multiple consequence oriented sensitivity analysis methods have been used to isolate these critical uncertainties including factor prioritisation which can specify important uncertainties contributing to output failure loucks et al 2005 factor mapping which can identify areas of input uncertainty responsible for failure bryant and lempert 2010 and variance cutting which searches for uncertainty ranges under which output values remain below a failure threshold saltelli and tarantola 2002 there are also alternative choices to be made in relation to different types of contingency actions for addressing these vulnerabilities e g corrective defensive or capitalising actions kwakkel et al 2010 stakeholder knowledge preferences and values can inform the choices researchers make about what should be monitored and evaluated critical levels for signposts and the contingency actions to respond to the violation of these trigger values however stakeholder judgement and elicitation can also make the choices of these parameters biased and misled by cognitive frameworks and heuristics morgan 2014 tversky and kahneman 1974 4 challenges opportunities and future research supporting scientific robust decision making in complex human natural systems undergoing global change becomes exceedingly difficult because of tensions between competing and sometimes conflicting frames of problems e g competing objectives high dimensional and stochastic nature of systems that are formulated under different stakeholder assumptions bosomworth et al 2017 stakeholder participation and engagement have been recognised as a determinant for achieving translational real world benefits cash et al 2003 via the co production of multiple potentially valid interpretations of reality quinn et al 2017 and drawing on a diverse range of values beliefs knowledge and perspectives halbe et al 2018 voinov 2017 voinov and bousquet 2010 voinov et al 2016 models enable the testing of these different rival problem frames under many scenarios and can assist in reasoning about pathways to sustainability that are insensitive to future deviations from scenario specifications bankes 1993 2002 bankes et al 2001 herman et al 2015 kwakkel and haasnoot 2019 here we have proposed a systematic way for evaluating the alternative constructs of decision support a crucial challenge that remains is how to effectively harness the ranges of modelling and participatory methodological choices in a way that manages the boundary between knowledge systems where scientists operate and the realm of action where policy makers and decision makers sit ballantyne 2016 cash et al 2003 dahlstrom 2014 kahan 2010 future research needs to focus on enhancing the sustainability science policy interface for decision making under uncertainty by harnessing the capabilities of modelling and participatory approaches to increase the effectiveness of scientific advice via the simultaneous improvement of the credibility salience and legitimacy of decision recommendations cash et al 2003 fiske and dupree 2014 we have discussed that human choices can sometimes lead to a poor path that does not effectively support robust decision making as an example of a poor path if decisions under uncertain global change are made based on methodological choices that oversimplify diverse possible futures into a handful of qualitative scenarios the final inferences may fail in the face of future s surprises and shocks what this article is proposing is to add more relevant information to the decision making processes not to make it less human we therefore suggest the following recommendations as a way of illuminating hidden preferences or biases involved in the choices made during the decision making process in order to improve the quality of final inferences 1 increasing awareness developing greater awareness of human factors and when and how they can affect judgement is a critical step nickerson 1998 the general awareness and recognition of the role of values and cultures can help stakeholders understand the limits of the results this awareness will also increase cooperation among stakeholders to address biases with negative impacts on decisions glynn et al 2017 suggested an adaptive framework to generate information and recognise human factors to help manage their impacts 2 de biasing de biasing aims to eliminate or reduce cognitive or motivational biases and their negative impacts on the decision support process lahtinen and hämäläinen 2016 montibeller and von winterfeldt 2015 for example a de biasing exercise could involve asking why the opposite of the choice taken by stakeholders is invalid or less effective nickerson 1998 montibeller and von winterfeldt 2015 have further discussed the usefulness of de biasing techniques in multi criteria decision analysis 3 devil s advocate devil s advocate lahtinen et al 2017 or red team glynn 2015 is a peer review group whose job is to challenge crucial assumptions set by primary researchers and analyse worst case conditions devil s advocate asks fundamental questions at each decision fork about the even obvious choices made e g why was this choice made what else can we choose why not other options two examples of the use of devil s advocate are using an independent team to help point out potential problems in conceptual modelling glynn 2015 and modelling team in problem formulation schwenk and thomas 1983 4 documentation and reflection documenting the records of the decision support process allows researchers and decision makers to identify cases where a decision is not consistent with the stated objectives assumptions and available information documentation captures the choices made and their rationale and extends beyond common technical documents e g a problem solving logbook consistent documentation of choices over the life cycle of decision support and the immediate reflection of other stakeholders on its contents will increase attention to human factors and can reduce their negative effects glynn et al 2017 in summary we believe that the consideration of rival decision support paths before selecting a project s methodology a priori can help to shape a transparent fair reproducible and contextually legitimate decision making practice the presence of such rival paths also enables comparability between different methodologies and the enhancement of the confidence in decision recommendations and their sensitivity to the choice of method using a computer might be helpful in decision making by creating counterfactuals or challenging conventional wisdom however ultimately the human co production of information is also critical for creating shared stakes evaluating discordance and creating the opportunities for changing perceptions conceptions of consequential choices this can be important for enabling decision making in human natural systems in particular for novice researchers and local practitioners who need to design and adapt their decision support process to best address sustainability issues that are sensitive to the local context moallemi et al 2019b consideration of decision support paths can help jurisdictions at all scales make more robust and legitimate decisions for progressing towards sustainable and robust futures declaration of competing interest the authors whose names are listed for the submitted manuscript entitled structuring and evaluating decision support processes to enhance the robustness of complex human natural systems have no conflict of interests in the current work 
26097,decision making in the context of complex human natural systems requires a transition towards robust model based inferences which are effective despite uncertainties of human and climate driven change supporting robust decision making needs a sequence of interactive methodological choices for setting the problem context framing the decision problem evaluating possible solutions and making recommendations these methodological choices are influenced by a variety of human factors originating from cognitive behavioural and mental frameworks of stakeholders we review a broad array of methodological constructs to better emphasise the choices that are most appropriate given different levels of knowledge consideration of these methodological constructs clarifies how problems can be perceived and framed in rival decision support paths emerging from the cumulative effects of individual methodological choices and the challenging human factors that shape decision making under deep uncertainty we conclude that the careful consideration of rival decision support paths can enhance the confidence in decision recommendations and illuminate sensitivities to the methodological choices keywords decision analysis deep uncertainty scenario discovery exploratory modelling robustness cognitive biases and heuristics 1 introduction the debate on global environmental and societal change has shifted from problems to solutions in recent years schellnhuber et al 2011 a globally prominent example is the un 2030 agenda for sustainable development with its urgent call to action for achieving 17 sustainable development goals sdgs un 2015 aiming at multiple environmental and socio economic aspects such as decarbonising energy systems conserving biodiversity reducing poverty and improving gender equality to achieve such diverse goals decision makers need to make robust decisions insensitive to well characterised as well as deep uncertainties ben haim 2006 lempert et al 2003 that are characteristics of complex and coupled human natural systems deep uncertainty emerges from the limited and contested knowledge among stakeholders about interacting human natural systems and their boundaries the state e g the likelihood of key drivers of these systems e g population growth water availability energy demand and the relative importance of the outcomes e g trade offs between goals for economic development and climate actions khatami et al 2019 kwakkel et al 2016a singh et al 2015 the shift towards robust decisions requires careful accounting of candidate costs or regrets associated with highly precautionary or risk averse decision alternatives dessai and hulme 2007 lincke and hinkel 2018 small and xian 2018 weaver et al 2013 we aim to develop a unified framework that can inform robust inferences in complex human natural systems by classifying sequencing and evaluating alternative framings of decision support we specifically focus on model based decision support under different degrees of uncertainty from well characterised to deep uncertainty where a model i e conceptual or quantitative of the system of interest is used for the evaluation of assumptions and hypotheses walker 2000 decision support in this context requires a sequence of interactive methodological choices about how to set the problem context e g deep or well defined uncertainty how to frame the problem e g limited scenario narratives or many quantitative scenarios how to evaluate candidate decisions e g model simulation or qualitative evaluation and how to drive robust inferences and provide decision recommendations tsoukiàs 2008 alternative combinations of the variety of choices made by analysts can lead to fundamentally different outcomes for the systems of focus see kwakkel 2017 decision support is also a subjective and social process involving various stakeholders who directly or indirectly influence the methodological choices underlying any analysis voinov et al 2014 within this subjective process methodological choices can be influenced deliberately e g through self interest or unintentionally e g through cognitive bias or ignorance argyris and schön 1978 lahtinen and hämäläinen 2016 mayer et al 2017 for example in new projects analysts often tend to favour methods and approaches with which they are familiar and have used previously potentially in unrelated contexts lahtinen et al 2017 these traits can influence the evaluation of robust decisions by making one particular methodological choice e g deterministic simulation to be selected over other possibly more effective ones e g a robustness analysis which considers deep uncertainty the risks of biased methodological choices are in sub optimal or even misleading decision implications for the project kasprzyk et al 2009 lempert and collins 2007 oddo et al 2017 singh et al 2015 several studies have reviewed and classified common methodological choices in robustness focused decision support frameworks dittrich et al 2016 herman et al 2015 kwakkel and haasnoot 2019 maier et al 2016 however they are limited in addressing how researchers and practitioners should a make the choices that are most appropriate given different levels of knowledge b become critically aware of the human factors that influence their methodological choices c evaluate the combined consequences of these choices and factors on planning outcomes and d take measures to tackle their negative consequences we clarify sequences and interdependencies of methodological choices under the influence of critical human factors e g biases beliefs heuristics values from psychology and cognitive sciences we articulate how alternative candidate options for undertaking decision support processes termed decision support paths emerge from the cumulative effects of choices made at different stages of the process termed decision forks this enables us to analyse how alternative decision support paths may lead to a diversity of final inferences that are sensitive to the path taken i e path dependency we also go beyond existing taxonomies in the literature which mostly focus on choices of analytical components e g how to generate decisions and scenarios and review new methodological steps for framing and evaluating the decision support processes themselves the articulation of methodological choices at decision forks the resulting decision support paths their path dependency and their influential human factors contributes to policy discussions around assessing global environmental and societal sustainability and change in three ways first it can pave the way for the exploration and the deliberative improvement of robust decision making by illuminating the presence of alternative sometimes less well understood rival paths to be considered quinn et al 2017 saltelli and giampietro 2017 wirtz and nowak 2017 recognising and identifying rival paths leads to awareness about the variety of potential alternative ways of informing decision making depending on the context without being locked into one dominant but not necessarily relevant implementation of decision support second the transparent articulation of available methodological choices and potential paths helps to demystify the decision support process and to facilitate a better understanding of the overall effects of methodological choices on final inferences carefully documenting decision support paths is critical for ensuring that decision recommendations are supported by credible evidence and therefore are defensible cash et al 2003 cooke 1991 third discussion of the impact of influential human factors helps researchers and practitioners to be critically aware of the effects of mental and behavioural aspects on the decision support process and their range of consequences for conclusions it also signifies the importance of recognition evaluation and management of influential human factors in research projects to improve the effectiveness of outcomes section 2 presents the four conceptual bases of the current work section 3 explains methodological choices and human factors and their potential influence throughout the decision support process section 4 discusses frontier challenges opportunities and recommendations for future decision support innovations 2 conceptual basis this section briefly introduces the four conceptual pillars for this article we use concepts drawn from a constructive decision aiding to set out a generic description of the decision support process and different forks at which choices need to be made b a taxonomy of robustness frameworks to articulate methodological choices at decision forks c the path perspective to demonstrate the way in which particular methodological choices can lead to specific implementations of decision support d human factors to discuss the ways that the decision making process and its final results can be biased 2 1 constructive decision aiding we adapted the concepts of constructive decision aiding from tsoukiàs 2008 creating the four stages represented in fig 1 1 representation of the problem context 2 problem framing 3 evaluation and implementation 4 decision recommendations and monitoring these stages can be implemented with different approaches simulation optimisation approaches have been used widely to inform the decision support process using model based analysis see amaran et al 2016 for a review in the context of robustness analysis these approaches are connected to monte carlo simulation reliability engineering and evolutionary optimisation beyer and sendhoff 2007 maier et al 2014 nicklow et al 2010 reed et al 2013 within model based decision support exploratory modelling emerged in the early 1990s to enhance decision making under deep uncertainty bankes 1993 hodges 1991 hodges and dewar 1992 lempert et al 2003 exploratory modelling enables robust decision making by exploring the implications of varying decision assumptions within a decision space and scenarios within an uncertainty space in terms of performance or robustness measures within an outcome space using a series of computational experiments bankes 1993 multiple so called robustness frameworks of which some are capable of adopting exploratory modelling also emerged in support of robust decision making among them are multi objective robust decision making mo rdm kasprzyk et al 2013 lempert et al 2003 info gap decision theory ben haim 2006 dynamic adaptive policy pathways haasnoot et al 2013 kwakkel et al 2015 and decision scaling brown et al 2012 amongst others see walker et al 2013 and kwakkel and haasnoot 2019 for a review of these frameworks robustness frameworks adopt two contrasting approaches to decision support herman et al 2015 tsoukiàs 2008 a priori analyses that generate and order or rank candidate decisions using modelled abstractions of decision maker preferences e g aggregation schemes which weight incommensurate measures or expectation operators such as maximising expected utility this is the dominant means by which traditional risk and reliability studies have typically been formulated and solved using single objective optimisation techniques e g see chankong and y 1983 haimes 1998 a posteriori analyses that relate exploratory modelling where candidate decisions key uncertainties their conflicting objectives and assumed preferences are discovered from a diverse search through data gathering computational exploration and multi objective optimisation this approach is mainly driven by the mantra generate then choose within the broader information context coello et al 2007 cohon and marks 1975 herman et al 2015 johnson and geldner 2019 simpson et al 2016 robustness frameworks with a posteriori analysis can cope with deep uncertainty by avoiding prior assumptions to allow for unforeseen conditions weaver et al 2013 and by reducing possible system behaviours based on stakeholder preferences but only after broad assessment of trade offs and assumptions herman et al 2015 2 2 the taxonomy of robustness frameworks robustness frameworks can be supported by simulation models to investigate the implications of decisions over scenarios in terms of performance robustness measures see fig 2 based on methodological choices made in robustness frameworks the mechanics of decision support can be implemented in several ways herman et al 2015 and kwakkel and haasnoot 2019 proposed the following taxonomy of robustness frameworks generation of decisions i e alternatives candidate decisions can be pre specified standardised alternatives e g put forward by experts e g liu et al 2008 or can be generated through computational search such as a systematic sampling e g latin hypercube sampling or optimisation kasprzyk et al 2013 over the decision space generation of scenarios i e states of the world scenarios can be specified as pre specified standardised scenarios e g defined by experts e g moallemi et al 2017b or can be generated through exploratory modelling over the uncertainty space lamontagne et al 2018 measurement of performance i e robustness different measures can be used to represent the performance of candidate decisions under uncertainty see the recent reviews e g giuliani and castelletti 2016 mcphail et al 2018 among them are descriptive statistical measures i e representing ranges of outcomes statistically satisficing measures i e fulfilling minimum performance requirements and regret measures i e minimising the gap between the performance of a selected decision with that of the best decision in a prevailing scenario vulnerability analysis i e robustness controls decision support can use different ways to isolate scenarios responsible for a specific behaviour of interest such as an undesirable performance among them are subspace partitioning methods such as scenario discovery bryant and lempert 2010 and sensitivity analysis methods such as factor mapping which identifies the sensitivity of uncertain factors leading to a specific behaviour guivarch et al 2016 and factor prioritisation which ranks uncertain factors in order of their sensitivity saltelli et al 2008 2 3 the path perspective the emerging idea of the path perspective originated in behavioural operations research hämäläinen and alaja 2008 hämäläinen et al 2013 landry et al 1983 morris 1967 walker 2009 and behavioural decision theory payne et al 1992 simon 1956 1957 with roots in the psychological literature payne et al 1999 lahtinen and hämäläinen 2016 defined a path as the sequence of choices made through the life cycle of the deliberative processes that support a given application of decision support paths reflect different ways that decision support can be implemented e g optimisation or design of experiments and the variety of conclusions that can be obtained e g what candidate decisions fulfil objectives or how sensitive they are to scenarios it is noteworthy that this particular conceptualisation of path is different from other uses of the term for example the sequence of policies and actions in dynamic adaptive policy pathways haasnoot et al 2013 or the concept of sustainability pathways for achieving the sdgs gao and bryan 2017 despite the difference there is a commonality between the meaning of the path concept in these contexts and our use which refers to open ended sequences of options that could lead to multiple alternative outcomes a path then in this context is similar to the chain of reasoning that shapes ideas about a problem in a decision pathway gregory et al 1997 pathway choices are made at decision forks by stakeholders see section 3 1 1 for the definition of stakeholders where they decide how the decision support process is designed and implemented lahtinen et al 2017 for example how to formulate the problem and what methods to choose to evaluate the candidate actions choices at different decision forks are interdependent in a sense that those made at one fork may create lock in effects in particular choices or eliminate certain possibilities at subsequent forks this makes final decision recommendations highly sensitive to the methodological path taken or path dependent lahtinen and hämäläinen 2016 other studies have acknowledged the challenges posed by path dependency arguing how sequences of individual methodological choices influence the final results for example herman et al 2015 and mcphail et al 2018 discussed ways that the choice of performance robustness measures influenced final conclusions moallemi et al 2018b discussed how sequences of methods create different decision insights and islam and pruyt 2016 and moallemi et al 2018d discussed how computational experiment design influences the final results 2 4 influences on human decision making decision support processes are affected by multiple human factors e g intuition heuristics biases or behavioural contexts there is an extensive theoretical body of literature that has a long history of addressing these influential factors glynn 2015 hämäläinen 2015 montibeller and von winterfeldt 2015 payne et al 1992 simon 1956 tversky and kahneman 1974 we classify these human factors conceptually based on their origin e g mental framework behaviour as follows beliefs values heuristics and cognitive frameworks can be highly influential on people s judgement in the decision making process hämäläinen 2015 payne et al 1992 simon 1957 tversky and kahneman 1974 stakeholders can believe in and value some observations and alternatives more than others these personal preferences beliefs and values can lead to different types of biases in practice relevant examples include cognitive and motivational biases montibeller and von winterfeldt 2015 confirmation bias nickerson 1998 and groupthink bias janis 1972 all of which can negatively or positively affect some choices to the detriment of others lahtinen et al 2017 motivational effects such as expressed or hidden preferences can be a source of bias in decision making they often happen when stakeholder strategic or self interest motives lead to unintentional subconscious or deliberate conscious choices of some alternatives and can lead to ignorance misjudgement of other potential alternatives lahtinen et al 2017 montibeller and von winterfeldt 2015 mental model and focused thinking approach influence human decisions too bessette et al 2017 lahtinen et al 2017 for example focused thinking is where stakeholders deliberately focus on particular issues or perspectives e g focusing on bio physical aspects more than socio economic causes in integrated water resources modelling see zare et al 2017b likewise narrow thinking which refers to disregarding actively or unintentionally alternatives is another source of influence also relevant to this category are cases when the problem solver s mental model is biased towards previous methods used in the past which exemplifies when you have a hammer everything looks like a nail see hämäläinen 2015 system of problem solving is another source of influence in decision making which occurs when the strong impact of the starting point and the structure of the decision support process shape and influence the future choices of stakeholders lahtinen and hämäläinen 2016 the system of problem solving is composed of those involved as stakeholders the assumptions that they have and stakeholder choices with regard to project schedule management of interactions and technical details i e choice of models experimental runs etc 3 rival decision support paths rival decision support paths can emerge from the succession of methodological choices at different decision forks their path dependency and the effects of influential human factors e g biases values fig 3 shows an overview of these decision forks with their potential methodological choices the interdependencies between decision forks create cumulative impacts on the methodological design and on the final planning outcomes path dependency for example limited stakeholder knowledge of future scenarios fork in section 3 1 2 can justify searching over a broad uncertainty space and exploring many scenarios for the consideration of future unknown events fork in section 3 2 2 while the impacts of all decision forks on the quality of final outcomes are significant the forks themselves can vary significantly in how they influence the design of the methodology for example the level of knowledge available for defining decisions and scenarios section 3 1 2 can strongly influence analytical goals section 3 1 3 how decisions and scenarios are formulated sections 3 2 1 and 3 2 2 and the required computational experiments section 3 3 3 alternatively some other forks may represent modest variations in methodology while effecting substantial impacts on final outcomes for example the choices of the measure of robustness section 3 2 3 do not fundamentally change the way that decision making process is designed but it may significantly impact the final planning outcomes see mcphail et al 2018 the significance i e strength of these interdependencies depend on the context of the study fig 4 presents an illustrative but not necessarily enumerative illustration of influential forward interdependencies that could be shared among different decision support contexts this illustration can guide more detailed assessments of these interdependencies and their cumulative impacts in the context of a specific decision support application while decision forks are introduced in a sequence they remain highly interactive in implementation and choices made at decision forks can be modified with backward interactions for example the choice of performance measures can inform the initial choice of candidate decisions herman et al 2015 and problem formulation depends on the intended implementation of decision support and vice versa woodruff et al 2013 this is related to constructive decision aiding tsoukiàs 2008 and negotiated design selection woodruff et al 2013 where decision support stages are integrated in a process based on learning how to best formulate and implement the decision problem 3 1 representation of the problem context 3 1 1 analysing stakeholders stakeholder analysis reed et al 2009 aims to identify stakeholders to categorise them and to examine relationships between them it is important to map who is involved in decision support and who knows what i e identification which individual stakeholders are more influential i e categorisation and how stakeholders influence each other s decisions i e relationships different groups of stakeholders including researchers decision makers clients advocacy groups power groups and communities can be involved in the decision support process they can be identified with a range of methods such as focus groups snowball sampling amongst others reed et al 2009 stakeholders can be categorised analytically based on their level of influence lindenberg and crosby 1981 and their and legitimacy mitchell et al 1997 or in a reconstructive manner for example using q methodology barry and proops 1999 in an empirical analysis of stakeholder perceptions reed et al 2009 different types of relationships among stakeholders can be also identified stakeholder interdependencies e g actor linkage metrics see biggs and matsaert 1999 the patterns of communication and influence between stakeholders e g social network analysis see prell et al 2009 and the content of information circulated between stakeholders e g knowledge mapping see wexler 2001 narrow thinking ermer et al 2006 and deliberate ignorance hertwig and engel 2016 are two examples of human factors which can lead to underestimating the role of some groups of stakeholders or individuals within each group in decision support they can result in a lack of awareness of alternative views in setting the stage for decision support leading to an unbalanced representation of the decision problem for example decision support dominated by theoretical researchers can overlook practical constraints e g institutional barriers or legitimacy in making recommendations stakeholder identification is also dependent upon those responsible and how they view the rationale behind stakeholder participation for example the values and heuristics of the analyst responsible for stakeholder identification may determine who is involved and who is not e g the exclusion of environmental advocacy groups by power groups and their level of involvement e g using stakeholders to inform consult involve collaborate or empower the decision problem bryson 2004 3 1 2 clarifying current knowledge and assumptions stakeholder assumptions and information availability e g trend data documents previous plans which is crucial for identifying those interventions that most effectively enhance the system performance can lead to different knowledge of candidate decisions a priori expert specification of candidate decisions is effective when the current knowledge is significant feedback and evidence on performance is readily available and their preferences for identifying key e g worst best and mid level decisions are well defined kahneman and klein 2009 shanteau 1992 this implies that candidate decisions can be assumed and ordered or ranked a priori based on the aggregation and weighting of stakeholder preferences the existing knowledge of candidate decisions however becomes inadequate when there are unknown or highly ambiguous consequences conflicting trade offs across values or measures of performance and significant disagreements among stakeholder preferences about the selection of candidate decisions this necessitates the identification of decisions a posteriori after analysing many potential possibilities of decisions see e g basdekas 2014 brill et al 1990 ferringer et al 2009 smith et al 2019 available information and stakeholder assumptions about driving forces of the future can lead to different levels of knowledge about future scenarios döll and romero lankao 2017 zare et al 2019 the knowledge of scenarios is subject to ranges of uncertainties related to natural variability aleatory and or to lack of knowledge and ambiguity epistemic about the future der kiureghian and ditlevsen 2009 robustness frameworks focus on the epistemic form of uncertainty which can exist in model structure inputs and outcomes walker et al 2003 stakeholder knowledge of future scenarios is reliable when stakeholders are confident about how the future unfolds i e a set of deterministic scenarios or they can approximate probability distributions of different regions of the uncertainty space a priori i e known likelihood of future events this corresponds to the first and second level of uncertainty in walker et al 2003 where a standard sensitivity analysis or statistical and probabilistic uncertainty quantification can be effective for dealing with uncertainty knowledge of scenarios can also be reliable when a set of known scenarios can be specified but likelihood cannot be assigned this corresponds to walker et al 2003 s third level of uncertainty where traditional scenario approaches are effective however stakeholder knowledge of scenarios becomes inadequate when a lack information or consensus exists for identifying scenarios ranking them or for associating a probability distribution to them this reflects deep uncertainty lempert et al 2003 where selecting the key scenarios of the futures a posteriori after analysing many plausible scenarios becomes an effective tool a higher level of this condition is ignorance where the knowledge of plausible scenarios and their probabilities is contested stirling 2010 fig 5 shows different degrees of stakeholder knowledge in relation to decision and uncertainty spaces the system of problem solving defined as the stakeholders identified and involved in decision support can influence the current knowledge and initial assumptions an unbalanced representation of stakeholders can present limited pictures of futures that only rely on dominant stakeholder imaginations if the initial decision support process also lacks open and transparent communication about the decision problem among groups of stakeholders the complexities and uncertainties of the decision problem can be oversimplified e g deep uncertainty can be mistaken as well characterised scenarios iwanaga et al 2018 the mental model biases of stakeholders to previous methods can also negatively influence assumptions about the problem at hand for example linear thinking can simplify non linear feedback effects characteristic of complex problems and lead to a blinkered representation of decision impact on system performance 3 1 3 defining analytical goal s what a decision support process aims to deliver or the analytical goal is another choice to make which shapes the specification of the methodological path taken the range of potential analytical goals for deliberative decision support can include standard trade off analysis to evaluate the performance of a set of well defined candidate decisions under given well characterised scenarios e g turan et al 2018 sensitivity analysis in decision making to analyse the sensitivity of outcomes to the set of well defined candidate decisions over many scenarios e g dessai and hulme 2007 pye et al 2017 stress testing also known as adaptation analysis and scenario discovery bryant and lempert 2010 groves and lempert 2007 to analyse when a set of well defined candidate decisions fails to achieve objectives and when adaptation is needed e g lempert et al 2013 ray et al 2018 worst case scenario discovery to identify the worst possible outcomes of a set of decisions perfect storm and scenarios leading to those outcomes e g halim et al 2015 multi objective optimisation to search for candidate decisions that can make good trade offs between multiple objectives under reference scenarios e g sleptchenko et al 2019 in press reed et al 2013 kasprzyk et al 2009 kollat and reed 2006 multi objective robust optimisation to search for candidate decisions that can robustly fulfil multiple objectives over many scenarios e g moallemi et al 2018a quinn et al 2017 trindade et al 2017 watson and kasprzyk 2017 quinn et al 2018 kwakkel et al 2015 the analytical goal is normally defined based on the type of insights decision makers need to inform decision making however stakeholder bias towards deliberately concentrating on a single issue and ignoring the broader picture hertwig and engel 2016 may lead to narrowly framed analytical goal s trade off analysis of a set of pre specified natural resource management actions where there are several other potentially better performing actions which could be identified through a multi objective optimisation is an example of narrowly framed analytical goals specification of analytical goals can also be heavily influenced by available knowledge and assumptions as discussed in section 3 1 2 i e backward interdependency fig 6 for example when stakeholder knowledge of candidate decisions is contested an analytical goal such as multi objective robust optimisation which can search over a broad decision space and generate many potential decision alternatives can be more effective in another example where there is limited or conflicting information about future scenarios analytical goals such as stress testing and worst case scenario discovery which can explore future uncertainties may be more effective choice of analytical goal may also lead to dependencies in future decision forks i e forward interdependency see section 3 3 2 3 2 problem framing 3 2 1 formulating decisions stakeholder knowledge of decisions in the previous decision fork enables the formulation of candidate decisions see fig 7 when decisions are well defined and our current knowledge is reliable candidate decisions are often formulated as a pre specified list proposed by stakeholders a priori before any computational analysis this is commonly used in the rdm framework lempert et al 2013 decision scaling brown et al 2012 and info gap analysis ben haim 2006 in the case of a priori identification of candidate decisions decision support relies on a classical discrete choice among the identified decisions based on their robustness e g a trade off analysis or stress testing see brown et al 2012 bryant and lempert 2010 moody and brown 2013 however when the knowledge of decisions is inadequate candidate decisions are often generated through a search heuristic over the decision space such as the approach used in the mordm frameworks kasprzyk et al 2013 this enumerates many decisions through computational exploration e g latin hypercube sampling matrosov et al 2013 or multi objective optimisation coello et al 2007 hadka and reed 2013 reed et al 2013 candidate decisions are only selected a posteriori for example based on stakeholder preferences candidate decisions can also be formulated in an iterative stress and test process where an initial pre specified list of candidate decisions is modified for example after a search heuristic and trade off analysis see e g eker and van daalen 2015 to enhance their robustness specifying and prioritising decisions a priori can be heavily affected by biases beliefs heuristics and values of stakeholders these human factors can limit the exploration of the decision space and lead to sub optimal decisions one illustration of these human factors is rooted in narrow thinking where stakeholders deliberately limit the consideration of alternative decisions with the tacit assumption that their expertise is sufficient by itself to attain high levels of system performance narrow thinking can overlook candidate decisions that are not known or do not seem to be effective in advance even though they could potentially deliver higher performance compared to pre specified candidate decisions this is a form of cognitive discounting glynn et al 2017 which focuses on some obvious decisions rather than to consider their out of the box alternatives another influential human factor is where the decision maker intentionally disregards those decisions that do not contribute to or contradict their self interest despite being potentially effective for enhancing system performance e g see khazaei et al 2019 madani and lund 2011 such influences can be related to expressed preferences and hidden motives of stakeholders in decision support for example zare et al 2017a showed that a lack of alternatives to proposed infrastructure heavy solutions e g dam construction for water shortage resulted either from narrow thinking or hidden motives which obscured the fundamental causes e g water use pattern agricultural expansion institutional failures led to short term mitigation of the problem symptoms and increased the side effects gleick 2002 kasprzyk et al 2009 olmstead 2010 3 2 2 framing future scenarios there are two main aspects of framing future scenarios i e states of the world over which decisions are to be evaluated the first is the identification of influential but uncertain factors shaping future scenarios e g exogenous factors xˮ in the rand s xlrm framework see lempert et al 2003 via participatory processes with stakeholders the second is the means by which uncertain factors are selected for representation in the model based quantitative assessment depending on the knowledge of scenarios they can be generated in one of two main ways see fig 7 pre specified standardised and deterministic frames of the future e g liu et al 2008 moallemi et al 2017b or globally sampled stochastic analysis e g herman et al 2015 ranger and niehörster 2012 trindade et al 2017 the former represents the uncertainty space based on a set of scenario specifications and model inputs set a priori by stakeholders the latter generates scenarios through computational search sampling or optimisation over a well characterised probability distribution brown et al 2012 moody and brown 2013 zeff et al 2014 or over deep uncertainty without clear information or consensus on priors about the uncertainty space bankes 1993 eker and kwakkel 2018 groves et al 2019 hall et al 2012 kwakkel 2017 lamontagne et al 2018 2019 moallemi and köhler 2019 trindade et al 2017 narrow thinking can favour imaginable scenarios of stakeholders underestimate or disregard the possibility of less considered surprises and shocks and cause the unintentional dismissal of some uncertainties an example is the unintentional exclusion of socio political uncertainties in integrated water resources assessment and modelling when bio physical aspects dominate stakeholder concerns zare et al 2017b 3 2 3 selecting robustness measures this decision fork is to specify a set of measures in the outcome space which represent the robustness of decisions over possible scenarios for quantitative analysis or qualitative measures when the measures are not easily quantified measures can be selected based on the decision context and stakeholder risk attitudes such as avoiding regret maximising performance and or minimising variance amongst other criteria giuliani and castelletti 2016 mcphail et al 2018 among these measures are the absolute value of model outcomes represented with descriptive statistics e g means and variances moallemi and köhler 2019 voudouris et al 2014 satisficing measures bryant and lempert 2010 halim et al 2015 lempert et al 2013 and regret measures hall et al 2012 kasprzyk et al 2013 lempert and collins 2007 popper et al 2009 focusing deliberately on specific robustness measures can misrepresent multiple dimensions of the system performance for example risk aversion bias glynn et al 2017 will favour measures e g regret measures which are more sensitive to potential loses than potential gains from a decision expressed preferences and hidden motives can also lead to interpreting the results in the ways that confirm stakeholder beliefs expectations or hypotheses see e g madani and khatami 2015 montibeller and von winterfeldt 2015 nickerson 1998 3 3 evaluation and implementation 3 3 1 developing or selecting an evaluation model this decision fork is about the development or selection of models to evaluate candidate decisions in what if scenarios maier et al 2016 through running batch simulations the development or selection of such models can include choices in multiple categories conceptual framing stakeholder participation methods in model development for participatory modelling and the modelling approach to be used multiple choices can be made for each category for example conceptual framing can be performed using causal loop diagrams see e g moallemi et al 2017a mind maps see e g davies 2011 and decision trees see robinson 2008 stakeholder participatory modelling methods can include focus groups workshops and role playing games amongst others see voinov 2017 simulation models can also be developed using system dynamics approach papachristos 2018 or agent based modelling hansen et al 2019 problem framing see section 3 2 can influence model specification e g system boundary abstraction and aggregation levels this interdependency can lead to the development of fit for purpose models haasnoot et al 2014 that portray the systems of interest at a sufficient level of aggregation to serve the specific problem framing a fit for purpose model can be agile and efficient in performing a large number of simulation runs with a minimum possible computational cost moallemi et al 2018c the presence of multiple rival problem frames quinn et al 2017 may lead to the development of multiple models this multiplicity of models can be reflected in alternative conceptual framing model structures and parameterisations see e g pruyt and kwakkel 2014 however it is often restrained by the fidelity and flexibility of the base model the fidelity of model representation and its flexibility e g computational platforms versioning extensibility data demands hardware constraints and software development strategy can cause fundamental issues for developing multiple models the researchers and stakeholders who become involved in the decision support process can influence model development selection at this decision fork one example of the influence of human factors is the expressed preference or hidden motives towards using locally developed tools or methods voinov et al 2018 methods and approaches used in previous projects can shape views among the problem solving team and provide justification for the use of established methods in new projects this can also be related to a confirmation bias nickerson 1998 where the aim is to prove the locally developed tools credibility salience and legitimacy cash et al 2003 an unbalanced representation of stakeholders system of problem solving can reinforce this human factor people prefer to use their own methods toolboxes and previous models than those of other peer groups even if they only have limited experience with them this exemplifies the not invented here syndrome webb 2010 where modellers are biased against other groups tools and prefer what they already have or know 3 3 2 implementing problem frames this decision fork is about the ways that models are used to analyse the performance of candidate decisions against key measures over scenarios this can involve ad hoc standardised simulation systematic sampling optimisation or a combination these choices have interdependencies with stakeholder knowledge section 3 1 2 analytical goals section 3 1 3 and framing decisions and scenarios sections 3 2 1 and 3 2 2 fig 8 when there are limited pre specified candidate decisions and scenarios stakeholders can set up the model for decisions under each scenario and run multiple simulations manually which we call ad hoc simulation when one or both of decision space and uncertainty space are characterised through computational search the model should be systematically set up and run for a large number of candidate decisions and scenarios under this condition systematic sampling or open exploration can be used to take quasi random samples e g with monte carlo sampling from the decision space and or the uncertainty space to explore different possibilities in the outcome space optimisation e g multi objective evolutionary optimisation reed et al 2013 can also be used to search over the decision space and or the uncertainty space to find a particular behaviour in the outcome space there are examples of existing robustness frameworks such as mordm kasprzyk et al 2013 which were enhanced by a new set of methods or by rearranging the ways that original methods interact with each other one example is watson and kasprzyk 2017 where they ran moea optimisation under multiple problem formulations obtained from scenario discovery model based decision analysis is being increasingly implemented using a combination of problem frames rather than a single problem frame kasprzyk et al 2013 kwakkel et al 2015 2016b moallemi et al 2018b quinn et al 2018 this necessitates choices to be made regarding how to combine the implementation of multiple problem frames moallemi et al 2019a these choices can be related to the direction of interaction level of the integration overlap between problem frames the type of information to be passed between problem frames and condition frequency and points of interactions morgan et al 2017 these choices can be articulated into six patterns see fig 9 for combining problem frames mingers and brocklesby 1997 moallemi et al 2018b morgan et al 2017 choices of mixed problem frames depend on choices made in previous decision forks as an example the limited knowledge of decisions and a deep uncertainty around future scenarios section 3 1 2 can lead to a search over both decision and uncertainty spaces and the selection of candidate decisions and scenarios a posteriori sections 3 2 1 and 3 2 2 this mix of problem framings can justify integrating the systematic sampling of uncertainties into an optimisation process where the robustness of many generated decisions is assessed over many scenarios in each iteration of optimisation similar to the mordm framework researchers and other stakeholders who become involved early in the decision support process can influence problem frame mixing section 3 1 1 for example cognitive frameworks and heuristic rules of thumb held by researchers glynn 2015 can lead to a process thinking rather than a systems thinking approach in the implementation of decision support where the focus is on mainly linear interactions between the methods steps rather than complex interactions see e g lempert et al 2003 3 3 3 designing the execution of computational experiments researchers make choices regarding the execution of computational experiments for the problem frame and model that they have developed this can include choices regarding sampling and optimisation algorithms e g monte carlo sampling full factorial sampling multi objective evolutionary optimisation algorithms moeas see reed et al 2013 the parametrisation of sampling and optimisation e g specifying sufficient monte carlo sample size to generate the diversity of model behaviour and setting the search algorithm parameters that control the implementation of optimisation algorithms tools and software e g the exploratory modelling em workbench kwakkel 2017 the mordm library hadka et al 2015 and analytical toolkits for modelling and simulation such as anylogic and vensim communication and visualisation e g graphical interfaces visual aids see keim et al 2010 reed and kollat 2013 woodruff et al 2013 choices for the execution of experiments depend on choices made at earlier decision forks for example depending on the complexity of the model developed its computational platform and data availability section 3 3 1 experiments may be executed differently in terms of number of experiments sampling method etc to make trade offs between computational costs the diversity of solutions and confidence in final conclusions kwakkel et al 2016b human factors can also influence choices for the execution of experiments one example is the expressed preference or hidden motives towards using the locally developed tools and methods or those previously used in a project 3 4 decision recommendations and monitoring 3 4 1 specifying decision implications researchers specify the implications of model results in the form of robust inferences for decision making depending on choices made at previous decision forks e g analytical goal problem framing the decision support process can lead to different inferences including the following comparing the fulfilment of objectives for a set of candidate decisions and making trade offs between their performance e g lempert et al 2013 understanding which candidate decision s leverage system performance most or under which scenario s the system performs the best e g dessai and van der sluijs 2007 adapting a decision to maintain its effectiveness in improving system performance e g hamarat et al 2014 herman et al 2014 kwakkel et al 2016b lempert and groves 2010 enhancing the robustness of a set of decisions through estimating the worst possible outcomes under extreme conditions and identifying scenarios leading to them e g halim et al 2015 finding decisions that can fulfil multiple objectives under a selected set of reference scenarios e g kasprzyk et al 2012 kollat and reed 2006 reed et al 2013 finding decisions actions or policy pathways that can fulfil multiple objectives robustly under future uncertainties e g eker and kwakkel 2018 kwakkel et al 2015 moallemi et al 2018a quinn et al 2017 quinn et al 2018 trindade et al 2017 watson and kasprzyk 2017 the decision inferences can be heavily influenced by stakeholder values and heuristics their expressed strategic motives and their hidden preferences this means that any given set of results can have different emergent medium term and long term implications if interpreted by different groups of stakeholders stakeholder influence is not necessarily negative as decision insights need to be shaped and informed by stakeholder preferences and values to be practical and implementable in real world cases however such strong human factors can sometimes lead to a negative confirmation bias glynn et al 2017 nickerson 1998 when a powerful group of stakeholders imposes its own pre existing assumptions on decision insights and when stakeholders close down the diversity of outcomes to obtain their own preferred conclusion leach et al 2010 stakeholder influence in shaping decision inferences can raise multiple questions how are competing problem frames compared from the perspectives of stakeholders who can express preferences in decision recommendations how are stakeholder conflicts resolved and asymmetries of power accounted for herman et al 2015 hermans et al 2014 woodruff et al 2013 3 4 2 monitoring system design and contingency planning the design of a monitoring system and contingency planning to secure successful decision support implementation are the two last decision forks the monitoring system is designed to observe the dynamic environment and to identify and signal ongoing changes through signposts and triggers that have critical influence on the success of decisions haasnoot et al 2018 discussed the design of the monitoring system in response to three questions as follows different answers to these questions would lead to different design of the monitoring system what changes need to be known to make the right decision at the right time various developments such as socio economic changes and stakeholder preferences that can trigger new decisions or invalidate the current knowledge and assumptions can constitute what must be known for monitoring the real world environment what signposts with what critical levels triggers need to be monitored to detect material change there are three dimensions to clarify the selection of signposts and triggers one is the identification of the qualitative and quantitative signposts as a parameter i e the variable of interest which is tracked the signpost in this sense can include ranges of parameters related to causes e g sea level rise and effects of the changes e g damage costs haasnoot et al 2018 the second is the selection of a best estimate for this signpost parameter raso et al 2019 that can be observed values of real data e g the worst 10th percentile of average rainfall to measure precipitation from different sources of measurements e g observation simulation extrapolation the third is the identification of critical values of the signposts that is the estimate value which triggers a new decision setting critical values on signposts is challenging garschagen and solecki 2017 in particular for qualitative signposts as they need to be technically sound and socially legitimate how should this information be analysed to produce useful information for decision support this relates to the selection of criteria for assessing the suitability of signposts and their critical values haasnoot et al 2018 have suggested three main criteria to choose among signposts salience including the relevance of a selected signpost its measurability observability its timeliness indicating a change over time and its reliability false and missed signals credibility including scientific and technical soundness and the level of agreement among users as well as how they are embedded into the exiting institutional contexts and legitimacy or the acceptability and fairness of signposts to the users the unexpected changes that are signalled through the monitoring system are responded to through adaptation or contingency planning kiparsky et al 2012 kwakkel et al 2010 preston et al 2011 wise et al 2014 contingency planning aims to enhance the robustness of decisions by addressing their vulnerabilities in fulfilling analytical goals under uncertainty hermans et al 2017 e g constructing sea walls to tackle extreme storm surge events in flood risk management the way that contingency planning is undertaken can lead to other choices for stakeholders such as different ways for identifying vulnerabilities i e uncertainties most responsible for the failure of decisions multiple consequence oriented sensitivity analysis methods have been used to isolate these critical uncertainties including factor prioritisation which can specify important uncertainties contributing to output failure loucks et al 2005 factor mapping which can identify areas of input uncertainty responsible for failure bryant and lempert 2010 and variance cutting which searches for uncertainty ranges under which output values remain below a failure threshold saltelli and tarantola 2002 there are also alternative choices to be made in relation to different types of contingency actions for addressing these vulnerabilities e g corrective defensive or capitalising actions kwakkel et al 2010 stakeholder knowledge preferences and values can inform the choices researchers make about what should be monitored and evaluated critical levels for signposts and the contingency actions to respond to the violation of these trigger values however stakeholder judgement and elicitation can also make the choices of these parameters biased and misled by cognitive frameworks and heuristics morgan 2014 tversky and kahneman 1974 4 challenges opportunities and future research supporting scientific robust decision making in complex human natural systems undergoing global change becomes exceedingly difficult because of tensions between competing and sometimes conflicting frames of problems e g competing objectives high dimensional and stochastic nature of systems that are formulated under different stakeholder assumptions bosomworth et al 2017 stakeholder participation and engagement have been recognised as a determinant for achieving translational real world benefits cash et al 2003 via the co production of multiple potentially valid interpretations of reality quinn et al 2017 and drawing on a diverse range of values beliefs knowledge and perspectives halbe et al 2018 voinov 2017 voinov and bousquet 2010 voinov et al 2016 models enable the testing of these different rival problem frames under many scenarios and can assist in reasoning about pathways to sustainability that are insensitive to future deviations from scenario specifications bankes 1993 2002 bankes et al 2001 herman et al 2015 kwakkel and haasnoot 2019 here we have proposed a systematic way for evaluating the alternative constructs of decision support a crucial challenge that remains is how to effectively harness the ranges of modelling and participatory methodological choices in a way that manages the boundary between knowledge systems where scientists operate and the realm of action where policy makers and decision makers sit ballantyne 2016 cash et al 2003 dahlstrom 2014 kahan 2010 future research needs to focus on enhancing the sustainability science policy interface for decision making under uncertainty by harnessing the capabilities of modelling and participatory approaches to increase the effectiveness of scientific advice via the simultaneous improvement of the credibility salience and legitimacy of decision recommendations cash et al 2003 fiske and dupree 2014 we have discussed that human choices can sometimes lead to a poor path that does not effectively support robust decision making as an example of a poor path if decisions under uncertain global change are made based on methodological choices that oversimplify diverse possible futures into a handful of qualitative scenarios the final inferences may fail in the face of future s surprises and shocks what this article is proposing is to add more relevant information to the decision making processes not to make it less human we therefore suggest the following recommendations as a way of illuminating hidden preferences or biases involved in the choices made during the decision making process in order to improve the quality of final inferences 1 increasing awareness developing greater awareness of human factors and when and how they can affect judgement is a critical step nickerson 1998 the general awareness and recognition of the role of values and cultures can help stakeholders understand the limits of the results this awareness will also increase cooperation among stakeholders to address biases with negative impacts on decisions glynn et al 2017 suggested an adaptive framework to generate information and recognise human factors to help manage their impacts 2 de biasing de biasing aims to eliminate or reduce cognitive or motivational biases and their negative impacts on the decision support process lahtinen and hämäläinen 2016 montibeller and von winterfeldt 2015 for example a de biasing exercise could involve asking why the opposite of the choice taken by stakeholders is invalid or less effective nickerson 1998 montibeller and von winterfeldt 2015 have further discussed the usefulness of de biasing techniques in multi criteria decision analysis 3 devil s advocate devil s advocate lahtinen et al 2017 or red team glynn 2015 is a peer review group whose job is to challenge crucial assumptions set by primary researchers and analyse worst case conditions devil s advocate asks fundamental questions at each decision fork about the even obvious choices made e g why was this choice made what else can we choose why not other options two examples of the use of devil s advocate are using an independent team to help point out potential problems in conceptual modelling glynn 2015 and modelling team in problem formulation schwenk and thomas 1983 4 documentation and reflection documenting the records of the decision support process allows researchers and decision makers to identify cases where a decision is not consistent with the stated objectives assumptions and available information documentation captures the choices made and their rationale and extends beyond common technical documents e g a problem solving logbook consistent documentation of choices over the life cycle of decision support and the immediate reflection of other stakeholders on its contents will increase attention to human factors and can reduce their negative effects glynn et al 2017 in summary we believe that the consideration of rival decision support paths before selecting a project s methodology a priori can help to shape a transparent fair reproducible and contextually legitimate decision making practice the presence of such rival paths also enables comparability between different methodologies and the enhancement of the confidence in decision recommendations and their sensitivity to the choice of method using a computer might be helpful in decision making by creating counterfactuals or challenging conventional wisdom however ultimately the human co production of information is also critical for creating shared stakes evaluating discordance and creating the opportunities for changing perceptions conceptions of consequential choices this can be important for enabling decision making in human natural systems in particular for novice researchers and local practitioners who need to design and adapt their decision support process to best address sustainability issues that are sensitive to the local context moallemi et al 2019b consideration of decision support paths can help jurisdictions at all scales make more robust and legitimate decisions for progressing towards sustainable and robust futures declaration of competing interest the authors whose names are listed for the submitted manuscript entitled structuring and evaluating decision support processes to enhance the robustness of complex human natural systems have no conflict of interests in the current work 
26098,the current routine of comparison and validation in climate science is frequently static and of low efficiency which hinders evidence based decision making and scientific confidence due to the aggressively increasing resolution complexity and associated data volumes of climate models objectively comparing multiple models and assessing their accuracy against observations is an ever increasing challenge we propose an integrated framework for harmonizing state of the art cyberinfrastructure techniques with the user habits formed by long term familiarity with existing community oriented software an open source prototype named covali is implemented and used to compare and validate the results of several widely used climate models and datasets our results show that the proposed cyberinfrastructure based strategy can significantly automate the comparison and validation processes in climate modeling more importantly the new strategy retains the existing user habits in the climate community while making it easier for scientists to adopt new technology in their research routine keywords model comparison model validation climate science web service cyberinfrastructure big data software availability name of software covali developer center for spatial information science and systems george mason university source language java availability the source code and application jar can be accessed via github http github com csiss cc 1 introduction big data features heavily in two essential and constantly applied steps in climate modeling workflows 1 model comparison which evaluates the level of agreement between models and 2 model validation which evaluates the level of agreement between models and observations anderson and bates 2001 flato et al 2014 climate models are comprised of complex systems of equations that include a number of assumptions and approximations that differ from model to model and intermodel comparison is a means of assessing their collective impact e g randall et al 2007 in addition to the obvious need to assess how accurate models are at reproducing the real climate system comparison between models is also essential currently comparison and validation of climate models is generally performed by individual scientists using their own unique codes and workflows the process frequently requires locating accessing transferring regridding and analyzing tens to hundreds of terrabytes of data while some standard community analysis packages do exist e g pmp cesm diagnostics the large number of steps and datasets lead to a situation where results obtained by one researcher cannot be readily reproduced and or evaluated by third parties gleckler et al 2008 kennedy et al 2011 a standard widely available strategy tools libraries for comparison and validation of model results would greatly improve our understanding of model accuracy applicability scope and error sources rood 2011 to standardize the comparison and validation processes thus improving reliability and reproducibility as well as saving considerable time and effort we present a cyberinfrastructure based framework to enable streamlined one stop comparison and validation in climate modeling inspired by state of the art big data manipulation strategies in computer science solutions involving cloud based data storage and high performance on demand computing can be transplanted to scientific modeling zhang 2019 the scripts commands and libraries used for comparison and validation will be managed remotely by cyberinfrastructure once any of the scripts commands or libraries is changed the cyberinfrastructure will automatically re compile and re run the entire workflow sun 2012 the results will be delivered to users once the execution is completed scientists can browse the results and side by side compare them with other model results or on field observations discovered by the cyberinfrastructure the proposed framework will bring a change of routine to climate scientists and will make the numeric models more intercomparable and their results easier to validate against observations a prototype system has been implemented to realize the framework and has been used to compare the results of several popular climate models and data products e g wrf done et al 2004 gfs saha et al 2010 hrrr smith et al 2008 narr mesinger et al 2006 asr bromwich et al 2018 era5 hersbach and dee 2016 via searching and visualizing in the web browser observational data from weather stations radar and sensor networks are indexed discovered and displayed in the prototype to validate the model results and calculate the model errors the results show that the proposed framework can greatly improve the efficiency of comparing and validating those models the expected advantages including reduced time cost increased scientific productivity standardized processes and greater confidence in results have been verified in the experiments for the community at large the proposed cyberinfrastructure will make it easier and faster to deliver model simulations from research to operation for climate related activities such as air quality monitoring water availability and climate change projections 2 urgent challenges in climate modeling 2 1 model intercomparison many models have been created by different groups to simulate the climate system at the time of writing thirty two separate models are listed as participating in the coupled model intercomparison project phase 6 cmip6 1 1 https pcmdi llnl gov cmip6 archivestatistics esgf data holdings eyring et al 2016 cmip6 is a community established framework for studying the output of coupled atmosphere ocean general circulation models it facilitates assessment of the strengths and weaknesses of climate models which can enhance and focus the development of future models zanchettin et al 2016 within cmip6 are multiple subprojects focusing on exploring particular components of the climate system in depth for example the coupled climate carbon cycle model intercomparison project c4mip jones et al 2016 consists of a subset of eleven coupled climate carbon cycle models using a common protocol to study the coupling between climate change and the carbon cycle for each model two simulations were performed in order to isolate the impact of climate change on the land and ocean carbon cycle and therefore the climate feedback on the atmospheric co2 concentration growth rate the results of different models are compared and analyzed to identify the agreement and disagreement among them and to help understand their causes another project the cloud forcing model intercomparison project cfmip focuses on quantifying and comparing the critical cloud radiative feedbacks across the suite of participating models webb et al 2017 taken together hundreds of derived versions of various models exist across the climate community as climate models have become increasingly complex and have moved to higher resolution the need for innovative methods of data discovery as well as intercomparison and analysis of the resulting simulations has grown more urgent 2 2 model validation governments organizations and individuals frequently ask if climate models are reliable hutton et al 2016 rigorous model validation is crucial to demonstrate that simulations are sufficiently accurate to inform decisions on grand challenges on behalf of both human society and our living environment randall et al 2007 in addition to making forecasts models are regularly tested on their ability to reproduce past weather and climate events karmalkar et al 2011 rood 2011 a process referred to as hindcasting hindcasting is particularly important for assessing climate change projections as an alternative to waiting for 30 years to verify the accuracy of a given projection if a model can correctly predict trends from a starting point somewhere in the past there is less reason to think its predictions of future climate states will be wrong pedersen and winther 2005 reanalysis products such as era5 hersbach h 2018 which are blended datasets comprised of both observed and simulated data are frequently used for validating models as well as providing initial conditions for forecasts and boundary conditions for regional models climate models are also assessed on their ability to reproduce the average state of the climate system known as the climatology for example researchers check to see if the average temperature of the earth in winter and summer is similar in the models and reality they also compare quantities such as sea ice extent between models and observations and may choose to use models that do a better job of representing the current amount of sea ice when trying to project future changes similarly specific events that have a large impact on the climate such as volcanic eruptions can also be used to test model performance the climate responds relatively quickly to volcanic eruptions so modelers can see if models accurately capture what happens after big eruptions after waiting only a few years studies show that significant uncertainties remain in model simulations of changes in temperature and in atmospheric water vapor after major volcanic eruptions zanchettin et al 2016 2 3 new technology vs old technology in recent years climate science has struggled with the growing disparity between conventional tools and new emerging techniques sun 2019 neither of which is sufficient alone gimeno 2013 ng et al 2017 the conventional tools are solid robust and have a large number of users craig et al 2005 kouzes et al 2009 lupo and kininmonth 2013 tsipis 2019 however as technology evolves and data accumulates the conventional tools are falling short of addressing new challenges such as higher resolution big data lopez and manogaran 2016 schnase et al 2016 sellars et al 2013 and the inherent limits of numeric models abramson et al 2005 mizielinski et al 2014 new technologies are growing in popularity because of their potential to meet those challenges hashem et al 2015 reed and dongarra 2015 and therefore it is to be hoped that they will gradually replace the dominant role of conventional tools in the climate research evangelinos and hill 2008 haupt et al 2008 huntington et al 2017 kouzes et al 2009 schnase et al 2017 stewart et al 2015 however it will never be simple to convince scientists to drop tools that they have been using for decades and use something new and unfamiliar even when the new technologies are more powerful and efficient fernández quiruelas et al 2011 this conflict follows the birth of all new technologies the chaotic multitude of languages tools and libraries makes the situation worse and leads to a situation where the abundancy of big data petabytes of observational reanalysis simulated datasets online cannot be fully exploited to benefit climate researchers 3 advanced cyberinfrastructure solution web systems have taken on a big portion of the market in scientific data processing today because of their high availability scalability and efficiency cyberinfrastructure which is a term summarizing these web systems and their served capabilities is not a new technology but a merger of tools data and human resources into a seamless interoperable platform while processors storage devices sensors and other physical assets are part of ci it is more than the practice of connecting people with advanced networks and sophisticated applications running on powerful computer systems it aims to involve those people as participants in the generation of knowledge giving them the opportunity to share expertise tools and facilities ci is also known as e research e science and e infrastructure in europe australia and asia bringing together high performance computing remote sensors large data sets middleware and sophisticated applications modeling simulation visualization cyberinfrastructure vision for 21st century discovery beyond the technology an essential part of ci is the way it allows distributed teams to turn flops bytes and bits into scientific breakthroughs cyberinfrastructure vision for 21st century discovery ci provides a technical infrastructure which knits together high speed networks with high performance high availability and high reliability computational resources hey 2005 data can be large aggregations of previously collected data e g reanalysis or live feeds from remote sensors e g satellite observations nexrad network many of the systems are housed in different locations and experiments typically run on virtual machines in which spare cycles from dozens or hundreds of computers are used for a single task data sets can be distributed as well with data coming from multiple institutions schnase 2016 ci provides an opportunity for a new kind of scholarly inquiry and education empowering communities of researchers to innovate and revolutionize what they do how they do it who participates revolutionizing science and engineering through cyberinfrastructure report of the national science foundation blue ribbon advisory panel on cyberinfrastructure data and experiment notebooks are posted online as they are collected facilitating real time distributed science to enhance climate science and the ability to meet the two critical challenges of model intercomparison and validation the required advanced cyberinfrastructure should at least provide the following capabilities 3 1 multisource data discovery and multifile subsetting collection due to the big data challenges in climate science there are only a few choices available for scientists to find their desired data sets and collect them as initial input data of their models currently a common routine is that scientists either search in google to find the data providers descriptive web pages and access the data via their portal entrance link or learn about the data sets from colleagues conferences seminars and go directly to their websites to access the data sets many datasets are too big e g over 100 tb to readily transfer and store locally requiring scientists to subset or downsample the data on site and prior to transferring the reduced data to their own facilities however the current cyberinfrastructure still has a lot of room for improvements e g thredds data server need search function to solve this problem advanced cyberinfrastructure should be equipped in the data centers to further facilitate the manipulation of big data to improve climate research 3 2 efficient regridding all veteran climate modelers know the sophisticated challenges raised by the gridding task hill et al 2004 scientists don t have a limitless supply of computing power at their disposal and so it is necessary for models to divide up the earth into grid cells to make the calculations more manageable this means that in every step of the model takes through time it calculates quantities averaged over a grid cell that may be 100 km on a side however the precise definition of the grid typically varies from model to model the proliferation of models discussed in section 2 1 means that scientists must be able to work with many different grid definitions and direct comparison of models requires first transforming the data to a common grid likewise many observational data products are either supplied on their own grids such as reanalysis or at point locations such as station data and validating models against such data again requires a regridding step automated efficient conversions between different grids would lead to significant savings in time and effort in model analysis workflows 3 3 interactive visualization recent development in web map services such as google maps bing maps apple maps has greatly reformed the habits of people examining spatial datasets when it comes to visualize spatial datasets people generally expect something similar to those widely used map applications which allow users to freely zoom pan rotate search and query on maps rapid and interactive visualization is also a key element in the development of successful climate analysis workflows allowing immediate and flexible feedback on the outcome of experiments and the identification of both errors and features of interest ci tools for visualization should be intuitive and provide steeing 3 or higher dimension mapping as well as many other functions such as on the fly clicking for values changing legends switching projections and adjusting opacity 3 4 result comparison and difference calculation advanced climate cyberinfrastructure should serve the capability of comparing the results from different models and data products and calculate their differences comparing to geographic information systems climate scientists are more comfortable with side by side comparison instead of layers that are overlaid together and compared by showing and hiding each layer cyberinfrastructure development should honor the user habit and make the comparison function friendly enough for veteran scientists to adopt difference calculation should follow the community paradigm to produce the difference map and avoid potential misunderstanding caused by the changes in map production 3 5 hindcast validation hindcasting in climate science or backtesting in many other disciplines is one of the major validation strategies for climate models at present 2 2 https www emc ncep noaa gov users meg fv3gfs sotillo et al 2005 in hindcast experiments researchers test the ability of a model to reproduce the known or estimated state of the climate system e g reanalysis data in the past chawla et al 2013 to facilitate the hindcasting experiment advanced cyberinfrastructure for conveniently retrieving both reanalysis data and observed data and automatically performing standard comparisons including the necessary regridding noted in section 3 2 would be in high demand dealing with the twin challenges of data volume and variety are key challenges in establishing a qualified cyberinfrastructure for hindcasting 3 6 fair data to reproducibility of and confidence in the results of climate models many 3 3 https copdess org enabling fair data project scientific communities and publishers have affirmed their commitment to the principle that scientific data should be findable accessible interoperable and reusable fair given the volume and complexity of climate simulations and observations implementing fair principles represents a significant technical and logistical challenge there should be cyberinfrastructure allowing climate modelers to easily publish their results in fair web services which could interoperate with the models of other components of the earth system and beyond to generate a big picture of environmental current status as well as the future trends 3 7 automation after years of performing challenging and complex data processing tasks yue 2010 climate scientists eagerly expect automation as one of the essential features in advanced cyberinfrastructure sun and di 2019 sun et al 2012 2014 scientists spend hours and hours on a daily basis on preprocessing datasets because one error in data processing could lead to hundreds of computation hours wasted they must recalculate the models again sun et al 2017 a workflow management module should be provided to help scientists oversee the entire process check provenance sun 2013 quickly locate the sources of errors and decrease the resource waste caused by wrong runs of models to the minimum sun 2019 in model comparison and validation once the files on either side of the comparison validation settings are changed the comparison workflow should be triggered by cyberinfrastructure to automatically calculate the difference sun 2016 and the results will be sent to the subscribers of the comparison instantly once the calculation is over 4 prototype implementation to implement such a cyberinfrastructure we utilized dozens of state of art techniques tools libraries and performed substantial software development work and held regular discussions with modelers a layered architecture interoperability framework including data repositories and discovery catalogs models workflow environments and analysis and visualization tools is adopted to manage resources and provided functionalities fig 1 the system is named covali short for intercomparision and validation the user interface in fig 2 which is implemented as a module system of the nsf funded earthcube cyberconnector different from other online web mapping systems covali is customized according to the user habits of climate scientists formed in their long term research careers the interface is divided into two side by side maps each of them has multiple view modes including 2 d and 3 d modes there are two toolbars on the top and left side providing redundant entries for scientists to capabilities such as data discovery retrieval uploading visualization metadata display layer manipulation animation pixel value query difference calculation print etc there is a scaler for each map to indicate the current scale of the corresponding map window a legend panel at the bottom shows the layer name and the color value legend of the top data layer in the map a small container on the top right area shows the coordinates of the mouse pointer to remind the real time location scientists are pointing at at the bottom right corner a projection mode selector is provided for users to switch among various projection and dimension choices covali is a decentralized open source system which can be downloaded from its github repository https www github com csiss cc and installed on most machines including servers clusters cloud virtual machines personal laptops where the climate models and datasets reside the normal usage of covali relies on no third party services after being installed covali allows scientists to browse the data files in public folders on the host servers interactively visualize them on the maps intercompare the simulation of different models side by side and validate the accuracy with observed data products ranging from gridded reanalysis data to individual station measurements of precipitation and temperature the development of the system combines various community standard tools which are very familiar to climate scientists for example the unidata netcdf library java is used to parse the variable metadata and visualize netcdf files in web browser ncwms is used for rendering the netcdf grib files into standard web map service which can be directly integrated by many map tool providers ucar thredds data server is connected to provide the nexad radar datasets recent two weeks ucar research data archive is connected to retrieve the reanalysis data products e g arctic system reanalysis nco netcdf operator command lines are integrated to manipulate and analyze netcdf data files with many powerful mathematical and statistical algorithms difference calculation ensemble subsetting average and etc openlayers a popular open source javascript library is used as the api for building the two rich web based maps which can be panned zoomed rotated and overlaid cesiumjs is utilized to provide 3 d virtual globe view mode for visualizing both static and dynamic data 5 experiment showcase we have helped climate scientists use covali in comparing and validating their model results they exercised covali to compare a series of climate model results some of which are introduced below however it is important to note that while the comparisons choose the closest reanalysis products and the dates might not be strictly the same these comparison results aim to demonstrate the capabilities of the proposed framework only 5 1 gfs vs hrrr the global forecasting system gfs 4 4 https www emc ncep noaa gov gfs php is an operational weather forecast model developed by the national centers for environmental prediction ncep whitaker et al 2008 gfs datasets consist of dozens of variables such as temperature wind precipitation and ozone concentration and are used by the forecasters to predict weather up to 16 days in advance hrrr 5 5 https rapidrefresh noaa gov hrrr short for the high resolution rapid refresh is a noaa real time 3 km resolution hourly updated cloud resolving convection allowing atmospheric model radar data is assimilated in the hrrr every 15 min over a 1 h period adding further detail to that provided by the hourly data assimilation from the 13 km radar enhanced rapid refresh we compared their convective available potential energy cape a measure of the energy present in the atmosphere to fuel the development of storms for conus at the same time period apr 22 2019 00 00 shown in fig 3 the two maps use the same legend maximum minimum color style base map scale and etc and can be zoomed to specific regions for detail comparison in covali the differences of the two models are very obvious in the central plain region and across the southeast the impact of the higher resolution in hrrr is very apparent in the more sharply defined nature of the main feature conveniently covali allows users to rapidly and easily adjust the focus region to compare results at various scales 5 2 merra2 vs narr the ncep north american regional reanalysis narr mesinger et al 2006 products are long term 1979 present atmospheric datasets with 3 h temporal 32 km horizontal and 45 layer vertical resolutions over the north american domain narr contains outputs of many atmospheric variables and fluxes and is nicely suited for diagnosis of synoptic and mesoscale conditions in this portion of the study we use narr monthly means and the nasa modern era retrospective analysis for research and applications version 2 merra 2 monthly data to compare their assessments of the air temperatures at 2 m above the surface t2m fig 4 a the results above the coastal states are relatively close however further in land states particularly in the region of the appalachian mountains the two datasets have clear disagreements these differences are difficult to see however using the default palette and value range so to highlight these differences we use covali to adjust the value range of the color legend fig 4b now the broader area of colder temperatures near the mountains in merra2 relative to narr can be idenfied clearly 5 3 asr vs era5 reanalysis asr arctic system reanalysis is a retrospective reanalysis of the great arctic region produced by high resolution versions of the polar weather forecast model pwrf and the wrf var and high resolution land data assimilation hrldas data assimilation systems that have been optimized for the arctic it covers the period of 2000 2016 at two resolutions 15 km and 30 km it has 29 pressure levels 27 surface and 10 upper air analysis variables 74 surface and 16 upper air forecast variables and 3 soil variables the era5 dataset is the fifth generation of ecmwf european centre for medium range weather forecast reanalysis of the global climate which started with the fgge reanalyses produced in the 1980s followed by era 15 era 40 and more recently era interim era5 provides hourly estimates of a large number of atmospheric land and oceanic climate variables and currently covers the period 1979 to within 3 months of real time it combines vast amounts of historical observations in its estimation specially era5 includes information about its uncertainties for all the variables at reduced spatial and temporal resolutions for quality assurance the hourly analysis and twice daily forecast parameters form the basis of the monthly means and monthly diurnal means in the dataset we used covali to load their estimates of the albedo in the greater arctic fig 5 shows the results with asr on the left map and era5 on the right the general distribution of high and low areas matches well especially in the arctic ocean canada russia and alaska differences are localized and driven by reasonable forcings via the comparison it could be concluded that both asr and era5 produced consistent estimates in the arctic more improvements on each model might be made at local scale and further validated by the future datasets collected by multidisciplinary drifting observatory for the study of arctic climate mosaic 5 4 model validation with ground station measurements covali not only provides a platform for interactively visualizing the model results but also allows directly overlaying observations collected by ground stations earthcube chords cloud hosted real time data services for the geosciences is a real time data service infrastructure that will provide an easy to use system to acquire navigate and distribute real time data streams from 3 d printed self deployed in situ sensors the chords group has deployed several sensors in colorado which are keeping streaming the observations to their cloud hosted databases using the chords api covali can real time retrieve and display those observations on the maps fig 6 we compared the precipitation in gfs hindcast predictions with the accurate observation from chords by clicking on the maps scientists can check their results against the real observations to evaluate the accuracy of their models and to help in identifying sources of errors besides chords covali can also directly load observations from iris station network although iris network is specifically used for seismology research they also have some surface sensors which provide near surface measurements of variables like radiation temperature and etc and can help climate models to evaluate their results at near ground surface level fig 7 shows the distribution of some stations in the iris network scientists can click on the station icons to get the station name the station network channels and the measurements of each channel in future more ground or remotely sensed datasets will be made available to scientists in covali to facilitate directly adjusting their results against the real world datasets 5 5 data processing workflow to assist covali to make model results more comparable geoweaver an open source geoscientific workflow system is used in combination with covali to assist the creation and running of data processing workflow to automate the tedious daily jobs like regridding averaging reformatting reprojecting and etc fig 8 covali uses geoweaver in the backend to complete the data processing tasks received through its front interface geoweaver is very flexible for managing resources like servers cloud virtual machines clusters scripts notebooks python code and workflows underneath the geoweaver processes are the community standard tools such as nco grads ncl thredds data server and etc which provide the actual algorithms and commands to operate on the data files geoweaver wraps all the resources into automated workflows and greatly reduces the time cost of scientists spent on the frequently repeated data processing per day 6 discussion one biggest advantage of covali is to make climate model results and observed datasets fairable findable accessible interoperable reusable via a single platform model results can be intercompared and validated in an interactive and uniform interface instead of static and random images since it runs in web browsers covali allows scientists to bypass many layers of protocols and connection techniques to directly manipulate the model results regarding the volume challenge of big data in climate science covali provides an advanced cyberinfrastructure solution to meet the substantial problems in data retrieval visualization pre post processing statistics intercomparison and validation the availability of covali could make these daily activities of climate scientists easy straightforward automatic and time efficient it can help improve climate modeling by providing objective comparison among various models and hindcasts and also enhance integrated modeling by allowing the model results to be accessible downloadable and reusable by other modelers the simple integration of ground station observations will facilitate validation efforts that answer the reliability questions asked by all the stakeholders in addition the tedious data processing tasks which take a big portion of work time of climate scientists will be simplified and automated in covali using scientific workflow techniques the software can run on windows mac os x and unix linux and others that can run java virtual machine every scientist can install a copy on their machines to take advantage of its capabilities the major barrier preventing the usage of cyberinfrastructure like covali in real world scenarios is the long term user habits and software practicality it takes a while for people not just scientists to transfer from their familiar tools to something brand new we are aware of this obstacle and have taken it into consideration since the beginning of the project the interface of covali is made into a similar layout and style as the ones that climate scientists are familiar with during the development we have periodically interacted with scientists to discuss the design of user interfaces and functionalities to eliminate resistance in their minds currently covali is at the prototype phase and we will follow the best practices of standard software engineering cycle to develop it into an operational system which is robust and useable by the large climate community 7 conclusion and future work we proposed an advanced cyberinfrastructure framework to enable intercomparison and validation of climate models and observed datasets via web based systems the proposed method will bring a change in the research routine of climate scientists and make the numeric models more intercomparable and the validation of results with ground truth data more efficient a prototype system has been implemented using java to realize the framework and has been used to compare the results of widely used climate data sets wrf gfs hrrr narr asr era5 and etc we worked together with specialists of climate models and help them to use the developed system to compare their results with other existing products via searching and visualizing in the web browser validation data from weather stationary datasets radar observation dataset sensor networks are indexed discovered and displayed in the prototype to validate the model results and calculate the model errors the results show that the proposed framework for advanced cyberinfrastructure can greatly improve the automation and efficiency of intercomparing and validating numerical models in future we will make more ground and remotely sensed datasets available in covali and provide more data processing and analysis functions by reaching out to the climate community we will solicit engagement of users and developers of different models to compare their results via covali the short term benefits of adopting the cyberinfrastructure in the climate domain include reduced time cost standardized process confident results and improved reproducibility ultimately covali will automate the model intercomparison and validation for improving the model simulation to support the requirements of operational decision making in atmosphere related activities such as air quality monitoring greenhouse effect prediction ozone monitoring dust forecasting atmosphere thickness monitoring air pollutant control etc acknowledgment this study is supported by a grant from national science foundation earthcube program grant ags 1740693 pi dr liping di the authors of this paper would also like to thank all the authors and copyright holders of the open source software used in this work and the experts of tested climate models dr sheng hung wang dr david bromwich dr jim kinter and dr daniel tong for their support and earth science information partnership federation esip for their support on geoweaver 
26098,the current routine of comparison and validation in climate science is frequently static and of low efficiency which hinders evidence based decision making and scientific confidence due to the aggressively increasing resolution complexity and associated data volumes of climate models objectively comparing multiple models and assessing their accuracy against observations is an ever increasing challenge we propose an integrated framework for harmonizing state of the art cyberinfrastructure techniques with the user habits formed by long term familiarity with existing community oriented software an open source prototype named covali is implemented and used to compare and validate the results of several widely used climate models and datasets our results show that the proposed cyberinfrastructure based strategy can significantly automate the comparison and validation processes in climate modeling more importantly the new strategy retains the existing user habits in the climate community while making it easier for scientists to adopt new technology in their research routine keywords model comparison model validation climate science web service cyberinfrastructure big data software availability name of software covali developer center for spatial information science and systems george mason university source language java availability the source code and application jar can be accessed via github http github com csiss cc 1 introduction big data features heavily in two essential and constantly applied steps in climate modeling workflows 1 model comparison which evaluates the level of agreement between models and 2 model validation which evaluates the level of agreement between models and observations anderson and bates 2001 flato et al 2014 climate models are comprised of complex systems of equations that include a number of assumptions and approximations that differ from model to model and intermodel comparison is a means of assessing their collective impact e g randall et al 2007 in addition to the obvious need to assess how accurate models are at reproducing the real climate system comparison between models is also essential currently comparison and validation of climate models is generally performed by individual scientists using their own unique codes and workflows the process frequently requires locating accessing transferring regridding and analyzing tens to hundreds of terrabytes of data while some standard community analysis packages do exist e g pmp cesm diagnostics the large number of steps and datasets lead to a situation where results obtained by one researcher cannot be readily reproduced and or evaluated by third parties gleckler et al 2008 kennedy et al 2011 a standard widely available strategy tools libraries for comparison and validation of model results would greatly improve our understanding of model accuracy applicability scope and error sources rood 2011 to standardize the comparison and validation processes thus improving reliability and reproducibility as well as saving considerable time and effort we present a cyberinfrastructure based framework to enable streamlined one stop comparison and validation in climate modeling inspired by state of the art big data manipulation strategies in computer science solutions involving cloud based data storage and high performance on demand computing can be transplanted to scientific modeling zhang 2019 the scripts commands and libraries used for comparison and validation will be managed remotely by cyberinfrastructure once any of the scripts commands or libraries is changed the cyberinfrastructure will automatically re compile and re run the entire workflow sun 2012 the results will be delivered to users once the execution is completed scientists can browse the results and side by side compare them with other model results or on field observations discovered by the cyberinfrastructure the proposed framework will bring a change of routine to climate scientists and will make the numeric models more intercomparable and their results easier to validate against observations a prototype system has been implemented to realize the framework and has been used to compare the results of several popular climate models and data products e g wrf done et al 2004 gfs saha et al 2010 hrrr smith et al 2008 narr mesinger et al 2006 asr bromwich et al 2018 era5 hersbach and dee 2016 via searching and visualizing in the web browser observational data from weather stations radar and sensor networks are indexed discovered and displayed in the prototype to validate the model results and calculate the model errors the results show that the proposed framework can greatly improve the efficiency of comparing and validating those models the expected advantages including reduced time cost increased scientific productivity standardized processes and greater confidence in results have been verified in the experiments for the community at large the proposed cyberinfrastructure will make it easier and faster to deliver model simulations from research to operation for climate related activities such as air quality monitoring water availability and climate change projections 2 urgent challenges in climate modeling 2 1 model intercomparison many models have been created by different groups to simulate the climate system at the time of writing thirty two separate models are listed as participating in the coupled model intercomparison project phase 6 cmip6 1 1 https pcmdi llnl gov cmip6 archivestatistics esgf data holdings eyring et al 2016 cmip6 is a community established framework for studying the output of coupled atmosphere ocean general circulation models it facilitates assessment of the strengths and weaknesses of climate models which can enhance and focus the development of future models zanchettin et al 2016 within cmip6 are multiple subprojects focusing on exploring particular components of the climate system in depth for example the coupled climate carbon cycle model intercomparison project c4mip jones et al 2016 consists of a subset of eleven coupled climate carbon cycle models using a common protocol to study the coupling between climate change and the carbon cycle for each model two simulations were performed in order to isolate the impact of climate change on the land and ocean carbon cycle and therefore the climate feedback on the atmospheric co2 concentration growth rate the results of different models are compared and analyzed to identify the agreement and disagreement among them and to help understand their causes another project the cloud forcing model intercomparison project cfmip focuses on quantifying and comparing the critical cloud radiative feedbacks across the suite of participating models webb et al 2017 taken together hundreds of derived versions of various models exist across the climate community as climate models have become increasingly complex and have moved to higher resolution the need for innovative methods of data discovery as well as intercomparison and analysis of the resulting simulations has grown more urgent 2 2 model validation governments organizations and individuals frequently ask if climate models are reliable hutton et al 2016 rigorous model validation is crucial to demonstrate that simulations are sufficiently accurate to inform decisions on grand challenges on behalf of both human society and our living environment randall et al 2007 in addition to making forecasts models are regularly tested on their ability to reproduce past weather and climate events karmalkar et al 2011 rood 2011 a process referred to as hindcasting hindcasting is particularly important for assessing climate change projections as an alternative to waiting for 30 years to verify the accuracy of a given projection if a model can correctly predict trends from a starting point somewhere in the past there is less reason to think its predictions of future climate states will be wrong pedersen and winther 2005 reanalysis products such as era5 hersbach h 2018 which are blended datasets comprised of both observed and simulated data are frequently used for validating models as well as providing initial conditions for forecasts and boundary conditions for regional models climate models are also assessed on their ability to reproduce the average state of the climate system known as the climatology for example researchers check to see if the average temperature of the earth in winter and summer is similar in the models and reality they also compare quantities such as sea ice extent between models and observations and may choose to use models that do a better job of representing the current amount of sea ice when trying to project future changes similarly specific events that have a large impact on the climate such as volcanic eruptions can also be used to test model performance the climate responds relatively quickly to volcanic eruptions so modelers can see if models accurately capture what happens after big eruptions after waiting only a few years studies show that significant uncertainties remain in model simulations of changes in temperature and in atmospheric water vapor after major volcanic eruptions zanchettin et al 2016 2 3 new technology vs old technology in recent years climate science has struggled with the growing disparity between conventional tools and new emerging techniques sun 2019 neither of which is sufficient alone gimeno 2013 ng et al 2017 the conventional tools are solid robust and have a large number of users craig et al 2005 kouzes et al 2009 lupo and kininmonth 2013 tsipis 2019 however as technology evolves and data accumulates the conventional tools are falling short of addressing new challenges such as higher resolution big data lopez and manogaran 2016 schnase et al 2016 sellars et al 2013 and the inherent limits of numeric models abramson et al 2005 mizielinski et al 2014 new technologies are growing in popularity because of their potential to meet those challenges hashem et al 2015 reed and dongarra 2015 and therefore it is to be hoped that they will gradually replace the dominant role of conventional tools in the climate research evangelinos and hill 2008 haupt et al 2008 huntington et al 2017 kouzes et al 2009 schnase et al 2017 stewart et al 2015 however it will never be simple to convince scientists to drop tools that they have been using for decades and use something new and unfamiliar even when the new technologies are more powerful and efficient fernández quiruelas et al 2011 this conflict follows the birth of all new technologies the chaotic multitude of languages tools and libraries makes the situation worse and leads to a situation where the abundancy of big data petabytes of observational reanalysis simulated datasets online cannot be fully exploited to benefit climate researchers 3 advanced cyberinfrastructure solution web systems have taken on a big portion of the market in scientific data processing today because of their high availability scalability and efficiency cyberinfrastructure which is a term summarizing these web systems and their served capabilities is not a new technology but a merger of tools data and human resources into a seamless interoperable platform while processors storage devices sensors and other physical assets are part of ci it is more than the practice of connecting people with advanced networks and sophisticated applications running on powerful computer systems it aims to involve those people as participants in the generation of knowledge giving them the opportunity to share expertise tools and facilities ci is also known as e research e science and e infrastructure in europe australia and asia bringing together high performance computing remote sensors large data sets middleware and sophisticated applications modeling simulation visualization cyberinfrastructure vision for 21st century discovery beyond the technology an essential part of ci is the way it allows distributed teams to turn flops bytes and bits into scientific breakthroughs cyberinfrastructure vision for 21st century discovery ci provides a technical infrastructure which knits together high speed networks with high performance high availability and high reliability computational resources hey 2005 data can be large aggregations of previously collected data e g reanalysis or live feeds from remote sensors e g satellite observations nexrad network many of the systems are housed in different locations and experiments typically run on virtual machines in which spare cycles from dozens or hundreds of computers are used for a single task data sets can be distributed as well with data coming from multiple institutions schnase 2016 ci provides an opportunity for a new kind of scholarly inquiry and education empowering communities of researchers to innovate and revolutionize what they do how they do it who participates revolutionizing science and engineering through cyberinfrastructure report of the national science foundation blue ribbon advisory panel on cyberinfrastructure data and experiment notebooks are posted online as they are collected facilitating real time distributed science to enhance climate science and the ability to meet the two critical challenges of model intercomparison and validation the required advanced cyberinfrastructure should at least provide the following capabilities 3 1 multisource data discovery and multifile subsetting collection due to the big data challenges in climate science there are only a few choices available for scientists to find their desired data sets and collect them as initial input data of their models currently a common routine is that scientists either search in google to find the data providers descriptive web pages and access the data via their portal entrance link or learn about the data sets from colleagues conferences seminars and go directly to their websites to access the data sets many datasets are too big e g over 100 tb to readily transfer and store locally requiring scientists to subset or downsample the data on site and prior to transferring the reduced data to their own facilities however the current cyberinfrastructure still has a lot of room for improvements e g thredds data server need search function to solve this problem advanced cyberinfrastructure should be equipped in the data centers to further facilitate the manipulation of big data to improve climate research 3 2 efficient regridding all veteran climate modelers know the sophisticated challenges raised by the gridding task hill et al 2004 scientists don t have a limitless supply of computing power at their disposal and so it is necessary for models to divide up the earth into grid cells to make the calculations more manageable this means that in every step of the model takes through time it calculates quantities averaged over a grid cell that may be 100 km on a side however the precise definition of the grid typically varies from model to model the proliferation of models discussed in section 2 1 means that scientists must be able to work with many different grid definitions and direct comparison of models requires first transforming the data to a common grid likewise many observational data products are either supplied on their own grids such as reanalysis or at point locations such as station data and validating models against such data again requires a regridding step automated efficient conversions between different grids would lead to significant savings in time and effort in model analysis workflows 3 3 interactive visualization recent development in web map services such as google maps bing maps apple maps has greatly reformed the habits of people examining spatial datasets when it comes to visualize spatial datasets people generally expect something similar to those widely used map applications which allow users to freely zoom pan rotate search and query on maps rapid and interactive visualization is also a key element in the development of successful climate analysis workflows allowing immediate and flexible feedback on the outcome of experiments and the identification of both errors and features of interest ci tools for visualization should be intuitive and provide steeing 3 or higher dimension mapping as well as many other functions such as on the fly clicking for values changing legends switching projections and adjusting opacity 3 4 result comparison and difference calculation advanced climate cyberinfrastructure should serve the capability of comparing the results from different models and data products and calculate their differences comparing to geographic information systems climate scientists are more comfortable with side by side comparison instead of layers that are overlaid together and compared by showing and hiding each layer cyberinfrastructure development should honor the user habit and make the comparison function friendly enough for veteran scientists to adopt difference calculation should follow the community paradigm to produce the difference map and avoid potential misunderstanding caused by the changes in map production 3 5 hindcast validation hindcasting in climate science or backtesting in many other disciplines is one of the major validation strategies for climate models at present 2 2 https www emc ncep noaa gov users meg fv3gfs sotillo et al 2005 in hindcast experiments researchers test the ability of a model to reproduce the known or estimated state of the climate system e g reanalysis data in the past chawla et al 2013 to facilitate the hindcasting experiment advanced cyberinfrastructure for conveniently retrieving both reanalysis data and observed data and automatically performing standard comparisons including the necessary regridding noted in section 3 2 would be in high demand dealing with the twin challenges of data volume and variety are key challenges in establishing a qualified cyberinfrastructure for hindcasting 3 6 fair data to reproducibility of and confidence in the results of climate models many 3 3 https copdess org enabling fair data project scientific communities and publishers have affirmed their commitment to the principle that scientific data should be findable accessible interoperable and reusable fair given the volume and complexity of climate simulations and observations implementing fair principles represents a significant technical and logistical challenge there should be cyberinfrastructure allowing climate modelers to easily publish their results in fair web services which could interoperate with the models of other components of the earth system and beyond to generate a big picture of environmental current status as well as the future trends 3 7 automation after years of performing challenging and complex data processing tasks yue 2010 climate scientists eagerly expect automation as one of the essential features in advanced cyberinfrastructure sun and di 2019 sun et al 2012 2014 scientists spend hours and hours on a daily basis on preprocessing datasets because one error in data processing could lead to hundreds of computation hours wasted they must recalculate the models again sun et al 2017 a workflow management module should be provided to help scientists oversee the entire process check provenance sun 2013 quickly locate the sources of errors and decrease the resource waste caused by wrong runs of models to the minimum sun 2019 in model comparison and validation once the files on either side of the comparison validation settings are changed the comparison workflow should be triggered by cyberinfrastructure to automatically calculate the difference sun 2016 and the results will be sent to the subscribers of the comparison instantly once the calculation is over 4 prototype implementation to implement such a cyberinfrastructure we utilized dozens of state of art techniques tools libraries and performed substantial software development work and held regular discussions with modelers a layered architecture interoperability framework including data repositories and discovery catalogs models workflow environments and analysis and visualization tools is adopted to manage resources and provided functionalities fig 1 the system is named covali short for intercomparision and validation the user interface in fig 2 which is implemented as a module system of the nsf funded earthcube cyberconnector different from other online web mapping systems covali is customized according to the user habits of climate scientists formed in their long term research careers the interface is divided into two side by side maps each of them has multiple view modes including 2 d and 3 d modes there are two toolbars on the top and left side providing redundant entries for scientists to capabilities such as data discovery retrieval uploading visualization metadata display layer manipulation animation pixel value query difference calculation print etc there is a scaler for each map to indicate the current scale of the corresponding map window a legend panel at the bottom shows the layer name and the color value legend of the top data layer in the map a small container on the top right area shows the coordinates of the mouse pointer to remind the real time location scientists are pointing at at the bottom right corner a projection mode selector is provided for users to switch among various projection and dimension choices covali is a decentralized open source system which can be downloaded from its github repository https www github com csiss cc and installed on most machines including servers clusters cloud virtual machines personal laptops where the climate models and datasets reside the normal usage of covali relies on no third party services after being installed covali allows scientists to browse the data files in public folders on the host servers interactively visualize them on the maps intercompare the simulation of different models side by side and validate the accuracy with observed data products ranging from gridded reanalysis data to individual station measurements of precipitation and temperature the development of the system combines various community standard tools which are very familiar to climate scientists for example the unidata netcdf library java is used to parse the variable metadata and visualize netcdf files in web browser ncwms is used for rendering the netcdf grib files into standard web map service which can be directly integrated by many map tool providers ucar thredds data server is connected to provide the nexad radar datasets recent two weeks ucar research data archive is connected to retrieve the reanalysis data products e g arctic system reanalysis nco netcdf operator command lines are integrated to manipulate and analyze netcdf data files with many powerful mathematical and statistical algorithms difference calculation ensemble subsetting average and etc openlayers a popular open source javascript library is used as the api for building the two rich web based maps which can be panned zoomed rotated and overlaid cesiumjs is utilized to provide 3 d virtual globe view mode for visualizing both static and dynamic data 5 experiment showcase we have helped climate scientists use covali in comparing and validating their model results they exercised covali to compare a series of climate model results some of which are introduced below however it is important to note that while the comparisons choose the closest reanalysis products and the dates might not be strictly the same these comparison results aim to demonstrate the capabilities of the proposed framework only 5 1 gfs vs hrrr the global forecasting system gfs 4 4 https www emc ncep noaa gov gfs php is an operational weather forecast model developed by the national centers for environmental prediction ncep whitaker et al 2008 gfs datasets consist of dozens of variables such as temperature wind precipitation and ozone concentration and are used by the forecasters to predict weather up to 16 days in advance hrrr 5 5 https rapidrefresh noaa gov hrrr short for the high resolution rapid refresh is a noaa real time 3 km resolution hourly updated cloud resolving convection allowing atmospheric model radar data is assimilated in the hrrr every 15 min over a 1 h period adding further detail to that provided by the hourly data assimilation from the 13 km radar enhanced rapid refresh we compared their convective available potential energy cape a measure of the energy present in the atmosphere to fuel the development of storms for conus at the same time period apr 22 2019 00 00 shown in fig 3 the two maps use the same legend maximum minimum color style base map scale and etc and can be zoomed to specific regions for detail comparison in covali the differences of the two models are very obvious in the central plain region and across the southeast the impact of the higher resolution in hrrr is very apparent in the more sharply defined nature of the main feature conveniently covali allows users to rapidly and easily adjust the focus region to compare results at various scales 5 2 merra2 vs narr the ncep north american regional reanalysis narr mesinger et al 2006 products are long term 1979 present atmospheric datasets with 3 h temporal 32 km horizontal and 45 layer vertical resolutions over the north american domain narr contains outputs of many atmospheric variables and fluxes and is nicely suited for diagnosis of synoptic and mesoscale conditions in this portion of the study we use narr monthly means and the nasa modern era retrospective analysis for research and applications version 2 merra 2 monthly data to compare their assessments of the air temperatures at 2 m above the surface t2m fig 4 a the results above the coastal states are relatively close however further in land states particularly in the region of the appalachian mountains the two datasets have clear disagreements these differences are difficult to see however using the default palette and value range so to highlight these differences we use covali to adjust the value range of the color legend fig 4b now the broader area of colder temperatures near the mountains in merra2 relative to narr can be idenfied clearly 5 3 asr vs era5 reanalysis asr arctic system reanalysis is a retrospective reanalysis of the great arctic region produced by high resolution versions of the polar weather forecast model pwrf and the wrf var and high resolution land data assimilation hrldas data assimilation systems that have been optimized for the arctic it covers the period of 2000 2016 at two resolutions 15 km and 30 km it has 29 pressure levels 27 surface and 10 upper air analysis variables 74 surface and 16 upper air forecast variables and 3 soil variables the era5 dataset is the fifth generation of ecmwf european centre for medium range weather forecast reanalysis of the global climate which started with the fgge reanalyses produced in the 1980s followed by era 15 era 40 and more recently era interim era5 provides hourly estimates of a large number of atmospheric land and oceanic climate variables and currently covers the period 1979 to within 3 months of real time it combines vast amounts of historical observations in its estimation specially era5 includes information about its uncertainties for all the variables at reduced spatial and temporal resolutions for quality assurance the hourly analysis and twice daily forecast parameters form the basis of the monthly means and monthly diurnal means in the dataset we used covali to load their estimates of the albedo in the greater arctic fig 5 shows the results with asr on the left map and era5 on the right the general distribution of high and low areas matches well especially in the arctic ocean canada russia and alaska differences are localized and driven by reasonable forcings via the comparison it could be concluded that both asr and era5 produced consistent estimates in the arctic more improvements on each model might be made at local scale and further validated by the future datasets collected by multidisciplinary drifting observatory for the study of arctic climate mosaic 5 4 model validation with ground station measurements covali not only provides a platform for interactively visualizing the model results but also allows directly overlaying observations collected by ground stations earthcube chords cloud hosted real time data services for the geosciences is a real time data service infrastructure that will provide an easy to use system to acquire navigate and distribute real time data streams from 3 d printed self deployed in situ sensors the chords group has deployed several sensors in colorado which are keeping streaming the observations to their cloud hosted databases using the chords api covali can real time retrieve and display those observations on the maps fig 6 we compared the precipitation in gfs hindcast predictions with the accurate observation from chords by clicking on the maps scientists can check their results against the real observations to evaluate the accuracy of their models and to help in identifying sources of errors besides chords covali can also directly load observations from iris station network although iris network is specifically used for seismology research they also have some surface sensors which provide near surface measurements of variables like radiation temperature and etc and can help climate models to evaluate their results at near ground surface level fig 7 shows the distribution of some stations in the iris network scientists can click on the station icons to get the station name the station network channels and the measurements of each channel in future more ground or remotely sensed datasets will be made available to scientists in covali to facilitate directly adjusting their results against the real world datasets 5 5 data processing workflow to assist covali to make model results more comparable geoweaver an open source geoscientific workflow system is used in combination with covali to assist the creation and running of data processing workflow to automate the tedious daily jobs like regridding averaging reformatting reprojecting and etc fig 8 covali uses geoweaver in the backend to complete the data processing tasks received through its front interface geoweaver is very flexible for managing resources like servers cloud virtual machines clusters scripts notebooks python code and workflows underneath the geoweaver processes are the community standard tools such as nco grads ncl thredds data server and etc which provide the actual algorithms and commands to operate on the data files geoweaver wraps all the resources into automated workflows and greatly reduces the time cost of scientists spent on the frequently repeated data processing per day 6 discussion one biggest advantage of covali is to make climate model results and observed datasets fairable findable accessible interoperable reusable via a single platform model results can be intercompared and validated in an interactive and uniform interface instead of static and random images since it runs in web browsers covali allows scientists to bypass many layers of protocols and connection techniques to directly manipulate the model results regarding the volume challenge of big data in climate science covali provides an advanced cyberinfrastructure solution to meet the substantial problems in data retrieval visualization pre post processing statistics intercomparison and validation the availability of covali could make these daily activities of climate scientists easy straightforward automatic and time efficient it can help improve climate modeling by providing objective comparison among various models and hindcasts and also enhance integrated modeling by allowing the model results to be accessible downloadable and reusable by other modelers the simple integration of ground station observations will facilitate validation efforts that answer the reliability questions asked by all the stakeholders in addition the tedious data processing tasks which take a big portion of work time of climate scientists will be simplified and automated in covali using scientific workflow techniques the software can run on windows mac os x and unix linux and others that can run java virtual machine every scientist can install a copy on their machines to take advantage of its capabilities the major barrier preventing the usage of cyberinfrastructure like covali in real world scenarios is the long term user habits and software practicality it takes a while for people not just scientists to transfer from their familiar tools to something brand new we are aware of this obstacle and have taken it into consideration since the beginning of the project the interface of covali is made into a similar layout and style as the ones that climate scientists are familiar with during the development we have periodically interacted with scientists to discuss the design of user interfaces and functionalities to eliminate resistance in their minds currently covali is at the prototype phase and we will follow the best practices of standard software engineering cycle to develop it into an operational system which is robust and useable by the large climate community 7 conclusion and future work we proposed an advanced cyberinfrastructure framework to enable intercomparison and validation of climate models and observed datasets via web based systems the proposed method will bring a change in the research routine of climate scientists and make the numeric models more intercomparable and the validation of results with ground truth data more efficient a prototype system has been implemented using java to realize the framework and has been used to compare the results of widely used climate data sets wrf gfs hrrr narr asr era5 and etc we worked together with specialists of climate models and help them to use the developed system to compare their results with other existing products via searching and visualizing in the web browser validation data from weather stationary datasets radar observation dataset sensor networks are indexed discovered and displayed in the prototype to validate the model results and calculate the model errors the results show that the proposed framework for advanced cyberinfrastructure can greatly improve the automation and efficiency of intercomparing and validating numerical models in future we will make more ground and remotely sensed datasets available in covali and provide more data processing and analysis functions by reaching out to the climate community we will solicit engagement of users and developers of different models to compare their results via covali the short term benefits of adopting the cyberinfrastructure in the climate domain include reduced time cost standardized process confident results and improved reproducibility ultimately covali will automate the model intercomparison and validation for improving the model simulation to support the requirements of operational decision making in atmosphere related activities such as air quality monitoring greenhouse effect prediction ozone monitoring dust forecasting atmosphere thickness monitoring air pollutant control etc acknowledgment this study is supported by a grant from national science foundation earthcube program grant ags 1740693 pi dr liping di the authors of this paper would also like to thank all the authors and copyright holders of the open source software used in this work and the experts of tested climate models dr sheng hung wang dr david bromwich dr jim kinter and dr daniel tong for their support and earth science information partnership federation esip for their support on geoweaver 
26099,this study presents r landslide a free and open source add on to the open source geographic information system gis grass software for landslide susceptibility mapping the tool was written in python language and works on the top of an artificial neural network ann fed with environmental parameters and landslide databases in order to illustrate the application and effectiveness of the developed tool a case study is presented for the municipality of porto alegre brazil the resulting landslide susceptibility maps are compared with the map published by the brazilian geological survey cprm and a direct comparison using unseen new landslide records indicate that the r landslide can identify and pinpoint susceptible areas with better accuracy the module can be used by natural disaster management bodies and land use planning organs as a support tool for the elaboration of landslide susceptibility maps in an agile and efficient manner keywords natural disasters landslide susceptibility assessment gis availability program name r landslide developer l bragagnolo r v da silva j m v grzybowski contact address lucimarabragagnolo hotmail com year first available 2019 program language python 2 7 package size 70 kb availability https github com uffsenvmodelling r landslide cost free of charge 1 introduction landslides are common hazards which can result in substantial economic losses casualties and adverse impacts on infrastructure the environment and communities severely affecting urban development and land use dai et al 2002 lee and choi 2004 zhu et al 2018 flentje and chowdhury 2018 shahabi and hashim 2015 as a result of increasing population density deforestation and uncontrolled urbanization activities the risks associated with landslides have increased over time flentje and chowdhury 2018 froude and petley 2018 in this context the mapping of landslide susceptibility plays an important role in the prospective planning of land occupation in order to avoid the occupation of susceptible locations schwab et al 2005 thus the accurate mapping of susceptible areas is key information for a wide range of users in the public and private sectors government departments and the scientific community at the local and international levels fell et al 2008 petschko et al 2014 a common difficulty in developing countries is the lack of technical infrastructure such that neither the population nor the government authorities are sufficiently aware of landslide hazards hervás 2003 petschko et al 2014 this can be broadly illustrated by the number of recent and deadly landslide events whose consequences could have been considerably mitigated if landslide susceptibility maps were available to substantiate decision making on land occupation policies daily mail 2018c b a bbc news 2018a b regarding the situation in brazil according to soriano et al 2017 many municipal civil defense agencies still have difficulties in developing practical proposals for consistent identification of risks control of natural disasters and mitigation of their consequences among the causes one can cite technical limitations of personnel and the institutional structure of civil defense agencies which are still not well consolidated in regards to their functions and prerogatives on the integration of artificial intelligence and geographic information systems gis methods that use gis and remote sensing yield a landslide susceptibility assessment that is more accurate than those using previous approaches chen et al 2018a besides the increasing availability of datasets with considerable spatial resolution and computers with fast processing capacity made it possible to automate the process of acquiring and pre processing high resolution images among the existing open source gis softwares the geographic resources analysis support system grass grass development team 2017 has received a considerable investment of time and effort by the developing community neteler et al 2012 up to date there are no modules available that integrate the ann with the grass for landslide susceptibility mapping further although some of the cited modules work with the identification of slide probabilities and slope stability analysis they often require a series of specific input parameters which in turn require massive field work and detailed surveys of the study areas as mentioned beforehand the severe lack of resources makes it difficult that such methodologies be implemented in developing countries with lacking infrastructure for worse in certain cases such parameters are simply generalized or estimated from empirical or oversimplified equations that not only limit the validity of the resulting maps but can inadvertently mask dangerous situations by not recognizing them appropriately or even by giving flawed grounds for decision making this article presents the r landslide module for grass gis software the module implements anns for landslide susceptibility mapping on the basis of available environmental geological and geophysical data obtained from satellite imagery and georeferenced landslide databases the module was developed in the python language following the open source ideal of the grass software the main contribution of this research is to deliver an open source and free module that bundles anns and a user friendly interface in the gis environment to deliver an intuitive end to end support tool for landslide susceptibility assessment for application of the module to a given study region the ann must be previously trained validated and tested using landslide data from such region thus the types of landslides that the trained ann model will be able to identify with accuracy will be those types that are present in corresponding training dataset in the form of landslide records while it is not necessary to differentiate one type of landslide from another the trained ann model will have its capability limited to the types of landslides that it was shown during the training process for each new application area or case study new training validation and test procedures are required in the upcoming sections we present and describe the functionality and operation of the module and a complete case study for the municipality of porto alegre brazil the results show that the resulting landslide susceptibility maps outperform the one published by the brazilian geological survey cprm cprm companhia de pesquisa de recursos minerais 2015 on identifying highly susceptible areas the text is organized as follows section 2 presents a brief description of the state of the art related to the mapping of landslide susceptibility section 3 presents the materials and methods section 4 presents the working and interface of the module and its application in a case study section 5 discusses the main results and section 6 gives final remarks 2 background and related work traditional methodologies of landslide mapping especially those based on empirical equations and field data nilsen and brabb 1977 naidu et al 2018 guzzetti et al 1999 gokceoglu and aksoy 1996 fall et al 2006 require a considerable amount of field studies and soil parameters not only such methodologies require considerable time and resources but also a high level of specialized work and a high degree of technical knowledge which limits their broad application further traditional methodologies are generally tailored to the study of small areas usually to a single hillslope as opposed to the mapping of an entire municipality which is the case of interest in landslide susceptibility mapping the development of alternative methodologies and tools that can support the identification of areas susceptible to landslides in an economical and efficient manner is essential to allow government agencies to advance in taking actions for the prevention and mitigation of potentially damages dang et al 2018 a number of methods and techniques for the mapping of landslide susceptibility have been proposed and tested in the literature such methodologies are usually grouped into two categories the qualitative and quantitative approaches the qualitative approach is based upon the judgment and evaluation by experts on the theme zare et al 2013 de brito et al 2017 which was widely used during the 1970s by engineers geologists and geomorphologists bai et al 2010 in turn the quantitative approach is based on numbers that express the likely relationship between the conditioning factors and the landslide events among the existing quantitative methodologies those based on artificial neural networks anns come as a strong alternative for their agility accuracy and effectiveness not only they allow the simplification of the investigation process neaupane and achet 2004 but as a data driven methodology they are flexible to adapt their learning process to available data additionally a number of features of the problem are well suited for anns the complex nonlinear relationships between variables tu 1996 the independence of statistical distributions of data lee et al 2004 the feasibility of using data at different scales and units of measure as well as qualitative variables kawabata and bandibas 2009 from this perspective anns are reported to be a powerful methodology given the availability of databases to be used for their learning process ermini et al 2005 melchiorre et al 2008 tsangaratos and benardos 2014 conforti et al 2014 the quantitative methodologies became popular a few decades ago due to the rise of gis zhou et al 2002 bai et al 2010 van westen et al 2003 van westen et al 2008 glade 2005 zare et al 2013 ilanloo 2011 gis based methodologies have been used to automate the mapping of landslide susceptibility in large areas thus reducing the time required to make and update susceptibility maps and trigger early warning systems temesgen et al 2001 ilanloo 2011 in addition gis makes it possible to efficiently collect manipulate and integrate a variety of spatial data such as geology structure surface coverage slope characteristics among other variables that are conditioning factors for the occurrence of landslides saha et al 2002 van westen 1994 biswajeet and saro 2007 ayalew and yamagishi 2005 pourghasemi et al 2012 chen et al 2018d a b 2019a rosi et al 2018 chen et al 2019c tien bui et al 2016 chen et al 2018c aghdam et al 2017 pham et al 2019 in the context of landslides and mass movements grass has the r massmov module initially implemented by beguería et al 2009 which performs the estimation of runout and deposition of landslide phenomena in complex topographies molinari et al 2014 the r shalstab module which implements the model proposed by montgomery and dietrich 1994 for susceptibility assessment of shallow landslides filipello and strigaro 2017 r slope stability developed by mergili et al 2014 performs the analysis of slope stability based on modified version of the hovland method hovland 1977 r avaflow presented by mergili et al 2017 simulates the movement of granular avalanches or debris flow regarding anns the module i ann maskrcnn developed by pesek 2018 performs object detection by means of convolutional neural networks cnns in short as allied to artificial intelligence techniques gis based methodologies could handle all the way from the download of new satellite images as they become available to the landslide susceptibility mapping monitoring of environmental variables and the supervised triggering of early warning systems as hazardous situations emerge 3 materials and methods this section briefly describes the operation of an ann and presents the details of the methodology used for the implementation of the r landslide module 3 1 artificial neural network ann the artificial neural network was developed from the principle of the multilayer perceptron with three layers of neurons the input layer which receives the input environmental parameters that will be applied in the training validation test and application of the ann intermediate or hidden layer which processes the interactions between the input parameters and maps the input to output thus allowing the ann to learn complex tasks and extract significant features of the input patterns haykin 1999 and the output layer which gives the result of the processing of the inputs fig 1 presents the structure of a multi layer neural network in this example the first layer of neurons contains seven input parameters which were the ones used in the case study and will be discussed in section 4 2 the landslide susceptibility map is generated by means of the values appearing in the output layer which were arbitrarily set in the scale from 0 non landslide to 1 landslide the connections between the neurons are given by synaptic weights w i j between neurons i and j in general the greater the value of w i j the greater the influence of the parameters associated to it negative values of w i j indicate that the parameter exerts a negative influence on the degree of susceptibility while positive values show a positive relation after the neurons are weighted by the synaptic weights an activation function is applied which is responsible for limiting the amplitude of the output value of a neuron normally the output value of a neuron is normalized to the interval 0 1 haykin 1999 the ann implemented in the r landslide module uses the sigmoid activation function which depicts a balance between linear and non linear behavior and is defined by eq 1 1 f x 1 1 e β x in which x is the input value of the intermediate or output layer neuron after weight bearing by the synaptic weights and β is the slope parameter by varying this parameter one obtains sigmoid functions of different slopes the module performs the training of anns by means of the backpropagation algorithm which is widely known and consists of computing the values of the weights by means of evaluation of the error between the response obtained by the propagation and the known real value of the output haykin 1999 in this way for each example presented during the training there is a corresponding output o which is compared to the reference output o d giving rise to the error function 2 e 1 2 o d o t o d o where t denotes the transpose for the synaptic weight w i j the increment δ w i j occurs by means of eq 3 3 δ w i j α e w i j where α is the learning rate which controls the intensity of the adjustment of the values associated with the synaptic weights the objective of the backpropagation algorithm is to adjust the synaptic weights in such a way that the error function e reaches a local minimum during the training process the validation process occurs simultaneously in a set of records that were not employed in the training it serves as a basis for the interruption of the training process the early stopping the early stopping procedure interrupts the training to collect the network parameters as the error function reaches a minimum on the validation set this is meant to avoid overfitting in which case the ann degrades its performance by memorizing the outputs and thus losing capacity to generalize haykin 1999 the training is finalized with the testing of the ann against unseen records to have its performance checked in regard to its accuracy and generalization capacity a sensitivity analysis is implemented in the module in order to identity the most influential input parameters to the identification of landslide susceptible areas this evaluation is performed by perturbing one input variable at a time while maintaining the other parameters constant from this procedure the module produces the sensitivity curve which provides information on the relevance of each input parameter as explanatory of the output the evaluations are performed with values normalized to the interval 0 1 3 2 the computational framework r landslide this section describes the functioning and operation of the r landslide module a flowchart describing the module is presented in fig 2 3 2 1 input data and parameters the idea behind the application of anns to landslide susceptibility mapping is the principle that landslide susceptible areas can be pinpointed by means of thematic parameters which represent conditioning factors to landslides the purpose of the ann is to map such conditioning factors based upon landslide data which means to find the patterns by which landslides occur in a given area as a function of thematic environmental parameters once this is done the input data maps containing the thematic variables are input to the module in raster format when the module process starts such raster information is automatically normalized to the interval 0 1 to standardize the scale of the input provided to the ann there is not a fixed set of parameters that must be used rather the user is allowed to test different sets of input parameters and decide which ones to consider on the basis of sensitivity analysis the sensitivity analysis is provided by the module when the ann training process is completed another input the user is requested to provide is a dataset containing the geographical location of landslides and roughly the same number of points of non landslide spots in vector or text file format the input of landslide and non landslide points is necessary to enable the neural network to learn the combinations of conditioning factors that are incident to the occurrence of landslides and those combinations that are not thus the adequate definition of non landslide points is of most importance and can be done by the user himself on the basis of dependable experience and reliable observation that allow the assured selection of landslide free areas a standard is to generate the non landslide points randomly inside the areas where the occurrence of landslides was not observed chen et al 2018a dang et al 2018 lin et al 2017 pascale et al 2013 other studies for example identified the non landslides areas with the usage of google earth imagery and temporal analysis of the digital elevation model chen et al 2019b the parameters required to build the architecture of the neural network are initially set to standard values however they can be changed by the user in the ann parameters tab such parameters include the number of neurons in the hidden layer the learning rate number of training epochs and number of initial conditions the latter in a case where the user chooses to generate a set of anns it is recommended that the maximum number of neurons in the hidden layer be defined based on the netch nielsen 1987 criterion defined in eq 4 given by 4 n h 2 n i 1 where n h is the number of neurons in the hidden layer and n i is the number of input variables this seeks to ensure that the neural network is able to approximate any continuous function such ability depends on the number of thematic input parameters chosen by the user the learning rate is generally defined to some value in the interval 0 1 lower learning rates may render in an increased number of training epochs while higher learning rates can make it more difficult for the ann to settle in a local minimum the learning rate is initially defined to the standard value of α 0 6 the user can also define the proportion of the records that will be used for training validation and test being the standard values set to 70 15 and 15 respectively additionally the user is allowed to define the computational region of the study area in which the generation of a landslide susceptibility map will take place this can be a portion of a larger map to that end the training of the ann and its application can be worked separately such that the user is allowed to isolate the desired area for evaluation thus reducing processing time and computational demand as the error function reaches a minimum in the validation set the training is completed and the module heads to the test step to evaluate the performance of the ann if the performance is satisfactory the ann is ready to be applied in the evaluation of landslide susceptibility otherwise the training procedure can be rerun until the ann reaches the expected performance 3 2 2 outputs as the ann training validation and test steps are completed the module presents performance results in the form of graphics and reports thus allowing the user to evaluate whether the network has achieved satisfactory results among the available information are the errors in the training validation and test steps and the results of sensitivity analysis the output tab of the module indicates the number of errors obtained in the test set false positives and false negatives the ann outputs are interpreted in the following way values above the threshold 0 5 are interpreted as positive for landslides whereas values below or equal to 0 5 are interpreted as negative 3 2 3 the interface the module features four main tabs i ann parameters ii training iii application and iv optional they are presented in figs 3 6 respectively in the ann parameters tab fig 3 the user is supposed to input the parameters referring to the architecture and general setup of the neural network these include the number of neurons in the hidden layer the learning rate and the number of training epochs in addition the user is required to provide information on the proportion of the dataset to be applied in the training validation and test of the ann the default values are the percentages 70 15 15 which are generally recommended and applied in the literature also the user has the option to require the module to generate a set of anns with different numbers of intermediate layer neurons and initial synaptic weights and then choose the one with best performance to do this it is necessary to specify the parameters marked with batch mode in this tab in addition to the learning rate and number of epochs parameters previously described in this mode the module will create architectures containing from two to n h neurons in the intermediate layer and with different initial conditions that is the number of initial sets of synaptic weight values that will be tested for each set of neurons in the intermediate layer from these the network that presents the best performance is then selected the training tab fig 4 requires the thematic layers to be inserted in raster format and the location of landslide events in vector point format alternatively the user has the option to load a text file containing the lat long coordinates of landslides events in this tab the user is required to specify the name of the output landslide susceptibility map and the location and name of the directory to be created to save the results generated in the training step the files containing the information on final network structure and parameter values the results from the sensitivity analysis and the landslide susceptibility map there is also the option to perform solely the training validation and test steps in this case the module will have the anns training process completed without generating any landslide susceptibility map this option allows different network structures to be tested before the final susceptibility of the study area is evaluated and it also allows the application of the trained ann to an area other than that where the ann was trained in the latter case the user should be vigilant in order to avoid the application of a trained ann for regions whose characteristics of the thematic variables feature any noticeable difference from the ones from training region in order to use this option the user is required to check the box perform only the training validation and test steps the application tab fig 5 should be used when the user has a trained ann and intends to apply it to generate a landslide susceptibility map to this end the user is required to check the perform only the application check box and insert the directory containing the architecture parameter files of the trained ann they intend to apply the user is also required to name the landslide susceptibility map before it is generated finally in the optional tab fig 6 the user is allowed to input a raster layer to delimit a computational domain for the generation of the landslide susceptibility map this should be used in the case the user intends to generate a map for a subregion of the whole image 4 case study landslide susceptibility map for the municipality of porto alegre brazil 4 1 description of the area the municipality of porto alegre is the capital of rio grande do sul state and is located in southern brazil as shown in fig 7 at coordinates 30 1 40 south and 51 13 43 west it has a well distributed rainfall regime throughout the year with annual average of 1 425 mm the predominant lithological units are the eluvial and colluvial deposits characterized by quartz sandy deposits with often occurrence in higher regions alluvial deposits consisting of pebbles sands gravel silt clay material and organic material dom feliciano suite characterized by porphyritic etagranitoids and granites and facies serra do herval composed basically of k feltdspato cprm companhia de pesquisa de recursos minerais 2017 according to reckziegel 2007 porto alegre has the highest number of landslide records among the municipalities of rio grande do sul state being considered in 2011 by the brazilian government a priority area with regard to the prevention of natural disasters the municipality has some systems to prevent damages associated to landslides as an example one can cite the mass movement susceptibility map developed by the geological service of brazil cprm companhia de pesquisa de recursos minerais 2015 which was used to compare the results of the module for porto alegre 4 2 data acquisition the thematic parameters to be used were defined based on the study carried out by pascale et al 2013 information regarding lithology soil use aspect elevation vertical curvature slope and topographic wetness index twi for the municipality of porto alegre were obtained from different sources the lithology information was extracted from the database of the geological service of brazil cprm companhia de pesquisa de recursos minerais 2017 in vector format and then converted to raster format the soil cover data were obtained from the publication environmental diagnosis of the municipality of porto alegre hasenack et al 2008 elevation slope aspect and vertical curvature were obtained from the digital elevation model dem produced by the topodata inpe instituto de pesquisas espaciais 2011 project with a spatial resolution of 30 m finally the raster map of the twi was generated from the dem with the aid of the r topidx module available in grass gis software version 7 2 2 grass development team 2017 the data layers that were used as input parameters in the r landslide module for the municipality of porto alegre are presented in figs 8 and 9 along with the true color satellite image of the area the landslide data containing 119 records was extracted from the study by de brito 2014 and it was used for training validation and test steps the dataset has 17 rockfall records and 102 slide records with their respective geographical coordinates from these records 50 were selected by means of random sampling to compose the dataset used for the ann learning since the spatial prediction of landslides is formulated as a binary classification problem landslide and non landslide it was necessary select randomly 50 non landslides points to include in the dataset the selection of a smaller set of records seeks to avoid uncertainties associated with the definition of non landslide points and to conserve a number of unseen records for further evaluation of the results thus the dataset applied in the training validation and test of the ann was composed of 100 records 50 landslides and 50 non landslides these records were randomly assigned to training validation and test sets in the proportion 70 15 15 the remaining landslides records 69 were used to evaluate the ann performance random sampling was applied in the selection of records to the training validation and test sets since it is a standard method that offers simplicity and low computational cost as a limitation it may result in subpar training and test sets that might cause the performance of machine learning models to slightly deteriorate if this is the case the user can re run the sampling procedure and obtain a new training set alternatively other more elaborate methods and techniques that can handle the sampling process in a systematic way are available marjanović et al 2011 hong et al 2018 goetz et al 2011 the spatial distribution of landslide and non landslide events is shown in fig 7 4 3 ann parameters the parameters used for the creation and training of the ann were the default values specified by the module fig 3 the default values are 200 training epochs learning rate α 0 6 minimum and maximum number of neurons in the hidden layer equal to 2 and 12 respectively and percentages of records in the training validation and test sets equal to 70 15 15 respectively 4 4 results this subsection presents the results from the application of the r landslide module first the results concerning the training validation test and sensitivity analysis are presented next the resulting landslide susceptibility map for porto alegre is presented and compared to the map generated by the brazilian geological survey which is currently the official publication on the topic 4 4 1 training validation test sensitivity analysis and performance measures the ann performance results during the training validation and test steps are presented in fig 10 fig 10 a shows the evolution of the training and validation errors along the training epochs as the synaptic weights were updated both errors rapidly decline in the early epochs and subsequently stabilize by means of the early stopping method the set of synaptic weights was collected at epoch 130 where the minimum of the validation error was observed fig 10 b presents results from the test step considering that in this case the ann outputs were rounded towards 1 or 0 it can be verified that there was only one classification error as a false positive fig 11 presents the confusion matrix the diagonal cells show the number and percentage of correct classifications which amount 93 3 for the test set of the case study further more meaningful measures can be extracted from the confusion matrix to illustrate the results such as accuracy precision recall the f measure and cohen s kappa coefficient for these measures higher values close to 1 0 indicate better models witten et al 2011 using the test set we obtained values for the accuracy of 0 95 precision of 1 0 recall of 0 86 and f measure of 0 92 considering that cohen s kappa coefficient is a more conservative metric the value obtained is within the range of the almost perfect agreement class landis and koch 1977 fig 12 presents the roc receiver operational characteristic curve which illustrate the auc area under curve for the ann trained in the case study which amounts to 0 96 for a perfect classifier the auc equals 1 0 and for an auc in the interval 0 9 1 0 the classifier quality is regarded as excellent concerning the sensitivity analysis performed by the module after the training process the parameters that had major influence on the susceptibility results were in descending order slope topographic wetness index land use and elevation table 1 4 4 2 landslide susceptibility map fig 13 presents the landslide susceptibility map for the municipality of porto alegre brazil generated by the r landslide module to the left and the map published by the brazilian geological survey to the right the landslide susceptibility classes high medium low were created from the definition of susceptibility thresholds that take into account two factors i the ann outputs and ii the proportion of landslide records that must belong to that class for example in this study we required that the high susceptibility class red areas must contain at least 65 of the landslide events under its classification while the medium and low susceptibility classes are required to have at least 25 and at most 10 respectively from these percentages two decision thresholds within the interval 0 1 are defined that separate the classes one threshold separates the low medium classes and the other the medium high this procedure was applied to the application data from the landslide database and resulted in threshold 0 50 and 0 95 for porto alegre by comparing the maps it can be shown that they hold strong qualitative and quantitative similarities a direct comparison between the matrices representing the maps shows that they agree to a great extent in the classification of susceptibility as low medium and high from the susceptibility map generated by the module a total area of 410 75 km2 88 05 is located in areas of low susceptibility 40 73 km2 8 73 in areas of medium susceptibility and 15 05 km2 3 22 in regions of high susceptibility compared with the cprm map cprm companhia de pesquisa de recursos minerais 2015 392 60 km2 83 98 66 62 km2 14 25 and 8 27 km2 1 77 are located in regions of low medium and high susceptibility respectively for landslide records not seen during the training validation and test steps the so called unseen records the ann was able to identify highly susceptible areas by means of the features of the set of thematic parameters applied as inputs as the unseen records were overlaid in both susceptibility maps it was perceived that the landslide points were located in areas to which the module had correctly assigned high susceptibility class by comparing the susceptibility map from the module to the susceptibility map by cprm it can be shown that the results obtained by means of the r landslide module outperformed those from the official publication by cprm as such it can be concluded that the map generated by the r landslide module has an increased level of hits in correctly assigning high susceptibility class to unseen landslide points which indicated an improved accuracy of the ann map as compared to the cprm map this can be seen from the histograms presented in fig 14 5 discussions the results obtained from r landslide module in the case study showed that the tool was able to generate satisfactory results when compared to the official publication by cprm cprm companhia de pesquisa de recursos minerais 2015 and also when the map was evaluated against unseen landslide records this was accomplished by means of available parameters and data which were acquired in open and free datasets the comparison between the results from r landslide and the map elaborated by cprm showed that the susceptibility map by r landslide outperformed that of cprm in correctly assigning high susceptibility classification to landslide areas this was confirmed by means of unseen records of real landslide data such that it is a well established conclusion thus the maps provided by the module using a trained ann featured most of the records correctly classified in the high susceptibility class in this regard we remark that some landslide events located in areas that were classified by the ann as high susceptibility class are located in low susceptibility areas according to the study by cprm these errors are classified as false negatives which are undesirable errors since they can lead to the inadvertent masking of susceptible areas as compared to previous studies of landslide susceptibility in porto alegre de brito et al 2017 found the auc of 0 96 thus the same value that was found in our study this shows that both models feature high quality in predicting landslides from the input data it is important to recall that de brito et al 2017 reported that the high value of the auc was obtained by means of expert support who helped in the assignment of the contribution and relative importance of conditioning factors upon the occurrence of landslides in this sense we showed that r landslide and its backbone methodology could provide an equivalent level of prediction quality with added scalability and reproducibility since it does not rely on expert knowledge in order to get hold of landslide susceptibility mapping this advantage is especially important in what concerns the evaluation of the influence of conditioning factors which are part of the learning process and completely data driven now it should be remarked that while expert knowledge is not required in order to set off landslide susceptibility mapping by means of the r landslide module expert knowledge and evaluations can be easily taken into account in the learning process by turning them into one or more extra input variables for the module to work with when comparing the results achieved in this study with studies that applied different methodologies for landslide susceptibility mapping the auc value 0 96 obtained in the case study can be considered high chen et al 2018b d 2019a pandey and sharma 2017 zêzere et al 2017 francipane et al 2014 hong et al 2016 de brito et al 2017 for instance chen et al 2018d evaluated three state of the art data mining techniques best first decision tree random forest and naïve bayes tree for landslide susceptibility modeling the random forest method presented the best performance with auc value of 0 87 an accuracy value of 0 77 precision and f measure of 0 66 in other study that involved an adaptive neuro fuzzy inference system anfis optimized by shuffled frog leaping algorithm sfla and particle swarm optimization pso chen et al 2019a the authors found auc value of 0 89 for the training and testing dataset a similar research that also applied the anfis methodology with different statistical bivariate methods frequency ratio fr and weights of evidence woe obtained the highest auc value with fr anfis ensemble 0 85 in turn chen et al 2018b who studied ensembles of bivariate statistical methods based kernel logistic regression classifiers evidential belief function ebf statistical index si and woe and found that the ebf klr method had the highest predictive capability auc values of 0 81 and 0 75 for the training and validation datasets respectively other studies that also applied ensembles of different techniques obtained similar results for example tien bui et al 2016 used functional trees classifier with adaboost bagging and multiboost ensembles the authors found auc values ranging from 0 92 to 0 96 chen et al 2018c studied a bagging based kernel logistic regression model to mapping landslide susceptibility with this approach the model obtained a auc value of 0 85 further we compile some previous studies that applied artificial neural networks to identify susceptible landslide areas pham et al 2017 zare et al 2013 tsangaratos and benardos 2014 conforti et al 2014 pascale et al 2013 pham et al 2017 obtained a maximum auc value of 0 89 with the multiboost ensemble model zare et al 2013 achieved a value of 0 92 for the same metric with a multi layer perceptron tsangaratos and benardos 2014 obtained an accuracy value of 0 89 a precision of 0 91 a recall of 0 88 and a f measure value of 0 89 using the validation dataset of an ann algorithm in a broader sense the results indicate that the application of anns in landslide susceptibility mapping features numerous advantages such as the ability to detect complex nonlinear relations between variables the ability to detect interactions between predictor variables and the ability to process data at different scales and units of measure tu 1996 kanungo et al 2006 regarding the strengths of the methodology we highlight that anns are computationally fast and suitable for parallel computing platforms since training test and application can be parallelized further anns and machine learning methods in general allow expert knowledge to be considered in the learning process such that the experience of specialists can be taken into account in a straightforward manner this feature is not as straightforward in methodologies based on logistic regression for example ozdemir and altural 2013 hosmer and lemeshow 2000 wang et al 2013 bai et al 2010 as a black box modeling approach anns do not require deep understanding of the underlying physical mechanisms of the phenomenon under study however also have limited contribution to the understanding of its underlying mechanisms this feature is shared by other quantitative methods chen et al 2017a 2018d b 2019b c concerning the performance past studies showed that anns achieved better results as compared with other methods chen et al 2017b yilmaz 2010 for example yilmaz 2010 compared different gis based landslide susceptibility mapping methods by comparing the resulting auc values the authors showed that the ann model provided better accuracy than any of the other methods considered conditional probability logistic regression and support vector machines in turn chen et al 2017b studied three different machine learning methods maximum entropy maxent support vector machine svm and artificial neural network ann and their ensembles as a result in the case of validating the models separately the ann presented the best performance among them the results from the case study for porto alegre brazil indicate that anns indeed have superior performance having achieved auc of 0 96 and thus higher than those found in the aforementioned studies nevertheless some issues may be considered as limiting factors such as the dependence of the methodology on the existence of local landslide databases this may restrict the applicability of the tool in some regions of the world where the process of creating maintaining and updating landslide databases is not a well consolidated practice for this reason it is suggested to the public agencies responsible for managing natural disasters to create maintain and update databases with information on landslides and make them available for the wide public this would facilitate the application of methodologies based on artificial intelligence to the identification of landslide susceptible areas as well as increase the reliability of the results the application of this methodology still allows the evaluation of the sensitivity of the output to the conditioning factors of susceptibility response to landslides the parameters that exerted the greatest influence in our case study were slope topographic wetness index land use and elevation a study carried out by de brito et al 2016 identified that the anthropic intervention ends up being the main agent of landslides in the urban area of porto alegre mainly due to the execution of excessive cuts of slopes excavations in bases of slopes leakage of pipes and cultivation of inappropriate species still according to de brito et al 2016 82 of the recorded mass movements occurred in places with irregular occupations in these conditions the most sensitive parameters are related to interferences observed and raised by de brito et al 2016 especially the slope due to cuts and excavations and land use due to irregular occupations other authors have also observed such parameters as factors that affect the occurrence of slides chen et al 2018a ilia and tsangaratos 2016 chen et al 2017a 2018c tien bui et al 2016 regarding the strengths of the module it packs a robust methodology based on anns in an intuitive gis add on and offers an user friendly interface that can promote its widespread application among final users further the open source nature of the software promotes its collaborative development towards improved versions concerning the production of knowledge on the mechanisms and triggers of landslides having as the backbone a data driven methodology the module can help in the identification of the different conditioning factors to landslides by means of sensitivity analysis this allows the identification of the parameters that exert greater influence on the occurrence of landslides at a given location or area thus also allowing that preventive measures be taken more effectively in the same way the flexible ann framework allows that input parameters be modified or updated at any time as new data and additional information becomes available this allows further training to be performed while different types of landslides exist varnes 1958 our research aims at validating a new grass gis module to assess general landslide susceptibility maps as a result of the ann training for a given study area the ann will be capable of identifying susceptibility in such area by searching for the same types of landslides it is familiar with due to training it is important to highlight that the module presented in this paper could be used by public agencies as support for decision making in order to help identify susceptible areas more quickly to promote a detailed and customized analysis of regions of interest to support decision making and the working and triggering of early warning systems in addition it can serve as an auxiliary tool for the zoning of urban land use 6 conclusions in this article we presented the r landslide module which is a gis implementation of artificial neural networks for the mapping of landslide susceptibility the module was written in python language as an add on to grass gis software and is compatible with other existing grass modules it provides the user with tools for training and application of anns for this purpose a case study was presented for the municipality of porto alegre brazil from which the use and characteristics of the module were demonstrated as well as its interface and capabilities in surveying susceptible areas we remark that the module can be advantageously used as an initial step towards the development of a support system that can make the process of landslide susceptibility mapping more agile and effective on the elaboration of future studies we propose the integration of the module and anns to the dynamic mapping of the susceptibility to landslides in real time from precipitation information from automatic gauge stations this would be intended to support early warning systems for events that are triggered by rainfall to facilitate the application of this methodology in the near future it is recommended to public agencies to create maintain and update reliable landslide databases the code for module is open source and can be freely downloaded from https github com uffsenvmodelling r landslide bragagnolo et al 2019 acknowledgment the authors thank capes for the support 
26099,this study presents r landslide a free and open source add on to the open source geographic information system gis grass software for landslide susceptibility mapping the tool was written in python language and works on the top of an artificial neural network ann fed with environmental parameters and landslide databases in order to illustrate the application and effectiveness of the developed tool a case study is presented for the municipality of porto alegre brazil the resulting landslide susceptibility maps are compared with the map published by the brazilian geological survey cprm and a direct comparison using unseen new landslide records indicate that the r landslide can identify and pinpoint susceptible areas with better accuracy the module can be used by natural disaster management bodies and land use planning organs as a support tool for the elaboration of landslide susceptibility maps in an agile and efficient manner keywords natural disasters landslide susceptibility assessment gis availability program name r landslide developer l bragagnolo r v da silva j m v grzybowski contact address lucimarabragagnolo hotmail com year first available 2019 program language python 2 7 package size 70 kb availability https github com uffsenvmodelling r landslide cost free of charge 1 introduction landslides are common hazards which can result in substantial economic losses casualties and adverse impacts on infrastructure the environment and communities severely affecting urban development and land use dai et al 2002 lee and choi 2004 zhu et al 2018 flentje and chowdhury 2018 shahabi and hashim 2015 as a result of increasing population density deforestation and uncontrolled urbanization activities the risks associated with landslides have increased over time flentje and chowdhury 2018 froude and petley 2018 in this context the mapping of landslide susceptibility plays an important role in the prospective planning of land occupation in order to avoid the occupation of susceptible locations schwab et al 2005 thus the accurate mapping of susceptible areas is key information for a wide range of users in the public and private sectors government departments and the scientific community at the local and international levels fell et al 2008 petschko et al 2014 a common difficulty in developing countries is the lack of technical infrastructure such that neither the population nor the government authorities are sufficiently aware of landslide hazards hervás 2003 petschko et al 2014 this can be broadly illustrated by the number of recent and deadly landslide events whose consequences could have been considerably mitigated if landslide susceptibility maps were available to substantiate decision making on land occupation policies daily mail 2018c b a bbc news 2018a b regarding the situation in brazil according to soriano et al 2017 many municipal civil defense agencies still have difficulties in developing practical proposals for consistent identification of risks control of natural disasters and mitigation of their consequences among the causes one can cite technical limitations of personnel and the institutional structure of civil defense agencies which are still not well consolidated in regards to their functions and prerogatives on the integration of artificial intelligence and geographic information systems gis methods that use gis and remote sensing yield a landslide susceptibility assessment that is more accurate than those using previous approaches chen et al 2018a besides the increasing availability of datasets with considerable spatial resolution and computers with fast processing capacity made it possible to automate the process of acquiring and pre processing high resolution images among the existing open source gis softwares the geographic resources analysis support system grass grass development team 2017 has received a considerable investment of time and effort by the developing community neteler et al 2012 up to date there are no modules available that integrate the ann with the grass for landslide susceptibility mapping further although some of the cited modules work with the identification of slide probabilities and slope stability analysis they often require a series of specific input parameters which in turn require massive field work and detailed surveys of the study areas as mentioned beforehand the severe lack of resources makes it difficult that such methodologies be implemented in developing countries with lacking infrastructure for worse in certain cases such parameters are simply generalized or estimated from empirical or oversimplified equations that not only limit the validity of the resulting maps but can inadvertently mask dangerous situations by not recognizing them appropriately or even by giving flawed grounds for decision making this article presents the r landslide module for grass gis software the module implements anns for landslide susceptibility mapping on the basis of available environmental geological and geophysical data obtained from satellite imagery and georeferenced landslide databases the module was developed in the python language following the open source ideal of the grass software the main contribution of this research is to deliver an open source and free module that bundles anns and a user friendly interface in the gis environment to deliver an intuitive end to end support tool for landslide susceptibility assessment for application of the module to a given study region the ann must be previously trained validated and tested using landslide data from such region thus the types of landslides that the trained ann model will be able to identify with accuracy will be those types that are present in corresponding training dataset in the form of landslide records while it is not necessary to differentiate one type of landslide from another the trained ann model will have its capability limited to the types of landslides that it was shown during the training process for each new application area or case study new training validation and test procedures are required in the upcoming sections we present and describe the functionality and operation of the module and a complete case study for the municipality of porto alegre brazil the results show that the resulting landslide susceptibility maps outperform the one published by the brazilian geological survey cprm cprm companhia de pesquisa de recursos minerais 2015 on identifying highly susceptible areas the text is organized as follows section 2 presents a brief description of the state of the art related to the mapping of landslide susceptibility section 3 presents the materials and methods section 4 presents the working and interface of the module and its application in a case study section 5 discusses the main results and section 6 gives final remarks 2 background and related work traditional methodologies of landslide mapping especially those based on empirical equations and field data nilsen and brabb 1977 naidu et al 2018 guzzetti et al 1999 gokceoglu and aksoy 1996 fall et al 2006 require a considerable amount of field studies and soil parameters not only such methodologies require considerable time and resources but also a high level of specialized work and a high degree of technical knowledge which limits their broad application further traditional methodologies are generally tailored to the study of small areas usually to a single hillslope as opposed to the mapping of an entire municipality which is the case of interest in landslide susceptibility mapping the development of alternative methodologies and tools that can support the identification of areas susceptible to landslides in an economical and efficient manner is essential to allow government agencies to advance in taking actions for the prevention and mitigation of potentially damages dang et al 2018 a number of methods and techniques for the mapping of landslide susceptibility have been proposed and tested in the literature such methodologies are usually grouped into two categories the qualitative and quantitative approaches the qualitative approach is based upon the judgment and evaluation by experts on the theme zare et al 2013 de brito et al 2017 which was widely used during the 1970s by engineers geologists and geomorphologists bai et al 2010 in turn the quantitative approach is based on numbers that express the likely relationship between the conditioning factors and the landslide events among the existing quantitative methodologies those based on artificial neural networks anns come as a strong alternative for their agility accuracy and effectiveness not only they allow the simplification of the investigation process neaupane and achet 2004 but as a data driven methodology they are flexible to adapt their learning process to available data additionally a number of features of the problem are well suited for anns the complex nonlinear relationships between variables tu 1996 the independence of statistical distributions of data lee et al 2004 the feasibility of using data at different scales and units of measure as well as qualitative variables kawabata and bandibas 2009 from this perspective anns are reported to be a powerful methodology given the availability of databases to be used for their learning process ermini et al 2005 melchiorre et al 2008 tsangaratos and benardos 2014 conforti et al 2014 the quantitative methodologies became popular a few decades ago due to the rise of gis zhou et al 2002 bai et al 2010 van westen et al 2003 van westen et al 2008 glade 2005 zare et al 2013 ilanloo 2011 gis based methodologies have been used to automate the mapping of landslide susceptibility in large areas thus reducing the time required to make and update susceptibility maps and trigger early warning systems temesgen et al 2001 ilanloo 2011 in addition gis makes it possible to efficiently collect manipulate and integrate a variety of spatial data such as geology structure surface coverage slope characteristics among other variables that are conditioning factors for the occurrence of landslides saha et al 2002 van westen 1994 biswajeet and saro 2007 ayalew and yamagishi 2005 pourghasemi et al 2012 chen et al 2018d a b 2019a rosi et al 2018 chen et al 2019c tien bui et al 2016 chen et al 2018c aghdam et al 2017 pham et al 2019 in the context of landslides and mass movements grass has the r massmov module initially implemented by beguería et al 2009 which performs the estimation of runout and deposition of landslide phenomena in complex topographies molinari et al 2014 the r shalstab module which implements the model proposed by montgomery and dietrich 1994 for susceptibility assessment of shallow landslides filipello and strigaro 2017 r slope stability developed by mergili et al 2014 performs the analysis of slope stability based on modified version of the hovland method hovland 1977 r avaflow presented by mergili et al 2017 simulates the movement of granular avalanches or debris flow regarding anns the module i ann maskrcnn developed by pesek 2018 performs object detection by means of convolutional neural networks cnns in short as allied to artificial intelligence techniques gis based methodologies could handle all the way from the download of new satellite images as they become available to the landslide susceptibility mapping monitoring of environmental variables and the supervised triggering of early warning systems as hazardous situations emerge 3 materials and methods this section briefly describes the operation of an ann and presents the details of the methodology used for the implementation of the r landslide module 3 1 artificial neural network ann the artificial neural network was developed from the principle of the multilayer perceptron with three layers of neurons the input layer which receives the input environmental parameters that will be applied in the training validation test and application of the ann intermediate or hidden layer which processes the interactions between the input parameters and maps the input to output thus allowing the ann to learn complex tasks and extract significant features of the input patterns haykin 1999 and the output layer which gives the result of the processing of the inputs fig 1 presents the structure of a multi layer neural network in this example the first layer of neurons contains seven input parameters which were the ones used in the case study and will be discussed in section 4 2 the landslide susceptibility map is generated by means of the values appearing in the output layer which were arbitrarily set in the scale from 0 non landslide to 1 landslide the connections between the neurons are given by synaptic weights w i j between neurons i and j in general the greater the value of w i j the greater the influence of the parameters associated to it negative values of w i j indicate that the parameter exerts a negative influence on the degree of susceptibility while positive values show a positive relation after the neurons are weighted by the synaptic weights an activation function is applied which is responsible for limiting the amplitude of the output value of a neuron normally the output value of a neuron is normalized to the interval 0 1 haykin 1999 the ann implemented in the r landslide module uses the sigmoid activation function which depicts a balance between linear and non linear behavior and is defined by eq 1 1 f x 1 1 e β x in which x is the input value of the intermediate or output layer neuron after weight bearing by the synaptic weights and β is the slope parameter by varying this parameter one obtains sigmoid functions of different slopes the module performs the training of anns by means of the backpropagation algorithm which is widely known and consists of computing the values of the weights by means of evaluation of the error between the response obtained by the propagation and the known real value of the output haykin 1999 in this way for each example presented during the training there is a corresponding output o which is compared to the reference output o d giving rise to the error function 2 e 1 2 o d o t o d o where t denotes the transpose for the synaptic weight w i j the increment δ w i j occurs by means of eq 3 3 δ w i j α e w i j where α is the learning rate which controls the intensity of the adjustment of the values associated with the synaptic weights the objective of the backpropagation algorithm is to adjust the synaptic weights in such a way that the error function e reaches a local minimum during the training process the validation process occurs simultaneously in a set of records that were not employed in the training it serves as a basis for the interruption of the training process the early stopping the early stopping procedure interrupts the training to collect the network parameters as the error function reaches a minimum on the validation set this is meant to avoid overfitting in which case the ann degrades its performance by memorizing the outputs and thus losing capacity to generalize haykin 1999 the training is finalized with the testing of the ann against unseen records to have its performance checked in regard to its accuracy and generalization capacity a sensitivity analysis is implemented in the module in order to identity the most influential input parameters to the identification of landslide susceptible areas this evaluation is performed by perturbing one input variable at a time while maintaining the other parameters constant from this procedure the module produces the sensitivity curve which provides information on the relevance of each input parameter as explanatory of the output the evaluations are performed with values normalized to the interval 0 1 3 2 the computational framework r landslide this section describes the functioning and operation of the r landslide module a flowchart describing the module is presented in fig 2 3 2 1 input data and parameters the idea behind the application of anns to landslide susceptibility mapping is the principle that landslide susceptible areas can be pinpointed by means of thematic parameters which represent conditioning factors to landslides the purpose of the ann is to map such conditioning factors based upon landslide data which means to find the patterns by which landslides occur in a given area as a function of thematic environmental parameters once this is done the input data maps containing the thematic variables are input to the module in raster format when the module process starts such raster information is automatically normalized to the interval 0 1 to standardize the scale of the input provided to the ann there is not a fixed set of parameters that must be used rather the user is allowed to test different sets of input parameters and decide which ones to consider on the basis of sensitivity analysis the sensitivity analysis is provided by the module when the ann training process is completed another input the user is requested to provide is a dataset containing the geographical location of landslides and roughly the same number of points of non landslide spots in vector or text file format the input of landslide and non landslide points is necessary to enable the neural network to learn the combinations of conditioning factors that are incident to the occurrence of landslides and those combinations that are not thus the adequate definition of non landslide points is of most importance and can be done by the user himself on the basis of dependable experience and reliable observation that allow the assured selection of landslide free areas a standard is to generate the non landslide points randomly inside the areas where the occurrence of landslides was not observed chen et al 2018a dang et al 2018 lin et al 2017 pascale et al 2013 other studies for example identified the non landslides areas with the usage of google earth imagery and temporal analysis of the digital elevation model chen et al 2019b the parameters required to build the architecture of the neural network are initially set to standard values however they can be changed by the user in the ann parameters tab such parameters include the number of neurons in the hidden layer the learning rate number of training epochs and number of initial conditions the latter in a case where the user chooses to generate a set of anns it is recommended that the maximum number of neurons in the hidden layer be defined based on the netch nielsen 1987 criterion defined in eq 4 given by 4 n h 2 n i 1 where n h is the number of neurons in the hidden layer and n i is the number of input variables this seeks to ensure that the neural network is able to approximate any continuous function such ability depends on the number of thematic input parameters chosen by the user the learning rate is generally defined to some value in the interval 0 1 lower learning rates may render in an increased number of training epochs while higher learning rates can make it more difficult for the ann to settle in a local minimum the learning rate is initially defined to the standard value of α 0 6 the user can also define the proportion of the records that will be used for training validation and test being the standard values set to 70 15 and 15 respectively additionally the user is allowed to define the computational region of the study area in which the generation of a landslide susceptibility map will take place this can be a portion of a larger map to that end the training of the ann and its application can be worked separately such that the user is allowed to isolate the desired area for evaluation thus reducing processing time and computational demand as the error function reaches a minimum in the validation set the training is completed and the module heads to the test step to evaluate the performance of the ann if the performance is satisfactory the ann is ready to be applied in the evaluation of landslide susceptibility otherwise the training procedure can be rerun until the ann reaches the expected performance 3 2 2 outputs as the ann training validation and test steps are completed the module presents performance results in the form of graphics and reports thus allowing the user to evaluate whether the network has achieved satisfactory results among the available information are the errors in the training validation and test steps and the results of sensitivity analysis the output tab of the module indicates the number of errors obtained in the test set false positives and false negatives the ann outputs are interpreted in the following way values above the threshold 0 5 are interpreted as positive for landslides whereas values below or equal to 0 5 are interpreted as negative 3 2 3 the interface the module features four main tabs i ann parameters ii training iii application and iv optional they are presented in figs 3 6 respectively in the ann parameters tab fig 3 the user is supposed to input the parameters referring to the architecture and general setup of the neural network these include the number of neurons in the hidden layer the learning rate and the number of training epochs in addition the user is required to provide information on the proportion of the dataset to be applied in the training validation and test of the ann the default values are the percentages 70 15 15 which are generally recommended and applied in the literature also the user has the option to require the module to generate a set of anns with different numbers of intermediate layer neurons and initial synaptic weights and then choose the one with best performance to do this it is necessary to specify the parameters marked with batch mode in this tab in addition to the learning rate and number of epochs parameters previously described in this mode the module will create architectures containing from two to n h neurons in the intermediate layer and with different initial conditions that is the number of initial sets of synaptic weight values that will be tested for each set of neurons in the intermediate layer from these the network that presents the best performance is then selected the training tab fig 4 requires the thematic layers to be inserted in raster format and the location of landslide events in vector point format alternatively the user has the option to load a text file containing the lat long coordinates of landslides events in this tab the user is required to specify the name of the output landslide susceptibility map and the location and name of the directory to be created to save the results generated in the training step the files containing the information on final network structure and parameter values the results from the sensitivity analysis and the landslide susceptibility map there is also the option to perform solely the training validation and test steps in this case the module will have the anns training process completed without generating any landslide susceptibility map this option allows different network structures to be tested before the final susceptibility of the study area is evaluated and it also allows the application of the trained ann to an area other than that where the ann was trained in the latter case the user should be vigilant in order to avoid the application of a trained ann for regions whose characteristics of the thematic variables feature any noticeable difference from the ones from training region in order to use this option the user is required to check the box perform only the training validation and test steps the application tab fig 5 should be used when the user has a trained ann and intends to apply it to generate a landslide susceptibility map to this end the user is required to check the perform only the application check box and insert the directory containing the architecture parameter files of the trained ann they intend to apply the user is also required to name the landslide susceptibility map before it is generated finally in the optional tab fig 6 the user is allowed to input a raster layer to delimit a computational domain for the generation of the landslide susceptibility map this should be used in the case the user intends to generate a map for a subregion of the whole image 4 case study landslide susceptibility map for the municipality of porto alegre brazil 4 1 description of the area the municipality of porto alegre is the capital of rio grande do sul state and is located in southern brazil as shown in fig 7 at coordinates 30 1 40 south and 51 13 43 west it has a well distributed rainfall regime throughout the year with annual average of 1 425 mm the predominant lithological units are the eluvial and colluvial deposits characterized by quartz sandy deposits with often occurrence in higher regions alluvial deposits consisting of pebbles sands gravel silt clay material and organic material dom feliciano suite characterized by porphyritic etagranitoids and granites and facies serra do herval composed basically of k feltdspato cprm companhia de pesquisa de recursos minerais 2017 according to reckziegel 2007 porto alegre has the highest number of landslide records among the municipalities of rio grande do sul state being considered in 2011 by the brazilian government a priority area with regard to the prevention of natural disasters the municipality has some systems to prevent damages associated to landslides as an example one can cite the mass movement susceptibility map developed by the geological service of brazil cprm companhia de pesquisa de recursos minerais 2015 which was used to compare the results of the module for porto alegre 4 2 data acquisition the thematic parameters to be used were defined based on the study carried out by pascale et al 2013 information regarding lithology soil use aspect elevation vertical curvature slope and topographic wetness index twi for the municipality of porto alegre were obtained from different sources the lithology information was extracted from the database of the geological service of brazil cprm companhia de pesquisa de recursos minerais 2017 in vector format and then converted to raster format the soil cover data were obtained from the publication environmental diagnosis of the municipality of porto alegre hasenack et al 2008 elevation slope aspect and vertical curvature were obtained from the digital elevation model dem produced by the topodata inpe instituto de pesquisas espaciais 2011 project with a spatial resolution of 30 m finally the raster map of the twi was generated from the dem with the aid of the r topidx module available in grass gis software version 7 2 2 grass development team 2017 the data layers that were used as input parameters in the r landslide module for the municipality of porto alegre are presented in figs 8 and 9 along with the true color satellite image of the area the landslide data containing 119 records was extracted from the study by de brito 2014 and it was used for training validation and test steps the dataset has 17 rockfall records and 102 slide records with their respective geographical coordinates from these records 50 were selected by means of random sampling to compose the dataset used for the ann learning since the spatial prediction of landslides is formulated as a binary classification problem landslide and non landslide it was necessary select randomly 50 non landslides points to include in the dataset the selection of a smaller set of records seeks to avoid uncertainties associated with the definition of non landslide points and to conserve a number of unseen records for further evaluation of the results thus the dataset applied in the training validation and test of the ann was composed of 100 records 50 landslides and 50 non landslides these records were randomly assigned to training validation and test sets in the proportion 70 15 15 the remaining landslides records 69 were used to evaluate the ann performance random sampling was applied in the selection of records to the training validation and test sets since it is a standard method that offers simplicity and low computational cost as a limitation it may result in subpar training and test sets that might cause the performance of machine learning models to slightly deteriorate if this is the case the user can re run the sampling procedure and obtain a new training set alternatively other more elaborate methods and techniques that can handle the sampling process in a systematic way are available marjanović et al 2011 hong et al 2018 goetz et al 2011 the spatial distribution of landslide and non landslide events is shown in fig 7 4 3 ann parameters the parameters used for the creation and training of the ann were the default values specified by the module fig 3 the default values are 200 training epochs learning rate α 0 6 minimum and maximum number of neurons in the hidden layer equal to 2 and 12 respectively and percentages of records in the training validation and test sets equal to 70 15 15 respectively 4 4 results this subsection presents the results from the application of the r landslide module first the results concerning the training validation test and sensitivity analysis are presented next the resulting landslide susceptibility map for porto alegre is presented and compared to the map generated by the brazilian geological survey which is currently the official publication on the topic 4 4 1 training validation test sensitivity analysis and performance measures the ann performance results during the training validation and test steps are presented in fig 10 fig 10 a shows the evolution of the training and validation errors along the training epochs as the synaptic weights were updated both errors rapidly decline in the early epochs and subsequently stabilize by means of the early stopping method the set of synaptic weights was collected at epoch 130 where the minimum of the validation error was observed fig 10 b presents results from the test step considering that in this case the ann outputs were rounded towards 1 or 0 it can be verified that there was only one classification error as a false positive fig 11 presents the confusion matrix the diagonal cells show the number and percentage of correct classifications which amount 93 3 for the test set of the case study further more meaningful measures can be extracted from the confusion matrix to illustrate the results such as accuracy precision recall the f measure and cohen s kappa coefficient for these measures higher values close to 1 0 indicate better models witten et al 2011 using the test set we obtained values for the accuracy of 0 95 precision of 1 0 recall of 0 86 and f measure of 0 92 considering that cohen s kappa coefficient is a more conservative metric the value obtained is within the range of the almost perfect agreement class landis and koch 1977 fig 12 presents the roc receiver operational characteristic curve which illustrate the auc area under curve for the ann trained in the case study which amounts to 0 96 for a perfect classifier the auc equals 1 0 and for an auc in the interval 0 9 1 0 the classifier quality is regarded as excellent concerning the sensitivity analysis performed by the module after the training process the parameters that had major influence on the susceptibility results were in descending order slope topographic wetness index land use and elevation table 1 4 4 2 landslide susceptibility map fig 13 presents the landslide susceptibility map for the municipality of porto alegre brazil generated by the r landslide module to the left and the map published by the brazilian geological survey to the right the landslide susceptibility classes high medium low were created from the definition of susceptibility thresholds that take into account two factors i the ann outputs and ii the proportion of landslide records that must belong to that class for example in this study we required that the high susceptibility class red areas must contain at least 65 of the landslide events under its classification while the medium and low susceptibility classes are required to have at least 25 and at most 10 respectively from these percentages two decision thresholds within the interval 0 1 are defined that separate the classes one threshold separates the low medium classes and the other the medium high this procedure was applied to the application data from the landslide database and resulted in threshold 0 50 and 0 95 for porto alegre by comparing the maps it can be shown that they hold strong qualitative and quantitative similarities a direct comparison between the matrices representing the maps shows that they agree to a great extent in the classification of susceptibility as low medium and high from the susceptibility map generated by the module a total area of 410 75 km2 88 05 is located in areas of low susceptibility 40 73 km2 8 73 in areas of medium susceptibility and 15 05 km2 3 22 in regions of high susceptibility compared with the cprm map cprm companhia de pesquisa de recursos minerais 2015 392 60 km2 83 98 66 62 km2 14 25 and 8 27 km2 1 77 are located in regions of low medium and high susceptibility respectively for landslide records not seen during the training validation and test steps the so called unseen records the ann was able to identify highly susceptible areas by means of the features of the set of thematic parameters applied as inputs as the unseen records were overlaid in both susceptibility maps it was perceived that the landslide points were located in areas to which the module had correctly assigned high susceptibility class by comparing the susceptibility map from the module to the susceptibility map by cprm it can be shown that the results obtained by means of the r landslide module outperformed those from the official publication by cprm as such it can be concluded that the map generated by the r landslide module has an increased level of hits in correctly assigning high susceptibility class to unseen landslide points which indicated an improved accuracy of the ann map as compared to the cprm map this can be seen from the histograms presented in fig 14 5 discussions the results obtained from r landslide module in the case study showed that the tool was able to generate satisfactory results when compared to the official publication by cprm cprm companhia de pesquisa de recursos minerais 2015 and also when the map was evaluated against unseen landslide records this was accomplished by means of available parameters and data which were acquired in open and free datasets the comparison between the results from r landslide and the map elaborated by cprm showed that the susceptibility map by r landslide outperformed that of cprm in correctly assigning high susceptibility classification to landslide areas this was confirmed by means of unseen records of real landslide data such that it is a well established conclusion thus the maps provided by the module using a trained ann featured most of the records correctly classified in the high susceptibility class in this regard we remark that some landslide events located in areas that were classified by the ann as high susceptibility class are located in low susceptibility areas according to the study by cprm these errors are classified as false negatives which are undesirable errors since they can lead to the inadvertent masking of susceptible areas as compared to previous studies of landslide susceptibility in porto alegre de brito et al 2017 found the auc of 0 96 thus the same value that was found in our study this shows that both models feature high quality in predicting landslides from the input data it is important to recall that de brito et al 2017 reported that the high value of the auc was obtained by means of expert support who helped in the assignment of the contribution and relative importance of conditioning factors upon the occurrence of landslides in this sense we showed that r landslide and its backbone methodology could provide an equivalent level of prediction quality with added scalability and reproducibility since it does not rely on expert knowledge in order to get hold of landslide susceptibility mapping this advantage is especially important in what concerns the evaluation of the influence of conditioning factors which are part of the learning process and completely data driven now it should be remarked that while expert knowledge is not required in order to set off landslide susceptibility mapping by means of the r landslide module expert knowledge and evaluations can be easily taken into account in the learning process by turning them into one or more extra input variables for the module to work with when comparing the results achieved in this study with studies that applied different methodologies for landslide susceptibility mapping the auc value 0 96 obtained in the case study can be considered high chen et al 2018b d 2019a pandey and sharma 2017 zêzere et al 2017 francipane et al 2014 hong et al 2016 de brito et al 2017 for instance chen et al 2018d evaluated three state of the art data mining techniques best first decision tree random forest and naïve bayes tree for landslide susceptibility modeling the random forest method presented the best performance with auc value of 0 87 an accuracy value of 0 77 precision and f measure of 0 66 in other study that involved an adaptive neuro fuzzy inference system anfis optimized by shuffled frog leaping algorithm sfla and particle swarm optimization pso chen et al 2019a the authors found auc value of 0 89 for the training and testing dataset a similar research that also applied the anfis methodology with different statistical bivariate methods frequency ratio fr and weights of evidence woe obtained the highest auc value with fr anfis ensemble 0 85 in turn chen et al 2018b who studied ensembles of bivariate statistical methods based kernel logistic regression classifiers evidential belief function ebf statistical index si and woe and found that the ebf klr method had the highest predictive capability auc values of 0 81 and 0 75 for the training and validation datasets respectively other studies that also applied ensembles of different techniques obtained similar results for example tien bui et al 2016 used functional trees classifier with adaboost bagging and multiboost ensembles the authors found auc values ranging from 0 92 to 0 96 chen et al 2018c studied a bagging based kernel logistic regression model to mapping landslide susceptibility with this approach the model obtained a auc value of 0 85 further we compile some previous studies that applied artificial neural networks to identify susceptible landslide areas pham et al 2017 zare et al 2013 tsangaratos and benardos 2014 conforti et al 2014 pascale et al 2013 pham et al 2017 obtained a maximum auc value of 0 89 with the multiboost ensemble model zare et al 2013 achieved a value of 0 92 for the same metric with a multi layer perceptron tsangaratos and benardos 2014 obtained an accuracy value of 0 89 a precision of 0 91 a recall of 0 88 and a f measure value of 0 89 using the validation dataset of an ann algorithm in a broader sense the results indicate that the application of anns in landslide susceptibility mapping features numerous advantages such as the ability to detect complex nonlinear relations between variables the ability to detect interactions between predictor variables and the ability to process data at different scales and units of measure tu 1996 kanungo et al 2006 regarding the strengths of the methodology we highlight that anns are computationally fast and suitable for parallel computing platforms since training test and application can be parallelized further anns and machine learning methods in general allow expert knowledge to be considered in the learning process such that the experience of specialists can be taken into account in a straightforward manner this feature is not as straightforward in methodologies based on logistic regression for example ozdemir and altural 2013 hosmer and lemeshow 2000 wang et al 2013 bai et al 2010 as a black box modeling approach anns do not require deep understanding of the underlying physical mechanisms of the phenomenon under study however also have limited contribution to the understanding of its underlying mechanisms this feature is shared by other quantitative methods chen et al 2017a 2018d b 2019b c concerning the performance past studies showed that anns achieved better results as compared with other methods chen et al 2017b yilmaz 2010 for example yilmaz 2010 compared different gis based landslide susceptibility mapping methods by comparing the resulting auc values the authors showed that the ann model provided better accuracy than any of the other methods considered conditional probability logistic regression and support vector machines in turn chen et al 2017b studied three different machine learning methods maximum entropy maxent support vector machine svm and artificial neural network ann and their ensembles as a result in the case of validating the models separately the ann presented the best performance among them the results from the case study for porto alegre brazil indicate that anns indeed have superior performance having achieved auc of 0 96 and thus higher than those found in the aforementioned studies nevertheless some issues may be considered as limiting factors such as the dependence of the methodology on the existence of local landslide databases this may restrict the applicability of the tool in some regions of the world where the process of creating maintaining and updating landslide databases is not a well consolidated practice for this reason it is suggested to the public agencies responsible for managing natural disasters to create maintain and update databases with information on landslides and make them available for the wide public this would facilitate the application of methodologies based on artificial intelligence to the identification of landslide susceptible areas as well as increase the reliability of the results the application of this methodology still allows the evaluation of the sensitivity of the output to the conditioning factors of susceptibility response to landslides the parameters that exerted the greatest influence in our case study were slope topographic wetness index land use and elevation a study carried out by de brito et al 2016 identified that the anthropic intervention ends up being the main agent of landslides in the urban area of porto alegre mainly due to the execution of excessive cuts of slopes excavations in bases of slopes leakage of pipes and cultivation of inappropriate species still according to de brito et al 2016 82 of the recorded mass movements occurred in places with irregular occupations in these conditions the most sensitive parameters are related to interferences observed and raised by de brito et al 2016 especially the slope due to cuts and excavations and land use due to irregular occupations other authors have also observed such parameters as factors that affect the occurrence of slides chen et al 2018a ilia and tsangaratos 2016 chen et al 2017a 2018c tien bui et al 2016 regarding the strengths of the module it packs a robust methodology based on anns in an intuitive gis add on and offers an user friendly interface that can promote its widespread application among final users further the open source nature of the software promotes its collaborative development towards improved versions concerning the production of knowledge on the mechanisms and triggers of landslides having as the backbone a data driven methodology the module can help in the identification of the different conditioning factors to landslides by means of sensitivity analysis this allows the identification of the parameters that exert greater influence on the occurrence of landslides at a given location or area thus also allowing that preventive measures be taken more effectively in the same way the flexible ann framework allows that input parameters be modified or updated at any time as new data and additional information becomes available this allows further training to be performed while different types of landslides exist varnes 1958 our research aims at validating a new grass gis module to assess general landslide susceptibility maps as a result of the ann training for a given study area the ann will be capable of identifying susceptibility in such area by searching for the same types of landslides it is familiar with due to training it is important to highlight that the module presented in this paper could be used by public agencies as support for decision making in order to help identify susceptible areas more quickly to promote a detailed and customized analysis of regions of interest to support decision making and the working and triggering of early warning systems in addition it can serve as an auxiliary tool for the zoning of urban land use 6 conclusions in this article we presented the r landslide module which is a gis implementation of artificial neural networks for the mapping of landslide susceptibility the module was written in python language as an add on to grass gis software and is compatible with other existing grass modules it provides the user with tools for training and application of anns for this purpose a case study was presented for the municipality of porto alegre brazil from which the use and characteristics of the module were demonstrated as well as its interface and capabilities in surveying susceptible areas we remark that the module can be advantageously used as an initial step towards the development of a support system that can make the process of landslide susceptibility mapping more agile and effective on the elaboration of future studies we propose the integration of the module and anns to the dynamic mapping of the susceptibility to landslides in real time from precipitation information from automatic gauge stations this would be intended to support early warning systems for events that are triggered by rainfall to facilitate the application of this methodology in the near future it is recommended to public agencies to create maintain and update reliable landslide databases the code for module is open source and can be freely downloaded from https github com uffsenvmodelling r landslide bragagnolo et al 2019 acknowledgment the authors thank capes for the support 
