index,text
26165,ecological processes may exhibit memory to past disturbances affecting the resilience of ecosystems to future disturbance understanding the role of ecological memory in shaping ecosystem responses to disturbance under global change is a critical step toward developing effective adaptive management strategies to maintain ecosystem function and biodiversity we developed ecomem an r package for quantifying ecological memory functions using common environmental time series data continuous count proportional applying a bayesian hierarchical framework the package estimates memory functions for continuous and binary e g disturbance chronology variables making no a priori assumption on the form of the functions ecomem allows users to quantify ecological memory for a wide range of ecosystem processes and responses the utility of the package to advance understanding of the memory of ecosystems to environmental drivers is demonstrated using a simulated dataset and a case study assessing the memory of boreal tree growth to insect defoliation keywords bayesian hierarchical model disturbance ecomem ecosystem resilience r package time series availability of software name of software ecomem type of software add on package for r https cran r project org first available 2019 program languages r requires r version 3 5 0 or later including developer tools license gpl 2 code repository https github com msitter ecomem git installation in r devtools install github msitter ecomem developer malcolm s itter malcolm itter helsinki fi contact address research centre for ecological change pl 65 viikinkaari 1 00014 university of helsinki finland 1 introduction ecological processes may exhibit memory to past conditions that is the current function of an ecosystem may be affected by exogenous e g weather disturbance management and endogenous e g successional stages community composition functional diversity factors over a range of past time points ecological memory is often used in the context of ecological responses to disturbance and the formation of resilient ecosystems gunderson 2000 johnstone et al 2016 in this context ecological memory is defined as the degree to which an ecosystem s response to a current or future disturbance is shaped by its responses to past disturbances padisák 1992 peterson 2002 this includes biological legacies such as changes in the structure function and diversity of a system following disturbance johnstone et al 2016 it also includes persistent responses to a disturbance which may limit the capacity of the system to respond to future disturbance events anderegg et al 2015 ecological memory has been demonstrated in forest systems anderegg et al 2015 freshwater phytoplankton padisák 1992 coral reefs nyström and folke 2001 and grasses walter et al 2011 resilient ecosystems are well adapted to regional disturbance regimes johnstone et al 2016 as disturbance events become more frequent and or severe under global change systems may accumulate stress over time such that future disturbance has an unexpectedly profound effect on ecosystem function by accounting for the legacy effects of disturbance ecological memory provides a mechanism to quantify the accumulation of stress within an ecosystem over repeated disturbance events identifying ecosystem attributes limiting the effects of disturbance over time as reflected by ecological memory is a critical step in the development of adaptive management strategies aimed at promoting resilient ecosystems under novel conditions folke et al 2004 ogle et al 2015 were the first to provide an integrated approach to quantify ecological memory they define memory as comprising three components i the length of an ecosystem s response to previous conditions ii the relative importance of conditions at specific times in the past and iii the strength of an ecosystem s response to temporally integrated conditions the bayesian hierarchical model developed by ogle et al 2015 provides a flexible framework to quantify each of the components of ecological memory we extend the bayesian hierarchical model developed by ogle et al 2015 and integrate the model into a new r package ecomem for application to a wide range of ecological time series data extensions to the model framework presented in ogle et al 2015 include developing an efficient and flexible spline based approach to quantify ecological memory and allowing for estimation of memory to both continuous covariates and binary event data e g a disturbance chronology 2 methods 2 1 setting ecological memory is quantified through the estimation of latent weights reflecting the relative importance of past conditions on current ecosystem function ogle et al 2015 these weights are used to construct temporally averaged covariates to model ecological processes suppose we are interested in an ecological process y observed over a range of time points y y 1 y 2 y t coincidentally we observe values for a set of p covariates believed to impact the process of interest represented by a t p matrix x where the tth row is given by x t x t 1 x t 2 x t p for t 1 t a common approach is to construct a model to estimate the process as a function of concurrent conditions g e y t x f x t θ where g is a link function e g identity logit e indicates the expected value and θ is a set of potentially unknown model parameters in the case where the ecological process is thought to depend on conditions over a series of past time points g e y t x can be estimated as a function of lagged covariate values for example in a regression model 1 g e y t x μ ℓ 0 l x t ℓ α ℓ where μ is an intercept ℓ indicates the time lag up to a maximum l and α ℓ is a p dimensional vector of regression coefficients corresponding to the ℓ th lag we assume that all covariates have the same maximum lag in eq 1 for simplicity although this need not be the case eq 1 defines a distributed lag model zanobetti et al 2000 heaton and peng 2012 when l is large or the lagged covariate values x j t x j t 1 x j t l are correlated α j α j 0 α j 1 α j l is modeled jointly taking into account the covariance among coefficients an alternative approach in such settings first presented by ogle et al 2015 is to filter covariate observations x j equivalent to the jth column of x j 1 p based on weights representing the relative importance of past conditions on the current process 2 x j t ℓ 0 l w j ℓ x j t ℓ where x j t is the filtered value of x j at time t and w j ℓ is the weight corresponding to the t ℓ th lag of x j the temporally filtered covariates are then be applied to model the ecological process at time t 3 g e y t x μ x t β where x t and β are p dimensional vectors of filtered covariate values and regression coeffecients respectively eq 2 represents the construction of a temporally filtered covariate in discrete time but can be extended to continuous time constraints are placed on the weights in eq 2 to ensure their idenfiability i w j ℓ 0 1 for ℓ 0 1 l ii ℓ 0 l w j ℓ 1 note that without these constraints eq 3 is identical to the distributed lag model with α j β j w j given w j w j 0 w j 1 w j l the set of weights for a given covariate w j defines an ecological memory function with several interpretations the number of lags with weights above a specified lower bound e g 0 01 indicates the length of a process memory to the covariate in practice the lower bound should be set to the minimum weight value producing a practical effect of the covariate on the mean of the response the magnitude of weights indicate the relative importance of covariate values at specific lags the temporally filtered covariates are similar in construction and interpretation to spatially averaged covariates for use in regression models heaton and gelfand 2011 and reflect the accumulation of covariate values over the period 0 l the latent weights w j can be difficult to estimate given they are not placed directly on the response and may have complex correlation structure see following section 2 2 model framework the initial approach to quantify ecological memory functions jointly estimates weights and fits a linear regression model using temporally filtered covariates within a bayesian hierarchical framework ogle et al 2015 weights are assigned a non informative dirichlet prior w j d 1 in cases where l is large it may be difficult to identify weights for all lags we can improve weight estimation by reducing the dimension of the parameter space and imposing greater structure on the weights which are likely to exhibit high temporal autocorrelation gaussian processes and penalized splines have been successfully applied to estimate coefficients in distributed lag models zanobetti et al 2000 heaton and peng 2012 and are natural candidates to estimate weights w j given their close connection to lagged coefficients α j we apply a bayesian hierarchical model utilizing penalized regression splines to estimate ecological memory functions specifically weights are estimated as 4 w j e h j η j 1 e h j η j where h j is an l 1 k matrix containing k spline basis function evaluations for each lag η j is a k dimensional vector of basis function coefficients 1 is an l 1 dimensional vector of ones and e defines a point wise operation modeling weights on the log scale and the sum in the denominator term in eq 4 ensure the identifiability constraints are met consistent with penalized spline models knots are placed within 0 l to model w j wood and augustin 2002 regularization is used to avoid overfitting and ensure the identifiability of the spline basis function coefficients η j we define a weakly informative prior for the basis function coefficients η j n 0 τ j 2 s j where τ j 2 is a scalar variance parameter s j is a k k penalty matrix based on basis functions and knot locations and indicates the generalized inverse as s j may not be full rank the variance τ j 2 serves as the regulator controlling the relative smoothness of the weights w j the weakly informative prior for η j is a form of shrinkage prior when τ j 2 is small the weight function is smooth identifying optimal regulator values τ j 2 is crucial for estimating meaningful weight functions w j defining prior distributions for regulator parameters in bayesian penalized spline models is an active area of research recent work has proposed a set of priors for τ j 2 that incorporate a penalty for model complexity ventrucci and rue 2016 simpson et al 2017 in the context of ecological memory functions we have found a folded t distribution assigned to the square root of the regulator τ j allows for sufficiently flexible basis function coefficients while controlling against overfitting similar to a penalized complexity prior we use markov chain monte carlo mcmc to sample from the joint posterior distribution for the ecological memory model after specifying prior distributions for remaining model parameters robert and casella 2004 details on the priors used for all model parameters and the mcmc procedure including efficient sampling considerations are provided in appendix a 3 results the ecomem package allows users to fit the model defined in section 2 the development version of the package is available on github and can be installed using devtools wickham et al 2018 a list of the core functions available within ecomem is provided in table 1 the current ecological memory model framework supports continuous count and proportional data utilizing gaussian poisson and binomial likelihoods respectively in all cases the mean of the data is estimated according to eq 3 3 1 simulated example we demonstrate the steps necessary to quantify ecological memory using the ecomem package through its application to a simulated dataset the mem dat dataset is included as part of the ecomem package and contains poisson count data y poisson λ that have been generated applying the ecological memory model using a log link function of two memory covariates v1 v2 and an auxiliary continuous covariate v3 defined in r syntax log λ μ β 1 v 1 β 2 v 2 β 3 v 3 β 4 v 1 v 2 β 5 v 2 v 3 where μ is an intercept term and the β s are regression coefficients as defined in eq 3 code to generate the simulated dataset is provided in appendix b the v2 variable is continuous while v1 is binary indicating the occurrence of discrete events 0 no event 1 event 3 1 1 fitting the model we apply ecomemglm to fit the ecological memory model to the poisson count data the ecomemglm call requires users to specify a linear model formula the likelihood function for the data poisson or binomial a model data frame including the response explanatory variables and all auxiliary variables a subset of covariates for which memory functions should be estimated the maximum lag for each memory covariate as well as time and if applicable group identifiers the group identifier is used if time series data exist for separate groups the model is fit using response data for which explanatory variable observations exist for at least max l previous time points image 1 3 1 2 model outputs the ecomemglm and ecomem functions return a list of class ecomem including posterior samples for each mcmc chain and the data used to fit the model continuous covariates are standardized to have mean zero and variance one prior to model fitting the posterior samples can be converted into an mcmc object using the mem2mcmc function allowing users to apply the coda package to assess convergence plummer et al 2006 the marginal posterior distribution for each ecological memory model parameter can be summarized using the memsum function see appendix b finally ecological memory functions can be plotted for each memory covariate using the plotmem function fig 1 image 2 3 2 case study boreal tree growth the utility of the ecomem package for ecological inference is demonstrated through a case study assessing the memory of boreal tree growth to insect defoliation events recent work has shown that boreal tree growth exhibits negative responses to insect defoliation for several years following a moderate to severe defoliation event itter et al 2018 we apply the ecomem package to annual tree growth and insect defoliation survey data for 34 sites across alberta canada to assess the ecological memory of tree growth to defoliation see itter et al 2018 for detailed description of dataset we model mean annual basal area increment of trembling aspen populus tremuloides michx as a function of forest tent caterpillar malascosoma disstria hub defoliation events and mean tree age allowing for memory to defoliation using a gaussian likelihood additional details in appendix b image 3 there is strong evidence of ecological memory in boreal tree growth responses to past forest tent caterpillar defoliation posterior mean weight values above 0 01 exist for lags 0 to 8 reflecting persistent impacts of defoliation on mean annual basal area increment fig 2 weighted defoliation event observations were negatively related to the mean annual basal area increment of aspen trees within study sites fig 3 the effect of defoliation on basal area increment is much less pronounced if an alternative linear regression model is applied that does not account for ecological memory to defoliation fig 3 the stronger effect of defoliation observed after accounting for ecological memory likely reflects the accumulation of defoliation stress on aspen growth over time 4 limitations extensions high autocorrelation in a memory covariate may lead to poorly identified ecological memory functions this is particularly evident for covariates that are smooth functions in time with high autocorrelation coefficients for more than 5 10 lags future work will focus on modeling weights orthogonal to covariate values reducing potential temporal confounding attributable to high autocorrelation in memory covariates sensu hanks et al 2015 the current version of the ecomem package requires users to specify a maximum lag value l for each memory covariate in some applications the length of memory may be the main inferential goal estimating l within the bayesian hierarchical model presented in section 2 2 is complicated given realizations of l define a unique set of basis functions used to estimate weights we are actively working to estimate l as part of ecomem 5 conclusion ecological memory and its role in shaping ecosystem responses to global change is an important area of ecological research we designed the ecomem package to provide ecologists with easily implemented tools to test and account for ecological memory within their respective study systems we hope that accounting for ecological memory when it is present will lead to improved understanding of the factors contributing to resistent and resilient ecosystem function in the face of changing global conditions users are encouraged to apply ecomem widely and report any bugs issues or desired extensions on our active issues page https github com msitter ecologicalmemory acknowledgements this work was supported by the jane and astos erkko foundation and by the national science foundation grant numbers ef 1137309 ef 1241874 ef 1253225 the authors acknowledge kiona ogle and co authors of ogle et al 2015 for their work developing an initial ecological memory model framework appendix a supplementary data the following are the supplementary data to this article appendix b annotated code for simulated and applied case study examples appendix b appendix a ecological memory model inference appendix a multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 004 
26165,ecological processes may exhibit memory to past disturbances affecting the resilience of ecosystems to future disturbance understanding the role of ecological memory in shaping ecosystem responses to disturbance under global change is a critical step toward developing effective adaptive management strategies to maintain ecosystem function and biodiversity we developed ecomem an r package for quantifying ecological memory functions using common environmental time series data continuous count proportional applying a bayesian hierarchical framework the package estimates memory functions for continuous and binary e g disturbance chronology variables making no a priori assumption on the form of the functions ecomem allows users to quantify ecological memory for a wide range of ecosystem processes and responses the utility of the package to advance understanding of the memory of ecosystems to environmental drivers is demonstrated using a simulated dataset and a case study assessing the memory of boreal tree growth to insect defoliation keywords bayesian hierarchical model disturbance ecomem ecosystem resilience r package time series availability of software name of software ecomem type of software add on package for r https cran r project org first available 2019 program languages r requires r version 3 5 0 or later including developer tools license gpl 2 code repository https github com msitter ecomem git installation in r devtools install github msitter ecomem developer malcolm s itter malcolm itter helsinki fi contact address research centre for ecological change pl 65 viikinkaari 1 00014 university of helsinki finland 1 introduction ecological processes may exhibit memory to past conditions that is the current function of an ecosystem may be affected by exogenous e g weather disturbance management and endogenous e g successional stages community composition functional diversity factors over a range of past time points ecological memory is often used in the context of ecological responses to disturbance and the formation of resilient ecosystems gunderson 2000 johnstone et al 2016 in this context ecological memory is defined as the degree to which an ecosystem s response to a current or future disturbance is shaped by its responses to past disturbances padisák 1992 peterson 2002 this includes biological legacies such as changes in the structure function and diversity of a system following disturbance johnstone et al 2016 it also includes persistent responses to a disturbance which may limit the capacity of the system to respond to future disturbance events anderegg et al 2015 ecological memory has been demonstrated in forest systems anderegg et al 2015 freshwater phytoplankton padisák 1992 coral reefs nyström and folke 2001 and grasses walter et al 2011 resilient ecosystems are well adapted to regional disturbance regimes johnstone et al 2016 as disturbance events become more frequent and or severe under global change systems may accumulate stress over time such that future disturbance has an unexpectedly profound effect on ecosystem function by accounting for the legacy effects of disturbance ecological memory provides a mechanism to quantify the accumulation of stress within an ecosystem over repeated disturbance events identifying ecosystem attributes limiting the effects of disturbance over time as reflected by ecological memory is a critical step in the development of adaptive management strategies aimed at promoting resilient ecosystems under novel conditions folke et al 2004 ogle et al 2015 were the first to provide an integrated approach to quantify ecological memory they define memory as comprising three components i the length of an ecosystem s response to previous conditions ii the relative importance of conditions at specific times in the past and iii the strength of an ecosystem s response to temporally integrated conditions the bayesian hierarchical model developed by ogle et al 2015 provides a flexible framework to quantify each of the components of ecological memory we extend the bayesian hierarchical model developed by ogle et al 2015 and integrate the model into a new r package ecomem for application to a wide range of ecological time series data extensions to the model framework presented in ogle et al 2015 include developing an efficient and flexible spline based approach to quantify ecological memory and allowing for estimation of memory to both continuous covariates and binary event data e g a disturbance chronology 2 methods 2 1 setting ecological memory is quantified through the estimation of latent weights reflecting the relative importance of past conditions on current ecosystem function ogle et al 2015 these weights are used to construct temporally averaged covariates to model ecological processes suppose we are interested in an ecological process y observed over a range of time points y y 1 y 2 y t coincidentally we observe values for a set of p covariates believed to impact the process of interest represented by a t p matrix x where the tth row is given by x t x t 1 x t 2 x t p for t 1 t a common approach is to construct a model to estimate the process as a function of concurrent conditions g e y t x f x t θ where g is a link function e g identity logit e indicates the expected value and θ is a set of potentially unknown model parameters in the case where the ecological process is thought to depend on conditions over a series of past time points g e y t x can be estimated as a function of lagged covariate values for example in a regression model 1 g e y t x μ ℓ 0 l x t ℓ α ℓ where μ is an intercept ℓ indicates the time lag up to a maximum l and α ℓ is a p dimensional vector of regression coefficients corresponding to the ℓ th lag we assume that all covariates have the same maximum lag in eq 1 for simplicity although this need not be the case eq 1 defines a distributed lag model zanobetti et al 2000 heaton and peng 2012 when l is large or the lagged covariate values x j t x j t 1 x j t l are correlated α j α j 0 α j 1 α j l is modeled jointly taking into account the covariance among coefficients an alternative approach in such settings first presented by ogle et al 2015 is to filter covariate observations x j equivalent to the jth column of x j 1 p based on weights representing the relative importance of past conditions on the current process 2 x j t ℓ 0 l w j ℓ x j t ℓ where x j t is the filtered value of x j at time t and w j ℓ is the weight corresponding to the t ℓ th lag of x j the temporally filtered covariates are then be applied to model the ecological process at time t 3 g e y t x μ x t β where x t and β are p dimensional vectors of filtered covariate values and regression coeffecients respectively eq 2 represents the construction of a temporally filtered covariate in discrete time but can be extended to continuous time constraints are placed on the weights in eq 2 to ensure their idenfiability i w j ℓ 0 1 for ℓ 0 1 l ii ℓ 0 l w j ℓ 1 note that without these constraints eq 3 is identical to the distributed lag model with α j β j w j given w j w j 0 w j 1 w j l the set of weights for a given covariate w j defines an ecological memory function with several interpretations the number of lags with weights above a specified lower bound e g 0 01 indicates the length of a process memory to the covariate in practice the lower bound should be set to the minimum weight value producing a practical effect of the covariate on the mean of the response the magnitude of weights indicate the relative importance of covariate values at specific lags the temporally filtered covariates are similar in construction and interpretation to spatially averaged covariates for use in regression models heaton and gelfand 2011 and reflect the accumulation of covariate values over the period 0 l the latent weights w j can be difficult to estimate given they are not placed directly on the response and may have complex correlation structure see following section 2 2 model framework the initial approach to quantify ecological memory functions jointly estimates weights and fits a linear regression model using temporally filtered covariates within a bayesian hierarchical framework ogle et al 2015 weights are assigned a non informative dirichlet prior w j d 1 in cases where l is large it may be difficult to identify weights for all lags we can improve weight estimation by reducing the dimension of the parameter space and imposing greater structure on the weights which are likely to exhibit high temporal autocorrelation gaussian processes and penalized splines have been successfully applied to estimate coefficients in distributed lag models zanobetti et al 2000 heaton and peng 2012 and are natural candidates to estimate weights w j given their close connection to lagged coefficients α j we apply a bayesian hierarchical model utilizing penalized regression splines to estimate ecological memory functions specifically weights are estimated as 4 w j e h j η j 1 e h j η j where h j is an l 1 k matrix containing k spline basis function evaluations for each lag η j is a k dimensional vector of basis function coefficients 1 is an l 1 dimensional vector of ones and e defines a point wise operation modeling weights on the log scale and the sum in the denominator term in eq 4 ensure the identifiability constraints are met consistent with penalized spline models knots are placed within 0 l to model w j wood and augustin 2002 regularization is used to avoid overfitting and ensure the identifiability of the spline basis function coefficients η j we define a weakly informative prior for the basis function coefficients η j n 0 τ j 2 s j where τ j 2 is a scalar variance parameter s j is a k k penalty matrix based on basis functions and knot locations and indicates the generalized inverse as s j may not be full rank the variance τ j 2 serves as the regulator controlling the relative smoothness of the weights w j the weakly informative prior for η j is a form of shrinkage prior when τ j 2 is small the weight function is smooth identifying optimal regulator values τ j 2 is crucial for estimating meaningful weight functions w j defining prior distributions for regulator parameters in bayesian penalized spline models is an active area of research recent work has proposed a set of priors for τ j 2 that incorporate a penalty for model complexity ventrucci and rue 2016 simpson et al 2017 in the context of ecological memory functions we have found a folded t distribution assigned to the square root of the regulator τ j allows for sufficiently flexible basis function coefficients while controlling against overfitting similar to a penalized complexity prior we use markov chain monte carlo mcmc to sample from the joint posterior distribution for the ecological memory model after specifying prior distributions for remaining model parameters robert and casella 2004 details on the priors used for all model parameters and the mcmc procedure including efficient sampling considerations are provided in appendix a 3 results the ecomem package allows users to fit the model defined in section 2 the development version of the package is available on github and can be installed using devtools wickham et al 2018 a list of the core functions available within ecomem is provided in table 1 the current ecological memory model framework supports continuous count and proportional data utilizing gaussian poisson and binomial likelihoods respectively in all cases the mean of the data is estimated according to eq 3 3 1 simulated example we demonstrate the steps necessary to quantify ecological memory using the ecomem package through its application to a simulated dataset the mem dat dataset is included as part of the ecomem package and contains poisson count data y poisson λ that have been generated applying the ecological memory model using a log link function of two memory covariates v1 v2 and an auxiliary continuous covariate v3 defined in r syntax log λ μ β 1 v 1 β 2 v 2 β 3 v 3 β 4 v 1 v 2 β 5 v 2 v 3 where μ is an intercept term and the β s are regression coefficients as defined in eq 3 code to generate the simulated dataset is provided in appendix b the v2 variable is continuous while v1 is binary indicating the occurrence of discrete events 0 no event 1 event 3 1 1 fitting the model we apply ecomemglm to fit the ecological memory model to the poisson count data the ecomemglm call requires users to specify a linear model formula the likelihood function for the data poisson or binomial a model data frame including the response explanatory variables and all auxiliary variables a subset of covariates for which memory functions should be estimated the maximum lag for each memory covariate as well as time and if applicable group identifiers the group identifier is used if time series data exist for separate groups the model is fit using response data for which explanatory variable observations exist for at least max l previous time points image 1 3 1 2 model outputs the ecomemglm and ecomem functions return a list of class ecomem including posterior samples for each mcmc chain and the data used to fit the model continuous covariates are standardized to have mean zero and variance one prior to model fitting the posterior samples can be converted into an mcmc object using the mem2mcmc function allowing users to apply the coda package to assess convergence plummer et al 2006 the marginal posterior distribution for each ecological memory model parameter can be summarized using the memsum function see appendix b finally ecological memory functions can be plotted for each memory covariate using the plotmem function fig 1 image 2 3 2 case study boreal tree growth the utility of the ecomem package for ecological inference is demonstrated through a case study assessing the memory of boreal tree growth to insect defoliation events recent work has shown that boreal tree growth exhibits negative responses to insect defoliation for several years following a moderate to severe defoliation event itter et al 2018 we apply the ecomem package to annual tree growth and insect defoliation survey data for 34 sites across alberta canada to assess the ecological memory of tree growth to defoliation see itter et al 2018 for detailed description of dataset we model mean annual basal area increment of trembling aspen populus tremuloides michx as a function of forest tent caterpillar malascosoma disstria hub defoliation events and mean tree age allowing for memory to defoliation using a gaussian likelihood additional details in appendix b image 3 there is strong evidence of ecological memory in boreal tree growth responses to past forest tent caterpillar defoliation posterior mean weight values above 0 01 exist for lags 0 to 8 reflecting persistent impacts of defoliation on mean annual basal area increment fig 2 weighted defoliation event observations were negatively related to the mean annual basal area increment of aspen trees within study sites fig 3 the effect of defoliation on basal area increment is much less pronounced if an alternative linear regression model is applied that does not account for ecological memory to defoliation fig 3 the stronger effect of defoliation observed after accounting for ecological memory likely reflects the accumulation of defoliation stress on aspen growth over time 4 limitations extensions high autocorrelation in a memory covariate may lead to poorly identified ecological memory functions this is particularly evident for covariates that are smooth functions in time with high autocorrelation coefficients for more than 5 10 lags future work will focus on modeling weights orthogonal to covariate values reducing potential temporal confounding attributable to high autocorrelation in memory covariates sensu hanks et al 2015 the current version of the ecomem package requires users to specify a maximum lag value l for each memory covariate in some applications the length of memory may be the main inferential goal estimating l within the bayesian hierarchical model presented in section 2 2 is complicated given realizations of l define a unique set of basis functions used to estimate weights we are actively working to estimate l as part of ecomem 5 conclusion ecological memory and its role in shaping ecosystem responses to global change is an important area of ecological research we designed the ecomem package to provide ecologists with easily implemented tools to test and account for ecological memory within their respective study systems we hope that accounting for ecological memory when it is present will lead to improved understanding of the factors contributing to resistent and resilient ecosystem function in the face of changing global conditions users are encouraged to apply ecomem widely and report any bugs issues or desired extensions on our active issues page https github com msitter ecologicalmemory acknowledgements this work was supported by the jane and astos erkko foundation and by the national science foundation grant numbers ef 1137309 ef 1241874 ef 1253225 the authors acknowledge kiona ogle and co authors of ogle et al 2015 for their work developing an initial ecological memory model framework appendix a supplementary data the following are the supplementary data to this article appendix b annotated code for simulated and applied case study examples appendix b appendix a ecological memory model inference appendix a multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 004 
26166,identifiability is a fundamental concept in parameter estimation and therefore key to the large majority of environmental modeling applications parameter identifiability analysis assesses whether it is theoretically possible to estimate unique parameter values from data given the quantities measured conditions present in the forcing data model structure and objective function and properties of errors in the model and observations in other words it tackles the problem of whether the right type of data is available to estimate the desired parameter values identifiability analysis is therefore an essential technique that should be adopted more routinely in practice alongside complementary methods such as uncertainty analysis and evaluation of model performance this article provides an introductory overview to the topic we recommend that any modeling study should document whether a model is non identifiable the source of potential non identifiability and how this affects intended project outcomes graphical abstract image 1 keywords identifiability response surface non uniqueness derivative based methods hessian emulation uncertainty learning objectives appreciate key concepts and methods of parameter identifiability analysis recognise the main consequences of parameter non identifiability i e the inability to infer unique parameters from data distinguish between different sources of parameter non uniqueness and how they influence identifiability understand that non uniqueness can occur even with ideal and or noise free data recognise non identifiability can be due to model structure and equations alone understand how non uniqueness in parameter estimation may relate to multiple optima and or flatness in the response surface investigate potential multiple optima and flatness using visualization derivatives and related indicators understand that noise in data can affect identifiability appreciate issues affecting use of identifiability analysis including computational considerations 1 introduction what is identifiability analysis models are widely used for understanding management and scenario analysis of environmental and other systems increasingly for social learning among stakeholders and in support of decision making kelly letcher et al 2013 the ultimate aim is to utilize data and knowledge about a system to help the modeler decision makers and other stakeholders make sense of how the system works how it may change in future and how it may respond to management actions and other perturbations in an ideal world data and knowledge about a system of interest would be sufficiently complete such that the model of that system would become an oracle that can be trusted to always provide the right answer in reality we typically have incomplete knowledge and data to adequately conceptualise and simulate a system and even in cases where we have an adequate understanding of the general principles governing the system we can typically only approximate its complexity spatial heterogeneity and temporal variability tackling this lack of certainty requires a pragmatic approach to managing information i e data or knowledge that informs understanding of the system underpinned by effective modeling practice e g badham et al 2019 and driven by the purpose of the analysis and the resources available it is necessary to balance efforts to 1 obtain cost effective information 2 make best use of that information to manage and reduce critical controls on uncertainty and 3 understand the remaining uncertainty in order to recognise the limitations on how the model should be used to develop a model there are several steps e g jakeman et al 2006 in these steps the available information is typically used for identifying a model structure and estimating its parameters the identified model structure defines which quantities in the model state variables and parameters are known which are unknown and need to be estimated and how they are related for short we refer here to different model structures as different models we refer to a specific model as a vector function f that receives the vector of parameter values θ which is typically time invariant and estimated using data and inputs u that are system drivers and may be varying in space x and time t in explicit form 1 y x t f u x t θ where y represents model outputs that like u may vary in space and time bold notation indicates variables may be vectors rather than just scalars in general since a model is simply a tool to express what we think we know f θ u and therefore y are all likely to be uncertain every aspect of any particular model will differ from observations to some extent they are all prone to error this is a very general definition of models that covers data based theory process based and conceptual modeling approaches but for an indicative list of model families and features covered by 1 see jakeman et al 2006 p 606 we note that data based models are often used in situations where data volumes are copious compared to problems requiring theory based or conceptual models nevertheless the issue in data based modeling is also to use a model structure f and parameters θ to establish a relationship between u and y that can reproduce aspects of interest of the observed output behaviour of the model likewise with so called integrated models consisting of linked model components see kelly letcher et al 2013 for five commonly used types in environmental applications equation 1 remains relevant despite the fact that the form of f may be very complicated and parameters may be defined for each sub model separately rather than all at once parameter values θ may be measured where they correspond to observable properties of a system or estimated also known as calibration from measured outputs by invoking an objective function that optimises constrained errors between the model and measured outputs in what is referred to as an inverse problem as opposed to the forward problem of simulating the model outputs given the parameters given errors in both observations and model structure model and measured outputs do not perfectly match resulting in uncertainty in the estimated parameters and residual errors between the estimated and observed outputs there are typically trade offs between minimizing different aspects of the residual errors such that the parameter estimation task itself is commonly seen as a multi criteria problem efstratiadis and koutsoyiannis 2010 gupta et al 1998 depending on the purpose of the analysis it is recognized as good practice to work with multiple model structure hypotheses clark et al 2011 jakeman et al 2006 and to quantify the total uncertainty in outputs resulting from using multiple models uncertain parameters estimation procedures criteria and residual errors ultimately equifinality is unavoidable beven 2006 given that errors cannot be eliminated or fully characterised it is always possible to conceive of multiple different model structures and different parameter vectors that provide an acceptable fit to observed data nevertheless in the context of making best use of available information a modeler seeks to reduce uncertainty as much as is reasonably possible or required there are four key complementary methods for measuring how well uncertainty has been reduced fig 1 firstly quantifying the uncertainty in outputs or some function of them provides a direct indicator but is dependent on how well uncertainty in model structure parameters and residuals has been quantified refsgaard et al 2007 secondly comparing observed and modeled outputs can be used in several ways amongst others predictive accuracy provides a measure of model performance see bennett et al 2013 for metrics and methods the information supplied by a model can be quantified nearing and gupta 2015 if error analysis shows systematic rather than randomly distributed residuals this typically indicates a problem with the model structure which in general leads to bias in the parameter estimates evaluating model adequacy is a substantial task of its own gupta et al 2012 given one or more model structures a modeler is generally in fact primarily interested in how data has helped reduce uncertainty in parameters specifically thirdly therefore the modeler can quantify the uncertainty in parameters including the covariance describing how the parameter estimates relate to one other checchi et al 2007 thyer et al 2009 vrugt 2016 with a suitable model structure more data should typically result in smaller parameter uncertainty however this will not occur if the right type of data is not collected which is the problem best tackled by investigating parameter identifiability it is important to understand the specific contribution of parameter identifiability analysis even or especially if one does not understand how to perform it parameter identifiability analysis focuses on whether it is possible to identify a unique vector of parameter values for a given model structure or whether multiple parameter values will fit the data equally well non identifiability means the modeler does not have the information needed to choose between alternative models rothenberg 1971 large parameter uncertainty is often indicative of non identifiability and reducing parameter uncertainty indicates that the situation has improved the idea of poor identifiability is frequently approached from a parameter uncertainty perspective strictly speaking however identifiability analysis is specifically interested in the yes no question of whether unique parameter values could be identified it can be used both before and after data collection identifiability analysis before data collection can check whether the right type of data will be collected identifiability analysis after data collection can check whether the right type of data is being used in the parameter estimation process it is therefore worth noting that even when the modeler s aim is to quantify parameter uncertainty rather than to estimate a single parameter vector it is still useful to test whether a unique parameter vector could ideally be identified in order to check whether mismatch between the model structure and type of data used is contributing to parameter uncertainty the power of identifiability analysis comes from focussing on a tightly defined mathematical problem analyzing the relationship between knowns and unknowns in a given model structure there is a simple rule when solving a linear system of equations that the number of unknowns should be less than the number of observations otherwise the problem is underdetermined and ill posed and an infinite number of solutions is possible this simple rule cannot be easily applied in more complex models and identifiability analysis provides some alternatives it is common sense that a modeler should know whether their linear problem is underdetermined and what they should do about it similarly it should be common sense for modelers to know whether their model structure is non identifiable and whether it matters for the purpose of their analysis this article aims at providing an introductory overview to key ideas of identifiability analysis the field of identifiability has been extensively researched in various disciplines including numerical examples in psychology in 1919 thomson 1919 and independent early development of theory in econometrics and system identification in the 1950s and 1970s bellman and åström 1970 koopmans and reiersol 1950 there is a number of other key reviews beck 1987 dobre et al 2012 godfrey and distefano iii 1987 miao et al 2011 walter and pronzato 1996 this article differs from existing reviews by seeking to provide an accessible introduction to encourage the environmental modeling community to think more systematically and strategically about what type of information is needed to estimate parameters in their model and increase adoption of the tools of identifiability analysis the ultimate aim is therefore to improve research and management outcomes by fostering reflection on whether the selected model structure and available data are indeed appropriate to the problem at hand in section 2 this paper first discusses the sources of non identifiability within the scope of parameter identifiability analysis and then introduces fundamental concepts and methods underlying identifiability analyses building on these concepts section 3 discusses how these methods can be used in practice section 4 contains the conclusions 2 fundamental concepts and methods 2 1 sources of non identifiability this paper seeks to capture key distinctions within the existing identifiability literature we distinguish between three high level sources of parameter non uniqueness source i is the model structure including conceptualisation of the system equations used to represent it and optimisation objective function or model of the error structure where applicable see sections 2 2 and 2 3 parameter non uniqueness can occur simply because of the choice of which quantities in the model are selected for observation this is referred to as structural non identifiability the use of non identifiable equations can be determined before having any input internal state or output data after data are available identifying structural non identifiability can show that eliminating non uniqueness requires data about different quantities in the model or adoption of a different model structure that is identifiable with the type of data available source ii is the input forcing dataset activation of different dynamics within a model depends on the inputs to the model including initial and boundary conditions if dynamics related to a parameter are not activated then no information will be available to estimate that parameter this is also referred to as persistence of excitation of the model dynamics given a data collection plan it is possible to ascertain whether the forcing data suffices to make a parameter identifiable and it may also be possible to identify what data needs to be collected in order to successfully estimate parameters after data are available identifying lack of activation indicates that eliminating non uniqueness requires observations in different experimental conditions or a different environmental context this is particularly crucial in climate change applications where the model is asked to make predictions in environmental conditions that are not reflected in historical data milly et al 2008 source iii consists of model and observation errors random noise or structural errors give rise to parameter uncertainty which may or may not be quantifiable more than one alternative model or parameter vector may be plausible so by definition a unique parameter vector cannot be identified it can however be useful to identify the most plausible parameter vector but this may not be possible due to the characteristics of the errors the interaction of errors with model structure and forcing dataset and errors mean that several parameter vectors provide identical performance according to the selected performance metrics on the other hand it is more common to have problems due to model structure forcing dataset or large parameter uncertainty around the most plausible parameters but it can still be useful to investigate how measurement errors can affect parameter estimation before embarking on expensive data collection processes or afterwards to help diagnose the source of problems and identify opportunities for improvement consider an obvious example of non identifiability involving estimation of parameters related to snow processes in a hydrological model if no information is collected about snowfall accumulation and melt then unless other measurements can provide some indirect information it is likely that the relevant parameters are non identifiable source i if there is no snowfall within the period measured then snowfall inducing dynamics will not have been activated so parameters cannot be estimated in that case source ii if snowfall is measured but with large uncertainty then the parameters will be uncertain depending on the properties of the errors it may be impossible to identify unique parameters that best fit the data source iii source i is traditionally considered the core concept of so called theoretical structural or a priori identifiability bellman and åström 1970 dobre et al 2012 the remaining sources fall in the domain of so called practical identifiability structural identifiability involves analysis of the equations of the model and can be undertaken without observational data practical identifiability is based upon analysis of the ability to estimate parameters from observational data structural non identifiability implies practical non identifiability if the equations are not identifiable then it does not matter under what conditions the data are collected how much is collected or how accurate they are the model structure determines it will not be possible to uniquely estimate parameters in practice structural identifiability however does not imply practical identifiability if the model structure theoretically allows parameters to be estimated one still needs to have the appropriate data to achieve this 2 2 identifiability of model equations focussing on model equations a model structure can be said to be globally identifiable at θ θ if for a given input u x t and measurable system output y x t all other parameter value vectors will yield different output vectors ljung and glad 1994 conversely only a single vector of parameter values will perfectly match a given set of inputs and outputs formally a model is globally identifiable if 2 f u x t θ f u x t θ θ θ otherwise the model is said to be non identifiable additionally a model structure is said to be locally identifiable if there is a neighbourhood of values around θ θ where this condition holds ljung and glad 1994 in that case other solutions can only occur in separated neighbourhoods such that there is usually a finite number of solutions for example consider quadratic functions for the equation y x 2 the value for x is locally but not globally identifiable it has two values x y and x y if a solution is locally as well as globally non identifiable there will instead be an infinite number of solutions this can be explained easily for under determined linear systems suppose we have the equation a x 1 b x 2 y with unknowns a and b and one observation with values for x 1 x 2 and y with one observation and two unknowns the problem is under determined and there is an infinite number of combinations of a and b that would fit the observation the problem is both locally and globally non identifiable identifiability is by definition a binary problem i e a parameter is either identifiable or non identifiable given the model structure and the type of data available although the equations above are strict mathematical definitions the underlying idea here is to understand whether identifying unique parameters is theoretically possible whilst in practice numerical errors in computing might need to be accounted for the idea of identifiability proper is not concerned with whether parameter values are approximately equal as long as there is some difference it is theoretically possible to differentiate between alternative parameter vectors for example by increasing the sample size of data collected which then becomes a question of uncertainty reduction if the parameter vectors cannot be distinguished even in theory this is important information for a modeler throughout section 2 two simple models are used as examples we represent them firstly as equations because identifiability depends solely on the mathematical relationship between variables not on their real world interpretation to help understand the implications of the mathematical relationship we use two different interpretations firstly they are single rate equations that describe the relationship between biomass concentration and oxygen consumption rate r kg o2 m3s in two different conditions in a second more informal interpretation we consider a restaurant where a waiter is trying to guess for a table of two regular customers how much each person usually gives as a tip r in usd the equation for the first model describes one species x kg biomass m3 with parameters for biomass growth θ 1 and maintenance θ 2 or a case where the table for two pays a single bill on a company card x usd but pool their cash tips resulting in different tipping percentages θ 1 and θ 2 thus 3 r θ 1 θ 2 x it is clear that an infinite number of combinations of θ 1 and θ 2 with the same sum will produce the same value of r hence no unique pair of parameter values can be found and we say that the parameters θ 1 and θ 2 are non identifiable this is an example of a priori or structural non identifiability source i knowing the amount of biomass and oxygen consumption does not allow us to uniquely identify the values for biomass growth and maintenance knowing the size of the bill and the tip does not allow the waiter to determine how much each person contributes the equation for the second model describes a rate equation for two species x 1 a n d x 2 with biomass parameters θ 1 and θ 2 or a case where the table for two paid separate bills 4 r θ 1 x 1 θ 2 x 2 then the model is structurally identifiable in that both θ 1 and θ 2 can be uniquely estimated with two measurements as long as x 1 and x 2 are not zero if x 1 or x 2 is zero we have no information about the respective parameter with only one measurement we would have no information about the dynamics between the variables how they change relative to one other it would be an underdetermined linear system this is an example of non identifiability due to insufficient excitation of model dynamics source ii if r and or x 1 and x 2 is measured with errors or the equation is not accurate then with two measurements the parameters will still fit perfectly but with biased parameters overfitting to the noise with three measurements or more the observations will not perfectly fit the model it would be an overdetermined linear system best fit solutions need to be found instead so θ 1 and θ 2 will be uncertain and there may not be a well defined best fit solution either this is an example of non identifiability due to observation errors source iii knowing oxygen consumption and both biomasses allows us to estimate the oxygen consumption rates but only if there is a biomass to measure and all the values are accurately measured knowing each person s bill and the total tip is sufficient for the waiter to estimate the tipping rate for each person as long as they actually had a bill and the waiter remembered the totals accurately analysis of identifiability of model equations can be carried out in two ways i e testing whether identical outputs imply identical parameters or testing that different parameters yield different outputs which is referred to as output distinguishability distefano iii and cobelli 1980 in practice such analytic approaches commonly involve transforming the model in order to facilitate analytical manipulation the easiest approach to use depends on model properties and in some cases computer algebra can be useful e g chiş et al 2011 karlsson et al 2012 saccomani and bellu 2008 except for relatively simple models this analytical problem is usually hard to solve in this general form but can be extremely useful if an analytical approach shows a model is non identifiable it will generally reveal why and hence suggest possible remedies stigter et al 2017 this introductory overview will not provide a detailed discussion of analytic methods for identifiability as they typically require an advanced knowledge of mathematics in future improvement in user friendliness of software may enable wider use of these methods in the meantime we refer the reader to the following useful literature walter and pronzato 1996 provide a summary of four key approaches involving systems of derivatives from a power series taylor series expansion coefficients of series using lie derivatives identification of local state isomorphisms and differential algebra equivalent approaches for linear systems involve markov parameters coefficients of laplace transforms and identification of similarity transforms norton 1980 and norton et al 1980 discuss transformation and analysis in terms of eigenvectors normal modes and linear regressive re parametrization is presented in keesman and doeswijk 2009 and keesman 2011 stigter et al 2017 2015 show that combinations of analytical and numerical approaches may improve computational efficiency a review by miao et al 2011 also includes approaches that directly use our definition of model equations identifiability formulate it as a constraint satisfaction problem and use the implicit function theorem other useful overviews include cobelli and distefano 1980 norton 1982 and godfrey and distefano iii 1987 2 3 identifiability with an objective function 2 3 1 parameter estimation ideally estimating parameters would involve a straightforward solution of an inverse problem the values of parameters could be mathematically deduced from observations this is for example the case for a linear system of equations as already illustrated for equation 5 however outside this narrow case model inversion tends to be approached as an optimisation problem for example when the model consists of non linear complex equations or errors are present in data parameter estimation commonly involves optimizing an objective loss function m θ see overview in bennett et al 2013 marsili libelli 2016 sometimes with constraints on the solution it quantifies how well the model and parameters fit the data the optimal parameter values θ therefore minimize m θ a common relatively simple objective function is the mean squared error mse let f be a model predicting n outputs at locations x and times t i e y x t y 1 y n t f u x t θ given a set of n data d d 1 d n t for example noisy data d f u x t θ t ε with independent measurement error ε ε 1 ε n t obtained using some unknown true parameter vector θ t the mse m for a parameter vector θ is 5 m θ y θ d 2 n in many statistical methods the objective function used is the likelihood that a parameter vector is the true value given the observations in that case the optimisation involves maximizing the likelihood and therefore produces the maximum likelihood estimate of the parameter values mle the likelihood needs to capture the distribution of errors notably the covariance of the measurement errors including autocorrelation and heteroscedasticity e g schoups and vrugt 2010 bayesian parameter estimation uses a prior probability distribution in addition to the likelihood and optimisation yields maximum a posteriori probability map estimates of the parameter values stuart 2010 however bayesian statistics typically focuses on the use of distributions rather than point estimates such that it is more common to report the mean or median parameter value and its uncertainty rather than the map estimate this is further discussed in section 2 3 6 2 3 2 identifiability as a unique optimal solution optimizing an objective function in effect provides some flexibility in parameter estimation instead of the model output having to fit the observations exactly some deviation is permitted this poses a problem in terms of equation 2 more than one set of parameter values will fit given inputs and outputs if we allow some deviation optimisation tries to solve this by quantifying the deviation with an objective function and minimizing that deviation rather than specifying how much is permitted in circumstances where a perfect fit is not possible we settle for the best available and would ideally still like the solution to be unique for parameters to be identifiable however the objective function should have a single optimal solution bellman and åström 1970 that is instead of equation 2 if our best available solution has an objective function value of m θ m we want to know that 6 m θ m θ θ θ subject to the constraint that m θ m in an optimisation context uniqueness and identifiability therefore depend on the objective function used some objective functions can be essentially handicapped to lead to unique parameter vectors if used individually for example if our objective function was simply the sum of residuals also referred to as the bias then simplification of equation 7a below shows that data has a limited effect on identifiability d only appears in the constraint 7a y θ d y θ d θ θ subject to the constraint that y θ d m 7b y θ y θ θ θ subject to the constraint that y θ m d specifically if we consider a hydrological model where the model output consists of river flows equation 7b says that parameters are only identifiable if the total river flow with least bias can only be obtained by a single parameter vector this is unlikely to occur for many models there are typically many combinations of parameter vectors that could achieve a given total flow the objective function in equation 6 is less likely to have that problem equation 8 though identifiability still depends on the model structure and the precise observations used 8 y θ d 2 y θ d 2 θ θ subject to the constraint that y θ d 2 m equation 6 can also be extended to a case where multiple objectives are of interest in a multi objective parameter identification setting the aim is to check the uniqueness of a given vector of parameter values on the pareto front a compromise solution between objectives where no other parameter values provide better performance on every objective of interest at once e g maier et al 2019 in this case equation 6 can simply be written in vector form equation 9 expands the vector notation for a case with only two objectives for a parameter vector on the pareto front with m 1 θ m 1 and m 2 θ m 2 the problem is identifiable if 9 m 1 θ m 1 θ θ θ m 2 θ m 2 θ θ θ subject to the constraint that m 1 θ m 1 and m 2 θ m 2 solving any of equations 7 10 is usually difficult even if we know the value of m ahead of time instead it is easier to think of the problem as simply exploring whether there is a single global optimum global optimality refers to being the best solution across the entire parameter space as opposed to local optimality meaning that the solution is only better than the parameter values in its immediate neighbourhood in the parameter space two fundamental approaches for examining global optimality and therefore identifiability involve visualizing the response surface and examining derivatives these are discussed in the next sections while it may seem like evaluating identifiability with an objective function requires data this is not necessarily the case identifiability can be assessed without observations by using synthetic data e g shin et al 2015 assumptions are made about the form of the input data and the model is run in order to obtain exact error free output data in controlled experiments the input corresponds to the intended experimental conditions in other settings the inputs correspond to the conditions in which data are expected to be obtained with error free input and output data it should be possible to obtain a perfect value of the objective function i e m θ 0 and any non uniqueness in parameters arises only from model equations including objective function and properties of the input sources i and ii not from noise or errors source iii such an experiment typically assumes that the model structure is a perfect representation of the underlying system it should be noted that when evaluating identifiability with data the analyst also needs to think about whether they can efficiently locate that solution it is likely that the model is not identifiable if different solutions are reached with repeated runs of optimisation algorithms there is more than one global solution for example with different seeds for stochastic algorithms or different initial values for deterministic algorithms shin et al 2015 optimisation can however also fail for other reasons local optima are a common problem iterative optimisation algorithms require the use of stopping criteria that affect the accuracy of the final optimal solution and in many environmental modeling problems identifying the global optimum is not guaranteed even if it does exist 2 3 3 visualizing the response surface or fitness landscape rather than analyzing equations visualization can be used to investigate identifiability taking a geometric point of view the value of the dependent response variable is interpreted as a surface varying dimensionally as a function of the independent variables usually parameters this is referred to as a response surface box and draper 1987 and can be used to visually evaluate whether different parameters give similar outputs as required by equation 2 when the response variable is an objective function the response surface is referred to as an error surface or fitness landscape also see maier et al 2019 2014 as shown in fig 2 the optimisation algorithm can be thought of as navigating this landscape in order to find the optimal highest or lowest point suppose we are at the top of a mountain to be able to say that we are at the single highest point of a mountain range we would need to check that we are actually on a peak not on a plateau or ridge line and that no other peak in the range is the same height that is visually to assess whether multiple points are optimal we need to check for two problems 1 flat surfaces in some direction such that points in that direction with different parameter values have the same output value these cases are both locally and globally non identifiable 2 distinct peaks with an equal objective function value these cases are locally identifiable but globally non identifiable note that a flat surface is not the same as having a flat slope at the top of the mountain zero gradient the slope might allow us to stand vertically at the top of a peak but any step to the side brings us down the mountain rather than walking along a plateau or ridge this is further discussed in section 2 3 4 a flat surface could also occur in any direction for example north west not just along the main axes north south and east west the surface might be flatter in a direction that varies multiple parameters at once rather than when varying a single parameter at a time the extent to which the combined effect of simultaneously varying parameters on an objective function differs from varying the parameters separately one at a time is indicative of parameter interaction it is an important feature to look out for because it tends to be easily overlooked especially if relying on one at a time analyses saltelli and annoni 2010 fig 2 a plots the unweighted mismatch equation 6 between the model in equation 3 and two observations of the consumption rate r for x 1 and x 2 with the true value of the parameters vector as θ t 0 5 0 5 t the black dot the red line represents the possible values of parameters that minimize r because of the lack of identifiability that we observed in the previous section an infinite number of parameter combinations can exactly reproduce the data fig 2 b plots the unweighted mismatch between the model in equation 4 and two observations of the consumption rate r for x 1 0 5 x 2 0 5 and x 1 1 x 2 0 at θ t 0 5 0 5 t the surface shows a single optimal low point corresponding to the optimal minimum other objectives could yield a single peak corresponding to an optimal maximum around the peak are concentric ellipses of parameter values with the same objective function or likelihood a model is identifiable if there are no flat surfaces at an optimum and in the case that distinct optima exist there is only one global optimum the response surface can easily be visualized for models with one or two parameters when models have larger numbers of parameters visualizing selected one and two dimensional subspaces of the higher dimensional parameter space can be useful the response surface in such subspaces illustrate the change in the objective function as a function of one and two parameters respectively keeping others constant for instance a one dimensional sub space corresponds to a cross section through a landscape razavi and gupta 2015 provide examples of the response surface of a complex high dimensional model when visually viewed within different two parameter sub spaces see fig 3 therein by which the modeler can learn about a range of features in the response surface from small scale features such as roughness and noise to large scale features such as trends and multi modality one dimensional projections also known as dotty plots beven 2006 or the profile likelihood raue et al 2009 and two dimensional projections e g shin et al 2015 are also useful for models with large numbers of parameters dotty plots show the values of one or two parameters while varying all other parameters this can help suggest potential flatness and interactions between pairs of parameters globally across all parameter space rather than at any particular point see e g shin et al 2015 for our two example models fig 3a and c shows that a large range of parameters have approximately the same objective function value whereas in fig 3b and d there is a well defined optimum visual inspection helps to build understanding of the response surface of a model in practice it is often necessary however to use numerical or analytical tests of the objective function for example when a model has many parameters e g 20 or if optima are not visible in the plot because they lie in a narrow hole or on top of a narrow peak it is typical to use multiple methods for checking identifiability balsa canto and banga 2011 2 3 4 examining the value of derivatives absence of flat surfaces and distinct optima can be more directly assessed by examining the gradient of the response surface at points across parameter space intuitively one might think a flat surface would be identified by a zero gradient but the situation is more complex when evaluating the gradient at a point for a model with only one parameter a minimum can be identified as a parameter value where the first derivative gradient is zero i e a stationary point and the second derivative is positive such that the objective function increases away from the minimum see fig 4 a c the gradient is the slope at a point and the second derivative is its curvature for a maximum the second derivative should be negative such that the objective function decreases away from the optimum if at a point the first derivative is zero i e a stationary point and the second derivative is zero as well but the third derivative is non zero i e an inflection point the point is a saddle point not an optimum see fig 4d f if there are multiple parameter values with gradient equal to zero there are multiple local optima see fig 4g i non identifiability where the objective function is flat therefore occurs when the first second and third derivatives are all zero as well as further higher order derivatives though in practice they may have little effect the flatness is referred to as lack of sensitivity and the derivatives allow us to check for insensitive parameters at a particular point if examining the derivatives shows that there are multiple local optima then their objective function values need to be compared to see whether there is more than one global optimum examining the derivatives can therefore help identify both the problems visualized with the response surface for a model with more than one parameter partial derivatives are calculated with respect to each parameter separately this results in a gradient vector with the partial derivative for each parameter for an optimum the first partial derivatives should be zero and a matrix of second partial derivatives is then constructed considering pairs of parameters together including second partial derivatives of each parameter with respect to itself this is known as the hessian matrix the hessian describes how the slope of the response surface changes in every direction in other words its curvature to illustrate this fig 5 shows a hypothetical response surface that features a global maximum and a local maximum a saddle point in between and a flat area these four points are all stationary meaning the gradient vector first order partial derivatives is zero however the hessian matrices at these four points have significantly different properties if all the second partial derivatives are non zero the response surface is not flat if one is zero then the response surface may be flat if the higher order derivatives are also zero in particular if any third order derivative is non zero then there is a saddle point rather than a flat surface as shown in the figure the hessian at a stationary point can be cast as a quadratic function fitted to the response surface at that point a parabola for a single parameter function mathematically this is equivalent to using the first three terms of a taylor series expansion the quadratic illustrates how the hessian contains information about the slope in all directions not just along each parameter axis if the hessian is a diagonal matrix the associated parabola is orthogonal i e no pairwise interaction term in the quadratic function and therefore there is no interaction effect between the parameters locally at that point however if the hessian is non diagonal the associated quadratic is non orthogonal as a result of interaction between the parameters in fig 5b the quadratics describing the local and global maxima both have significant parameter interactions if the axes were compass directions the surface around the local maximum would be flattest along a northwest southeast axis and steepest along a northeast southwest axis the global maximum is flattest along a northeast southwest axis and steepest along a northwest southeast axis the interaction effects at a stationary point can be summarised with an eigendecomposition calculating the eigenvectors and eigenvalues of the hessian which respectively describe a set of orthogonal directions an orthogonal basis and the second partial derivative in that direction the advantage of this transformation is that the first eigenvalue is the largest second partial derivative in any direction and the last eigenvalue is the smallest the response surface at a point may therefore be flat if the last eigenvalue s are zero meaning that the second partial derivative is zero in the corresponding direction and the hessian is then referred to as singular conversely the point is an identifiable minimum if all eigenvalues are positive such that second partial derivatives are positive and the gradient and objective function increase in all directions the hessian is then referred to as non singular and positive definite similarly the point is a maximum if all eigenvalues are negative and the hessian is referred to as negative definite if the eigenvalues have different signs the point is a saddle point which in a two parameter case resembles a horseback riding saddle see fig 5b for more detail on the role and the numerical aspects of the hessian computation see for example marsili libelli 1992 seber and wild 1989 and marsili libelli et al 2003 the eigenvectors of the models in equations 3 and 4 were also shown in fig 2 for equation 3 the eigenvalues are 10 and 0 the hessian is semi positive definite indicating that an optimum exists but it is not unique for equation 4 the eigenvalues are approximately 1 31 and 0 19 the larger eigenvalue corresponds to the direction eigenvector along which the misfit changes the fastest in practice the eigenvalues of the hessian may not be examined directly the determinant of the hessian can be calculated as the product of eigenvalues amongst other methods if any of the eigenvalues are zero the determinant will therefore also be zero which also tells the analyst that the hessian is singular and may have a flat response surface in some direction in statistics textbooks the reader may come across this approach in likelihood based methods that analyse the observed fisher information matrix fim or the expected fim the observed fim corresponds to the negative of the hessian matrix when using the log likelihood as objective function the expected fim averages over the likelihood function rather than using observed values if the fim is non singular then the model structure is locally identifiable bellman and åström 1970 rothenberg 1971 in general it is difficult to demonstrate singularity due to numerical errors it is often better to perform a rank test with a singular value decomposition svd of the sensitivity matrix which consists of the partial derivatives of the model outputs with respect to each parameter see e g miao et al 2011 as the hessian indicated the objective function may also be flat due to interactions between parameters the combined effect of varying two parameters may cancel out such that the objective function does not change a variety of other indices have been proposed that measure the strength of interaction of parameters or its effect on identifiability see e g brun et al 2001 doherty and hunt 2009 hill and tiedeman 2007 sorooshian and gupta 1985 as an example active subspaces provide a global indicator constantine et al 2014 similar to how the eigendecomposition summarises the hessian if all singular values are non zero and of similar magnitude then the system is likely to be globally identifiable alternatively variance based sensitivity analysis methods can quantify the combined effect of specific combinations of parameters averaged across parameter space however sensitivity of a combination of parameters i e strong interaction does not necessarily mean that they are non identifiable dobre et al 2012 at any specific point the interaction may not result in a flat objective function derivative based methods including the techniques using sensitivity analysis are relevant when the derivative can be calculated whether analytically or numerically and non identifiability is more likely to occur due to flat surfaces than distinct optima there are few optima to find and examine derivative based methods may not be appropriate if there are numerical problems in the model or if the model output or objective function is characterised by discontinuities or many distinct solutions in these situations other approaches need to be used including examining the response surface 2 3 5 sensitivity versus identifiability identifiability analysis ia is closely related to sensitivity analysis sa sa seeks to measure the sensitivity of a model response to perturbations in different model parameters see norton 2015 for an introductory overview of sa saltelli et al 2000 as a comprehensive reference and razavi and gupta 2015 and pianosi et al 2016 for recent reviews of the state of the art if the objective function is insensitive to a parameter it means that the objective function is flat and the parameter is not identifiable however one needs to distinguish between sensitivity and identifiability even if the objective function is sensitive to all parameters it is not guaranteed that the parameters are identifiable for the reasons below sa aims to establish which parameters exert stronger or weaker controls on the model response which is an attribute of the forward problem in contrast ia aims to establish which parameters are identifiable given observations on the target response which is an attribute of the inverse problem measuring sensitivity on an objective function combines attributes of the forward and inverse problems but plays a filtering role that may obscure the information gained gupta and razavi 2018 local sensitivity analysis lsa at a nominal point in the parameter space is based on partial derivatives at that point similar to the approach discussed for identifiability in the previous section the same condition therefore applies that assessing identifiability using lsa of the objective function needs to be complemented by evaluation of other local optima in order to determine whether a single global optimum exists lsa provides a limited view on sensitivity only locally around the nominal point saltelli and annoni 2010 optimal parameter values therefore need to be obtained before using lsa to assess identifiability with an objective function alternatively a wide range of methods have been developed that seek global sensitivity analysis gsa measuring sensitivity across the entire parameter space examples include derivative based methods campolongo et al 2007 rakovec et al 2014 sobol and kucherenko 2009 variance based methods homma and saltelli 1996 sobol 2001 and variogram based methods razavi et al 2019 razavi and gupta 2016a 2016b the latter bridging derivative and variance based methods by characterizing the response surface across the full spectrum of perturbation scales haghnegahdar and razavi 2017 if a parameter has no effect on the objective function anywhere in parameter space it also has no effect at the optimum wherever it is located however even if a parameter is sensitive on average across the parameter space it is not guaranteed that it is sensitive at the global optimum the parameter still may not be identifiable gsa also allows evaluating the effect of parameters on model outputs and therefore investigating structural identifiability similarly if a parameter has no effect on the model output anywhere in parameter space it is non identifiable as noted above it is also common practice to use sa to identify interactions between parameters while remembering that the absence or presence of interactions does not necessarily indicate non identifiability sensitivity analysis is therefore an effective tool for identifying non identifiability but not for ruling it out see gupta and razavi 2018 and dobre et al 2012 for further detail on commonalities and differences between sa and ia 2 3 6 role of noise and systematic errors as presented in section 2 3 2 the use of an objective function provides a mechanism for taking into account noise and errors in assessing identifiability we can assess whether the best fit is only attainable using a single vector of parameter values as equation 6 depends on the observed data non uniqueness may indicate that the properties of errors prevent uniqueness source iii in addition to which quantities are observed source i and the conditions represented in the forcing data source ii by errors we mean any discrepancy between the modeled and observed systems these errors can take many forms noise or systematic errors in the measurement of either inputs or outputs discrepancy between the model structure and modeled system as well as misrepresentation of the properties of errors themselves as captured by the objective function we focus here on general principles rather than explaining the differences in effect of these different errors in addition to potentially affecting the identifiability of the optimal solution errors influence the identifiability problem in another key way the estimated parameters will generally change depending on the data used and the objective function invoked samples of observations with different data errors result in different estimates of parameter values values of parameters will therefore remain uncertain even if a unique optimum solution can be identified when using noise free data vanrolleghem and keesman 1996 if the parameters are anyway non unique this provides a strong motivation for switching from optimisation based to uncertainty based parameter estimation in that case rather than aiming to identify a single parameter vector the aim is to identify a set of many parameter vectors that all adequately fit the data i e to quantify the uncertainty in parameters see matott et al 2009 for a review of methods for quantifying uncertainty in parameters as foreshadowed in section 2 3 1 this is the approach preferred by bayesian inference which focuses on identifying a credible region of parameters within which an unobserved parameter falls with a given subjective probability it can also be approached more directly as a set membership problem as notably used in the generalised likelihood uncertainty estimation glue approach where only the set of behavioural parameters satisfying pre defined constraints are retained from the prior set beven 2006 guillaume et al 2015 it is worth repeating that even if the concept of a unique parameter vector is no longer used in parameter estimation it is still useful in the context of identifiability analysis in helping to rule out that non uniqueness is due to sources i and ii which is often avoidable in addition to being non unique parameters estimated with noisy data may also be biased in the sense that the mean parameter estimate may no longer represent the true value where it is known the uncertainty and bias in parameters also tend to decrease as sample size increases depending on the properties of the errors and increase as uncertainty in measurements increases these phenomena can be illustrated with our simple objective function m θ with a single scalar parameter consider the following model for oxygen consumption rate 10 r 35 8 θ 4 x 1 1 80 θ 1 2 x 2 3 8 where θ is again the biomass parameter we take two noise free measurements at x 1 1 x 2 1 we then investigate behaviour of the objective function for different true parameter values summarised in fig 6 in the top row of the figure we have set θ t 0 5 and the bottom row we have set θ t 0 7 in the first column of fig 6 we plot the rate r as a function of the parameter the black dots depict r θ t and the blue dots represent one realization of noisy data d r θ t η with η 0 1 in the second column of fig 6 we plot the loss functions m θ for noise free and noisy observations the loss function is minimized at the true parameter value black dot the blue dot represents the estimated parameter for the noisy data the sensitivity of the recovered parameter is heavily dependent on the true parameter value even for the same noise magnitude in fig 6b the parameter is less sensitive the response surface is flatter with noisy data many parameters produce a rate r that is close to the data in contrast when θ t 0 7 fig 6e the model response has a much larger gradient around the estimated parameter a sampling procedure can be used to estimate the reliability of recovering the true parameter in the presence of noise of varying magnitude miao et al 2011 for example let the noise η be normally distributed with mean zero and standard deviation σ the third column of fig 6 depicts the recovered parameter values for 100 realizations of the noise for three levels of noise σ 0 01 0 05 0 1 in every case we find a unique minimum the parameter is at least locally identifiable but the minimum is no longer the true parameter that we could recover with noise free data the uncertainty of the estimated parameter depends on the noise level and the response surface in a local region around the true parameter when the response surface is flat top row the parameter estimates are much more uncertain than when the response surface has a larger gradient bottom row in practice noise and errors are usually unavoidable an analyst cannot tell for sure whether they are using or collecting the right type of information to obtain reliable estimates of parameters or model outputs representing real world phenomena instead they can only make an educated judgement that is dependent on how closely that model structure and data reflects reality identifiability analysis by definition tests for non uniqueness with a given model structure and understanding of errors and therefore objective function if these assumptions poorly reflect reality then identifiability analysis may be of limited use in informing the analyst s judgement the better the model structure and the dataset the more useful identifiability analysis will be 3 identifiability analysis in practice parameter identifiability is a fundamental concept in parameter estimation and therefore in many types of environmental modeling modelers should know whether the type of data they are using is capable of identifying unique parameter values they should know if they do not have information about the right quantities within a given model structure source i they should know if the forcing data and associated response does not reflect the conditions needed to estimate their parameters source ii and they should know if the errors are such that more data will not improve parameter uncertainty source iii first tests are often even trivial to implement simply repeat an optimisation with different initialisations to check if it returns different values for parameters see e g shin et al 2015 sensitivity analysis can be performed using black box software and dotty plots can easily be produced for each parameter amongst other methods borgonovo et al 2017 knowing that an identifiability problem exists however is only a starting point determining the cause of non identifiability within a model structure forcing data or messy dataset is likely to fall at one of two extremes quite difficult or trivial for example trying to estimate snow related parameters on a tropical island excluding the trivial case issues of identifiability tend by their very nature to be concealed from the modeler s view if the issues were obvious the modeler would have addressed them and no identifiability issue would be experienced similarly improving identifiability is often non trivial it requires thorough understanding of the cause s of non identifiability and the time knowledge and resources to collect or select new data and modify model structures or objective functions even simple fixes such as fixing parameter values or using a different model structure may have significant and often overlooked impacts on results while tools are available to support identifiability analysis there is still a great need for development of methods software and training to ensure all modelers are able to assess and react appropriately to non identifiability in the meantime to provide some guidance we make five pragmatic recommendations and briefly discuss computational considerations 3 1 know whether your model parameters are identifiable it cannot be repeated too often identifiability is a fundamental concept in parameter estimation particularly given the difficulty of eliminating non identifiability it is important to explicitly acknowledge and document for other analysts how identifiability concerns have influenced the modeling process and how they have been addressed regardless of the response taken the analyst should be transparent about their treatment of identifiability documentation should describe whether the model is identifiable and how this was assessed if the model is non identifiable the documentation should describe the expected consequences for the particular analysis and the responses that were taken as a starting point a modeler should keep an eye open for symptoms of non identifiability box 1 and apply simple diagnostics like repeated optimisation sensitivity analysis and 1 or 2d dotty plots if there are signs of non identifiability a reviewer should at least want to know what the source of the non identifiability is in order to judge whether it is acceptable model structures of some complex models are known to be non identifiable for example in spatially distributed groundwater modeling there is a common risk of interactions between parameters in adjacent grid cells similar to the case described in equation 3 uniquely estimating transmissivity therefore depends on having water level measurements within appropriate parts of the flow region neuman 1973 similar problems have also been observed for rainfall runoff models with many parameters shin et al 2015 if identifiability is not already known techniques in section 2 can be used either analytically or with ideal input data and noise free outputs to show the model structure is responsible source i using synthetic data real input data and noise free outputs can reveal if the input data provide insufficient excitation of modes of model behaviour source ii for example parameters of some hydrological models may only be active in rare conditions gupta and sorooshian 1983 such as observed streamflow in a drought period may not provide information about parameters related to flood periods and vice versa for some models it can be possible to use analytical methods to identify the data characteristics necessary to achieve persistence of excitation norton 2009 other techniques can also be used to assess information in data such as by observing the effect of data on parameter sensitivity wagener et al 2001 examples include time varying or spatially varying sensitivity analysis gupta and razavi 2018 pianosi et al 2016 razavi and gupta 2019 and notablythe dynamic identifiability analysis technique wagener et al 2003 if tests suggest that neither the structure source i nor input data source ii are responsible then the role of noise and systematic errors can be explored in input data output data model structure and error model testing alternative model data combinations it may also be that the optimisation algorithm is failing to find the unique solution due to multiple regions of attraction minor local optima roughness poor sensitivity and non convex shape of the response surface duan et al 1992 3 2 consider how non identifiability fits within your analysis in addition to helping to avoid negative effects of non identifiability box 1 it is worth keeping in mind the benefits of identifiability analysis box 2 there is a variety of ways it might influence modeling and model based analyses in practice while non identifiability means the modeler lacks the appropriate information to choose between alternative models in practice the right information may take too much time or resources to obtain or may never be available at all depending on the prediction to be made it may not matter that multiple models are plausible the differences in prediction may be small enough to be acceptable or able to be addressed by decision makers through adaptive management williams 2011 how best to handle non identifiability is highly dependent on purpose and context the modeler must use their professional judgement to select practices that suit their problem consistent with expectations in their modeling domain and document the reasons for their choice acknowledging possible alternatives in addition to simply tolerating non identifiability there are three other high level philosophical approaches possible wagener and gupta 2005 which can be viewed as quantifying the effect of lack of information improving the information used by obtaining the right type of observations and or inputs and avoiding the need for missing information by modifying the model these four options are briefly described here consider whether non identifiability can be tolerated uncertainty due to identifiability issues whether explicitly quantified or not can be assessed in terms of its risk that is its effect on the final product of the analysis such as quantities of predictive interest for decision making for example if the uncertainty induced by non identifiability does not change a decision then perhaps it can be ignored guillaume et al 2015 this is typically the default approach if modelers are aware of identifiability issues by professional judgment modelers often assert that a given issue is not significant in order to be able to provide results efficiently rather than futilely trying to tie up every loose end guillaume et al 2017 consider quantifying uncertainty due to non identifiability it may be advantageous to formally quantify the effect of non identifiable parameters for example if a process based model is used to estimate model outputs not used in parameter estimation then a complex model may be necessary even though it is not strictly identifiable in the context of unsaturated zone models in hydrology for example brunner et al 2012 observed that certain observations can significantly reduce predictive uncertainty without informing any specific parameters in cases such as these the limitations of the model should be documented and the uncertainty induced by non identifiability should be quantified for some problems dedicated methods are available that identify parameter vectors that yield identical outputs or objective function values e g null space monte carlo tonkin and doherty 2009 consider obtaining information to make the model identifiable some symptoms of identifiability may be considered intolerable such as non uniqueness of optimised parameters lack of observability of a parameter or lack of transferability of a model to specific conditions for instance from flood to drought in hydrological applications in these situations steps can be taken to improve identifiability by measuring different quantities source i obtaining data in different conditions source ii or that is more accurate source iii literature on data acquisition planning and optimal input design e g dausman et al 2010 freeze et al 1992 goodwin and payne 1977 tiedeman et al 2004 walsh et al 2017 along with using soft data and expert knowledge gharari et al 2014 can be of assistance targeted reviews of types of data used for parameter estimation can also provide guidance e g for hydrogeology schilling et al 2019 alternatively it may be possible to constrain the solution space by making assumptions to reduce the number of unknowns or providing information about which values are preferred using prior distributions stuart 2010 or regularization tikhonov et al 1995 machine learning based methods provide another example non uniqueness can occur due to redundant information in inputs maier et al 2010 or local optima due to non linearities kingston et al 2005 and steps can be taken to avoid them including through input variable selection may et al 2011 consider modifying model information requirements it may be more appropriate to use a model that is identifiable with the available information grayson et al 2002 jakeman and hornberger 1993 young et al 1996 the principle of parsimony suggests that unnecessary complexity should be discarded a number of model selection criteria prefer models with less parameters e g akaike information criterion akaike 1974 in the case where particular observations can reduce predictive uncertainty without informing any specific parameters information is likely being provided about combinations of parameters these combinations could possibly be replaced by a single parameter within a simplified model e g croke and jakeman 2004 young et al 1996 this is different to eliminating parameters or fixing parameter values a single combined parameter captures information from data about multiple more detailed parameters and their total effect after accounting for interactions present in the parameter estimation data parameter elimination or fixing on the other hand introduces new information in the form of assumptions about the value of that parameter if those assumptions are incorrect even if the model performs well in some circumstances it will be right for the wrong reasons and may predict poorly eliminating the symptoms of non identifiability however typically requires invasive changes to the model or model identification procedure for example using different or transformed parameters selecting specific data periods changing model structure and or using a more sophisticated objective function it is important that these changes do not undermine the ability to understand the concepts that went into creating the model in the first place 3 3 computational considerations constructing response surfaces cross sections dotty plots exploring parameter sensitivities or estimating the impact of noise can require large numbers of model evaluations this can render identifiability analysis infeasible if the model being analysed is computationally expensive while high performance computing may be a solution in some cases it may also be possible to use computationally frugal methods hill et al 2016 or to build an approximation of the model s response surface to reduce the computational demands of identifiability analysis the remainder of this section focuses on this last option which is rapidly maturing a response surface can be explicitly represented mathematically by model emulation resulting in a surrogate model this has two key advantages firstly having an analytical representation of the response surface allows additional mathematical analyses secondly the computational cost of building and running a surrogate model is typically less than relying on the original model therefore facilitating the use of more computationally intensive methods the efficacy of response surface methods depends on the ability to build and capture the salient features of the response surface with a feasible number of model simulations the approach known as response surface methodology box and draper 1987 approximates the response surface by a quadratic function and allows analysis of flatness near the optimum although requiring a limited number of model simulations quadratic response surfaces are often poor representations of a model response surface alternative methods that can approximate the non linearity of the model response and higher order interactions between variables are typically needed polynomial chaos expansions pce sudret 2008 xiu and karniadakis 2002 and gaussian processes gp marrel et al 2009 oakley and o hagan 2004 are two popular and effective methods for approximating highly non linear response surfaces gp and pce can both be efficiently used as surrogates for the simulation model in any of the numerical identifiability methods outlined in this paper for example gradient and hessian data can be computed analytically from the approximations and used for derivative based identifiability analysis or the approximations can be used as surrogates within the likelihood function when conducting bayesian inference e g blanchard et al 2010 or calibration using maximum likelihood estimation e g arendt et al 2012 in the water resources sector and in groundwater modeling particularly razavi et al 2012 and asher et al 2015 respectively provide reviews of a range of surrogate modeling methods in addition to the aforementioned techniques based upon function approximation reduced order models can also be used to decrease the computational cost of analyzing expensive simulation models carlberg 2015 cui et al 2015 soize and farhat 2017 these methods do not construct response surface approximations but rather solve the governing equations on a reduced basis the advantage of such an approach is that one can cheaply obtain approximations of the entire solution to the governing equations rather than a small number of functions of that solution moreover unlike surrogate methods reduced order models do not suffer from the curse of parameter dimensionality the number of simulations required to build a response surface grows rapidly with parameter dimension consequently the practical use of response surface methods is typically restricted to models with the order of 10 parameters for example a survey of the literature has shown that less than 65 of the applications of these methods are on functions having less than ten parameters and more than 85 have less than 20 razavi et al 2012 the exact number of simulations required to achieve a given accuracy is model dependent however simulation requirements can be reduced by using methods that exploit the structure in the response surface constantine 2015 gorodetsky and jakeman 2018 jakeman et al 2015 and goal oriented approximations thereby focussing computational effort on regions and or dimensions of parameter space that significantly affect quantities of interest lastly when using a surrogate model the user needs to be mindful of approximation uncertainty that is the potential errors of the surrogate model that can be added to the process of parameter estimation razavi et al 2012 illustrate this source of uncertainty by simple examples see figs 2 3 and 6 therein and review methods to address it 4 conclusions this article has provided an introductory overview to key issues in identifiability analysis and fundamental concepts and methods to quantify and understand the impact of non uniqueness of parameters on modeling results identifiability analysis aims to i assess whether it is possible to identify unique parameter values and ii understand why that occurs it therefore makes an essential contribution to more systematically learning about sources of uncertainty and how to reduce them and hence improving the practice credibility and outcomes of modeling exercises in the long run our discussion of identifiability analysis in practice emphasizes that the right tools to use depend on modeling context especially the purpose and the modeler s professional judgement our fundamental recommendation is therefore that assessment of whether a modeling exercise is fit for purpose explicitly needs to address three points whether the model is identifiable the source s of any non identifiability issue the extent to which any non identifiability impacts the problem at hand it is important to explicitly acknowledge and document for other analysts how identifiability concerns have influenced the modeling process and how they have been addressed regardless of the response taken the analyst should be transparent about their treatment of identifiability documentation should describe whether the model is identifiable and how this was assessed if the model is non identifiable the documentation should describe the expected consequences for the particular analysis and the responses that were taken the modeler s professional judgement naturally depends on knowing what alternative methods of identifiability analysis are available and why they might be useful the emphasis of this introductory overview is therefore on awareness raising encouraging wider acknowledgement of non identifiability as a critical issue and that there are methods to assess it and its sources there is however still much to be done to make practices for assessing identifiability easier to apply and for them to be commonly applied and there are huge opportunities to improve identifiability of models and reduce uncertainties especially where there is a large community of resourceful researchers that uses a particular non identifiable model there are several well known hydrological and water quality models for instance where sensitivity analysis indicates they are structurally non identifiable but other sectors are replete with over parameterized models that do not use mitigating mechanisms such as regularization or investigate alternative model structure hypotheses that may lead to improved identifiability promoting the thoughtful use of identifiability analysis practices provides a firm foundation for improving models into the future acknowledgements this introductory overview builds on an iemss conference paper marsili libelli et al 2014 joseph guillaume was supported by academy of finland funded project wasco grant no 305471 and emil aaltonen foundation funded project eat less water the views expressed in the article do not necessarily represent the views of the u s department of energy or the united states government sandia national laboratories is a multimission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 
26166,identifiability is a fundamental concept in parameter estimation and therefore key to the large majority of environmental modeling applications parameter identifiability analysis assesses whether it is theoretically possible to estimate unique parameter values from data given the quantities measured conditions present in the forcing data model structure and objective function and properties of errors in the model and observations in other words it tackles the problem of whether the right type of data is available to estimate the desired parameter values identifiability analysis is therefore an essential technique that should be adopted more routinely in practice alongside complementary methods such as uncertainty analysis and evaluation of model performance this article provides an introductory overview to the topic we recommend that any modeling study should document whether a model is non identifiable the source of potential non identifiability and how this affects intended project outcomes graphical abstract image 1 keywords identifiability response surface non uniqueness derivative based methods hessian emulation uncertainty learning objectives appreciate key concepts and methods of parameter identifiability analysis recognise the main consequences of parameter non identifiability i e the inability to infer unique parameters from data distinguish between different sources of parameter non uniqueness and how they influence identifiability understand that non uniqueness can occur even with ideal and or noise free data recognise non identifiability can be due to model structure and equations alone understand how non uniqueness in parameter estimation may relate to multiple optima and or flatness in the response surface investigate potential multiple optima and flatness using visualization derivatives and related indicators understand that noise in data can affect identifiability appreciate issues affecting use of identifiability analysis including computational considerations 1 introduction what is identifiability analysis models are widely used for understanding management and scenario analysis of environmental and other systems increasingly for social learning among stakeholders and in support of decision making kelly letcher et al 2013 the ultimate aim is to utilize data and knowledge about a system to help the modeler decision makers and other stakeholders make sense of how the system works how it may change in future and how it may respond to management actions and other perturbations in an ideal world data and knowledge about a system of interest would be sufficiently complete such that the model of that system would become an oracle that can be trusted to always provide the right answer in reality we typically have incomplete knowledge and data to adequately conceptualise and simulate a system and even in cases where we have an adequate understanding of the general principles governing the system we can typically only approximate its complexity spatial heterogeneity and temporal variability tackling this lack of certainty requires a pragmatic approach to managing information i e data or knowledge that informs understanding of the system underpinned by effective modeling practice e g badham et al 2019 and driven by the purpose of the analysis and the resources available it is necessary to balance efforts to 1 obtain cost effective information 2 make best use of that information to manage and reduce critical controls on uncertainty and 3 understand the remaining uncertainty in order to recognise the limitations on how the model should be used to develop a model there are several steps e g jakeman et al 2006 in these steps the available information is typically used for identifying a model structure and estimating its parameters the identified model structure defines which quantities in the model state variables and parameters are known which are unknown and need to be estimated and how they are related for short we refer here to different model structures as different models we refer to a specific model as a vector function f that receives the vector of parameter values θ which is typically time invariant and estimated using data and inputs u that are system drivers and may be varying in space x and time t in explicit form 1 y x t f u x t θ where y represents model outputs that like u may vary in space and time bold notation indicates variables may be vectors rather than just scalars in general since a model is simply a tool to express what we think we know f θ u and therefore y are all likely to be uncertain every aspect of any particular model will differ from observations to some extent they are all prone to error this is a very general definition of models that covers data based theory process based and conceptual modeling approaches but for an indicative list of model families and features covered by 1 see jakeman et al 2006 p 606 we note that data based models are often used in situations where data volumes are copious compared to problems requiring theory based or conceptual models nevertheless the issue in data based modeling is also to use a model structure f and parameters θ to establish a relationship between u and y that can reproduce aspects of interest of the observed output behaviour of the model likewise with so called integrated models consisting of linked model components see kelly letcher et al 2013 for five commonly used types in environmental applications equation 1 remains relevant despite the fact that the form of f may be very complicated and parameters may be defined for each sub model separately rather than all at once parameter values θ may be measured where they correspond to observable properties of a system or estimated also known as calibration from measured outputs by invoking an objective function that optimises constrained errors between the model and measured outputs in what is referred to as an inverse problem as opposed to the forward problem of simulating the model outputs given the parameters given errors in both observations and model structure model and measured outputs do not perfectly match resulting in uncertainty in the estimated parameters and residual errors between the estimated and observed outputs there are typically trade offs between minimizing different aspects of the residual errors such that the parameter estimation task itself is commonly seen as a multi criteria problem efstratiadis and koutsoyiannis 2010 gupta et al 1998 depending on the purpose of the analysis it is recognized as good practice to work with multiple model structure hypotheses clark et al 2011 jakeman et al 2006 and to quantify the total uncertainty in outputs resulting from using multiple models uncertain parameters estimation procedures criteria and residual errors ultimately equifinality is unavoidable beven 2006 given that errors cannot be eliminated or fully characterised it is always possible to conceive of multiple different model structures and different parameter vectors that provide an acceptable fit to observed data nevertheless in the context of making best use of available information a modeler seeks to reduce uncertainty as much as is reasonably possible or required there are four key complementary methods for measuring how well uncertainty has been reduced fig 1 firstly quantifying the uncertainty in outputs or some function of them provides a direct indicator but is dependent on how well uncertainty in model structure parameters and residuals has been quantified refsgaard et al 2007 secondly comparing observed and modeled outputs can be used in several ways amongst others predictive accuracy provides a measure of model performance see bennett et al 2013 for metrics and methods the information supplied by a model can be quantified nearing and gupta 2015 if error analysis shows systematic rather than randomly distributed residuals this typically indicates a problem with the model structure which in general leads to bias in the parameter estimates evaluating model adequacy is a substantial task of its own gupta et al 2012 given one or more model structures a modeler is generally in fact primarily interested in how data has helped reduce uncertainty in parameters specifically thirdly therefore the modeler can quantify the uncertainty in parameters including the covariance describing how the parameter estimates relate to one other checchi et al 2007 thyer et al 2009 vrugt 2016 with a suitable model structure more data should typically result in smaller parameter uncertainty however this will not occur if the right type of data is not collected which is the problem best tackled by investigating parameter identifiability it is important to understand the specific contribution of parameter identifiability analysis even or especially if one does not understand how to perform it parameter identifiability analysis focuses on whether it is possible to identify a unique vector of parameter values for a given model structure or whether multiple parameter values will fit the data equally well non identifiability means the modeler does not have the information needed to choose between alternative models rothenberg 1971 large parameter uncertainty is often indicative of non identifiability and reducing parameter uncertainty indicates that the situation has improved the idea of poor identifiability is frequently approached from a parameter uncertainty perspective strictly speaking however identifiability analysis is specifically interested in the yes no question of whether unique parameter values could be identified it can be used both before and after data collection identifiability analysis before data collection can check whether the right type of data will be collected identifiability analysis after data collection can check whether the right type of data is being used in the parameter estimation process it is therefore worth noting that even when the modeler s aim is to quantify parameter uncertainty rather than to estimate a single parameter vector it is still useful to test whether a unique parameter vector could ideally be identified in order to check whether mismatch between the model structure and type of data used is contributing to parameter uncertainty the power of identifiability analysis comes from focussing on a tightly defined mathematical problem analyzing the relationship between knowns and unknowns in a given model structure there is a simple rule when solving a linear system of equations that the number of unknowns should be less than the number of observations otherwise the problem is underdetermined and ill posed and an infinite number of solutions is possible this simple rule cannot be easily applied in more complex models and identifiability analysis provides some alternatives it is common sense that a modeler should know whether their linear problem is underdetermined and what they should do about it similarly it should be common sense for modelers to know whether their model structure is non identifiable and whether it matters for the purpose of their analysis this article aims at providing an introductory overview to key ideas of identifiability analysis the field of identifiability has been extensively researched in various disciplines including numerical examples in psychology in 1919 thomson 1919 and independent early development of theory in econometrics and system identification in the 1950s and 1970s bellman and åström 1970 koopmans and reiersol 1950 there is a number of other key reviews beck 1987 dobre et al 2012 godfrey and distefano iii 1987 miao et al 2011 walter and pronzato 1996 this article differs from existing reviews by seeking to provide an accessible introduction to encourage the environmental modeling community to think more systematically and strategically about what type of information is needed to estimate parameters in their model and increase adoption of the tools of identifiability analysis the ultimate aim is therefore to improve research and management outcomes by fostering reflection on whether the selected model structure and available data are indeed appropriate to the problem at hand in section 2 this paper first discusses the sources of non identifiability within the scope of parameter identifiability analysis and then introduces fundamental concepts and methods underlying identifiability analyses building on these concepts section 3 discusses how these methods can be used in practice section 4 contains the conclusions 2 fundamental concepts and methods 2 1 sources of non identifiability this paper seeks to capture key distinctions within the existing identifiability literature we distinguish between three high level sources of parameter non uniqueness source i is the model structure including conceptualisation of the system equations used to represent it and optimisation objective function or model of the error structure where applicable see sections 2 2 and 2 3 parameter non uniqueness can occur simply because of the choice of which quantities in the model are selected for observation this is referred to as structural non identifiability the use of non identifiable equations can be determined before having any input internal state or output data after data are available identifying structural non identifiability can show that eliminating non uniqueness requires data about different quantities in the model or adoption of a different model structure that is identifiable with the type of data available source ii is the input forcing dataset activation of different dynamics within a model depends on the inputs to the model including initial and boundary conditions if dynamics related to a parameter are not activated then no information will be available to estimate that parameter this is also referred to as persistence of excitation of the model dynamics given a data collection plan it is possible to ascertain whether the forcing data suffices to make a parameter identifiable and it may also be possible to identify what data needs to be collected in order to successfully estimate parameters after data are available identifying lack of activation indicates that eliminating non uniqueness requires observations in different experimental conditions or a different environmental context this is particularly crucial in climate change applications where the model is asked to make predictions in environmental conditions that are not reflected in historical data milly et al 2008 source iii consists of model and observation errors random noise or structural errors give rise to parameter uncertainty which may or may not be quantifiable more than one alternative model or parameter vector may be plausible so by definition a unique parameter vector cannot be identified it can however be useful to identify the most plausible parameter vector but this may not be possible due to the characteristics of the errors the interaction of errors with model structure and forcing dataset and errors mean that several parameter vectors provide identical performance according to the selected performance metrics on the other hand it is more common to have problems due to model structure forcing dataset or large parameter uncertainty around the most plausible parameters but it can still be useful to investigate how measurement errors can affect parameter estimation before embarking on expensive data collection processes or afterwards to help diagnose the source of problems and identify opportunities for improvement consider an obvious example of non identifiability involving estimation of parameters related to snow processes in a hydrological model if no information is collected about snowfall accumulation and melt then unless other measurements can provide some indirect information it is likely that the relevant parameters are non identifiable source i if there is no snowfall within the period measured then snowfall inducing dynamics will not have been activated so parameters cannot be estimated in that case source ii if snowfall is measured but with large uncertainty then the parameters will be uncertain depending on the properties of the errors it may be impossible to identify unique parameters that best fit the data source iii source i is traditionally considered the core concept of so called theoretical structural or a priori identifiability bellman and åström 1970 dobre et al 2012 the remaining sources fall in the domain of so called practical identifiability structural identifiability involves analysis of the equations of the model and can be undertaken without observational data practical identifiability is based upon analysis of the ability to estimate parameters from observational data structural non identifiability implies practical non identifiability if the equations are not identifiable then it does not matter under what conditions the data are collected how much is collected or how accurate they are the model structure determines it will not be possible to uniquely estimate parameters in practice structural identifiability however does not imply practical identifiability if the model structure theoretically allows parameters to be estimated one still needs to have the appropriate data to achieve this 2 2 identifiability of model equations focussing on model equations a model structure can be said to be globally identifiable at θ θ if for a given input u x t and measurable system output y x t all other parameter value vectors will yield different output vectors ljung and glad 1994 conversely only a single vector of parameter values will perfectly match a given set of inputs and outputs formally a model is globally identifiable if 2 f u x t θ f u x t θ θ θ otherwise the model is said to be non identifiable additionally a model structure is said to be locally identifiable if there is a neighbourhood of values around θ θ where this condition holds ljung and glad 1994 in that case other solutions can only occur in separated neighbourhoods such that there is usually a finite number of solutions for example consider quadratic functions for the equation y x 2 the value for x is locally but not globally identifiable it has two values x y and x y if a solution is locally as well as globally non identifiable there will instead be an infinite number of solutions this can be explained easily for under determined linear systems suppose we have the equation a x 1 b x 2 y with unknowns a and b and one observation with values for x 1 x 2 and y with one observation and two unknowns the problem is under determined and there is an infinite number of combinations of a and b that would fit the observation the problem is both locally and globally non identifiable identifiability is by definition a binary problem i e a parameter is either identifiable or non identifiable given the model structure and the type of data available although the equations above are strict mathematical definitions the underlying idea here is to understand whether identifying unique parameters is theoretically possible whilst in practice numerical errors in computing might need to be accounted for the idea of identifiability proper is not concerned with whether parameter values are approximately equal as long as there is some difference it is theoretically possible to differentiate between alternative parameter vectors for example by increasing the sample size of data collected which then becomes a question of uncertainty reduction if the parameter vectors cannot be distinguished even in theory this is important information for a modeler throughout section 2 two simple models are used as examples we represent them firstly as equations because identifiability depends solely on the mathematical relationship between variables not on their real world interpretation to help understand the implications of the mathematical relationship we use two different interpretations firstly they are single rate equations that describe the relationship between biomass concentration and oxygen consumption rate r kg o2 m3s in two different conditions in a second more informal interpretation we consider a restaurant where a waiter is trying to guess for a table of two regular customers how much each person usually gives as a tip r in usd the equation for the first model describes one species x kg biomass m3 with parameters for biomass growth θ 1 and maintenance θ 2 or a case where the table for two pays a single bill on a company card x usd but pool their cash tips resulting in different tipping percentages θ 1 and θ 2 thus 3 r θ 1 θ 2 x it is clear that an infinite number of combinations of θ 1 and θ 2 with the same sum will produce the same value of r hence no unique pair of parameter values can be found and we say that the parameters θ 1 and θ 2 are non identifiable this is an example of a priori or structural non identifiability source i knowing the amount of biomass and oxygen consumption does not allow us to uniquely identify the values for biomass growth and maintenance knowing the size of the bill and the tip does not allow the waiter to determine how much each person contributes the equation for the second model describes a rate equation for two species x 1 a n d x 2 with biomass parameters θ 1 and θ 2 or a case where the table for two paid separate bills 4 r θ 1 x 1 θ 2 x 2 then the model is structurally identifiable in that both θ 1 and θ 2 can be uniquely estimated with two measurements as long as x 1 and x 2 are not zero if x 1 or x 2 is zero we have no information about the respective parameter with only one measurement we would have no information about the dynamics between the variables how they change relative to one other it would be an underdetermined linear system this is an example of non identifiability due to insufficient excitation of model dynamics source ii if r and or x 1 and x 2 is measured with errors or the equation is not accurate then with two measurements the parameters will still fit perfectly but with biased parameters overfitting to the noise with three measurements or more the observations will not perfectly fit the model it would be an overdetermined linear system best fit solutions need to be found instead so θ 1 and θ 2 will be uncertain and there may not be a well defined best fit solution either this is an example of non identifiability due to observation errors source iii knowing oxygen consumption and both biomasses allows us to estimate the oxygen consumption rates but only if there is a biomass to measure and all the values are accurately measured knowing each person s bill and the total tip is sufficient for the waiter to estimate the tipping rate for each person as long as they actually had a bill and the waiter remembered the totals accurately analysis of identifiability of model equations can be carried out in two ways i e testing whether identical outputs imply identical parameters or testing that different parameters yield different outputs which is referred to as output distinguishability distefano iii and cobelli 1980 in practice such analytic approaches commonly involve transforming the model in order to facilitate analytical manipulation the easiest approach to use depends on model properties and in some cases computer algebra can be useful e g chiş et al 2011 karlsson et al 2012 saccomani and bellu 2008 except for relatively simple models this analytical problem is usually hard to solve in this general form but can be extremely useful if an analytical approach shows a model is non identifiable it will generally reveal why and hence suggest possible remedies stigter et al 2017 this introductory overview will not provide a detailed discussion of analytic methods for identifiability as they typically require an advanced knowledge of mathematics in future improvement in user friendliness of software may enable wider use of these methods in the meantime we refer the reader to the following useful literature walter and pronzato 1996 provide a summary of four key approaches involving systems of derivatives from a power series taylor series expansion coefficients of series using lie derivatives identification of local state isomorphisms and differential algebra equivalent approaches for linear systems involve markov parameters coefficients of laplace transforms and identification of similarity transforms norton 1980 and norton et al 1980 discuss transformation and analysis in terms of eigenvectors normal modes and linear regressive re parametrization is presented in keesman and doeswijk 2009 and keesman 2011 stigter et al 2017 2015 show that combinations of analytical and numerical approaches may improve computational efficiency a review by miao et al 2011 also includes approaches that directly use our definition of model equations identifiability formulate it as a constraint satisfaction problem and use the implicit function theorem other useful overviews include cobelli and distefano 1980 norton 1982 and godfrey and distefano iii 1987 2 3 identifiability with an objective function 2 3 1 parameter estimation ideally estimating parameters would involve a straightforward solution of an inverse problem the values of parameters could be mathematically deduced from observations this is for example the case for a linear system of equations as already illustrated for equation 5 however outside this narrow case model inversion tends to be approached as an optimisation problem for example when the model consists of non linear complex equations or errors are present in data parameter estimation commonly involves optimizing an objective loss function m θ see overview in bennett et al 2013 marsili libelli 2016 sometimes with constraints on the solution it quantifies how well the model and parameters fit the data the optimal parameter values θ therefore minimize m θ a common relatively simple objective function is the mean squared error mse let f be a model predicting n outputs at locations x and times t i e y x t y 1 y n t f u x t θ given a set of n data d d 1 d n t for example noisy data d f u x t θ t ε with independent measurement error ε ε 1 ε n t obtained using some unknown true parameter vector θ t the mse m for a parameter vector θ is 5 m θ y θ d 2 n in many statistical methods the objective function used is the likelihood that a parameter vector is the true value given the observations in that case the optimisation involves maximizing the likelihood and therefore produces the maximum likelihood estimate of the parameter values mle the likelihood needs to capture the distribution of errors notably the covariance of the measurement errors including autocorrelation and heteroscedasticity e g schoups and vrugt 2010 bayesian parameter estimation uses a prior probability distribution in addition to the likelihood and optimisation yields maximum a posteriori probability map estimates of the parameter values stuart 2010 however bayesian statistics typically focuses on the use of distributions rather than point estimates such that it is more common to report the mean or median parameter value and its uncertainty rather than the map estimate this is further discussed in section 2 3 6 2 3 2 identifiability as a unique optimal solution optimizing an objective function in effect provides some flexibility in parameter estimation instead of the model output having to fit the observations exactly some deviation is permitted this poses a problem in terms of equation 2 more than one set of parameter values will fit given inputs and outputs if we allow some deviation optimisation tries to solve this by quantifying the deviation with an objective function and minimizing that deviation rather than specifying how much is permitted in circumstances where a perfect fit is not possible we settle for the best available and would ideally still like the solution to be unique for parameters to be identifiable however the objective function should have a single optimal solution bellman and åström 1970 that is instead of equation 2 if our best available solution has an objective function value of m θ m we want to know that 6 m θ m θ θ θ subject to the constraint that m θ m in an optimisation context uniqueness and identifiability therefore depend on the objective function used some objective functions can be essentially handicapped to lead to unique parameter vectors if used individually for example if our objective function was simply the sum of residuals also referred to as the bias then simplification of equation 7a below shows that data has a limited effect on identifiability d only appears in the constraint 7a y θ d y θ d θ θ subject to the constraint that y θ d m 7b y θ y θ θ θ subject to the constraint that y θ m d specifically if we consider a hydrological model where the model output consists of river flows equation 7b says that parameters are only identifiable if the total river flow with least bias can only be obtained by a single parameter vector this is unlikely to occur for many models there are typically many combinations of parameter vectors that could achieve a given total flow the objective function in equation 6 is less likely to have that problem equation 8 though identifiability still depends on the model structure and the precise observations used 8 y θ d 2 y θ d 2 θ θ subject to the constraint that y θ d 2 m equation 6 can also be extended to a case where multiple objectives are of interest in a multi objective parameter identification setting the aim is to check the uniqueness of a given vector of parameter values on the pareto front a compromise solution between objectives where no other parameter values provide better performance on every objective of interest at once e g maier et al 2019 in this case equation 6 can simply be written in vector form equation 9 expands the vector notation for a case with only two objectives for a parameter vector on the pareto front with m 1 θ m 1 and m 2 θ m 2 the problem is identifiable if 9 m 1 θ m 1 θ θ θ m 2 θ m 2 θ θ θ subject to the constraint that m 1 θ m 1 and m 2 θ m 2 solving any of equations 7 10 is usually difficult even if we know the value of m ahead of time instead it is easier to think of the problem as simply exploring whether there is a single global optimum global optimality refers to being the best solution across the entire parameter space as opposed to local optimality meaning that the solution is only better than the parameter values in its immediate neighbourhood in the parameter space two fundamental approaches for examining global optimality and therefore identifiability involve visualizing the response surface and examining derivatives these are discussed in the next sections while it may seem like evaluating identifiability with an objective function requires data this is not necessarily the case identifiability can be assessed without observations by using synthetic data e g shin et al 2015 assumptions are made about the form of the input data and the model is run in order to obtain exact error free output data in controlled experiments the input corresponds to the intended experimental conditions in other settings the inputs correspond to the conditions in which data are expected to be obtained with error free input and output data it should be possible to obtain a perfect value of the objective function i e m θ 0 and any non uniqueness in parameters arises only from model equations including objective function and properties of the input sources i and ii not from noise or errors source iii such an experiment typically assumes that the model structure is a perfect representation of the underlying system it should be noted that when evaluating identifiability with data the analyst also needs to think about whether they can efficiently locate that solution it is likely that the model is not identifiable if different solutions are reached with repeated runs of optimisation algorithms there is more than one global solution for example with different seeds for stochastic algorithms or different initial values for deterministic algorithms shin et al 2015 optimisation can however also fail for other reasons local optima are a common problem iterative optimisation algorithms require the use of stopping criteria that affect the accuracy of the final optimal solution and in many environmental modeling problems identifying the global optimum is not guaranteed even if it does exist 2 3 3 visualizing the response surface or fitness landscape rather than analyzing equations visualization can be used to investigate identifiability taking a geometric point of view the value of the dependent response variable is interpreted as a surface varying dimensionally as a function of the independent variables usually parameters this is referred to as a response surface box and draper 1987 and can be used to visually evaluate whether different parameters give similar outputs as required by equation 2 when the response variable is an objective function the response surface is referred to as an error surface or fitness landscape also see maier et al 2019 2014 as shown in fig 2 the optimisation algorithm can be thought of as navigating this landscape in order to find the optimal highest or lowest point suppose we are at the top of a mountain to be able to say that we are at the single highest point of a mountain range we would need to check that we are actually on a peak not on a plateau or ridge line and that no other peak in the range is the same height that is visually to assess whether multiple points are optimal we need to check for two problems 1 flat surfaces in some direction such that points in that direction with different parameter values have the same output value these cases are both locally and globally non identifiable 2 distinct peaks with an equal objective function value these cases are locally identifiable but globally non identifiable note that a flat surface is not the same as having a flat slope at the top of the mountain zero gradient the slope might allow us to stand vertically at the top of a peak but any step to the side brings us down the mountain rather than walking along a plateau or ridge this is further discussed in section 2 3 4 a flat surface could also occur in any direction for example north west not just along the main axes north south and east west the surface might be flatter in a direction that varies multiple parameters at once rather than when varying a single parameter at a time the extent to which the combined effect of simultaneously varying parameters on an objective function differs from varying the parameters separately one at a time is indicative of parameter interaction it is an important feature to look out for because it tends to be easily overlooked especially if relying on one at a time analyses saltelli and annoni 2010 fig 2 a plots the unweighted mismatch equation 6 between the model in equation 3 and two observations of the consumption rate r for x 1 and x 2 with the true value of the parameters vector as θ t 0 5 0 5 t the black dot the red line represents the possible values of parameters that minimize r because of the lack of identifiability that we observed in the previous section an infinite number of parameter combinations can exactly reproduce the data fig 2 b plots the unweighted mismatch between the model in equation 4 and two observations of the consumption rate r for x 1 0 5 x 2 0 5 and x 1 1 x 2 0 at θ t 0 5 0 5 t the surface shows a single optimal low point corresponding to the optimal minimum other objectives could yield a single peak corresponding to an optimal maximum around the peak are concentric ellipses of parameter values with the same objective function or likelihood a model is identifiable if there are no flat surfaces at an optimum and in the case that distinct optima exist there is only one global optimum the response surface can easily be visualized for models with one or two parameters when models have larger numbers of parameters visualizing selected one and two dimensional subspaces of the higher dimensional parameter space can be useful the response surface in such subspaces illustrate the change in the objective function as a function of one and two parameters respectively keeping others constant for instance a one dimensional sub space corresponds to a cross section through a landscape razavi and gupta 2015 provide examples of the response surface of a complex high dimensional model when visually viewed within different two parameter sub spaces see fig 3 therein by which the modeler can learn about a range of features in the response surface from small scale features such as roughness and noise to large scale features such as trends and multi modality one dimensional projections also known as dotty plots beven 2006 or the profile likelihood raue et al 2009 and two dimensional projections e g shin et al 2015 are also useful for models with large numbers of parameters dotty plots show the values of one or two parameters while varying all other parameters this can help suggest potential flatness and interactions between pairs of parameters globally across all parameter space rather than at any particular point see e g shin et al 2015 for our two example models fig 3a and c shows that a large range of parameters have approximately the same objective function value whereas in fig 3b and d there is a well defined optimum visual inspection helps to build understanding of the response surface of a model in practice it is often necessary however to use numerical or analytical tests of the objective function for example when a model has many parameters e g 20 or if optima are not visible in the plot because they lie in a narrow hole or on top of a narrow peak it is typical to use multiple methods for checking identifiability balsa canto and banga 2011 2 3 4 examining the value of derivatives absence of flat surfaces and distinct optima can be more directly assessed by examining the gradient of the response surface at points across parameter space intuitively one might think a flat surface would be identified by a zero gradient but the situation is more complex when evaluating the gradient at a point for a model with only one parameter a minimum can be identified as a parameter value where the first derivative gradient is zero i e a stationary point and the second derivative is positive such that the objective function increases away from the minimum see fig 4 a c the gradient is the slope at a point and the second derivative is its curvature for a maximum the second derivative should be negative such that the objective function decreases away from the optimum if at a point the first derivative is zero i e a stationary point and the second derivative is zero as well but the third derivative is non zero i e an inflection point the point is a saddle point not an optimum see fig 4d f if there are multiple parameter values with gradient equal to zero there are multiple local optima see fig 4g i non identifiability where the objective function is flat therefore occurs when the first second and third derivatives are all zero as well as further higher order derivatives though in practice they may have little effect the flatness is referred to as lack of sensitivity and the derivatives allow us to check for insensitive parameters at a particular point if examining the derivatives shows that there are multiple local optima then their objective function values need to be compared to see whether there is more than one global optimum examining the derivatives can therefore help identify both the problems visualized with the response surface for a model with more than one parameter partial derivatives are calculated with respect to each parameter separately this results in a gradient vector with the partial derivative for each parameter for an optimum the first partial derivatives should be zero and a matrix of second partial derivatives is then constructed considering pairs of parameters together including second partial derivatives of each parameter with respect to itself this is known as the hessian matrix the hessian describes how the slope of the response surface changes in every direction in other words its curvature to illustrate this fig 5 shows a hypothetical response surface that features a global maximum and a local maximum a saddle point in between and a flat area these four points are all stationary meaning the gradient vector first order partial derivatives is zero however the hessian matrices at these four points have significantly different properties if all the second partial derivatives are non zero the response surface is not flat if one is zero then the response surface may be flat if the higher order derivatives are also zero in particular if any third order derivative is non zero then there is a saddle point rather than a flat surface as shown in the figure the hessian at a stationary point can be cast as a quadratic function fitted to the response surface at that point a parabola for a single parameter function mathematically this is equivalent to using the first three terms of a taylor series expansion the quadratic illustrates how the hessian contains information about the slope in all directions not just along each parameter axis if the hessian is a diagonal matrix the associated parabola is orthogonal i e no pairwise interaction term in the quadratic function and therefore there is no interaction effect between the parameters locally at that point however if the hessian is non diagonal the associated quadratic is non orthogonal as a result of interaction between the parameters in fig 5b the quadratics describing the local and global maxima both have significant parameter interactions if the axes were compass directions the surface around the local maximum would be flattest along a northwest southeast axis and steepest along a northeast southwest axis the global maximum is flattest along a northeast southwest axis and steepest along a northwest southeast axis the interaction effects at a stationary point can be summarised with an eigendecomposition calculating the eigenvectors and eigenvalues of the hessian which respectively describe a set of orthogonal directions an orthogonal basis and the second partial derivative in that direction the advantage of this transformation is that the first eigenvalue is the largest second partial derivative in any direction and the last eigenvalue is the smallest the response surface at a point may therefore be flat if the last eigenvalue s are zero meaning that the second partial derivative is zero in the corresponding direction and the hessian is then referred to as singular conversely the point is an identifiable minimum if all eigenvalues are positive such that second partial derivatives are positive and the gradient and objective function increase in all directions the hessian is then referred to as non singular and positive definite similarly the point is a maximum if all eigenvalues are negative and the hessian is referred to as negative definite if the eigenvalues have different signs the point is a saddle point which in a two parameter case resembles a horseback riding saddle see fig 5b for more detail on the role and the numerical aspects of the hessian computation see for example marsili libelli 1992 seber and wild 1989 and marsili libelli et al 2003 the eigenvectors of the models in equations 3 and 4 were also shown in fig 2 for equation 3 the eigenvalues are 10 and 0 the hessian is semi positive definite indicating that an optimum exists but it is not unique for equation 4 the eigenvalues are approximately 1 31 and 0 19 the larger eigenvalue corresponds to the direction eigenvector along which the misfit changes the fastest in practice the eigenvalues of the hessian may not be examined directly the determinant of the hessian can be calculated as the product of eigenvalues amongst other methods if any of the eigenvalues are zero the determinant will therefore also be zero which also tells the analyst that the hessian is singular and may have a flat response surface in some direction in statistics textbooks the reader may come across this approach in likelihood based methods that analyse the observed fisher information matrix fim or the expected fim the observed fim corresponds to the negative of the hessian matrix when using the log likelihood as objective function the expected fim averages over the likelihood function rather than using observed values if the fim is non singular then the model structure is locally identifiable bellman and åström 1970 rothenberg 1971 in general it is difficult to demonstrate singularity due to numerical errors it is often better to perform a rank test with a singular value decomposition svd of the sensitivity matrix which consists of the partial derivatives of the model outputs with respect to each parameter see e g miao et al 2011 as the hessian indicated the objective function may also be flat due to interactions between parameters the combined effect of varying two parameters may cancel out such that the objective function does not change a variety of other indices have been proposed that measure the strength of interaction of parameters or its effect on identifiability see e g brun et al 2001 doherty and hunt 2009 hill and tiedeman 2007 sorooshian and gupta 1985 as an example active subspaces provide a global indicator constantine et al 2014 similar to how the eigendecomposition summarises the hessian if all singular values are non zero and of similar magnitude then the system is likely to be globally identifiable alternatively variance based sensitivity analysis methods can quantify the combined effect of specific combinations of parameters averaged across parameter space however sensitivity of a combination of parameters i e strong interaction does not necessarily mean that they are non identifiable dobre et al 2012 at any specific point the interaction may not result in a flat objective function derivative based methods including the techniques using sensitivity analysis are relevant when the derivative can be calculated whether analytically or numerically and non identifiability is more likely to occur due to flat surfaces than distinct optima there are few optima to find and examine derivative based methods may not be appropriate if there are numerical problems in the model or if the model output or objective function is characterised by discontinuities or many distinct solutions in these situations other approaches need to be used including examining the response surface 2 3 5 sensitivity versus identifiability identifiability analysis ia is closely related to sensitivity analysis sa sa seeks to measure the sensitivity of a model response to perturbations in different model parameters see norton 2015 for an introductory overview of sa saltelli et al 2000 as a comprehensive reference and razavi and gupta 2015 and pianosi et al 2016 for recent reviews of the state of the art if the objective function is insensitive to a parameter it means that the objective function is flat and the parameter is not identifiable however one needs to distinguish between sensitivity and identifiability even if the objective function is sensitive to all parameters it is not guaranteed that the parameters are identifiable for the reasons below sa aims to establish which parameters exert stronger or weaker controls on the model response which is an attribute of the forward problem in contrast ia aims to establish which parameters are identifiable given observations on the target response which is an attribute of the inverse problem measuring sensitivity on an objective function combines attributes of the forward and inverse problems but plays a filtering role that may obscure the information gained gupta and razavi 2018 local sensitivity analysis lsa at a nominal point in the parameter space is based on partial derivatives at that point similar to the approach discussed for identifiability in the previous section the same condition therefore applies that assessing identifiability using lsa of the objective function needs to be complemented by evaluation of other local optima in order to determine whether a single global optimum exists lsa provides a limited view on sensitivity only locally around the nominal point saltelli and annoni 2010 optimal parameter values therefore need to be obtained before using lsa to assess identifiability with an objective function alternatively a wide range of methods have been developed that seek global sensitivity analysis gsa measuring sensitivity across the entire parameter space examples include derivative based methods campolongo et al 2007 rakovec et al 2014 sobol and kucherenko 2009 variance based methods homma and saltelli 1996 sobol 2001 and variogram based methods razavi et al 2019 razavi and gupta 2016a 2016b the latter bridging derivative and variance based methods by characterizing the response surface across the full spectrum of perturbation scales haghnegahdar and razavi 2017 if a parameter has no effect on the objective function anywhere in parameter space it also has no effect at the optimum wherever it is located however even if a parameter is sensitive on average across the parameter space it is not guaranteed that it is sensitive at the global optimum the parameter still may not be identifiable gsa also allows evaluating the effect of parameters on model outputs and therefore investigating structural identifiability similarly if a parameter has no effect on the model output anywhere in parameter space it is non identifiable as noted above it is also common practice to use sa to identify interactions between parameters while remembering that the absence or presence of interactions does not necessarily indicate non identifiability sensitivity analysis is therefore an effective tool for identifying non identifiability but not for ruling it out see gupta and razavi 2018 and dobre et al 2012 for further detail on commonalities and differences between sa and ia 2 3 6 role of noise and systematic errors as presented in section 2 3 2 the use of an objective function provides a mechanism for taking into account noise and errors in assessing identifiability we can assess whether the best fit is only attainable using a single vector of parameter values as equation 6 depends on the observed data non uniqueness may indicate that the properties of errors prevent uniqueness source iii in addition to which quantities are observed source i and the conditions represented in the forcing data source ii by errors we mean any discrepancy between the modeled and observed systems these errors can take many forms noise or systematic errors in the measurement of either inputs or outputs discrepancy between the model structure and modeled system as well as misrepresentation of the properties of errors themselves as captured by the objective function we focus here on general principles rather than explaining the differences in effect of these different errors in addition to potentially affecting the identifiability of the optimal solution errors influence the identifiability problem in another key way the estimated parameters will generally change depending on the data used and the objective function invoked samples of observations with different data errors result in different estimates of parameter values values of parameters will therefore remain uncertain even if a unique optimum solution can be identified when using noise free data vanrolleghem and keesman 1996 if the parameters are anyway non unique this provides a strong motivation for switching from optimisation based to uncertainty based parameter estimation in that case rather than aiming to identify a single parameter vector the aim is to identify a set of many parameter vectors that all adequately fit the data i e to quantify the uncertainty in parameters see matott et al 2009 for a review of methods for quantifying uncertainty in parameters as foreshadowed in section 2 3 1 this is the approach preferred by bayesian inference which focuses on identifying a credible region of parameters within which an unobserved parameter falls with a given subjective probability it can also be approached more directly as a set membership problem as notably used in the generalised likelihood uncertainty estimation glue approach where only the set of behavioural parameters satisfying pre defined constraints are retained from the prior set beven 2006 guillaume et al 2015 it is worth repeating that even if the concept of a unique parameter vector is no longer used in parameter estimation it is still useful in the context of identifiability analysis in helping to rule out that non uniqueness is due to sources i and ii which is often avoidable in addition to being non unique parameters estimated with noisy data may also be biased in the sense that the mean parameter estimate may no longer represent the true value where it is known the uncertainty and bias in parameters also tend to decrease as sample size increases depending on the properties of the errors and increase as uncertainty in measurements increases these phenomena can be illustrated with our simple objective function m θ with a single scalar parameter consider the following model for oxygen consumption rate 10 r 35 8 θ 4 x 1 1 80 θ 1 2 x 2 3 8 where θ is again the biomass parameter we take two noise free measurements at x 1 1 x 2 1 we then investigate behaviour of the objective function for different true parameter values summarised in fig 6 in the top row of the figure we have set θ t 0 5 and the bottom row we have set θ t 0 7 in the first column of fig 6 we plot the rate r as a function of the parameter the black dots depict r θ t and the blue dots represent one realization of noisy data d r θ t η with η 0 1 in the second column of fig 6 we plot the loss functions m θ for noise free and noisy observations the loss function is minimized at the true parameter value black dot the blue dot represents the estimated parameter for the noisy data the sensitivity of the recovered parameter is heavily dependent on the true parameter value even for the same noise magnitude in fig 6b the parameter is less sensitive the response surface is flatter with noisy data many parameters produce a rate r that is close to the data in contrast when θ t 0 7 fig 6e the model response has a much larger gradient around the estimated parameter a sampling procedure can be used to estimate the reliability of recovering the true parameter in the presence of noise of varying magnitude miao et al 2011 for example let the noise η be normally distributed with mean zero and standard deviation σ the third column of fig 6 depicts the recovered parameter values for 100 realizations of the noise for three levels of noise σ 0 01 0 05 0 1 in every case we find a unique minimum the parameter is at least locally identifiable but the minimum is no longer the true parameter that we could recover with noise free data the uncertainty of the estimated parameter depends on the noise level and the response surface in a local region around the true parameter when the response surface is flat top row the parameter estimates are much more uncertain than when the response surface has a larger gradient bottom row in practice noise and errors are usually unavoidable an analyst cannot tell for sure whether they are using or collecting the right type of information to obtain reliable estimates of parameters or model outputs representing real world phenomena instead they can only make an educated judgement that is dependent on how closely that model structure and data reflects reality identifiability analysis by definition tests for non uniqueness with a given model structure and understanding of errors and therefore objective function if these assumptions poorly reflect reality then identifiability analysis may be of limited use in informing the analyst s judgement the better the model structure and the dataset the more useful identifiability analysis will be 3 identifiability analysis in practice parameter identifiability is a fundamental concept in parameter estimation and therefore in many types of environmental modeling modelers should know whether the type of data they are using is capable of identifying unique parameter values they should know if they do not have information about the right quantities within a given model structure source i they should know if the forcing data and associated response does not reflect the conditions needed to estimate their parameters source ii and they should know if the errors are such that more data will not improve parameter uncertainty source iii first tests are often even trivial to implement simply repeat an optimisation with different initialisations to check if it returns different values for parameters see e g shin et al 2015 sensitivity analysis can be performed using black box software and dotty plots can easily be produced for each parameter amongst other methods borgonovo et al 2017 knowing that an identifiability problem exists however is only a starting point determining the cause of non identifiability within a model structure forcing data or messy dataset is likely to fall at one of two extremes quite difficult or trivial for example trying to estimate snow related parameters on a tropical island excluding the trivial case issues of identifiability tend by their very nature to be concealed from the modeler s view if the issues were obvious the modeler would have addressed them and no identifiability issue would be experienced similarly improving identifiability is often non trivial it requires thorough understanding of the cause s of non identifiability and the time knowledge and resources to collect or select new data and modify model structures or objective functions even simple fixes such as fixing parameter values or using a different model structure may have significant and often overlooked impacts on results while tools are available to support identifiability analysis there is still a great need for development of methods software and training to ensure all modelers are able to assess and react appropriately to non identifiability in the meantime to provide some guidance we make five pragmatic recommendations and briefly discuss computational considerations 3 1 know whether your model parameters are identifiable it cannot be repeated too often identifiability is a fundamental concept in parameter estimation particularly given the difficulty of eliminating non identifiability it is important to explicitly acknowledge and document for other analysts how identifiability concerns have influenced the modeling process and how they have been addressed regardless of the response taken the analyst should be transparent about their treatment of identifiability documentation should describe whether the model is identifiable and how this was assessed if the model is non identifiable the documentation should describe the expected consequences for the particular analysis and the responses that were taken as a starting point a modeler should keep an eye open for symptoms of non identifiability box 1 and apply simple diagnostics like repeated optimisation sensitivity analysis and 1 or 2d dotty plots if there are signs of non identifiability a reviewer should at least want to know what the source of the non identifiability is in order to judge whether it is acceptable model structures of some complex models are known to be non identifiable for example in spatially distributed groundwater modeling there is a common risk of interactions between parameters in adjacent grid cells similar to the case described in equation 3 uniquely estimating transmissivity therefore depends on having water level measurements within appropriate parts of the flow region neuman 1973 similar problems have also been observed for rainfall runoff models with many parameters shin et al 2015 if identifiability is not already known techniques in section 2 can be used either analytically or with ideal input data and noise free outputs to show the model structure is responsible source i using synthetic data real input data and noise free outputs can reveal if the input data provide insufficient excitation of modes of model behaviour source ii for example parameters of some hydrological models may only be active in rare conditions gupta and sorooshian 1983 such as observed streamflow in a drought period may not provide information about parameters related to flood periods and vice versa for some models it can be possible to use analytical methods to identify the data characteristics necessary to achieve persistence of excitation norton 2009 other techniques can also be used to assess information in data such as by observing the effect of data on parameter sensitivity wagener et al 2001 examples include time varying or spatially varying sensitivity analysis gupta and razavi 2018 pianosi et al 2016 razavi and gupta 2019 and notablythe dynamic identifiability analysis technique wagener et al 2003 if tests suggest that neither the structure source i nor input data source ii are responsible then the role of noise and systematic errors can be explored in input data output data model structure and error model testing alternative model data combinations it may also be that the optimisation algorithm is failing to find the unique solution due to multiple regions of attraction minor local optima roughness poor sensitivity and non convex shape of the response surface duan et al 1992 3 2 consider how non identifiability fits within your analysis in addition to helping to avoid negative effects of non identifiability box 1 it is worth keeping in mind the benefits of identifiability analysis box 2 there is a variety of ways it might influence modeling and model based analyses in practice while non identifiability means the modeler lacks the appropriate information to choose between alternative models in practice the right information may take too much time or resources to obtain or may never be available at all depending on the prediction to be made it may not matter that multiple models are plausible the differences in prediction may be small enough to be acceptable or able to be addressed by decision makers through adaptive management williams 2011 how best to handle non identifiability is highly dependent on purpose and context the modeler must use their professional judgement to select practices that suit their problem consistent with expectations in their modeling domain and document the reasons for their choice acknowledging possible alternatives in addition to simply tolerating non identifiability there are three other high level philosophical approaches possible wagener and gupta 2005 which can be viewed as quantifying the effect of lack of information improving the information used by obtaining the right type of observations and or inputs and avoiding the need for missing information by modifying the model these four options are briefly described here consider whether non identifiability can be tolerated uncertainty due to identifiability issues whether explicitly quantified or not can be assessed in terms of its risk that is its effect on the final product of the analysis such as quantities of predictive interest for decision making for example if the uncertainty induced by non identifiability does not change a decision then perhaps it can be ignored guillaume et al 2015 this is typically the default approach if modelers are aware of identifiability issues by professional judgment modelers often assert that a given issue is not significant in order to be able to provide results efficiently rather than futilely trying to tie up every loose end guillaume et al 2017 consider quantifying uncertainty due to non identifiability it may be advantageous to formally quantify the effect of non identifiable parameters for example if a process based model is used to estimate model outputs not used in parameter estimation then a complex model may be necessary even though it is not strictly identifiable in the context of unsaturated zone models in hydrology for example brunner et al 2012 observed that certain observations can significantly reduce predictive uncertainty without informing any specific parameters in cases such as these the limitations of the model should be documented and the uncertainty induced by non identifiability should be quantified for some problems dedicated methods are available that identify parameter vectors that yield identical outputs or objective function values e g null space monte carlo tonkin and doherty 2009 consider obtaining information to make the model identifiable some symptoms of identifiability may be considered intolerable such as non uniqueness of optimised parameters lack of observability of a parameter or lack of transferability of a model to specific conditions for instance from flood to drought in hydrological applications in these situations steps can be taken to improve identifiability by measuring different quantities source i obtaining data in different conditions source ii or that is more accurate source iii literature on data acquisition planning and optimal input design e g dausman et al 2010 freeze et al 1992 goodwin and payne 1977 tiedeman et al 2004 walsh et al 2017 along with using soft data and expert knowledge gharari et al 2014 can be of assistance targeted reviews of types of data used for parameter estimation can also provide guidance e g for hydrogeology schilling et al 2019 alternatively it may be possible to constrain the solution space by making assumptions to reduce the number of unknowns or providing information about which values are preferred using prior distributions stuart 2010 or regularization tikhonov et al 1995 machine learning based methods provide another example non uniqueness can occur due to redundant information in inputs maier et al 2010 or local optima due to non linearities kingston et al 2005 and steps can be taken to avoid them including through input variable selection may et al 2011 consider modifying model information requirements it may be more appropriate to use a model that is identifiable with the available information grayson et al 2002 jakeman and hornberger 1993 young et al 1996 the principle of parsimony suggests that unnecessary complexity should be discarded a number of model selection criteria prefer models with less parameters e g akaike information criterion akaike 1974 in the case where particular observations can reduce predictive uncertainty without informing any specific parameters information is likely being provided about combinations of parameters these combinations could possibly be replaced by a single parameter within a simplified model e g croke and jakeman 2004 young et al 1996 this is different to eliminating parameters or fixing parameter values a single combined parameter captures information from data about multiple more detailed parameters and their total effect after accounting for interactions present in the parameter estimation data parameter elimination or fixing on the other hand introduces new information in the form of assumptions about the value of that parameter if those assumptions are incorrect even if the model performs well in some circumstances it will be right for the wrong reasons and may predict poorly eliminating the symptoms of non identifiability however typically requires invasive changes to the model or model identification procedure for example using different or transformed parameters selecting specific data periods changing model structure and or using a more sophisticated objective function it is important that these changes do not undermine the ability to understand the concepts that went into creating the model in the first place 3 3 computational considerations constructing response surfaces cross sections dotty plots exploring parameter sensitivities or estimating the impact of noise can require large numbers of model evaluations this can render identifiability analysis infeasible if the model being analysed is computationally expensive while high performance computing may be a solution in some cases it may also be possible to use computationally frugal methods hill et al 2016 or to build an approximation of the model s response surface to reduce the computational demands of identifiability analysis the remainder of this section focuses on this last option which is rapidly maturing a response surface can be explicitly represented mathematically by model emulation resulting in a surrogate model this has two key advantages firstly having an analytical representation of the response surface allows additional mathematical analyses secondly the computational cost of building and running a surrogate model is typically less than relying on the original model therefore facilitating the use of more computationally intensive methods the efficacy of response surface methods depends on the ability to build and capture the salient features of the response surface with a feasible number of model simulations the approach known as response surface methodology box and draper 1987 approximates the response surface by a quadratic function and allows analysis of flatness near the optimum although requiring a limited number of model simulations quadratic response surfaces are often poor representations of a model response surface alternative methods that can approximate the non linearity of the model response and higher order interactions between variables are typically needed polynomial chaos expansions pce sudret 2008 xiu and karniadakis 2002 and gaussian processes gp marrel et al 2009 oakley and o hagan 2004 are two popular and effective methods for approximating highly non linear response surfaces gp and pce can both be efficiently used as surrogates for the simulation model in any of the numerical identifiability methods outlined in this paper for example gradient and hessian data can be computed analytically from the approximations and used for derivative based identifiability analysis or the approximations can be used as surrogates within the likelihood function when conducting bayesian inference e g blanchard et al 2010 or calibration using maximum likelihood estimation e g arendt et al 2012 in the water resources sector and in groundwater modeling particularly razavi et al 2012 and asher et al 2015 respectively provide reviews of a range of surrogate modeling methods in addition to the aforementioned techniques based upon function approximation reduced order models can also be used to decrease the computational cost of analyzing expensive simulation models carlberg 2015 cui et al 2015 soize and farhat 2017 these methods do not construct response surface approximations but rather solve the governing equations on a reduced basis the advantage of such an approach is that one can cheaply obtain approximations of the entire solution to the governing equations rather than a small number of functions of that solution moreover unlike surrogate methods reduced order models do not suffer from the curse of parameter dimensionality the number of simulations required to build a response surface grows rapidly with parameter dimension consequently the practical use of response surface methods is typically restricted to models with the order of 10 parameters for example a survey of the literature has shown that less than 65 of the applications of these methods are on functions having less than ten parameters and more than 85 have less than 20 razavi et al 2012 the exact number of simulations required to achieve a given accuracy is model dependent however simulation requirements can be reduced by using methods that exploit the structure in the response surface constantine 2015 gorodetsky and jakeman 2018 jakeman et al 2015 and goal oriented approximations thereby focussing computational effort on regions and or dimensions of parameter space that significantly affect quantities of interest lastly when using a surrogate model the user needs to be mindful of approximation uncertainty that is the potential errors of the surrogate model that can be added to the process of parameter estimation razavi et al 2012 illustrate this source of uncertainty by simple examples see figs 2 3 and 6 therein and review methods to address it 4 conclusions this article has provided an introductory overview to key issues in identifiability analysis and fundamental concepts and methods to quantify and understand the impact of non uniqueness of parameters on modeling results identifiability analysis aims to i assess whether it is possible to identify unique parameter values and ii understand why that occurs it therefore makes an essential contribution to more systematically learning about sources of uncertainty and how to reduce them and hence improving the practice credibility and outcomes of modeling exercises in the long run our discussion of identifiability analysis in practice emphasizes that the right tools to use depend on modeling context especially the purpose and the modeler s professional judgement our fundamental recommendation is therefore that assessment of whether a modeling exercise is fit for purpose explicitly needs to address three points whether the model is identifiable the source s of any non identifiability issue the extent to which any non identifiability impacts the problem at hand it is important to explicitly acknowledge and document for other analysts how identifiability concerns have influenced the modeling process and how they have been addressed regardless of the response taken the analyst should be transparent about their treatment of identifiability documentation should describe whether the model is identifiable and how this was assessed if the model is non identifiable the documentation should describe the expected consequences for the particular analysis and the responses that were taken the modeler s professional judgement naturally depends on knowing what alternative methods of identifiability analysis are available and why they might be useful the emphasis of this introductory overview is therefore on awareness raising encouraging wider acknowledgement of non identifiability as a critical issue and that there are methods to assess it and its sources there is however still much to be done to make practices for assessing identifiability easier to apply and for them to be commonly applied and there are huge opportunities to improve identifiability of models and reduce uncertainties especially where there is a large community of resourceful researchers that uses a particular non identifiable model there are several well known hydrological and water quality models for instance where sensitivity analysis indicates they are structurally non identifiable but other sectors are replete with over parameterized models that do not use mitigating mechanisms such as regularization or investigate alternative model structure hypotheses that may lead to improved identifiability promoting the thoughtful use of identifiability analysis practices provides a firm foundation for improving models into the future acknowledgements this introductory overview builds on an iemss conference paper marsili libelli et al 2014 joseph guillaume was supported by academy of finland funded project wasco grant no 305471 and emil aaltonen foundation funded project eat less water the views expressed in the article do not necessarily represent the views of the u s department of energy or the united states government sandia national laboratories is a multimission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 
26167,new zealand s isolation and primarily land based economy make it heavily dependent on future global climate and economic change we combined global assumptions from the new ipcc scenario framework and local conditions to conduct a site specific assessment of future scenarios we used a spatially explicit integrated assessment that combined economic and biophysical models to help quantify the potential impacts of a complex set of climate induced impacts coupled with regional environmental and land use policy three scenarios were chosen involving stakeholders to gain insight into local sensitivity to climate versus socio economic change the results provide the direction of changes for demographic economic and environmental factors including some ecosystem services they highlight the strong influence of policies on regional economic output and ecosystem services relative to just accounting for the physical impacts of climate change the quantitative modelling results might be used to identify adaptation options in light of climate change implications keywords climate change ecosystem services land use change spatial modelling biophysical model economic model 1 introduction new zealand s mixed economy includes a range of sectors e g primary production 1 1 in the new zealand context the primary production sector includes agriculture fisheries forestry and horticulture and is the main economic driver along with tourism the term is used throughout the paper energy tourism that depend heavily on the state of natural resource capital its small size relative geographical isolation and strong reliance on trade and migration make its economy particularly vulnerable to the world s economic situation royal society of new zealand 2016 both climate and socio economic conditions are difficult to predict socio economic developments cannot be captured appropriately by simply downscaling global scenarios frame et al 2018 reisinger et al 2014 van vuuren et al 2010 new zealand s approach to managing natural resources and the weight given to environmental considerations may critically influence the impacts of climate related changes in land and water resources on its society and ecosystems in new zealand research has been extensive at the sector level to look at the impacts of climate change adaptation and mitigation options for the agricultural and forestry sectors ministry for primary industries 2016 warrick et al 2001 mainly on the production side cross cutting issues have also been considered including economic analysis life cycle analysis farm and catchment analysis social impacts ministry for primary industries 2016 and risk assessment ministry for the environment 2008 however changes in climate will probably have interacting and feedback loop effects on the environment compounded by changes in global markets and international policy and in turn on society challinor et al 2018 the ability to understand the full spectrum of impacts on ecosystem services is necessary if decision makers are to respond to the trade offs and benefits of both changing climate and changing land use elmhagen et al 2015 hauck et al 2015 rounsevell et al 2010 shaw et al 2011 the new scenarios developed in the fifth assessment report ar5 of the intergovernmental panel on climate change ipcc separated greenhouse gas emissions trajectories the representative concentration pathways rcps from potential socio economic pathways ebi et al 2014b ipcc 2014 kriegler et al 2014 o neill et al 2014 van vuuren et al 2014 in what is referred to as a parallel process the resulting framework provides greater flexibility to explore mitigation and adaptation strategies the motivation is to provide stakeholders with insights into the implications of various decisions shifting from a predictive mindset to a more exploratory and option driven mindset for solution pathways many scenarios models and standard methods have now been developed to quantify indirect and direct drivers of change mallampalli et al 2016 pichs madruga et al 2016 that are ready to be used for impact assessment however multi scale issues ranging from global to regional and national requires a participatory approach if there is to be meaningful engagement with the scenarios and the pathways most likely to deliver change and in order to inform decision making and ensure transferability to other areas frame et al 2018 kebede et al 2018 the utility of the parallel process is determined in our view by three qualities synonymous with boundary objects and linking scientific knowledge with action cash et al 2003 credibility which relates to the scientific adequacy of the technical evidence and arguments salience which deals with the relevance of the assessment to the needs of decision makers legitimacy which involves the production of information and technology that are respectful of stakeholders divergent values and beliefs the complexity of the human nature system interactions and interdependencies makes it difficult to comprehend the feedbacks tipping points side effects trade offs and benefits in ecosystem services of both climate and socio economic drivers integrated assessment has the potential to better represent these combined effects on ecosystem services the economy and society dunford et al 2015 though it requires working across scales to accommodate different issues disciplines processes and scales kelly et al 2013 to capture this complexity several modelling approaches can be used a fully integrated modelling approach is complex as processes may operate at various scales with various influencing drivers pichs madruga et al 2016 integrated assessment models iams such as the image model incorporate several modules for both human and natural systems have been used at the global scale by the ipcc stehfest et al 2014 they are a very useful tool to understand and project the interaction and feedback loops between the climate and the economic system van vuuren et al 2011 iams have been used for raising awareness and testing future scenarios at the european scale climsave harrison et al 2015 holman et al 2017 creating a space for discussion with different groups of stakeholders harrison et al 2015 jäger and omann 2013 mixing qualitative and quantitative understanding of climate and socio economic impacts enables stakeholders to explore options for the future and to understand cross sectoral interactions and vulnerabilities absar and preston 2015 harrison et al 2013 however since human nature system interactions and feedbacks are complex simplifications are necessary for instance the assessment of global land use and climate change impacts across rcps and shared socio economic pathways ssps involves a relatively crude representation of the agriculture and forest sector with results aggregated across regions and forest type popp et al 2017 riahi et al 2017 iams may also under represent a critical phenomenon and therefore run the risk of misrepresenting responses to climate change at a local scale for instance the amplitude of adaptation measures studies at the regional scale have tended to loosely couple the quantification of land use change scenarios under future climates with the use of biophysical models to estimate impacts on ecosystem services booth et al 2016 byrd et al 2015 krkoška lorencová et al 2016 langerwisch et al 2018 qiu et al 2017 schirpke et al 2017 this loose coupling preserves the integrity of specialised models and keeps control of the interpretation of passing data between models kelly et al 2013 changes and interactions between sectors are particularly complex in lowland environments where climate change could have an impact on productivity and trends in the global economy may have an even greater influence on the land and its use in this paper we evaluate the impacts of climate and land use change in a lowland environment based on plausible alternative futures the scenario analysis follows the principles of the so called story and simulations sas approach which combines narrative stories supported by simulations from quantitative models alcamo 2008 our analysis is done across a diverse set of plausible scenarios to 2065 and 2100 using an integrated assessment modelling approach that incorporates biophysical economic and socio demographic models in a single framework we use this framework to estimate the impacts of climate and land use changes on a suite of ecosystem services the paper contributes to the integrated assessment literature in several ways first it presents a framework for how to combine global assumptions and local conditions to conduct site specific assessments second it details how a diverse set of models can be integrated to help quantify the potential impacts of a complex set of climate induced impacts coupled with regional environmental and land use policy third it highlights the strong influence of policies on regional economic output and ecosystem services relative to just accounting for the physical impacts of climate change furthermore we argue that as there are few tools currently available for local government decision makers and business groups to identify potential adverse outcomes of climate change and to weigh adaptation options the qualitiative quantitative mixed method approach discussed provides an opportunity to promote foresight at the local scale the balance between methodological rigorous and resource intensive approaches needs to be balanced with nimble modelling that engages stakeholders without imposing a burden yet still providing sufficient credible guidance to be both salient and legitimate though open to revision as new information becomes available in other words there is a tension between providing highly structured fully developed models which take a long time to produce and the need to engage stakeholders in meaningful processes even if with declared omissions and incomplete information this paper presents an example of addressing this tension as a means to further productive research the paper is organised as follows first we describe the case study area where we tested the scenario analysis second we introduce the elements of the future scenarios as a mix of climate socio economic and policy assumptions three scenarios were chosen to represent individually plausible yet contrasting exemplifications of socio economic developments in new zealand that could matter for the future impacts of climate change societal vulnerability to climate change and adaptation options at 2065 and 2100 third we introduce the method for the quantitative integrated assessment composed of drivers of change and impacts on ecosystem services finally we present results from the scenarios and a discussion of our findings 2 methods 2 1 case study area a set of national climate change scenarios to 2100 were tested across a landscape gradient alpine upland lowland coastal marine as part of a larger project frame and reisinger 2016 tait et al 2016a b here the lowland case study is taken to illustrate an assessment of impacts and implications for three scenarios to identify a lowland case study area we compiled a nationwide shortlist of options which were assessed against several key criteria including access to existing scientific data and models potential significance of climate change impacts diversity of land use a wide range of natural vegetation types and current relationships with local stakeholders as the bay of plenty region contained several potential locations discussions were held with the regional council and as a result the kaituna catchment including a coastal zone around papamoa beach bay of plenty on the east coast of the north island was identified and an initial research plan was constructed the local government authority were part of the initial development of the project and have remained closely involved throughout the overall area is 126 100 ha including lake rotorua 11 600 ha the lower part downstream of the lake is a typical lowland environment in new zealand with a mixture of natural ecosystems freshwater wetlands and native forests and a wide range of primary production maize cropping kiwifruit horticulture forestry dairy sheep and beef farming the land in primary production is mainly grassland 47 000 ha used for dairy in the lowland areas and sheep and beef farming in the hill country fig 1 exotic forestry and indigenous forest are present in the upper part of the catchment while the lower part is covered by the kiwifruit industry c 6000 ha near te puke this area and its existing land uses are likely to be affected by both drought and flood related climate issues kenny 2011 the local environmental authority is developing a community strategy for a vision for the future issues include pressures from increased population growth and land use intensification a workshop of local stakeholders organisations and agencies was held to present the proposed research identify issues related to land use change and refine the proposed work based on what was most important to stakeholders identify additional data sources and spatially locate the wider impacts and implications of climate change ausseil et al 2017 examples of issues raised that we could address included how could climate change exacerbate or counteract current land use change trends how will kiwifruit industry be impacted by climate change how will sea level rise impact the kaituna catchment 2 2 scenario elements the ipcc research community recently introduced a new scenario framework for climate change research that combines levels of radiative forcing characterised by the representative concentration pathways rcps and alternative plausible trajectories of global development described as shared socio economic pathways ssps ebi et al 2014a kriegler et al 2014 o neill et al 2014 van vuuren et al 2014 our architecture adopts these two elements ebi et al 2014b and adds one national scale element the shared policy assumptions spas frame and reisinger 2016 frame et al 2018 2 2 1 representative concentration pathways rcps the ipcc fifth assessment report ar5 adopted four greenhouse gas concentration trajectories the rcps climate outcomes based on rcps are modelled via the coupled model inter comparison project cmip5 through numerous earth system models or general circulation models gcms six gcms bcc csm1 1 cesm1 cam5 gfdl cm3 giss e2 r hadgem2 es and noresm1 m had been selected to update and improve regional scale projections of new zealand climate trends and variability to 2100 they were chosen because they validated well new zealand climate tait et al 2016b we used the output variables that were precipitation maximum and minimum air temperature relative humidity solar radiation and wind speed each variable was calculated on a regular grid 0 05 at a daily monthly and annual temporal resolution for the 1971 2100 period for our study area future climate change projections based on the extreme scenario rcp 8 5 and the average of several climate models by the end of the century compared with the present day ministry for the environment 2016a tait et al 2016a show a general increase in mean air temperature across seasons the number of hot days dry days and heavy rainfall the number of cold nights and wind speed may decrease while precipitation shows a seasonally variable trend with generally more rain in summer and less in spring table 1 2 2 2 shared socio economic pathways ssps unlike earlier assessments ar5 scenarios for climate change decoupled the climate model outputs expressed through rcps from their socio economic drivers expressed through the concept of shared socio economic pathways ssps ssps describe plausible trends in the evolution of society and the global economy they outline plausible alternative states of human and natural societies at a macro scale including both narrative and quantitative elements of socio ecological systems such as demographic political social cultural institutional lifestyle economic and technological variables and trends the global ssps are designed to be extended to regional and sectoral scenarios but make no assumptions about global or national level climate change policy absar and preston 2015 cradock henry 2017 2 2 3 shared policy assumptions for new zealand spas the shared policy assumptions spas for climate are specific to new zealand and were developed by frame and reisinger 2016 spas describe potential climate change mitigation and or adaptation policies not specified in the ssps they provide a third axis to the scenario matrix and allow national level development choices that may reinforce global trends or actively go against them they contain a mix of climate specific policies and non climate specific policies that have indirect but significant climate impacts or influence climate related vulnerability or adaptation options this enables us to create a mix of new zealand specific situations of greater relevance 2 2 4 scenario development for the wider project covering all five case studies on the landscape gradient alpine upland lowland coastal marine three scenarios were developed for new zealand at 2065 and 2100 the selected set as shown in table 2 covers a spread of possibilities that would provide sufficient material to explore the influence of radiative forcing unspecified pacific rcp 8 5 3 a and kicking screaming rcp 4 5 3 a keeping ssp and spa comparable and the influence of socio economic conditions kicking screaming rcp 4 5 3 a and clean leader rcp 4 5 5 f fig 2 keeping radiative forcing comparable the first step for scenario development following the involvement of the local government authority in the selection of the research site was to develop some national level narratives that would describe new zealand in relation to the rest of the world frame and reisinger 2016 appendix a these scenarios were developed using some indicators from the international institute for applied systems analysis iiasa database iiasa 2016 that have provided information to support the ssp development ebi et al 2014a van vuuren and carter 2014 frame et al 2018 designed a general methodology and guiding principles to downscale these national level narratives by incorporating locally specific conditions and concerns e g adaptation to sea level rise adaptation to management practices that could be directly accounted for in the modelling framework it should also be noted that as part of a much larger project and associated research projects the research team were closely embedded with the local authority and stakeholders in the development and sense checking of the scenarios the results of this process are discussed elsewhere frame et al 2018 cradock henry et al 2018 following this methodology the downscaling modelling processes used the o neill et al 2014 elements relevant for defining both challenges to mitigation and adaptation these include elements relating to demographics economic development environmental factors resources welfare institutions and governance technological development broader societal factors and policies table 3 they were evaluated quantitatively via modelling where feasible see section 2 3 2 3 integrated modelling assessment while we could not obtain quantitative models for all the elements suggested by o neill et al 2014 we were able to inform four elements demographics economic development environmental factors and resources fig 3 these models have inputs of climate variables from the rcp scenario only primary production pests and disease sea level rise socio economic variables from ssp only demographics or a combination of both drivers of change for ecosystems and their services can be classified into indirect and direct drivers diaz et al 2015 millennium ecosystem assessment 2005 this typology is now used in scenario analysis by the intergovernmental platform for biodiversity and ecosystem services pichs madruga et al 2016 the elements suggested by o neill et al 2014 can easily be categorized as indirect or direct drivers indirect drivers include economic demographic socio cultural governance and institutional and technological influences direct drivers include climate change over exploitation invasive alien species and land use change they then have an impact on natural capital and ecosystem services fig 3 as we were constrained from covering the whole suite of biodiversity and ecosystem services we chose to prioritise our assessment on the vulnerability of wetlands to water supply change as this was of most interest to stakeholders and have used ecosystem services models developed for new zealand to look at changes in some key indicators i e sediment and nutrient loss water yield and greenhouse gas emissions we took a coupled component modelling approach kelly et al 2013 as much as possible as it was essential to integrate the various models to create a broader perspective on how the components influence each other landowners can adapt to both climate signals and socio economic signals so in a first step a global economic trade model climat dge was used at the national level to study the efficient re allocation of resources within the economy and the response over time to resource or productivity shocks fernandez and daigneault 2015 climat dge was tuned to closely match projections published in the iiasa rcp and ssp databases iiasa 2016 and adapted to the global and new zealand populations provided by a demographic model cameron 2013 iiasa 2016 rutledge et al 2017 gross domestic product and greenhouse gas emissions doing so facilitated the estimation of global and domestic commodity prices that could be used as inputs to other modelling used in this study namely for the land use change model nzfarm daigneault et al 2018 however land use is also influenced by changes in yields in various primary sectors due to climate change this is covered by various tailored biophysical models that have been calibrated at the sector level including cropping holzworth et al 2014 livestock farming keller et al 2014 forestry kirschbaum et al 2012 and kiwifruit tait et al 2017 this is summarised in table 4 to separate the influence of climate change from influence of socio economic and climate change we tested scenarios using rcp using productivity change only or rcp and ssp using productivity change and commodity prices other important aspects necessary to incorporate into the land use change modelling relate to costs of production in particular pest and disease control is of critical importance to protect the production system climex is a mechanistic species distribution model that represents the vulnerability of the land to pest invasion kriticos et al 2015 while it does not provide a quantitative measure of extra costs due to added pesticides it does give some indication of future concerns and assumptions to be made for the kiwifruit industry winter chilling is essential to ensure flowering buds as the risks for warmer weather in the case study area increase the likelihood of needing additional chemical application such as hydrogen cyanamide hc increases cradock henry 2017 bioclimatic model outputs were available with and without hc to test in the three scenarios tait et al 2017 for cropping the impacts of climate change on maize silage were assessed by considering model runs with and without adaptation of crop genotype short and long maturity hybrids and sowing dates earlier in response to warmer temperature these two key independent tactical adaptation options are used by farmers to adjust to year by year weather variability teixeira et al 2017 and are expected to play an important role under climate change ipcc projections of global sea level rise between now and 2100 cover a range of 0 4 1 m church et al 2013 small increases of the order of 1 5 in wave height and storm surges from climate change effects are also likely by 2090 stephens and bell 2015 we mapped a future sea level rise of 0 8 m by 2100 ausseil et al 2017 following national guidance ministry for the environment 2016b vulnerability of natural ecosystem was assessed for freshwater wetlands bodmin et al 2016 while impacts on key land use based ecosystem services were assessed through nzfarm daigneault et al 2018 using indicators such as nutrient loss water yield carbon sequestration and greenhouse gas emission ausseil et al 2013 see appendix c for details the three scenarios were assessed by all the quantitative models except for demographics only tested for ssp3 and sea level rise the models from table 4 were run with assumptions fitting the rcps ssps and spas table 5 we chose the set of biophysical assumptions most internally consistent with the socio economic assumptions these include the use or not of additional chemical hc for kiwifruit cradock henry 2017 tait et al 2017 and adapted sowing dates for cropping teixeira et al 2017 2018 the three scenarios were compared for two timeframes of interest to stakeholders to reflect mid century 2065 and end of century 2100 future projections 3 results scenario impacts were modelled at a temporal scale of up to a decade between 2015 and 2100 and we present results for two points 2065 and 2100 to highlight the main trends arising table 6 shows the direction of change up to 2065 and 2100 from the quantitative models relative to a baseline case with no climate change and historical socio economic conditions 3 1 demographics in ssp3 new zealand s population trends do not mirror global trends ssp3 has the highest population at 2100 at the global scale while in new zealand population peaks then declines to a final value of 3 8 million lower than the current population of 4 4 million in ssp5 new zealand population will more than double to almost 10 million by 2100 3 2 economic development commodity prices are estimated to increase for nearly all major energy uses and agricultural outputs regardless of the scenario this is due to a combination of changes in population consumer preferences and resource availability furthermore pastoral commodities are estimated to see dramatic price increases over time in both rcp 4 5 scenarios not only due to increases in demand associated with global population growth but also as a result of national and global climate change reduction policies that place a price on greenhouse emissions from all sectors of the economy including agriculture and livestock prices are estimated to increase the most in the ssp5 scenario which is driven by high global consumption preferences coupled with a high carbon price required to meet an emissions trajectory consistent with an rcp of 4 5 w m2 target for 2100 in terms of productivity forest growth had similar responses under rcp 4 5 and 8 5 across the six gcms under constant co2 growth reductions averaged around 5 10 and were spatially variable from south east to north east ausseil et al 2017 however when factoring in increasing co2 the general negative response turned into general positive growth responses the difference was more pronounced for simulations under rcp 8 5 than under rcp 4 5 by 2065 there were general growth enhancements by about 5 10 under rcp 4 5 and 15 20 under rcp 8 5 for cropping model results indicate a higher risk of yield losses when sowing dates are not adapted particularly for short maturity hybrids for these conditions median yield loss estimates increase from mid century 5 to the end of the century 14 mainly because the crop cycle is shortened due to faster crop development in contrast by adapting sowing dates to a warmer climate i e sowing early yield losses were minimised and yield gains occurred for specific locations particularly when using long maturity hybrids teixeira et al 2018 by adapting sowing dates yield gains occur for specific areas in the catchment especially higher altitude areas in the south west that are currently limited by lower temperature in the current climate for pasture production the change is overall positive under all scenarios if we were to convert the whole catchment under either sheep and beef or dairy the magnitude of change is larger for dairy than for sheep mainly because dairy systems are more intensive requiring more nitrogen inputs and growing more grass table 6 total annual pasture growth across both dairy and sheep and beef averaged spatially over the entire kaituna region increases by 1 5 around mid century and by 2 7 5 by 2100 ausseil et al 2017 the increase is largely attributable to the co2 fertilization effect as the amount of increase in production follows the trends in co2 atmospheric concentrations from each rcp the increase in production is largest in rcp 8 5 at 2100 in which the co2 atmospheric concentration is highest c 850 ppm this effect is more than enough to offset any adverse climate effects on production for rcp4 5 the increase in production is marginal and ranges between 0 and 6 for kiwifruit sufficient winter chilling is vital to ensure sufficient flowering buds as temperature increases the risk to poor kiwifruit production increases however this can be compensated for by the use of hc since there is a risk that this chemical may be banned in the future because of its environmental toxicity and possible impact on human health cradock henry 2017 the assumptions of use or not for future scenarios have a strong influence on the projected viability of this industry we compared land use changes with and without accounting ssp assumptions fig 4 it can be seen that the ssp effects on changes in land uses relative to 2015 areas are much greater than the rcp only scenario this means that the changes in commodity prices are more influential for land use change than the changes in yields the impacts on land use changes are largely a combination of commodity prices increasing effect for most land uses and carbon prices decreased net revenue effect for pasture and crops increased effect for forestry looking towards 2065 in both scenarios involving rcp 4 5 the greatest change in land use is estimated to be a conversion from sheep and beef and dairy farms to forest plantations this is because although there is an estimated increase in milk and meat prices compared to 2015 the carbon price increase is stronger leading to a projected increase in forestry area thus the price effect has a larger positive effect on forestry than on pastoral enterprises the other land uses tracked in nzfarm including arable and horticulture are not estimated to change nearly as much in scenario rcp 8 5 3 a the absence of a carbon price and the large price effects estimated in climat dge cause sheep and beef to be relatively more profitable and hence there is a large shift back into that land use from forestry and scrub interestingly the prospects towards 2100 see a reverse of that trend with an increase in forestry relative to 2015 similar to the other scenarios the explanation is that the forestry prices catch up with the meat and milk prices making timber production more profitable even without carbon price rise finally coastal inundation due to sea level rise under a 0 8m scenario is projected to affect about 5500 ha of land that could become regularly inundated every couple of years during high tides this land currently under dairying could become permanently unavailable or uneconomical to manage for primary production reversion to a natural wetland may depend on other factors such as local level topography sediment and seed sources and anthropogenic infrastructure investment schuerch et al 2018 3 3 environmental and ecological factors the kaituna catchment used to have an extensive area covered by freshwater wetlands with only 8 wetland cover left compared to the historical extent ausseil et al 2008 the remaining wetlands are under considerable threat the main wetland type existing in the catchment is lowland swamps swamps are defined by johnson and gerbeaux 2004 as wetlands receiving a relatively high supply of nutrients with a water table usually permanently above some of the ground surface they have an abundance of cabbage trees flax and raupo along with kahikatea and swamp maire trees with a significant area in the kaituna reserve this type of wetland is more resilient to water supply changes than other wetland types such as bogs or fens climate change projections in the kaituna catchment are showing increases in rainfall so this may indicate a marginal effect on swamps bodmin et al 2016 however a proper water balance model would be required in future to account for increasing temperatures that may significantly change the evaporation and evapotranspiration rates this may increase water deficit and create added risk for these systems pest and disease risks are likely to increase substantially as temperature increases climex outputs for several pest species show a general trend towards increased suitability of major pests for both natural and managed ecosystems ausseil et al 2017 ecosystem services are equally impacted in both kicking screaming rcp 4 5 3 a and clean leader rcp 4 5 5 f up to 2065 fig 5 climate regulation in the form of reduced net greenhouse gas emission improves as carbon sequestration increases relative to 2015 and greenhouse gas emissions decrease because of the decrease in livestock numbers pressure on water quality is also decreasing with the decrease in total animal excreta decrease in both nitrogen and phosphorus losses erosion control improves with a decrease in soil loss due to the increase in tree cover especially on erosion prone areas such as the hill country of the catchment conversely water regulation is decreasing slightly as the increase in tree cover reduces water yield through higher interception davie and fahey 2005 the scenario rcp 8 5 3 a contrasts with the two others climate regulation is worse with a net increase in greenhouse gas emissions water quality could also be affected with more phophorus loss although nitrate leaching is projected to be at a similar level to 2015 this is due to the increase in soil loss especially on steep slopes resulting in phosphorus lost through sediment in 2100 the three scenarios show positive changes in ecosystem services except for the loss of the water regulation service this is consistent with forest expansion projected for all the scenarios regardless of the rcp ssp combination for rcp 4 5 3 a there is a greater shift of sheep and beef into forestry thus we have larger changes in positive water quality and erosion control decrease in phosphorus and soil loss particularly if this is happening on steep slopes there are relatively fewer water quality changes for rcp 8 5 3 a and rcp 4 5 5 f due to dairy still being a relatively large component of the landscape but not rcp 4 5 3 a 4 discussion this paper provides an overview of potential future impacts of both climate change and socio economic changes in a typical lowland environment of new zealand we demonstrated the use of quantitative and narrative statements for three plausible future scenarios unspecified pacific rcp 8 5 3 a kicking screaming rcp 4 5 3 a and clean leader rcp 4 5 5 f in the unspecified pacific scenario there was almost no attempt to curtail climate change on a global scale and only very limited reactive local efforts costs of production would generally increase due to a need for increased environmental management for pest control and water shortages with a higher risk for a decline in commodity prices due to increased global competition in scenarios kicking screaming rcp 4 5 3 a and clean leader rcp 4 5 5 f the rcp projection is the same only the socio economic pathways differ the modelling approach showed that the impact on land use and thus the ecosystem services was similar with high increases in forestry although the clean leader scenario projected a greater shift of sheep and beef to forestry these changes imply general positive benefits from ecosystem services except for a decrease in water yield due to the expansion of forest this is an important factor to keep in mind as the possibly drier conditions may have an impact on water availability in the catchment reporting results around two key periods 2065 and 2100 reveal some possible reverse shifts for example for scenario unspecified pacific rcp 8 5 3 a the land use change matches the two other scenarios with once again a shift towards forestry our approach used guiding principles from frame et al 2018 to combine global assumptions and local conditions and helped conduct a site specific assessment in a lowland environment of new zealand using landscape models enabled us to perform the assessment of climate change impacts giving insights into the implications for the local communities a diversity of models was integrated across multiple land uses keeping the underlying assumptions for regional environmental and land use policy in line with the chosen socio economic pathways it enabled us to explore the strong influence of policies and global economic context on regional economic output and ecosystem services relative to just accounting for the physical impacts of climate change one conclusion is that in our case the influence of commodity prices overwhelms the impact on land use change compared to the sole influence of climate change on the biophysical aspect in return changes in ecosystem services being directly related to land use change are mainly due to the socio economic context this finding has been found in other modelling work that has looked at the both the isolated and integrated effect of climate impacts and socio economic impacts on land use ahmed et al 2016 favero et al 2018 popp et al 2017 tian et al 2016 overall the process was highly interdisciplinary mixing biophysical economic and social science as noted earlier key stakeholders notably the local government authority were involved in workshops from the beginning of the process to ensure the relevance of the study for their own priorities and issues ausseil et al 2017 in turn the local government authority have been undertaking riparian management partnerships with private landowners for several years this exemplifies the new zealand context where interactions between local authorities landowners stakeholders and researchers take place very easily and informally a review of this process took place after the modelling was completed and is discussed later while we used categories of key elements from o neil et al 2014 not all elements including welfare institutions and governance technological development broader societal factors and policies were able to be modelled quantitatively the assumption is made that these will not provide a material difference to the models though this would benefit from testing in subsequent models it should also be noted that the levels of complexity required to accurately model any of these categories would likely to be beyond the scope of any modest study indeed there is a tension between the comprehensiveness of the o neill et al 2014 categories and what is feasible technically from a modelling perspective this process highlighted the inter dependencies between elements and gave insight into the complex chain of events and feedbacks that could occur in the future in other words local effects may overwhelm those arising from modelled global climate change effects and this leads to a tension between top down and bottom up modelling especially when compounded by the number of missing elements for which the impacts are complex and not necessarily linear for example the impact of climate change on some primary sectors can trigger land use change creating trade offs between food and timber provision dunford et al 2015 a key point to remember is that national and local scenarios are more than downscaled global or regional scenarios and it should be emphasised that some local issues not featured in the global models such as welfare institutions and governance technological development broader societal factors and policies as noted above may either overwhelm the effect of global parameters or drive the differences in drivers at local vs global scale the quantitative models had a degree of integration primarily through soft linking wene 1996 helgesen and tomasgard 2018 this was mainly done bypassing along the biophysical outputs for production changes to the economic models to drive land use decisions which then have a resulting impact on ecosystem services uncertainties are inherent to this integrated modelling approach and more sensitivity analyses would be required in future to understand how the outcome and narratives may be affected the main sources of uncertainties reside in data inputs e g different gcms and model parameters for the biophysical models economic models and ecosystem services models nzfarm has already been used to test uncertainty of erosion modelling outputs under different climate scenarios utilizing the same data inputs monge et al 2018 a similar approach could be tested here with a variety of tools available from the modelling community matott et al 2009 this could help understand how sensitive the whole system is to a range of uncertain inputs like the climate variables similar to ahmed et al 2016 or the socio economic assumptions e g commodity price pathways a careful selection of which components to sensitivity testing on would be required due to the complexity of the modelling system this is where the value of interacting with stakeholders and co designing scenarios becomes fundamental to ensure usefulness of the modelling approach identify components of interest and give insights on how important they are for the outcome some of these discussions could lead to new quantitative assumptions based on qualitative statements for instance pest and disease can affect primary production was translated as an assumed increased cost under rcp 8 5 3 a table 5 further discussions could identify the range of increased costs that would be included in the uncertainty analysis there is also potential for additional interdependencies with dynamic feedbacks to be modelled through hard coupling e g directly linking hydrological models with the land use change model however the extra computational efforts need to be balanced against the value of the additional information for example if the land use effect is revealed to be negligible then the hydrology model could assume a constant land use pattern over time since it is uncertain how these results would differ under an alternative approach future research would be needed to test this effect by attempting to hard couple some of the key models in our integrated system the combination of the three dimensions represented by rcp ssp and spa enabled a mix of new zealand specific scenarios of high relevancy to stakeholders as noted elsewhere these were discussed with stakeholders from central and local government business leaders and others as part of a review of the national scenarios developed across five landscape types of which this lowland example was one frame et al 2018 however this approach multiplies the number of possible scenarios given the number of rcps ssps and spas the goal then is not to describe every possible policy landscape but to select a finite number of representative central policy assumptions to produce a set of climate policy scenarios that are plausible within the global rcp ssp spa architecture we chose three scenarios to provide key messages to decision makers identifying trade offs and synergies between positive and negative outcomes from climate and socio economic pathways testing different scenarios allows comparisons and demonstrates the potential of societal adaptation options to make positive changes the results from the integrated modelling gave some insights into the direction of change for the chosen scenarios from there the quantitative results can be supplemented by narrative statements for each element ausseil et al 2016 frame et al 2018 see appendix b however it is the value judgements required to provide interested parties with meaningful scenarios that can influence policy development and decision making these will be subject to subjective interpretations the narratives should therefore be tested with modellers and stakeholders for relevance only through a process of co production with stakeholders can narratives be considered meaningful without that direct involvement scenarios run the risk of not having enough relevancy and legitimacy to be endorsed subtle indirect connections could also be overlooked if not discussed in a wide group of various interested parties in terms of further work it is clear that challenges to ensuring internal consistency between the socio economic and climate scenarios and new methods will remain and it is unclear if it will be possible to consistently link elements across scales schweizer and kurniawan 2016 or if an approach is needed that accepts incompleteness if this were the case then greater care may be needed in structuring end user expectations and reducing the likelihood of anticipating precision in climate change forecasts the implication of this may well be accompanied by a shift towards greater interest in adaptation than mitigation even though timescales may be less clearly defined one solution is the use of dynamic adaptive pathways cradock henry et al 2018 dynamic adaptive pathways can be used to explore a combination of adaptation options with stakeholders ensuring their participative efforts for developing responses to plausible futures for the lowland case study for example even if total pasture yield increases the seasonality shift may induce a significant level of change in livestock management practices log prices have been predicted to increase but motivations for change from land owners may not result in such a dramatic change in land use as was predicted solely by the economic model a close and iterative process is required between qualitative information from local people s motivations for change and quantitative models to explore future options and increase models fitness for purpose and utility in future planning in terms of the specific context discussed in this paper we have examined an approach that has attempted to balance modelling rigour and accuracy with a highly complex local example where not all parameters can be modelled or even be fully understood to provide support to decision makers and stakeholders in the short term has required making assumptions based on local conditions and knowledge inevitably this leads to a softening of accuracy in order to maintain stakeholder involvement such trade offs between what can be termed internal and external validity of models appears to be not just an unavoidable consequence of modelling climate change impacts but an essential component of engaging society with the science of climate change and the realisation that none of it will be simple acknowledgements we thank funding from the ministry of business innovation and employment as part of project climate change impacts and implications contract c01x1225 we acknowledge the world climate research programme s working group on coupled modelling which is responsible for cmip producing and making available their model output we acknowledge the bay of plenty regional council for their support and identification of key issues in the study area an earlier version of this paper was presented at the international environmental modelling and software society conference in july 2016 toulouse france a g ausseil et al 2016 authors contributions a a designed the research a a a d e t performed the research and the analysis a a a d and b f wrote the paper we are also grateful for the reviewers comments appendix a new zealand national narratives for three scenarios from frame and reisinger 2016 id rcp ssp spa scenario sketches at 2050 unspecific pacific 8 5 3 a very high emissions 8 5 w m2 ssp3 fragmented world a with nz lagging relative to global efforts to mitigate nationally there is only incremental and reactive adaptation on a piecemeal basis an increasingly highly populated world with break down of open trade and security regional crises and financial instability new zealand is part of a pacific backwater that is sliding downwards economically with refugees from pacific islands with military conflicts between australia and indonesia impacts of climate change on nz are dealt with as and when they occur with local solutions generating problems due to short term decision making free trade is with volatile partners who require constant attention and accept their prices to protect trade routes and access with military force a severely resource constrained world neither cares nor can afford the luxury of environmental goals and bar a few militant voices there is little premium on such values investment in technology is very short term and environmental conservation is a low government priority 4 5 3 akicking screaming 4 5 3 a intermediate stabilization 4 5 w m2 ssp 3 fragmented world a with nz lagging relative to global efforts to mitigate nationally there is only incremental and reactive adaptation on a piecemeal basis as climate change impacts mount there is increasing global concern with responses led by a few large emitters with ongoing argument over responsibility and accountability nz positions itself with a group of nations who due to their small size and distance to markets have a lesser obligation to reduce emissions global trade is not free with many artificial barriers between the group of sustainability nations and those for whom economic growth is the key to managing environmental risks and providing resilience impacts on nz are dealt with as and when they arise which often means short term economic concerns win over longer term societal and environmental objectives assets at risk are protected by the crown but with limited budgets the cost of doing so is a major influence on decisions to do so discount rates are negotiable with preferred suppliers environmental conservation is a low government priority 4 5 5 fclean leader 4 5 5 f intermediate stabilization 4 5 w m2 ssp 5 conventional development fossil powered growth f nz ahead of increasingly stringent global efforts to mitigate and strategic approach to adaptation to maximize not just economic opportunities but to achieve sustainability across three pillars global emissions are actively being reduced through sharing more efficient conventional technologies and ensuring mitigation is achieved where most cost effective countries continue to argue for economically important sectors to be exempt from emissions obligations nz is developing innovative low cost mitigation opportunities and actively trading those for economic gain to the highest bidder factory based ge farming is unregulated but restricted to offshore islands strategic land use decisions are made with a view to maximizing economic growth in a warming and carbon constrained world when it comes to balancing trade offs in adaptation responses economic concerns win over environmental or societal equity concerns nz leads global practice in free market emissions trading variable discount rates are explored in nz as a means of creating change which is attracting global profile for nz goods and services with environmental conservation a strong government business partnership appendix b example of narratives for three scenarios in the lowland case study unspecific pacific rcp 8 5 3 a from ausseil et al 2017 frame et al 2018 kicking screaming rcp 4 5 3 a from the authors clean leader rcp 4 5 5 f from the authors demographics the rural population could continue to decline rural areas typically have older populations which leads to lower natural increase or even natural decrease of the population ageing is likely to lead to a less mobile population that is less able to avoid hazardous situations like flood events the new zealand population is projected to peak at 5 million in 2040 compared to a current population of just over 4 million development on coastal areas may slow or stop as a result of declining rural population there is likely to be further agglomeration of farm enterprises the rural population continues to decline rural areas have older populations mostly in the lower socio economic groupings and a resulting decrease of overall rural population urban areas have increased large dwellings with multiple generations all living under a single roof for economic reasons ageing is likely to lead to a less mobile population that is less able to avoid hazardous situations like flood events the new zealand population is projected to peak at 5 million in 2040 compared to a current population of just over 4 million development on coastal areas is for low socio economic groups who end up with previous life style properties which have become low priced but very expensive to maintain due to more frequent storm events farm enterprises are very responsive to increasingly dynamic market conditions urban intensification coupled with urban farming using a mix of hydroponics and 3d printed food reverses the trend towards urban sprawl the auckland hamilton corridor is the centre of this new form of urban horticulture which is seen as a global leader including the development of gene edited seed production facilities of global benefit due to far sighted investment in new technologies in the 2030snz is very highly urbanised 99 the resulting depopulation of the south island which has become predator free due to the same gene editing technologies means former primary production centres are now highly sought after top end retirement communities with exceptional recreation opportunities farm enterprises are large and geographically based often with community ownership able to set local tax rates economic development in general we could expect a decline in new zealand s economic health food security both internationally and within new zealand is expected to be a major driver leading to a decline in overseas markets trade e g kiwifruit and an increase in diverse local markets the limiting factor for primary production is likely to be appropriate and consistent access to water and the impacts of any extreme weather events dairy farming is likely to increase to the detriment of sheep farming a concern for the catchment might be the decline of kiwifruit biophysical suitability due to lack of winter chilling adding extra costs to production by requiring the use of chemicals to improve flowering this could be exacerbated by increased costs due to disease outbreaks and infrastructure costs regional climatic suitability for agricultural crops will change for example areas currently limited by low temperatures will be more suitable for cropping and rain fed agriculture will become more vulnerable to drought particularly for soils with low water holding capacity primary produce is traded more within nz than globally in highly volatile markets influenced heavily by the burgeoning asian markets the carbon price is high incurring high costs for the dairy industry primary production suffers unpredictable cost fluctuations due to disease outbreaks caused by erratic weather patterns the increased and unmet cost of production due to roading infrastructure limits forestry production kiwifruit as a land use may be suitable with interventions to compensate lack of winter chilling overseas competition is worsening the picture with a decrease in commodity prices dairy kiwifruit most primary produce is traded within nz except for technology transfer of gene editing and predator management regimes internationally this has led to a shift from high volume low value add production to very high value added service exports a good carbon price and high costs for dairy primary production has high overhead costs that covers very effective disease control with very little detrimental impact from predators forestry production has shifted to small volume high value native hardwoods on restored landkiwifruit production faded in the mid 2050s and gave way to new 3d production of heritage fruits from across the world though new zealand s high technology gene editing facilities overseas competition has made commodity production unviable except for technology transfer of precision interventions which are piloted on the quarantined chatham islands environmental and ecological factors if land is abandoned due to unfavourable climatic or trade changes the land could revert to natural wetlands however lack of funding for control measures could exacerbate the spread of exotic plants in wetland areas and create a risk for weed infestation of nearby cropping and pastoral farming native forests wetlands and rivers could see a decline in biodiversity due to pest invasions increased sedimentation water diversion for economic uses salinization in the coastal zone and lack of funding for conservation with warmer temperatures pests currently limited to warmer climates could expand their range into the case study area and become more prolific causing a reduction in abundance or loss of native species water discharge could reduce due to a reduction in precipitation creating water stress during summer native forests and wetlands have declining biodiversity due to pest invasions and lack of funding for conservation natural areas wetlands forest are degrading rapidly due to pest invasion and salinization and diversion of water for economic uses impacting on recreation impacts on water and sewerage infrastructure lead to a requirement for increased investment but there is only patchy provision of investment by councils retreat from the coast is unmanaged unplanned but forced by lack of funding determined by political economic preferences not by social or environmental objectives sea level rise decreases coastal property values with coastal walls built on an individual basis without accounting for the social environmental costs or those whose properties are then at greater risk nitrogen phosphorus and sediment are well managed through highly engineered systems sediment traps stormwater or drains that remove contaminants etc or technology advances new fertiliser that binds to soils or isn t leached the agricultural sector is exempt from the greenhouse gas emissions scheme nz is a global leader in carbon sequestration and emissions trading although the methods are not environmentally orientated there is strong development of carbon sequestration and filtering scrubbing methods and technologies resources fuel costs are expected to rise with an increased reliance on fossil fuel based electricity primarily coal this in turn could increase primary production and household utility costs and the use of public transport the tourism sector could suffer from these additional costs through greater travel costs the coastal zone could be impacted by sea level rise affecting agricultural land mainly dairy and maize cropping very high energy costs both renewable and non renewable result in very high efficiency devices which use dynamic pricing to keep costs down this results in a less reliable supply which in turn maintains low greenhouse emissions nationally this does result in market friction including illegal mining of open cast coal on the west coast for pirate energy providers tourism responds through interest in alternative2 energy tourism which markets coal powered communities along the lines of steam punk sympathetic managed retreat from the coast is funded by new forms of resource taxation that have displaced income tax as main form of personal taxation this is also funded through premiums placed on non nz nationals travel costs including a tourism levy for international visitors and through differentials on domestic transport including on electric vehicle recharging stations nz finally started to increase its of energy produced through renewables after many years in decline when low cost high volume batteries reduced reliance on non renewables i e coal and oil as back ups for wind and solar energy welfare sea level rise is expected to lead to a decline in coastal property values and eventual abandonment of the most vulnerable properties due to coastal encroachment human vulnerability to natural disasters could increase due to more frequent extreme events e g floods life expectancy may decline especially with potential reduced funding for health care services and the likely increases in the incidence of infectious diseases with coastal areas becoming an increasingly important reservoir for disease vectors such as mosquitoes there is a highly inequitable societal split between a small green elite and a low greenhouse gas impact majority resulting in the net effect of a low carbon nz this produces weak responses to increasing infrastructural damage from climatic change events by 2100 this shows up in differences in health statistics between those eligible for comprehensive private medical care and those subject to a public system with reduced resources health care is universal for long term citizens but very expensive for recent immigrants working visitors and tourists while this balances economically it results in a thriving black market for citizenship which aligns with place of residence one s post code is an increasingly important status symbol even for those in low cost low impact dwellings prevention of new climate related diseases is available through gm based pharmaceuticals creating ethical tensions between communities rather than individuals institutions governance excluding climate policies due to limited investment in sector catchment scale adaptation options to reduce risks flood events could increase dramatically from added sedimentation in the rivers and lack of funding to raise stopbanks road networks are likely to deteriorate worsening the economic conditions in the region social inequities could deepen due an inability to pay global agreements such as the kyoto protocol could be regularly breached and contingent liability could be transferred to central government tourism levies on routes air land and sea to major attractions reshape traffic infrastructure resulting in marked differentiation between honeypot sites and land that has deteriorated due to overuse in turn this puts high pressure on resorts to maintain unesco accreditation and its accompanying national subsidies there are knock on effects to wages in support sectors and the potential for falsification of greenhouse gas credentials user pays taxation continues to dominate over the modest introduction of resource usage instruments planning and regulations preserve highly productive land for primary production ownership is through new forms of public private partnership which involve lengthy consultation processes with iwi led community groups that rely on monthly compulsory voting by all members aged 16 75 taxation regimes are dominated by complex resource taxation systems that collect data using remote sensing techniques developed in nz this big data low taxes model is made available through the creative commons platform which nz administers for australia the pacific and latin america as part of the new global order technological development we assume that no new climate change mitigation options will be developed but that local adaptation solutions will be created in a reactive way lagging behind global initiatives we expect that fewer research efforts will be funded by government and more by industry most professional researchers supplement their modest incomes through provision of on line fee for services consulting and lecturing to a global market interested in nz s unique solutions to climate change gene editing abounds with a low barrier to entry regulation not proving effective at mitigating adverse effects nz leads greenhouse gas mitigation globally through highly innovative gene engineering technologies piloted on the quarantined chatham islands for 10 years before beta testing on the north island and subsequent launching globally nz capitalized on its remoteness excellent rs t investments and its desire to move on from low value high volume primary production to a high tech provider of solutions broader social factors there could be a general disconnect from nature recreation in and aesthetic appreciation of the outdoors ranks low on the list of people s priorities due to the high costs of living gated communities extend to wilderness areas as well as in urban areas with numbers to sites of exceptional scenery sexs limited annually through competitive bidding on line in turn these are re sold through the dark net as one of many alternative economies that failed to be regulated through crypto currencies virtual reality of nature franchises boom in gated urban communities especially through the fiercely competitive virtual golf the conservation programmes of the early 21st century have given way to the new nature now nnn movement which nz championed through the intergovernmental platform for biodiversity and ecosystem services ipbes in the mid century following acceptance that species extinctions were inevitable and that climate adaptation was the only available avenue there were sweeping changes in societal engagement with nature while many species were lost to science though preserved through the national collections human society was much more connected with other life forms which helped stem carbon emissions policies excluding climate policies loss of population and sea level rise could lead to ad hoc coastal protection insurance may be difficult to obtain or would not cover natural events development initiatives could be market driven and lacking policies to include social environmental or cultural elements regional tensions become marked resulting in unofficial permits to live in other regions driven by grey market preferences for family and friends officially these are driven by greenhouse gas targets but in reality financial and social concerns dominate māori pākehā tensions rise policies are driven by consensus and subsidiarity this results in markedly differing forms of solutions between regions while publicly accepted as part of national identity underlying tensions are never far from the surface and lead to a surprisingly strict government centre in terms of national legislation 2 the ecosystem services indicators were all tracked in nzfarm using look up tables from new zealand models such as overseer watyield nzeem the greenhouse gas model followed nz mfe inventory methods ausseil et al 2013 appendix cstudy model descriptions demographic model we developed a bottom up sub national population projections model wherein gross migration between pairs of regions in new zealand n 16 is modelled using a set of age sex specific gravity models our model allows all inter regional migration flows to be estimated and projected in a common framework with a single set of assumptions this method offers a number of advantages over traditional methods in particular that factors known to affect migration flows such as climate etc can be explicitly based on regression modelling incorporated into the population projections in a transparent and justifiable manner the estimated internal migration gravity model demonstrated that climate variables sunshine rainfall had statistically significant but very small effects on internal migration in new zealand as such most of the projected population change in new zealand is likely to be driven by other factors e g economic factors population ageing and momentum etc as discussed in cameron 2013 sea level rise slr the intergovernmental panel for climate change ipcc released its fifth assessment report ar5 in 2013 14 the report showed that the primary climate driver for slr is global and regional surface temperature which in turn is strongly influenced by greenhouse gas emissions slr occurs as a result of ocean warming which causes thermal expansion of the sea and melting of glacial ice both of which have contributed to sea level rise throughout the 20th century in new zealand the sea level rose by an average of 0 18 m around the country last century relative to the land mass it is rising relatively faster in some locations where the landmass is subsiding such as wellington the rate of slr was approximately linear over the last century but is forecast to accelerate over the next century due to global warming and possible rapid melting of the greenland and antarctic ice sheets the ministry for the environment guidance manual ministry for the environment 2016b recommends that for planning and decision timeframes out to the 2090s 2090 2099 at the very least all assessments should consider the consequences of a mean sea level rise of at least 0 8 m relative to the 1980 1999 average pasture growth modelling to model pasture growth we used the biome bgc model v4 2 thornton et al 2002 2005 adapted to two types of new zealand managed grassland systems sheep beef low intensity and dairy high intensity the biome bgc model is an ecosystem process model that simulates the biological and physical processes controlling fluxes of carbon nitrogen and water in vegetation and soil in terrestrial ecosystems this includes the co2 fertilization effect which enhances the rate of photosynthesis and reduces water loss in plants under elevated co2 atmospheric concentrations we used the model s built in c3 grasslands mode with some key ecological parameters modified and re interpreted to represent managed pasture and the presence of grazing animals keller et al 2014 model parameters were calibrated against new zealand pasture growth data and validated for both dairy and sheep systems as described in keller et al 2014 the primary model inputs daily minimum and maximum temperature precipitation vapour pressure deficit and solar radiation were derived from the outputs of the 6 gcms listed in section 4 2 1 unique parameterizations were assigned to both sheep and dairy ecosystems the main difference between the two is the intensity of farming dairy systems receive more nitrogen inputs to simulate more fertiliser use more grass is eaten in the form of increased whole plant mortality and more animal products milk or meat are extracted from the system in addition the dairy parameterization effectively results in increased water use efficiency note that irrigation is not simulated in either system the model was run for each 5 5 km vcsn grid square in the kaituna catchment with location specific weather inputs soil texture and rooting depth a reference or baseline pasture production for each gcm was simulated using the rcp past climate input representative of modern day conditions and averaged over the nominal years 1985 2005 for all future scenarios the model was first spun up using rcp past climate and then restarted and run as a transient simulation from 2005 to 2100 using each model and scenario specific projected climate forestry modelling the simulation results described here used the comprehensive process based ecophysiological model cenw 4 1 to simulate the growth of pinus radiata in the bay of plenty region the model had previously been parameterised for the growth of p radiata based on data from the whole of new zealand kirschbaum and watt 2011 it had also previously been used for climate change impact assessments for new zealand kirschbaum et al 2012 and essentially the same modelling procedure was followed here it essentially models tree growth over 30 years with initial stand densities and thinning regimes as described by kirschbaum et al 2012 the novel aspect of the present work is the availability of actual daily output of key weather parameters from six different gcms for each day up to 2100 in past work only average changes in weather parameters were available and theses weather anomalies had to be added to a current day weather sequence that preserved a realistic pattern of seasonal changes in weather parameter but did not include any possible changes in those patterns themselves the climatic data used here are the direct output from gcm runs and thus include any possible changes in weather patterns such as changing inter annual frequency of drought periods or changes in seasonal temperature or rainfall patterns data are presented both as changes from current 1980 2010 productivity to productivity in 2055 2040 2070 or 2085 2070 2100 and as progressive changes by running the model to simulate productivity from 1980 to 2010 followed by a run with data from 1981 to 2011 and so on crop modelling the agricultural production systems simulator apsim is a biophysical model that simulates crop growth at daily time steps in response to climate soil and crop management holzworth et al 2014 the model represents processes that control dynamics of carbon water and nitrogen in plants and soils the timing and specifications of management interventions e g sowing harvesting fertiliser and water application are also represented in apsim to extend apsim applications beyond its original point basis configuration i e single location to a catchment scale 5 km resolution dynamic simulations of crop management sowing dates fertiliser application and crop type choice in response to environmental conditions were developed model simulations did not account for biotic stresses insects pathogens and weed competition or damage by extreme events e g floods heat waves or storms simulations were performed for silage maize zea mays maize was selected as an indicator of climatic suitability to arable cropping due to its importance as a forage option for the dairy sector and its wide presence across new zealand to explore adaptation options to climate change apsim runs were performed for two maize genotypes short and long maturity hybrids with and without considering adaptation of sowing dates i e sowing early in response to warmer climates simulations were performed for a baseline 1986 2005 mid century 2046 2065 and end century 2081 2099 with the six selected gcms global circulation models simulations were run continuously with a spin up period of 3 8 years depending on data availability as maize in the kaituna catchment is typically grown under rain fed conditions on soils with high water holding capacity assumed 160 mm m these conditions were therefore assumed in simulations a 30 year baseline model run using the historical climate era databases from 1971 to 2000 was used to calibrate and test the model sowing dates and yields of historical model runs were scrutinised by maize crop experts dr john de ruiter senior scientist at plant and food research pfr and mr allister holmes research extension team leader at the foundation for arable research far in addition model results for historical weather were compared with data from an online survey developed by far and pfr with maize growers across new zealand model results were analysed for grid cells else than lake areas kiwifruit suitability the coldness of the winter period in new zealand may july has a very strong influence on both the quantity and quality of kiwifruit flowers as well as the timing of flowering this in turn has a direct influence on the number of buds the timing of bud break and hence the number and quality of fruit produced by the vine sufficient winter chilling is therefore vital for kiwifruit production viability as temperatures increase due to global warming the risk of insufficient winter chilling also increases as does the risk of poor kiwifruit crops a simple temperature threshold based empirical model has been developed for assessing current and future to the year 2100 hayward kiwifruit production viability for the te puke area tait et al 2017 the model is derived from the phenology of the crop and published literature that relates temperature to four phenological stages of the hayward vine development the model includes the effects of applying or not applying hydrogen cyanamide to aid the number of flower buds for simplicity the model runs are labelled with hc and without hc soil loss model nzeem erosion control is defined as the prevention of soil loss by an ecosystem the demand for erosion control is mainly local coming from the farming sector at risk of losing productive soils and from river users who desire clear water and nondegraded river habitats in new zealand mass movement erosion is the dominating erosion process dymond et al 2010 which is mitigated primarily by tree roots we chose soil erosion rate tonnes of soil km2 yr as an indicator of erosion control erosion control can be derived using the difference in sediment loss with and without respective ecosystems we estimated soil loss using the nzeem erosion model which has been calibrated from sediment discharges measured in new zealand rivers dymond et al 2010 this model estimates the long term mean erosion rate from all sources of erosion both mass movement and surficial and accounts for all sizes of rainfall events in this paper we consider any reduction in soil loss as a positive outcome for erosion control water regulation water supply through rivers is important for drinking water for both animal stock and humans irrigation and hydropower generation the maintenance of flows in rivers their timing and magnitude by ecosystems is defined as water flow regulation we chose the net supply of water remaining after evapo transpiration losses mm yr as an indicator of waterflow regula ion we used watyield fahey et al 2010 that models daily water transfers of rainfall interception evapotranspiration and drainage associated with a soil profile input data to the model are daily rainfall and daily potential evapotranspiration pet parameters required for the model include the fraction of intercepted rainfall vegetation factors for transpiration and total and readily available water holding capacity of the soil the model was run for several homogenous soil climate units for different land covers forest scrub tussock and pasture we stored the proportion of rainfall that becomes water yield in a lookup table so that a simple computer workflow comprising national spatial data layers could be implemented to calculate impact of land use change on water yield in this paper we consider any increase in water yield as a positive outcome for water regulation vice versa a reduction in water yield has a negative impact on water regulation nitrate and phosphorus loss through leaching overseer the provision of water for drinking purposes and habitat for aquatic organisms is linked to high levels of water purity whereas for hydropower quality standards are less important in new zealand the main pollutants to reduce water quality originate from agricultural activities which generally increase the level of nutrients in soils above natural background levels especially nitrate and phosphorus we estimated nitrogen leaching using overseer version 5 4 ministry of agriculture and forestry et al 2011 a nutrient budget tool that takes farm management soil and climate variables as inputs and produces annual nutrient budgets including nitrogen leaching as for watyield we ran overseer for 100 combinations of soils and climate dymond et al 2012 ausseil et al 2013 the nitrogen and phosphorus leaching rates per stock unit were combined with the animal numbers from nzfarm to produce a map of nitrogen leaching for all of new zealand in this paper we consider any reduction in nitrogen or phosphorus sources as a positive outcome for water quality climate regulation greenhouse gas emission ghg fluxes fluxes of greenhouse gases were considered as an indicator of influences on the global climate that act through radiative forcing of the atmosphere we did not consider the influences on local climate such as the effect of riparian shading on river water temperature new zealand is a signatory to the kyoto protocol and is thus legally bound to control net greenhouse gas emission in new zealand global climate regulation is strongly influenced by the agricultural sector which emits greenhouse gases into the atmosphere and the forestry sector which sequesters carbon from the atmosphere we therefore estimate the influence of agriculture and forestry on fluxes of greenhouse gases separately for agricultural greenhouse gas emission we considered the three main greenhouse gas emissions methane nitrous oxide and carbon dioxide we used the animal numbers for dairy sheep beef and deer and multiply them by an implied emission factor per animal from the greenhouse gas inventory report from the ministry for the environment ausseil et al 2013 to model changes in carbon sequestration we assumed that any additional carbon sequestered would be related to forestry cover we used the process based model cenw used for the forestry modelling model efficiencies were 0 83 for height 0 89 for diameter 0 82 for basal area and 0 84 for volume the model was applied to new zealand on a 0 05 latitude longitude grid using 20 years of daily weather input data and soil surfaces fertility water holding capacity texture kirschbaum and watt 2011 to produce a map of potential carbon sequestration rates over 30 years in this paper we consider reduction in greenhouse gas emission and increase in carbon sequestration as a positive outcome for climate regulation wetland vulnerability changes in precipitation may impact wetland extent wetland condition community composition or ultimately a shift in either wetland type or ecosystem wetland vulnerability to climate change was explored at national scale using a risk framework that combined exposure to a defined climate change stressor sensitivity of an ecosystem to change impacts and adaptive capacity bodmin et al 2016 this initial investigation examined exposure of wetlands to the climate change stressor water availability exposure to water availability was assessed as the change in precipitation for new zealand between 1980 1999 and 2080 2099 using gis these likely precipitation changes were intersected with the current extent of different freshwater wetland types freshwater wetlands were classified into bog fen swamp marsh seepage gumland and pakihi ausseil et al 2011 pest simulations pest simulations we conducted using climex v 4 02 model primarily through the compare location function 1 species extended ausseil et al 2017 this analysis used both the monthly data as well as 20 year time period climate normals centred on 2005 2050 and 2090 data was provided as netcdf files these where manipulated in python to create the input datasets required by climex an audit was undertaken to ensure that transformation did not corrupt the data due to database size limitations separate climex data files where created for the monthly data on a 5 year time step for each species x rcm x rcp and one climate input file for the normals data set locations are national for each 5 km grid cell in the vcsn climate change projection data was loaded into the climate input file by repurposing variables of continent country state to hold rcm rcp and year labels all parameters other than climate data and species were kept to the default two climate data sets where used to run the simulations each species is modelled at 5 year intervals 2015 2120 for each rcm and rcp capturing the inter annual variation in potential distribution each species is modelled using the 20 year normal data centred on 2005 2050 2090 for each rcm and rcp these data were reduced by using the maximum ei for each cell from each of the rcm i e the worst case scenario of ei risk is presented the models were provided by john kean agresearch via the climenz website http b3 net nz climenz index php and were able to be downloaded as climex ready parameter files saving considerable time as well as ensuring parameter accuracy 55 species modelled were selected from a range of sources that identified pests or unwanted organisms in nz such as the national pest plant accord legal sources notifiable organisms and those recommend as pests either as have been modelled in nz before using earlier climate data sets or as potential threats to biodiversity or production systems climat dge to estimate the effect of global ssp on the new zealand economy we used the climate and trade dynamic general equilibrium climat dge model developed by landcare research climat dge is a multiregional multi sectoral forward looking dynamic general equilibrium model with a relatively long time horizon of 100 years or more this model is suited to studying the efficient re allocation of resources within the economy and the response over time to resource or productivity shocks climat dge primarily uses the global trade analysis project gtap version 8 data set the base year of the benchmark projection is 2007 the model then develops a benchmark projection of the economic variables and ghg emissions and simulates scenarios to evaluate the impacts of mitigation policies based on long run conditions and constraints on physical resources which restrict the opportunity set of agents the model predicts the behaviour of the economy energy use and emissions by region and sector fæhn et al 2013 climat dge covers 18 aggregated production sectors we focused on the cattle and food sectors model dynamics follow a forward looking behaviour where decisions made today about production consumption and investment are based on future expectations estimated in 5 year time steps the economic agents have perfect foresight and know exactly what will happen in all future periods of the time horizon thus households are able to smooth their consumption over time in anticipation of large price shocks that may arise as a result of resource constraints or environmental taxes for a thorough description of climat dge see fernandez and daigneault 2015 nzfarm the new zealand forest and agriculture regional model nzfarm is a comparative static non linear partial equilibrium mathematical programming model of new zealand land use operating at the catchment scale daigneault et al 2012 2018 in this study it was used to assess how changes in climate i e yields socio economic conditions e g commodity prices and input costs resource constraints and environmental policy e g ghg reduction pathways could affect a host of economic or environmental performance indicators that are important to decision makers and rural landowners the version of the model used for this analysis can track changes in land use land management agricultural production freshwater contaminant loads and ghg emissions see fig a1 figure a 1 diagram of inputs and outputs from nzfarm red indicates key inputs for this anlaysis figure a 1 in this case study we use nzfarm to assess the implications on farm income land use and the environment when farmers in the kaituna catchment are faced with variations in agricultural yields due to climate change and or alternative shared socio economic pathways this analysis builds on previous work on climate change impacts on agriculture and forestry in new zealand by indicating not only the likely impact of climate change on production but also the effect that landowner adaptation may have on land use economics production and environmental outputs within a simultaneous modelling framework the model s objective function maximizes the net revenue of agricultural production subject to land use and land management options production costs and output prices and environmental factors such as soil type water available for irrigation and any regulated environmental outputs e g ghg emissions taxes imposed on the catchment catchments can be disaggregated into sub regions i e zones based on different criteria e g land use capability irrigation schemes such that all land in the same zone will yield similar levels of productivity for a given enterprise and land management option in this case each vcsn grid cell is modelled as an individual zone within the kaituna catchment simulating endogenous land management is an integral part of the model which can differentiate between baseline land use and farm practices based on average yields achieved under the current climate and those that could be experienced under a range of rcps landowner responses to changing climate and socio economic conditions are parameterised using estimates from biophysical and ecosystem service models described elsewhere in this manuscript commodity prices estimated from climat dge and farm budgeting models described in daigneault et al 2018 
26167,new zealand s isolation and primarily land based economy make it heavily dependent on future global climate and economic change we combined global assumptions from the new ipcc scenario framework and local conditions to conduct a site specific assessment of future scenarios we used a spatially explicit integrated assessment that combined economic and biophysical models to help quantify the potential impacts of a complex set of climate induced impacts coupled with regional environmental and land use policy three scenarios were chosen involving stakeholders to gain insight into local sensitivity to climate versus socio economic change the results provide the direction of changes for demographic economic and environmental factors including some ecosystem services they highlight the strong influence of policies on regional economic output and ecosystem services relative to just accounting for the physical impacts of climate change the quantitative modelling results might be used to identify adaptation options in light of climate change implications keywords climate change ecosystem services land use change spatial modelling biophysical model economic model 1 introduction new zealand s mixed economy includes a range of sectors e g primary production 1 1 in the new zealand context the primary production sector includes agriculture fisheries forestry and horticulture and is the main economic driver along with tourism the term is used throughout the paper energy tourism that depend heavily on the state of natural resource capital its small size relative geographical isolation and strong reliance on trade and migration make its economy particularly vulnerable to the world s economic situation royal society of new zealand 2016 both climate and socio economic conditions are difficult to predict socio economic developments cannot be captured appropriately by simply downscaling global scenarios frame et al 2018 reisinger et al 2014 van vuuren et al 2010 new zealand s approach to managing natural resources and the weight given to environmental considerations may critically influence the impacts of climate related changes in land and water resources on its society and ecosystems in new zealand research has been extensive at the sector level to look at the impacts of climate change adaptation and mitigation options for the agricultural and forestry sectors ministry for primary industries 2016 warrick et al 2001 mainly on the production side cross cutting issues have also been considered including economic analysis life cycle analysis farm and catchment analysis social impacts ministry for primary industries 2016 and risk assessment ministry for the environment 2008 however changes in climate will probably have interacting and feedback loop effects on the environment compounded by changes in global markets and international policy and in turn on society challinor et al 2018 the ability to understand the full spectrum of impacts on ecosystem services is necessary if decision makers are to respond to the trade offs and benefits of both changing climate and changing land use elmhagen et al 2015 hauck et al 2015 rounsevell et al 2010 shaw et al 2011 the new scenarios developed in the fifth assessment report ar5 of the intergovernmental panel on climate change ipcc separated greenhouse gas emissions trajectories the representative concentration pathways rcps from potential socio economic pathways ebi et al 2014b ipcc 2014 kriegler et al 2014 o neill et al 2014 van vuuren et al 2014 in what is referred to as a parallel process the resulting framework provides greater flexibility to explore mitigation and adaptation strategies the motivation is to provide stakeholders with insights into the implications of various decisions shifting from a predictive mindset to a more exploratory and option driven mindset for solution pathways many scenarios models and standard methods have now been developed to quantify indirect and direct drivers of change mallampalli et al 2016 pichs madruga et al 2016 that are ready to be used for impact assessment however multi scale issues ranging from global to regional and national requires a participatory approach if there is to be meaningful engagement with the scenarios and the pathways most likely to deliver change and in order to inform decision making and ensure transferability to other areas frame et al 2018 kebede et al 2018 the utility of the parallel process is determined in our view by three qualities synonymous with boundary objects and linking scientific knowledge with action cash et al 2003 credibility which relates to the scientific adequacy of the technical evidence and arguments salience which deals with the relevance of the assessment to the needs of decision makers legitimacy which involves the production of information and technology that are respectful of stakeholders divergent values and beliefs the complexity of the human nature system interactions and interdependencies makes it difficult to comprehend the feedbacks tipping points side effects trade offs and benefits in ecosystem services of both climate and socio economic drivers integrated assessment has the potential to better represent these combined effects on ecosystem services the economy and society dunford et al 2015 though it requires working across scales to accommodate different issues disciplines processes and scales kelly et al 2013 to capture this complexity several modelling approaches can be used a fully integrated modelling approach is complex as processes may operate at various scales with various influencing drivers pichs madruga et al 2016 integrated assessment models iams such as the image model incorporate several modules for both human and natural systems have been used at the global scale by the ipcc stehfest et al 2014 they are a very useful tool to understand and project the interaction and feedback loops between the climate and the economic system van vuuren et al 2011 iams have been used for raising awareness and testing future scenarios at the european scale climsave harrison et al 2015 holman et al 2017 creating a space for discussion with different groups of stakeholders harrison et al 2015 jäger and omann 2013 mixing qualitative and quantitative understanding of climate and socio economic impacts enables stakeholders to explore options for the future and to understand cross sectoral interactions and vulnerabilities absar and preston 2015 harrison et al 2013 however since human nature system interactions and feedbacks are complex simplifications are necessary for instance the assessment of global land use and climate change impacts across rcps and shared socio economic pathways ssps involves a relatively crude representation of the agriculture and forest sector with results aggregated across regions and forest type popp et al 2017 riahi et al 2017 iams may also under represent a critical phenomenon and therefore run the risk of misrepresenting responses to climate change at a local scale for instance the amplitude of adaptation measures studies at the regional scale have tended to loosely couple the quantification of land use change scenarios under future climates with the use of biophysical models to estimate impacts on ecosystem services booth et al 2016 byrd et al 2015 krkoška lorencová et al 2016 langerwisch et al 2018 qiu et al 2017 schirpke et al 2017 this loose coupling preserves the integrity of specialised models and keeps control of the interpretation of passing data between models kelly et al 2013 changes and interactions between sectors are particularly complex in lowland environments where climate change could have an impact on productivity and trends in the global economy may have an even greater influence on the land and its use in this paper we evaluate the impacts of climate and land use change in a lowland environment based on plausible alternative futures the scenario analysis follows the principles of the so called story and simulations sas approach which combines narrative stories supported by simulations from quantitative models alcamo 2008 our analysis is done across a diverse set of plausible scenarios to 2065 and 2100 using an integrated assessment modelling approach that incorporates biophysical economic and socio demographic models in a single framework we use this framework to estimate the impacts of climate and land use changes on a suite of ecosystem services the paper contributes to the integrated assessment literature in several ways first it presents a framework for how to combine global assumptions and local conditions to conduct site specific assessments second it details how a diverse set of models can be integrated to help quantify the potential impacts of a complex set of climate induced impacts coupled with regional environmental and land use policy third it highlights the strong influence of policies on regional economic output and ecosystem services relative to just accounting for the physical impacts of climate change furthermore we argue that as there are few tools currently available for local government decision makers and business groups to identify potential adverse outcomes of climate change and to weigh adaptation options the qualitiative quantitative mixed method approach discussed provides an opportunity to promote foresight at the local scale the balance between methodological rigorous and resource intensive approaches needs to be balanced with nimble modelling that engages stakeholders without imposing a burden yet still providing sufficient credible guidance to be both salient and legitimate though open to revision as new information becomes available in other words there is a tension between providing highly structured fully developed models which take a long time to produce and the need to engage stakeholders in meaningful processes even if with declared omissions and incomplete information this paper presents an example of addressing this tension as a means to further productive research the paper is organised as follows first we describe the case study area where we tested the scenario analysis second we introduce the elements of the future scenarios as a mix of climate socio economic and policy assumptions three scenarios were chosen to represent individually plausible yet contrasting exemplifications of socio economic developments in new zealand that could matter for the future impacts of climate change societal vulnerability to climate change and adaptation options at 2065 and 2100 third we introduce the method for the quantitative integrated assessment composed of drivers of change and impacts on ecosystem services finally we present results from the scenarios and a discussion of our findings 2 methods 2 1 case study area a set of national climate change scenarios to 2100 were tested across a landscape gradient alpine upland lowland coastal marine as part of a larger project frame and reisinger 2016 tait et al 2016a b here the lowland case study is taken to illustrate an assessment of impacts and implications for three scenarios to identify a lowland case study area we compiled a nationwide shortlist of options which were assessed against several key criteria including access to existing scientific data and models potential significance of climate change impacts diversity of land use a wide range of natural vegetation types and current relationships with local stakeholders as the bay of plenty region contained several potential locations discussions were held with the regional council and as a result the kaituna catchment including a coastal zone around papamoa beach bay of plenty on the east coast of the north island was identified and an initial research plan was constructed the local government authority were part of the initial development of the project and have remained closely involved throughout the overall area is 126 100 ha including lake rotorua 11 600 ha the lower part downstream of the lake is a typical lowland environment in new zealand with a mixture of natural ecosystems freshwater wetlands and native forests and a wide range of primary production maize cropping kiwifruit horticulture forestry dairy sheep and beef farming the land in primary production is mainly grassland 47 000 ha used for dairy in the lowland areas and sheep and beef farming in the hill country fig 1 exotic forestry and indigenous forest are present in the upper part of the catchment while the lower part is covered by the kiwifruit industry c 6000 ha near te puke this area and its existing land uses are likely to be affected by both drought and flood related climate issues kenny 2011 the local environmental authority is developing a community strategy for a vision for the future issues include pressures from increased population growth and land use intensification a workshop of local stakeholders organisations and agencies was held to present the proposed research identify issues related to land use change and refine the proposed work based on what was most important to stakeholders identify additional data sources and spatially locate the wider impacts and implications of climate change ausseil et al 2017 examples of issues raised that we could address included how could climate change exacerbate or counteract current land use change trends how will kiwifruit industry be impacted by climate change how will sea level rise impact the kaituna catchment 2 2 scenario elements the ipcc research community recently introduced a new scenario framework for climate change research that combines levels of radiative forcing characterised by the representative concentration pathways rcps and alternative plausible trajectories of global development described as shared socio economic pathways ssps ebi et al 2014a kriegler et al 2014 o neill et al 2014 van vuuren et al 2014 our architecture adopts these two elements ebi et al 2014b and adds one national scale element the shared policy assumptions spas frame and reisinger 2016 frame et al 2018 2 2 1 representative concentration pathways rcps the ipcc fifth assessment report ar5 adopted four greenhouse gas concentration trajectories the rcps climate outcomes based on rcps are modelled via the coupled model inter comparison project cmip5 through numerous earth system models or general circulation models gcms six gcms bcc csm1 1 cesm1 cam5 gfdl cm3 giss e2 r hadgem2 es and noresm1 m had been selected to update and improve regional scale projections of new zealand climate trends and variability to 2100 they were chosen because they validated well new zealand climate tait et al 2016b we used the output variables that were precipitation maximum and minimum air temperature relative humidity solar radiation and wind speed each variable was calculated on a regular grid 0 05 at a daily monthly and annual temporal resolution for the 1971 2100 period for our study area future climate change projections based on the extreme scenario rcp 8 5 and the average of several climate models by the end of the century compared with the present day ministry for the environment 2016a tait et al 2016a show a general increase in mean air temperature across seasons the number of hot days dry days and heavy rainfall the number of cold nights and wind speed may decrease while precipitation shows a seasonally variable trend with generally more rain in summer and less in spring table 1 2 2 2 shared socio economic pathways ssps unlike earlier assessments ar5 scenarios for climate change decoupled the climate model outputs expressed through rcps from their socio economic drivers expressed through the concept of shared socio economic pathways ssps ssps describe plausible trends in the evolution of society and the global economy they outline plausible alternative states of human and natural societies at a macro scale including both narrative and quantitative elements of socio ecological systems such as demographic political social cultural institutional lifestyle economic and technological variables and trends the global ssps are designed to be extended to regional and sectoral scenarios but make no assumptions about global or national level climate change policy absar and preston 2015 cradock henry 2017 2 2 3 shared policy assumptions for new zealand spas the shared policy assumptions spas for climate are specific to new zealand and were developed by frame and reisinger 2016 spas describe potential climate change mitigation and or adaptation policies not specified in the ssps they provide a third axis to the scenario matrix and allow national level development choices that may reinforce global trends or actively go against them they contain a mix of climate specific policies and non climate specific policies that have indirect but significant climate impacts or influence climate related vulnerability or adaptation options this enables us to create a mix of new zealand specific situations of greater relevance 2 2 4 scenario development for the wider project covering all five case studies on the landscape gradient alpine upland lowland coastal marine three scenarios were developed for new zealand at 2065 and 2100 the selected set as shown in table 2 covers a spread of possibilities that would provide sufficient material to explore the influence of radiative forcing unspecified pacific rcp 8 5 3 a and kicking screaming rcp 4 5 3 a keeping ssp and spa comparable and the influence of socio economic conditions kicking screaming rcp 4 5 3 a and clean leader rcp 4 5 5 f fig 2 keeping radiative forcing comparable the first step for scenario development following the involvement of the local government authority in the selection of the research site was to develop some national level narratives that would describe new zealand in relation to the rest of the world frame and reisinger 2016 appendix a these scenarios were developed using some indicators from the international institute for applied systems analysis iiasa database iiasa 2016 that have provided information to support the ssp development ebi et al 2014a van vuuren and carter 2014 frame et al 2018 designed a general methodology and guiding principles to downscale these national level narratives by incorporating locally specific conditions and concerns e g adaptation to sea level rise adaptation to management practices that could be directly accounted for in the modelling framework it should also be noted that as part of a much larger project and associated research projects the research team were closely embedded with the local authority and stakeholders in the development and sense checking of the scenarios the results of this process are discussed elsewhere frame et al 2018 cradock henry et al 2018 following this methodology the downscaling modelling processes used the o neill et al 2014 elements relevant for defining both challenges to mitigation and adaptation these include elements relating to demographics economic development environmental factors resources welfare institutions and governance technological development broader societal factors and policies table 3 they were evaluated quantitatively via modelling where feasible see section 2 3 2 3 integrated modelling assessment while we could not obtain quantitative models for all the elements suggested by o neill et al 2014 we were able to inform four elements demographics economic development environmental factors and resources fig 3 these models have inputs of climate variables from the rcp scenario only primary production pests and disease sea level rise socio economic variables from ssp only demographics or a combination of both drivers of change for ecosystems and their services can be classified into indirect and direct drivers diaz et al 2015 millennium ecosystem assessment 2005 this typology is now used in scenario analysis by the intergovernmental platform for biodiversity and ecosystem services pichs madruga et al 2016 the elements suggested by o neill et al 2014 can easily be categorized as indirect or direct drivers indirect drivers include economic demographic socio cultural governance and institutional and technological influences direct drivers include climate change over exploitation invasive alien species and land use change they then have an impact on natural capital and ecosystem services fig 3 as we were constrained from covering the whole suite of biodiversity and ecosystem services we chose to prioritise our assessment on the vulnerability of wetlands to water supply change as this was of most interest to stakeholders and have used ecosystem services models developed for new zealand to look at changes in some key indicators i e sediment and nutrient loss water yield and greenhouse gas emissions we took a coupled component modelling approach kelly et al 2013 as much as possible as it was essential to integrate the various models to create a broader perspective on how the components influence each other landowners can adapt to both climate signals and socio economic signals so in a first step a global economic trade model climat dge was used at the national level to study the efficient re allocation of resources within the economy and the response over time to resource or productivity shocks fernandez and daigneault 2015 climat dge was tuned to closely match projections published in the iiasa rcp and ssp databases iiasa 2016 and adapted to the global and new zealand populations provided by a demographic model cameron 2013 iiasa 2016 rutledge et al 2017 gross domestic product and greenhouse gas emissions doing so facilitated the estimation of global and domestic commodity prices that could be used as inputs to other modelling used in this study namely for the land use change model nzfarm daigneault et al 2018 however land use is also influenced by changes in yields in various primary sectors due to climate change this is covered by various tailored biophysical models that have been calibrated at the sector level including cropping holzworth et al 2014 livestock farming keller et al 2014 forestry kirschbaum et al 2012 and kiwifruit tait et al 2017 this is summarised in table 4 to separate the influence of climate change from influence of socio economic and climate change we tested scenarios using rcp using productivity change only or rcp and ssp using productivity change and commodity prices other important aspects necessary to incorporate into the land use change modelling relate to costs of production in particular pest and disease control is of critical importance to protect the production system climex is a mechanistic species distribution model that represents the vulnerability of the land to pest invasion kriticos et al 2015 while it does not provide a quantitative measure of extra costs due to added pesticides it does give some indication of future concerns and assumptions to be made for the kiwifruit industry winter chilling is essential to ensure flowering buds as the risks for warmer weather in the case study area increase the likelihood of needing additional chemical application such as hydrogen cyanamide hc increases cradock henry 2017 bioclimatic model outputs were available with and without hc to test in the three scenarios tait et al 2017 for cropping the impacts of climate change on maize silage were assessed by considering model runs with and without adaptation of crop genotype short and long maturity hybrids and sowing dates earlier in response to warmer temperature these two key independent tactical adaptation options are used by farmers to adjust to year by year weather variability teixeira et al 2017 and are expected to play an important role under climate change ipcc projections of global sea level rise between now and 2100 cover a range of 0 4 1 m church et al 2013 small increases of the order of 1 5 in wave height and storm surges from climate change effects are also likely by 2090 stephens and bell 2015 we mapped a future sea level rise of 0 8 m by 2100 ausseil et al 2017 following national guidance ministry for the environment 2016b vulnerability of natural ecosystem was assessed for freshwater wetlands bodmin et al 2016 while impacts on key land use based ecosystem services were assessed through nzfarm daigneault et al 2018 using indicators such as nutrient loss water yield carbon sequestration and greenhouse gas emission ausseil et al 2013 see appendix c for details the three scenarios were assessed by all the quantitative models except for demographics only tested for ssp3 and sea level rise the models from table 4 were run with assumptions fitting the rcps ssps and spas table 5 we chose the set of biophysical assumptions most internally consistent with the socio economic assumptions these include the use or not of additional chemical hc for kiwifruit cradock henry 2017 tait et al 2017 and adapted sowing dates for cropping teixeira et al 2017 2018 the three scenarios were compared for two timeframes of interest to stakeholders to reflect mid century 2065 and end of century 2100 future projections 3 results scenario impacts were modelled at a temporal scale of up to a decade between 2015 and 2100 and we present results for two points 2065 and 2100 to highlight the main trends arising table 6 shows the direction of change up to 2065 and 2100 from the quantitative models relative to a baseline case with no climate change and historical socio economic conditions 3 1 demographics in ssp3 new zealand s population trends do not mirror global trends ssp3 has the highest population at 2100 at the global scale while in new zealand population peaks then declines to a final value of 3 8 million lower than the current population of 4 4 million in ssp5 new zealand population will more than double to almost 10 million by 2100 3 2 economic development commodity prices are estimated to increase for nearly all major energy uses and agricultural outputs regardless of the scenario this is due to a combination of changes in population consumer preferences and resource availability furthermore pastoral commodities are estimated to see dramatic price increases over time in both rcp 4 5 scenarios not only due to increases in demand associated with global population growth but also as a result of national and global climate change reduction policies that place a price on greenhouse emissions from all sectors of the economy including agriculture and livestock prices are estimated to increase the most in the ssp5 scenario which is driven by high global consumption preferences coupled with a high carbon price required to meet an emissions trajectory consistent with an rcp of 4 5 w m2 target for 2100 in terms of productivity forest growth had similar responses under rcp 4 5 and 8 5 across the six gcms under constant co2 growth reductions averaged around 5 10 and were spatially variable from south east to north east ausseil et al 2017 however when factoring in increasing co2 the general negative response turned into general positive growth responses the difference was more pronounced for simulations under rcp 8 5 than under rcp 4 5 by 2065 there were general growth enhancements by about 5 10 under rcp 4 5 and 15 20 under rcp 8 5 for cropping model results indicate a higher risk of yield losses when sowing dates are not adapted particularly for short maturity hybrids for these conditions median yield loss estimates increase from mid century 5 to the end of the century 14 mainly because the crop cycle is shortened due to faster crop development in contrast by adapting sowing dates to a warmer climate i e sowing early yield losses were minimised and yield gains occurred for specific locations particularly when using long maturity hybrids teixeira et al 2018 by adapting sowing dates yield gains occur for specific areas in the catchment especially higher altitude areas in the south west that are currently limited by lower temperature in the current climate for pasture production the change is overall positive under all scenarios if we were to convert the whole catchment under either sheep and beef or dairy the magnitude of change is larger for dairy than for sheep mainly because dairy systems are more intensive requiring more nitrogen inputs and growing more grass table 6 total annual pasture growth across both dairy and sheep and beef averaged spatially over the entire kaituna region increases by 1 5 around mid century and by 2 7 5 by 2100 ausseil et al 2017 the increase is largely attributable to the co2 fertilization effect as the amount of increase in production follows the trends in co2 atmospheric concentrations from each rcp the increase in production is largest in rcp 8 5 at 2100 in which the co2 atmospheric concentration is highest c 850 ppm this effect is more than enough to offset any adverse climate effects on production for rcp4 5 the increase in production is marginal and ranges between 0 and 6 for kiwifruit sufficient winter chilling is vital to ensure sufficient flowering buds as temperature increases the risk to poor kiwifruit production increases however this can be compensated for by the use of hc since there is a risk that this chemical may be banned in the future because of its environmental toxicity and possible impact on human health cradock henry 2017 the assumptions of use or not for future scenarios have a strong influence on the projected viability of this industry we compared land use changes with and without accounting ssp assumptions fig 4 it can be seen that the ssp effects on changes in land uses relative to 2015 areas are much greater than the rcp only scenario this means that the changes in commodity prices are more influential for land use change than the changes in yields the impacts on land use changes are largely a combination of commodity prices increasing effect for most land uses and carbon prices decreased net revenue effect for pasture and crops increased effect for forestry looking towards 2065 in both scenarios involving rcp 4 5 the greatest change in land use is estimated to be a conversion from sheep and beef and dairy farms to forest plantations this is because although there is an estimated increase in milk and meat prices compared to 2015 the carbon price increase is stronger leading to a projected increase in forestry area thus the price effect has a larger positive effect on forestry than on pastoral enterprises the other land uses tracked in nzfarm including arable and horticulture are not estimated to change nearly as much in scenario rcp 8 5 3 a the absence of a carbon price and the large price effects estimated in climat dge cause sheep and beef to be relatively more profitable and hence there is a large shift back into that land use from forestry and scrub interestingly the prospects towards 2100 see a reverse of that trend with an increase in forestry relative to 2015 similar to the other scenarios the explanation is that the forestry prices catch up with the meat and milk prices making timber production more profitable even without carbon price rise finally coastal inundation due to sea level rise under a 0 8m scenario is projected to affect about 5500 ha of land that could become regularly inundated every couple of years during high tides this land currently under dairying could become permanently unavailable or uneconomical to manage for primary production reversion to a natural wetland may depend on other factors such as local level topography sediment and seed sources and anthropogenic infrastructure investment schuerch et al 2018 3 3 environmental and ecological factors the kaituna catchment used to have an extensive area covered by freshwater wetlands with only 8 wetland cover left compared to the historical extent ausseil et al 2008 the remaining wetlands are under considerable threat the main wetland type existing in the catchment is lowland swamps swamps are defined by johnson and gerbeaux 2004 as wetlands receiving a relatively high supply of nutrients with a water table usually permanently above some of the ground surface they have an abundance of cabbage trees flax and raupo along with kahikatea and swamp maire trees with a significant area in the kaituna reserve this type of wetland is more resilient to water supply changes than other wetland types such as bogs or fens climate change projections in the kaituna catchment are showing increases in rainfall so this may indicate a marginal effect on swamps bodmin et al 2016 however a proper water balance model would be required in future to account for increasing temperatures that may significantly change the evaporation and evapotranspiration rates this may increase water deficit and create added risk for these systems pest and disease risks are likely to increase substantially as temperature increases climex outputs for several pest species show a general trend towards increased suitability of major pests for both natural and managed ecosystems ausseil et al 2017 ecosystem services are equally impacted in both kicking screaming rcp 4 5 3 a and clean leader rcp 4 5 5 f up to 2065 fig 5 climate regulation in the form of reduced net greenhouse gas emission improves as carbon sequestration increases relative to 2015 and greenhouse gas emissions decrease because of the decrease in livestock numbers pressure on water quality is also decreasing with the decrease in total animal excreta decrease in both nitrogen and phosphorus losses erosion control improves with a decrease in soil loss due to the increase in tree cover especially on erosion prone areas such as the hill country of the catchment conversely water regulation is decreasing slightly as the increase in tree cover reduces water yield through higher interception davie and fahey 2005 the scenario rcp 8 5 3 a contrasts with the two others climate regulation is worse with a net increase in greenhouse gas emissions water quality could also be affected with more phophorus loss although nitrate leaching is projected to be at a similar level to 2015 this is due to the increase in soil loss especially on steep slopes resulting in phosphorus lost through sediment in 2100 the three scenarios show positive changes in ecosystem services except for the loss of the water regulation service this is consistent with forest expansion projected for all the scenarios regardless of the rcp ssp combination for rcp 4 5 3 a there is a greater shift of sheep and beef into forestry thus we have larger changes in positive water quality and erosion control decrease in phosphorus and soil loss particularly if this is happening on steep slopes there are relatively fewer water quality changes for rcp 8 5 3 a and rcp 4 5 5 f due to dairy still being a relatively large component of the landscape but not rcp 4 5 3 a 4 discussion this paper provides an overview of potential future impacts of both climate change and socio economic changes in a typical lowland environment of new zealand we demonstrated the use of quantitative and narrative statements for three plausible future scenarios unspecified pacific rcp 8 5 3 a kicking screaming rcp 4 5 3 a and clean leader rcp 4 5 5 f in the unspecified pacific scenario there was almost no attempt to curtail climate change on a global scale and only very limited reactive local efforts costs of production would generally increase due to a need for increased environmental management for pest control and water shortages with a higher risk for a decline in commodity prices due to increased global competition in scenarios kicking screaming rcp 4 5 3 a and clean leader rcp 4 5 5 f the rcp projection is the same only the socio economic pathways differ the modelling approach showed that the impact on land use and thus the ecosystem services was similar with high increases in forestry although the clean leader scenario projected a greater shift of sheep and beef to forestry these changes imply general positive benefits from ecosystem services except for a decrease in water yield due to the expansion of forest this is an important factor to keep in mind as the possibly drier conditions may have an impact on water availability in the catchment reporting results around two key periods 2065 and 2100 reveal some possible reverse shifts for example for scenario unspecified pacific rcp 8 5 3 a the land use change matches the two other scenarios with once again a shift towards forestry our approach used guiding principles from frame et al 2018 to combine global assumptions and local conditions and helped conduct a site specific assessment in a lowland environment of new zealand using landscape models enabled us to perform the assessment of climate change impacts giving insights into the implications for the local communities a diversity of models was integrated across multiple land uses keeping the underlying assumptions for regional environmental and land use policy in line with the chosen socio economic pathways it enabled us to explore the strong influence of policies and global economic context on regional economic output and ecosystem services relative to just accounting for the physical impacts of climate change one conclusion is that in our case the influence of commodity prices overwhelms the impact on land use change compared to the sole influence of climate change on the biophysical aspect in return changes in ecosystem services being directly related to land use change are mainly due to the socio economic context this finding has been found in other modelling work that has looked at the both the isolated and integrated effect of climate impacts and socio economic impacts on land use ahmed et al 2016 favero et al 2018 popp et al 2017 tian et al 2016 overall the process was highly interdisciplinary mixing biophysical economic and social science as noted earlier key stakeholders notably the local government authority were involved in workshops from the beginning of the process to ensure the relevance of the study for their own priorities and issues ausseil et al 2017 in turn the local government authority have been undertaking riparian management partnerships with private landowners for several years this exemplifies the new zealand context where interactions between local authorities landowners stakeholders and researchers take place very easily and informally a review of this process took place after the modelling was completed and is discussed later while we used categories of key elements from o neil et al 2014 not all elements including welfare institutions and governance technological development broader societal factors and policies were able to be modelled quantitatively the assumption is made that these will not provide a material difference to the models though this would benefit from testing in subsequent models it should also be noted that the levels of complexity required to accurately model any of these categories would likely to be beyond the scope of any modest study indeed there is a tension between the comprehensiveness of the o neill et al 2014 categories and what is feasible technically from a modelling perspective this process highlighted the inter dependencies between elements and gave insight into the complex chain of events and feedbacks that could occur in the future in other words local effects may overwhelm those arising from modelled global climate change effects and this leads to a tension between top down and bottom up modelling especially when compounded by the number of missing elements for which the impacts are complex and not necessarily linear for example the impact of climate change on some primary sectors can trigger land use change creating trade offs between food and timber provision dunford et al 2015 a key point to remember is that national and local scenarios are more than downscaled global or regional scenarios and it should be emphasised that some local issues not featured in the global models such as welfare institutions and governance technological development broader societal factors and policies as noted above may either overwhelm the effect of global parameters or drive the differences in drivers at local vs global scale the quantitative models had a degree of integration primarily through soft linking wene 1996 helgesen and tomasgard 2018 this was mainly done bypassing along the biophysical outputs for production changes to the economic models to drive land use decisions which then have a resulting impact on ecosystem services uncertainties are inherent to this integrated modelling approach and more sensitivity analyses would be required in future to understand how the outcome and narratives may be affected the main sources of uncertainties reside in data inputs e g different gcms and model parameters for the biophysical models economic models and ecosystem services models nzfarm has already been used to test uncertainty of erosion modelling outputs under different climate scenarios utilizing the same data inputs monge et al 2018 a similar approach could be tested here with a variety of tools available from the modelling community matott et al 2009 this could help understand how sensitive the whole system is to a range of uncertain inputs like the climate variables similar to ahmed et al 2016 or the socio economic assumptions e g commodity price pathways a careful selection of which components to sensitivity testing on would be required due to the complexity of the modelling system this is where the value of interacting with stakeholders and co designing scenarios becomes fundamental to ensure usefulness of the modelling approach identify components of interest and give insights on how important they are for the outcome some of these discussions could lead to new quantitative assumptions based on qualitative statements for instance pest and disease can affect primary production was translated as an assumed increased cost under rcp 8 5 3 a table 5 further discussions could identify the range of increased costs that would be included in the uncertainty analysis there is also potential for additional interdependencies with dynamic feedbacks to be modelled through hard coupling e g directly linking hydrological models with the land use change model however the extra computational efforts need to be balanced against the value of the additional information for example if the land use effect is revealed to be negligible then the hydrology model could assume a constant land use pattern over time since it is uncertain how these results would differ under an alternative approach future research would be needed to test this effect by attempting to hard couple some of the key models in our integrated system the combination of the three dimensions represented by rcp ssp and spa enabled a mix of new zealand specific scenarios of high relevancy to stakeholders as noted elsewhere these were discussed with stakeholders from central and local government business leaders and others as part of a review of the national scenarios developed across five landscape types of which this lowland example was one frame et al 2018 however this approach multiplies the number of possible scenarios given the number of rcps ssps and spas the goal then is not to describe every possible policy landscape but to select a finite number of representative central policy assumptions to produce a set of climate policy scenarios that are plausible within the global rcp ssp spa architecture we chose three scenarios to provide key messages to decision makers identifying trade offs and synergies between positive and negative outcomes from climate and socio economic pathways testing different scenarios allows comparisons and demonstrates the potential of societal adaptation options to make positive changes the results from the integrated modelling gave some insights into the direction of change for the chosen scenarios from there the quantitative results can be supplemented by narrative statements for each element ausseil et al 2016 frame et al 2018 see appendix b however it is the value judgements required to provide interested parties with meaningful scenarios that can influence policy development and decision making these will be subject to subjective interpretations the narratives should therefore be tested with modellers and stakeholders for relevance only through a process of co production with stakeholders can narratives be considered meaningful without that direct involvement scenarios run the risk of not having enough relevancy and legitimacy to be endorsed subtle indirect connections could also be overlooked if not discussed in a wide group of various interested parties in terms of further work it is clear that challenges to ensuring internal consistency between the socio economic and climate scenarios and new methods will remain and it is unclear if it will be possible to consistently link elements across scales schweizer and kurniawan 2016 or if an approach is needed that accepts incompleteness if this were the case then greater care may be needed in structuring end user expectations and reducing the likelihood of anticipating precision in climate change forecasts the implication of this may well be accompanied by a shift towards greater interest in adaptation than mitigation even though timescales may be less clearly defined one solution is the use of dynamic adaptive pathways cradock henry et al 2018 dynamic adaptive pathways can be used to explore a combination of adaptation options with stakeholders ensuring their participative efforts for developing responses to plausible futures for the lowland case study for example even if total pasture yield increases the seasonality shift may induce a significant level of change in livestock management practices log prices have been predicted to increase but motivations for change from land owners may not result in such a dramatic change in land use as was predicted solely by the economic model a close and iterative process is required between qualitative information from local people s motivations for change and quantitative models to explore future options and increase models fitness for purpose and utility in future planning in terms of the specific context discussed in this paper we have examined an approach that has attempted to balance modelling rigour and accuracy with a highly complex local example where not all parameters can be modelled or even be fully understood to provide support to decision makers and stakeholders in the short term has required making assumptions based on local conditions and knowledge inevitably this leads to a softening of accuracy in order to maintain stakeholder involvement such trade offs between what can be termed internal and external validity of models appears to be not just an unavoidable consequence of modelling climate change impacts but an essential component of engaging society with the science of climate change and the realisation that none of it will be simple acknowledgements we thank funding from the ministry of business innovation and employment as part of project climate change impacts and implications contract c01x1225 we acknowledge the world climate research programme s working group on coupled modelling which is responsible for cmip producing and making available their model output we acknowledge the bay of plenty regional council for their support and identification of key issues in the study area an earlier version of this paper was presented at the international environmental modelling and software society conference in july 2016 toulouse france a g ausseil et al 2016 authors contributions a a designed the research a a a d e t performed the research and the analysis a a a d and b f wrote the paper we are also grateful for the reviewers comments appendix a new zealand national narratives for three scenarios from frame and reisinger 2016 id rcp ssp spa scenario sketches at 2050 unspecific pacific 8 5 3 a very high emissions 8 5 w m2 ssp3 fragmented world a with nz lagging relative to global efforts to mitigate nationally there is only incremental and reactive adaptation on a piecemeal basis an increasingly highly populated world with break down of open trade and security regional crises and financial instability new zealand is part of a pacific backwater that is sliding downwards economically with refugees from pacific islands with military conflicts between australia and indonesia impacts of climate change on nz are dealt with as and when they occur with local solutions generating problems due to short term decision making free trade is with volatile partners who require constant attention and accept their prices to protect trade routes and access with military force a severely resource constrained world neither cares nor can afford the luxury of environmental goals and bar a few militant voices there is little premium on such values investment in technology is very short term and environmental conservation is a low government priority 4 5 3 akicking screaming 4 5 3 a intermediate stabilization 4 5 w m2 ssp 3 fragmented world a with nz lagging relative to global efforts to mitigate nationally there is only incremental and reactive adaptation on a piecemeal basis as climate change impacts mount there is increasing global concern with responses led by a few large emitters with ongoing argument over responsibility and accountability nz positions itself with a group of nations who due to their small size and distance to markets have a lesser obligation to reduce emissions global trade is not free with many artificial barriers between the group of sustainability nations and those for whom economic growth is the key to managing environmental risks and providing resilience impacts on nz are dealt with as and when they arise which often means short term economic concerns win over longer term societal and environmental objectives assets at risk are protected by the crown but with limited budgets the cost of doing so is a major influence on decisions to do so discount rates are negotiable with preferred suppliers environmental conservation is a low government priority 4 5 5 fclean leader 4 5 5 f intermediate stabilization 4 5 w m2 ssp 5 conventional development fossil powered growth f nz ahead of increasingly stringent global efforts to mitigate and strategic approach to adaptation to maximize not just economic opportunities but to achieve sustainability across three pillars global emissions are actively being reduced through sharing more efficient conventional technologies and ensuring mitigation is achieved where most cost effective countries continue to argue for economically important sectors to be exempt from emissions obligations nz is developing innovative low cost mitigation opportunities and actively trading those for economic gain to the highest bidder factory based ge farming is unregulated but restricted to offshore islands strategic land use decisions are made with a view to maximizing economic growth in a warming and carbon constrained world when it comes to balancing trade offs in adaptation responses economic concerns win over environmental or societal equity concerns nz leads global practice in free market emissions trading variable discount rates are explored in nz as a means of creating change which is attracting global profile for nz goods and services with environmental conservation a strong government business partnership appendix b example of narratives for three scenarios in the lowland case study unspecific pacific rcp 8 5 3 a from ausseil et al 2017 frame et al 2018 kicking screaming rcp 4 5 3 a from the authors clean leader rcp 4 5 5 f from the authors demographics the rural population could continue to decline rural areas typically have older populations which leads to lower natural increase or even natural decrease of the population ageing is likely to lead to a less mobile population that is less able to avoid hazardous situations like flood events the new zealand population is projected to peak at 5 million in 2040 compared to a current population of just over 4 million development on coastal areas may slow or stop as a result of declining rural population there is likely to be further agglomeration of farm enterprises the rural population continues to decline rural areas have older populations mostly in the lower socio economic groupings and a resulting decrease of overall rural population urban areas have increased large dwellings with multiple generations all living under a single roof for economic reasons ageing is likely to lead to a less mobile population that is less able to avoid hazardous situations like flood events the new zealand population is projected to peak at 5 million in 2040 compared to a current population of just over 4 million development on coastal areas is for low socio economic groups who end up with previous life style properties which have become low priced but very expensive to maintain due to more frequent storm events farm enterprises are very responsive to increasingly dynamic market conditions urban intensification coupled with urban farming using a mix of hydroponics and 3d printed food reverses the trend towards urban sprawl the auckland hamilton corridor is the centre of this new form of urban horticulture which is seen as a global leader including the development of gene edited seed production facilities of global benefit due to far sighted investment in new technologies in the 2030snz is very highly urbanised 99 the resulting depopulation of the south island which has become predator free due to the same gene editing technologies means former primary production centres are now highly sought after top end retirement communities with exceptional recreation opportunities farm enterprises are large and geographically based often with community ownership able to set local tax rates economic development in general we could expect a decline in new zealand s economic health food security both internationally and within new zealand is expected to be a major driver leading to a decline in overseas markets trade e g kiwifruit and an increase in diverse local markets the limiting factor for primary production is likely to be appropriate and consistent access to water and the impacts of any extreme weather events dairy farming is likely to increase to the detriment of sheep farming a concern for the catchment might be the decline of kiwifruit biophysical suitability due to lack of winter chilling adding extra costs to production by requiring the use of chemicals to improve flowering this could be exacerbated by increased costs due to disease outbreaks and infrastructure costs regional climatic suitability for agricultural crops will change for example areas currently limited by low temperatures will be more suitable for cropping and rain fed agriculture will become more vulnerable to drought particularly for soils with low water holding capacity primary produce is traded more within nz than globally in highly volatile markets influenced heavily by the burgeoning asian markets the carbon price is high incurring high costs for the dairy industry primary production suffers unpredictable cost fluctuations due to disease outbreaks caused by erratic weather patterns the increased and unmet cost of production due to roading infrastructure limits forestry production kiwifruit as a land use may be suitable with interventions to compensate lack of winter chilling overseas competition is worsening the picture with a decrease in commodity prices dairy kiwifruit most primary produce is traded within nz except for technology transfer of gene editing and predator management regimes internationally this has led to a shift from high volume low value add production to very high value added service exports a good carbon price and high costs for dairy primary production has high overhead costs that covers very effective disease control with very little detrimental impact from predators forestry production has shifted to small volume high value native hardwoods on restored landkiwifruit production faded in the mid 2050s and gave way to new 3d production of heritage fruits from across the world though new zealand s high technology gene editing facilities overseas competition has made commodity production unviable except for technology transfer of precision interventions which are piloted on the quarantined chatham islands environmental and ecological factors if land is abandoned due to unfavourable climatic or trade changes the land could revert to natural wetlands however lack of funding for control measures could exacerbate the spread of exotic plants in wetland areas and create a risk for weed infestation of nearby cropping and pastoral farming native forests wetlands and rivers could see a decline in biodiversity due to pest invasions increased sedimentation water diversion for economic uses salinization in the coastal zone and lack of funding for conservation with warmer temperatures pests currently limited to warmer climates could expand their range into the case study area and become more prolific causing a reduction in abundance or loss of native species water discharge could reduce due to a reduction in precipitation creating water stress during summer native forests and wetlands have declining biodiversity due to pest invasions and lack of funding for conservation natural areas wetlands forest are degrading rapidly due to pest invasion and salinization and diversion of water for economic uses impacting on recreation impacts on water and sewerage infrastructure lead to a requirement for increased investment but there is only patchy provision of investment by councils retreat from the coast is unmanaged unplanned but forced by lack of funding determined by political economic preferences not by social or environmental objectives sea level rise decreases coastal property values with coastal walls built on an individual basis without accounting for the social environmental costs or those whose properties are then at greater risk nitrogen phosphorus and sediment are well managed through highly engineered systems sediment traps stormwater or drains that remove contaminants etc or technology advances new fertiliser that binds to soils or isn t leached the agricultural sector is exempt from the greenhouse gas emissions scheme nz is a global leader in carbon sequestration and emissions trading although the methods are not environmentally orientated there is strong development of carbon sequestration and filtering scrubbing methods and technologies resources fuel costs are expected to rise with an increased reliance on fossil fuel based electricity primarily coal this in turn could increase primary production and household utility costs and the use of public transport the tourism sector could suffer from these additional costs through greater travel costs the coastal zone could be impacted by sea level rise affecting agricultural land mainly dairy and maize cropping very high energy costs both renewable and non renewable result in very high efficiency devices which use dynamic pricing to keep costs down this results in a less reliable supply which in turn maintains low greenhouse emissions nationally this does result in market friction including illegal mining of open cast coal on the west coast for pirate energy providers tourism responds through interest in alternative2 energy tourism which markets coal powered communities along the lines of steam punk sympathetic managed retreat from the coast is funded by new forms of resource taxation that have displaced income tax as main form of personal taxation this is also funded through premiums placed on non nz nationals travel costs including a tourism levy for international visitors and through differentials on domestic transport including on electric vehicle recharging stations nz finally started to increase its of energy produced through renewables after many years in decline when low cost high volume batteries reduced reliance on non renewables i e coal and oil as back ups for wind and solar energy welfare sea level rise is expected to lead to a decline in coastal property values and eventual abandonment of the most vulnerable properties due to coastal encroachment human vulnerability to natural disasters could increase due to more frequent extreme events e g floods life expectancy may decline especially with potential reduced funding for health care services and the likely increases in the incidence of infectious diseases with coastal areas becoming an increasingly important reservoir for disease vectors such as mosquitoes there is a highly inequitable societal split between a small green elite and a low greenhouse gas impact majority resulting in the net effect of a low carbon nz this produces weak responses to increasing infrastructural damage from climatic change events by 2100 this shows up in differences in health statistics between those eligible for comprehensive private medical care and those subject to a public system with reduced resources health care is universal for long term citizens but very expensive for recent immigrants working visitors and tourists while this balances economically it results in a thriving black market for citizenship which aligns with place of residence one s post code is an increasingly important status symbol even for those in low cost low impact dwellings prevention of new climate related diseases is available through gm based pharmaceuticals creating ethical tensions between communities rather than individuals institutions governance excluding climate policies due to limited investment in sector catchment scale adaptation options to reduce risks flood events could increase dramatically from added sedimentation in the rivers and lack of funding to raise stopbanks road networks are likely to deteriorate worsening the economic conditions in the region social inequities could deepen due an inability to pay global agreements such as the kyoto protocol could be regularly breached and contingent liability could be transferred to central government tourism levies on routes air land and sea to major attractions reshape traffic infrastructure resulting in marked differentiation between honeypot sites and land that has deteriorated due to overuse in turn this puts high pressure on resorts to maintain unesco accreditation and its accompanying national subsidies there are knock on effects to wages in support sectors and the potential for falsification of greenhouse gas credentials user pays taxation continues to dominate over the modest introduction of resource usage instruments planning and regulations preserve highly productive land for primary production ownership is through new forms of public private partnership which involve lengthy consultation processes with iwi led community groups that rely on monthly compulsory voting by all members aged 16 75 taxation regimes are dominated by complex resource taxation systems that collect data using remote sensing techniques developed in nz this big data low taxes model is made available through the creative commons platform which nz administers for australia the pacific and latin america as part of the new global order technological development we assume that no new climate change mitigation options will be developed but that local adaptation solutions will be created in a reactive way lagging behind global initiatives we expect that fewer research efforts will be funded by government and more by industry most professional researchers supplement their modest incomes through provision of on line fee for services consulting and lecturing to a global market interested in nz s unique solutions to climate change gene editing abounds with a low barrier to entry regulation not proving effective at mitigating adverse effects nz leads greenhouse gas mitigation globally through highly innovative gene engineering technologies piloted on the quarantined chatham islands for 10 years before beta testing on the north island and subsequent launching globally nz capitalized on its remoteness excellent rs t investments and its desire to move on from low value high volume primary production to a high tech provider of solutions broader social factors there could be a general disconnect from nature recreation in and aesthetic appreciation of the outdoors ranks low on the list of people s priorities due to the high costs of living gated communities extend to wilderness areas as well as in urban areas with numbers to sites of exceptional scenery sexs limited annually through competitive bidding on line in turn these are re sold through the dark net as one of many alternative economies that failed to be regulated through crypto currencies virtual reality of nature franchises boom in gated urban communities especially through the fiercely competitive virtual golf the conservation programmes of the early 21st century have given way to the new nature now nnn movement which nz championed through the intergovernmental platform for biodiversity and ecosystem services ipbes in the mid century following acceptance that species extinctions were inevitable and that climate adaptation was the only available avenue there were sweeping changes in societal engagement with nature while many species were lost to science though preserved through the national collections human society was much more connected with other life forms which helped stem carbon emissions policies excluding climate policies loss of population and sea level rise could lead to ad hoc coastal protection insurance may be difficult to obtain or would not cover natural events development initiatives could be market driven and lacking policies to include social environmental or cultural elements regional tensions become marked resulting in unofficial permits to live in other regions driven by grey market preferences for family and friends officially these are driven by greenhouse gas targets but in reality financial and social concerns dominate māori pākehā tensions rise policies are driven by consensus and subsidiarity this results in markedly differing forms of solutions between regions while publicly accepted as part of national identity underlying tensions are never far from the surface and lead to a surprisingly strict government centre in terms of national legislation 2 the ecosystem services indicators were all tracked in nzfarm using look up tables from new zealand models such as overseer watyield nzeem the greenhouse gas model followed nz mfe inventory methods ausseil et al 2013 appendix cstudy model descriptions demographic model we developed a bottom up sub national population projections model wherein gross migration between pairs of regions in new zealand n 16 is modelled using a set of age sex specific gravity models our model allows all inter regional migration flows to be estimated and projected in a common framework with a single set of assumptions this method offers a number of advantages over traditional methods in particular that factors known to affect migration flows such as climate etc can be explicitly based on regression modelling incorporated into the population projections in a transparent and justifiable manner the estimated internal migration gravity model demonstrated that climate variables sunshine rainfall had statistically significant but very small effects on internal migration in new zealand as such most of the projected population change in new zealand is likely to be driven by other factors e g economic factors population ageing and momentum etc as discussed in cameron 2013 sea level rise slr the intergovernmental panel for climate change ipcc released its fifth assessment report ar5 in 2013 14 the report showed that the primary climate driver for slr is global and regional surface temperature which in turn is strongly influenced by greenhouse gas emissions slr occurs as a result of ocean warming which causes thermal expansion of the sea and melting of glacial ice both of which have contributed to sea level rise throughout the 20th century in new zealand the sea level rose by an average of 0 18 m around the country last century relative to the land mass it is rising relatively faster in some locations where the landmass is subsiding such as wellington the rate of slr was approximately linear over the last century but is forecast to accelerate over the next century due to global warming and possible rapid melting of the greenland and antarctic ice sheets the ministry for the environment guidance manual ministry for the environment 2016b recommends that for planning and decision timeframes out to the 2090s 2090 2099 at the very least all assessments should consider the consequences of a mean sea level rise of at least 0 8 m relative to the 1980 1999 average pasture growth modelling to model pasture growth we used the biome bgc model v4 2 thornton et al 2002 2005 adapted to two types of new zealand managed grassland systems sheep beef low intensity and dairy high intensity the biome bgc model is an ecosystem process model that simulates the biological and physical processes controlling fluxes of carbon nitrogen and water in vegetation and soil in terrestrial ecosystems this includes the co2 fertilization effect which enhances the rate of photosynthesis and reduces water loss in plants under elevated co2 atmospheric concentrations we used the model s built in c3 grasslands mode with some key ecological parameters modified and re interpreted to represent managed pasture and the presence of grazing animals keller et al 2014 model parameters were calibrated against new zealand pasture growth data and validated for both dairy and sheep systems as described in keller et al 2014 the primary model inputs daily minimum and maximum temperature precipitation vapour pressure deficit and solar radiation were derived from the outputs of the 6 gcms listed in section 4 2 1 unique parameterizations were assigned to both sheep and dairy ecosystems the main difference between the two is the intensity of farming dairy systems receive more nitrogen inputs to simulate more fertiliser use more grass is eaten in the form of increased whole plant mortality and more animal products milk or meat are extracted from the system in addition the dairy parameterization effectively results in increased water use efficiency note that irrigation is not simulated in either system the model was run for each 5 5 km vcsn grid square in the kaituna catchment with location specific weather inputs soil texture and rooting depth a reference or baseline pasture production for each gcm was simulated using the rcp past climate input representative of modern day conditions and averaged over the nominal years 1985 2005 for all future scenarios the model was first spun up using rcp past climate and then restarted and run as a transient simulation from 2005 to 2100 using each model and scenario specific projected climate forestry modelling the simulation results described here used the comprehensive process based ecophysiological model cenw 4 1 to simulate the growth of pinus radiata in the bay of plenty region the model had previously been parameterised for the growth of p radiata based on data from the whole of new zealand kirschbaum and watt 2011 it had also previously been used for climate change impact assessments for new zealand kirschbaum et al 2012 and essentially the same modelling procedure was followed here it essentially models tree growth over 30 years with initial stand densities and thinning regimes as described by kirschbaum et al 2012 the novel aspect of the present work is the availability of actual daily output of key weather parameters from six different gcms for each day up to 2100 in past work only average changes in weather parameters were available and theses weather anomalies had to be added to a current day weather sequence that preserved a realistic pattern of seasonal changes in weather parameter but did not include any possible changes in those patterns themselves the climatic data used here are the direct output from gcm runs and thus include any possible changes in weather patterns such as changing inter annual frequency of drought periods or changes in seasonal temperature or rainfall patterns data are presented both as changes from current 1980 2010 productivity to productivity in 2055 2040 2070 or 2085 2070 2100 and as progressive changes by running the model to simulate productivity from 1980 to 2010 followed by a run with data from 1981 to 2011 and so on crop modelling the agricultural production systems simulator apsim is a biophysical model that simulates crop growth at daily time steps in response to climate soil and crop management holzworth et al 2014 the model represents processes that control dynamics of carbon water and nitrogen in plants and soils the timing and specifications of management interventions e g sowing harvesting fertiliser and water application are also represented in apsim to extend apsim applications beyond its original point basis configuration i e single location to a catchment scale 5 km resolution dynamic simulations of crop management sowing dates fertiliser application and crop type choice in response to environmental conditions were developed model simulations did not account for biotic stresses insects pathogens and weed competition or damage by extreme events e g floods heat waves or storms simulations were performed for silage maize zea mays maize was selected as an indicator of climatic suitability to arable cropping due to its importance as a forage option for the dairy sector and its wide presence across new zealand to explore adaptation options to climate change apsim runs were performed for two maize genotypes short and long maturity hybrids with and without considering adaptation of sowing dates i e sowing early in response to warmer climates simulations were performed for a baseline 1986 2005 mid century 2046 2065 and end century 2081 2099 with the six selected gcms global circulation models simulations were run continuously with a spin up period of 3 8 years depending on data availability as maize in the kaituna catchment is typically grown under rain fed conditions on soils with high water holding capacity assumed 160 mm m these conditions were therefore assumed in simulations a 30 year baseline model run using the historical climate era databases from 1971 to 2000 was used to calibrate and test the model sowing dates and yields of historical model runs were scrutinised by maize crop experts dr john de ruiter senior scientist at plant and food research pfr and mr allister holmes research extension team leader at the foundation for arable research far in addition model results for historical weather were compared with data from an online survey developed by far and pfr with maize growers across new zealand model results were analysed for grid cells else than lake areas kiwifruit suitability the coldness of the winter period in new zealand may july has a very strong influence on both the quantity and quality of kiwifruit flowers as well as the timing of flowering this in turn has a direct influence on the number of buds the timing of bud break and hence the number and quality of fruit produced by the vine sufficient winter chilling is therefore vital for kiwifruit production viability as temperatures increase due to global warming the risk of insufficient winter chilling also increases as does the risk of poor kiwifruit crops a simple temperature threshold based empirical model has been developed for assessing current and future to the year 2100 hayward kiwifruit production viability for the te puke area tait et al 2017 the model is derived from the phenology of the crop and published literature that relates temperature to four phenological stages of the hayward vine development the model includes the effects of applying or not applying hydrogen cyanamide to aid the number of flower buds for simplicity the model runs are labelled with hc and without hc soil loss model nzeem erosion control is defined as the prevention of soil loss by an ecosystem the demand for erosion control is mainly local coming from the farming sector at risk of losing productive soils and from river users who desire clear water and nondegraded river habitats in new zealand mass movement erosion is the dominating erosion process dymond et al 2010 which is mitigated primarily by tree roots we chose soil erosion rate tonnes of soil km2 yr as an indicator of erosion control erosion control can be derived using the difference in sediment loss with and without respective ecosystems we estimated soil loss using the nzeem erosion model which has been calibrated from sediment discharges measured in new zealand rivers dymond et al 2010 this model estimates the long term mean erosion rate from all sources of erosion both mass movement and surficial and accounts for all sizes of rainfall events in this paper we consider any reduction in soil loss as a positive outcome for erosion control water regulation water supply through rivers is important for drinking water for both animal stock and humans irrigation and hydropower generation the maintenance of flows in rivers their timing and magnitude by ecosystems is defined as water flow regulation we chose the net supply of water remaining after evapo transpiration losses mm yr as an indicator of waterflow regula ion we used watyield fahey et al 2010 that models daily water transfers of rainfall interception evapotranspiration and drainage associated with a soil profile input data to the model are daily rainfall and daily potential evapotranspiration pet parameters required for the model include the fraction of intercepted rainfall vegetation factors for transpiration and total and readily available water holding capacity of the soil the model was run for several homogenous soil climate units for different land covers forest scrub tussock and pasture we stored the proportion of rainfall that becomes water yield in a lookup table so that a simple computer workflow comprising national spatial data layers could be implemented to calculate impact of land use change on water yield in this paper we consider any increase in water yield as a positive outcome for water regulation vice versa a reduction in water yield has a negative impact on water regulation nitrate and phosphorus loss through leaching overseer the provision of water for drinking purposes and habitat for aquatic organisms is linked to high levels of water purity whereas for hydropower quality standards are less important in new zealand the main pollutants to reduce water quality originate from agricultural activities which generally increase the level of nutrients in soils above natural background levels especially nitrate and phosphorus we estimated nitrogen leaching using overseer version 5 4 ministry of agriculture and forestry et al 2011 a nutrient budget tool that takes farm management soil and climate variables as inputs and produces annual nutrient budgets including nitrogen leaching as for watyield we ran overseer for 100 combinations of soils and climate dymond et al 2012 ausseil et al 2013 the nitrogen and phosphorus leaching rates per stock unit were combined with the animal numbers from nzfarm to produce a map of nitrogen leaching for all of new zealand in this paper we consider any reduction in nitrogen or phosphorus sources as a positive outcome for water quality climate regulation greenhouse gas emission ghg fluxes fluxes of greenhouse gases were considered as an indicator of influences on the global climate that act through radiative forcing of the atmosphere we did not consider the influences on local climate such as the effect of riparian shading on river water temperature new zealand is a signatory to the kyoto protocol and is thus legally bound to control net greenhouse gas emission in new zealand global climate regulation is strongly influenced by the agricultural sector which emits greenhouse gases into the atmosphere and the forestry sector which sequesters carbon from the atmosphere we therefore estimate the influence of agriculture and forestry on fluxes of greenhouse gases separately for agricultural greenhouse gas emission we considered the three main greenhouse gas emissions methane nitrous oxide and carbon dioxide we used the animal numbers for dairy sheep beef and deer and multiply them by an implied emission factor per animal from the greenhouse gas inventory report from the ministry for the environment ausseil et al 2013 to model changes in carbon sequestration we assumed that any additional carbon sequestered would be related to forestry cover we used the process based model cenw used for the forestry modelling model efficiencies were 0 83 for height 0 89 for diameter 0 82 for basal area and 0 84 for volume the model was applied to new zealand on a 0 05 latitude longitude grid using 20 years of daily weather input data and soil surfaces fertility water holding capacity texture kirschbaum and watt 2011 to produce a map of potential carbon sequestration rates over 30 years in this paper we consider reduction in greenhouse gas emission and increase in carbon sequestration as a positive outcome for climate regulation wetland vulnerability changes in precipitation may impact wetland extent wetland condition community composition or ultimately a shift in either wetland type or ecosystem wetland vulnerability to climate change was explored at national scale using a risk framework that combined exposure to a defined climate change stressor sensitivity of an ecosystem to change impacts and adaptive capacity bodmin et al 2016 this initial investigation examined exposure of wetlands to the climate change stressor water availability exposure to water availability was assessed as the change in precipitation for new zealand between 1980 1999 and 2080 2099 using gis these likely precipitation changes were intersected with the current extent of different freshwater wetland types freshwater wetlands were classified into bog fen swamp marsh seepage gumland and pakihi ausseil et al 2011 pest simulations pest simulations we conducted using climex v 4 02 model primarily through the compare location function 1 species extended ausseil et al 2017 this analysis used both the monthly data as well as 20 year time period climate normals centred on 2005 2050 and 2090 data was provided as netcdf files these where manipulated in python to create the input datasets required by climex an audit was undertaken to ensure that transformation did not corrupt the data due to database size limitations separate climex data files where created for the monthly data on a 5 year time step for each species x rcm x rcp and one climate input file for the normals data set locations are national for each 5 km grid cell in the vcsn climate change projection data was loaded into the climate input file by repurposing variables of continent country state to hold rcm rcp and year labels all parameters other than climate data and species were kept to the default two climate data sets where used to run the simulations each species is modelled at 5 year intervals 2015 2120 for each rcm and rcp capturing the inter annual variation in potential distribution each species is modelled using the 20 year normal data centred on 2005 2050 2090 for each rcm and rcp these data were reduced by using the maximum ei for each cell from each of the rcm i e the worst case scenario of ei risk is presented the models were provided by john kean agresearch via the climenz website http b3 net nz climenz index php and were able to be downloaded as climex ready parameter files saving considerable time as well as ensuring parameter accuracy 55 species modelled were selected from a range of sources that identified pests or unwanted organisms in nz such as the national pest plant accord legal sources notifiable organisms and those recommend as pests either as have been modelled in nz before using earlier climate data sets or as potential threats to biodiversity or production systems climat dge to estimate the effect of global ssp on the new zealand economy we used the climate and trade dynamic general equilibrium climat dge model developed by landcare research climat dge is a multiregional multi sectoral forward looking dynamic general equilibrium model with a relatively long time horizon of 100 years or more this model is suited to studying the efficient re allocation of resources within the economy and the response over time to resource or productivity shocks climat dge primarily uses the global trade analysis project gtap version 8 data set the base year of the benchmark projection is 2007 the model then develops a benchmark projection of the economic variables and ghg emissions and simulates scenarios to evaluate the impacts of mitigation policies based on long run conditions and constraints on physical resources which restrict the opportunity set of agents the model predicts the behaviour of the economy energy use and emissions by region and sector fæhn et al 2013 climat dge covers 18 aggregated production sectors we focused on the cattle and food sectors model dynamics follow a forward looking behaviour where decisions made today about production consumption and investment are based on future expectations estimated in 5 year time steps the economic agents have perfect foresight and know exactly what will happen in all future periods of the time horizon thus households are able to smooth their consumption over time in anticipation of large price shocks that may arise as a result of resource constraints or environmental taxes for a thorough description of climat dge see fernandez and daigneault 2015 nzfarm the new zealand forest and agriculture regional model nzfarm is a comparative static non linear partial equilibrium mathematical programming model of new zealand land use operating at the catchment scale daigneault et al 2012 2018 in this study it was used to assess how changes in climate i e yields socio economic conditions e g commodity prices and input costs resource constraints and environmental policy e g ghg reduction pathways could affect a host of economic or environmental performance indicators that are important to decision makers and rural landowners the version of the model used for this analysis can track changes in land use land management agricultural production freshwater contaminant loads and ghg emissions see fig a1 figure a 1 diagram of inputs and outputs from nzfarm red indicates key inputs for this anlaysis figure a 1 in this case study we use nzfarm to assess the implications on farm income land use and the environment when farmers in the kaituna catchment are faced with variations in agricultural yields due to climate change and or alternative shared socio economic pathways this analysis builds on previous work on climate change impacts on agriculture and forestry in new zealand by indicating not only the likely impact of climate change on production but also the effect that landowner adaptation may have on land use economics production and environmental outputs within a simultaneous modelling framework the model s objective function maximizes the net revenue of agricultural production subject to land use and land management options production costs and output prices and environmental factors such as soil type water available for irrigation and any regulated environmental outputs e g ghg emissions taxes imposed on the catchment catchments can be disaggregated into sub regions i e zones based on different criteria e g land use capability irrigation schemes such that all land in the same zone will yield similar levels of productivity for a given enterprise and land management option in this case each vcsn grid cell is modelled as an individual zone within the kaituna catchment simulating endogenous land management is an integral part of the model which can differentiate between baseline land use and farm practices based on average yields achieved under the current climate and those that could be experienced under a range of rcps landowner responses to changing climate and socio economic conditions are parameterised using estimates from biophysical and ecosystem service models described elsewhere in this manuscript commodity prices estimated from climat dge and farm budgeting models described in daigneault et al 2018 
26168,check dams are one of the most popular measures for soil erosion control despite the availability of various process based modelling tools their design is often carried out in an uncoordinated manner potentially leading to check dam systems with limited life expectancy or unreliable sediment retention capacity in this study we consider the problem of determining the optimal location and size i e initial storage capacity of a given number of check dams and tackle this problem by contributing a numerical framework that builds on geo processing two process based models watem sedem and stodym and multi objective evolutionary computation the application of our framework to two catchments in the chinese loess plateau characterized by similar erosion processes but different extensions 4 26 and 13 97 km 2 reveals a strong trade off between three criteria of system s performance namely life expectancy sediment retention capacity and storage dynamics results also show that there are opportunities for improving the performance of existing check dam systems through a coordinated optimization based planning exercise keywords soil erosion check dams optimization multi objective evolutionary algorithms watem sedem loess plateau software availability name of software stodym version 1 0 developers d pal software required python arcgis watem sedem contact email debasish pal sutd edu sg year first available 2019 available from development version available on github https github com debpal stodym 1 introduction revegetation and check dam construction are among the most popular measures available for soil erosion control revegetation or planting acts by mitigating the effect of hydro meteorological forcing on soil erosion thereby preventing the formation of gullies and reducing sediment load to the stream network xiangzhou et al 2004 on the other hand a check dam acts as a sink for sediment this hydraulic infrastructure reduces the water flow velocity and helps retain sediment in the upstream pond and drainage canal tang et al 2018 in the long run the flat area resulting from the sedimentation process can be used as cropland whilst different studies have shown that these two measures work best when combined verstraeten and prosser 2008 zhao et al 2017a fang 2017 shi et al 2019 revegetation and check dam construction are typically planned independently with check dams being often privileged owing to their limited investment and operating costs check dams are indeed adopted in several countries such as spain castillo et al 2007 italy bombino et al 2009 iran hassanli et al 2009 or china xiangzhou et al 2002 in the chinese loess plateau alone for instance there is an estimated number of at least 100 000 check dams jin et al 2012 to understand the morphological hydraulic sedimentary and ecological effects of check dams researchers tend to adopt two complementary approaches namely field observations and numerical simulations see for example the recent work of shi et al 2019 and lü et al 2019 who investigated the effect of check dams on the sediment budget of two hilly catchments in china field observations provide key information on sedimentation in ponds and drainage canals while numerical simulations are used to evaluate the response of a catchment e g the sediment yield to different land use and check dam deployment scenarios these modelling efforts typically rely on process based models such as swat xu et al 2013 li et al 2017 shi et al 2019 sedd zhao et al 2017a or watem sedem zhao et al 2015 fang 2017 lü et al 2019 all adopting modifications of the universal soil loess equation renard et al 1997 merritt et al 2003 to estimate soil erosion rates despite the presence of these studies focussing on both field work and numerical simulation there seems to be a lack of knowledge on how check dams should be designed and tailored to the hydrological characteristics of a given area this is a planning problem that challenges the existing simulation based paradigm in two ways first check dams are generally implemented as a system of dams across a catchment so a coordinated implementation effort could hardly rely on a few simulations runs or what if scenarios which would provide a narrow representation of the available design alternatives along with their corresponding performance second the aforementioned models cannot simulate how the storage capacity of check dams varies with time zhao et al 2017b a key factor controlling their life expectancy and capability to retain sediment to address these challenges our work proposes a numerical framework for the optimal design of check dam systems in particular we consider the problem of determining the optimal location and size i e initial storage capacity of a given number of check dams in an erosion prone area the framework builds on two process based models and multi objective evolutionary computation the models are watem sedem van oost et al 2000 which estimates sediment erosion and delivery to the stream network and stodym storage dynamics model an improved version of the model recently introduced by pal et al 2018 to simulate the sediment trapping efficiency and storage dynamics of check dams stodym is then coupled with a multi objective evolutionary algorithm moea maier et al 2014 which determines the location and size of the check dams that optimize the life expectancy capability to retain sediment and storage dynamics of the whole system the latter criterion ensures that the storage capacity of all dams decreases with comparable rates thereby reducing the chances of abrupt changes in the sediment load we show that the proposed framework has three desirable features first it requires minimal user inputs provided that the two process based models i e watem sedem and stodym are available for the study site at hand second it allows studying the trade offs between the different measures of performance characterizing a check dam system third the computational requirements are in the order of a few hours for catchments with an extension of 5 15 km 2 thereby allowing the user to experiment with different settings in a reasonable amount of time in the remainder of the manuscript we first describe the numerical framework section 2 and then proceed by illustrating its experimental setup for two neighbouring catchments with similar climate land use and geology but different extensions located in the chinese loess plateau shejigaou 4 26 km 2 and majiagou 13 97 km 2 section 3 in section 4 we analyze the trade offs and show that the performance of the existing check dam systems could be improved through an optimization based coordinated planning effort the limitations of this study along with the main conclusions and future research avenues are presented in section 5 2 numerical framework 2 1 overview the framework presented in this study is conceived to help improve the design of check dam systems in erosion prone areas this is achieved through a two step approach illustrated in fig 1 which relies on data preparation section 2 2 and simulation optimization section 2 3 the data preparation takes as input the only two files required by the framework i e the digital elevation model dem and land cover map of the catchment and provides as output multiple raster files that characterize the stream segments as well as the sediment delivery to the stream path the former are generated through geo processing scripts section 2 2 1 while the latter is estimated with the aid of watem sedem section 2 2 2 in the second step we couple stodym section 2 3 1 with a moea section 2 3 2 for any potential solution i e location and size of the check dams stodym calculates multiple measures of performance accounting for the life expectancy storage dynamics and sediment retention capability of the check dams this information is iteratively used by the moea to identify a set of pareto efficient solutions 2 2 data preparation 2 2 1 geo processing the data preparation begins with the geo processing carried out in arcgis and python which takes as input the dem and land cover map of the catchment the geo processing produces four raster files i e stream link map flow accumulation map dem with extended boundary and a modified land cover map compatible with watem sedem the extended dem modified land cover map and stream link map are used as input to watem sedem stodym uses as input the stream link and flow accumulation maps another operation carried out in this step is a spatial analysis aimed at determining all potential locations of the check dams as illustrated in fig 2 each pixel in the stream link map is a potential dam location to reduce the number of potential sites and therefore the complexity of the optimization problem we assume that each stream segment can host only one dam with this assumption we also ensure that all dams within the system are exposed to a reasonable sediment load throughout their life cycle one might imagine that if two dams are deployed in the same stream segment the downstream dam starts collecting sediments only when the upstream one reaches the end of its life cycle for stream segments consisting of more than one pixel we choose as potential dam location the second most downstream pixel see the illustration in fig 2 the rationale behind this choice is twofold to select a site that maximizes the drainage area of a dam and to keep a minimum distance between dams located in different contiguous stream segments note that the use of a single dam per stream segment does not prevent the numerical framework from deploying staggered check dams in long channels to this purpose one just needs to divide the long channels into multiple stream links a procedure that can be executed in arcgis when creating the flow accumulation and stream link maps this point is further exemplified in section 3 1 2 2 2 watem sedem watem sedem is a spatially distributed model consisting of three modules that estimate sediment erosion sediment transport capacity and sediment delivery to the stream path van oost et al 2000 van rompaey et al 2001 verstraeten et al 2002 the inputs to watem sedem are the dem and five maps outlining the land cover stream link crop factor rainfall erosivity factor and soil erodibility factor the soil erosion in each cell is calculated with the revised universal soil loss equation renard et al 1997 while the sediment transport capacity of each cell is computed as a function of the local slope gradient and a few parameters accounting for rainfall erosivity and soil erodibility the reader is referred to van oost et al 2000 for additional details about the sediment transport capacity module the sediment delivery to the stream path is based on a flow routing algorithm verstraeten et al 2002 which compares the incoming sediment flux to the transport capacity in each morphological unit sediment deposition occurs if the transport capacity is smaller than the sediment flux otherwise the sediment is routed downstream towards the stream path watem sedem provides as output the mean annual values of sediment erosion deposition and sediment delivery to the stream path this includes detailed information on the sediment dynamics for each stream segment i e sediment input upstream sediment input and cumulative sediment output 2 3 simulation optimization 2 3 1 stodym stodym is a process based model designed to simulate the storage dynamics of check dams the model adopts an annual simulation time step and for a given check dam system provides as output the annual changes in the dams storage capacity due to sediment trapping the annual values of the catchment s sediment delivery ratio and the life expectancy of each check dam stodym requires as input i the stream link map and flow accumulation values at the potential location of the check dams contained in the flow accumulation map generated with the geo processing ii the sediment input to the stream path estimated by watem sedem iii the initial storage capacity of each dam and iv the location of each dam the location and initial storage capacity can be determined by the user or optimized by a moea as shown in our study the key difference between the version of stodym presented in pal et al 2018 and the one adopted here is that the current version automatically calculates the drainage area and sediment input of all dams within a system given their number and locations thanks to this feature stodym can be easily coupled with optimization algorithms tasked with the problem of determining the optimal location and size of check dams before running a simulation the model creates a directed network extracted from the stream link map where nodes represent check dams and arcs the connection between them i e the direction with which sediment is routed between the check dams stodym also calculates the drainage area of each check dam using the information contained in the flow accumulation map if there are no upstream check dams the drainage area corresponds to the drainage area of the stream segment in which the dam is deployed the pour point is the dam s location otherwise the dam s drainage area is obtained by calculating the difference between the drainage area of the stream segment and the one of the upstream check dams then the model estimates the sediment trapping efficiency ste of each dam and calculates the amount of trapped sediment which is equal to the product between ste and sediment input the estimation of the ste is based on brown s equation brown 1943 1 s t e 100 1 1 1 0 0021 d d s c d d a where d s c and d d a represent the storage capacity m 3 and drainage area km 2 of the check dam the parameter d depends on several characteristics of the check dam e g annual inflow texture of incoming sediment etc and can vary between 0 046 and 1 verstraeten and poesen 2000 the ste value is finally used to calculate the amount of trapped and released sediment in each dam at this step the model updates the storage capacity of each dam as well as its ste we assume that a check dam becomes inactive when its capacity is null or the ste is lower than 10 in such case stodym removes the inactive dams from the directed network and re calculates the drainage area of the active check dams the simulation stops when all dams become inactive or a pre defined simulation horizon e g 30 years is reached 2 3 2 optimization problem to determine the optimal location and size of the check dams we formulate a 4 objective optimization problem which is solved with the aid of a moea in the followings we provide a detailed description of the decision variables objectives and constraints characterizing the problem 2 3 2 1 decision variables consider a stream path containing m segments and n check dams to be deployed with 1 n m and n m n the decision vector x containing the locations of the check dams could be defined using either binary or integer variables in our formulation we opted for the second option that is x x 1 x 2 x n where each decision variable x i specifies the identification number of the stream segment in which the i th dam is deployed with 1 x i m i 1 n with this option we limit the dimensionality of x to n as compared to a formulation with binary variables which would yield a dimensionality equal to m as for the check dams size we define a second decision vector y y 1 y 2 y n where each decision variable y i specifies the size of the i th dam i 1 n we limit the size y i of the i th dam within a pre defined minimum and maximum value s m i n and s m a x hence s m i n y i s m a x i 1 n 3 objectives the objectives considered in the optimization problem account for three important features of any check dam system namely its life expectancy capability to retain sediment and storage dynamics below we provide a detailed description of all objectives whose value is calculated by stodym life expectancy j l e to be maximized intuitively one would like to deploy a system that can sustain the sediment load for a long period of time we account for this criterion by maximizing the average life expectancy l a which is calculated across the life expectancy l 1 l 2 l n of all n dams within the system initial and long term value of sdr j s d r 1 and j s d r 20 to be minimized to account for the system s capability to retain sediment we resort to the sediment delivery ratio sdr defined as the ratio between the annual amount of sediment discharged at the catchment s outlet and delivered to the stream path small values of sdr indicate that a check dam system is capable of retaining most of the sediment within the catchment when accounting for this criterion one must consider that the retention capability of a system and therefore the catchment s sdr vary with time as dams get filled with sediment the retention capability decreases while the sdr increases for this reason it is important to account for both initial and long term retention capability two objectives that we express by calculating the sdr after 1 and 20 years which are denoted by j s d r 1 and j s d r 20 respectively the choice of the first and twentieth year is not completely arbitrary as we shall see later it is based on the average life expectancy of check dams in the loess plateau section 3 2 note that these two objectives are potentially conflicting a check dam system designed to collect a large amount of sediment at the beginning of its life cycle may get filled quickly and thus not perform well in the long run and vice versa storage dynamics j s d to be minimized ideally one would like to design a system where the storage capacity of all dams decreases with a similar rate so as to limit the chances of sudden changes in the annual amount of sediment that dams must retain which can have detrimental impacts on a dam s structural stability pal et al 2018 this observation is synthesized in the objective j s d which is calculated as follows let s d i s i 1 s i 2 s i 20 be a vector containing the annual values of storage capacity for the i th dam during the first 20 years of deployment note that s i 1 y i first we normalize the observations in s d i i e n s d i 1 s i 2 s i 1 s i 20 s i 1 so that each observation varies between 0 and 1 then we calculate j s d through the following equation 2 j s d max 1 i n n s d i n s d 2 20 where 2 is the euclidean norm and n s d a vector containing the average normalized storage capacity across all check dams by minimizing j s d we thus minimize the worst deviation in terms of euclidean distance of a dam s storage dynamics from the average storage dynamics constraints in our formulation we consider the following constraints since each stream segment can contain at most one check dam see section 2 2 1 we impose that x i x j for all i j with i j 1 n we allow the modeller to control the total storage capacity i e i 1 n y i s where s is the user defined capacity for the entire system to reflect the modeller s expectation on the life cycle of each dam we constrain the life expectancy l i within a minimum of 20 and a maximum of 30 years i e 20 l i 30 i 1 n naturally these values can be modified and tailored to the hydrological characteristics of the study site as well as the desired life expectancy of the system s components 4 study area and experimental setup 4 1 study area to test the proposed framework we consider two catchments characterized by similar climate land use and geology but different extension shejiagou has an area of 4 26 km 2 while majiagou an area of 13 97 km 2 both catchments are located in the same region chabagou watershed of the chinese loess plateau the rationale behind the choice of these two catchments is to evaluate the framework s performance on catchments with different extensions and therefore different number of check dams the region in which both catchments are located has a semi arid climate scarce vegetation a mean annual rainfall of about 400 mm and an average annual sediment yield of 1 58 10 4 t km 2 zhang et al 2016 the elevation range of these catchment lies between 920 m and 1172 5 m as illustrated in fig 3 the land cover classes are bare land forest pasture and stream see table 1 according to the data reported by the loess plateau data centre lp 2013 the existing check dam systems of shejiagou and majiagou consist of 5 and 10 dams respectively see fig 3 finding an alternative and perhaps more efficient deployment of these dams is a computationally demanding problem if one considers that the catchments have 35 and 70 stream segments illustrated in fig 4 4 2 experimental setup annual sediment yield data are available for shejiagou catchment over the period 1960 1969 these data were used to calibrate watem sedem so as to accurately estimate the sediment input to the stream path using the data reported by the loess plateau data centre lp 2013 we set the rainfall erosivity factor to 0 1026 mj mm m 2 h 1 yr 1 the soil erodibility factor to 40 7 kg h mj 1 mm 1 and the crop management factor to 1 0 09 and 0 12 for bare land forest and pasture respectively as for the parameter d governing brown s equation see eq 1 we used a value equal to 0 1 which was adopted by other studies in the loess plateau e g fang 2017 further details about the data and calibration process for shejiagou catchment are reported in pal et al 2018 unfortunately sediment yield data are not available for majiagou catchment to run watem sedem we thus adopted the same parameterization that was optimized for shejiagou whilst sub optimal such parameterization is justified by the similar climate land use and geology characterizing the two catchments we also note that the aim of this study is not to provide the best model possible to predict erosion and sediment transport but to show that erosion and sediment transport models can be effectively coupled with optimization algorithms to aid the design of check dam systems to setup the optimization problem we take the existing check dam systems as baseline scenarios recall that shejiagou and majiagou catchments have 5 and 10 dams respectively and formulate two problem instances with 10 and 20 decision variables in other words the dimensionality n of the decision vectors x dams location and y dams size is equal to 5 for shejiagou and 10 for majiagou the decision variables x 1 x n can thus take value in the set 1 2 35 for shejiagou and 1 2 70 for majiagou these sets represent the stream segments of the two catchments as explained in the previous section as for the decision variables y 1 y n the search range s m i n s m a x is equal to 20 000 60 000 m 3 for shejiagou and 40 000 100 000 m 3 for majiagou these values were determined through a preliminary investigation not reported here that studied the sediment load to the existing check dam systems to parameterize the second constraint i e the total storage capacity we leveraged the information available in the 1978 check dam survey report and set the value of s to 240 000 m 3 for shejiagou catchment and 700 000 m 3 for majiagou as shown later these values are close to the remaining storage capacity reported in 1978 the minimum and maximum life expectancy of each dam third constraint was determined based on the existing information on chabagou watershed zhang et al 2010 which suggests that 50 of the dams in the region have a life expectancy of about 20 years overall the setting of the optimization problem allows us to make a qualitative comparison between the performance of the existing and optimized check dam systems the optimization problem is solved with the aid of the ε nsgaii algorithm reed et al 2013 which improves the well known nsga ii algorithm deb et al 2002 with adaptive population sizing and ε dominance archiving in addition its python implementation https github com project platypus platypus is designed to handle both discrete and continuous decision variables in our study all objectives are scaled between 0 and 1 so we set only one value of ε equal to 0 001 the other ε nsgaii parameters to setup are the size of the initial population and the number of function evaluations which are equal to 50 and 100 000 a setting that guarantees reliable results for both catchments each optimization problem is solved with 25 different random seeds so as to characterize the variability in the ε nsgaii stochastic search process the final set of pareto efficient solutions thus corresponds to the set of pareto efficient solutions identified across all 25 seeds all experiments are carried out on a dual intel xeon cpu e5 2630 v3 2 40 ghz with 32 gb ram running microsoft windows 8 1 the average runtime across the 25 seeds is equal to 52 min for shejiagou and 120 min for majiagou 5 results to illustrate the results provided by the framework we first present the pareto efficient solutions identified for shejiagou and majiagou catchments and then proceed by comparing the performance of two selected solutions against the one attained by the existing check dam systems 5 1 trade space exploration figs 5 and 6 illustrate the pareto frontiers identified by the simulation optimization framework for shejiagou and majiagou catchments we found 1826 alternative deployment scenarios for shejiagou and 2772 for majiagou this discrepancy in the number of solutions is due to the larger number of decision variables and stream segments characterizing majiagou catchment as explained in section 2 3 2 the goal of the optimization process is to maximize the average life expectancy j l e and minimize the value of the objectives associated with the sediment retention capability j s d r 1 j s d r 20 and storage dynamics j s d in figs 5 and 6 these objectives are represented by the horizontal and vertical axes j l e and j s d and the colour j s d r 1 and size j s d r 20 of each circle so the ideal solution or utopia point is a blue small circle with coordinates 1 0 a graphical analysis of fig 5 reveals a trade off between all objective functions first there is a strong trade off between the average life expectancy j l e and both initial and long term sdr j s d r 1 j s d r 20 as shown in fig 5 main panel check dam systems with poor sediment retention capacity yellow and red large circles are associated to large values of average life expectancy vice versa systems with high retention capacity blue and cyan small circles have an average life expectancy 30 smaller than the largest one these trade offs are also illustrated in panels a and b as one might expect check dam systems loaded with a limited amount of sediment are more durable than those designed to collect most of the sediment delivered to the stream network and thereby delivering a limited amount of sediment to the catchment s outlet second results show that it is not possible to deploy check dam systems that simultaneously minimize both initial and long term sdr j s d r 1 j s d r 20 indeed systems trapping most of the sediment at the beginning of their life cycle with values of j s d r 1 close to 0 tend to get filled quickly thus performing poorly after 20 years values of j s d r 20 in the range 0 30 0 60 this trade off is depicted by the colour and size of the circles in the main panel of fig 5 deep blue circles tend to have larger size and further illustrated in panel c an interesting observation is the fact that there are trade off solutions yielding reasonable values of both objectives for example it is possible to design systems with reasonable initial and long term retention capacity j s d r 1 and j s d r 20 lower than 0 10 and 0 30 respectively third there is a conflict between the objective associated with the storage dynamics j s d and both initial and long term sdr j s d r 1 j s d r 20 see fig 5 panels d and e this means that if one wants to deploy a system in which the storage capacity of all dams decreases with a similar rate small values of j s d must be ready to comprise at least to some extent the system s retention capacity a possible explanation for this trade off may be found by recalling that the exact goal of j s d is to minimize the worst deviation of a dam s storage capacity from the average storage dynamics our results seem to suggest that if a check dam system traps a large amount of sediment in both short and long run there are higher chances of observing different patterns of storage dynamics across the dams the storage dynamics is also in weak conflict with the average life expectancy j l e see fig 5 main panel and panel f meaning that the most durable check dam systems values of j l e close to 1 present a slight decrease in the storage dynamics objective values of j s d close to 0 10 interestingly the results show the same trade offs between the objective functions for the second case study majiagou catchment fig 6 in particular note the strong conflict between the average life expectancy j l e and both initial and long term sdr j s d r 1 j s d r 20 check dam systems with long life expectancy j l e close to 1 have poor retention capacity after 1 and 20 years of deployment j s d r 1 and j s d r 20 are close to 0 30 overall results for majiagou catchment empirically confirm that the objectives described in section 2 3 2 help capture three important aspects of check dam system s performance i e life expectancy capability to retain sediment and storage dynamics as well as the conflict between them 5 2 analysis of two representative solutions to further understand the behaviour of the optimized check dam systems we proceed by comparing the performance of two selected solutions one per catchment against the performance attained by the existing systems whose performance is calculated via simulation with watem sedem and stodym under the assumption that the total storage capacity of the dams is equal to the remaining storage capacity reported in 1978 that is 241 400 m 3 for shejiagou and 830 700 m 3 for majiagou see table 2 to select the two solutions from the pareto efficient sets we adopted a two step procedure for each catchment we first identified the subset of check dam systems whose initial total drainage area is comparable 5 to the one of the existing system this is to ensure that the selected and existing system have a comparable sediment load at the beginning of their life cycle from this subset we then selected the solution nearest to the utopia point in terms of euclidean distance with this approach rather common in field of multi objective analysis soncini sessa et al 2007 we thus select a solution that strikes a balance between the four objectives figs 5 and 6 main panels naturally such comparison must be taken cautiously the existing check dam systems are not the product of a coordinated planning effort meaning that check dams are not necessarily deployed while accounting for both individual and system wide retention capacity in addition the objective functions used in our framework may not fully capture the design criteria considered by planners and catchment authorities in the loess plateau this said we believe that a qualitative comparison can provide some initial indications on the benefits provided by the adoption of process based models and multi objective evolutionary computation in the planning of check dam systems the comparison for shejiagou catchment shows an important difference in performance between the optimized and existing systems as shown in table 2 both systems have similar initial total storage capacity 240 000 and 241 400 m 3 and total drainage area 88 8 of the catchment is drained by the check dams but the optimized system has a better value of j l e which corresponds to an average life expectancy of 24 2 years in comparison the average life expectancy of the dams in the existing system is 15 4 years table 2 also shows that the value of j s d r 1 initial retention capacity is the same for both optimized and existing system but the optimized one largely outperforms the existing one in the long run see the value of j s d r 20 a better understanding of the systems behaviour underpinning these values of the objective functions is offered by fig 7 which illustrates the location and annual storage dynamics of the check dams left and central panels along with the annual variation of the sdr right panels in the case of the optimized system upper panels the sdr increases slowly during the first 20 years and then more suddenly during the last 10 when all dams get completely filled on the other hand the sdr of the existing system bottom panels starts increasing quickly after 15 years a point in time in which only the most downstream dam remains active fig 7 also shows that the storage of all dams in the optimized system decreases with a similar rate for the entire duration of the system s life cycle leading to a small value of j s d in contrast the storage capacity of the dams in the existing system varies with different rates and presents abrupt changes at the time some dams become inactive note that it is not possible to calculate the value of j s d for the existing system since four dams have a life cycle shorter than 20 years recall that the value of j s d is a function of the annual values of storage capacity of all dams during the first 20 years of deployment 2 initial total drainage area similar observations can be made for the comparison between optimized and existing systems in majiagou catchment both systems drain the majority of the catchment area the initial total drainage area is 97 6 and have a comparable initial retention capacity j s d r 1 is equal to 0 03 and 0 01 respectively see table 2 however the optimized system achieves better average life expectancy j l e equal to 0 82 corresponding to 24 5 years and long term retention capacity j s d r 20 equal to 0 14 this improved performance may be explained by comparing the storage capacity dynamics of both systems as shown in fig 8 several dams in the existing system have a short life cycle so the majority of the sediment is captured by the few dams with life cycle longer than 20 years note in particular the role of the most downstream dam deployed in the stream segment with id 49 this dam has a very large initial storage capacity 550 000 m 3 which allows it to collect sediment when all other dams become inactive this setting contrasts with the one adopted by the numerical framework which found the location and size that guarantee a more consistent performance for all dams within the system 6 conclusions this work contributes to the existing literature on soil erosion control by addressing the challenge of understanding how check dam systems should be tailored to the hydrological characteristics of a given catchment in particular we considered the problem of determining the optimal location and size of check dams and proposed a numerical framework that combines state of the art process based models i e watem sedem and stodym with multi objective evolutionary computation its application to two catchments in the chinese loess plateau revealed a strong conflict between the criteria considered to quantify the performance of a check dam system namely its life expectancy capability to retain sediment and storage dynamics among the various trade offs the conflicts between 1 life expectancy and retention capacity and 2 initial and long term retention capacity have important design implications since they suggest that given a limited storage capacity it is not possible to deploy a durable system that is capable of collecting most of the sediment for a prolonged period of time this said our results also show that there are several solutions that can strike a reasonable balance between all objectives when comparing some of these solutions against the existing check dam systems we found that the proposed numerical framework may indeed be beneficial coordinating and optimizing the deployment of check dams help improve some important objectives such as the average life expectancy or the long term retention capacity naturally the framework proposed here has a few limitations which should be properly accounted for and where possible overcome first our approach must rely on well calibrated process based models whose accuracy can largely influence the result of the design process in this respect it is paramount to improve the quality of the data capturing both temporal and spatial aspects of the in dam sedimentation process xu et al 2013 perhaps this challenge could be tackled by relying more on high resolution topographical maps and unmanned aerial vehicles imagery which have been recently shown to have an added value in the model calibration lü et al 2019 second our framework assumes that all potential dam sites are equal thereby neglecting the local topographic constrictions looking forward the framework could be modified to include an objective function expressing the costs of constructing a dam in a particular site such issue has been successfully tackled by petheram et al 2017 for the problem of identifying dam wall locations for water supply reservoirs third the application of the framework to large catchments e g 100 km 2 would increase the computational requirements and thereby limits its applicability this issue could be tackled by resorting to surrogate or emulation modelling castelletti et al 2012 yazdi and neyshabouri 2014 a technique that replaces computationally expensive models with lower complexity yet accurate surrogates finally we note that dams filled with sediment still function as stabilization structures that reduce the slope of the channel profile and slow down the stream velocity an important feature that our framework does not account for in fact this is a process that the existing process based models such as watem sedem cannot account for a possible modelling alternative might stand in the adoption of more sophisticated physics based models such as the integrated hydrology model ran et al 2012 overall the encouraging results shown here suggest that the there are pathways for harnessing the knowledge on sedimentation processes and availability of various process based models to aid the design of measures for soil erosion control acknowledgements this study was supported by the sutd zju research collaboration grant sutd zju res 01 2015 the authors also acknowledge the technical reports 1978 1993 and 2001 check dam survey and data support from the loess plateau data centre national earth system science data sharing infrastructure national science and technology infrastructure of china http loess geodata cn finally the authors would like to thank honglei tang and qihua ran for their help in processing and analyzing the data used in this research appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 007 
26168,check dams are one of the most popular measures for soil erosion control despite the availability of various process based modelling tools their design is often carried out in an uncoordinated manner potentially leading to check dam systems with limited life expectancy or unreliable sediment retention capacity in this study we consider the problem of determining the optimal location and size i e initial storage capacity of a given number of check dams and tackle this problem by contributing a numerical framework that builds on geo processing two process based models watem sedem and stodym and multi objective evolutionary computation the application of our framework to two catchments in the chinese loess plateau characterized by similar erosion processes but different extensions 4 26 and 13 97 km 2 reveals a strong trade off between three criteria of system s performance namely life expectancy sediment retention capacity and storage dynamics results also show that there are opportunities for improving the performance of existing check dam systems through a coordinated optimization based planning exercise keywords soil erosion check dams optimization multi objective evolutionary algorithms watem sedem loess plateau software availability name of software stodym version 1 0 developers d pal software required python arcgis watem sedem contact email debasish pal sutd edu sg year first available 2019 available from development version available on github https github com debpal stodym 1 introduction revegetation and check dam construction are among the most popular measures available for soil erosion control revegetation or planting acts by mitigating the effect of hydro meteorological forcing on soil erosion thereby preventing the formation of gullies and reducing sediment load to the stream network xiangzhou et al 2004 on the other hand a check dam acts as a sink for sediment this hydraulic infrastructure reduces the water flow velocity and helps retain sediment in the upstream pond and drainage canal tang et al 2018 in the long run the flat area resulting from the sedimentation process can be used as cropland whilst different studies have shown that these two measures work best when combined verstraeten and prosser 2008 zhao et al 2017a fang 2017 shi et al 2019 revegetation and check dam construction are typically planned independently with check dams being often privileged owing to their limited investment and operating costs check dams are indeed adopted in several countries such as spain castillo et al 2007 italy bombino et al 2009 iran hassanli et al 2009 or china xiangzhou et al 2002 in the chinese loess plateau alone for instance there is an estimated number of at least 100 000 check dams jin et al 2012 to understand the morphological hydraulic sedimentary and ecological effects of check dams researchers tend to adopt two complementary approaches namely field observations and numerical simulations see for example the recent work of shi et al 2019 and lü et al 2019 who investigated the effect of check dams on the sediment budget of two hilly catchments in china field observations provide key information on sedimentation in ponds and drainage canals while numerical simulations are used to evaluate the response of a catchment e g the sediment yield to different land use and check dam deployment scenarios these modelling efforts typically rely on process based models such as swat xu et al 2013 li et al 2017 shi et al 2019 sedd zhao et al 2017a or watem sedem zhao et al 2015 fang 2017 lü et al 2019 all adopting modifications of the universal soil loess equation renard et al 1997 merritt et al 2003 to estimate soil erosion rates despite the presence of these studies focussing on both field work and numerical simulation there seems to be a lack of knowledge on how check dams should be designed and tailored to the hydrological characteristics of a given area this is a planning problem that challenges the existing simulation based paradigm in two ways first check dams are generally implemented as a system of dams across a catchment so a coordinated implementation effort could hardly rely on a few simulations runs or what if scenarios which would provide a narrow representation of the available design alternatives along with their corresponding performance second the aforementioned models cannot simulate how the storage capacity of check dams varies with time zhao et al 2017b a key factor controlling their life expectancy and capability to retain sediment to address these challenges our work proposes a numerical framework for the optimal design of check dam systems in particular we consider the problem of determining the optimal location and size i e initial storage capacity of a given number of check dams in an erosion prone area the framework builds on two process based models and multi objective evolutionary computation the models are watem sedem van oost et al 2000 which estimates sediment erosion and delivery to the stream network and stodym storage dynamics model an improved version of the model recently introduced by pal et al 2018 to simulate the sediment trapping efficiency and storage dynamics of check dams stodym is then coupled with a multi objective evolutionary algorithm moea maier et al 2014 which determines the location and size of the check dams that optimize the life expectancy capability to retain sediment and storage dynamics of the whole system the latter criterion ensures that the storage capacity of all dams decreases with comparable rates thereby reducing the chances of abrupt changes in the sediment load we show that the proposed framework has three desirable features first it requires minimal user inputs provided that the two process based models i e watem sedem and stodym are available for the study site at hand second it allows studying the trade offs between the different measures of performance characterizing a check dam system third the computational requirements are in the order of a few hours for catchments with an extension of 5 15 km 2 thereby allowing the user to experiment with different settings in a reasonable amount of time in the remainder of the manuscript we first describe the numerical framework section 2 and then proceed by illustrating its experimental setup for two neighbouring catchments with similar climate land use and geology but different extensions located in the chinese loess plateau shejigaou 4 26 km 2 and majiagou 13 97 km 2 section 3 in section 4 we analyze the trade offs and show that the performance of the existing check dam systems could be improved through an optimization based coordinated planning effort the limitations of this study along with the main conclusions and future research avenues are presented in section 5 2 numerical framework 2 1 overview the framework presented in this study is conceived to help improve the design of check dam systems in erosion prone areas this is achieved through a two step approach illustrated in fig 1 which relies on data preparation section 2 2 and simulation optimization section 2 3 the data preparation takes as input the only two files required by the framework i e the digital elevation model dem and land cover map of the catchment and provides as output multiple raster files that characterize the stream segments as well as the sediment delivery to the stream path the former are generated through geo processing scripts section 2 2 1 while the latter is estimated with the aid of watem sedem section 2 2 2 in the second step we couple stodym section 2 3 1 with a moea section 2 3 2 for any potential solution i e location and size of the check dams stodym calculates multiple measures of performance accounting for the life expectancy storage dynamics and sediment retention capability of the check dams this information is iteratively used by the moea to identify a set of pareto efficient solutions 2 2 data preparation 2 2 1 geo processing the data preparation begins with the geo processing carried out in arcgis and python which takes as input the dem and land cover map of the catchment the geo processing produces four raster files i e stream link map flow accumulation map dem with extended boundary and a modified land cover map compatible with watem sedem the extended dem modified land cover map and stream link map are used as input to watem sedem stodym uses as input the stream link and flow accumulation maps another operation carried out in this step is a spatial analysis aimed at determining all potential locations of the check dams as illustrated in fig 2 each pixel in the stream link map is a potential dam location to reduce the number of potential sites and therefore the complexity of the optimization problem we assume that each stream segment can host only one dam with this assumption we also ensure that all dams within the system are exposed to a reasonable sediment load throughout their life cycle one might imagine that if two dams are deployed in the same stream segment the downstream dam starts collecting sediments only when the upstream one reaches the end of its life cycle for stream segments consisting of more than one pixel we choose as potential dam location the second most downstream pixel see the illustration in fig 2 the rationale behind this choice is twofold to select a site that maximizes the drainage area of a dam and to keep a minimum distance between dams located in different contiguous stream segments note that the use of a single dam per stream segment does not prevent the numerical framework from deploying staggered check dams in long channels to this purpose one just needs to divide the long channels into multiple stream links a procedure that can be executed in arcgis when creating the flow accumulation and stream link maps this point is further exemplified in section 3 1 2 2 2 watem sedem watem sedem is a spatially distributed model consisting of three modules that estimate sediment erosion sediment transport capacity and sediment delivery to the stream path van oost et al 2000 van rompaey et al 2001 verstraeten et al 2002 the inputs to watem sedem are the dem and five maps outlining the land cover stream link crop factor rainfall erosivity factor and soil erodibility factor the soil erosion in each cell is calculated with the revised universal soil loss equation renard et al 1997 while the sediment transport capacity of each cell is computed as a function of the local slope gradient and a few parameters accounting for rainfall erosivity and soil erodibility the reader is referred to van oost et al 2000 for additional details about the sediment transport capacity module the sediment delivery to the stream path is based on a flow routing algorithm verstraeten et al 2002 which compares the incoming sediment flux to the transport capacity in each morphological unit sediment deposition occurs if the transport capacity is smaller than the sediment flux otherwise the sediment is routed downstream towards the stream path watem sedem provides as output the mean annual values of sediment erosion deposition and sediment delivery to the stream path this includes detailed information on the sediment dynamics for each stream segment i e sediment input upstream sediment input and cumulative sediment output 2 3 simulation optimization 2 3 1 stodym stodym is a process based model designed to simulate the storage dynamics of check dams the model adopts an annual simulation time step and for a given check dam system provides as output the annual changes in the dams storage capacity due to sediment trapping the annual values of the catchment s sediment delivery ratio and the life expectancy of each check dam stodym requires as input i the stream link map and flow accumulation values at the potential location of the check dams contained in the flow accumulation map generated with the geo processing ii the sediment input to the stream path estimated by watem sedem iii the initial storage capacity of each dam and iv the location of each dam the location and initial storage capacity can be determined by the user or optimized by a moea as shown in our study the key difference between the version of stodym presented in pal et al 2018 and the one adopted here is that the current version automatically calculates the drainage area and sediment input of all dams within a system given their number and locations thanks to this feature stodym can be easily coupled with optimization algorithms tasked with the problem of determining the optimal location and size of check dams before running a simulation the model creates a directed network extracted from the stream link map where nodes represent check dams and arcs the connection between them i e the direction with which sediment is routed between the check dams stodym also calculates the drainage area of each check dam using the information contained in the flow accumulation map if there are no upstream check dams the drainage area corresponds to the drainage area of the stream segment in which the dam is deployed the pour point is the dam s location otherwise the dam s drainage area is obtained by calculating the difference between the drainage area of the stream segment and the one of the upstream check dams then the model estimates the sediment trapping efficiency ste of each dam and calculates the amount of trapped sediment which is equal to the product between ste and sediment input the estimation of the ste is based on brown s equation brown 1943 1 s t e 100 1 1 1 0 0021 d d s c d d a where d s c and d d a represent the storage capacity m 3 and drainage area km 2 of the check dam the parameter d depends on several characteristics of the check dam e g annual inflow texture of incoming sediment etc and can vary between 0 046 and 1 verstraeten and poesen 2000 the ste value is finally used to calculate the amount of trapped and released sediment in each dam at this step the model updates the storage capacity of each dam as well as its ste we assume that a check dam becomes inactive when its capacity is null or the ste is lower than 10 in such case stodym removes the inactive dams from the directed network and re calculates the drainage area of the active check dams the simulation stops when all dams become inactive or a pre defined simulation horizon e g 30 years is reached 2 3 2 optimization problem to determine the optimal location and size of the check dams we formulate a 4 objective optimization problem which is solved with the aid of a moea in the followings we provide a detailed description of the decision variables objectives and constraints characterizing the problem 2 3 2 1 decision variables consider a stream path containing m segments and n check dams to be deployed with 1 n m and n m n the decision vector x containing the locations of the check dams could be defined using either binary or integer variables in our formulation we opted for the second option that is x x 1 x 2 x n where each decision variable x i specifies the identification number of the stream segment in which the i th dam is deployed with 1 x i m i 1 n with this option we limit the dimensionality of x to n as compared to a formulation with binary variables which would yield a dimensionality equal to m as for the check dams size we define a second decision vector y y 1 y 2 y n where each decision variable y i specifies the size of the i th dam i 1 n we limit the size y i of the i th dam within a pre defined minimum and maximum value s m i n and s m a x hence s m i n y i s m a x i 1 n 3 objectives the objectives considered in the optimization problem account for three important features of any check dam system namely its life expectancy capability to retain sediment and storage dynamics below we provide a detailed description of all objectives whose value is calculated by stodym life expectancy j l e to be maximized intuitively one would like to deploy a system that can sustain the sediment load for a long period of time we account for this criterion by maximizing the average life expectancy l a which is calculated across the life expectancy l 1 l 2 l n of all n dams within the system initial and long term value of sdr j s d r 1 and j s d r 20 to be minimized to account for the system s capability to retain sediment we resort to the sediment delivery ratio sdr defined as the ratio between the annual amount of sediment discharged at the catchment s outlet and delivered to the stream path small values of sdr indicate that a check dam system is capable of retaining most of the sediment within the catchment when accounting for this criterion one must consider that the retention capability of a system and therefore the catchment s sdr vary with time as dams get filled with sediment the retention capability decreases while the sdr increases for this reason it is important to account for both initial and long term retention capability two objectives that we express by calculating the sdr after 1 and 20 years which are denoted by j s d r 1 and j s d r 20 respectively the choice of the first and twentieth year is not completely arbitrary as we shall see later it is based on the average life expectancy of check dams in the loess plateau section 3 2 note that these two objectives are potentially conflicting a check dam system designed to collect a large amount of sediment at the beginning of its life cycle may get filled quickly and thus not perform well in the long run and vice versa storage dynamics j s d to be minimized ideally one would like to design a system where the storage capacity of all dams decreases with a similar rate so as to limit the chances of sudden changes in the annual amount of sediment that dams must retain which can have detrimental impacts on a dam s structural stability pal et al 2018 this observation is synthesized in the objective j s d which is calculated as follows let s d i s i 1 s i 2 s i 20 be a vector containing the annual values of storage capacity for the i th dam during the first 20 years of deployment note that s i 1 y i first we normalize the observations in s d i i e n s d i 1 s i 2 s i 1 s i 20 s i 1 so that each observation varies between 0 and 1 then we calculate j s d through the following equation 2 j s d max 1 i n n s d i n s d 2 20 where 2 is the euclidean norm and n s d a vector containing the average normalized storage capacity across all check dams by minimizing j s d we thus minimize the worst deviation in terms of euclidean distance of a dam s storage dynamics from the average storage dynamics constraints in our formulation we consider the following constraints since each stream segment can contain at most one check dam see section 2 2 1 we impose that x i x j for all i j with i j 1 n we allow the modeller to control the total storage capacity i e i 1 n y i s where s is the user defined capacity for the entire system to reflect the modeller s expectation on the life cycle of each dam we constrain the life expectancy l i within a minimum of 20 and a maximum of 30 years i e 20 l i 30 i 1 n naturally these values can be modified and tailored to the hydrological characteristics of the study site as well as the desired life expectancy of the system s components 4 study area and experimental setup 4 1 study area to test the proposed framework we consider two catchments characterized by similar climate land use and geology but different extension shejiagou has an area of 4 26 km 2 while majiagou an area of 13 97 km 2 both catchments are located in the same region chabagou watershed of the chinese loess plateau the rationale behind the choice of these two catchments is to evaluate the framework s performance on catchments with different extensions and therefore different number of check dams the region in which both catchments are located has a semi arid climate scarce vegetation a mean annual rainfall of about 400 mm and an average annual sediment yield of 1 58 10 4 t km 2 zhang et al 2016 the elevation range of these catchment lies between 920 m and 1172 5 m as illustrated in fig 3 the land cover classes are bare land forest pasture and stream see table 1 according to the data reported by the loess plateau data centre lp 2013 the existing check dam systems of shejiagou and majiagou consist of 5 and 10 dams respectively see fig 3 finding an alternative and perhaps more efficient deployment of these dams is a computationally demanding problem if one considers that the catchments have 35 and 70 stream segments illustrated in fig 4 4 2 experimental setup annual sediment yield data are available for shejiagou catchment over the period 1960 1969 these data were used to calibrate watem sedem so as to accurately estimate the sediment input to the stream path using the data reported by the loess plateau data centre lp 2013 we set the rainfall erosivity factor to 0 1026 mj mm m 2 h 1 yr 1 the soil erodibility factor to 40 7 kg h mj 1 mm 1 and the crop management factor to 1 0 09 and 0 12 for bare land forest and pasture respectively as for the parameter d governing brown s equation see eq 1 we used a value equal to 0 1 which was adopted by other studies in the loess plateau e g fang 2017 further details about the data and calibration process for shejiagou catchment are reported in pal et al 2018 unfortunately sediment yield data are not available for majiagou catchment to run watem sedem we thus adopted the same parameterization that was optimized for shejiagou whilst sub optimal such parameterization is justified by the similar climate land use and geology characterizing the two catchments we also note that the aim of this study is not to provide the best model possible to predict erosion and sediment transport but to show that erosion and sediment transport models can be effectively coupled with optimization algorithms to aid the design of check dam systems to setup the optimization problem we take the existing check dam systems as baseline scenarios recall that shejiagou and majiagou catchments have 5 and 10 dams respectively and formulate two problem instances with 10 and 20 decision variables in other words the dimensionality n of the decision vectors x dams location and y dams size is equal to 5 for shejiagou and 10 for majiagou the decision variables x 1 x n can thus take value in the set 1 2 35 for shejiagou and 1 2 70 for majiagou these sets represent the stream segments of the two catchments as explained in the previous section as for the decision variables y 1 y n the search range s m i n s m a x is equal to 20 000 60 000 m 3 for shejiagou and 40 000 100 000 m 3 for majiagou these values were determined through a preliminary investigation not reported here that studied the sediment load to the existing check dam systems to parameterize the second constraint i e the total storage capacity we leveraged the information available in the 1978 check dam survey report and set the value of s to 240 000 m 3 for shejiagou catchment and 700 000 m 3 for majiagou as shown later these values are close to the remaining storage capacity reported in 1978 the minimum and maximum life expectancy of each dam third constraint was determined based on the existing information on chabagou watershed zhang et al 2010 which suggests that 50 of the dams in the region have a life expectancy of about 20 years overall the setting of the optimization problem allows us to make a qualitative comparison between the performance of the existing and optimized check dam systems the optimization problem is solved with the aid of the ε nsgaii algorithm reed et al 2013 which improves the well known nsga ii algorithm deb et al 2002 with adaptive population sizing and ε dominance archiving in addition its python implementation https github com project platypus platypus is designed to handle both discrete and continuous decision variables in our study all objectives are scaled between 0 and 1 so we set only one value of ε equal to 0 001 the other ε nsgaii parameters to setup are the size of the initial population and the number of function evaluations which are equal to 50 and 100 000 a setting that guarantees reliable results for both catchments each optimization problem is solved with 25 different random seeds so as to characterize the variability in the ε nsgaii stochastic search process the final set of pareto efficient solutions thus corresponds to the set of pareto efficient solutions identified across all 25 seeds all experiments are carried out on a dual intel xeon cpu e5 2630 v3 2 40 ghz with 32 gb ram running microsoft windows 8 1 the average runtime across the 25 seeds is equal to 52 min for shejiagou and 120 min for majiagou 5 results to illustrate the results provided by the framework we first present the pareto efficient solutions identified for shejiagou and majiagou catchments and then proceed by comparing the performance of two selected solutions against the one attained by the existing check dam systems 5 1 trade space exploration figs 5 and 6 illustrate the pareto frontiers identified by the simulation optimization framework for shejiagou and majiagou catchments we found 1826 alternative deployment scenarios for shejiagou and 2772 for majiagou this discrepancy in the number of solutions is due to the larger number of decision variables and stream segments characterizing majiagou catchment as explained in section 2 3 2 the goal of the optimization process is to maximize the average life expectancy j l e and minimize the value of the objectives associated with the sediment retention capability j s d r 1 j s d r 20 and storage dynamics j s d in figs 5 and 6 these objectives are represented by the horizontal and vertical axes j l e and j s d and the colour j s d r 1 and size j s d r 20 of each circle so the ideal solution or utopia point is a blue small circle with coordinates 1 0 a graphical analysis of fig 5 reveals a trade off between all objective functions first there is a strong trade off between the average life expectancy j l e and both initial and long term sdr j s d r 1 j s d r 20 as shown in fig 5 main panel check dam systems with poor sediment retention capacity yellow and red large circles are associated to large values of average life expectancy vice versa systems with high retention capacity blue and cyan small circles have an average life expectancy 30 smaller than the largest one these trade offs are also illustrated in panels a and b as one might expect check dam systems loaded with a limited amount of sediment are more durable than those designed to collect most of the sediment delivered to the stream network and thereby delivering a limited amount of sediment to the catchment s outlet second results show that it is not possible to deploy check dam systems that simultaneously minimize both initial and long term sdr j s d r 1 j s d r 20 indeed systems trapping most of the sediment at the beginning of their life cycle with values of j s d r 1 close to 0 tend to get filled quickly thus performing poorly after 20 years values of j s d r 20 in the range 0 30 0 60 this trade off is depicted by the colour and size of the circles in the main panel of fig 5 deep blue circles tend to have larger size and further illustrated in panel c an interesting observation is the fact that there are trade off solutions yielding reasonable values of both objectives for example it is possible to design systems with reasonable initial and long term retention capacity j s d r 1 and j s d r 20 lower than 0 10 and 0 30 respectively third there is a conflict between the objective associated with the storage dynamics j s d and both initial and long term sdr j s d r 1 j s d r 20 see fig 5 panels d and e this means that if one wants to deploy a system in which the storage capacity of all dams decreases with a similar rate small values of j s d must be ready to comprise at least to some extent the system s retention capacity a possible explanation for this trade off may be found by recalling that the exact goal of j s d is to minimize the worst deviation of a dam s storage capacity from the average storage dynamics our results seem to suggest that if a check dam system traps a large amount of sediment in both short and long run there are higher chances of observing different patterns of storage dynamics across the dams the storage dynamics is also in weak conflict with the average life expectancy j l e see fig 5 main panel and panel f meaning that the most durable check dam systems values of j l e close to 1 present a slight decrease in the storage dynamics objective values of j s d close to 0 10 interestingly the results show the same trade offs between the objective functions for the second case study majiagou catchment fig 6 in particular note the strong conflict between the average life expectancy j l e and both initial and long term sdr j s d r 1 j s d r 20 check dam systems with long life expectancy j l e close to 1 have poor retention capacity after 1 and 20 years of deployment j s d r 1 and j s d r 20 are close to 0 30 overall results for majiagou catchment empirically confirm that the objectives described in section 2 3 2 help capture three important aspects of check dam system s performance i e life expectancy capability to retain sediment and storage dynamics as well as the conflict between them 5 2 analysis of two representative solutions to further understand the behaviour of the optimized check dam systems we proceed by comparing the performance of two selected solutions one per catchment against the performance attained by the existing systems whose performance is calculated via simulation with watem sedem and stodym under the assumption that the total storage capacity of the dams is equal to the remaining storage capacity reported in 1978 that is 241 400 m 3 for shejiagou and 830 700 m 3 for majiagou see table 2 to select the two solutions from the pareto efficient sets we adopted a two step procedure for each catchment we first identified the subset of check dam systems whose initial total drainage area is comparable 5 to the one of the existing system this is to ensure that the selected and existing system have a comparable sediment load at the beginning of their life cycle from this subset we then selected the solution nearest to the utopia point in terms of euclidean distance with this approach rather common in field of multi objective analysis soncini sessa et al 2007 we thus select a solution that strikes a balance between the four objectives figs 5 and 6 main panels naturally such comparison must be taken cautiously the existing check dam systems are not the product of a coordinated planning effort meaning that check dams are not necessarily deployed while accounting for both individual and system wide retention capacity in addition the objective functions used in our framework may not fully capture the design criteria considered by planners and catchment authorities in the loess plateau this said we believe that a qualitative comparison can provide some initial indications on the benefits provided by the adoption of process based models and multi objective evolutionary computation in the planning of check dam systems the comparison for shejiagou catchment shows an important difference in performance between the optimized and existing systems as shown in table 2 both systems have similar initial total storage capacity 240 000 and 241 400 m 3 and total drainage area 88 8 of the catchment is drained by the check dams but the optimized system has a better value of j l e which corresponds to an average life expectancy of 24 2 years in comparison the average life expectancy of the dams in the existing system is 15 4 years table 2 also shows that the value of j s d r 1 initial retention capacity is the same for both optimized and existing system but the optimized one largely outperforms the existing one in the long run see the value of j s d r 20 a better understanding of the systems behaviour underpinning these values of the objective functions is offered by fig 7 which illustrates the location and annual storage dynamics of the check dams left and central panels along with the annual variation of the sdr right panels in the case of the optimized system upper panels the sdr increases slowly during the first 20 years and then more suddenly during the last 10 when all dams get completely filled on the other hand the sdr of the existing system bottom panels starts increasing quickly after 15 years a point in time in which only the most downstream dam remains active fig 7 also shows that the storage of all dams in the optimized system decreases with a similar rate for the entire duration of the system s life cycle leading to a small value of j s d in contrast the storage capacity of the dams in the existing system varies with different rates and presents abrupt changes at the time some dams become inactive note that it is not possible to calculate the value of j s d for the existing system since four dams have a life cycle shorter than 20 years recall that the value of j s d is a function of the annual values of storage capacity of all dams during the first 20 years of deployment 2 initial total drainage area similar observations can be made for the comparison between optimized and existing systems in majiagou catchment both systems drain the majority of the catchment area the initial total drainage area is 97 6 and have a comparable initial retention capacity j s d r 1 is equal to 0 03 and 0 01 respectively see table 2 however the optimized system achieves better average life expectancy j l e equal to 0 82 corresponding to 24 5 years and long term retention capacity j s d r 20 equal to 0 14 this improved performance may be explained by comparing the storage capacity dynamics of both systems as shown in fig 8 several dams in the existing system have a short life cycle so the majority of the sediment is captured by the few dams with life cycle longer than 20 years note in particular the role of the most downstream dam deployed in the stream segment with id 49 this dam has a very large initial storage capacity 550 000 m 3 which allows it to collect sediment when all other dams become inactive this setting contrasts with the one adopted by the numerical framework which found the location and size that guarantee a more consistent performance for all dams within the system 6 conclusions this work contributes to the existing literature on soil erosion control by addressing the challenge of understanding how check dam systems should be tailored to the hydrological characteristics of a given catchment in particular we considered the problem of determining the optimal location and size of check dams and proposed a numerical framework that combines state of the art process based models i e watem sedem and stodym with multi objective evolutionary computation its application to two catchments in the chinese loess plateau revealed a strong conflict between the criteria considered to quantify the performance of a check dam system namely its life expectancy capability to retain sediment and storage dynamics among the various trade offs the conflicts between 1 life expectancy and retention capacity and 2 initial and long term retention capacity have important design implications since they suggest that given a limited storage capacity it is not possible to deploy a durable system that is capable of collecting most of the sediment for a prolonged period of time this said our results also show that there are several solutions that can strike a reasonable balance between all objectives when comparing some of these solutions against the existing check dam systems we found that the proposed numerical framework may indeed be beneficial coordinating and optimizing the deployment of check dams help improve some important objectives such as the average life expectancy or the long term retention capacity naturally the framework proposed here has a few limitations which should be properly accounted for and where possible overcome first our approach must rely on well calibrated process based models whose accuracy can largely influence the result of the design process in this respect it is paramount to improve the quality of the data capturing both temporal and spatial aspects of the in dam sedimentation process xu et al 2013 perhaps this challenge could be tackled by relying more on high resolution topographical maps and unmanned aerial vehicles imagery which have been recently shown to have an added value in the model calibration lü et al 2019 second our framework assumes that all potential dam sites are equal thereby neglecting the local topographic constrictions looking forward the framework could be modified to include an objective function expressing the costs of constructing a dam in a particular site such issue has been successfully tackled by petheram et al 2017 for the problem of identifying dam wall locations for water supply reservoirs third the application of the framework to large catchments e g 100 km 2 would increase the computational requirements and thereby limits its applicability this issue could be tackled by resorting to surrogate or emulation modelling castelletti et al 2012 yazdi and neyshabouri 2014 a technique that replaces computationally expensive models with lower complexity yet accurate surrogates finally we note that dams filled with sediment still function as stabilization structures that reduce the slope of the channel profile and slow down the stream velocity an important feature that our framework does not account for in fact this is a process that the existing process based models such as watem sedem cannot account for a possible modelling alternative might stand in the adoption of more sophisticated physics based models such as the integrated hydrology model ran et al 2012 overall the encouraging results shown here suggest that the there are pathways for harnessing the knowledge on sedimentation processes and availability of various process based models to aid the design of measures for soil erosion control acknowledgements this study was supported by the sutd zju research collaboration grant sutd zju res 01 2015 the authors also acknowledge the technical reports 1978 1993 and 2001 check dam survey and data support from the loess plateau data centre national earth system science data sharing infrastructure national science and technology infrastructure of china http loess geodata cn finally the authors would like to thank honglei tang and qihua ran for their help in processing and analyzing the data used in this research appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 007 
26169,hydraulic simulation tools such as epanet are the primary tools for evaluating water distribution systems performance this work presents a first step towards demonstrating a simple and straightforward implementation of plugins in the new epanet gui to facilitate plugins development by the water systems modeling community the paper shows the code structure and the basic functionalities of a custom plugin demonstrated using three specific examples count fireflow and elevations plugins a prototype repository where developers and users can share and download epanet plugins is presented and discussed epanet plugins framework can support knowledge transfer by increasing the visibility and usability of developed analytical tools and software thus providing benefits for researchers and practitioners the proposed plugins are freely available through github keywords epanet plugins graphical user interface software availability epanet plugins are available from github repository https github com eladsal epanet plugins epanet plugins prototype repository is available at plugins epanet net system requirements epanet ui mtp4r2 exe available at https github com usepa swmm epanet user interface releases tag mtp4r2 1 introduction hydraulic simulation tools such as epanet rossman 1994 are the primary tools for evaluating water distribution systems wds performance the open source epanet is the most widely used software for wds hydraulic and water quality analysis for commercial and research purposes epanet graphical user interface gui enables the end user to create and edit a water network model run steady state and extended period simulations of the hydraulic and water quality dynamics in wds the first official release of epanet by the u s environmental protection agency usepa was in 1993 and the last official release of epanet was in 2008 version 2 00 12 this version allows the user to interact with epanet and perform hydraulic simulations through the gui as well as through a dynamic link library dll of functions that enables the developers programmatically link epanet engine with external software in 2016 the open water analytics owa community released a new epanet version 2 1 under the open source project which provides some performance improvements bug fixes and usage features for the computational engine water analytics 2018 since its original release epanet has been used across a multitude of applications in wds analysis such as optimal design and operation kapelan et al 2005 murphy et al 1994 ormsbee and lansey 2006 perelman et al 2013 savic and walters 2002 xie et al 2014 zierolf et al 2002 leak detection and localization boulos and aboujaoude 2011 martínez solano et al 2017 whitman et al 2018 sensor placement eliades et al 2014 phillips et al 2008 and system security housh and ohar 2017 2018 over the years several independent efforts have been administrated to improve and extend the modeling and simulation capabilities of epanet these efforts can be classified as enhancing the 1 computational engine and 2 gui of epanet this paper focuses on enhancing epanet capabilities through the gui the majority of research efforts have been predominantly focused on enhancing the modeling and computational engine some examples include the epanet msx shang et al 2008 for modeling multi species dynamics epanet rtx hatchett et al 2011 for real time modeling extension and epanet pdx siew and tanyimboh 2012 for pressure driven analysis in wds however these and many other extensions were not incorporated into the core epanet libraries nor into the gui and as a result these projects are not widely adopted in addition to enhancing the computational engine several prior works created augmented versions of epanet by creating new guis that resemble the original epanet gui and include the new features e g epanet bam ho and khalsa 2009 to model incomplete mixing at cross junctions epanet z zonum solutions 2009 to include online maps display and irrigateplus 2017 to optimize irrigation system management however the usability of these tools is limited since some tools have not been released as open source and each tool requires a dedicated gui hence requiring many different versions of epanet for each specialized functionality iglesias rey et al 2017 noteworthy exception is the work by iglesias rey et al 2017 which presented an architecture to exchange information between epanet and third party applications following the prototype of swmm software architecture rossman 2015 another noteworthy effort is the new re engineered epanet gui first released by the usepa in march 2016 swmm epanet ui 2018a b the new epanet gui developed using python scripting language is intended to maintain the original functionalities of the epanet software e g run hydraulic and water quality simulations as well as provide additional functionalities such as scripting and plugins management this paper explores the new plugins framework of epanet gui plugins are pieces of software which extend the capabilities and features of an already existing software predominantly the existing software is already compiled and its source code is not affected by the added plugins plugins are available for many common desktop and web software web browsers google chrome 2018 microsoft edge 2018 are a prominent example of software applications that allow users to extend its basic functionalities with additional features such as block ads better protect privacy or just personalize the look of the browser many free and commercial engineering software utilize a plugin platform including arcgis qgis and autocad autodesk inc 2015 environmental systems research institute esri 2011 each software hosts its plugins in a different way but largely the plugins are centralized in one location for example the wordpress plugins are hosted in a plugins repository with over 56 000 items wordpress 2018 similarly to mozilla firefox mozilla 2018 and the qgis plugins qgis 2016 that are hosted in a centralized repository by having plugins in a centralized location it makes it easier on one hand for the users to find review and download plugins and on the other hand for the developers to maintain and support their plugins plugins framework has become a standard approach in many applications to add new features and enhance the basic capabilities the main two reasons for an application to support a plugin framework is to keep its size and complexity to a minimum and at the same time to allow the addition of new features and capabilities furthermore not all users need all the capabilities of all the plugins as such users can select plugins relevant to their specific application or task that is plugins are optional and are not required to run the original application plugins framework enables third party developers to develop new features thus reducing the burden of the main application developers plugins may even have a different software license than the main application while the main application may be an open source software with a permissive software license for example the mit license open source initiative 2018 a plugin may have a more restrictive one such as the gpl license negus 2015 furthermore plugins for commercial software can be either commercial such as the analytic solver optimization frontline solvers 2015 which extends the basic features of the excel solver add in or free such as the yalmip toolbox lofberg 2004 which adds mathematical programming capabilities for matlab in general plugins framework offers a systematic way to separate the software license and the business model of the main application from these of the added plugins advanced hydraulic modeling and analytics tools coupled with a plugin framework within a user familiar environment of epanet create an opportunity to transform how new models and techniques are being developed shared and used in the water systems modeling community plugin framework has already transformed a number of other software applications in engineering but the water systems modeling community has been slow to adopt although myriad of modeling and analytics tools evolving around epanet and hydraulic modeling have been developed these typically remained limited to the domain of their developers and are not being widely used furthermore none of the modeling advances have been incorporated in any official version of epanet and little efforts have been made to make these advances accessible to the typical end user that relies on the epanet gui hence there is a gap between scientific progress and practical needs uber et al 2018 sela and housh 2019 the changing paradigm of a community driven software development as opposed to an individual driven product development increases the need and motivates a more collaborative development environment of models and software for water systems modeling the new epanet gui plugins framework can help in bridging this gap and facilitates the integration of different tools developed in the water systems modeling community under the epanet umbrella by so doing epanet will leverage the different models and tools which are continuously being developed by the water distribution systems analysis wdsa community to benefit researchers and practitioners and make these tools accessible to a wider community of potential epanet users the abovementioned new epanet gui swmm epanet ui 2018a b developed as an open source project is the first version of the software to include plugins support framework the goal of this paper is two fold to motivate users developers to use develop plugins using the new framework and to motivate the further development and improvement of the plugin framework we achieve our goal by developing three custom plugins that demonstrate the capabilities of the epanet plugins framework for advanced hydraulic analysis and provide the complete codes for testing reproduction and development of new plugins functionalities initial applications utilizing the new epanet gui plugin environment were recently presented and discussed in the first joint wdsa ccwi conference canada 2018 kandjani et al 2018 salomons et al 2018 this paper presents the plugin framework through detailed implementation examples then a prototype plugins repository is outlined followed by conclusions and further development needs 2 the plugin framework in the epanet gui the new epanet gui is being developed in python 2018 a high level open source programming language coupled with qgis a free and open source geographic information system application the new epanet gui download and installation instructions as well as system design overview and source code can be found on the usepa github repository swmm epanet ui 2018a the new gui shown in fig 1 resembles that of epanet 2 00 12 with only few minor exceptions such as the navigation menu on the left hand side of the screen and two new plugins and scripting options in the main menu toolbar the downloadable files include several components related to the software which are stored in relevant sub directories under the top level source directory epanet ui including examples scripts and plugins the plugins are placed in the plugins sub directory and each plugin is placed in a dedicated folder each plugin contains a set of mandatory settings and files and all plugins share a common set of management options that prescribe the core program how to communicate and control the plugin functionalities upon execution upon the initialization of the epanet application a search for available plugins is made in the plugins directory under the application path each found plugin is automatically added to the application main toolbar plugins as shown in fig 1 for illustration purposes four available plugins are shown fig 3 count fireflow import export gis and summary a plugin may be activated by clicking its name on the plugins dropdown menu each plugin sub folder must have a python file named init py note the double underscores before and after the init string this init file should include at least two variables plugin name string and plugin create menu boolean the first is the plugin s name also the same as the name of its sub directory while the second variable indicates whether the plugin will have its own menu once activated in such case a dictionary for the menu items must be defined using a variable named all note the double underscores before and after the all string next we briefly describe the data and model objects that are available for the plugin in the following sections we provide snippets of plugin codes demonstrating the available functionalities and provide the complete codes in github repository https github com eladsal epanet plugins to inform similar efforts the plugin framework supported by epanet is shown in fig 2 when the epanet gui program is started the main map form is initialized and loaded the main map object holds the entire data model of the epanet project and after the hydraulic and water quality simulations are executed the output results a reference to the main map object is passed to all the plugins by the session object the session object contains the project and the output objects which hold the network input and output data respectively as shown in fig 2 the project object is conveniently structured by sections according to the traditional epanet inp input file format for example the project object includes the pipes sub class which corresponds to the pipes section in the epanet input file and defines pipes properties such as name inlet node outlet node diameter length roughness loss coefficient initial status and description a full list of the epanet input file sections format can be found in the epanet user manual rossman 2000 the output object interacts with the simulation engine and contains the simulation results for each time step of the simulation period for all network elements organized by the nodes and links sub classes the time series results can be extracted with the nodes get series and the links get series methods these results include demand head pressure and quality values for nodes and flow headloss quality status and velocity for links a detailed description of available results from epanet hydraulic and water quality simulations is included in the program user manual rossman 2000 in addition to the project and output objects the session object includes a set of basic methods for opening and saving network models and running hydraulic and water quality simulations before demonstrating the plugins we show some of the basic functionalities available to the developer to perform hydraulic simulations as well as setting and extracting model parameters using a prototype code listed in table 1 in lines 1 and 2 the main session and the project objects are defined in line 3 net1 inp file is read into the current session line 4 retrieves the base demand for node 22 using the session project junctions value method and line 5 assigns a new base demand for node 22 the hydraulic simulation is performed in line 6 and results are stored in the session output object in line 7 finally in line 8 the simulated pressures at node 22 are retrieved using the session output get series method 3 examples for developing custom plugins 3 1 plugin 1 count for illustrative purpose a snippet of python code for a simple plugin named count is given in table 2 when activated this plugin adds a new menu to the main toolbar in the epanet gui with two sub menus that report the number of junctions and pipes in the water network that is currently loaded in the gui lines 1 and 2 in table 2 are the name of the plugin and menu creation respectively as mentioned above line 3 includes a dictionary of the plugin s sub menus as shown in fig 3 line 4 is a standard python declaration to import a message box object for reporting the main function of the plugin run in line 5 is called when the user clicks on one of the sub menus according to the user s selection one of the choices in lines 6 or 9 is executed the number of junctions or pipes are retrieved in lines 7 and 10 respectively and reported in lines 8 and 11 this code calls the session project junctions object that retrieves the list of junctions in the open project of the current session to integrate the count plugin with the main epanet application a sub directory count needs to be created with the code listed in table 2 saved as init py and placed under the plugins sub directory under the main epanet ui directory fig 4 3 2 plugin 2 fire flow in this section we demonstrate the fireflow plugin which conducts a fire flow analysis in a wds fire flow analysis is a common practice used by water engineers to ensure protection is provided during fire emergencies boulos et al 2006 xiao et al 2014 the aim of a fire flow analysis is to determine whether the required flow is available at fire hydrants while adequate pressures are maintained in the wds during the stress conditions most of the commercial hydraulic simulation software include a fire flow analysis tool bentley 2018 hcp 2018 infowater 2018 a basic feature of any fire flow analysis is to determine the relationship between the available fire flow discharge at a specific node in the network and the network wide minimum pressure the pressure demand relationship can be evaluated using a rating curve the rating curve is achieved by performing a series of hydraulic simulations each time increasing the fire flow discharge at a given network location and recording the minimum pressure in the wds this process is repeated for each fire flow node in the network given a rating curve the network engineer or operator can evaluate the performance of the wds under different conditions the current version of epanet gui is designed to perform only a single hydraulic simulation and does not offer a way to automate or run multiple hydraulic simulations hence for an epanet gui user fire flow analysis is a tedious process which requires the engineer to repeatedly change the boundary conditions preform hydraulic simulations and manually record and process the pressures in response to changes in the fire flow conditions the fire flow analysis can be easily automated to perform multiple simulations to calculate the fire flow rating curve through a plugin extension an open source fireflow epanet plugin was developed herein and can be freely downloaded from a github epnaet plugins repository fireflow 2018 after the fireflow plugin folder is downloaded and placed in the epanet plugins directory a new fireflow menu will appear in the main epanet gui as shown in fig 5 after the user selects the analyze option a new window will appear fig 6 with the list of the junctions included in the network model in this example net1 inp was used see fig 1 to run the fire flow analysis the user selects the fire flow node to conduct the analysis along with three additional parameters the minimum and maximum fire flow discharge to analyze and the incremental step between the minimum and maximum flows after selecting the node and setting the parameters the user can click the run button to perform the simulations once the simulation runs are completed the demand pressure rating curve is presented the user may select additional nodes for the fire flow analysis fig 6 shows the fireflow analysis window two analyses were performed for nodes 22 and 32 respectively ranging the fire flow discharge from 0 to 1200 gpm the top plot demonstrates the change in the minimum pressures in the network as a response to increasing the discharge in node 22 the minimum pressures gradually decrease from 120 psi to 100 psi as the discharge increases to 900 gpm then the minimum pressure rapidly decrease to 10 psi as fire flow discharge increases to 1200 gpm similar analysis is performed for node 32 bottom plot in fig 6 however the results show a rapid decrease in minimum pressures reaching negative values as fire flow discharge increases to 600 gpm these results indicate that node 22 satisfies the fire flow upper limit whereas node 32 is sensitive to fire flow conditions and does not satisfy the fire flow upper limit table 3 shows a code snippet from the fireflow plugin main code line 1 is the main loop over the number of requested hydraulic simulations based on the range of the fire flow discharge in lines 2 3 the current demand is calculated and assigned to the analyzed junction then a full hydraulic simulation is performed by calling the session run simulation method line 4 which performs both the hydraulic and water quality simulations the system s pressures are extracted lines 5 6 using the get series method finally the demand and minimum pressure are recorded in lines 7 and 8 respectively which are then plotted for the user the fireflow plugin is a simple example of how the basic epanet software may be extended via the plugins framework in order to add new capabilities to the program with new algorithms and graphical user interface the fireflow plugin relies on epanet hydraulic simulator but does not alter the main code of epanet furthermore the fireflow plugin is optional and is not required to run the original application 3 3 plugin 3 elevations the process of building a hydraulic model typically originates from a water utility s records on the location of network elements and characteristics that are maintained in geographic information systems gis deuerlein et al 2015 roma et al 2015 however most utilities do not have detailed enough records of their pipeline infrastructure and additionally some loss of information is inevitable when transforming gis pipeline records into hydraulic models hence when required information is not available some estimates must be made in order to assess the network pressures and hydraulic grade lines traditionally commercial vendors are able to provide very accurate digital elevation model dem of any given area however usually at a high cost other companies such as google and microsoft provide a less accurate dem for a small fee or in some cases for free via their mapping products such as google maps and bing maps respectively the third epanet plugin developed in this work the elevations plugin uses the google maps elevations api to retrieve the elevation data for the network nodes google maps platform 2018 table 4 shows a code snippet from the main code of the elevations plugin the full plugin code is available on github first the list of junctions is extracted from the session project object line 1 then the x and y longitude and latitude coordinates are extracted for each junction line 3 in line 5 the url request with the google maps api is constructed and opened line 6 it should be noted that a private api key is used for authentication line 7 executes a custom function which extracts the elevation data from the xml data structure returned by the google maps api to the elev variable finally the elevation data is assigned to the junction s elevation property line 8 this plugin demonstrates the possibility for the epanet software to interact with web services to get and set data 4 a prototype repository for epanet plugins three prototype plugins were presented in the previous sections however for a successful and sustainable transition to the plugins framework there is a need for a centralized place where developers and users can upload and download plugins and share information a centralized plugins repository is common across many well known applications such arcgis 2018 qgis 2016 and autodesk 2016 fig 7 shows a prototype repository for epanet plugins which can be used to easily share plugin files between developers and users the user could find a list of available plugins description installation instructions test cases and reviews by other users the developer can find documentation code snippets and submission instructions the users can evaluate the published plugins by providing feedback to the developers and the wdsa community and help determine how robust and reliable these plugins are such a repository can provide a viable pathway towards research transfer and dissemination of new scientific tools developed by the wdsa community and help distributing these tools to the multitude of epanet users to enhance their hydraulic modeling and analysis capabilities this is especially true for users that rely on the user interface who often cannot take advantage of advanced modeling tools because these typically require advanced programming skills moreover such a repository can create new opportunities for exchanging information ideas and benchmarking related to computing and analysis of wdss as well as simplifying plugins implementation by providing documentation tutorials and templates as the expectations for more transparent and reproducible research are rising there is an increasing pressure on the wdsa community to adopt a more open and collaborative research and software development see for example the publication data policy in the leading journals of environmental modeling and software elsevier 2018 water resources research american geophysical union 2013 and journal of water resources planning and management rosenberg et al 2018 the epanet plugins framework can support achieving this goal thus providing benefits for the researchers and the practitioners certainly sharing research tools and making them accessible to the wider water systems modeling community through plugins will not resolve all modeling limitations of epanet nevertheless if successful it could result in hydraulic simulation improvements active community engagements and interest in epanet software development uber et al 2018 the count fireflow and elevations plugins presented here are three simple examples of enhancement to current capabilities of epanet other pressing examples include automatic demand assignment from raw billing data and smart meters into demand pattern format for hydraulic modeling robust gis tools to transform gis data sets into hydraulic models and pressure driven modeling 5 conclusions this work presents a first step towards demonstrating a straightforward implementation of plugins in the new epanet gui the plugin framework supported by epanet through python scripting program is responsible for setting up the plugin environment the plugins are independent components from the main epanet software that can be developed and distributed separately and are not required to execute the original application plugins integrated with the main epanet application can provide additional modeling and analysis functionalities that are not available in the current epanet software e g fire flow analysis demonstrated in this work advanced hydraulic modeling and analytics tools coupled with a plugin framework within a user familiar environment of epanet create an opportunity to transform how new models and techniques are being developed shared and used in the water systems modeling community as the expectations for more transparent and reproducible research are rising epanet plugins framework can support knowledge transfer thus providing benefits for the researchers and the practitioners acknowledgments this work was supported by the university of texas at austin new faculty startup grant and by the israeli water authority grant 4501284516 
26169,hydraulic simulation tools such as epanet are the primary tools for evaluating water distribution systems performance this work presents a first step towards demonstrating a simple and straightforward implementation of plugins in the new epanet gui to facilitate plugins development by the water systems modeling community the paper shows the code structure and the basic functionalities of a custom plugin demonstrated using three specific examples count fireflow and elevations plugins a prototype repository where developers and users can share and download epanet plugins is presented and discussed epanet plugins framework can support knowledge transfer by increasing the visibility and usability of developed analytical tools and software thus providing benefits for researchers and practitioners the proposed plugins are freely available through github keywords epanet plugins graphical user interface software availability epanet plugins are available from github repository https github com eladsal epanet plugins epanet plugins prototype repository is available at plugins epanet net system requirements epanet ui mtp4r2 exe available at https github com usepa swmm epanet user interface releases tag mtp4r2 1 introduction hydraulic simulation tools such as epanet rossman 1994 are the primary tools for evaluating water distribution systems wds performance the open source epanet is the most widely used software for wds hydraulic and water quality analysis for commercial and research purposes epanet graphical user interface gui enables the end user to create and edit a water network model run steady state and extended period simulations of the hydraulic and water quality dynamics in wds the first official release of epanet by the u s environmental protection agency usepa was in 1993 and the last official release of epanet was in 2008 version 2 00 12 this version allows the user to interact with epanet and perform hydraulic simulations through the gui as well as through a dynamic link library dll of functions that enables the developers programmatically link epanet engine with external software in 2016 the open water analytics owa community released a new epanet version 2 1 under the open source project which provides some performance improvements bug fixes and usage features for the computational engine water analytics 2018 since its original release epanet has been used across a multitude of applications in wds analysis such as optimal design and operation kapelan et al 2005 murphy et al 1994 ormsbee and lansey 2006 perelman et al 2013 savic and walters 2002 xie et al 2014 zierolf et al 2002 leak detection and localization boulos and aboujaoude 2011 martínez solano et al 2017 whitman et al 2018 sensor placement eliades et al 2014 phillips et al 2008 and system security housh and ohar 2017 2018 over the years several independent efforts have been administrated to improve and extend the modeling and simulation capabilities of epanet these efforts can be classified as enhancing the 1 computational engine and 2 gui of epanet this paper focuses on enhancing epanet capabilities through the gui the majority of research efforts have been predominantly focused on enhancing the modeling and computational engine some examples include the epanet msx shang et al 2008 for modeling multi species dynamics epanet rtx hatchett et al 2011 for real time modeling extension and epanet pdx siew and tanyimboh 2012 for pressure driven analysis in wds however these and many other extensions were not incorporated into the core epanet libraries nor into the gui and as a result these projects are not widely adopted in addition to enhancing the computational engine several prior works created augmented versions of epanet by creating new guis that resemble the original epanet gui and include the new features e g epanet bam ho and khalsa 2009 to model incomplete mixing at cross junctions epanet z zonum solutions 2009 to include online maps display and irrigateplus 2017 to optimize irrigation system management however the usability of these tools is limited since some tools have not been released as open source and each tool requires a dedicated gui hence requiring many different versions of epanet for each specialized functionality iglesias rey et al 2017 noteworthy exception is the work by iglesias rey et al 2017 which presented an architecture to exchange information between epanet and third party applications following the prototype of swmm software architecture rossman 2015 another noteworthy effort is the new re engineered epanet gui first released by the usepa in march 2016 swmm epanet ui 2018a b the new epanet gui developed using python scripting language is intended to maintain the original functionalities of the epanet software e g run hydraulic and water quality simulations as well as provide additional functionalities such as scripting and plugins management this paper explores the new plugins framework of epanet gui plugins are pieces of software which extend the capabilities and features of an already existing software predominantly the existing software is already compiled and its source code is not affected by the added plugins plugins are available for many common desktop and web software web browsers google chrome 2018 microsoft edge 2018 are a prominent example of software applications that allow users to extend its basic functionalities with additional features such as block ads better protect privacy or just personalize the look of the browser many free and commercial engineering software utilize a plugin platform including arcgis qgis and autocad autodesk inc 2015 environmental systems research institute esri 2011 each software hosts its plugins in a different way but largely the plugins are centralized in one location for example the wordpress plugins are hosted in a plugins repository with over 56 000 items wordpress 2018 similarly to mozilla firefox mozilla 2018 and the qgis plugins qgis 2016 that are hosted in a centralized repository by having plugins in a centralized location it makes it easier on one hand for the users to find review and download plugins and on the other hand for the developers to maintain and support their plugins plugins framework has become a standard approach in many applications to add new features and enhance the basic capabilities the main two reasons for an application to support a plugin framework is to keep its size and complexity to a minimum and at the same time to allow the addition of new features and capabilities furthermore not all users need all the capabilities of all the plugins as such users can select plugins relevant to their specific application or task that is plugins are optional and are not required to run the original application plugins framework enables third party developers to develop new features thus reducing the burden of the main application developers plugins may even have a different software license than the main application while the main application may be an open source software with a permissive software license for example the mit license open source initiative 2018 a plugin may have a more restrictive one such as the gpl license negus 2015 furthermore plugins for commercial software can be either commercial such as the analytic solver optimization frontline solvers 2015 which extends the basic features of the excel solver add in or free such as the yalmip toolbox lofberg 2004 which adds mathematical programming capabilities for matlab in general plugins framework offers a systematic way to separate the software license and the business model of the main application from these of the added plugins advanced hydraulic modeling and analytics tools coupled with a plugin framework within a user familiar environment of epanet create an opportunity to transform how new models and techniques are being developed shared and used in the water systems modeling community plugin framework has already transformed a number of other software applications in engineering but the water systems modeling community has been slow to adopt although myriad of modeling and analytics tools evolving around epanet and hydraulic modeling have been developed these typically remained limited to the domain of their developers and are not being widely used furthermore none of the modeling advances have been incorporated in any official version of epanet and little efforts have been made to make these advances accessible to the typical end user that relies on the epanet gui hence there is a gap between scientific progress and practical needs uber et al 2018 sela and housh 2019 the changing paradigm of a community driven software development as opposed to an individual driven product development increases the need and motivates a more collaborative development environment of models and software for water systems modeling the new epanet gui plugins framework can help in bridging this gap and facilitates the integration of different tools developed in the water systems modeling community under the epanet umbrella by so doing epanet will leverage the different models and tools which are continuously being developed by the water distribution systems analysis wdsa community to benefit researchers and practitioners and make these tools accessible to a wider community of potential epanet users the abovementioned new epanet gui swmm epanet ui 2018a b developed as an open source project is the first version of the software to include plugins support framework the goal of this paper is two fold to motivate users developers to use develop plugins using the new framework and to motivate the further development and improvement of the plugin framework we achieve our goal by developing three custom plugins that demonstrate the capabilities of the epanet plugins framework for advanced hydraulic analysis and provide the complete codes for testing reproduction and development of new plugins functionalities initial applications utilizing the new epanet gui plugin environment were recently presented and discussed in the first joint wdsa ccwi conference canada 2018 kandjani et al 2018 salomons et al 2018 this paper presents the plugin framework through detailed implementation examples then a prototype plugins repository is outlined followed by conclusions and further development needs 2 the plugin framework in the epanet gui the new epanet gui is being developed in python 2018 a high level open source programming language coupled with qgis a free and open source geographic information system application the new epanet gui download and installation instructions as well as system design overview and source code can be found on the usepa github repository swmm epanet ui 2018a the new gui shown in fig 1 resembles that of epanet 2 00 12 with only few minor exceptions such as the navigation menu on the left hand side of the screen and two new plugins and scripting options in the main menu toolbar the downloadable files include several components related to the software which are stored in relevant sub directories under the top level source directory epanet ui including examples scripts and plugins the plugins are placed in the plugins sub directory and each plugin is placed in a dedicated folder each plugin contains a set of mandatory settings and files and all plugins share a common set of management options that prescribe the core program how to communicate and control the plugin functionalities upon execution upon the initialization of the epanet application a search for available plugins is made in the plugins directory under the application path each found plugin is automatically added to the application main toolbar plugins as shown in fig 1 for illustration purposes four available plugins are shown fig 3 count fireflow import export gis and summary a plugin may be activated by clicking its name on the plugins dropdown menu each plugin sub folder must have a python file named init py note the double underscores before and after the init string this init file should include at least two variables plugin name string and plugin create menu boolean the first is the plugin s name also the same as the name of its sub directory while the second variable indicates whether the plugin will have its own menu once activated in such case a dictionary for the menu items must be defined using a variable named all note the double underscores before and after the all string next we briefly describe the data and model objects that are available for the plugin in the following sections we provide snippets of plugin codes demonstrating the available functionalities and provide the complete codes in github repository https github com eladsal epanet plugins to inform similar efforts the plugin framework supported by epanet is shown in fig 2 when the epanet gui program is started the main map form is initialized and loaded the main map object holds the entire data model of the epanet project and after the hydraulic and water quality simulations are executed the output results a reference to the main map object is passed to all the plugins by the session object the session object contains the project and the output objects which hold the network input and output data respectively as shown in fig 2 the project object is conveniently structured by sections according to the traditional epanet inp input file format for example the project object includes the pipes sub class which corresponds to the pipes section in the epanet input file and defines pipes properties such as name inlet node outlet node diameter length roughness loss coefficient initial status and description a full list of the epanet input file sections format can be found in the epanet user manual rossman 2000 the output object interacts with the simulation engine and contains the simulation results for each time step of the simulation period for all network elements organized by the nodes and links sub classes the time series results can be extracted with the nodes get series and the links get series methods these results include demand head pressure and quality values for nodes and flow headloss quality status and velocity for links a detailed description of available results from epanet hydraulic and water quality simulations is included in the program user manual rossman 2000 in addition to the project and output objects the session object includes a set of basic methods for opening and saving network models and running hydraulic and water quality simulations before demonstrating the plugins we show some of the basic functionalities available to the developer to perform hydraulic simulations as well as setting and extracting model parameters using a prototype code listed in table 1 in lines 1 and 2 the main session and the project objects are defined in line 3 net1 inp file is read into the current session line 4 retrieves the base demand for node 22 using the session project junctions value method and line 5 assigns a new base demand for node 22 the hydraulic simulation is performed in line 6 and results are stored in the session output object in line 7 finally in line 8 the simulated pressures at node 22 are retrieved using the session output get series method 3 examples for developing custom plugins 3 1 plugin 1 count for illustrative purpose a snippet of python code for a simple plugin named count is given in table 2 when activated this plugin adds a new menu to the main toolbar in the epanet gui with two sub menus that report the number of junctions and pipes in the water network that is currently loaded in the gui lines 1 and 2 in table 2 are the name of the plugin and menu creation respectively as mentioned above line 3 includes a dictionary of the plugin s sub menus as shown in fig 3 line 4 is a standard python declaration to import a message box object for reporting the main function of the plugin run in line 5 is called when the user clicks on one of the sub menus according to the user s selection one of the choices in lines 6 or 9 is executed the number of junctions or pipes are retrieved in lines 7 and 10 respectively and reported in lines 8 and 11 this code calls the session project junctions object that retrieves the list of junctions in the open project of the current session to integrate the count plugin with the main epanet application a sub directory count needs to be created with the code listed in table 2 saved as init py and placed under the plugins sub directory under the main epanet ui directory fig 4 3 2 plugin 2 fire flow in this section we demonstrate the fireflow plugin which conducts a fire flow analysis in a wds fire flow analysis is a common practice used by water engineers to ensure protection is provided during fire emergencies boulos et al 2006 xiao et al 2014 the aim of a fire flow analysis is to determine whether the required flow is available at fire hydrants while adequate pressures are maintained in the wds during the stress conditions most of the commercial hydraulic simulation software include a fire flow analysis tool bentley 2018 hcp 2018 infowater 2018 a basic feature of any fire flow analysis is to determine the relationship between the available fire flow discharge at a specific node in the network and the network wide minimum pressure the pressure demand relationship can be evaluated using a rating curve the rating curve is achieved by performing a series of hydraulic simulations each time increasing the fire flow discharge at a given network location and recording the minimum pressure in the wds this process is repeated for each fire flow node in the network given a rating curve the network engineer or operator can evaluate the performance of the wds under different conditions the current version of epanet gui is designed to perform only a single hydraulic simulation and does not offer a way to automate or run multiple hydraulic simulations hence for an epanet gui user fire flow analysis is a tedious process which requires the engineer to repeatedly change the boundary conditions preform hydraulic simulations and manually record and process the pressures in response to changes in the fire flow conditions the fire flow analysis can be easily automated to perform multiple simulations to calculate the fire flow rating curve through a plugin extension an open source fireflow epanet plugin was developed herein and can be freely downloaded from a github epnaet plugins repository fireflow 2018 after the fireflow plugin folder is downloaded and placed in the epanet plugins directory a new fireflow menu will appear in the main epanet gui as shown in fig 5 after the user selects the analyze option a new window will appear fig 6 with the list of the junctions included in the network model in this example net1 inp was used see fig 1 to run the fire flow analysis the user selects the fire flow node to conduct the analysis along with three additional parameters the minimum and maximum fire flow discharge to analyze and the incremental step between the minimum and maximum flows after selecting the node and setting the parameters the user can click the run button to perform the simulations once the simulation runs are completed the demand pressure rating curve is presented the user may select additional nodes for the fire flow analysis fig 6 shows the fireflow analysis window two analyses were performed for nodes 22 and 32 respectively ranging the fire flow discharge from 0 to 1200 gpm the top plot demonstrates the change in the minimum pressures in the network as a response to increasing the discharge in node 22 the minimum pressures gradually decrease from 120 psi to 100 psi as the discharge increases to 900 gpm then the minimum pressure rapidly decrease to 10 psi as fire flow discharge increases to 1200 gpm similar analysis is performed for node 32 bottom plot in fig 6 however the results show a rapid decrease in minimum pressures reaching negative values as fire flow discharge increases to 600 gpm these results indicate that node 22 satisfies the fire flow upper limit whereas node 32 is sensitive to fire flow conditions and does not satisfy the fire flow upper limit table 3 shows a code snippet from the fireflow plugin main code line 1 is the main loop over the number of requested hydraulic simulations based on the range of the fire flow discharge in lines 2 3 the current demand is calculated and assigned to the analyzed junction then a full hydraulic simulation is performed by calling the session run simulation method line 4 which performs both the hydraulic and water quality simulations the system s pressures are extracted lines 5 6 using the get series method finally the demand and minimum pressure are recorded in lines 7 and 8 respectively which are then plotted for the user the fireflow plugin is a simple example of how the basic epanet software may be extended via the plugins framework in order to add new capabilities to the program with new algorithms and graphical user interface the fireflow plugin relies on epanet hydraulic simulator but does not alter the main code of epanet furthermore the fireflow plugin is optional and is not required to run the original application 3 3 plugin 3 elevations the process of building a hydraulic model typically originates from a water utility s records on the location of network elements and characteristics that are maintained in geographic information systems gis deuerlein et al 2015 roma et al 2015 however most utilities do not have detailed enough records of their pipeline infrastructure and additionally some loss of information is inevitable when transforming gis pipeline records into hydraulic models hence when required information is not available some estimates must be made in order to assess the network pressures and hydraulic grade lines traditionally commercial vendors are able to provide very accurate digital elevation model dem of any given area however usually at a high cost other companies such as google and microsoft provide a less accurate dem for a small fee or in some cases for free via their mapping products such as google maps and bing maps respectively the third epanet plugin developed in this work the elevations plugin uses the google maps elevations api to retrieve the elevation data for the network nodes google maps platform 2018 table 4 shows a code snippet from the main code of the elevations plugin the full plugin code is available on github first the list of junctions is extracted from the session project object line 1 then the x and y longitude and latitude coordinates are extracted for each junction line 3 in line 5 the url request with the google maps api is constructed and opened line 6 it should be noted that a private api key is used for authentication line 7 executes a custom function which extracts the elevation data from the xml data structure returned by the google maps api to the elev variable finally the elevation data is assigned to the junction s elevation property line 8 this plugin demonstrates the possibility for the epanet software to interact with web services to get and set data 4 a prototype repository for epanet plugins three prototype plugins were presented in the previous sections however for a successful and sustainable transition to the plugins framework there is a need for a centralized place where developers and users can upload and download plugins and share information a centralized plugins repository is common across many well known applications such arcgis 2018 qgis 2016 and autodesk 2016 fig 7 shows a prototype repository for epanet plugins which can be used to easily share plugin files between developers and users the user could find a list of available plugins description installation instructions test cases and reviews by other users the developer can find documentation code snippets and submission instructions the users can evaluate the published plugins by providing feedback to the developers and the wdsa community and help determine how robust and reliable these plugins are such a repository can provide a viable pathway towards research transfer and dissemination of new scientific tools developed by the wdsa community and help distributing these tools to the multitude of epanet users to enhance their hydraulic modeling and analysis capabilities this is especially true for users that rely on the user interface who often cannot take advantage of advanced modeling tools because these typically require advanced programming skills moreover such a repository can create new opportunities for exchanging information ideas and benchmarking related to computing and analysis of wdss as well as simplifying plugins implementation by providing documentation tutorials and templates as the expectations for more transparent and reproducible research are rising there is an increasing pressure on the wdsa community to adopt a more open and collaborative research and software development see for example the publication data policy in the leading journals of environmental modeling and software elsevier 2018 water resources research american geophysical union 2013 and journal of water resources planning and management rosenberg et al 2018 the epanet plugins framework can support achieving this goal thus providing benefits for the researchers and the practitioners certainly sharing research tools and making them accessible to the wider water systems modeling community through plugins will not resolve all modeling limitations of epanet nevertheless if successful it could result in hydraulic simulation improvements active community engagements and interest in epanet software development uber et al 2018 the count fireflow and elevations plugins presented here are three simple examples of enhancement to current capabilities of epanet other pressing examples include automatic demand assignment from raw billing data and smart meters into demand pattern format for hydraulic modeling robust gis tools to transform gis data sets into hydraulic models and pressure driven modeling 5 conclusions this work presents a first step towards demonstrating a straightforward implementation of plugins in the new epanet gui the plugin framework supported by epanet through python scripting program is responsible for setting up the plugin environment the plugins are independent components from the main epanet software that can be developed and distributed separately and are not required to execute the original application plugins integrated with the main epanet application can provide additional modeling and analysis functionalities that are not available in the current epanet software e g fire flow analysis demonstrated in this work advanced hydraulic modeling and analytics tools coupled with a plugin framework within a user familiar environment of epanet create an opportunity to transform how new models and techniques are being developed shared and used in the water systems modeling community as the expectations for more transparent and reproducible research are rising epanet plugins framework can support knowledge transfer thus providing benefits for the researchers and the practitioners acknowledgments this work was supported by the university of texas at austin new faculty startup grant and by the israeli water authority grant 4501284516 
