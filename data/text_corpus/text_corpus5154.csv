index,text
25770,this study compares the performances of two sampling based strategies for the simultaneous estimation of the first and total order variance based sensitivity indices a k a sobol indices the first strategy corresponds to the current approach employed by practitioners and recommended in the literature the second one was only recently introduced by the first and last authors of the present article both strategies rely on different estimators of first and total order sobol indices the asymptotic normal variances of the two sets of estimators are established and their accuracies are compared theoretically and numerically the results show that the new strategy outperforms the current one the global sensitivity analysis of the radiative forcing model of sulfur aerosols is performed with the new strategy the results confirm that in this model interactions are important and only one input variable is irrelevant keywords global sensitivity analysis variance based sensitivity indices first order sobol index total order sobol index asymptotic normality radiative forcing model 1 background uncertainty and sensitivity analyses are essential ingredients of modelling saltelli et al 2004 they allow to point out the key uncertain assumptions input factors that can be random variables or random fields responsible for the uncertainty into the model outcome of interest this is particularly relevant when models are used for decision making assessing model output uncertainty requires several runs of the model monte carlo simulations allow to carry out this task by sampling the input factors from their joint probability distribution and propagating the sample through the model response of interest i e running the model sensitivity analysis sa can then be undertaken to identify for instance the input factors mostly responsible for the uncertainty in the model response depending on the method used sa can be conducted directly from the monte carlo sample at hand i e the one generated to assess model output uncertainty or can require extra monte carlo simulations by following an appropriate sampling design the method to be used depends on the sensitivity indices also called importance measures that the analyst wants to compute as recommended in saltelli et al 2004 see also saltelli and tarantola 2002 the sensitivity indices to assess should be related to the question that sa is called to answer to the authors enumerate several questions called sa settings that can be addressed with the so called variance based sensitivity indices in the sequel we focus on the estimation of variance based sensitivity indices also called sobol indices sobol 1993 as eluded previously a monte carlo sample is required to carry out uncertainty analysis that is assessing the predictive uncertainty of the model output of interest we assume that there is only one scalar output denoted y f x the input factors are represented by a random vector of scalar variables x x 1 x d possibly grouped into two complementary vectors u v they are assumed independent of each other for the case of dependent inputs see for instance li et al 2010 kucherenko et al 2012 mara and tarantola 2012 mara et al 2015 li and rabitz 2017 tarantola and mara 2017 there exist several sobol indices called first order closed second order and so forth a closed second order sobol index and more generally a closed d th order sobol index can be defined as the first order sobol index of a group of two resp d inputs in the sequel we will use the term first and total order sobol indices whether they refer to an individual variable say x i or a group of variables e g u first and total order sobol indices are respectively defined as follows 1 s u v e y u v y 2 s t u e v y v v y where v stands for the unconditional variance operator resp v the conditional variance and e stands for the mathematical expectation resp e the conditional expectation the sobol indices range over 0 1 and st u s u eq 1 is the first order sobol index of the group of inputs u while eq 2 is the total order sobol index of u the higher the sobol indices the more the group of inputs u is important for the model response the difference between s u and st u is that the latter not only accounts for the amount of variance of y explained by the input variables within u like s u does but it also contains cooperative contributions due to the interactions between the variables in u with those in v therefore a noticeable result is that s u st v 1 let d 1 d be the number of elements in u s u represents in percentage the expected reduction in v y if the variables in u where fixed to their true value that is why the first order sensitivity indices of individual inputs i e d 1 are to be estimated if the goal of the sa is to identify the input variable that would induce the largest reduction in variance if its value was known accurately this sa setting is called factors prioritization instead if the goal is to identify the irrelevant inputs called screening analysis or factors fixing setting then the individual total order sobol indices are to be estimated indeed we note that if st u 0 the variables in u do not contribute at all to the variance of y more sa settings are discussed in saltelli and tarantola 2002 there are typically two direct methods to estimate the first and total order sobol indices the first one uses monte carlo methods e g sobol 1993 saltelli 2002 the second one casts the total variance onto orthogonal functions like the fourier expansion a k a the fourier amplitude sensitivity test cukier et al 1978 saltelli et al 1999 mara 2009 or the polynomial chaos expansion sudret 2008 blatman and sudret 2011 shao et al 2017 indirect methods employ surrogate models first also called metamodels to mimic the input output relationship and then often use one of the aforementioned direct methods to compute the sobol indices e g marseguerra et al 2003 estimating the sobol indices with monte carlo estimators is rather computationally expensive but it does not require any assumption except that the variance of f x be numerically tractable in the present work we study the performances of two monte carlo estimators of eq 1 and eq 2 respectively that rely on two different sampling designs the paper is organised as follows in section 2 we introduce the two sampling strategies as well as their associated monte carlo estimators to compute both the first and total order sobol indices their asymptotic normal variances derived in the appendices are also compared to each other in section 3 the performances of the two sets of estimators are compared through numerical exercises on well known benchmark functions the new set of estimators is applied to the radiative forcing model of sulfur aerosols in section 4 finally the key results of our work are summarized in section 5 2 monte carlo estimators 2 1 integral approximation when ilya m sobol introduced the variance based sensitivity indices in sobol 1993 he also proposed their monte carlo mc estimators indeed the sobol indices defined in eqs 1 and 2 are nothing but ratio of integrals approximating these integrals numerically provides estimates of the sobol indices monte carlo mc estimators rely on the fact that multidimensional integrals can be approximated via mc samples as follows 3 r d f q x p x x d x 1 n n 1 n f q x n 1 x n d where x p x meaning that p x is the joint probability density of x and x n x n 1 x n d is the n th out of n mc draw of the input factors sampled w r t p x in the sequel we assume that x is a vector of independent input variables that is p x i 1 d p x i x i where p x i x i is the marginal distribution of x i let y a y b y a u y b u be four distinct model output samples whose n th element for each of them is respectively defined as follows y n a f u n a v n a f x n a y n b f u n b v n b f x n b y n a u f u n a v n b f x n a u y n b u f u n b v n a f x n b u where x n a and x n b are two independent input vectors identically distributed as well as x n a u and x n b u the u values in vector x n a u resp x n b u are identical to those in x n a resp x n b while the v values are those of x n b resp x n a 2 2 current estimators the most popular sampling design to compute simultaneously the first and total order sensitivity indices as recommended by saltelli et al 2012 requires three samples namely y a y b y a u or equivalently y a y b y b u to compute the sensitivity indices of u their estimators are respectively defined as follows 4 s u s s 1 n n 1 n y n a y n a u y n b 1 2 n n 1 n y n a y n b 2 5 s t u s j 1 2 n n 1 n y n a u y n b 2 1 2 n n 1 n y n a y n b 2 note that we do not simplify these equations e g the 2n at the numerator and denominator cancel each other for the purpose of the discussion that just follows but latter on we will the superscript ss stands for sobol saltelli as the former derived the integral formulation of the numerator in sobol et al 2007 while saltelli proposed an estimator similar to the numerator of eq 4 in saltelli 2002 the superscript sj often refers to sobol jansen although one can date back the numerator of eq 5 to šaltenis and dzemyda 1982 and jansen et al 1994 see saltelli et al 2000 page 177 therefore sj can also be read šaltenis jansen the denominators of the previous formulas are identical but they differ from the one proposed in saltelli 2002 saltelli et al 2010 as defined the denominator of eqs 4 and 5 is an mc estimator of v y we find it convenient to formulate the denominator in this way because it highlights the symmetry between y a y b in the denominator eq 4 is known to provide an accurate estimate of small first order sensitivity indices sobol et al 2007 as eq 5 does for the total order sensitivity indices importantly although in theory st u s u the previous estimators do not satisfy this criterion indeed by noticing that v y 1 2 n n 1 n y n a y n b 2 at the denominator of eqs 4 and 5 is a positive scalar we find that 6 v y s t u s j s u s s 1 n n 1 n y n a u y n b 2 2 y n a y n a u y n b which because 2 y n a y n a u y n b can be either positive or negative does not ensure that s t u s j s u s s these observations advocate for a more symmetrical and coherent estimator for the first order sensitivity index this is the subject of the next subsection 2 3 new estimators as previously mentioned the denominator of eq 5 converges towards v y that is lim n 1 2 n n 1 n y n a y n b 2 lim n 1 2 n n 1 n y n a u y n b u 2 v y while the numerator is such that lim n 1 2 n n 1 n y n b y n a u 2 lim n 1 2 n n 1 n y n a y n b u 2 e v y v hence the following symmetrical estimator for the total order sensitivity index can be derived 7 s t u i a n 1 n y n b y n a u 2 y n a y n b u 2 n 1 n y n a y n b 2 y n a u y n b u 2 this is because as already mentioned x n a and x n b are two independent input vectors identically distributed as well as x n a u and x n b u notice the perfect symmetry of the formula which remains unchanged by switching the superscripts b and a incidentally the superscript ia stands indifferently for innovative algorithm and ivano azzini the first author of this article who initiated the work on these estimators azzini and rosati 2021 interchanging y a u y b u in eq 7 only changes the numerator and provides the estimator for s t v i a the law of total variance implies that s u i a s t v i a 1 therefore the first order sensitivity index s u is estimated as follows s u i a 1 s t v i a 1 n 1 n y n b y n b u 2 y n a y n a u 2 n 1 n y n a y n b 2 y n a u y n b u 2 which after some developments yields 8 s u i a 2 n 1 n y n a u y n b y n a y n b u n 1 n y n a y n b 2 y n a u y n b u 2 besides s t u i a s u i a n 1 n y n b y n a u 2 y n a y n b u 2 2 y n a u y n b y n a y n b u n 1 n y n a y n b 2 y n a u y n b u 2 which yields 9 s t u i a s u i a n 1 n y n b y n a u y n a y n b u 2 n 1 n y n a y n b 2 y n a u y n b u 2 0 and proves that s t u i a s u i a for any n eq 9 also shows that s t u i a s u i a if and only if f x is additive with respect to u in effect an additive function with respect to u and v reads in this case y f u v f 0 f u u f v v and it is straightforward to prove that the numerator of eq 9 equals zero for any sample size n it turns out that the numerator of eq 8 is very similar to the one proposed by owen in owen 2012 apart from the factor 2 due to the denominator the difference is the use by the author of y n c u f u n c v n a instead of y n a u or y n b u in eq 8 by doing so the symmetry of the estimator is lost interestingly lamboni 2020 also derived unbiased estimators with minimum variance of the non normalized sobol indices by leaning on the theory of u statistics see hoeffding 1948 gamboa et al 2021 his construction led to estimators exactly equal to the numerators of eqs 7 and 8 however neither lamboni in lamboni 2020 nor owen in owen 2012 paid attention to the estimation of the total variance the proof that s t u i a s u i a does not depend on the choice of the total variance estimator i e the denominator of eq 9 therefore the estimators proposed by lamboni in lamboni 2019 2020 also satisfy this property but they do not form a set of complementary formulas contrarily to the new estimators defined by eqs 7 and 8 also called the ia estimators in the sequel indeed thanks to the special choice of the total variance estimator i e the denominator of eqs 7 and 8 the ia estimators also satisfy the following equation s u i a s t v i a 1 to our best knowledge there are no other estimators of first and total order sensitivity indices that comply with both the equation s u s t v 1 and the inequation s u s t u 2 4 variances of the estimators the performance of an estimator is characterized by its bias and its variance for a given sample size n the sobol indices can be computed several times by re sampling the mc draws thus providing different estimates unbiased estimators provide replicates that on average yield the true values of the sobol indices estimators with small variances provide estimates that remain close to the true values of the sobol indices in the sequel the focus is on the variances of the estimators in appendices a and b we establish the variances of the estimators discussed in the present paper under the asymptotic normality assumption van der vaart 2000 janon et al 2014 they respectively read as follows 10 σ s s 2 v 2 y a y a u y b s u y a y b 2 4 v y 2 11 τ s j 2 v y a u y b 2 s t u y a y b 2 4 v y 2 and 12 σ i a 2 v 2 y a y b u y a u y b s u y a y b 2 y a u y b u 2 16 v y 2 13 τ i a 2 v y a y b u 2 y b y a u 2 s t u y a y b 2 y a u y b u 2 16 v y 2 in eqs 10 13 y a y b y a u and y b u are random variables in practice to compute the variances of the estimators they are replaced by their samples y a y b y a u and y b u see section 3 it can be qualitatively speculated that the sobol jansen estimator is more accurate than the one of sobol saltelli indeed we have according to sobol 1993 y f u v f 0 f u u f v v f u v u v this implies that y a u y b f u u b f u u a f u v u b v b f u v u a v b y a y b u f u u b f u u a f u v u b v a f u v u a v a therefore the variance of y a u y b 2 is expected to be smaller than the one of 2 y a y a u y b because the former does not contain neither f 0 nor f v contrarily to the latter with y a what is worse we guess that eq 4 may perform very poorly for high values of f 0 for the same reason the variance of y a u y b y a y b u is expected to be smaller than the one of 2 y a y a u y b therefore the ia estimator of the first order sobol index should also perform better than sobol saltelli especially when f 0 is high compared to v y it is less obvious to infer whether τ s j 2 is higher or lower than τ i a 2 therefore this is investigated through numerical simulations in the next section 3 numerical examples it is worth noticing that the current estimators eqs 4 and 5 require n d 2 model calls to estimate the overall set of first and total order sobol indices while eqs 7 and 8 require 2n d 1 to ensure a fair comparison we set the sample size of the new estimators to half the one of the current estimators in this way the computational cost is 2n d 1 for the former and 2n d 2 for the latter this means that when we write that a sample of size n is used this refers to the actual size of the samples for the new estimators while the sample size is 2n for the current estimators in the following exercises we exclusively use the latin hypercube sampler lhs because it allows for the replication of the sobol indices estimate and furthermore it performs better than random sampling for the interested readers more intensive numerical exercises are undertaken in azzini and rosati 2021 with different sampling techniques and different estimators of first order sobol index 3 1 the ishigami function let us consider the following three dimensional function 14 f x 1 x 2 x 3 f 0 sin x 1 7 sin 2 x 2 0 1 x 3 4 sin x 1 where the input variables are independently and uniformly distributed over π π 3 as compared to the original ishigami function we introduce a constant parameter f 0 which has no impact on the variance of the function this simple function for which the exact sobol indices are known has the following features x 1 and x 3 interact strongly while x 2 is additively influential that is s 2 st 2 0 44 this allows to check whether as previously guessed we find s 2 i a s t 2 i a in this exercise we numerically compare the performances of eqs 4 and 5 with eqs 7 and 8 for this purpose we set n 64 which means 128 for the current estimators and we replicate one hundred estimates of the first and total order sobol indices with the estimators discussed in this paper each replicate is obtained as follows 1 generate x a and x b two independent lhs samples 2 run the model with each sample and collect y a f x a and y b f x b 3 for all input factors i 1 to d a generate x a i and also x b i for the ia estimators by switching the i th column of the x a sample matrix and the i th column of the x b sample matrix b run the model and collect y a i f x a i and y b i f x b i for the ia estimators c compute s i s s and s t i s j from eqs 4 and 5 with the sample set y a y b y a i or s t i i a and s i i a from eqs 7 and 8 with the sample set y a y b y a i y b i by setting u i for a fair comparison in step 1 the sample size is 2n for the current estimators and n for the new estimators 3 1 1 case 1 f 0 0 we first set f 0 0 the results are depicted in fig 1 which clearly shows that as far as the first order sobol indices are concerned the new estimator eq 8 provides more robust estimates than eq 4 thus confirming our comments in 2 4 notably the estimated first order sobol indices of x 3 can be smaller than zero which is not consistent with the theory sobol indices shall be within 0 1 this is due to its interaction with x 1 the new total order estimator that is eq 7 has slightly lower variances for st 1 and st 2 than eq 5 and conversely for st 3 fig 2 depicts s 2 versus s t 2 for both sets of estimators the current and new ones we can see that the pairs s 2 i a s t 2 i a spread along the line s 2 i a s t 2 i a contrarily to s 2 s s s t 2 s j this is also in accordance with our findings in 2 4 that s i i a s t i i a if x i does not interact with the other variables this is not the case with s 2 s s s t 2 s j actually for some replicates we even find s 2 s s s t 2 s j which is not consistent at all with the definition of first and total order sobol indices we stress that when x i has only an additive effect on the response s i i a s t i i a is independent of the sample size n this information can be obtained at any sample size even for n 1 this is also illustrated in fig 2 the red crosses were obtained with n 1 2 10 without replicate at these sample sizes the pairs s 2 i a s t 2 i a are also located along the diagonal s 2 i a s t 2 i a 3 1 2 case 2 f 0 100 this case illustrates the sensitivity of the current first order estimator to model responses with high expected value as compared with the total variance we set f 0 100 which yields 15 f x 1 x 2 x 3 100 sin x 1 7 sin 2 x 2 0 1 x 3 4 sin x 1 keeping in mind that the ishigami function has a total variance approximately equal to v y 13 84 one hundred lhs replicates of size n 64 are employed the results are displayed in fig 3 they show that while the shift in the ishigami function has no impact on the estimators of the total order estimators and on the new first order estimator namely eq 8 it significantly deteriorates the performance of the current first order estimator eq 4 when the variables highly interact with each other indeed on the top of fig 3 we can notice that s 2 s s is not affected this result is in line with our comments in section 2 4 one might propose to circumvent this issue by shifting the vectors of model responses by a factor μ e y before applying eq 4 this is indeed proposed in sobol and myshetskaya 2007 however by doing so one would introduce another degree of freedom in the estimator namely the value of μ that would vary from one replicate estimate to another unless it is left invariant such a solution might impact the bias or the variance of the estimator so defined as argued in owen 2012 n 1 n y n a u y n b y n a y n b u the numerator of eq 8 can be seen as a random shifting whereas a constant shifting namely n 1 n μ y n b y n a μ is proposed in sobol and myshetskaya 2007 the main finding of owen 2012 is that the former solution outperforms the latter for small first order sobol indices regarding the performance of the total order estimators it is not obvious to guess which one is the best a glance at the plot on the bottom of fig 3 reveals that the new estimator has lower variance for st 3 and higher or equal variances for the two others one might conclude that the new total order estimator is more accurate for high total order sobol indices we investigate this hypothesis further in the next numerical exercise 3 2 the g function in this exercise we study the performance of the two estimators of total order sobol index specifically we investigate whether the variance of the new estimator is always smaller than the current one or if it depends on the value of st i for this purpose we consider a ten dimensional function whose total order sobol indices of the input variables spread uniformly over 0 1 hence we consider the sobol g function defined as follows f x i 1 10 4 x i 2 a i a i 1 where x i u 0 1 for all i 1 10 and the coefficients are chosen as follows a 1 13 1 24 1 33 1 42 1 52 1 64 1 79 2 00 2 37 1 52 this choice approximately yields the following total order sobol indices 0 95 0 85 0 15 0 05 thus x 1 has the highest total order effect and x 10 the lowest using negative g function coefficients a is somewhat unusual but in our case provides a set of total order sobol indices that evenly spreads over 0 1 in this way we can investigate numerically whether the performance of the two estimators depends on the magnitude of the total order sobol index indeed the variance of some estimators may depend on the value of the targeted statistic while some may not the numerical setting is as follows we compute one hundred lhs replicate estimates of the total order sensitivity indices samples of size n 220 is employed 221 for the current estimator eq 5 in order to get accurate estimates with no overlapping of the ranges of variation for each estimate the asymptotic normal variances are evaluated by replacing in eqs 11 13 the exact sobol index i e st i and total variance i e v y by their estimated value the lhs replicates provide also the empirical variances which can be confronted to the asymptotic normal variances the one hundred estimates are depicted in fig 4 with the exact total order sobol indices despite of the very large sample size the ranges of variation of the high sobol indices estimates are rather large but do not overlap this indicates that the studied function is a very difficult one for the monte carlo estimators we note that the spread of the ia estimator for the total order sobol index is slightly narrower than the sj estimator in this example on the top of fig 5 we represent the estimated variance of the new estimator namely τ i a 2 versus the variance of the current estimator τ s j 2 because there are one hundred replicates of the sensitivity indices for each sensitivity index st i i 1 10 we have one hundred estimates of the asymptotic normal variances they are depicted in different coloured circles in the top plot on the bottom of fig 5 we represent the empirical estimated variances obtained by computing directly the variance of the one hundred lhs replicates of each total order sobol index first we can note that the y and x axes of the two plots bottom and top have the same ranges this indicates that the asymptotic variances eqs 13 11 are good proxies of the empirical variances for the function under study the continuous line in fig 5 represents τ i a 2 τ s j 2 the scatter plots located below this line mean that τ i a 2 τ s j 2 we observe that the scatter plots associated with the highest sensitivity indices namely from st 1 to st 4 are clearly below this line either for the asymptotic normal variances top or the empirical variances bottom this confirms that likewise the ishigami function the new estimator eq 7 is more accurate than eq 5 at least for high sensitivity indices say st i 0 55 of course this inference has been obtained numerically and cannot be generalised for the sake of completeness the results for the first order sobol indices are depicted in fig 6 we note that the first order sobol indices are virtually zero which confirm that this case is a very difficult one for global sensitivity analysis both estimators provide results rather centered on the true value although one hundred replicates might not be sufficient to provide stable results we can notice that for the first inputs x 1 x 4 which have the relatively highest first order effects the ranges of variation of the replicates of the sobol saltelli estimator are sligthly narrower than those produced by the ia estimator for inputs x 5 x 6 x 7 their results are rather similar and for x 8 x 9 x 10 with the smallest sensitivity indices the ia estimator provides narrower ranges of variation 4 gsa of a radiative forcing model 4 1 problem setting aerosol particles influence the earth s radiative balance directly by backscattering and absorption of solar radiation thus contributing to the global climate change charlson et al 1992 sokolik and toon 1996 radiative forcing models are developed to assess the impact of aerosols in the present work we study the direct forcing δf by sulfate aerosols in the analytical form provided by charlson et al 1992 16 δ f 1 2 s 0 1 a c t 2 1 r s 2 β ψ e f ψ e 3 q y l a where s 0 is the solar constant t is the transmittance of the atmospheric layer above the aerosol a c is the fractional cloud cover r s is the mean albedo β is the fraction of the radiation scattered upward by the aerosol ψ e is the mass scattering efficiency f ψ e is the scaling factor that takes into account the dependence of ψ e to the relative humidity q is the global input flux of anthropogenic sulfur y is the fraction of so2 oxydized to s o 4 2 l is the sulfate lifetime in the atmosphere and a is the area of the earth tatang et al 1997 the negative sign in eq 16 indicates that the forcing has a cooling effect the uncertainties associated with these parameters are taken from tatang et al 1997 and reported in table 1 the log normal distribution with geometric mean μ and geometric standard deviation σ is denoted ln μ σ if z is a standard normal variable that is z n 0 1 by setting x μ σ z yields x ln μ σ according to this transformation the lhs samples of the uncertain input parameters reported in table 1 have been generated to carry out the sensitivity analysis of the direct forcing model note that s 0 and a are treated as deterministic input parameters as they are known accurately therefore nine uncertain input variables are considered in this study i e d 9 samples of size n 1 000 have been chosen to perform the analysis as the model is given in an analytical form this corresponds to a total of n t 2n 9 1 20 000 model runs the aim of the analysis is to identify i the irrelevant input variables and ii possibly the inputs which do not interact with the other ones 4 2 results the results are reported in the last column of table 1 they correspond to the point estimate of the first and total order sobol indices at n 1 000 they are associated with their 95 uncertainty range obtained from the ia estimators variances as follows 1 96 σ i a for s i a and 1 96 τ i a for s t i a they show that t ψ e y l have first and total order sobol indices higher than 10 their estimated uncertainty ranges overlap which means that it cannot be inferred which parameter is the most important one the backscattered fraction β has a total order effect higher than 10 and a first order effect slightly lower the other variables have smaller sobol indices but with the exception of a c they cannot be neglected s t i i a 1 96 τ i a 0 05 actually the total order sobol index of each variable is virtually the double of the first order effect hence although the direct forcing model of sulfate aerosols has an apparent simple form it entails strong interactions between the variables s i i a 0 72 and only one of them can be deemed irrelevant namely a c fig 7 depicts the point estimates of the sobol indices versus the sample size up to 1 000 we can notice that the ia estimators require at least lhs samples of size n 730 to provide stable results which is quite much for such a low dimensional model d 9 this is due as previously discussed to the presence of strong interactions between the variables and the relatively high effective dimension of the model eight out of nine inputs have a total order effect greater than 5 in the denomination of kucherenko et al 2011 the direct forcing model of sulfate aerosols can be classified as a type c model which is a very difficult case for variance based global sensitivity analysis see kucherenko et al 2011 5 conclusions we have studied the properties of two mc estimators of the first and total order sobol indices their asymptotic variances have been derived under the asymptotic normality assumption the so called ia estimators possess interesting features one of these features is that the estimated first order sobol index is always smaller than or equal to the total order sobol index while forming a set of complementary formulas unlike the current estimators mostly in use by practitioners by analysing their asymptotic normal variances and by conducting numerical exercises we have shown that the new sampling strategy and its associated estimators perform better than the current sampling strategy originally introduced in saltelli 2002 the improvement is especially significant for the first order sobol index estimate hence if one wishes to estimate both the first and total order sobol indices by monte carlo integral approximations we recommend the use of the ia estimators and the associated sampling design declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a asymptotic normality of s u s s and s t u s j the law of large numbers ensures that the estimator s u s s in eq 4 is consistent that is lim n s u s s s u almost surely we denote by s u s s n the estimator for a sample size n in the sequel we follow the steps of janon et al 2014 to establish that the asymptotic normality of this estimator is 17 lim n n s u s s n s u n 0 σ s s 2 with σ s s 2 defined by eq 10 proof we set α n β n 2 y n a y n a u y n b y n a y n b 2 we also denote the associated random vector α β 2 y a y a u y b y a y b 2 since their statistics do not depend on n we then have α β lim n 1 n n 1 n α n β n 2 s u v y 2 v y and from eq 4 we can write s u φ α β α β the so called delta method van der vaart 2000 allows for evaluating the variance of the estimator as follows σ s s 2 g γ g t g φ α β with γ v α c o v α β c o v α β v β we find that g α β 1 β α β 2 g α β 1 2 v y s u 2 v y by accounting for the definition of α β above therefore we find that the variance of this estimator is 4 v y 2 σ s s 2 v α 2 s u c o v α β s u 2 v β which can be rearranged as follows 18 4 v y 2 σ s s 2 v α s u β replacing α β by their expression provides the announced result moreover by noticing that in eq 18 α is the numerator of eq 4 and β the denominator it is straightforward to demonstrate that the variance of estimator 5 is eq 11 this is merely established by setting α y a u y b 2 β remaining unchanged flushleft appendix b asymptotic normality of s u i a and s t u i a in the same way it can be established that the asymptotic normality of s u i a is 19 lim n n s u i a n s u n 0 σ i a 2 with σ i a 2 given by eq 12 proof from eq 8 we can write s u φ α β γ α β γ with α n β n γ n 2 y n b u y n a y n b y n a u y n a y n b 2 y n a u y n b u 2 which yields α β γ lim n 1 n n 1 n α n β n γ n 4 s u v y 2 v y 2 v y we also denote the associated random vector α β γ 2 y b u y a y b y a u y a y b 2 y a u y b u 2 since their statistics do not depend on n the so called delta method van der vaart 2000 yields σ i a 2 g γ g t g φ α β γ with γ v α c o v α β c o v α γ c o v α β v β c o v β γ c o v α γ c o v β γ v γ we find that g α β γ 1 β γ α β γ 2 α β γ 2 g α β γ 1 4 v y s u 4 v y s u 4 v y by accounting for the definition of α β γ above therefore we find that the variance of the estimator is 16 v y 2 σ i a 2 v α 2 s u c o v α β c o v α γ s u 2 v β 2 c o v β γ v γ v β γ which can be rearranged as follows 16 v y 2 σ i a 2 v α 2 c o v α s u β γ v s u β γ to finally give σ i a 2 v α s u β γ 16 v y 2 furthermore by replacing α β γ by their expression we find eq 12 by changing α β γ accordingly we establish the variance of s t i a as flushleft τ i a 2 v y a y b u 2 y b y a u 2 s t u y a y b 2 y a u y b u 2 16 v y 2 which is eq 13 
25770,this study compares the performances of two sampling based strategies for the simultaneous estimation of the first and total order variance based sensitivity indices a k a sobol indices the first strategy corresponds to the current approach employed by practitioners and recommended in the literature the second one was only recently introduced by the first and last authors of the present article both strategies rely on different estimators of first and total order sobol indices the asymptotic normal variances of the two sets of estimators are established and their accuracies are compared theoretically and numerically the results show that the new strategy outperforms the current one the global sensitivity analysis of the radiative forcing model of sulfur aerosols is performed with the new strategy the results confirm that in this model interactions are important and only one input variable is irrelevant keywords global sensitivity analysis variance based sensitivity indices first order sobol index total order sobol index asymptotic normality radiative forcing model 1 background uncertainty and sensitivity analyses are essential ingredients of modelling saltelli et al 2004 they allow to point out the key uncertain assumptions input factors that can be random variables or random fields responsible for the uncertainty into the model outcome of interest this is particularly relevant when models are used for decision making assessing model output uncertainty requires several runs of the model monte carlo simulations allow to carry out this task by sampling the input factors from their joint probability distribution and propagating the sample through the model response of interest i e running the model sensitivity analysis sa can then be undertaken to identify for instance the input factors mostly responsible for the uncertainty in the model response depending on the method used sa can be conducted directly from the monte carlo sample at hand i e the one generated to assess model output uncertainty or can require extra monte carlo simulations by following an appropriate sampling design the method to be used depends on the sensitivity indices also called importance measures that the analyst wants to compute as recommended in saltelli et al 2004 see also saltelli and tarantola 2002 the sensitivity indices to assess should be related to the question that sa is called to answer to the authors enumerate several questions called sa settings that can be addressed with the so called variance based sensitivity indices in the sequel we focus on the estimation of variance based sensitivity indices also called sobol indices sobol 1993 as eluded previously a monte carlo sample is required to carry out uncertainty analysis that is assessing the predictive uncertainty of the model output of interest we assume that there is only one scalar output denoted y f x the input factors are represented by a random vector of scalar variables x x 1 x d possibly grouped into two complementary vectors u v they are assumed independent of each other for the case of dependent inputs see for instance li et al 2010 kucherenko et al 2012 mara and tarantola 2012 mara et al 2015 li and rabitz 2017 tarantola and mara 2017 there exist several sobol indices called first order closed second order and so forth a closed second order sobol index and more generally a closed d th order sobol index can be defined as the first order sobol index of a group of two resp d inputs in the sequel we will use the term first and total order sobol indices whether they refer to an individual variable say x i or a group of variables e g u first and total order sobol indices are respectively defined as follows 1 s u v e y u v y 2 s t u e v y v v y where v stands for the unconditional variance operator resp v the conditional variance and e stands for the mathematical expectation resp e the conditional expectation the sobol indices range over 0 1 and st u s u eq 1 is the first order sobol index of the group of inputs u while eq 2 is the total order sobol index of u the higher the sobol indices the more the group of inputs u is important for the model response the difference between s u and st u is that the latter not only accounts for the amount of variance of y explained by the input variables within u like s u does but it also contains cooperative contributions due to the interactions between the variables in u with those in v therefore a noticeable result is that s u st v 1 let d 1 d be the number of elements in u s u represents in percentage the expected reduction in v y if the variables in u where fixed to their true value that is why the first order sensitivity indices of individual inputs i e d 1 are to be estimated if the goal of the sa is to identify the input variable that would induce the largest reduction in variance if its value was known accurately this sa setting is called factors prioritization instead if the goal is to identify the irrelevant inputs called screening analysis or factors fixing setting then the individual total order sobol indices are to be estimated indeed we note that if st u 0 the variables in u do not contribute at all to the variance of y more sa settings are discussed in saltelli and tarantola 2002 there are typically two direct methods to estimate the first and total order sobol indices the first one uses monte carlo methods e g sobol 1993 saltelli 2002 the second one casts the total variance onto orthogonal functions like the fourier expansion a k a the fourier amplitude sensitivity test cukier et al 1978 saltelli et al 1999 mara 2009 or the polynomial chaos expansion sudret 2008 blatman and sudret 2011 shao et al 2017 indirect methods employ surrogate models first also called metamodels to mimic the input output relationship and then often use one of the aforementioned direct methods to compute the sobol indices e g marseguerra et al 2003 estimating the sobol indices with monte carlo estimators is rather computationally expensive but it does not require any assumption except that the variance of f x be numerically tractable in the present work we study the performances of two monte carlo estimators of eq 1 and eq 2 respectively that rely on two different sampling designs the paper is organised as follows in section 2 we introduce the two sampling strategies as well as their associated monte carlo estimators to compute both the first and total order sobol indices their asymptotic normal variances derived in the appendices are also compared to each other in section 3 the performances of the two sets of estimators are compared through numerical exercises on well known benchmark functions the new set of estimators is applied to the radiative forcing model of sulfur aerosols in section 4 finally the key results of our work are summarized in section 5 2 monte carlo estimators 2 1 integral approximation when ilya m sobol introduced the variance based sensitivity indices in sobol 1993 he also proposed their monte carlo mc estimators indeed the sobol indices defined in eqs 1 and 2 are nothing but ratio of integrals approximating these integrals numerically provides estimates of the sobol indices monte carlo mc estimators rely on the fact that multidimensional integrals can be approximated via mc samples as follows 3 r d f q x p x x d x 1 n n 1 n f q x n 1 x n d where x p x meaning that p x is the joint probability density of x and x n x n 1 x n d is the n th out of n mc draw of the input factors sampled w r t p x in the sequel we assume that x is a vector of independent input variables that is p x i 1 d p x i x i where p x i x i is the marginal distribution of x i let y a y b y a u y b u be four distinct model output samples whose n th element for each of them is respectively defined as follows y n a f u n a v n a f x n a y n b f u n b v n b f x n b y n a u f u n a v n b f x n a u y n b u f u n b v n a f x n b u where x n a and x n b are two independent input vectors identically distributed as well as x n a u and x n b u the u values in vector x n a u resp x n b u are identical to those in x n a resp x n b while the v values are those of x n b resp x n a 2 2 current estimators the most popular sampling design to compute simultaneously the first and total order sensitivity indices as recommended by saltelli et al 2012 requires three samples namely y a y b y a u or equivalently y a y b y b u to compute the sensitivity indices of u their estimators are respectively defined as follows 4 s u s s 1 n n 1 n y n a y n a u y n b 1 2 n n 1 n y n a y n b 2 5 s t u s j 1 2 n n 1 n y n a u y n b 2 1 2 n n 1 n y n a y n b 2 note that we do not simplify these equations e g the 2n at the numerator and denominator cancel each other for the purpose of the discussion that just follows but latter on we will the superscript ss stands for sobol saltelli as the former derived the integral formulation of the numerator in sobol et al 2007 while saltelli proposed an estimator similar to the numerator of eq 4 in saltelli 2002 the superscript sj often refers to sobol jansen although one can date back the numerator of eq 5 to šaltenis and dzemyda 1982 and jansen et al 1994 see saltelli et al 2000 page 177 therefore sj can also be read šaltenis jansen the denominators of the previous formulas are identical but they differ from the one proposed in saltelli 2002 saltelli et al 2010 as defined the denominator of eqs 4 and 5 is an mc estimator of v y we find it convenient to formulate the denominator in this way because it highlights the symmetry between y a y b in the denominator eq 4 is known to provide an accurate estimate of small first order sensitivity indices sobol et al 2007 as eq 5 does for the total order sensitivity indices importantly although in theory st u s u the previous estimators do not satisfy this criterion indeed by noticing that v y 1 2 n n 1 n y n a y n b 2 at the denominator of eqs 4 and 5 is a positive scalar we find that 6 v y s t u s j s u s s 1 n n 1 n y n a u y n b 2 2 y n a y n a u y n b which because 2 y n a y n a u y n b can be either positive or negative does not ensure that s t u s j s u s s these observations advocate for a more symmetrical and coherent estimator for the first order sensitivity index this is the subject of the next subsection 2 3 new estimators as previously mentioned the denominator of eq 5 converges towards v y that is lim n 1 2 n n 1 n y n a y n b 2 lim n 1 2 n n 1 n y n a u y n b u 2 v y while the numerator is such that lim n 1 2 n n 1 n y n b y n a u 2 lim n 1 2 n n 1 n y n a y n b u 2 e v y v hence the following symmetrical estimator for the total order sensitivity index can be derived 7 s t u i a n 1 n y n b y n a u 2 y n a y n b u 2 n 1 n y n a y n b 2 y n a u y n b u 2 this is because as already mentioned x n a and x n b are two independent input vectors identically distributed as well as x n a u and x n b u notice the perfect symmetry of the formula which remains unchanged by switching the superscripts b and a incidentally the superscript ia stands indifferently for innovative algorithm and ivano azzini the first author of this article who initiated the work on these estimators azzini and rosati 2021 interchanging y a u y b u in eq 7 only changes the numerator and provides the estimator for s t v i a the law of total variance implies that s u i a s t v i a 1 therefore the first order sensitivity index s u is estimated as follows s u i a 1 s t v i a 1 n 1 n y n b y n b u 2 y n a y n a u 2 n 1 n y n a y n b 2 y n a u y n b u 2 which after some developments yields 8 s u i a 2 n 1 n y n a u y n b y n a y n b u n 1 n y n a y n b 2 y n a u y n b u 2 besides s t u i a s u i a n 1 n y n b y n a u 2 y n a y n b u 2 2 y n a u y n b y n a y n b u n 1 n y n a y n b 2 y n a u y n b u 2 which yields 9 s t u i a s u i a n 1 n y n b y n a u y n a y n b u 2 n 1 n y n a y n b 2 y n a u y n b u 2 0 and proves that s t u i a s u i a for any n eq 9 also shows that s t u i a s u i a if and only if f x is additive with respect to u in effect an additive function with respect to u and v reads in this case y f u v f 0 f u u f v v and it is straightforward to prove that the numerator of eq 9 equals zero for any sample size n it turns out that the numerator of eq 8 is very similar to the one proposed by owen in owen 2012 apart from the factor 2 due to the denominator the difference is the use by the author of y n c u f u n c v n a instead of y n a u or y n b u in eq 8 by doing so the symmetry of the estimator is lost interestingly lamboni 2020 also derived unbiased estimators with minimum variance of the non normalized sobol indices by leaning on the theory of u statistics see hoeffding 1948 gamboa et al 2021 his construction led to estimators exactly equal to the numerators of eqs 7 and 8 however neither lamboni in lamboni 2020 nor owen in owen 2012 paid attention to the estimation of the total variance the proof that s t u i a s u i a does not depend on the choice of the total variance estimator i e the denominator of eq 9 therefore the estimators proposed by lamboni in lamboni 2019 2020 also satisfy this property but they do not form a set of complementary formulas contrarily to the new estimators defined by eqs 7 and 8 also called the ia estimators in the sequel indeed thanks to the special choice of the total variance estimator i e the denominator of eqs 7 and 8 the ia estimators also satisfy the following equation s u i a s t v i a 1 to our best knowledge there are no other estimators of first and total order sensitivity indices that comply with both the equation s u s t v 1 and the inequation s u s t u 2 4 variances of the estimators the performance of an estimator is characterized by its bias and its variance for a given sample size n the sobol indices can be computed several times by re sampling the mc draws thus providing different estimates unbiased estimators provide replicates that on average yield the true values of the sobol indices estimators with small variances provide estimates that remain close to the true values of the sobol indices in the sequel the focus is on the variances of the estimators in appendices a and b we establish the variances of the estimators discussed in the present paper under the asymptotic normality assumption van der vaart 2000 janon et al 2014 they respectively read as follows 10 σ s s 2 v 2 y a y a u y b s u y a y b 2 4 v y 2 11 τ s j 2 v y a u y b 2 s t u y a y b 2 4 v y 2 and 12 σ i a 2 v 2 y a y b u y a u y b s u y a y b 2 y a u y b u 2 16 v y 2 13 τ i a 2 v y a y b u 2 y b y a u 2 s t u y a y b 2 y a u y b u 2 16 v y 2 in eqs 10 13 y a y b y a u and y b u are random variables in practice to compute the variances of the estimators they are replaced by their samples y a y b y a u and y b u see section 3 it can be qualitatively speculated that the sobol jansen estimator is more accurate than the one of sobol saltelli indeed we have according to sobol 1993 y f u v f 0 f u u f v v f u v u v this implies that y a u y b f u u b f u u a f u v u b v b f u v u a v b y a y b u f u u b f u u a f u v u b v a f u v u a v a therefore the variance of y a u y b 2 is expected to be smaller than the one of 2 y a y a u y b because the former does not contain neither f 0 nor f v contrarily to the latter with y a what is worse we guess that eq 4 may perform very poorly for high values of f 0 for the same reason the variance of y a u y b y a y b u is expected to be smaller than the one of 2 y a y a u y b therefore the ia estimator of the first order sobol index should also perform better than sobol saltelli especially when f 0 is high compared to v y it is less obvious to infer whether τ s j 2 is higher or lower than τ i a 2 therefore this is investigated through numerical simulations in the next section 3 numerical examples it is worth noticing that the current estimators eqs 4 and 5 require n d 2 model calls to estimate the overall set of first and total order sobol indices while eqs 7 and 8 require 2n d 1 to ensure a fair comparison we set the sample size of the new estimators to half the one of the current estimators in this way the computational cost is 2n d 1 for the former and 2n d 2 for the latter this means that when we write that a sample of size n is used this refers to the actual size of the samples for the new estimators while the sample size is 2n for the current estimators in the following exercises we exclusively use the latin hypercube sampler lhs because it allows for the replication of the sobol indices estimate and furthermore it performs better than random sampling for the interested readers more intensive numerical exercises are undertaken in azzini and rosati 2021 with different sampling techniques and different estimators of first order sobol index 3 1 the ishigami function let us consider the following three dimensional function 14 f x 1 x 2 x 3 f 0 sin x 1 7 sin 2 x 2 0 1 x 3 4 sin x 1 where the input variables are independently and uniformly distributed over π π 3 as compared to the original ishigami function we introduce a constant parameter f 0 which has no impact on the variance of the function this simple function for which the exact sobol indices are known has the following features x 1 and x 3 interact strongly while x 2 is additively influential that is s 2 st 2 0 44 this allows to check whether as previously guessed we find s 2 i a s t 2 i a in this exercise we numerically compare the performances of eqs 4 and 5 with eqs 7 and 8 for this purpose we set n 64 which means 128 for the current estimators and we replicate one hundred estimates of the first and total order sobol indices with the estimators discussed in this paper each replicate is obtained as follows 1 generate x a and x b two independent lhs samples 2 run the model with each sample and collect y a f x a and y b f x b 3 for all input factors i 1 to d a generate x a i and also x b i for the ia estimators by switching the i th column of the x a sample matrix and the i th column of the x b sample matrix b run the model and collect y a i f x a i and y b i f x b i for the ia estimators c compute s i s s and s t i s j from eqs 4 and 5 with the sample set y a y b y a i or s t i i a and s i i a from eqs 7 and 8 with the sample set y a y b y a i y b i by setting u i for a fair comparison in step 1 the sample size is 2n for the current estimators and n for the new estimators 3 1 1 case 1 f 0 0 we first set f 0 0 the results are depicted in fig 1 which clearly shows that as far as the first order sobol indices are concerned the new estimator eq 8 provides more robust estimates than eq 4 thus confirming our comments in 2 4 notably the estimated first order sobol indices of x 3 can be smaller than zero which is not consistent with the theory sobol indices shall be within 0 1 this is due to its interaction with x 1 the new total order estimator that is eq 7 has slightly lower variances for st 1 and st 2 than eq 5 and conversely for st 3 fig 2 depicts s 2 versus s t 2 for both sets of estimators the current and new ones we can see that the pairs s 2 i a s t 2 i a spread along the line s 2 i a s t 2 i a contrarily to s 2 s s s t 2 s j this is also in accordance with our findings in 2 4 that s i i a s t i i a if x i does not interact with the other variables this is not the case with s 2 s s s t 2 s j actually for some replicates we even find s 2 s s s t 2 s j which is not consistent at all with the definition of first and total order sobol indices we stress that when x i has only an additive effect on the response s i i a s t i i a is independent of the sample size n this information can be obtained at any sample size even for n 1 this is also illustrated in fig 2 the red crosses were obtained with n 1 2 10 without replicate at these sample sizes the pairs s 2 i a s t 2 i a are also located along the diagonal s 2 i a s t 2 i a 3 1 2 case 2 f 0 100 this case illustrates the sensitivity of the current first order estimator to model responses with high expected value as compared with the total variance we set f 0 100 which yields 15 f x 1 x 2 x 3 100 sin x 1 7 sin 2 x 2 0 1 x 3 4 sin x 1 keeping in mind that the ishigami function has a total variance approximately equal to v y 13 84 one hundred lhs replicates of size n 64 are employed the results are displayed in fig 3 they show that while the shift in the ishigami function has no impact on the estimators of the total order estimators and on the new first order estimator namely eq 8 it significantly deteriorates the performance of the current first order estimator eq 4 when the variables highly interact with each other indeed on the top of fig 3 we can notice that s 2 s s is not affected this result is in line with our comments in section 2 4 one might propose to circumvent this issue by shifting the vectors of model responses by a factor μ e y before applying eq 4 this is indeed proposed in sobol and myshetskaya 2007 however by doing so one would introduce another degree of freedom in the estimator namely the value of μ that would vary from one replicate estimate to another unless it is left invariant such a solution might impact the bias or the variance of the estimator so defined as argued in owen 2012 n 1 n y n a u y n b y n a y n b u the numerator of eq 8 can be seen as a random shifting whereas a constant shifting namely n 1 n μ y n b y n a μ is proposed in sobol and myshetskaya 2007 the main finding of owen 2012 is that the former solution outperforms the latter for small first order sobol indices regarding the performance of the total order estimators it is not obvious to guess which one is the best a glance at the plot on the bottom of fig 3 reveals that the new estimator has lower variance for st 3 and higher or equal variances for the two others one might conclude that the new total order estimator is more accurate for high total order sobol indices we investigate this hypothesis further in the next numerical exercise 3 2 the g function in this exercise we study the performance of the two estimators of total order sobol index specifically we investigate whether the variance of the new estimator is always smaller than the current one or if it depends on the value of st i for this purpose we consider a ten dimensional function whose total order sobol indices of the input variables spread uniformly over 0 1 hence we consider the sobol g function defined as follows f x i 1 10 4 x i 2 a i a i 1 where x i u 0 1 for all i 1 10 and the coefficients are chosen as follows a 1 13 1 24 1 33 1 42 1 52 1 64 1 79 2 00 2 37 1 52 this choice approximately yields the following total order sobol indices 0 95 0 85 0 15 0 05 thus x 1 has the highest total order effect and x 10 the lowest using negative g function coefficients a is somewhat unusual but in our case provides a set of total order sobol indices that evenly spreads over 0 1 in this way we can investigate numerically whether the performance of the two estimators depends on the magnitude of the total order sobol index indeed the variance of some estimators may depend on the value of the targeted statistic while some may not the numerical setting is as follows we compute one hundred lhs replicate estimates of the total order sensitivity indices samples of size n 220 is employed 221 for the current estimator eq 5 in order to get accurate estimates with no overlapping of the ranges of variation for each estimate the asymptotic normal variances are evaluated by replacing in eqs 11 13 the exact sobol index i e st i and total variance i e v y by their estimated value the lhs replicates provide also the empirical variances which can be confronted to the asymptotic normal variances the one hundred estimates are depicted in fig 4 with the exact total order sobol indices despite of the very large sample size the ranges of variation of the high sobol indices estimates are rather large but do not overlap this indicates that the studied function is a very difficult one for the monte carlo estimators we note that the spread of the ia estimator for the total order sobol index is slightly narrower than the sj estimator in this example on the top of fig 5 we represent the estimated variance of the new estimator namely τ i a 2 versus the variance of the current estimator τ s j 2 because there are one hundred replicates of the sensitivity indices for each sensitivity index st i i 1 10 we have one hundred estimates of the asymptotic normal variances they are depicted in different coloured circles in the top plot on the bottom of fig 5 we represent the empirical estimated variances obtained by computing directly the variance of the one hundred lhs replicates of each total order sobol index first we can note that the y and x axes of the two plots bottom and top have the same ranges this indicates that the asymptotic variances eqs 13 11 are good proxies of the empirical variances for the function under study the continuous line in fig 5 represents τ i a 2 τ s j 2 the scatter plots located below this line mean that τ i a 2 τ s j 2 we observe that the scatter plots associated with the highest sensitivity indices namely from st 1 to st 4 are clearly below this line either for the asymptotic normal variances top or the empirical variances bottom this confirms that likewise the ishigami function the new estimator eq 7 is more accurate than eq 5 at least for high sensitivity indices say st i 0 55 of course this inference has been obtained numerically and cannot be generalised for the sake of completeness the results for the first order sobol indices are depicted in fig 6 we note that the first order sobol indices are virtually zero which confirm that this case is a very difficult one for global sensitivity analysis both estimators provide results rather centered on the true value although one hundred replicates might not be sufficient to provide stable results we can notice that for the first inputs x 1 x 4 which have the relatively highest first order effects the ranges of variation of the replicates of the sobol saltelli estimator are sligthly narrower than those produced by the ia estimator for inputs x 5 x 6 x 7 their results are rather similar and for x 8 x 9 x 10 with the smallest sensitivity indices the ia estimator provides narrower ranges of variation 4 gsa of a radiative forcing model 4 1 problem setting aerosol particles influence the earth s radiative balance directly by backscattering and absorption of solar radiation thus contributing to the global climate change charlson et al 1992 sokolik and toon 1996 radiative forcing models are developed to assess the impact of aerosols in the present work we study the direct forcing δf by sulfate aerosols in the analytical form provided by charlson et al 1992 16 δ f 1 2 s 0 1 a c t 2 1 r s 2 β ψ e f ψ e 3 q y l a where s 0 is the solar constant t is the transmittance of the atmospheric layer above the aerosol a c is the fractional cloud cover r s is the mean albedo β is the fraction of the radiation scattered upward by the aerosol ψ e is the mass scattering efficiency f ψ e is the scaling factor that takes into account the dependence of ψ e to the relative humidity q is the global input flux of anthropogenic sulfur y is the fraction of so2 oxydized to s o 4 2 l is the sulfate lifetime in the atmosphere and a is the area of the earth tatang et al 1997 the negative sign in eq 16 indicates that the forcing has a cooling effect the uncertainties associated with these parameters are taken from tatang et al 1997 and reported in table 1 the log normal distribution with geometric mean μ and geometric standard deviation σ is denoted ln μ σ if z is a standard normal variable that is z n 0 1 by setting x μ σ z yields x ln μ σ according to this transformation the lhs samples of the uncertain input parameters reported in table 1 have been generated to carry out the sensitivity analysis of the direct forcing model note that s 0 and a are treated as deterministic input parameters as they are known accurately therefore nine uncertain input variables are considered in this study i e d 9 samples of size n 1 000 have been chosen to perform the analysis as the model is given in an analytical form this corresponds to a total of n t 2n 9 1 20 000 model runs the aim of the analysis is to identify i the irrelevant input variables and ii possibly the inputs which do not interact with the other ones 4 2 results the results are reported in the last column of table 1 they correspond to the point estimate of the first and total order sobol indices at n 1 000 they are associated with their 95 uncertainty range obtained from the ia estimators variances as follows 1 96 σ i a for s i a and 1 96 τ i a for s t i a they show that t ψ e y l have first and total order sobol indices higher than 10 their estimated uncertainty ranges overlap which means that it cannot be inferred which parameter is the most important one the backscattered fraction β has a total order effect higher than 10 and a first order effect slightly lower the other variables have smaller sobol indices but with the exception of a c they cannot be neglected s t i i a 1 96 τ i a 0 05 actually the total order sobol index of each variable is virtually the double of the first order effect hence although the direct forcing model of sulfate aerosols has an apparent simple form it entails strong interactions between the variables s i i a 0 72 and only one of them can be deemed irrelevant namely a c fig 7 depicts the point estimates of the sobol indices versus the sample size up to 1 000 we can notice that the ia estimators require at least lhs samples of size n 730 to provide stable results which is quite much for such a low dimensional model d 9 this is due as previously discussed to the presence of strong interactions between the variables and the relatively high effective dimension of the model eight out of nine inputs have a total order effect greater than 5 in the denomination of kucherenko et al 2011 the direct forcing model of sulfate aerosols can be classified as a type c model which is a very difficult case for variance based global sensitivity analysis see kucherenko et al 2011 5 conclusions we have studied the properties of two mc estimators of the first and total order sobol indices their asymptotic variances have been derived under the asymptotic normality assumption the so called ia estimators possess interesting features one of these features is that the estimated first order sobol index is always smaller than or equal to the total order sobol index while forming a set of complementary formulas unlike the current estimators mostly in use by practitioners by analysing their asymptotic normal variances and by conducting numerical exercises we have shown that the new sampling strategy and its associated estimators perform better than the current sampling strategy originally introduced in saltelli 2002 the improvement is especially significant for the first order sobol index estimate hence if one wishes to estimate both the first and total order sobol indices by monte carlo integral approximations we recommend the use of the ia estimators and the associated sampling design declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a asymptotic normality of s u s s and s t u s j the law of large numbers ensures that the estimator s u s s in eq 4 is consistent that is lim n s u s s s u almost surely we denote by s u s s n the estimator for a sample size n in the sequel we follow the steps of janon et al 2014 to establish that the asymptotic normality of this estimator is 17 lim n n s u s s n s u n 0 σ s s 2 with σ s s 2 defined by eq 10 proof we set α n β n 2 y n a y n a u y n b y n a y n b 2 we also denote the associated random vector α β 2 y a y a u y b y a y b 2 since their statistics do not depend on n we then have α β lim n 1 n n 1 n α n β n 2 s u v y 2 v y and from eq 4 we can write s u φ α β α β the so called delta method van der vaart 2000 allows for evaluating the variance of the estimator as follows σ s s 2 g γ g t g φ α β with γ v α c o v α β c o v α β v β we find that g α β 1 β α β 2 g α β 1 2 v y s u 2 v y by accounting for the definition of α β above therefore we find that the variance of this estimator is 4 v y 2 σ s s 2 v α 2 s u c o v α β s u 2 v β which can be rearranged as follows 18 4 v y 2 σ s s 2 v α s u β replacing α β by their expression provides the announced result moreover by noticing that in eq 18 α is the numerator of eq 4 and β the denominator it is straightforward to demonstrate that the variance of estimator 5 is eq 11 this is merely established by setting α y a u y b 2 β remaining unchanged flushleft appendix b asymptotic normality of s u i a and s t u i a in the same way it can be established that the asymptotic normality of s u i a is 19 lim n n s u i a n s u n 0 σ i a 2 with σ i a 2 given by eq 12 proof from eq 8 we can write s u φ α β γ α β γ with α n β n γ n 2 y n b u y n a y n b y n a u y n a y n b 2 y n a u y n b u 2 which yields α β γ lim n 1 n n 1 n α n β n γ n 4 s u v y 2 v y 2 v y we also denote the associated random vector α β γ 2 y b u y a y b y a u y a y b 2 y a u y b u 2 since their statistics do not depend on n the so called delta method van der vaart 2000 yields σ i a 2 g γ g t g φ α β γ with γ v α c o v α β c o v α γ c o v α β v β c o v β γ c o v α γ c o v β γ v γ we find that g α β γ 1 β γ α β γ 2 α β γ 2 g α β γ 1 4 v y s u 4 v y s u 4 v y by accounting for the definition of α β γ above therefore we find that the variance of the estimator is 16 v y 2 σ i a 2 v α 2 s u c o v α β c o v α γ s u 2 v β 2 c o v β γ v γ v β γ which can be rearranged as follows 16 v y 2 σ i a 2 v α 2 c o v α s u β γ v s u β γ to finally give σ i a 2 v α s u β γ 16 v y 2 furthermore by replacing α β γ by their expression we find eq 12 by changing α β γ accordingly we establish the variance of s t i a as flushleft τ i a 2 v y a y b u 2 y b y a u 2 s t u y a y b 2 y a u y b u 2 16 v y 2 which is eq 13 
25771,urbanization typically leads to erosion and instability in rivers and many management and restoration strategies have been developed to dampen the worst impacts stream power defined as the rate of energy expenditure in a river is a promising metric for analyzing cumulative effects in this paper we describe a spatial decision support system called the stream power index for networks spin toolbox that can be used to assess urban river stability at a watershed scale the objectives of the paper are to a describe the toolbox algorithms and procedures and b demonstrate the utility of the approach spin is written in python and packaged as an arcgis toolbox the toolbox combines existing landscape analysis algorithms with new algorithms to model river confluences channel sinuosity and threshold sediment particle sizes data can also be ingested from a standard hydraulic model two case studies demonstrate use of the toolbox to i anticipate current morphology ii predict urban morphologic change and iii analyze the benefits for stormwater management and channel restoration scenarios on channel stability keywords sediment erosion urban hydrology spatial decision support systems cumulative risk assessment 1 introduction urbanization is occurring at an unprecedented rate with a significant global effect on natural ecosystems paul and meyer 2001 poff et al 2006 walsh et al 2012 for rivers the transformation of land cover from rural to urban may be more pervasive than any other human modification chin 2006 particularly in smaller watersheds where any climate effects will be amplified by urbanization ashmore and church 2001 the addition of impervious surfaces and efficient conveyance of runoff by drainage infrastructure fundamentally alters the water balance in a watershed and drives a set of changes to the physical chemical and ecological processes of a river that together are referred as the urban stream syndrome bernhardt and palmer 2007 hawley and vietz 2016 vietz et al 2014 walsh et al 2005 different stream management strategies have been used to provide hydrologic and sediment control to treat various symptoms of this syndrome with varying degrees of success avellaneda and jefferson 2020 bledsoe and watson 2001 walsh et al 2016 strategies can include centralized stormwater management swm practices such as retention ponds and wetlands low impact development lid practices such as downspout disconnection green roofs and infiltration trenches and stream restoration techniques such as reconstruction of riffle pool bedforms and removal of dams however poor outcomes can result despite their implementation bledsoe 2002 burns et al 2012 jefferson et al 2017 and streams typically continue to experience erosion and degradation booth and jackson 1997 hancock et al 2010 due to increased discharges exceeding sediment entrainment thresholds navratil et al 2013 tillinghast et al 2011 with the myriad of techniques available and the varying drainage areas or channel lengths over which they may be applied it is challenging for researchers and water managers to understand the cumulative impact of urbanization on stream networks and the marginal change that can be expected from local modifications to hydrologic routing and channel morphology anim et al 2019 bell et al 2020 bernhardt et al 2005 booth and fischenich 2015 li et al 2017 roy et al 2008 wohl et al 2005 erosion and degradation resulting from imbalances in sediment transport in urban streams have been widely demonstrated bevan et al 2018 booth 1990 hammer 1972 hawley et al 2020 o driscoll et al 2009 trimble 1997 the characteristic instability is thought to be related most directly to increases in total and peak discharges q and changes in channel gradient s as a consequence of channelization and changes to channel sinuosity both q and s contribute to stream power ω which represents the rate of expenditure of potential energy in a river and can be used to anticipate reach scale channel adjustments church and ferguson 2015 lane 1955 schumm 1969 where reach is used to indicate a length of river over which a useful average morphology can be related to channel processes and responses despite smaller scale variability over replicating features such as riffles and pools montgomery and buffington 1997 in the last two decades many studies have confirmed the importance of ω as a metric to estimate sediment transport ferguson 2005 lammers and bledsoe 2018 parker et al 2011 petit et al 2005 model hydraulic geometry discriminate between channels and floodplains with different alluvial patterns kleinhans and van den berg 2011 nanson and croke 1992 phillips and desloges 2015 van den berg 1995 and predict spatial patterns of aggradation and degradation knighton 1999 lague 2014 reinfelds et al 2004 yochum et al 2017 stream power analysis is now widely applied at the watershed scale to identify areas of potential morphologic change and channel instability alber and piégay 2017 bawa et al 2014 bizzi and lerner 2015 jain et al 2006 lea and legleiter 2016 phillips and desloges 2014 soar et al 2017 tucker and hancock 2010 vocal ferencevic and ashmore 2012 stream power analysis of river networks at a watershed scale has been spurred by the development of spatial analysis tools using digital elevation models dems a dem is a raster with each pixel representing a square unit of area and characterized by a numeric elevation value dems are a valuable source of information on the form of drainage networks on a land surface and modern technology has made them more widely available and cheaper to obtain at higher resolutions watershed analysis tools such as archydro maidment 2002 and taudem arcgis tarboton 2005 make it easy to use dems to characterize terrain morphology and automatically extract watershed boundaries and channel networks more specialized toolboxes such as tak forte and whipple 2019 schwanghart and kuhn 2010 v bet gilbert et al 2016 o brien et al 2019 fldpln williams et al 2013 fluvial corridor roux et al 2015 and st ream parker et al 2015 have also been developed to extract topographic features related to river valleys and channels and perform more advanced analyses fluvial corridor for example is a multi scale tool that can extract valley bottom widths and reach metrics such as channel sinuosity roux et al 2015 while st ream assesses the potential for stream channel adjustment by calculating streamwise gradients in specific stream power parker et al 2015 despite the rapid development of toolboxes for spatial analysis of stream networks there is currently no tool available specifically adapted for urban river management the goal of this research was to develop a spatial decision support system to help assess the cumulative impact of land use change on urban river stability the developed system called the stream power index for networks spin toolbox can be added to a widely used geographic information system arcgis environmental systems research institute esri 2016 spin allows the user to calculate stream power based indices to investigate the temporal and spatial sensitivity of streams to urbanization stormwater management practices and restoration the objectives of this submission are to a describe the toolbox algorithms and procedures and b demonstrate the utility of the approach using two case studies the overall approach is empirically based since variables such as discharge and channel width are estimated from the drainage area but the flexibility of the approach is increased by providing tools to ingest output files from the u s army corps of engineers hydrologic engineering center s river analysis system hec ras which means that measured data where available can be used to constrain the outputs the spatial associations between specific stream power and surficial geology fisheries management zones and urban land developments can be used to both retrospectively understand existing impacts and anticipate areas of erosion and morphologic change in watersheds slated for future development the expectation is that this toolbox will inform storm water and river management strategies by allowing impacts of urban land developments to be considered at a watershed scale the manuscript is organized as follows the theoretical background for the stream power and channel parameter models used in the developed toolbox are presented in section 2 0 the structure and capabilities of the toolbox are presented in section 3 0 as part of the toolbox channel slope width and threshold bed particle sizes are calculated and these parameters are compared with field data for two small watersheds in section 4 0 field results are used to drive a discussion on the impact of urbanization on stream stability as a demonstration of the utility of the results in section 5 0 a set of brief conclusions are presented in section 6 0 2 background in rivers the moving water exerts a force on the channel bed which is can be expressed for reach averaged conditions in quasi steady flow conditions as the bed shear stress τ o n m2 1 τ o ρ g r h s where ρ is the density of water kg m3 g is the gravitational acceleration 9 81 m s2 and r h is the hydraulic radius defined as the ratio of the cross sectional area a to the wetted perimeter p the dimensionless shear stress τ is a key scaling parameter for modeling sediment transport thresholds and rates parker 2008 shields 1936 2 τ τ o ρ s ρ g d where ρ s is the density of the sediment and d is a representative bed particle diameter many of the most commonly applied sediment transport equations are based on a function of the difference between τ and a dimensionless shear stress threshold τ c parker 2008 equation 2 can be simplified assuming ρ s ρ ρ 1 6 and approximating r h by the channel depth y which is typical of many wide natural rivers so that 3 τ y s 1 6 d expressed in this way equation 3 illustrates the dependency of τ on y and d both of these terms are challenging to model at watershed scale without extensive field calibration ferguson 2005 which makes shear stress based models of channel erosion difficult to parameterize at a watershed scale an alternative approach was described by bagnold 1968 and 1980 based on the stream power ω which is defined as 4 ω ρ g q s whereand q is the discharge m3 s ω can be expressed per unit area of the channel by dividing by the channel width w to obtain the specific stream power ω 5 ω ω w by combining equations 4 and 5 we can obtain 6 ω ρ g q s w equations 4 and 6 illustrate the dependency of stream power estimates on q and w unlike the difficult to obtain information about y and d that is required for equation 3 these parameters are more readily estimated at a watershed scale for example q can be obtained from hydrologic modelling or by assuming an empirical relation with the drainage area galster et al 2006 knighton 1999 phillips and desloges 2014 w is frequently measured as part of surveys for flood modelling readily estimated based on hydraulic geometry relations with q ferguson 1986 parker et al 2007 and possible to measure directly from aerial imagery galster et al 2008 while s is readily extracted from dems both ω and ω can then be used to interpret and predict channel morphology bed erosion and sediment transport because ω is a function of the total discharge q it better represents the total bed material load and adjustments to a channel s hydraulic geometry w and y on the other hand ω is a function of the specific discharge q w which defines the local intensity of energy expenditure against the channel boundary and is relevant for estimating both the threshold at which sediment is entrained and the rate of bedload transport per unit width of a channel sediment transport is a highly non linear process with rapid increases in transport rate above a threshold or critical specific stream power ω c which is defined as the stream power magnitude just sufficient to induce transport of the particles sitting on the surface of the channel bed bagnold 1980 it is thus important for the interpretation of stream power results to understand how the current size of bed material compares with transport thresholds ferguson 2005 revisited the bagnold approach by combining equation 1 with the logarithmic flow resistance equation keulegan 1938 and a hiding function for bed sediment distributions andrews 1983 to obtain an equation for the critical stream power required to entrain a particle of size d i ω ci expressed as 7 ω c i 2 30 κ ρ τ c b ρ g d b 3 2 log 30 τ c b ρ e m s d i d b 1 b d i d b 3 2 1 b where κ is the von karman coefficient for velocity profiles d b and m are the surface grain diameter and empirical multiplier respectively that together represent the bed roughness height i e k s m d b e is the base of the natural logarithm and b is a hiding factor to account for the interactions between mixed grain sizes by setting d i d b to model the particle size that represents the bed roughness and by assuming that this particle is stable at bankfull condition i e ω c i ω b f equation 7 can be rearranged to obtain a model for a roughness threshold particle size d b c 8 d b c 1 τ c b ρ g κ ω b f 2 30 ρ log 30 τ c b ρ log e m s 2 3 expressed in this way the measured bed material size d b can be compared with a threshold condition d b c that would result in a static bed while there is continued debate in the literature on the best formulation for a relation between ω and d parker et al 2011 phillips and desloges 2014 lammers and bledsoe 2018 gilbert and wilcox 2020 the above relation was adopted because it was suitable for modeling the coarse fraction of the size distribution which is the most appropriate for modelling flow roughness in gravel bed rivers ferguson 2007 lópez and barragán 2008 and appears to be critical for understanding bed degradation mackenzie et al 2018 stream power is fundamental to physical processes and morphology of rivers continuous mapping of the relevant variables within a network should thus provide insights into river morphology and a variety of toolboxes have been developed for this purpose in urbanizing areas there is a further need to consider the changes in power that will result from changes to land cover direct modifications to the channel and this is the gap that spin is intended to fill 3 spin toolbox description 3 1 introduction the spin toolbox was written in python for arcgis version 10 3 x environmental systems research institute esri 2016 python was chosen because it does not require a licence and the developed script pyt can integrate other codes for spatial analysis and geoprocessing such as those available in the arcgis python library arcpy the arcgis environment was chosen because it is widely used in industry and academia and it provides many of the essential components of a spatial decision support system sdss including interface management data management model management knowledge management and multi linear problem solving environment matthies et al 2007 it also has significant existing functionality including archydro which was used for some steps in spin in section 3 2 we present a quick overview of the rationale calculation procedure required inputs and typical outputs parameter estimation is described in section 3 3 3 2 program function the spin toolbox was conceived as a series of discrete steps to ensure that users can provide intelligent support to the model based knowledge encoded in the scripts at each step new point line features representing locations within stream networks may be created or new attributes of those features may be determined for subsequent steps the user would typically feed results from previous steps however the discreteness of each step means that the user can interrupt the analysis to further investigate and verify the quality of the intermediate data as needed the independence of each step also allows the model s parameters to be modified and to facilitate continuous development of the toolbox a total of nine ordered tools are included in the toolbox table 1 each of the tools are outlined briefly here a full description of each of the steps is available in the user s guide supplemental information the first three tools analyze a digital elevation model dem to 1 identify the river channels 2 find the channel segments defined as lines between the centers of adjacent raster cells in the stream network and 3 calculate segment and reach averaged slope s these steps are similar to what is available in other dem analysis tools such as taudem tarboton 2005 the novelty of the current toolbox lies in tools 4 through 9 which allows the determination of stream power indices for various land use scenarios tool 4 calculates stream power for a pre development condition which we refer to as the rural watershed scenario following the approach of annable 1996 a threshold bed particle size d b c is modelled during this step using equation 8 where information on urban development is available in the form of land use polygons the impact of urbanization on discharge and stream power can be calculated using tools 5 and 6 a vector shapefile containing land parcels representing land use type and their associated imperviousness is required for these tools where hydrologic information is available in the form of discharge estimates embedded in a hecras hydraulic model the impact of urbanization and other stormwater management strategies can be calculated using tools 7 9 an output file of the hecras model that gives flow information at river stations csv and a point shapefile containing the river stations found in the model csv are required as inputs for this latter set of tools a set of optional algorithms is also available to allow customization of the slope smoothing algorithm identify the watershed outlet point and update land use polygons with new information on imperviousness 3 3 parameter estimation 3 3 1 channel network and slope channel networks bed elevations and slopes are extracted from a hydrologically enforced digital elevation model edem hydrological enforcement refers to the process of correcting raw dems where pixel elevations may not represent stream elevations accurately and in urban areas where culverts and sewers can allow water to flow against topographical gradients if an edem is not available a dem can be hydrologically conditioned with an enhanced stream network using plug in tools such as archydro maidment 2002 and taudem tarboton 2005 the process for delineating stream networks from an edem is a widely used method by o callaghan and mark 1984 in spin the drainage area a contributing to each cell is calculated using the flow accumulation tool in archydro maidment 2002 and a minimum threshold is set for a to define the stream network the stream cells are linked to each other and their link magnitude is identified using the shreve 1966 method to allow differentiation of the location of the stream tributaries with respect to a watershed outlet segment slope is calculated as the difference in elevation divided by length for each segment following the horizontal slice method bizzi and lerner 2015 jordan and fonstad 2005 in rivers a common measure of the curviness of the river is called the channel sinuosity p written as 9 p l c h l v where l c h is the length of the channel measured along the centerline and l v is the length of the valley measured along the centerline of the valley in a completely straight channel p 1 applied to a raster image the algorithms for calculating the flow path will result in a series of straight lines connecting grid cell centers where the resolution of the dem is coarse relative to the size of the river these segments tend to follow the valley more than the channel which means that the calculated flow path length l p is typically less than the actual l c h s will be overestimated if it is calculated simply as δ z l p which will also overestimate ω and ω to correct for this error we modelled p as 10 p p p where p is the grid measured sinuosity l p l v and p is the missing or sub grid sinuosity l c h l p in this way the true slope of the channel s can be calculated as 11 s δ z p l p where p is a user supplied corrective factor that is calculated by comparing the calculated value for l p with a value for l c h derived from data outside the model such values can be readily obtained from aerial photography a topographical map or surveyed data local slope estimates from an edem raster file are typically highly variable particularly where the grid cells are relatively large because the single recorded elevation for each grid cell masks heterogeneity at the sub grid scale the application of the spin tool in contrast was conceived as a reach level analysis of stream power and stability for this reason some smoothing of local slope is required to obtain representative reach averaged values for s in spin s is smoothed by averaging it over a set length of the channel centered on the grid cell of interest channel confluences are a challenge for smoothing algorithms because major tributaries can increase the water discharge and sediment loads such that the slope of the main branch shows a discontinuity at the confluence rice and church 1998 for minor tributaries however no change in slope in the main channel should be expected to differentiate between these two cases the spin toolbox calculates the relative contribution of the smaller basin a a t r i b a m a i n and sets a threshold default of a c 0 1 so that smoothing is only applied if a a c 3 3 2 discharge bankfull discharge q b f is commonly used to represent the most effective flow for geomorphic work emmett and wolman 2001 wolman and miller 1960 which makes it the most appropriate for modelling channel instability at a watershed scale for the purposes of this analysis q b f is assumed to be approximately equal to the mean annual flood event in rural channels q 2 and the stream power analysis assumes that q q 2 q b f as a first order approximation a power law relationship can be used to estimate this discharge q for undeveloped watersheds as 12 q a a b where a is the drainage area km2 and a and b are coefficients derived from statistical regression of data jain et al 2006 knighton 1999 lea and legleiter 2016 magilligan 1992 this approach is also built into landscape adjustment models where the vertical rate of channel adjustment is assumed to be proportional to the channel slope and the drainage area to an exponent that varies between 0 and 1 lague 2014 for urban scenarios many studies have shown that discharge is sensitive to impervious land cover imperviousness has therefore been used to predict the potential for flooding arnold and gibbons 1996 hatt et al 2004 shuster et al 2005 a practical approach for this level of analysis is to add the relative imperviousness of the land area ia as a modifying factor to regional empirical relationships between a and q bledsoe and watson 2001 ward et al 2015 so that 13 q c a d i a e where i a is the imperviousness of the land area 1 100 and coefficients c d and e are derived from statistical regression of data in spin c and d are set equal to the coefficients a and b equation 12 respectively to ensure continuity in predictions of q as land cover changes from pervious to impervious i a is then calculated for a given watershed based on a shapefile containing land use polygons with impervious values either supplied directly or obtained from a look up table of standard values for any given watershed the total i a is then calculated as the weighted average of all grid cell values a second approach to urban discharge was developed for spin using outputs from the u s army corps of engineers hydrologic engineering center s river analysis system hecras this second approach requires a completed hydraulic model of the channel but does have the advantage of being sensitive to hydrologic differences between urban areas i e changes in connectivity or stormwater management and can be adapted to test for the effects of in channel modifications and restoration hecras models are also widely used for urban flood modelling as part of a hecras model it is necessary to include discharge estimates which are typically derived from hydrologic simulations intended to replicate flows at different return periods with q 2 frequently included as one of the modelled discharges differences in urban hydrology can thus be captured in these discharge inputs in channel differences between channels can be captured because the flow conveyed between the channel banks hecras variable name q c h a n n e l is considered separately from flow over the floodplain only the in channel portion is responsible for shear stress exerted on a channel bed so the tool sets q q c h a n n e l for the 2 year return period flow urban channels are frequently disconnected from their floodplain and one restoration approach is to reconnect the channel to the floodplain to reduce stresses on the channel bed during floods so this refinement is intended to allow the benefit of this approach to be tested once ingested spatial information about the hecras node locations is read from a supplied shape file the river stations are joined to the closest stream segments and q values are applied to downstream cells until the next river station with a new value for q 3 3 3 channel width two methods are available in spin to calculate the channel width w the first method is based on hydraulic geometry relations that model width as a power function of q in rural areas where q can be modelled as a power function of a equation 12 w can also be written as 14 w f a g where the coefficients f and g are derived from statistical regression of data knighton 1999 leopold and wolman 1957 parker et al 2007 given the sensitivity to these models to differences in runoff coefficients morphology and bank strength such models should be verified for the region and watershed land cover where they are being applied the second method to obtain w follows from the hecras based method to obtain estimates of q hecras models require a channel geometry file that specifies station elevation data at a series of cross sections once a simulation has been run the outputs will include a corresponding wetted total width hecras variable name t o p w i d t h and wetted channel width hecras variable name t o p w c h n l to focus on the in channel portion of the channel width as was done for q spin sets w t o p w c h n l for the stream power analysis again as was done for q the relatively sparse hecras values of w were joined to the closest stream segments and then interpolated through the whole network 4 parameter validation 4 1 stream width slope and particle size in headwater field sites 4 1 1 site description data availability and parameterization the spin toolbox was tested on two headwater streams in ontario canada the watersheds are located close to the city of toronto where there are concerns about the impacts of urbanization on erosion as urban development expands maps of stream power are useful in this context to understand current or potential areas of stream erosion and to assess the benefits of storm water management field measurements of channel slope width and bed particle size at the sites were gathered as part of a larger study bevan et al 2018 cain and macvicar 2020 papangelakis et al 2019 an edem with a spatial resolution of 30 m was available through land information ontario lio 2019 higher resolution topographical information may be available for different jurisdictions but this edem was hydrologically enforced by the government mapping unit lio and made available for the majority of the province of ontario the low resolution dataset was used here to represent the minimum that is likely to be available in most jurisdictions the streams are small relative to the grid size of the edem w 10 m so empiricism is required to estimate their hydraulic geometry flow gauges with long periods of records were not available at these locations the two streams on which the model was tested differ primarily based on their degree of urbanization ganatsekaigon creek a 13 1 km2 is a tributary of duffins creek in pickering ontario land use is predominantly agricultural 58 with approximately 40 natural cover cain and macvicar 2020 papangelakis et al 2019 surficial geology consists of glacial till that is predominantly a mixture of sand and silt though stone lines are common sharpe et al 1997 the stream is considered to be important cold water habitat for fish and has populations of trout and redside dace clinostomus elongatus an endangered species in canada trca 2004 trca 2018 urban development is planned for much of the watershed over the next 10 years and polygons of future land use parcels and their associated land use type were digitized from watershed planning maps provided by the local watershed management authority toronto and region conservation authority trca land cover imperviousness by type was assumed to follow the hydrologic modeling guidelines published by the city of toronto 2006 no hecras model for the future urban land use scenario was available wilket creek in toronto ontario a 15 5 km2 is a tributary of the west don river in an area where the surface geology is similar to that of ganatsekaigon creek sharpe et al 1997 the main difference between the two watersheds is that the wilket creek watershed is 100 urbanized papangelakis et al 2019 trca 2015 development occurred between 1930 and 1970 in an era when the stormwater management infrastructure was designed to convey water quickly off the land much of the length of tributaries and main stem of the creek were buried in pipes during urbanization barr 2017 the remaining downstream open channel portion of the creek is characterized by erosion of the banks and valley walls channel enlargement and extensive stabilization and channel redesign projects bevan et al 2018 trca 2015 a hec ras model of wilket creek is maintained by the trca discharge values entered into the hecras model include design storms ranging from 2 to 100 year events based on a visual otthymo model of watershed hydrology trca 2015 existing land use data was also available from historical aerial photos based on the analysis of bevan et al 2018 so that again assuming that imperviousness follows the municipal modelling guidelines city of toronto 2006 the two methods for computing urban stream power could be compared regional models of discharge in rural channels was applied based on the empirical study of phillips and desloges for southern ontario 2014 for equation 12 they found a 0 25 and b 0 91 sample size n 210 r 2 0 86 the model of channel width from phillips and desloges 2014 tended to underestimate width for the sites investigated so the results from annable 1996 for the same region were applied where values of f 2 69 and g 0 36 in equation 14 were determined by combining empirical expressions for q as a function of a and w as a function of q sample size n 47 for the urban condition equation 13 no regional information was available and it was assumed that c a 0 25 and d b 0 91 equations 12 and 13 which matches the rural watersheds and that a typical exponent of e 0 30 could be applied from studies in other regions bledsoe and watson 2001 ward et al 2015 for the slope smoothing algorithm a moving window of 500 m in the streamwise direction was used based on a sensitivity assessment of the minimum distance required to remove high frequency variability in the slope tributaries were considered small i e not large enough to interrupt the smoothing algorithm if they represented less than 10 of the watershed from the larger tributary see section 3 3 1 4 1 2 stream network and sinuosity despite the relatively low spatial resolution of the edem spin mapped the position of the channel with reasonable accuracy fig 1 over the open channel portion of wilket creek for example the calculated position of the channel closely matched field observation fig 1a discrepancies were typically on the order of one or two pixels larger exceptions occur where the field survey was incomplete for e g at x 2600 m y 600 m where a construction project limited access to the site and a reach with high sinuosity centered at x 1000 m y 2100 m where p 1 65 for a 1 5 km reach bevan et al 2018 the sinuous reach helps to demonstrate the problem that necessitated the consideration of sub grid scale sinuosity p downstream from the highly sinuous reach centerline distance from 0 to 3 0 km p 1 30 bevan et al 2018 which is relatively close to the grid sinuosity for the whole of wilket creek p 1 18 for this reason the dem line though it is more stepped in appearance due to the coarseness of the dem largely follows the increasing elevation in the channel fig 1b however the highly sinuous reach missed by the dem analysis means that dem streamwise distances diverge from the field survey in the upper portion of the creek fig 1b applying the sub grid scale correction to the whole channel p 1 14 rotates the dem profile so that the overall channel slope is correct fig 1b the correction can locally increase the error in longitudinal profile however as shown for the lower reaches of the channel where the local p 0 correct estimates of s will therefore require reach level variation of p which can be done within the spin toolbox the importance of the smoothing algorithm is demonstrated in fig 1c where initial estimates of s show high magnitude variability and occasional negative values as a result of the coarse edem the smoothing algorithm erases these irregularities such that the edem derived slope varies much more slowly and in the range of the field measured average value of 0 85 a higher resolution field dataset was available for the stream profile of one reach of ganatsekaigon creek which largely confirmed the above observations errors of 1 2 pixels in the position of the creek were common fig 2 a the edem extracted channel had a shorter length than the field surveyed channel again due to the limitations of the 30m resolution of the edem fig 2b but the slope was estimated accurately using equation 11 for ganatsekaigon the field surveyed results showed that p 1 49 but p 1 14 from the available edem so that p 1 29 this high value for p reflects the small size of the channel relative to the edem resolution local slope estimates were variable due to the coarse steps in the estimates of channel elevation that result from the coarse edem resolution but the smoothing algorithm reduced the variation resulting in overall slope between 1 4 and 2 0 which matched the field derived slope of 1 60 fig 2c 4 1 3 width available field measured channel widths were compared with the rural annable 1996 model equation 14 including three reaches from each of ganatsekaigon creek and wilket creek fig 3 wilket creek data are top of bank measurements from a 1958 survey for a sanitary sewer pipe installation bevan et al 2018 which captured the channel in a relatively early stage of urbanization error bars represent the standard error of the mean obtained from 3 7 and 8 cross sections respectively in the three major reaches defined based on slope breaks in the long profile outliers thought by bevan et al 2018 to have been modified prior to the 1958 survey were not included for ganatsekaigon creek reach 1 represents top of bank measurements for 15 cross sections made by cain 2019 while reach 2 and 3 represent similar measurements for 19 and 23 sections respectively made by papangelakis 2019 as shown the modelled relation from annable 1996 tended to underpredict the field measurements with a couple of exceptions the generally narrow modelled values may reflect a methodological difference as the annable 1996 was focused on the determination of bankfull hydraulic geometry and used a set of bankfull indicators while the measured values were determined solely based on the floodplain elevation 4 1 4 particle size the model for the threshold particle size was compared with field estimates from the two study sites where multiple bed pebble counts had been carried out as part of previous studies for wilket creek pebble counts were comprised of a minimum of 200 particles and equally sampled all parts of the bed following bunte et al 2009 and harrelson et al 1994 for ganatsekaigon creek 16 estimates of the particle size distribution on riffles were made with a minimum of 100 particles cain 2019 in both cases particles less than 2 mm in diameter were recorded separately i e as fine gravel sand bare till or fines and not counted in the field tally to ensure a minimum sample size of the coarse particles particle sizes and the difference between measured and modelled values were calculated using a krumbein ϕ scale 1951 modified as described by parker and andrews 1985 15 ϕ log 2 d d o where d o is a reference diameter 1 mm to ensure dimensional consistency all field estimates of d 84 were compared with d b c for the two sites to assess the utility of the model for understanding the variation of bed material size fig 4 a 1 1 line of agreement and guide lines at 1 ϕ are included to help the presentation of results for ganatsekaigon the modelled d b c values are relatively tightly clustered between 6 6 5 ϕ while the field measured d 84 values are spread out between 5 7 ϕ such variability in the field results is expected given the smaller sample sizes for the ganatsekaigon sites n 100 particles and within reach variability from localized inputs of coarse bank material and scalloping of the long profile as described by cain and macvicar 2020 the variability of d b c is limited by the relatively coarse mesh and slope smoothing distance of 500 m which represents most of the reach length shown in fig 2 the key result for this analysis is thus that the measured d 84 range straddles the 1 1 line and is generally bounded by the 1 ϕ guidelines which demonstrates that this channel could be considered a threshold or supply limited channel for this type of channel an increase in stream power will likely increase sediment transport and degradation in contrast the bed material samples from wilket creek which come from larger samples that are intended to be reach averaged rather than local samples all show that d b c d 84 1 ϕ when using the rural value of ω b f but that d b c is bounded by the 1 ϕ guidelines when the urban value of ω b f is used in equation 8 this indicates that the channel is either a type of inherited understeepened channel as described by phillips and desloges 2015 or that the bed has coarsened as a result of urbanization understeepened channels can occur in southern ontario because coarse bed material has a glacial legacy and sediment transport rates are low however wilket creek is known to have undergone significant enlargement bevan et al 2018 barr 2017 and the degree of possible coarsening agrees with expectations that the bed material size is controlled by the threshold condition to give a broader context for the model of bed particle size threshold d b c was also compared with the 33 streams with gravel to cobble sized bed substrate 2 m m d 84 256 m m from the database prepared by annable 1996 for rural streams in southern ontario this database contains field estimates of s w and sediment particle sizes d 50 and d 84 and estimates of manning s coefficient q 2 and q b f from water survey of canada gauging records as shown d b c effectively models the lower limit of the field based d 84 estimates fig 5 a many of the sites fall within the 1 ϕ guide lines but for many other streams d 84 d b c 1 ϕ including some where d 84 d b c 4 ϕ to understand how the channel morphology may influence the relation between the transport threshold and the field measurements we plotted the difference d 84 d b c against s and w fig 5b and c as shown the channels with large differences all have s 1 and tend to be wider fig 5c this result indicates that narrow and steep channels tend to have bed material that is close to the threshold particle size while wider and lower sloped channels can have bed particle sizes that exceed what would be expected from a threshold analysis 5 model results and interpretation it is envisioned that spin will be used for three primary purposes a desktop analysis of likely current morphology b prediction of morphologic change as a consequence of urban development and c stormwater management and channel restoration scenario analysis to illustrate the first application maps of ω were generated at the watershed scale for ganatsekaigon and wilket creeks figs 6 and 7 respectively the rural ganatsekaigon map fig 6a shows that stream power peaks in two locations within the watershed the first is near the watershed outlet surveyed reach g3 was located in this area where high power 75 ω 100 w m 2 is driven by high discharge from the entire watershed a second peak in a similar power range appears in each of the main tributaries surveyed reach g1 was located in this area on west branch where high power is driven by locally high slopes s shaped profiles have been observed in other rivers in southern ontario where headwaters are on low relief till plains middle reaches incise through the till and lower reaches flatten out near the lakeshore phillips and desloges 2014 vocal ferencevic and ashmore 2012 similar patterns also appear on the remaining open channel portion of the rural wilket creek fig 7a where ω increases at the transition from w2 to w3 regional studies of morphology are useful for interpreting such results in the southern ontario region for example phillips and desloges 2015 coupled a principal component analysis with a clustering procedure to describe floodplain archetypes ω emerged as the most important fluvial process variable largely agreeing with previous studies such as the seminal work of nanson and croke 1992 the clustering procedure was used to identify four floodplain archetypes as ω ranged from 5 to 10 w m 2 for clay to 60 100 w m 2 for cobble dominated floodplains with coarser beds and reduced floodplain thickness as shown in the map of ganatsekaigon creek fig 6a ω 100 w m 2 for the rural scenario which indicates that the floodplain types largely fit within the ω range used to develop the regional model phillips and desloges 2015 available descriptions of morphology of the system generally agree with this understanding in particular cain and macvicar 2020 describe reach g1 as a coarse bedded channel with coarse eroding banks which stands in stark contrast to the channel a few hundred meters upstream which is characterized by lower slopes and finer sediments within wilket creek ω is also 100 w m 2 for the rural condition fig 7a though it is not possible to confirm the floodplain archetypes given the altered state of the catchment but a sharp transition from ω 100 w m 2 to 150 w m 2 is modelled at the transition from reach w2 to w3 in wilket creek in the urban condition fig 7b and c bevan et al 2018 observed a clear morphologic transition in this area with reaches w2 characterized as meandering river with an abundance of sand and fine gravel sediment and w3 as a coarse gravel cobble river though a complete morphologic assessment of the creeks is beyond the scope of the current analysis this rough agreement between morphologic descriptions and stream power indicates that spin results can be used to anticipate real differences in morphology the second anticipated application of spin is that it would be used to predict morphologic changes as a consequence of urban development in ganatsekaigon creek for example the additional flows from the urban areas could more than double ω after development fig 6b pushing some areas up over 200 n m2 while similar increases have already been realized in wilket creek fig 7b while it is generally understood that the increase in ω will influence bed particle size sediment transport and channel stability it is not immediately clear what specific changes can be expected to guide the interpretation a ω vs d 84 phase diagram was prepared using classification ranges and morphologic thresholds from the literature fig 8 the critical particle size equation 8 is shown as a narrow range due to the slight dependency of the threshold on s the range shown represents channels with s 0 5 1 5 which is typical for wilket and ganatsekaigon creeks also shown are bar dynamics thresholds developed by kleinhans and van den berg 2011 who used a large database of river morphology and bar theory to a refine the discriminator between dominantly braiding and meandering systems at 16 d 84 2 2 ω 900 2 4 and b identify a transition between meandering rivers characterized by scroll bars and moderately braiding meandering systems where chute bars become more common 17 d 84 2 2 ω 285 2 4 both of the above relations include a relation for equivalent roughness expression d 84 2 2 d 50 lópez and barragán 2008 to model the particle size used to represent the bed roughness approximate stream power and particle size ranges are also shown for the phillips and desloges 2015 floodplain archetypes similar to equations 16 and 17 the expression for equivalent roughness was applied to estimate the d 84 ranges from the d 50 ranges given by phillips and desloges 2015 measured values from the database of annable 1996 are also shown for comparison the mobility threshold can be used to interpret channel coarsening as a response to increased stream power the database compiled by annable 1996 shows that the d 84 mobility threshold is a lower bound on bed particle sizes for gravel and cobble bedded channels in southern ontario which means that coarse bedded rural channels are typically either threshold or understeepened in this region the spin results show that the maximum ω 100 w m 2 in the rural condition and 250 w m 2 in the urban condition for both wilket and ganatsekaigon creeks figs 6 and 7 the currently rural ganatsekaigon creek has beds with d 84 128 mm which is close to the current mobility threshold in the urban condition i e if ω increases to 250 w m 2 the current bed will fall below the mobility threshold the apparent response to a similar change in wilket creek has been coarsening such that d 84 remains above the mobility threshold the spin results could thus be used to anticipate bed material coarsening in ganatsekaigon following equation 8 where the core relation is d 84 ω 2 3 another consideration is that there may be a natural limit to bed coarsening based on sediment supply in the available databases there were few examples with d 84 256 mm low supply of material larger than this size means that there exists a narrowing triangle of conditions where d 84 d c i as ω increases where ω 500 w m 2 d c i exceeds 256 mm and channels within this power range would presumably be at risk for more profound channel adjustments that could involve removal of the alluvial cover which has been observed in urban rivers in this region vocal ferencevic and ashmore 2012 bevan et al 2018 morphologic transitions are modelled to cross the phase space following the general relation of d 84 ω 2 4 kleinhans and van den berg 2011 which means that it is possible for channels to cross multiple thresholds as they are urbanized as shown the phillips and desloges 2015 floodplain archetypes and the majority of the annable 1996 sites fit within the meandering range which are generally characterized by scroll bar dynamics kleinhans and van den berg 2011 such descriptions match with the descriptions of lateral accretion for the s and m archetypes which have higher sand content than both the c and b types while the low power environment of the c type is described by vertical accretion of fine sediments and a thick floodplain phillips and desloges 2015 the high power b type environment are described as having a thinner floodplain and low accretion rates while sitting close to the morphologic transition where chute bars become more prevalent fig 8 ganatsekaigon creek appears to straddle this transition and the mobility threshold relation which would indicate higher rates of chute bar formation and sediment transport these implications are supported by field observations including moderate braiding within the study reach cain and macvicar 2020 relatively high sediment transport rates cain et al 2020 and erosion of the inner bank following a chute bar cutoff morphology during a high transport event papangelakis et al 2019 the urban wilket creek is almost entirely within the area of the phase diagram characterized by a higher prevalence of chute bars and relatively unstable lateral morphologies field observations of a channel avulsion in wilket creek by bevan et al 2018 fits with this description where the avulsion occurred the power was high enough to move even the coarsest particles such that the underlying till was exposed the till itself is relatively easily eroded pike et al 2018 and degradation of the bed occurred bevan et al 2018 more examples are needed but these field studies show spin results can be used to anticipate changes in bar dynamics and lateral instability as a result of urbanization the third anticipated application of spin is that it would be used for scenario analysis involving stormwater management or local stream modifications to illustrate this potential it is useful to first consider the upper portion 1 75 km length of wilket creek fig 9 empirical relations indicate that q is relatively constant over this stream distance with an expected rural discharge of 2 3 m3 s fig 9a estimates using the land use polygons more than double these values to between 6 and 7 m3 s but the hecras model maintained by the watershed authorities indicates that the in channel flow during the 2 year flood is higher still at 15 m3 s this value drops around a low head weir in the channel where some of the flow is forced out onto the floodplain slope is variable and generally varies between 0 6 and 1 0 with the higher values near the upstream limit fig 9b in the empirical rural and urban land use approaches w is nearly constant due to the relatively small change in drainage area over this length of the channel but the hecras model has a variable w from a minimum downstream of the weir that is approximately equal to the empirical estimate to a maximum of nearly 50 m upstream of the weir fig 9c the variable q and w in the hecras model mean that ω also varies more than with the empirical approaches ranging from a minimum that is below the rural value upstream of the weir where the channel is wide to a maximum of ω 200 n m2 downstream of the weir in the narrowest cross section the results show a good correspondence with the field study by bevan et al 2018 who found that sediment deposition occurs upstream of the weir but that the area downstream of the weir is prone to erosion the spin hec ras approach is suited to scenario analysis because it can be linked with hydrologic models and capture local variability in ω associated with channel morphology a series of potential channel morphology hydrology modifications and the impacts on ω for the upper portion of wilket creek are shown in fig 10 for this approach a set of feasible modifications were proposed including a constant q of 10 m3 s which could be achieved either by improving the stormwater management within the watershed to reduce the peak or by redesigning the channel floodplain so that more flow is conveyed over the channel floodplain similar to what occurs now close to the low head weir a 20 increase of p which could be achieved by reconstructing the current relatively straight planform to follow a meandering path and a constant w of 10 m which could be achieved by redesigning the channel section such that the width is higher but the flow depth is lower than the average for the current channel taken separately each of the modifications will have a proportional impact on the specific stream power fig 10 application of all of the proposed modifications together i e a wider channel with increased sinuosity and reduced channel flows reduces ω to 50 n m2 which is 50 higher than the rural values as shown but within the range of natural m or b type morphologies for the region despite the urban character of the watershed keeping ω within the natural range of regional variability would likely result in more stable channel morphologies consistent values for q c h a n n e l and w will reduce the streamwise variability of ω which would be beneficial for sediment continuity and reduce maintenance costs a recent study of a form based restoration project within wilket creek showed that despite the stability of the channel within the project reach the project had led to upstream aggradation and downstream degradation papangelakis and macvicar 2020 spin results could be used to anticipate these problems and support a process based approach to stream restoration such that sediment continuity is maintained users of the spin toolbox should be aware of the limitations of the available tools first the results are largely dependant on the dem being used higher resolution dems are commonly available and may offer better detail on slope variation which would eliminate the need to account for the subgrid sinuosity p in urban areas however watercourses are frequently crossed by roads and embankments which means that raw elevation data from a lidar or other survey must be preprocessed by burning the streams into a dem before it can be used to delineate the channels and estimate s this process of hydrologic enforcement significantly influences the results and has been the subject of considerable research lindsay 2016 campforts and govers 2015 but is beyond the scope of the current investigation instead a relatively coarse dem was selected because the pre processing of the dem was completed by the mapping authority for the region and was found to be suitable future work could examine the trade offs between dem resolution hydrologic enforcement and the estimation of s in spin a second limitation of spin is that river channels may be influenced by in channel wood or bedrock outcrops in channel wood for example can increase the storage of sediment and reduce the effective channel slope so that alluvial channels occur where bedrock channels might otherwise be expected montgomery et al 1996 welling et al 2021 such effects are not captured by the model the empiricism of spin is also a limitation particularly for estimates of q and w in the rural model and the urban land use approach for the reaches with available field data w appeared to be underestimated by equation 14 fig 3 while the q2 included in the hecras model provided by the watershed authority was almost double the estimate from equation 13 fig 9a clearly then the empirical results do not necessarily provide locally precise estimates of ω at a watershed scale other factors such as vegetation or bank protection measures can strongly influence bank stability and w eaton and millar 2004 tal and paola 2007 while urban values of q can be expected to vary considerably as a function of the nature age and connectivity of the impervious parcels of land rather than precise estimates the empirical approaches allow for a rapid and high level desktop analysis to visualize the qualitative patterns of ω at a watershed scale field morphologic assessments are still necessary to verify model assumptions and tie the morphologic estimates with morphologic types where data is available the spin hecras option allows users to capture real variability of width and discharge with this option various development and restoration scenarios can be analyzed from a stream power perspective 6 conclusions the spin toolbox has been developed to assist decision makers responsible for protecting and rehabilitating streams affected by the urban stream syndrome based on an analysis of stream power the toolbox is available as an add on to the widely used arcgis environment which allows both the magnitude and degree of change that can be expected as a result of urbanization to be easily visualized and analyzed within a catchment the toolbox can be used based entirely on watershed topography and land use polygons to anticipate current morphology from a desktop analysis and predict morphologic change as a consequence of urban development or it can be used in tandem with a hydraulic hecras model of the channels to analyze possible stormwater management and channel restoration scenarios application of the toolbox to two watersheds demonstrated its effectiveness at using a dem to delineate and describe a stream network channel width slope and bed particle size can be reasonably predicted stream power can also be used to anticipate channel morphology and understand channel evolution in response to urbanization future work should more widely apply the toolbox to test its utility for understanding morphologic change and scenario analysis for the management of urban river systems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the toronto region conservation authority helped in the initial development of the spin toolbox participated in grant writing and shared data from the example watersheds matrix environmental solutions shared field data for both sites and financially supported some of the data collection work credit valley conservation supported the work through funding for grant applications and offered feedback on toolbox development vernon bevan aryn cain margot chapuis julian krick and elli papangelakis collected the field data that was used to test the toolbox funding was provided through a strategic partnership grant stpgp 463321 14 from the national science and engineering research council of canada nserc author contributions k g developed and coded the toolbox b m and p a conceived of the research and supported toolbox development k g and b m performed the research and wrote the paper and k g b m and p a edited the paper we are grateful to the editors j desloges and one anonymous reviewer for detailed and constructive comments on an earlier version of the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105185 software availability name of software stream power index for networks spin developer kimisha ghunowa year first available 2021 hardware required none software required arcgis availability github in the macvicab spin repository cost none program language python program size 213 kb 
25771,urbanization typically leads to erosion and instability in rivers and many management and restoration strategies have been developed to dampen the worst impacts stream power defined as the rate of energy expenditure in a river is a promising metric for analyzing cumulative effects in this paper we describe a spatial decision support system called the stream power index for networks spin toolbox that can be used to assess urban river stability at a watershed scale the objectives of the paper are to a describe the toolbox algorithms and procedures and b demonstrate the utility of the approach spin is written in python and packaged as an arcgis toolbox the toolbox combines existing landscape analysis algorithms with new algorithms to model river confluences channel sinuosity and threshold sediment particle sizes data can also be ingested from a standard hydraulic model two case studies demonstrate use of the toolbox to i anticipate current morphology ii predict urban morphologic change and iii analyze the benefits for stormwater management and channel restoration scenarios on channel stability keywords sediment erosion urban hydrology spatial decision support systems cumulative risk assessment 1 introduction urbanization is occurring at an unprecedented rate with a significant global effect on natural ecosystems paul and meyer 2001 poff et al 2006 walsh et al 2012 for rivers the transformation of land cover from rural to urban may be more pervasive than any other human modification chin 2006 particularly in smaller watersheds where any climate effects will be amplified by urbanization ashmore and church 2001 the addition of impervious surfaces and efficient conveyance of runoff by drainage infrastructure fundamentally alters the water balance in a watershed and drives a set of changes to the physical chemical and ecological processes of a river that together are referred as the urban stream syndrome bernhardt and palmer 2007 hawley and vietz 2016 vietz et al 2014 walsh et al 2005 different stream management strategies have been used to provide hydrologic and sediment control to treat various symptoms of this syndrome with varying degrees of success avellaneda and jefferson 2020 bledsoe and watson 2001 walsh et al 2016 strategies can include centralized stormwater management swm practices such as retention ponds and wetlands low impact development lid practices such as downspout disconnection green roofs and infiltration trenches and stream restoration techniques such as reconstruction of riffle pool bedforms and removal of dams however poor outcomes can result despite their implementation bledsoe 2002 burns et al 2012 jefferson et al 2017 and streams typically continue to experience erosion and degradation booth and jackson 1997 hancock et al 2010 due to increased discharges exceeding sediment entrainment thresholds navratil et al 2013 tillinghast et al 2011 with the myriad of techniques available and the varying drainage areas or channel lengths over which they may be applied it is challenging for researchers and water managers to understand the cumulative impact of urbanization on stream networks and the marginal change that can be expected from local modifications to hydrologic routing and channel morphology anim et al 2019 bell et al 2020 bernhardt et al 2005 booth and fischenich 2015 li et al 2017 roy et al 2008 wohl et al 2005 erosion and degradation resulting from imbalances in sediment transport in urban streams have been widely demonstrated bevan et al 2018 booth 1990 hammer 1972 hawley et al 2020 o driscoll et al 2009 trimble 1997 the characteristic instability is thought to be related most directly to increases in total and peak discharges q and changes in channel gradient s as a consequence of channelization and changes to channel sinuosity both q and s contribute to stream power ω which represents the rate of expenditure of potential energy in a river and can be used to anticipate reach scale channel adjustments church and ferguson 2015 lane 1955 schumm 1969 where reach is used to indicate a length of river over which a useful average morphology can be related to channel processes and responses despite smaller scale variability over replicating features such as riffles and pools montgomery and buffington 1997 in the last two decades many studies have confirmed the importance of ω as a metric to estimate sediment transport ferguson 2005 lammers and bledsoe 2018 parker et al 2011 petit et al 2005 model hydraulic geometry discriminate between channels and floodplains with different alluvial patterns kleinhans and van den berg 2011 nanson and croke 1992 phillips and desloges 2015 van den berg 1995 and predict spatial patterns of aggradation and degradation knighton 1999 lague 2014 reinfelds et al 2004 yochum et al 2017 stream power analysis is now widely applied at the watershed scale to identify areas of potential morphologic change and channel instability alber and piégay 2017 bawa et al 2014 bizzi and lerner 2015 jain et al 2006 lea and legleiter 2016 phillips and desloges 2014 soar et al 2017 tucker and hancock 2010 vocal ferencevic and ashmore 2012 stream power analysis of river networks at a watershed scale has been spurred by the development of spatial analysis tools using digital elevation models dems a dem is a raster with each pixel representing a square unit of area and characterized by a numeric elevation value dems are a valuable source of information on the form of drainage networks on a land surface and modern technology has made them more widely available and cheaper to obtain at higher resolutions watershed analysis tools such as archydro maidment 2002 and taudem arcgis tarboton 2005 make it easy to use dems to characterize terrain morphology and automatically extract watershed boundaries and channel networks more specialized toolboxes such as tak forte and whipple 2019 schwanghart and kuhn 2010 v bet gilbert et al 2016 o brien et al 2019 fldpln williams et al 2013 fluvial corridor roux et al 2015 and st ream parker et al 2015 have also been developed to extract topographic features related to river valleys and channels and perform more advanced analyses fluvial corridor for example is a multi scale tool that can extract valley bottom widths and reach metrics such as channel sinuosity roux et al 2015 while st ream assesses the potential for stream channel adjustment by calculating streamwise gradients in specific stream power parker et al 2015 despite the rapid development of toolboxes for spatial analysis of stream networks there is currently no tool available specifically adapted for urban river management the goal of this research was to develop a spatial decision support system to help assess the cumulative impact of land use change on urban river stability the developed system called the stream power index for networks spin toolbox can be added to a widely used geographic information system arcgis environmental systems research institute esri 2016 spin allows the user to calculate stream power based indices to investigate the temporal and spatial sensitivity of streams to urbanization stormwater management practices and restoration the objectives of this submission are to a describe the toolbox algorithms and procedures and b demonstrate the utility of the approach using two case studies the overall approach is empirically based since variables such as discharge and channel width are estimated from the drainage area but the flexibility of the approach is increased by providing tools to ingest output files from the u s army corps of engineers hydrologic engineering center s river analysis system hec ras which means that measured data where available can be used to constrain the outputs the spatial associations between specific stream power and surficial geology fisheries management zones and urban land developments can be used to both retrospectively understand existing impacts and anticipate areas of erosion and morphologic change in watersheds slated for future development the expectation is that this toolbox will inform storm water and river management strategies by allowing impacts of urban land developments to be considered at a watershed scale the manuscript is organized as follows the theoretical background for the stream power and channel parameter models used in the developed toolbox are presented in section 2 0 the structure and capabilities of the toolbox are presented in section 3 0 as part of the toolbox channel slope width and threshold bed particle sizes are calculated and these parameters are compared with field data for two small watersheds in section 4 0 field results are used to drive a discussion on the impact of urbanization on stream stability as a demonstration of the utility of the results in section 5 0 a set of brief conclusions are presented in section 6 0 2 background in rivers the moving water exerts a force on the channel bed which is can be expressed for reach averaged conditions in quasi steady flow conditions as the bed shear stress τ o n m2 1 τ o ρ g r h s where ρ is the density of water kg m3 g is the gravitational acceleration 9 81 m s2 and r h is the hydraulic radius defined as the ratio of the cross sectional area a to the wetted perimeter p the dimensionless shear stress τ is a key scaling parameter for modeling sediment transport thresholds and rates parker 2008 shields 1936 2 τ τ o ρ s ρ g d where ρ s is the density of the sediment and d is a representative bed particle diameter many of the most commonly applied sediment transport equations are based on a function of the difference between τ and a dimensionless shear stress threshold τ c parker 2008 equation 2 can be simplified assuming ρ s ρ ρ 1 6 and approximating r h by the channel depth y which is typical of many wide natural rivers so that 3 τ y s 1 6 d expressed in this way equation 3 illustrates the dependency of τ on y and d both of these terms are challenging to model at watershed scale without extensive field calibration ferguson 2005 which makes shear stress based models of channel erosion difficult to parameterize at a watershed scale an alternative approach was described by bagnold 1968 and 1980 based on the stream power ω which is defined as 4 ω ρ g q s whereand q is the discharge m3 s ω can be expressed per unit area of the channel by dividing by the channel width w to obtain the specific stream power ω 5 ω ω w by combining equations 4 and 5 we can obtain 6 ω ρ g q s w equations 4 and 6 illustrate the dependency of stream power estimates on q and w unlike the difficult to obtain information about y and d that is required for equation 3 these parameters are more readily estimated at a watershed scale for example q can be obtained from hydrologic modelling or by assuming an empirical relation with the drainage area galster et al 2006 knighton 1999 phillips and desloges 2014 w is frequently measured as part of surveys for flood modelling readily estimated based on hydraulic geometry relations with q ferguson 1986 parker et al 2007 and possible to measure directly from aerial imagery galster et al 2008 while s is readily extracted from dems both ω and ω can then be used to interpret and predict channel morphology bed erosion and sediment transport because ω is a function of the total discharge q it better represents the total bed material load and adjustments to a channel s hydraulic geometry w and y on the other hand ω is a function of the specific discharge q w which defines the local intensity of energy expenditure against the channel boundary and is relevant for estimating both the threshold at which sediment is entrained and the rate of bedload transport per unit width of a channel sediment transport is a highly non linear process with rapid increases in transport rate above a threshold or critical specific stream power ω c which is defined as the stream power magnitude just sufficient to induce transport of the particles sitting on the surface of the channel bed bagnold 1980 it is thus important for the interpretation of stream power results to understand how the current size of bed material compares with transport thresholds ferguson 2005 revisited the bagnold approach by combining equation 1 with the logarithmic flow resistance equation keulegan 1938 and a hiding function for bed sediment distributions andrews 1983 to obtain an equation for the critical stream power required to entrain a particle of size d i ω ci expressed as 7 ω c i 2 30 κ ρ τ c b ρ g d b 3 2 log 30 τ c b ρ e m s d i d b 1 b d i d b 3 2 1 b where κ is the von karman coefficient for velocity profiles d b and m are the surface grain diameter and empirical multiplier respectively that together represent the bed roughness height i e k s m d b e is the base of the natural logarithm and b is a hiding factor to account for the interactions between mixed grain sizes by setting d i d b to model the particle size that represents the bed roughness and by assuming that this particle is stable at bankfull condition i e ω c i ω b f equation 7 can be rearranged to obtain a model for a roughness threshold particle size d b c 8 d b c 1 τ c b ρ g κ ω b f 2 30 ρ log 30 τ c b ρ log e m s 2 3 expressed in this way the measured bed material size d b can be compared with a threshold condition d b c that would result in a static bed while there is continued debate in the literature on the best formulation for a relation between ω and d parker et al 2011 phillips and desloges 2014 lammers and bledsoe 2018 gilbert and wilcox 2020 the above relation was adopted because it was suitable for modeling the coarse fraction of the size distribution which is the most appropriate for modelling flow roughness in gravel bed rivers ferguson 2007 lópez and barragán 2008 and appears to be critical for understanding bed degradation mackenzie et al 2018 stream power is fundamental to physical processes and morphology of rivers continuous mapping of the relevant variables within a network should thus provide insights into river morphology and a variety of toolboxes have been developed for this purpose in urbanizing areas there is a further need to consider the changes in power that will result from changes to land cover direct modifications to the channel and this is the gap that spin is intended to fill 3 spin toolbox description 3 1 introduction the spin toolbox was written in python for arcgis version 10 3 x environmental systems research institute esri 2016 python was chosen because it does not require a licence and the developed script pyt can integrate other codes for spatial analysis and geoprocessing such as those available in the arcgis python library arcpy the arcgis environment was chosen because it is widely used in industry and academia and it provides many of the essential components of a spatial decision support system sdss including interface management data management model management knowledge management and multi linear problem solving environment matthies et al 2007 it also has significant existing functionality including archydro which was used for some steps in spin in section 3 2 we present a quick overview of the rationale calculation procedure required inputs and typical outputs parameter estimation is described in section 3 3 3 2 program function the spin toolbox was conceived as a series of discrete steps to ensure that users can provide intelligent support to the model based knowledge encoded in the scripts at each step new point line features representing locations within stream networks may be created or new attributes of those features may be determined for subsequent steps the user would typically feed results from previous steps however the discreteness of each step means that the user can interrupt the analysis to further investigate and verify the quality of the intermediate data as needed the independence of each step also allows the model s parameters to be modified and to facilitate continuous development of the toolbox a total of nine ordered tools are included in the toolbox table 1 each of the tools are outlined briefly here a full description of each of the steps is available in the user s guide supplemental information the first three tools analyze a digital elevation model dem to 1 identify the river channels 2 find the channel segments defined as lines between the centers of adjacent raster cells in the stream network and 3 calculate segment and reach averaged slope s these steps are similar to what is available in other dem analysis tools such as taudem tarboton 2005 the novelty of the current toolbox lies in tools 4 through 9 which allows the determination of stream power indices for various land use scenarios tool 4 calculates stream power for a pre development condition which we refer to as the rural watershed scenario following the approach of annable 1996 a threshold bed particle size d b c is modelled during this step using equation 8 where information on urban development is available in the form of land use polygons the impact of urbanization on discharge and stream power can be calculated using tools 5 and 6 a vector shapefile containing land parcels representing land use type and their associated imperviousness is required for these tools where hydrologic information is available in the form of discharge estimates embedded in a hecras hydraulic model the impact of urbanization and other stormwater management strategies can be calculated using tools 7 9 an output file of the hecras model that gives flow information at river stations csv and a point shapefile containing the river stations found in the model csv are required as inputs for this latter set of tools a set of optional algorithms is also available to allow customization of the slope smoothing algorithm identify the watershed outlet point and update land use polygons with new information on imperviousness 3 3 parameter estimation 3 3 1 channel network and slope channel networks bed elevations and slopes are extracted from a hydrologically enforced digital elevation model edem hydrological enforcement refers to the process of correcting raw dems where pixel elevations may not represent stream elevations accurately and in urban areas where culverts and sewers can allow water to flow against topographical gradients if an edem is not available a dem can be hydrologically conditioned with an enhanced stream network using plug in tools such as archydro maidment 2002 and taudem tarboton 2005 the process for delineating stream networks from an edem is a widely used method by o callaghan and mark 1984 in spin the drainage area a contributing to each cell is calculated using the flow accumulation tool in archydro maidment 2002 and a minimum threshold is set for a to define the stream network the stream cells are linked to each other and their link magnitude is identified using the shreve 1966 method to allow differentiation of the location of the stream tributaries with respect to a watershed outlet segment slope is calculated as the difference in elevation divided by length for each segment following the horizontal slice method bizzi and lerner 2015 jordan and fonstad 2005 in rivers a common measure of the curviness of the river is called the channel sinuosity p written as 9 p l c h l v where l c h is the length of the channel measured along the centerline and l v is the length of the valley measured along the centerline of the valley in a completely straight channel p 1 applied to a raster image the algorithms for calculating the flow path will result in a series of straight lines connecting grid cell centers where the resolution of the dem is coarse relative to the size of the river these segments tend to follow the valley more than the channel which means that the calculated flow path length l p is typically less than the actual l c h s will be overestimated if it is calculated simply as δ z l p which will also overestimate ω and ω to correct for this error we modelled p as 10 p p p where p is the grid measured sinuosity l p l v and p is the missing or sub grid sinuosity l c h l p in this way the true slope of the channel s can be calculated as 11 s δ z p l p where p is a user supplied corrective factor that is calculated by comparing the calculated value for l p with a value for l c h derived from data outside the model such values can be readily obtained from aerial photography a topographical map or surveyed data local slope estimates from an edem raster file are typically highly variable particularly where the grid cells are relatively large because the single recorded elevation for each grid cell masks heterogeneity at the sub grid scale the application of the spin tool in contrast was conceived as a reach level analysis of stream power and stability for this reason some smoothing of local slope is required to obtain representative reach averaged values for s in spin s is smoothed by averaging it over a set length of the channel centered on the grid cell of interest channel confluences are a challenge for smoothing algorithms because major tributaries can increase the water discharge and sediment loads such that the slope of the main branch shows a discontinuity at the confluence rice and church 1998 for minor tributaries however no change in slope in the main channel should be expected to differentiate between these two cases the spin toolbox calculates the relative contribution of the smaller basin a a t r i b a m a i n and sets a threshold default of a c 0 1 so that smoothing is only applied if a a c 3 3 2 discharge bankfull discharge q b f is commonly used to represent the most effective flow for geomorphic work emmett and wolman 2001 wolman and miller 1960 which makes it the most appropriate for modelling channel instability at a watershed scale for the purposes of this analysis q b f is assumed to be approximately equal to the mean annual flood event in rural channels q 2 and the stream power analysis assumes that q q 2 q b f as a first order approximation a power law relationship can be used to estimate this discharge q for undeveloped watersheds as 12 q a a b where a is the drainage area km2 and a and b are coefficients derived from statistical regression of data jain et al 2006 knighton 1999 lea and legleiter 2016 magilligan 1992 this approach is also built into landscape adjustment models where the vertical rate of channel adjustment is assumed to be proportional to the channel slope and the drainage area to an exponent that varies between 0 and 1 lague 2014 for urban scenarios many studies have shown that discharge is sensitive to impervious land cover imperviousness has therefore been used to predict the potential for flooding arnold and gibbons 1996 hatt et al 2004 shuster et al 2005 a practical approach for this level of analysis is to add the relative imperviousness of the land area ia as a modifying factor to regional empirical relationships between a and q bledsoe and watson 2001 ward et al 2015 so that 13 q c a d i a e where i a is the imperviousness of the land area 1 100 and coefficients c d and e are derived from statistical regression of data in spin c and d are set equal to the coefficients a and b equation 12 respectively to ensure continuity in predictions of q as land cover changes from pervious to impervious i a is then calculated for a given watershed based on a shapefile containing land use polygons with impervious values either supplied directly or obtained from a look up table of standard values for any given watershed the total i a is then calculated as the weighted average of all grid cell values a second approach to urban discharge was developed for spin using outputs from the u s army corps of engineers hydrologic engineering center s river analysis system hecras this second approach requires a completed hydraulic model of the channel but does have the advantage of being sensitive to hydrologic differences between urban areas i e changes in connectivity or stormwater management and can be adapted to test for the effects of in channel modifications and restoration hecras models are also widely used for urban flood modelling as part of a hecras model it is necessary to include discharge estimates which are typically derived from hydrologic simulations intended to replicate flows at different return periods with q 2 frequently included as one of the modelled discharges differences in urban hydrology can thus be captured in these discharge inputs in channel differences between channels can be captured because the flow conveyed between the channel banks hecras variable name q c h a n n e l is considered separately from flow over the floodplain only the in channel portion is responsible for shear stress exerted on a channel bed so the tool sets q q c h a n n e l for the 2 year return period flow urban channels are frequently disconnected from their floodplain and one restoration approach is to reconnect the channel to the floodplain to reduce stresses on the channel bed during floods so this refinement is intended to allow the benefit of this approach to be tested once ingested spatial information about the hecras node locations is read from a supplied shape file the river stations are joined to the closest stream segments and q values are applied to downstream cells until the next river station with a new value for q 3 3 3 channel width two methods are available in spin to calculate the channel width w the first method is based on hydraulic geometry relations that model width as a power function of q in rural areas where q can be modelled as a power function of a equation 12 w can also be written as 14 w f a g where the coefficients f and g are derived from statistical regression of data knighton 1999 leopold and wolman 1957 parker et al 2007 given the sensitivity to these models to differences in runoff coefficients morphology and bank strength such models should be verified for the region and watershed land cover where they are being applied the second method to obtain w follows from the hecras based method to obtain estimates of q hecras models require a channel geometry file that specifies station elevation data at a series of cross sections once a simulation has been run the outputs will include a corresponding wetted total width hecras variable name t o p w i d t h and wetted channel width hecras variable name t o p w c h n l to focus on the in channel portion of the channel width as was done for q spin sets w t o p w c h n l for the stream power analysis again as was done for q the relatively sparse hecras values of w were joined to the closest stream segments and then interpolated through the whole network 4 parameter validation 4 1 stream width slope and particle size in headwater field sites 4 1 1 site description data availability and parameterization the spin toolbox was tested on two headwater streams in ontario canada the watersheds are located close to the city of toronto where there are concerns about the impacts of urbanization on erosion as urban development expands maps of stream power are useful in this context to understand current or potential areas of stream erosion and to assess the benefits of storm water management field measurements of channel slope width and bed particle size at the sites were gathered as part of a larger study bevan et al 2018 cain and macvicar 2020 papangelakis et al 2019 an edem with a spatial resolution of 30 m was available through land information ontario lio 2019 higher resolution topographical information may be available for different jurisdictions but this edem was hydrologically enforced by the government mapping unit lio and made available for the majority of the province of ontario the low resolution dataset was used here to represent the minimum that is likely to be available in most jurisdictions the streams are small relative to the grid size of the edem w 10 m so empiricism is required to estimate their hydraulic geometry flow gauges with long periods of records were not available at these locations the two streams on which the model was tested differ primarily based on their degree of urbanization ganatsekaigon creek a 13 1 km2 is a tributary of duffins creek in pickering ontario land use is predominantly agricultural 58 with approximately 40 natural cover cain and macvicar 2020 papangelakis et al 2019 surficial geology consists of glacial till that is predominantly a mixture of sand and silt though stone lines are common sharpe et al 1997 the stream is considered to be important cold water habitat for fish and has populations of trout and redside dace clinostomus elongatus an endangered species in canada trca 2004 trca 2018 urban development is planned for much of the watershed over the next 10 years and polygons of future land use parcels and their associated land use type were digitized from watershed planning maps provided by the local watershed management authority toronto and region conservation authority trca land cover imperviousness by type was assumed to follow the hydrologic modeling guidelines published by the city of toronto 2006 no hecras model for the future urban land use scenario was available wilket creek in toronto ontario a 15 5 km2 is a tributary of the west don river in an area where the surface geology is similar to that of ganatsekaigon creek sharpe et al 1997 the main difference between the two watersheds is that the wilket creek watershed is 100 urbanized papangelakis et al 2019 trca 2015 development occurred between 1930 and 1970 in an era when the stormwater management infrastructure was designed to convey water quickly off the land much of the length of tributaries and main stem of the creek were buried in pipes during urbanization barr 2017 the remaining downstream open channel portion of the creek is characterized by erosion of the banks and valley walls channel enlargement and extensive stabilization and channel redesign projects bevan et al 2018 trca 2015 a hec ras model of wilket creek is maintained by the trca discharge values entered into the hecras model include design storms ranging from 2 to 100 year events based on a visual otthymo model of watershed hydrology trca 2015 existing land use data was also available from historical aerial photos based on the analysis of bevan et al 2018 so that again assuming that imperviousness follows the municipal modelling guidelines city of toronto 2006 the two methods for computing urban stream power could be compared regional models of discharge in rural channels was applied based on the empirical study of phillips and desloges for southern ontario 2014 for equation 12 they found a 0 25 and b 0 91 sample size n 210 r 2 0 86 the model of channel width from phillips and desloges 2014 tended to underestimate width for the sites investigated so the results from annable 1996 for the same region were applied where values of f 2 69 and g 0 36 in equation 14 were determined by combining empirical expressions for q as a function of a and w as a function of q sample size n 47 for the urban condition equation 13 no regional information was available and it was assumed that c a 0 25 and d b 0 91 equations 12 and 13 which matches the rural watersheds and that a typical exponent of e 0 30 could be applied from studies in other regions bledsoe and watson 2001 ward et al 2015 for the slope smoothing algorithm a moving window of 500 m in the streamwise direction was used based on a sensitivity assessment of the minimum distance required to remove high frequency variability in the slope tributaries were considered small i e not large enough to interrupt the smoothing algorithm if they represented less than 10 of the watershed from the larger tributary see section 3 3 1 4 1 2 stream network and sinuosity despite the relatively low spatial resolution of the edem spin mapped the position of the channel with reasonable accuracy fig 1 over the open channel portion of wilket creek for example the calculated position of the channel closely matched field observation fig 1a discrepancies were typically on the order of one or two pixels larger exceptions occur where the field survey was incomplete for e g at x 2600 m y 600 m where a construction project limited access to the site and a reach with high sinuosity centered at x 1000 m y 2100 m where p 1 65 for a 1 5 km reach bevan et al 2018 the sinuous reach helps to demonstrate the problem that necessitated the consideration of sub grid scale sinuosity p downstream from the highly sinuous reach centerline distance from 0 to 3 0 km p 1 30 bevan et al 2018 which is relatively close to the grid sinuosity for the whole of wilket creek p 1 18 for this reason the dem line though it is more stepped in appearance due to the coarseness of the dem largely follows the increasing elevation in the channel fig 1b however the highly sinuous reach missed by the dem analysis means that dem streamwise distances diverge from the field survey in the upper portion of the creek fig 1b applying the sub grid scale correction to the whole channel p 1 14 rotates the dem profile so that the overall channel slope is correct fig 1b the correction can locally increase the error in longitudinal profile however as shown for the lower reaches of the channel where the local p 0 correct estimates of s will therefore require reach level variation of p which can be done within the spin toolbox the importance of the smoothing algorithm is demonstrated in fig 1c where initial estimates of s show high magnitude variability and occasional negative values as a result of the coarse edem the smoothing algorithm erases these irregularities such that the edem derived slope varies much more slowly and in the range of the field measured average value of 0 85 a higher resolution field dataset was available for the stream profile of one reach of ganatsekaigon creek which largely confirmed the above observations errors of 1 2 pixels in the position of the creek were common fig 2 a the edem extracted channel had a shorter length than the field surveyed channel again due to the limitations of the 30m resolution of the edem fig 2b but the slope was estimated accurately using equation 11 for ganatsekaigon the field surveyed results showed that p 1 49 but p 1 14 from the available edem so that p 1 29 this high value for p reflects the small size of the channel relative to the edem resolution local slope estimates were variable due to the coarse steps in the estimates of channel elevation that result from the coarse edem resolution but the smoothing algorithm reduced the variation resulting in overall slope between 1 4 and 2 0 which matched the field derived slope of 1 60 fig 2c 4 1 3 width available field measured channel widths were compared with the rural annable 1996 model equation 14 including three reaches from each of ganatsekaigon creek and wilket creek fig 3 wilket creek data are top of bank measurements from a 1958 survey for a sanitary sewer pipe installation bevan et al 2018 which captured the channel in a relatively early stage of urbanization error bars represent the standard error of the mean obtained from 3 7 and 8 cross sections respectively in the three major reaches defined based on slope breaks in the long profile outliers thought by bevan et al 2018 to have been modified prior to the 1958 survey were not included for ganatsekaigon creek reach 1 represents top of bank measurements for 15 cross sections made by cain 2019 while reach 2 and 3 represent similar measurements for 19 and 23 sections respectively made by papangelakis 2019 as shown the modelled relation from annable 1996 tended to underpredict the field measurements with a couple of exceptions the generally narrow modelled values may reflect a methodological difference as the annable 1996 was focused on the determination of bankfull hydraulic geometry and used a set of bankfull indicators while the measured values were determined solely based on the floodplain elevation 4 1 4 particle size the model for the threshold particle size was compared with field estimates from the two study sites where multiple bed pebble counts had been carried out as part of previous studies for wilket creek pebble counts were comprised of a minimum of 200 particles and equally sampled all parts of the bed following bunte et al 2009 and harrelson et al 1994 for ganatsekaigon creek 16 estimates of the particle size distribution on riffles were made with a minimum of 100 particles cain 2019 in both cases particles less than 2 mm in diameter were recorded separately i e as fine gravel sand bare till or fines and not counted in the field tally to ensure a minimum sample size of the coarse particles particle sizes and the difference between measured and modelled values were calculated using a krumbein ϕ scale 1951 modified as described by parker and andrews 1985 15 ϕ log 2 d d o where d o is a reference diameter 1 mm to ensure dimensional consistency all field estimates of d 84 were compared with d b c for the two sites to assess the utility of the model for understanding the variation of bed material size fig 4 a 1 1 line of agreement and guide lines at 1 ϕ are included to help the presentation of results for ganatsekaigon the modelled d b c values are relatively tightly clustered between 6 6 5 ϕ while the field measured d 84 values are spread out between 5 7 ϕ such variability in the field results is expected given the smaller sample sizes for the ganatsekaigon sites n 100 particles and within reach variability from localized inputs of coarse bank material and scalloping of the long profile as described by cain and macvicar 2020 the variability of d b c is limited by the relatively coarse mesh and slope smoothing distance of 500 m which represents most of the reach length shown in fig 2 the key result for this analysis is thus that the measured d 84 range straddles the 1 1 line and is generally bounded by the 1 ϕ guidelines which demonstrates that this channel could be considered a threshold or supply limited channel for this type of channel an increase in stream power will likely increase sediment transport and degradation in contrast the bed material samples from wilket creek which come from larger samples that are intended to be reach averaged rather than local samples all show that d b c d 84 1 ϕ when using the rural value of ω b f but that d b c is bounded by the 1 ϕ guidelines when the urban value of ω b f is used in equation 8 this indicates that the channel is either a type of inherited understeepened channel as described by phillips and desloges 2015 or that the bed has coarsened as a result of urbanization understeepened channels can occur in southern ontario because coarse bed material has a glacial legacy and sediment transport rates are low however wilket creek is known to have undergone significant enlargement bevan et al 2018 barr 2017 and the degree of possible coarsening agrees with expectations that the bed material size is controlled by the threshold condition to give a broader context for the model of bed particle size threshold d b c was also compared with the 33 streams with gravel to cobble sized bed substrate 2 m m d 84 256 m m from the database prepared by annable 1996 for rural streams in southern ontario this database contains field estimates of s w and sediment particle sizes d 50 and d 84 and estimates of manning s coefficient q 2 and q b f from water survey of canada gauging records as shown d b c effectively models the lower limit of the field based d 84 estimates fig 5 a many of the sites fall within the 1 ϕ guide lines but for many other streams d 84 d b c 1 ϕ including some where d 84 d b c 4 ϕ to understand how the channel morphology may influence the relation between the transport threshold and the field measurements we plotted the difference d 84 d b c against s and w fig 5b and c as shown the channels with large differences all have s 1 and tend to be wider fig 5c this result indicates that narrow and steep channels tend to have bed material that is close to the threshold particle size while wider and lower sloped channels can have bed particle sizes that exceed what would be expected from a threshold analysis 5 model results and interpretation it is envisioned that spin will be used for three primary purposes a desktop analysis of likely current morphology b prediction of morphologic change as a consequence of urban development and c stormwater management and channel restoration scenario analysis to illustrate the first application maps of ω were generated at the watershed scale for ganatsekaigon and wilket creeks figs 6 and 7 respectively the rural ganatsekaigon map fig 6a shows that stream power peaks in two locations within the watershed the first is near the watershed outlet surveyed reach g3 was located in this area where high power 75 ω 100 w m 2 is driven by high discharge from the entire watershed a second peak in a similar power range appears in each of the main tributaries surveyed reach g1 was located in this area on west branch where high power is driven by locally high slopes s shaped profiles have been observed in other rivers in southern ontario where headwaters are on low relief till plains middle reaches incise through the till and lower reaches flatten out near the lakeshore phillips and desloges 2014 vocal ferencevic and ashmore 2012 similar patterns also appear on the remaining open channel portion of the rural wilket creek fig 7a where ω increases at the transition from w2 to w3 regional studies of morphology are useful for interpreting such results in the southern ontario region for example phillips and desloges 2015 coupled a principal component analysis with a clustering procedure to describe floodplain archetypes ω emerged as the most important fluvial process variable largely agreeing with previous studies such as the seminal work of nanson and croke 1992 the clustering procedure was used to identify four floodplain archetypes as ω ranged from 5 to 10 w m 2 for clay to 60 100 w m 2 for cobble dominated floodplains with coarser beds and reduced floodplain thickness as shown in the map of ganatsekaigon creek fig 6a ω 100 w m 2 for the rural scenario which indicates that the floodplain types largely fit within the ω range used to develop the regional model phillips and desloges 2015 available descriptions of morphology of the system generally agree with this understanding in particular cain and macvicar 2020 describe reach g1 as a coarse bedded channel with coarse eroding banks which stands in stark contrast to the channel a few hundred meters upstream which is characterized by lower slopes and finer sediments within wilket creek ω is also 100 w m 2 for the rural condition fig 7a though it is not possible to confirm the floodplain archetypes given the altered state of the catchment but a sharp transition from ω 100 w m 2 to 150 w m 2 is modelled at the transition from reach w2 to w3 in wilket creek in the urban condition fig 7b and c bevan et al 2018 observed a clear morphologic transition in this area with reaches w2 characterized as meandering river with an abundance of sand and fine gravel sediment and w3 as a coarse gravel cobble river though a complete morphologic assessment of the creeks is beyond the scope of the current analysis this rough agreement between morphologic descriptions and stream power indicates that spin results can be used to anticipate real differences in morphology the second anticipated application of spin is that it would be used to predict morphologic changes as a consequence of urban development in ganatsekaigon creek for example the additional flows from the urban areas could more than double ω after development fig 6b pushing some areas up over 200 n m2 while similar increases have already been realized in wilket creek fig 7b while it is generally understood that the increase in ω will influence bed particle size sediment transport and channel stability it is not immediately clear what specific changes can be expected to guide the interpretation a ω vs d 84 phase diagram was prepared using classification ranges and morphologic thresholds from the literature fig 8 the critical particle size equation 8 is shown as a narrow range due to the slight dependency of the threshold on s the range shown represents channels with s 0 5 1 5 which is typical for wilket and ganatsekaigon creeks also shown are bar dynamics thresholds developed by kleinhans and van den berg 2011 who used a large database of river morphology and bar theory to a refine the discriminator between dominantly braiding and meandering systems at 16 d 84 2 2 ω 900 2 4 and b identify a transition between meandering rivers characterized by scroll bars and moderately braiding meandering systems where chute bars become more common 17 d 84 2 2 ω 285 2 4 both of the above relations include a relation for equivalent roughness expression d 84 2 2 d 50 lópez and barragán 2008 to model the particle size used to represent the bed roughness approximate stream power and particle size ranges are also shown for the phillips and desloges 2015 floodplain archetypes similar to equations 16 and 17 the expression for equivalent roughness was applied to estimate the d 84 ranges from the d 50 ranges given by phillips and desloges 2015 measured values from the database of annable 1996 are also shown for comparison the mobility threshold can be used to interpret channel coarsening as a response to increased stream power the database compiled by annable 1996 shows that the d 84 mobility threshold is a lower bound on bed particle sizes for gravel and cobble bedded channels in southern ontario which means that coarse bedded rural channels are typically either threshold or understeepened in this region the spin results show that the maximum ω 100 w m 2 in the rural condition and 250 w m 2 in the urban condition for both wilket and ganatsekaigon creeks figs 6 and 7 the currently rural ganatsekaigon creek has beds with d 84 128 mm which is close to the current mobility threshold in the urban condition i e if ω increases to 250 w m 2 the current bed will fall below the mobility threshold the apparent response to a similar change in wilket creek has been coarsening such that d 84 remains above the mobility threshold the spin results could thus be used to anticipate bed material coarsening in ganatsekaigon following equation 8 where the core relation is d 84 ω 2 3 another consideration is that there may be a natural limit to bed coarsening based on sediment supply in the available databases there were few examples with d 84 256 mm low supply of material larger than this size means that there exists a narrowing triangle of conditions where d 84 d c i as ω increases where ω 500 w m 2 d c i exceeds 256 mm and channels within this power range would presumably be at risk for more profound channel adjustments that could involve removal of the alluvial cover which has been observed in urban rivers in this region vocal ferencevic and ashmore 2012 bevan et al 2018 morphologic transitions are modelled to cross the phase space following the general relation of d 84 ω 2 4 kleinhans and van den berg 2011 which means that it is possible for channels to cross multiple thresholds as they are urbanized as shown the phillips and desloges 2015 floodplain archetypes and the majority of the annable 1996 sites fit within the meandering range which are generally characterized by scroll bar dynamics kleinhans and van den berg 2011 such descriptions match with the descriptions of lateral accretion for the s and m archetypes which have higher sand content than both the c and b types while the low power environment of the c type is described by vertical accretion of fine sediments and a thick floodplain phillips and desloges 2015 the high power b type environment are described as having a thinner floodplain and low accretion rates while sitting close to the morphologic transition where chute bars become more prevalent fig 8 ganatsekaigon creek appears to straddle this transition and the mobility threshold relation which would indicate higher rates of chute bar formation and sediment transport these implications are supported by field observations including moderate braiding within the study reach cain and macvicar 2020 relatively high sediment transport rates cain et al 2020 and erosion of the inner bank following a chute bar cutoff morphology during a high transport event papangelakis et al 2019 the urban wilket creek is almost entirely within the area of the phase diagram characterized by a higher prevalence of chute bars and relatively unstable lateral morphologies field observations of a channel avulsion in wilket creek by bevan et al 2018 fits with this description where the avulsion occurred the power was high enough to move even the coarsest particles such that the underlying till was exposed the till itself is relatively easily eroded pike et al 2018 and degradation of the bed occurred bevan et al 2018 more examples are needed but these field studies show spin results can be used to anticipate changes in bar dynamics and lateral instability as a result of urbanization the third anticipated application of spin is that it would be used for scenario analysis involving stormwater management or local stream modifications to illustrate this potential it is useful to first consider the upper portion 1 75 km length of wilket creek fig 9 empirical relations indicate that q is relatively constant over this stream distance with an expected rural discharge of 2 3 m3 s fig 9a estimates using the land use polygons more than double these values to between 6 and 7 m3 s but the hecras model maintained by the watershed authorities indicates that the in channel flow during the 2 year flood is higher still at 15 m3 s this value drops around a low head weir in the channel where some of the flow is forced out onto the floodplain slope is variable and generally varies between 0 6 and 1 0 with the higher values near the upstream limit fig 9b in the empirical rural and urban land use approaches w is nearly constant due to the relatively small change in drainage area over this length of the channel but the hecras model has a variable w from a minimum downstream of the weir that is approximately equal to the empirical estimate to a maximum of nearly 50 m upstream of the weir fig 9c the variable q and w in the hecras model mean that ω also varies more than with the empirical approaches ranging from a minimum that is below the rural value upstream of the weir where the channel is wide to a maximum of ω 200 n m2 downstream of the weir in the narrowest cross section the results show a good correspondence with the field study by bevan et al 2018 who found that sediment deposition occurs upstream of the weir but that the area downstream of the weir is prone to erosion the spin hec ras approach is suited to scenario analysis because it can be linked with hydrologic models and capture local variability in ω associated with channel morphology a series of potential channel morphology hydrology modifications and the impacts on ω for the upper portion of wilket creek are shown in fig 10 for this approach a set of feasible modifications were proposed including a constant q of 10 m3 s which could be achieved either by improving the stormwater management within the watershed to reduce the peak or by redesigning the channel floodplain so that more flow is conveyed over the channel floodplain similar to what occurs now close to the low head weir a 20 increase of p which could be achieved by reconstructing the current relatively straight planform to follow a meandering path and a constant w of 10 m which could be achieved by redesigning the channel section such that the width is higher but the flow depth is lower than the average for the current channel taken separately each of the modifications will have a proportional impact on the specific stream power fig 10 application of all of the proposed modifications together i e a wider channel with increased sinuosity and reduced channel flows reduces ω to 50 n m2 which is 50 higher than the rural values as shown but within the range of natural m or b type morphologies for the region despite the urban character of the watershed keeping ω within the natural range of regional variability would likely result in more stable channel morphologies consistent values for q c h a n n e l and w will reduce the streamwise variability of ω which would be beneficial for sediment continuity and reduce maintenance costs a recent study of a form based restoration project within wilket creek showed that despite the stability of the channel within the project reach the project had led to upstream aggradation and downstream degradation papangelakis and macvicar 2020 spin results could be used to anticipate these problems and support a process based approach to stream restoration such that sediment continuity is maintained users of the spin toolbox should be aware of the limitations of the available tools first the results are largely dependant on the dem being used higher resolution dems are commonly available and may offer better detail on slope variation which would eliminate the need to account for the subgrid sinuosity p in urban areas however watercourses are frequently crossed by roads and embankments which means that raw elevation data from a lidar or other survey must be preprocessed by burning the streams into a dem before it can be used to delineate the channels and estimate s this process of hydrologic enforcement significantly influences the results and has been the subject of considerable research lindsay 2016 campforts and govers 2015 but is beyond the scope of the current investigation instead a relatively coarse dem was selected because the pre processing of the dem was completed by the mapping authority for the region and was found to be suitable future work could examine the trade offs between dem resolution hydrologic enforcement and the estimation of s in spin a second limitation of spin is that river channels may be influenced by in channel wood or bedrock outcrops in channel wood for example can increase the storage of sediment and reduce the effective channel slope so that alluvial channels occur where bedrock channels might otherwise be expected montgomery et al 1996 welling et al 2021 such effects are not captured by the model the empiricism of spin is also a limitation particularly for estimates of q and w in the rural model and the urban land use approach for the reaches with available field data w appeared to be underestimated by equation 14 fig 3 while the q2 included in the hecras model provided by the watershed authority was almost double the estimate from equation 13 fig 9a clearly then the empirical results do not necessarily provide locally precise estimates of ω at a watershed scale other factors such as vegetation or bank protection measures can strongly influence bank stability and w eaton and millar 2004 tal and paola 2007 while urban values of q can be expected to vary considerably as a function of the nature age and connectivity of the impervious parcels of land rather than precise estimates the empirical approaches allow for a rapid and high level desktop analysis to visualize the qualitative patterns of ω at a watershed scale field morphologic assessments are still necessary to verify model assumptions and tie the morphologic estimates with morphologic types where data is available the spin hecras option allows users to capture real variability of width and discharge with this option various development and restoration scenarios can be analyzed from a stream power perspective 6 conclusions the spin toolbox has been developed to assist decision makers responsible for protecting and rehabilitating streams affected by the urban stream syndrome based on an analysis of stream power the toolbox is available as an add on to the widely used arcgis environment which allows both the magnitude and degree of change that can be expected as a result of urbanization to be easily visualized and analyzed within a catchment the toolbox can be used based entirely on watershed topography and land use polygons to anticipate current morphology from a desktop analysis and predict morphologic change as a consequence of urban development or it can be used in tandem with a hydraulic hecras model of the channels to analyze possible stormwater management and channel restoration scenarios application of the toolbox to two watersheds demonstrated its effectiveness at using a dem to delineate and describe a stream network channel width slope and bed particle size can be reasonably predicted stream power can also be used to anticipate channel morphology and understand channel evolution in response to urbanization future work should more widely apply the toolbox to test its utility for understanding morphologic change and scenario analysis for the management of urban river systems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the toronto region conservation authority helped in the initial development of the spin toolbox participated in grant writing and shared data from the example watersheds matrix environmental solutions shared field data for both sites and financially supported some of the data collection work credit valley conservation supported the work through funding for grant applications and offered feedback on toolbox development vernon bevan aryn cain margot chapuis julian krick and elli papangelakis collected the field data that was used to test the toolbox funding was provided through a strategic partnership grant stpgp 463321 14 from the national science and engineering research council of canada nserc author contributions k g developed and coded the toolbox b m and p a conceived of the research and supported toolbox development k g and b m performed the research and wrote the paper and k g b m and p a edited the paper we are grateful to the editors j desloges and one anonymous reviewer for detailed and constructive comments on an earlier version of the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105185 software availability name of software stream power index for networks spin developer kimisha ghunowa year first available 2021 hardware required none software required arcgis availability github in the macvicab spin repository cost none program language python program size 213 kb 
25772,this paper presents a novel transversal agnostic infrastructure and generic processing model to build environmental big data services in the cloud transversality is used for building processing structures ps by reusing coupling multiple existent software for processing environmental monitoring climate and earth observation data even in execution time with datasets available in cloud based repositories infrastructure agnosticism is used for deploying executing pss on in edge fog and or cloud genericity is used to embed analytic merging information machine learning and statistic micro services into pss for automatically and transparently converting pss into big data services to support decision making procedures a prototype was developed for conducting case studies based on the data climate classification earth observation products and making predictions of air data pollution by merging different monitoring climate data sources the experimental evaluation revealed the efficacy and flexibility of this model to create complex environmental big data services keywords big data cloud computing environmental data climate data machine learning data analytic 1 introduction visualization and analysis services have become key for processing and managing large volumes of environmental data which have been keystone for scientific community to conduct complex scientific studies such as climate schnase et al 2017 environment hempelmann et al 2018 ujjwal et al 2020 and earth observation sánchez gallegos et al 2019 montella et al 2019 online repositories following fair principles findable accessible interoperable and reusable tarboton et al 2014 zhang et al 2019 have been created for collecting and making available visualization and analysis software as well as datasets and information for scientific community to establish collaborative work by sharing these tools with end users sun et al 2019 zhang et al 2020 these repositories not only represent a source of solutions for organizations to produce useful information for decision makers xiang and demir 2020 but also an opportunity area to create environmental big data services based on software processing structures ps these structures are built by grouping and reusing available software and datasets pipelines patterns service mesh and workflows are examples of pss available for organizations to build big data services a ps is modelled as the interconnection of a set of processing stages a stage executes a given software represented by a directed acyclic graph dag where the nodes represent stages and edges represent the interconnection of a stage with any of other stage data source sink or other pss these pss process and or manage environmental monitoring data from repositories modeling and office 2015 for instance in a traditional big data scenario a ps commonly consider the following stages i acquisition of environmental monitoring data from a source e g either a dataset published in an online repository modeling and office 2015 data monitoring repositories 2012a red automá 2012a 2012b red de met 2012b or a downloaded produced dataset ii data preparation for converting extracted data into a given format e g geojson json or queries for a database iii pre processing for removing potential erroneous data and or enriching the data by calculating fulfilling missing data iv processing or analysis to convert data into useful information by using software for classification grouping prediction of values merging data information to name a few v visualization that prepare information for being consumed by decision makers today multiple pss are already available for scientific community to process and analyze climate hu et al 2018 pollution cabaneros et al 2019 montella et al 2016 earth observation data sánchez gallegos et al 2019 montella et al 2018a processing contents gonzalez compean et al 2018 montella et al 2018b or predicting meteorological changes muller 2020 goodman et al 2019 these pss produce useful information di luccio et al 2018 zhang et al 2020 that results critical hu et al 2018 for decision making procedures e g to prevent disasters preston et al 2011 nevertheless building environmental big data services based on existent software pieces and or pss is not trivial because of the following restrictions 1 the programming languages used by the available frameworks for creating pss is not necessarily the same which is a problem when the environmental processing software has been written by using a different programming languages 2 pss and big data frameworks could require to be executed on a given platform and because of this it staff commonly ends up installing third party software and performing troubleshooting processes to overcome errors due to installation configuration data unavailability and path mistakes attariyan and flinn 2010 moreover these frameworks commonly impose the installation of third party solution for the management of computational resources 3 reusing solutions is not an immediate option for end users to build a new service this means in most cases a new solution must be built when creating a ps and is not feasible for end users to reuse existent and already tested pss 4 analytic statistic and machine learning are commonly added to the solutions as a third party software hu et al 2018 which could produce issues of programming languages portability and interoperability could arise when considering these frameworks in a solution 5 the access to the data at any stage in traditional big data solutions the pss are created as black boxes where a solution retrieves data from a source e g any of folder cloud location or data lake executes a set of applications at different stages for converting incoming data into useful information that is stored in a sink e g cloud storage location the end user thus only gets access to the final results placed at the sink but not necessarily to the results created produced by intermediate stages which could be useful either as input data for other solutions or being consumed by other end users or applications in this context it makes sense that the scientific community can reuse either portions or whole successfully installed and configured pss paradis 2020 vitolo et al 2015 to create comprehensive environmental big data services which will result in saving time and human infraestructure resources it also makes sense that these pss can be agnostic from programming languages infrastructure and platforms for enabling end users to deploy execute them on in different infrastructures such as edge e g personal computers fog e g servers of a company organization and or cloud e g virtual containers and machines provided by public providers without needing to download data and software or making installing configurations which reduces the need for it staff of organizations to perform troubleshooting processes however reusing already installed and configured pss coupling it to existent pss gao et al 2019 sharing data produced in a ps with another one yi et al 2018 publishing and consuming information at any stage of an existing ps during the data life cycle xue et al 2019 are challenging tasks which have been only partially addressed by solutions in the state of the art such as pipelines laster 2018 workflows montella et al 2018 babuji et al 2018 badia et al 2015 or services deelman et al 2004 albrecht et al 2012 currently for our best knowledge there are no feasible and immediate options provided by different traditional frameworks for organizations to create environmental big data including the aforementioned features without making hardworking adaptations this paper presents a novel transversal agnostic infrastructure and generic processing model to build environmental big data services in the cloud transversality property is used for building ps by reusing coupling multiple existent software for processing environmental monitoring climate and earth observation data even in execution time with datasets available in cloud based repositories the transversal model adds the transversality property to pss by creating and using a set of coupling and extracting transversal points fig 1 shows an example of a transversal service built in the cloud by reusing two existent pss one for processing re analysis merra 2 products and the other for processing monitoring data from ground stations by conagua 1 1 https smn conagua gob mx tools gui emas php placed at mexico as it can be seen the transversal coupling points tcps interconnect two stages one from ps1 and the other one from ps2 transversal union between ps1 and ps2 in fig 1 through stages b and g this virtual coupling is performed even in execution time to create a new independent transversal service ts the transversal extraction points teps enable end users or applications to consume and process data see tep1 and tep2 in fig 1 the extraction services enable either end users or other services to retrieve deliver and or visualize data results produced by a given stage of an existing ps see tep1 between a and b of ps1 and tep2 between g and h of ps2 in fig 1 genericity in the context of this paper refers to the property of analyzing and producing information produced by any environmental software without restrictions of language programming infrastructure platform and software this property is used to embed analytic merging information machine learning and statistic into generic micro services coupled to pss for automatically and transparently converting them into big data services to support decision making procedures the embedded micro services take advantage of teps providing services such as data fusion to produce useful information to support decision making procedures fig 1 shows how two pss are processing two different climate data sources and how embedded micro services consume data from teps for producing visualizing information the first embedded micro service connected to the tep1 executes a classification tool to merge metrics from two data sources and then to group climate metrics by statistic similitude values the second embedded micro service connected to tep2 executes a visualization tool a geoportal in this case for extracting results produced by stage g classification of metrics and to show them in a map by using gis embedded service infrastructure agnosticism is used for deploying executing pss on in edge fog and or cloud without end users requiring to perform installation configuration steps which reduces the need for performing troubleshooting procedures this model also adds agnosticism from infrastructure to the ts any of ps systems of pss or big data services by encapsulating environmental software into virtual containers and managing them by using micro service architecture the transversal generic and agnostic properties added to existent pss enable end users to create complex big data services by reusing multiple existing software created with different frameworks and deployed on different it infrastructures the building of transversal environmental big data services thus can be performed in three simple steps i builds a ps by choosing existing software and datasets tarboton et al 2014 ii creates a new independent service by defining tcps reusing already configured and tested pss any of online available downloaded in a given infrastructure or created by this model iii creates a big data service by using teps and the embedded micro services that produce useful information when performing these three steps a new transversal service is created for end users and or decision makers to consume the information produced by this solution through independent transversal processes service tps applying the three steps to the creation of environmental services also enable decoupling the environmental software from the pss as well as from the analytic and machine learning tools in short the tpss converts a traditional set of software pieces into an environmental big data service a prototype was developed and implemented based on this model to create environmental big data services which were created based on existing environmental software for conducting three case studies the first one for processing of satellite imagery to build earth observation products eops and yielding environmental indexes the second one for processing monitoring and re analysis climate data the resultant data were processed by clustering algorithms to create groups of ground stations by using statistical similitude values the third one is a data fusion service for producing pollution predictions using machine learning tools included in embedded micro services by using multiple data sources such as monitoring and re analysis climate merra modeling and office 2015 redmet 2012b red de met 2012b and monitoring air pollution rama 2012a red automá 2012a sources the experimental evaluation based on the three case studies revealed the efficacy and flexibility of this model to create complex environmental big data services for processing heterogeneous data such as re analysis and monitoring climate and environment data it also revealed the feasibility of this model to perform data information fusion to produce useful results which can be consumed by decision makers through online services tps the paper is organized as follows preliminaries and background information are given in section 2 section 3 frames the state of the art about the topic the novel transversal computing model is described in section 4 the experimental evaluation is explained in section 5 finally section 6 is dedicated to the conclusion remarks and future directions 2 preliminaries 2 1 an overview of processing models for big data in a traditional big data scenario the stages are created as blocks executing a given application or tool by using an extract transform load etl processing model in this model a block extracts data from a source e g any of folder cloud location or data lake executes the applications encapsulated into the stage for transforming the incoming data into useful information that is loaded in a sink e g cloud storage location pipelines and computerized workflows are examples of pss that allow end users to process data through multiple blocks stages such as acquisition pre processing processing and preservation that are chained by following a given sequence the simplest coupling method is a pipeline where the stages are interconnected sequentially following an etl the coupling is performed by using the data sources and sinks of the stages for instance in a traditional big data pipeline an acquisition block stage extracts data from a web page creates indexes transforms and loads the indexes in a database placed commonly in a cloud location where a pre processing stage extracts the indexes from the database to locate the extracted data for cleaning them e g removing outliers and noise data or calculating missing data by using extrapolation this pre processing commonly transforms raw data into a cleaned data version which is also loaded as indexes in the database and stored in a cloud location a processing stage e g analytics or machine learning tools extracts the indexes to get the cleaned data and to transform them into reduced information e g any of categories classification or ordered groups which is loaded in a database as information at this point this information can be consumed by end users in decision making processes vaghefi et al 2017 from the point of view of the users participating in a decision making process all these stages are integrated in a single service and they only has access to the raw data and or resultant information 2 2 an overview of stages coupling method and data management the etl model is used for the coupling of stages through data sources and sinks it is commonly represented as a dag badia et al 2015 deelman et al 2004 which is used for a wide range of tools brikman 2019 and frameworks montella et al 2018 albrecht et al 2012 smart 2011 to create analytic and processing services in either an automatic or semi automatic manner some frameworks impose to end users a programming language e g python montella et al 2018 deelman et al 2004 babuji et al 2018 or java smart 2011 for building pss other frameworks also impose infrastructure platform requirements badia et al 2015 babuji et al 2018 or they only enable the users to use solutions in the cloud cloud 2011 abadi et al 2016 brikman 2019 in real world scenarios this results in a complex ps ecosystem where multiple pss are built by multiple frameworks using different programming languages and deployed on multiple platforms over different infrastructures the following tasks could be required in such an ecosystem which are addressed by the model presented in this paper 1 reusing partial or whole pss any of applications stages pipelines or workflows 2 building services based on multiple existing pss by interconnecting them either in execution or configuration times 3 allowing on the fly and on demand changes in existing ps 4 requiring that different stages sharing a data source e g data warehouse or data lake or sinks 5 consuming data produced by intermediate stages not only from the starting data sources and ending sinks 6 creating information crossing processes by including multiple already developed applications processing multiple data sources or sinks including the sources and or sinks of intermediate stages 3 related work engines and frameworks have been key for the scientific community to build big data pss yue et al 2015 processing workflows and pipelines are not a new technology there are multiple tools available that allow users to design and manage the execution of tasks either locally or in a distributed environment taverna hull et al 2006 for example is a framework created for the execution of bioinformatic workflows offering a variety of processes services in a catalog used for the construction of multiple workflows it also enables designers to invoke external services to incorporate them into their workflows as well as interfaces for the construction and execution of these processes in this way users can build their processing workflows without much experience in computing areas these solutions can be shared through myexperiment goble and de roure 2007 a repository of workflows which allows using algorithms for detecting useful fragments or workflows to create tasks hierarchy of tasks similar to this framework a wide catalog of workflow systems workflow engines has been reported in the state of the art including but not reduced to popular examples such as comps badia et al 2015 tejedor et al 2017 makeflow albrecht et al 2012 pegasus deelman et al 2004 galaxy goecks et al 2010 and dagonstar montella et al 2018 although those engines and frameworks have different characteristics such as the programming model or the tasks execution method all of them have solutions based on dags these structures do not consider explicitly the coupling of multiple workflows by building meta workflows workflows over workflows for instance it is not feasible for these solutions to reuse data already processed by previously executed ps or being executed without changing codifications of applications moreover the installation and configuration of a new ps must be created instead to reuse existing pss which only can be modified at configuration time not in running time after studying these frameworks we observed that the challenge of managing ecosystems of pss has been focused mainly on two directions the first one is focused on the construction of solutions based on hierarchical levels the second one is focused on the composition of new solutions by reusing ps fragments the main usage of the solutions based on a hierarchical approach yildiz et al 2019 is the discovery of pss and the composition of different solutions as a single one it is usually based on two hierarchy levels the first one e g decaf dreher and peterka 2017 for the in situ workflow composition with tasks that exchange messages through memory composed of a set of fields that may or may not be fully utilized by the solutions the second one is the development of new solutions by using meta dags e g by using pycomps and decaf tejedor et al 2017 badia et al 2015 to execute in situ workflows as tasks in a distributed environment in this type of solution the management of dependencies and the transport of data between tasks associated to the edges of the dag is performed by following the meta dag and encompassing the task execution task coordination task parallelization and transport of data in turn the solutions that reuse workflow fragments for the composition of new solutions are mainly focused on algorithms that search for fragments of tasks within workflow repositories that are suitable to be reused when building new solutions zhou et al 2020 garijo et al 2014 some of these works produce the automatic design of solutions by using fragments of workflows but there are still limitations and areas of opportunity of this type of approaches that the model proposed in this paper supports three are three main differences between the former proposals and the transversal model proposed in this paper the first one are the transversal coupling services that provides a new scope for the problem of dealing with a ps ecosystem by managing pss as services these services can be coupled with other ps mainly external ones through a new independent services transversal processing service this can be done without affecting modifying the applications of an existing ps by using some of these stages on different solutions the second one is the management and analytical processing of data based on extracting publishing tss which are focused on transform data into information as well as enabling crossing information processes this converts multiple ps into a configurable big data service instead of a single purpose solution as performed in traditional approaches the last one is that this model allows creating external solutions which means that this model can be also used by traditional frameworks to create pss as services by reusing solutions and the data produced by their stages as shown in the experimental evaluation of this paper 3 1 usability comparison study in order to show the aforementioned differences a comparative usability study from the end user point of view was carried out between some frameworks found in the literature that address the problem described in this paper such as taverna hull et al 2006 galaxy goecks et al 2010 cross workflow fragments zhou et al 2020 and pycomps decaf yildiz et al 2019 and the transversal model proposed in this paper fig 2 shows the differences considering eight different characteristics bidirectional ps coupling it refers to the situations where a solution requires coupling two different pss bidirectionally pss reusing data source and or sink of other pss it refers to situations where pss to reuse results produced by other pss reusing data produced by intermediate ps stages it refers to situations where reusing data produced by intermediate stages of other ps without modifying the code of applications supporting multiple programming and execution models for each ps belonging to a solution adding new pss to a solution reusing portions of a ps reusing a complete ps consolidation of results produced by different pss referring to the ability to consolidate the results analyze index publish visualize etc and to convert data into information each characteristic was rated on a scale of 1 5 1 it is not currently supported by the tool model schema to perform the activity 2 it is not currently supported by the tool model schema to perform the activity but it can be implemented according to its design principles 3 it is currently supported by the tool model scheme to carry out the activity but external tools are required to do it 4 it is currently supported by the tool model schema to perform the activity 5 it is currently supported by the tool model scheme to perform the activity and additionally it offers non functional requirements scalability efficiency security modularity reliability etc as it can be seen fig 2 describes from an end user point of view the opportunity areas of more prominent frameworks and engines and how its integration with a transversal model could fulfill these areas for improving the flexibility and functionality of current frameworks and engines the proposed transversal model takes into account these opportunity areas allowing a management of solutions composed by others using the workflow engines themselves as the task execution tool unifying the programming model and maintaining the characteristics of each model of execution table 1 shows a summary of the qualitative differences of the works considered in this study 4 a novel transversal processing model for environmental big data services in this section we present a novel transversal processing model for building big data services in fog cloud environments the model allows the integration of multiple existing deployed software and or applications into new solutions exposed as a service 4 1 transversal processing structures a ps can be represented as a tuple including a set of stages v and their corresponding connections e 1 p s v e where v represents a set of stages stage i in the form v stage 1 stage 2 stage v e represents a set of ordered pairs including elements of v representing the interconnections between the stages e x y v v x y each stage stage i performs a transformation task following the etl processing model obtaining data from a source ds j and depositing the transformed data in a sink ds k which can be used by either end users or other stages for getting access to the transformed data both the source and the sink belong to a data repository psdata in this context a stage can be represented as follows 2 s t a g e i d s j t a s k d s k d s j d s k p s d a t a j k where ds j represents the input data which is an input for the transformation task ds k represents the output data deposited in a sink e g data warehouse task represents a task an activity or a data transformation process psdata it is a set of all the data sources used by the stages of a ps to represent a structure as a dag nodes represent each stage stage i v of a ps while the edges represent the corresponding ordered pair e which defines the sequence of execution of each stage fig 3 shows an example of a dag defined for a ps a processing pipeline in this example where the ordered execution of the tasks a b c d of each stage produces different versions of data d v x from one stage to another one the model mainly considers scenarios where a set of ps is used to build a transversal service ts that is defined by its dag representation dag tp which is created by using a set of transversal processing points tpp two types of tpp have been defined in this model fig 4 transversal coupling points tcp these points create an abstract intersection between tasks that belong to different pss transversal extraction points tep these points create input output interfaces for solutions to extract deliver data from to existing pss either at setup or run times these points also includes a link to access data or to be used as input in a tps see section 4 1 to transform data into information a dag tp thus can be represented with the following expression 3 d a g t p s o l t l i n k s where solt is the set of ps included in a transversal solution while links is a set of tpp that represents the virtual interconnections of the ps solt each tpp i can be a tcp or a tep this is represented as follows 4 l i n k s t p p 1 t p p 2 t p p m m 1 5 s o l t p s 1 p s 2 p s n n 1 t p p i is a t e p n 2 t p p i is a t c p t e p a tcp represents an interconnection between stages through a data source belonging to the set psdata k of a ps k ds j psdata k and a data source belonging to the set psdata l of a ps l ds h psdata l in this sense at least two ps n 2 must exist to define a tcp on the other hand a tep is connected to a data source ds j in a ps k which is in the psdata k set this is represented as 6 t c p x y x y p s d a t a k p s d a t a l k l 7 t e p d s j p s d a t a k 4 2 transversal processing services tps collaborative work between different ps does not necessarily imply reusing either stages or information generated to be consumed by other ps but it can also be used by an external process to provide useful information to the end user we call this type of process a transversal processing services tps a tps takes advantage of data produced by a ps for performing a transformation process to create useful information a tps performs data collection ds j by using teps tep i ds j psdata k multiple data sources can be joined together e g to carry out a data fusion and published for later use by tps to produce information this is defined by joining structured data as follows 8 t e p i d s j r e g j a t t j k g j where reg j is a set of records of data collection reg j reg 1 reg 2 reg n att j is a set of attributes of data collection att j a 1 a 2 a m a k name k type k role k 1 name name of the attribute 2 type data type int double char string etc 3 role role of the attribute which can assume the values of value or keygroup kg j represents a set of attributes a k whose role role assumes the value of keygroup kg j a a k att j role k keygroup to match data from two sources a relation between them must be created in the case of structured data the model considers creating joins among multiple records from one table to another based on keys or group attributes keygroups in this sense two teps are consolidated in a single data source by joining the records from both sources based on the kg j as defined below 9 r e g τ r e g j r e g k a i k g j a h k g k j k 10 a t t τ a t t j a t t k j k 11 k g τ a a i a t t τ r o l e i k e y g r o u p 12 d s τ r e g τ a t t τ k g τ d s τ is a t e p as it is a recursive process multiple extracted data sources teps can be joined in pairs at different levels for example tep 1 and tep 2 can be joined into a single tep 3 and later join tep 3 with a tep 4 a tps i see the example in fig 5 follows an etl process with one input tep j one embedded task embtask and one output tep k in this sense the input of a tps can be any of the union of data sources dsτ a data source obtained by a tep or the output of another tps tep l thus the data is extracted from a tep j and deposited in tep k however there are no restrictions as to which tep to use for the results so j can be equal to k j k or j k 13 t p s i t e p j e m b t a s k t e p k in this paper a service mesh is defined as a pool of transversal coupling and extracting publish services tcps teps as well as pss and the ts created by the end user tps thus a mesh can be represented by the set mesh tps tcp ps tep where each component is exposed as a service 4 3 transversal service ts components design details the proposed model has been developed by using the following components for managing in distributed environments the tss registered in the service mesh coupler uses tcp for coupling either tasks that belong to different ps or multiple solutions through inputs and outputs this entity creates new solutions as a service manager coordinates the integration of the tcps into a new service in either configuration or execution times supervises the arisen of cycles in the resulting solution dag tp publishes the solutions as a service in a service mesh extractors use tep for data retrieval from any intersection of tasks stages of a solution executes embedded services e g analytics machine learning statistics or probabilistic methods and index these data to make them available as a service the transversal processing model considers the following services of transversal processing integrated solutions as a service tpis these services represent the union of either tasks that belong to different ps tss included into a new service these services are created by integrating a set of coupling tcp into a dag tp transversal processing services tps this is an etl based service for consuming extracted data from a dag tp and transforming them into information a tps thus extracts data from a tep transforms data into useful information by using embedded services e g any combination of analytics processing statistics machine learning and delivers load it to other transversal extracting publish point fig 6 depicts the interactions of the model components the dataflows and the materialization of the transversal model when creating big data transversal solutions as a service the following methods have been designed to create a tpis 1 adjacent coupling ac this method enables developers to create new services based on existing tps and or pss this method is useful when the data produced by one solution d a g t p 1 must be used as input data for another solution d a g t p 2 in this method d a g t p 1 is reused even in execution time and no new installation or configuration is required to create a new solution the execution schedule of d a g t p 1 and d a g t p 2 is coordinated by the manager 2 recursive coupling rc this method allows to create a new service by using portions of a ps allowing to divide large solutions into small modules with simpler objectives in this method a dag tp includes a set of solution portions connected through tcp see the example of solution in fig 7 3 multiple sink consume scheme msc this method creates teps to compare results produced by different solutions by using tpss msc is useful when the data produced by multiple ps ps solt and or tps can be used together to get information this offers a way to provide users with crossing information functions as a service ciaas see fig 8 4 4 a prototype based on the transversal processing model fig 9 shows a conceptual representation of the prototype which depicts all the components of the prototype it also considers the services developed to materialize the transversal model pink the existent software reused by this model orange bold lined and the existent software available but not used orange the first implementation of the transversal model was performed over the dagonstar engine by using the dependency management schema known as dagonstar workflow schema sánchez gallegos et al 2021 this schema manages the data used by tasks of the stages running in a ps either locally or on a remote node tcps based on this schema were defined to manage the dependencies of multiple pss by using reserved tag t and then to apply the transversal model to create services this label represents the virtual path in which the results produced by a task can be found by a tpps in this way a user can establish the data path for processing it using the following syntax t ps task in this notation ps is an identifier of the ps and task is the identifier of the task which produce the data e g t ps1 taska this analysis of the dependency management in an engine gave us the insights to identify the place where the tpcs could be invoked by an engine with this experience the transversal model was adjusted not only for the creation of transversal solutions tcp tep and embedded services but also for the creation of rules that the workflow engines could understand in order to create ps by using the programming models used by these engines in this context these rules were defined for dagonstar montella et al 2018 and makeflow albrecht et al 2012 these rules showed that this model is quite extensible to other engines and ps frameworks by creating rules based on the input output results management of each framework fig 9 shows a landscape of the multiple services that can integrate a transversal service 4 4 1 implementation details of transversal processing prototype components the teps tps the coupler extractors and manager were developed in python the tps apis included rest i o messages exchange and ftp for data exchange 2 2 a content delivery network gonzalez et al 2015 is currently on development for data management in the transversal model and a parallelism management schema reyes anastacio et al 2020 as well moreover the end user can consume data from online repositories hydroshare tarboton et al 2014 google drive onedrive ftp servers and skycds gonzalez et al 2015 for this it is only necessary to provide a url that points directly to the data the filename and catalog in the case of skycds in this way an acquisition service is in charge of obtaining it and applying the user defined processing ps or tps the components of the tpss were encapsulated into virtual containers docker 3 3 https docs docker com was used for this prototype the manager organized the virtual containers in the form of micro services including a rest based i o management and a request dispatcher virtual an underlying container management docker compose 4 4 https docs docker com compose for this prototype is used for the deployment of tss table 2 shows the infrastructure used for deploying the prototype on container based cloud where the experiments considered in the case studies were conducted in 5 experimental evaluation and results for testing the functionality of the prototype implementing the transversal processing model three case studies were considered a processing landsat8 imagery to produce earth observation products b processing climate data from the merra 2 project for building an online climate map and c a multiple sink crossing information service based on machine learning for the classification of air pollution values 5 1 case study i processing landsat8 imagery for producing eops by using a recursive coupling of pss the first case study is based on the building and operation of a solution for processing satellite images captured by landsat8 5 5 https glovis usgs gov app roy et al 2014 satellite see fig 10 this solution considers pss for image decompression the correction of bands the application of filters to obtain and produce a set of earth observation products eops and its publication in a geoportal these pss consider the following four tasks uncompress it decompresses tar files that contain all the bands of landsat8 images and corresponding metadata corrections it performs radiometric and atmospheric corrections to each image in the landsat8 imagery and produces corrected images indexes and eops it receives corrected images and creates surface reflectance derived spectral indices such as normalized difference vegetation index ndvi enhanced vegetation index evi soil adjusted vegetation index savi modified soil adjusted vegetation index msavi normalized difference moisture index ndmi normalized burn ratio nbr and normalized burn ratio 2 nbr2 parser it obtains information from the metadata to determine spatio temporal parameters of each eop created by the ps based on geographical location of each eop tiff2jpg it converts the eop s tiff image format into a jpg for reducing the resolution and file size for efficiently visualizing by a geoportal for this case study the pss were organized in the form of two ps by using the recursive coupling rc method method 2 to create a big data transversal service bigdata tp the first ps called corrections landsat8 cl8 includes the uncompress and corrections tasks to transform the bands of the images in the landsat8 repository into new corrected products see fig 11 the second one called processing landsat8 pl8 includes the indexes and eops and tiff2jpg tasks for producing and indexing new eops and lowering the resolution of these eops see fig 12 the bigdata tp service included extractors tep for indexing each of the products original images corrected products and index derivative eops by using the multiple sink consume msc method method 3 this means that the end users not only can get access to the raw data and derivative eops but also to the corrected images radiometric and atmospheric corrections and the indexes the processing and data management of the bigdata tp service is expensive in terms of time execution and memory consumption for example for each landsat8 image with an average size of 250 mb are produced a large set of products to be indexed managed and preserved when the ps finishes its executions in average 10 gb of data are produced per processed image the execution of experiments was carried out based on the following characteristics dataset it includes 23 satellite images m landsat8 of 250 mb of average size with 16 32 bands each band is an image which were processed one by one by the bigdata tp service memory and storage the processing of each image produced an average of 10 gb of data corrected original images and indexed images which required 230 gb of storage for processing the whole dataset it is important to note that this is only a fraction of imagery to conduct eop production for spatio temporal studies as a result it is expected that this capacity considerably increases in production resulting in a big data issue a tep is created for each index the teps run in parallel the experiments were performed 31 times and the median response time metric was evaluated the response time metric represents the sum of the times produced by each stage of each ps considered by the big data service bigdata tp evaluated in this case study a comparison of the response times was performed by each request attended by bigdata tp service fig 13 shows the response times in minutes vertical axis for each component of the bigdata tp in this case cl8 produced significantly longer response time than pl8 meaning this ps creates a bottleneck and it would be re structured to solve this issue fig 14 also shows a performance comparison between bigdata tp service and the traditional pipeline using in both cases two parallel threads per task a reduction in response time of approximately 3 min was obtained in the case of cl8 and pl8 joined by the transversal model in comparison with a solution built traditionally in terms of performance it was observed that the solutions built by using the transversal model not only enable the reusing of the parts of a ps independently and extracting indexing data from these parts but also it is performed without evident overhead even a reduction in time was observed in the experiments by this model because cl8 and pl8 are executed independently and some bottlenecks were reduced in this context the usage of implicit parallelism management inside a solution could even be studied to improve the performance of the resulting services which is evaluated in case study ii fig 14 shows that with the traditional method it is not feasible to identify the stages that represents bottlenecks in of pss where one stage begins and the other ends moreover it is important to note that the solutions created with the transversal processing model are reusable and can be coupled with other existing solutions in addition when using the transversal processing model it is possible not only to modularize the pss to identify performance differences but also to create clones of the slow stages to improve response times although this could be implemented in a simple manner according to the solution design principles this is not the main scope of the model presented in this paper 5 2 case study ii a big data service for processing climate data extracted from the merra 2 project to conduct this case study a solution was created to acquire interpolate and index climate data produced by the nasa project called merra 2 modeling and office 2015 see fig 15 by using the following processing applications 1 acquisition m acq the climate data and products that the merra 2 project makes available through various urls were acquired by a crawler by using spatio temporal parameters the pattern in the web page is defined for the crawler to download data which is available in the nc binary file format 2 interpolation m int in this stage were carried out the extraction and interpolation of data contained in sets of nc files this stage receives two elements as input the path of the folder containing the downloaded nc files and the path of a file that contains the geographical points latitude longitude that will be used to find values in merra products these points are used in the interpolation process 3 formatting and indexing m f i in this stage the metrics of temperature recovered by m int form merra products are standardized from fahrenheit scale to celsius scale the indexing process stores the all the metrics from both sources in a database service the above applications were connected as a processing pipeline then were coupled to the following embedded services tps 1 clustering embedded service this service executes a clustering algorithm k means for grouping the results obtained by the ps for temperature parameters the number of groups value of k in k means is defined in the tps by either the end user or automatically calculated by a quality clustering validation index silhouette index rousseeuw 1987 2 visualization this service receives the temperature groups produced by the clustering service and creates a temperature map by grouping geographical points with similar values which are placed in a map calculated on the fly and on demand depending on the spatio temporal parameters delivered to the bigdata tp service by the end user 3 acquisition and control of data this service supervises the continuous data delivery for each component in the solution moreover this service controls the data distribution in scenarios where the cloning of a ps be required to solve bottleneck issues as those presented in previous case study the solution performs the following sequence of processes i retrieves data from merra 2 project by using m adq task ii executes m int for the interpolation process of a list of coordinates latitude longitude which are obtained from either a geoportal in production or from a configuration file for laboratory experiments iii indexes the results by using m f i tasks and produces data in json format iv executes the clustering of each geographical point included in the spatio temporal parameters received in the bigdata tp service by using the values obtained from merra 2 and uses these metrics for creating groups e g maximum medium temperatures or water precipitation etc with statistical similarity and v creates the climate map for visualizing the results in a web geoportal in this case study the bigdata tp service was used on the fly to retrieve climate data from the merra 2 project corresponding to a series of geographical points of a shape that cover the entire mexican territory and to create a map of climate groups depending on the metric chosen as input parameter in the bigdata tp service this service deployed 32 pss clones this is because mexican territory has 32 administrative regions states one ps per each state as a result the dataset was processed in parallel by the 32 pss see this bigdata tp service depicted in fig 16 the case study considered the acquisition of meteorological data corresponding to seven days subsequently the interpolation process m int was run by the 32 tpss to process a total of 302 099 geographical points corresponding to all the locations of mexico the list of geographical points for the mexican territory was generated by an additional task called m split which takes as input statistical data from inegi 6 6 https www inegi org mx app ageeml national institute of statistics and geography for each location in mexico an identifier and geographic coordinates were extracted generating the list of locations this task additionally creates sub lists of the geographical points per each state which are delivered to the corresponding ps clone for processing the data found in merra 2 products per each geographical point the programming model was used to create scripts for dagonstar and makeflow engines which created the original ps a single ps and deployed it on a containerized cloud infrastructure the pss created by each engine were used in the transversal prototype to create the above described bigdata tp service see fig 16 fig 17 shows the results of the execution times produced by bigdata tp service as a single ps created by using dagonstar and makeflow as it can be seen there is a difference in the response times vertical axis between dagonstar and makeflow horizontal axis which is caused by the different processing schemes used by each engine to create a ps the experiments were executed on the same infrastructure and running the same tasks the times shown in figs 18 and 19 correspond to the response times for each ps vertical axis obtained by makeflow and dagonstar respectively by following the transversal processing model when processing the data from each of 32 states of mexico horizontal axis there are subtle differences in the processing times but the performance behavior is similar for both engines which is proportional to the number of locations that each state has and must be processed when comparing the results obtained by the traditional ps with the bigdata tp service a significant reduction in the execution times was observed for each engine the ps created by makeflow processed all the geographical points in 3930 s 65 5 min whereas the ps created by makeflow with bigdata tp service processed the dataset in 888 s 14 8 min which gives us a 76 improvement difference in execution times in comparison with using the original ps in the case of the pss created by dagonstar similar results were observed the execution time produced by a single ps was 3588 s 59 8 min while the pss produced by this engine deployed by using bigdata tp service was 828 s 13 8 min which also produced an improvement difference in execution times of up to 76 as it can be seen it was possible to process the same number of geographic points in a shorter time when using the bigdata tp service in comparison with the times obtained with a single original ps although the purpose of the transversal model is not to improve the ps performance it is possible for engines to generate solutions that take advantage of previously processed data in order to improve the performance of a solution by cloning pss and execute them in overlapped manner which also can be combined with the parallelism model of each engine to improve the performance of big data services these experiments basically showed that the model can be used by available engines to couple multiple pss and to extract information from different points in the final service at this point we recall that the final stage of the bigdata tp service applies a clustering algorithm to the merra temperature values processed by the 32 pss and produces on demand and on the fly climate maps depending on spatio temporal parameters fig 20 shows the results of a distribution of geographical points throughout the entire mexican territory that were grouped by the clustering embedded service by using a k 12 parameter this number of groups k was chosen according to the number of topoforms 7 7 a set of landforms associated according to some structural and or degradative pattern or patterns into which the country has been divided according to its geology and topography defined by inegi for mexico this study enabled us to compare results produced by the bigdata tp service with topoforms identified for mexico differences were observed when making this comparison for multiples geographical points which however depend on the temporal patterns as a result it is required to carry out an in depth study on the contrast between the merra 2 data and data obtained by ground stations as well as a possible relationship with the topoforms defined for mexico with extensive temporal parameters as well as reduced spatial parameters to quantify and explain these differences this is quite feasible for end users to perform by invoking the above evaluated bigdata tp service as many times as spatio temporal parameters used in each execution 5 3 case study iii a multiple sink crossing information service based on machine learning for the classification of air pollution values three datasets were processed in this case study rama 2012a red automá 2012a spanish acronym for automatic atmospheric monitoring network it contains annual databases with information about the concentrations of pollutants recorded every hour since 1986 redmet 2012b red de met 2012b spanish acronym for meteorological network and solar radiation it contains information on meteorological parameters recorded every hour since 1986 merra modeling and office 2015 modern era retrospective analysis for research and applications it contains databases with meteorological data generated from reanalysis processes three pss were created to process the datasets for mexico city these pss were assembled recursively to create a bigdata tp service for crossing information by using the msc method method 3 this service consolidates the above described data sources into a single data source by using a tep this consolidation of the data sources is feasible since the sources have common fields spatial and temporal parameters in this way it is possible to unify the results using the latitude and longitude data as key groups see fig 21 in this case study a list with geographic coordinates belonging to spatial records in the rama and redmet stations 8 8 http www aire cdmx gob mx default php opc 27zabhnmi 27 dc za were used as input of the crossing information bigdata tp service the pss designed for processing and monitoring data repositories rama and redmet consider the acquisition of data from the primary source see fig 22 these pss also consider a set of pre processing services embedded in tps which were used for preparing in automatic and transparent manners the data in rama and redmet monitoring datasets to adapt these data to data analysis techniques used in processing stage these embedded services depicted in fig 23 include the following data cleaning and preparation services cleaning data c in fig 23 this service eliminates missing or atypical data all non numerical data are discretized missing values are normalized by the mode median of the census station according to the datatype transforming data into records t in fig 23 all data in files each variable of data are tables where columns represent the ground stations rama and redmet stations deployed on mexico city while rows are the measurements made every hour by the stations this task restructures this table to one with the columns of data ground station and value of the variable grouping records g in fig 23 this service groups ground stations by calculating descriptive statistics such as average median mode standard deviation etc for this case the records associated to spatial parameters i e describing the location of ground stations and temporal the same day but different sensing time were described by using a descriptive statistic median used in the experiments nevertheless the statistic metrics mode mean etc can also be selected as input parameter and the service will use this parameter to perform this clustering process the ps processing the merra 2 repository was described in study case ii and it was reused in this bigdata tp service this pre processing procedure was applied to each of the variables produced by both redmet and rama to finally unify and index them into a single table see pollutants for of rama and temperatures for redmet and merra 2 in fig 23 the results produced by the solution for processing rama and redmet were used as input parameters by tpss see data analysis in fig 24 such as clustering descriptive statistics and visualization gratification which were reused from the bigdata tp service previously described in case study ii the overall structure of the solution is shown in fig 24 in terms of performance the execution time produced by processing merra data is three times in comparison with rama and redmet including its corresponding post and pre processing embedded services fig 25 to solve this bottleneck the bigdata tp service executes the embedded services in the tpss for rama and redmet in the bigdata tp service which executes all these tasks in parallel the next step for the manager is to use tps to conduct the crossing information task which includes embedded services for analysis and machine learning an exploratory analysis of all the variables was performed in the data analysis considering a date range of 2016 2020 in this analysis the temperatures obtained by the redmet sensors were compared with those obtained by the merra reanalysis for each station and each day of sensing in the date range an embedded service calculated a new variable called diff that reflects the merra t2mmean and redmet tmp temperature difference for each day and each station fig 26 shows a comparative histogram between the temperatures of the redmet and merra sources diff is not represented in this graph the bigdata tp service allowed us to consume the data produced from any ps this means the data representing the merra redmet temperature difference is used by an embedded service that executes two clustering algorithms k means and the hierarchical clustering algorithm which were validated by using other embedded service executing the silhouette clustering index rousseeuw 1987 this validation was performed to observe how the different data sensing stations are grouped based on the value of variable in each record indexed by the teps e g max tempereature this clustering service was used in two scenarios determining groups based on merra temperature redmet temperature and diff merra temperature redmet temperature determining groups based on all pollutants and air particles associated to spatio temporal parameters in the first scenario k 2 for both clustering algorithms yielded the highest score in the silhouette index which means that the best number for grouping records of the studied datasets according to the temperature is 2 as can be seen in fig 27 the performance of the two algorithms is measured through different k values and the k means algorithm yielded the best performance for k 2 the resulting groups are showed in fig 28 where a clear difference between both groups is evident cluster 0 is the group where the value of the difference between temperatures diff is less than or equal to zero whereas in cluster 1 only the inn and aju stations produced positive diff values as it can be seen a comparative study could be performed in this bigdata tp service by using services embedded in its tpss for the second scenario the same clustering algorithms were applied to the pollution data in order to obtain a result that can be understood graphically and without altering the data to a great extent the selection of variables was chosen through principal component analysis pca schölkopf et al 1997 pca is also provided as an embedded service connected to the clustering embedded service so it is only necessary to specify the parameters to the service to carry out this task either by selecting a variance range that is selecting a number of components that maintain a specified variance or by selecting the specified number of components for this case three main components were chosen among the total of pollution variables thus maintaining a variance in the data greater than 90 instead of the first scenario the clustering algorithm with the highest performance in the silhouette index was the hierarchical clustering algorithm with a total of two clusters fig 29 another notable difference is that the stations obtained in the first scenario do not appear in the same group obtained in the second scenario fig 30 at this point the data has been grouped in automatic manner executed by the crossing information of the bigdata tp service by using spatio temporal parameters the next step for this service was to execute another embedded service to perform a predictive analysis this service extracts the meteorological data obtained by the stations rama and redmet as well as the verification by the merra data and process them by using a machine learning technique to predict pollution values for this study an embedded service based on multi layer perceptron neural network mlp nn was executed in the bigdata tp service for the prediction of pollutants by regression the service extracts from the complete datasets a third part for training the network taking redmet and merra temperatures differential humidity wind speed and direction and airborne particles pm10 and pm25 as input parameters when the mlp nn was tested it was executed by using the remaining dataset and measuring the quality of the results with the metric r2 r squared lewis beck and skalaban 1990 a statistical measure to know how close near to 1 0 the data is to the regression line fig 31 shows the results obtained by predicting co particles having an acceptable variance with respect to the real data represented with an r2 score of 0 92 on the other hand that performance was not obtained when predicting particles such as no fig 32 and no2 fig 33 obtaining r2 scores in a range of 0 6 and 0 7 nevertheless it is possible to improve the performance of the network by configuring the parameters the number of neurons increasing the amount of training data or the selection of another type of neural network in the embedded service of the tps these parameters can be passed through the transversal extracting publish services in this model to do this type of prediction end users are only required for selecting parameters for the corresponding tps which will use it to execute the corresponding algorithm in automatic and transparent manners 6 conclusions this paper presented a novel transversal processing model to build environmental big data services in fog cloud environments this model enables the scientific community to reuse applications and to create bigdata tp services the transversality property of bigdata tp services is achieved by creating transversal coupling points and transversal extraction points through multiple existing solutions the coupling of transversal services create virtual connections between multiple solutions even in execution time to create a new big data services the extraction publish transversal services create pipes for end users services to consume data from different points of a solution and processing the extracted data to produce information in crossing information by using analytics and machine learning services embedded in tps of big data transversal services in this way applications for processing environmental products e g corrections transformations manufacturing coding etc and or for creating useful information by using analytics e g clustering data cleaning anova graphing etc and machine learning neuronal networks random forests etc can be encapsulated into tps as embedded services the results of these services can be consumed by other applications web pages robots or other services these services also can be coupled to other ps to create bigdata tp services by following the transversal model and by using traditional frameworks and engines rules were created for programming models of dagonstar and makeflow engines for end users to add transversality property to these engines the bigdata tp services also enables end users to consume both raw cleaned and processed data by using extracting publish services in this paper was showed that bigdata tp service can be built for the management of the scientific data life cycle from the acquisition of raw data to the preparation to the processing by using analytics and machine learning tools to the retrieval searching of data raw and prepared data as well as information until to the crossing information and decision making process this processing is performed by reusing existing available applications services systems tools without altering the code of these solutions the coupling of multiple existing transversal services recursively is also feasible in this model a prototype was implemented based on this transversal model to create big data processing services which were implemented for conducting three case studies based on processing environment climate and pollution data as well as the building of earth observation products crossing information big data services were also developed by using this prototype the experimental evaluation revealed the efficacy and flexibility of this model to create complex big data processing services by reusing multiple existing applications created with different frameworks and deployed on different it infrastructures it also revealed the efficacy of applying extracting publishing transversal points to create crossing information services and information content extraction services from different data sources the development of a comprehensive workflow technology aware ecosystem enabling creatives and final users to implement widely interconnected computational pipelines and dags devoted to solving ever before handled computational science problems is our vision the short term future evolution will be the implementation of continuously running workflow tasks to enable iot data fed workflows laccetti et al 2013 performing on line data processing in a routinely production oriented fashion peculiar but strategically relevant real world applications as crowdsourced bathymetry data processing marcellino et al 2017 can leverage on the proposed transversal model producing open data distributed on cloud infrastructures as the instrument as a service iaas model montella et al 2008 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work has been partially supported by the project 41756 plataforma tecnológica para la gestión aseguramiento intercambio y preservación de grandes volúmenes de datos en salud y construcción de un repositorio nacional de servicios de análisis de datos de salud by the fordecyt pronaces 
25772,this paper presents a novel transversal agnostic infrastructure and generic processing model to build environmental big data services in the cloud transversality is used for building processing structures ps by reusing coupling multiple existent software for processing environmental monitoring climate and earth observation data even in execution time with datasets available in cloud based repositories infrastructure agnosticism is used for deploying executing pss on in edge fog and or cloud genericity is used to embed analytic merging information machine learning and statistic micro services into pss for automatically and transparently converting pss into big data services to support decision making procedures a prototype was developed for conducting case studies based on the data climate classification earth observation products and making predictions of air data pollution by merging different monitoring climate data sources the experimental evaluation revealed the efficacy and flexibility of this model to create complex environmental big data services keywords big data cloud computing environmental data climate data machine learning data analytic 1 introduction visualization and analysis services have become key for processing and managing large volumes of environmental data which have been keystone for scientific community to conduct complex scientific studies such as climate schnase et al 2017 environment hempelmann et al 2018 ujjwal et al 2020 and earth observation sánchez gallegos et al 2019 montella et al 2019 online repositories following fair principles findable accessible interoperable and reusable tarboton et al 2014 zhang et al 2019 have been created for collecting and making available visualization and analysis software as well as datasets and information for scientific community to establish collaborative work by sharing these tools with end users sun et al 2019 zhang et al 2020 these repositories not only represent a source of solutions for organizations to produce useful information for decision makers xiang and demir 2020 but also an opportunity area to create environmental big data services based on software processing structures ps these structures are built by grouping and reusing available software and datasets pipelines patterns service mesh and workflows are examples of pss available for organizations to build big data services a ps is modelled as the interconnection of a set of processing stages a stage executes a given software represented by a directed acyclic graph dag where the nodes represent stages and edges represent the interconnection of a stage with any of other stage data source sink or other pss these pss process and or manage environmental monitoring data from repositories modeling and office 2015 for instance in a traditional big data scenario a ps commonly consider the following stages i acquisition of environmental monitoring data from a source e g either a dataset published in an online repository modeling and office 2015 data monitoring repositories 2012a red automá 2012a 2012b red de met 2012b or a downloaded produced dataset ii data preparation for converting extracted data into a given format e g geojson json or queries for a database iii pre processing for removing potential erroneous data and or enriching the data by calculating fulfilling missing data iv processing or analysis to convert data into useful information by using software for classification grouping prediction of values merging data information to name a few v visualization that prepare information for being consumed by decision makers today multiple pss are already available for scientific community to process and analyze climate hu et al 2018 pollution cabaneros et al 2019 montella et al 2016 earth observation data sánchez gallegos et al 2019 montella et al 2018a processing contents gonzalez compean et al 2018 montella et al 2018b or predicting meteorological changes muller 2020 goodman et al 2019 these pss produce useful information di luccio et al 2018 zhang et al 2020 that results critical hu et al 2018 for decision making procedures e g to prevent disasters preston et al 2011 nevertheless building environmental big data services based on existent software pieces and or pss is not trivial because of the following restrictions 1 the programming languages used by the available frameworks for creating pss is not necessarily the same which is a problem when the environmental processing software has been written by using a different programming languages 2 pss and big data frameworks could require to be executed on a given platform and because of this it staff commonly ends up installing third party software and performing troubleshooting processes to overcome errors due to installation configuration data unavailability and path mistakes attariyan and flinn 2010 moreover these frameworks commonly impose the installation of third party solution for the management of computational resources 3 reusing solutions is not an immediate option for end users to build a new service this means in most cases a new solution must be built when creating a ps and is not feasible for end users to reuse existent and already tested pss 4 analytic statistic and machine learning are commonly added to the solutions as a third party software hu et al 2018 which could produce issues of programming languages portability and interoperability could arise when considering these frameworks in a solution 5 the access to the data at any stage in traditional big data solutions the pss are created as black boxes where a solution retrieves data from a source e g any of folder cloud location or data lake executes a set of applications at different stages for converting incoming data into useful information that is stored in a sink e g cloud storage location the end user thus only gets access to the final results placed at the sink but not necessarily to the results created produced by intermediate stages which could be useful either as input data for other solutions or being consumed by other end users or applications in this context it makes sense that the scientific community can reuse either portions or whole successfully installed and configured pss paradis 2020 vitolo et al 2015 to create comprehensive environmental big data services which will result in saving time and human infraestructure resources it also makes sense that these pss can be agnostic from programming languages infrastructure and platforms for enabling end users to deploy execute them on in different infrastructures such as edge e g personal computers fog e g servers of a company organization and or cloud e g virtual containers and machines provided by public providers without needing to download data and software or making installing configurations which reduces the need for it staff of organizations to perform troubleshooting processes however reusing already installed and configured pss coupling it to existent pss gao et al 2019 sharing data produced in a ps with another one yi et al 2018 publishing and consuming information at any stage of an existing ps during the data life cycle xue et al 2019 are challenging tasks which have been only partially addressed by solutions in the state of the art such as pipelines laster 2018 workflows montella et al 2018 babuji et al 2018 badia et al 2015 or services deelman et al 2004 albrecht et al 2012 currently for our best knowledge there are no feasible and immediate options provided by different traditional frameworks for organizations to create environmental big data including the aforementioned features without making hardworking adaptations this paper presents a novel transversal agnostic infrastructure and generic processing model to build environmental big data services in the cloud transversality property is used for building ps by reusing coupling multiple existent software for processing environmental monitoring climate and earth observation data even in execution time with datasets available in cloud based repositories the transversal model adds the transversality property to pss by creating and using a set of coupling and extracting transversal points fig 1 shows an example of a transversal service built in the cloud by reusing two existent pss one for processing re analysis merra 2 products and the other for processing monitoring data from ground stations by conagua 1 1 https smn conagua gob mx tools gui emas php placed at mexico as it can be seen the transversal coupling points tcps interconnect two stages one from ps1 and the other one from ps2 transversal union between ps1 and ps2 in fig 1 through stages b and g this virtual coupling is performed even in execution time to create a new independent transversal service ts the transversal extraction points teps enable end users or applications to consume and process data see tep1 and tep2 in fig 1 the extraction services enable either end users or other services to retrieve deliver and or visualize data results produced by a given stage of an existing ps see tep1 between a and b of ps1 and tep2 between g and h of ps2 in fig 1 genericity in the context of this paper refers to the property of analyzing and producing information produced by any environmental software without restrictions of language programming infrastructure platform and software this property is used to embed analytic merging information machine learning and statistic into generic micro services coupled to pss for automatically and transparently converting them into big data services to support decision making procedures the embedded micro services take advantage of teps providing services such as data fusion to produce useful information to support decision making procedures fig 1 shows how two pss are processing two different climate data sources and how embedded micro services consume data from teps for producing visualizing information the first embedded micro service connected to the tep1 executes a classification tool to merge metrics from two data sources and then to group climate metrics by statistic similitude values the second embedded micro service connected to tep2 executes a visualization tool a geoportal in this case for extracting results produced by stage g classification of metrics and to show them in a map by using gis embedded service infrastructure agnosticism is used for deploying executing pss on in edge fog and or cloud without end users requiring to perform installation configuration steps which reduces the need for performing troubleshooting procedures this model also adds agnosticism from infrastructure to the ts any of ps systems of pss or big data services by encapsulating environmental software into virtual containers and managing them by using micro service architecture the transversal generic and agnostic properties added to existent pss enable end users to create complex big data services by reusing multiple existing software created with different frameworks and deployed on different it infrastructures the building of transversal environmental big data services thus can be performed in three simple steps i builds a ps by choosing existing software and datasets tarboton et al 2014 ii creates a new independent service by defining tcps reusing already configured and tested pss any of online available downloaded in a given infrastructure or created by this model iii creates a big data service by using teps and the embedded micro services that produce useful information when performing these three steps a new transversal service is created for end users and or decision makers to consume the information produced by this solution through independent transversal processes service tps applying the three steps to the creation of environmental services also enable decoupling the environmental software from the pss as well as from the analytic and machine learning tools in short the tpss converts a traditional set of software pieces into an environmental big data service a prototype was developed and implemented based on this model to create environmental big data services which were created based on existing environmental software for conducting three case studies the first one for processing of satellite imagery to build earth observation products eops and yielding environmental indexes the second one for processing monitoring and re analysis climate data the resultant data were processed by clustering algorithms to create groups of ground stations by using statistical similitude values the third one is a data fusion service for producing pollution predictions using machine learning tools included in embedded micro services by using multiple data sources such as monitoring and re analysis climate merra modeling and office 2015 redmet 2012b red de met 2012b and monitoring air pollution rama 2012a red automá 2012a sources the experimental evaluation based on the three case studies revealed the efficacy and flexibility of this model to create complex environmental big data services for processing heterogeneous data such as re analysis and monitoring climate and environment data it also revealed the feasibility of this model to perform data information fusion to produce useful results which can be consumed by decision makers through online services tps the paper is organized as follows preliminaries and background information are given in section 2 section 3 frames the state of the art about the topic the novel transversal computing model is described in section 4 the experimental evaluation is explained in section 5 finally section 6 is dedicated to the conclusion remarks and future directions 2 preliminaries 2 1 an overview of processing models for big data in a traditional big data scenario the stages are created as blocks executing a given application or tool by using an extract transform load etl processing model in this model a block extracts data from a source e g any of folder cloud location or data lake executes the applications encapsulated into the stage for transforming the incoming data into useful information that is loaded in a sink e g cloud storage location pipelines and computerized workflows are examples of pss that allow end users to process data through multiple blocks stages such as acquisition pre processing processing and preservation that are chained by following a given sequence the simplest coupling method is a pipeline where the stages are interconnected sequentially following an etl the coupling is performed by using the data sources and sinks of the stages for instance in a traditional big data pipeline an acquisition block stage extracts data from a web page creates indexes transforms and loads the indexes in a database placed commonly in a cloud location where a pre processing stage extracts the indexes from the database to locate the extracted data for cleaning them e g removing outliers and noise data or calculating missing data by using extrapolation this pre processing commonly transforms raw data into a cleaned data version which is also loaded as indexes in the database and stored in a cloud location a processing stage e g analytics or machine learning tools extracts the indexes to get the cleaned data and to transform them into reduced information e g any of categories classification or ordered groups which is loaded in a database as information at this point this information can be consumed by end users in decision making processes vaghefi et al 2017 from the point of view of the users participating in a decision making process all these stages are integrated in a single service and they only has access to the raw data and or resultant information 2 2 an overview of stages coupling method and data management the etl model is used for the coupling of stages through data sources and sinks it is commonly represented as a dag badia et al 2015 deelman et al 2004 which is used for a wide range of tools brikman 2019 and frameworks montella et al 2018 albrecht et al 2012 smart 2011 to create analytic and processing services in either an automatic or semi automatic manner some frameworks impose to end users a programming language e g python montella et al 2018 deelman et al 2004 babuji et al 2018 or java smart 2011 for building pss other frameworks also impose infrastructure platform requirements badia et al 2015 babuji et al 2018 or they only enable the users to use solutions in the cloud cloud 2011 abadi et al 2016 brikman 2019 in real world scenarios this results in a complex ps ecosystem where multiple pss are built by multiple frameworks using different programming languages and deployed on multiple platforms over different infrastructures the following tasks could be required in such an ecosystem which are addressed by the model presented in this paper 1 reusing partial or whole pss any of applications stages pipelines or workflows 2 building services based on multiple existing pss by interconnecting them either in execution or configuration times 3 allowing on the fly and on demand changes in existing ps 4 requiring that different stages sharing a data source e g data warehouse or data lake or sinks 5 consuming data produced by intermediate stages not only from the starting data sources and ending sinks 6 creating information crossing processes by including multiple already developed applications processing multiple data sources or sinks including the sources and or sinks of intermediate stages 3 related work engines and frameworks have been key for the scientific community to build big data pss yue et al 2015 processing workflows and pipelines are not a new technology there are multiple tools available that allow users to design and manage the execution of tasks either locally or in a distributed environment taverna hull et al 2006 for example is a framework created for the execution of bioinformatic workflows offering a variety of processes services in a catalog used for the construction of multiple workflows it also enables designers to invoke external services to incorporate them into their workflows as well as interfaces for the construction and execution of these processes in this way users can build their processing workflows without much experience in computing areas these solutions can be shared through myexperiment goble and de roure 2007 a repository of workflows which allows using algorithms for detecting useful fragments or workflows to create tasks hierarchy of tasks similar to this framework a wide catalog of workflow systems workflow engines has been reported in the state of the art including but not reduced to popular examples such as comps badia et al 2015 tejedor et al 2017 makeflow albrecht et al 2012 pegasus deelman et al 2004 galaxy goecks et al 2010 and dagonstar montella et al 2018 although those engines and frameworks have different characteristics such as the programming model or the tasks execution method all of them have solutions based on dags these structures do not consider explicitly the coupling of multiple workflows by building meta workflows workflows over workflows for instance it is not feasible for these solutions to reuse data already processed by previously executed ps or being executed without changing codifications of applications moreover the installation and configuration of a new ps must be created instead to reuse existing pss which only can be modified at configuration time not in running time after studying these frameworks we observed that the challenge of managing ecosystems of pss has been focused mainly on two directions the first one is focused on the construction of solutions based on hierarchical levels the second one is focused on the composition of new solutions by reusing ps fragments the main usage of the solutions based on a hierarchical approach yildiz et al 2019 is the discovery of pss and the composition of different solutions as a single one it is usually based on two hierarchy levels the first one e g decaf dreher and peterka 2017 for the in situ workflow composition with tasks that exchange messages through memory composed of a set of fields that may or may not be fully utilized by the solutions the second one is the development of new solutions by using meta dags e g by using pycomps and decaf tejedor et al 2017 badia et al 2015 to execute in situ workflows as tasks in a distributed environment in this type of solution the management of dependencies and the transport of data between tasks associated to the edges of the dag is performed by following the meta dag and encompassing the task execution task coordination task parallelization and transport of data in turn the solutions that reuse workflow fragments for the composition of new solutions are mainly focused on algorithms that search for fragments of tasks within workflow repositories that are suitable to be reused when building new solutions zhou et al 2020 garijo et al 2014 some of these works produce the automatic design of solutions by using fragments of workflows but there are still limitations and areas of opportunity of this type of approaches that the model proposed in this paper supports three are three main differences between the former proposals and the transversal model proposed in this paper the first one are the transversal coupling services that provides a new scope for the problem of dealing with a ps ecosystem by managing pss as services these services can be coupled with other ps mainly external ones through a new independent services transversal processing service this can be done without affecting modifying the applications of an existing ps by using some of these stages on different solutions the second one is the management and analytical processing of data based on extracting publishing tss which are focused on transform data into information as well as enabling crossing information processes this converts multiple ps into a configurable big data service instead of a single purpose solution as performed in traditional approaches the last one is that this model allows creating external solutions which means that this model can be also used by traditional frameworks to create pss as services by reusing solutions and the data produced by their stages as shown in the experimental evaluation of this paper 3 1 usability comparison study in order to show the aforementioned differences a comparative usability study from the end user point of view was carried out between some frameworks found in the literature that address the problem described in this paper such as taverna hull et al 2006 galaxy goecks et al 2010 cross workflow fragments zhou et al 2020 and pycomps decaf yildiz et al 2019 and the transversal model proposed in this paper fig 2 shows the differences considering eight different characteristics bidirectional ps coupling it refers to the situations where a solution requires coupling two different pss bidirectionally pss reusing data source and or sink of other pss it refers to situations where pss to reuse results produced by other pss reusing data produced by intermediate ps stages it refers to situations where reusing data produced by intermediate stages of other ps without modifying the code of applications supporting multiple programming and execution models for each ps belonging to a solution adding new pss to a solution reusing portions of a ps reusing a complete ps consolidation of results produced by different pss referring to the ability to consolidate the results analyze index publish visualize etc and to convert data into information each characteristic was rated on a scale of 1 5 1 it is not currently supported by the tool model schema to perform the activity 2 it is not currently supported by the tool model schema to perform the activity but it can be implemented according to its design principles 3 it is currently supported by the tool model scheme to carry out the activity but external tools are required to do it 4 it is currently supported by the tool model schema to perform the activity 5 it is currently supported by the tool model scheme to perform the activity and additionally it offers non functional requirements scalability efficiency security modularity reliability etc as it can be seen fig 2 describes from an end user point of view the opportunity areas of more prominent frameworks and engines and how its integration with a transversal model could fulfill these areas for improving the flexibility and functionality of current frameworks and engines the proposed transversal model takes into account these opportunity areas allowing a management of solutions composed by others using the workflow engines themselves as the task execution tool unifying the programming model and maintaining the characteristics of each model of execution table 1 shows a summary of the qualitative differences of the works considered in this study 4 a novel transversal processing model for environmental big data services in this section we present a novel transversal processing model for building big data services in fog cloud environments the model allows the integration of multiple existing deployed software and or applications into new solutions exposed as a service 4 1 transversal processing structures a ps can be represented as a tuple including a set of stages v and their corresponding connections e 1 p s v e where v represents a set of stages stage i in the form v stage 1 stage 2 stage v e represents a set of ordered pairs including elements of v representing the interconnections between the stages e x y v v x y each stage stage i performs a transformation task following the etl processing model obtaining data from a source ds j and depositing the transformed data in a sink ds k which can be used by either end users or other stages for getting access to the transformed data both the source and the sink belong to a data repository psdata in this context a stage can be represented as follows 2 s t a g e i d s j t a s k d s k d s j d s k p s d a t a j k where ds j represents the input data which is an input for the transformation task ds k represents the output data deposited in a sink e g data warehouse task represents a task an activity or a data transformation process psdata it is a set of all the data sources used by the stages of a ps to represent a structure as a dag nodes represent each stage stage i v of a ps while the edges represent the corresponding ordered pair e which defines the sequence of execution of each stage fig 3 shows an example of a dag defined for a ps a processing pipeline in this example where the ordered execution of the tasks a b c d of each stage produces different versions of data d v x from one stage to another one the model mainly considers scenarios where a set of ps is used to build a transversal service ts that is defined by its dag representation dag tp which is created by using a set of transversal processing points tpp two types of tpp have been defined in this model fig 4 transversal coupling points tcp these points create an abstract intersection between tasks that belong to different pss transversal extraction points tep these points create input output interfaces for solutions to extract deliver data from to existing pss either at setup or run times these points also includes a link to access data or to be used as input in a tps see section 4 1 to transform data into information a dag tp thus can be represented with the following expression 3 d a g t p s o l t l i n k s where solt is the set of ps included in a transversal solution while links is a set of tpp that represents the virtual interconnections of the ps solt each tpp i can be a tcp or a tep this is represented as follows 4 l i n k s t p p 1 t p p 2 t p p m m 1 5 s o l t p s 1 p s 2 p s n n 1 t p p i is a t e p n 2 t p p i is a t c p t e p a tcp represents an interconnection between stages through a data source belonging to the set psdata k of a ps k ds j psdata k and a data source belonging to the set psdata l of a ps l ds h psdata l in this sense at least two ps n 2 must exist to define a tcp on the other hand a tep is connected to a data source ds j in a ps k which is in the psdata k set this is represented as 6 t c p x y x y p s d a t a k p s d a t a l k l 7 t e p d s j p s d a t a k 4 2 transversal processing services tps collaborative work between different ps does not necessarily imply reusing either stages or information generated to be consumed by other ps but it can also be used by an external process to provide useful information to the end user we call this type of process a transversal processing services tps a tps takes advantage of data produced by a ps for performing a transformation process to create useful information a tps performs data collection ds j by using teps tep i ds j psdata k multiple data sources can be joined together e g to carry out a data fusion and published for later use by tps to produce information this is defined by joining structured data as follows 8 t e p i d s j r e g j a t t j k g j where reg j is a set of records of data collection reg j reg 1 reg 2 reg n att j is a set of attributes of data collection att j a 1 a 2 a m a k name k type k role k 1 name name of the attribute 2 type data type int double char string etc 3 role role of the attribute which can assume the values of value or keygroup kg j represents a set of attributes a k whose role role assumes the value of keygroup kg j a a k att j role k keygroup to match data from two sources a relation between them must be created in the case of structured data the model considers creating joins among multiple records from one table to another based on keys or group attributes keygroups in this sense two teps are consolidated in a single data source by joining the records from both sources based on the kg j as defined below 9 r e g τ r e g j r e g k a i k g j a h k g k j k 10 a t t τ a t t j a t t k j k 11 k g τ a a i a t t τ r o l e i k e y g r o u p 12 d s τ r e g τ a t t τ k g τ d s τ is a t e p as it is a recursive process multiple extracted data sources teps can be joined in pairs at different levels for example tep 1 and tep 2 can be joined into a single tep 3 and later join tep 3 with a tep 4 a tps i see the example in fig 5 follows an etl process with one input tep j one embedded task embtask and one output tep k in this sense the input of a tps can be any of the union of data sources dsτ a data source obtained by a tep or the output of another tps tep l thus the data is extracted from a tep j and deposited in tep k however there are no restrictions as to which tep to use for the results so j can be equal to k j k or j k 13 t p s i t e p j e m b t a s k t e p k in this paper a service mesh is defined as a pool of transversal coupling and extracting publish services tcps teps as well as pss and the ts created by the end user tps thus a mesh can be represented by the set mesh tps tcp ps tep where each component is exposed as a service 4 3 transversal service ts components design details the proposed model has been developed by using the following components for managing in distributed environments the tss registered in the service mesh coupler uses tcp for coupling either tasks that belong to different ps or multiple solutions through inputs and outputs this entity creates new solutions as a service manager coordinates the integration of the tcps into a new service in either configuration or execution times supervises the arisen of cycles in the resulting solution dag tp publishes the solutions as a service in a service mesh extractors use tep for data retrieval from any intersection of tasks stages of a solution executes embedded services e g analytics machine learning statistics or probabilistic methods and index these data to make them available as a service the transversal processing model considers the following services of transversal processing integrated solutions as a service tpis these services represent the union of either tasks that belong to different ps tss included into a new service these services are created by integrating a set of coupling tcp into a dag tp transversal processing services tps this is an etl based service for consuming extracted data from a dag tp and transforming them into information a tps thus extracts data from a tep transforms data into useful information by using embedded services e g any combination of analytics processing statistics machine learning and delivers load it to other transversal extracting publish point fig 6 depicts the interactions of the model components the dataflows and the materialization of the transversal model when creating big data transversal solutions as a service the following methods have been designed to create a tpis 1 adjacent coupling ac this method enables developers to create new services based on existing tps and or pss this method is useful when the data produced by one solution d a g t p 1 must be used as input data for another solution d a g t p 2 in this method d a g t p 1 is reused even in execution time and no new installation or configuration is required to create a new solution the execution schedule of d a g t p 1 and d a g t p 2 is coordinated by the manager 2 recursive coupling rc this method allows to create a new service by using portions of a ps allowing to divide large solutions into small modules with simpler objectives in this method a dag tp includes a set of solution portions connected through tcp see the example of solution in fig 7 3 multiple sink consume scheme msc this method creates teps to compare results produced by different solutions by using tpss msc is useful when the data produced by multiple ps ps solt and or tps can be used together to get information this offers a way to provide users with crossing information functions as a service ciaas see fig 8 4 4 a prototype based on the transversal processing model fig 9 shows a conceptual representation of the prototype which depicts all the components of the prototype it also considers the services developed to materialize the transversal model pink the existent software reused by this model orange bold lined and the existent software available but not used orange the first implementation of the transversal model was performed over the dagonstar engine by using the dependency management schema known as dagonstar workflow schema sánchez gallegos et al 2021 this schema manages the data used by tasks of the stages running in a ps either locally or on a remote node tcps based on this schema were defined to manage the dependencies of multiple pss by using reserved tag t and then to apply the transversal model to create services this label represents the virtual path in which the results produced by a task can be found by a tpps in this way a user can establish the data path for processing it using the following syntax t ps task in this notation ps is an identifier of the ps and task is the identifier of the task which produce the data e g t ps1 taska this analysis of the dependency management in an engine gave us the insights to identify the place where the tpcs could be invoked by an engine with this experience the transversal model was adjusted not only for the creation of transversal solutions tcp tep and embedded services but also for the creation of rules that the workflow engines could understand in order to create ps by using the programming models used by these engines in this context these rules were defined for dagonstar montella et al 2018 and makeflow albrecht et al 2012 these rules showed that this model is quite extensible to other engines and ps frameworks by creating rules based on the input output results management of each framework fig 9 shows a landscape of the multiple services that can integrate a transversal service 4 4 1 implementation details of transversal processing prototype components the teps tps the coupler extractors and manager were developed in python the tps apis included rest i o messages exchange and ftp for data exchange 2 2 a content delivery network gonzalez et al 2015 is currently on development for data management in the transversal model and a parallelism management schema reyes anastacio et al 2020 as well moreover the end user can consume data from online repositories hydroshare tarboton et al 2014 google drive onedrive ftp servers and skycds gonzalez et al 2015 for this it is only necessary to provide a url that points directly to the data the filename and catalog in the case of skycds in this way an acquisition service is in charge of obtaining it and applying the user defined processing ps or tps the components of the tpss were encapsulated into virtual containers docker 3 3 https docs docker com was used for this prototype the manager organized the virtual containers in the form of micro services including a rest based i o management and a request dispatcher virtual an underlying container management docker compose 4 4 https docs docker com compose for this prototype is used for the deployment of tss table 2 shows the infrastructure used for deploying the prototype on container based cloud where the experiments considered in the case studies were conducted in 5 experimental evaluation and results for testing the functionality of the prototype implementing the transversal processing model three case studies were considered a processing landsat8 imagery to produce earth observation products b processing climate data from the merra 2 project for building an online climate map and c a multiple sink crossing information service based on machine learning for the classification of air pollution values 5 1 case study i processing landsat8 imagery for producing eops by using a recursive coupling of pss the first case study is based on the building and operation of a solution for processing satellite images captured by landsat8 5 5 https glovis usgs gov app roy et al 2014 satellite see fig 10 this solution considers pss for image decompression the correction of bands the application of filters to obtain and produce a set of earth observation products eops and its publication in a geoportal these pss consider the following four tasks uncompress it decompresses tar files that contain all the bands of landsat8 images and corresponding metadata corrections it performs radiometric and atmospheric corrections to each image in the landsat8 imagery and produces corrected images indexes and eops it receives corrected images and creates surface reflectance derived spectral indices such as normalized difference vegetation index ndvi enhanced vegetation index evi soil adjusted vegetation index savi modified soil adjusted vegetation index msavi normalized difference moisture index ndmi normalized burn ratio nbr and normalized burn ratio 2 nbr2 parser it obtains information from the metadata to determine spatio temporal parameters of each eop created by the ps based on geographical location of each eop tiff2jpg it converts the eop s tiff image format into a jpg for reducing the resolution and file size for efficiently visualizing by a geoportal for this case study the pss were organized in the form of two ps by using the recursive coupling rc method method 2 to create a big data transversal service bigdata tp the first ps called corrections landsat8 cl8 includes the uncompress and corrections tasks to transform the bands of the images in the landsat8 repository into new corrected products see fig 11 the second one called processing landsat8 pl8 includes the indexes and eops and tiff2jpg tasks for producing and indexing new eops and lowering the resolution of these eops see fig 12 the bigdata tp service included extractors tep for indexing each of the products original images corrected products and index derivative eops by using the multiple sink consume msc method method 3 this means that the end users not only can get access to the raw data and derivative eops but also to the corrected images radiometric and atmospheric corrections and the indexes the processing and data management of the bigdata tp service is expensive in terms of time execution and memory consumption for example for each landsat8 image with an average size of 250 mb are produced a large set of products to be indexed managed and preserved when the ps finishes its executions in average 10 gb of data are produced per processed image the execution of experiments was carried out based on the following characteristics dataset it includes 23 satellite images m landsat8 of 250 mb of average size with 16 32 bands each band is an image which were processed one by one by the bigdata tp service memory and storage the processing of each image produced an average of 10 gb of data corrected original images and indexed images which required 230 gb of storage for processing the whole dataset it is important to note that this is only a fraction of imagery to conduct eop production for spatio temporal studies as a result it is expected that this capacity considerably increases in production resulting in a big data issue a tep is created for each index the teps run in parallel the experiments were performed 31 times and the median response time metric was evaluated the response time metric represents the sum of the times produced by each stage of each ps considered by the big data service bigdata tp evaluated in this case study a comparison of the response times was performed by each request attended by bigdata tp service fig 13 shows the response times in minutes vertical axis for each component of the bigdata tp in this case cl8 produced significantly longer response time than pl8 meaning this ps creates a bottleneck and it would be re structured to solve this issue fig 14 also shows a performance comparison between bigdata tp service and the traditional pipeline using in both cases two parallel threads per task a reduction in response time of approximately 3 min was obtained in the case of cl8 and pl8 joined by the transversal model in comparison with a solution built traditionally in terms of performance it was observed that the solutions built by using the transversal model not only enable the reusing of the parts of a ps independently and extracting indexing data from these parts but also it is performed without evident overhead even a reduction in time was observed in the experiments by this model because cl8 and pl8 are executed independently and some bottlenecks were reduced in this context the usage of implicit parallelism management inside a solution could even be studied to improve the performance of the resulting services which is evaluated in case study ii fig 14 shows that with the traditional method it is not feasible to identify the stages that represents bottlenecks in of pss where one stage begins and the other ends moreover it is important to note that the solutions created with the transversal processing model are reusable and can be coupled with other existing solutions in addition when using the transversal processing model it is possible not only to modularize the pss to identify performance differences but also to create clones of the slow stages to improve response times although this could be implemented in a simple manner according to the solution design principles this is not the main scope of the model presented in this paper 5 2 case study ii a big data service for processing climate data extracted from the merra 2 project to conduct this case study a solution was created to acquire interpolate and index climate data produced by the nasa project called merra 2 modeling and office 2015 see fig 15 by using the following processing applications 1 acquisition m acq the climate data and products that the merra 2 project makes available through various urls were acquired by a crawler by using spatio temporal parameters the pattern in the web page is defined for the crawler to download data which is available in the nc binary file format 2 interpolation m int in this stage were carried out the extraction and interpolation of data contained in sets of nc files this stage receives two elements as input the path of the folder containing the downloaded nc files and the path of a file that contains the geographical points latitude longitude that will be used to find values in merra products these points are used in the interpolation process 3 formatting and indexing m f i in this stage the metrics of temperature recovered by m int form merra products are standardized from fahrenheit scale to celsius scale the indexing process stores the all the metrics from both sources in a database service the above applications were connected as a processing pipeline then were coupled to the following embedded services tps 1 clustering embedded service this service executes a clustering algorithm k means for grouping the results obtained by the ps for temperature parameters the number of groups value of k in k means is defined in the tps by either the end user or automatically calculated by a quality clustering validation index silhouette index rousseeuw 1987 2 visualization this service receives the temperature groups produced by the clustering service and creates a temperature map by grouping geographical points with similar values which are placed in a map calculated on the fly and on demand depending on the spatio temporal parameters delivered to the bigdata tp service by the end user 3 acquisition and control of data this service supervises the continuous data delivery for each component in the solution moreover this service controls the data distribution in scenarios where the cloning of a ps be required to solve bottleneck issues as those presented in previous case study the solution performs the following sequence of processes i retrieves data from merra 2 project by using m adq task ii executes m int for the interpolation process of a list of coordinates latitude longitude which are obtained from either a geoportal in production or from a configuration file for laboratory experiments iii indexes the results by using m f i tasks and produces data in json format iv executes the clustering of each geographical point included in the spatio temporal parameters received in the bigdata tp service by using the values obtained from merra 2 and uses these metrics for creating groups e g maximum medium temperatures or water precipitation etc with statistical similarity and v creates the climate map for visualizing the results in a web geoportal in this case study the bigdata tp service was used on the fly to retrieve climate data from the merra 2 project corresponding to a series of geographical points of a shape that cover the entire mexican territory and to create a map of climate groups depending on the metric chosen as input parameter in the bigdata tp service this service deployed 32 pss clones this is because mexican territory has 32 administrative regions states one ps per each state as a result the dataset was processed in parallel by the 32 pss see this bigdata tp service depicted in fig 16 the case study considered the acquisition of meteorological data corresponding to seven days subsequently the interpolation process m int was run by the 32 tpss to process a total of 302 099 geographical points corresponding to all the locations of mexico the list of geographical points for the mexican territory was generated by an additional task called m split which takes as input statistical data from inegi 6 6 https www inegi org mx app ageeml national institute of statistics and geography for each location in mexico an identifier and geographic coordinates were extracted generating the list of locations this task additionally creates sub lists of the geographical points per each state which are delivered to the corresponding ps clone for processing the data found in merra 2 products per each geographical point the programming model was used to create scripts for dagonstar and makeflow engines which created the original ps a single ps and deployed it on a containerized cloud infrastructure the pss created by each engine were used in the transversal prototype to create the above described bigdata tp service see fig 16 fig 17 shows the results of the execution times produced by bigdata tp service as a single ps created by using dagonstar and makeflow as it can be seen there is a difference in the response times vertical axis between dagonstar and makeflow horizontal axis which is caused by the different processing schemes used by each engine to create a ps the experiments were executed on the same infrastructure and running the same tasks the times shown in figs 18 and 19 correspond to the response times for each ps vertical axis obtained by makeflow and dagonstar respectively by following the transversal processing model when processing the data from each of 32 states of mexico horizontal axis there are subtle differences in the processing times but the performance behavior is similar for both engines which is proportional to the number of locations that each state has and must be processed when comparing the results obtained by the traditional ps with the bigdata tp service a significant reduction in the execution times was observed for each engine the ps created by makeflow processed all the geographical points in 3930 s 65 5 min whereas the ps created by makeflow with bigdata tp service processed the dataset in 888 s 14 8 min which gives us a 76 improvement difference in execution times in comparison with using the original ps in the case of the pss created by dagonstar similar results were observed the execution time produced by a single ps was 3588 s 59 8 min while the pss produced by this engine deployed by using bigdata tp service was 828 s 13 8 min which also produced an improvement difference in execution times of up to 76 as it can be seen it was possible to process the same number of geographic points in a shorter time when using the bigdata tp service in comparison with the times obtained with a single original ps although the purpose of the transversal model is not to improve the ps performance it is possible for engines to generate solutions that take advantage of previously processed data in order to improve the performance of a solution by cloning pss and execute them in overlapped manner which also can be combined with the parallelism model of each engine to improve the performance of big data services these experiments basically showed that the model can be used by available engines to couple multiple pss and to extract information from different points in the final service at this point we recall that the final stage of the bigdata tp service applies a clustering algorithm to the merra temperature values processed by the 32 pss and produces on demand and on the fly climate maps depending on spatio temporal parameters fig 20 shows the results of a distribution of geographical points throughout the entire mexican territory that were grouped by the clustering embedded service by using a k 12 parameter this number of groups k was chosen according to the number of topoforms 7 7 a set of landforms associated according to some structural and or degradative pattern or patterns into which the country has been divided according to its geology and topography defined by inegi for mexico this study enabled us to compare results produced by the bigdata tp service with topoforms identified for mexico differences were observed when making this comparison for multiples geographical points which however depend on the temporal patterns as a result it is required to carry out an in depth study on the contrast between the merra 2 data and data obtained by ground stations as well as a possible relationship with the topoforms defined for mexico with extensive temporal parameters as well as reduced spatial parameters to quantify and explain these differences this is quite feasible for end users to perform by invoking the above evaluated bigdata tp service as many times as spatio temporal parameters used in each execution 5 3 case study iii a multiple sink crossing information service based on machine learning for the classification of air pollution values three datasets were processed in this case study rama 2012a red automá 2012a spanish acronym for automatic atmospheric monitoring network it contains annual databases with information about the concentrations of pollutants recorded every hour since 1986 redmet 2012b red de met 2012b spanish acronym for meteorological network and solar radiation it contains information on meteorological parameters recorded every hour since 1986 merra modeling and office 2015 modern era retrospective analysis for research and applications it contains databases with meteorological data generated from reanalysis processes three pss were created to process the datasets for mexico city these pss were assembled recursively to create a bigdata tp service for crossing information by using the msc method method 3 this service consolidates the above described data sources into a single data source by using a tep this consolidation of the data sources is feasible since the sources have common fields spatial and temporal parameters in this way it is possible to unify the results using the latitude and longitude data as key groups see fig 21 in this case study a list with geographic coordinates belonging to spatial records in the rama and redmet stations 8 8 http www aire cdmx gob mx default php opc 27zabhnmi 27 dc za were used as input of the crossing information bigdata tp service the pss designed for processing and monitoring data repositories rama and redmet consider the acquisition of data from the primary source see fig 22 these pss also consider a set of pre processing services embedded in tps which were used for preparing in automatic and transparent manners the data in rama and redmet monitoring datasets to adapt these data to data analysis techniques used in processing stage these embedded services depicted in fig 23 include the following data cleaning and preparation services cleaning data c in fig 23 this service eliminates missing or atypical data all non numerical data are discretized missing values are normalized by the mode median of the census station according to the datatype transforming data into records t in fig 23 all data in files each variable of data are tables where columns represent the ground stations rama and redmet stations deployed on mexico city while rows are the measurements made every hour by the stations this task restructures this table to one with the columns of data ground station and value of the variable grouping records g in fig 23 this service groups ground stations by calculating descriptive statistics such as average median mode standard deviation etc for this case the records associated to spatial parameters i e describing the location of ground stations and temporal the same day but different sensing time were described by using a descriptive statistic median used in the experiments nevertheless the statistic metrics mode mean etc can also be selected as input parameter and the service will use this parameter to perform this clustering process the ps processing the merra 2 repository was described in study case ii and it was reused in this bigdata tp service this pre processing procedure was applied to each of the variables produced by both redmet and rama to finally unify and index them into a single table see pollutants for of rama and temperatures for redmet and merra 2 in fig 23 the results produced by the solution for processing rama and redmet were used as input parameters by tpss see data analysis in fig 24 such as clustering descriptive statistics and visualization gratification which were reused from the bigdata tp service previously described in case study ii the overall structure of the solution is shown in fig 24 in terms of performance the execution time produced by processing merra data is three times in comparison with rama and redmet including its corresponding post and pre processing embedded services fig 25 to solve this bottleneck the bigdata tp service executes the embedded services in the tpss for rama and redmet in the bigdata tp service which executes all these tasks in parallel the next step for the manager is to use tps to conduct the crossing information task which includes embedded services for analysis and machine learning an exploratory analysis of all the variables was performed in the data analysis considering a date range of 2016 2020 in this analysis the temperatures obtained by the redmet sensors were compared with those obtained by the merra reanalysis for each station and each day of sensing in the date range an embedded service calculated a new variable called diff that reflects the merra t2mmean and redmet tmp temperature difference for each day and each station fig 26 shows a comparative histogram between the temperatures of the redmet and merra sources diff is not represented in this graph the bigdata tp service allowed us to consume the data produced from any ps this means the data representing the merra redmet temperature difference is used by an embedded service that executes two clustering algorithms k means and the hierarchical clustering algorithm which were validated by using other embedded service executing the silhouette clustering index rousseeuw 1987 this validation was performed to observe how the different data sensing stations are grouped based on the value of variable in each record indexed by the teps e g max tempereature this clustering service was used in two scenarios determining groups based on merra temperature redmet temperature and diff merra temperature redmet temperature determining groups based on all pollutants and air particles associated to spatio temporal parameters in the first scenario k 2 for both clustering algorithms yielded the highest score in the silhouette index which means that the best number for grouping records of the studied datasets according to the temperature is 2 as can be seen in fig 27 the performance of the two algorithms is measured through different k values and the k means algorithm yielded the best performance for k 2 the resulting groups are showed in fig 28 where a clear difference between both groups is evident cluster 0 is the group where the value of the difference between temperatures diff is less than or equal to zero whereas in cluster 1 only the inn and aju stations produced positive diff values as it can be seen a comparative study could be performed in this bigdata tp service by using services embedded in its tpss for the second scenario the same clustering algorithms were applied to the pollution data in order to obtain a result that can be understood graphically and without altering the data to a great extent the selection of variables was chosen through principal component analysis pca schölkopf et al 1997 pca is also provided as an embedded service connected to the clustering embedded service so it is only necessary to specify the parameters to the service to carry out this task either by selecting a variance range that is selecting a number of components that maintain a specified variance or by selecting the specified number of components for this case three main components were chosen among the total of pollution variables thus maintaining a variance in the data greater than 90 instead of the first scenario the clustering algorithm with the highest performance in the silhouette index was the hierarchical clustering algorithm with a total of two clusters fig 29 another notable difference is that the stations obtained in the first scenario do not appear in the same group obtained in the second scenario fig 30 at this point the data has been grouped in automatic manner executed by the crossing information of the bigdata tp service by using spatio temporal parameters the next step for this service was to execute another embedded service to perform a predictive analysis this service extracts the meteorological data obtained by the stations rama and redmet as well as the verification by the merra data and process them by using a machine learning technique to predict pollution values for this study an embedded service based on multi layer perceptron neural network mlp nn was executed in the bigdata tp service for the prediction of pollutants by regression the service extracts from the complete datasets a third part for training the network taking redmet and merra temperatures differential humidity wind speed and direction and airborne particles pm10 and pm25 as input parameters when the mlp nn was tested it was executed by using the remaining dataset and measuring the quality of the results with the metric r2 r squared lewis beck and skalaban 1990 a statistical measure to know how close near to 1 0 the data is to the regression line fig 31 shows the results obtained by predicting co particles having an acceptable variance with respect to the real data represented with an r2 score of 0 92 on the other hand that performance was not obtained when predicting particles such as no fig 32 and no2 fig 33 obtaining r2 scores in a range of 0 6 and 0 7 nevertheless it is possible to improve the performance of the network by configuring the parameters the number of neurons increasing the amount of training data or the selection of another type of neural network in the embedded service of the tps these parameters can be passed through the transversal extracting publish services in this model to do this type of prediction end users are only required for selecting parameters for the corresponding tps which will use it to execute the corresponding algorithm in automatic and transparent manners 6 conclusions this paper presented a novel transversal processing model to build environmental big data services in fog cloud environments this model enables the scientific community to reuse applications and to create bigdata tp services the transversality property of bigdata tp services is achieved by creating transversal coupling points and transversal extraction points through multiple existing solutions the coupling of transversal services create virtual connections between multiple solutions even in execution time to create a new big data services the extraction publish transversal services create pipes for end users services to consume data from different points of a solution and processing the extracted data to produce information in crossing information by using analytics and machine learning services embedded in tps of big data transversal services in this way applications for processing environmental products e g corrections transformations manufacturing coding etc and or for creating useful information by using analytics e g clustering data cleaning anova graphing etc and machine learning neuronal networks random forests etc can be encapsulated into tps as embedded services the results of these services can be consumed by other applications web pages robots or other services these services also can be coupled to other ps to create bigdata tp services by following the transversal model and by using traditional frameworks and engines rules were created for programming models of dagonstar and makeflow engines for end users to add transversality property to these engines the bigdata tp services also enables end users to consume both raw cleaned and processed data by using extracting publish services in this paper was showed that bigdata tp service can be built for the management of the scientific data life cycle from the acquisition of raw data to the preparation to the processing by using analytics and machine learning tools to the retrieval searching of data raw and prepared data as well as information until to the crossing information and decision making process this processing is performed by reusing existing available applications services systems tools without altering the code of these solutions the coupling of multiple existing transversal services recursively is also feasible in this model a prototype was implemented based on this transversal model to create big data processing services which were implemented for conducting three case studies based on processing environment climate and pollution data as well as the building of earth observation products crossing information big data services were also developed by using this prototype the experimental evaluation revealed the efficacy and flexibility of this model to create complex big data processing services by reusing multiple existing applications created with different frameworks and deployed on different it infrastructures it also revealed the efficacy of applying extracting publishing transversal points to create crossing information services and information content extraction services from different data sources the development of a comprehensive workflow technology aware ecosystem enabling creatives and final users to implement widely interconnected computational pipelines and dags devoted to solving ever before handled computational science problems is our vision the short term future evolution will be the implementation of continuously running workflow tasks to enable iot data fed workflows laccetti et al 2013 performing on line data processing in a routinely production oriented fashion peculiar but strategically relevant real world applications as crowdsourced bathymetry data processing marcellino et al 2017 can leverage on the proposed transversal model producing open data distributed on cloud infrastructures as the instrument as a service iaas model montella et al 2008 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work has been partially supported by the project 41756 plataforma tecnológica para la gestión aseguramiento intercambio y preservación de grandes volúmenes de datos en salud y construcción de un repositorio nacional de servicios de análisis de datos de salud by the fordecyt pronaces 
25773,gaussian processes gps provide statistically optimal predictions in the sense of unbiasedness and maximal precision although the modern implementation of gps as a machine learning technique is more capable and flexible than kriging their employment in environmental science is less routine their flexibility and capability as a spatial data interpolation technique are demonstrated by applying them to groundwater salinity prediction in a data sparse region in australia by learning from multiple data sources including aem and dem data gps have generated groundwater salinity maps with rich local details and quantified uncertainty to support risk based decision making the results demonstrate the great worth of nonpoint data with regional spatial coverage to provide more realistic heterogeneity in aquifer properties that are critical for many studies such as contaminant transport gps should be further encouraged in groundwater science for data interpolation and prediction especially when point measurements are sparse and multiple predictors are available keywords groundwater salinity airborne electromagnetic aem musgrave province australia cokriging 1 introduction salinity is a critical parameter that must be taken into consideration for groundwater exploration and management to locate a water supply bore the salinity needs to meet a minimum requirement depending on the purpose of the water supply salinity is among the standard water properties to be reported when a groundwater sample is analyzed however it is still a challenge to compile a regional salinity map due to its complex interactions with multiple factors such as irrigation climate recharge and rock properties the salinity interpolation is even more difficult in a data sparse region where measurements are only available in limited locations or data density is low traditionally groundwater salinity maps are often compiled based on point samples using spatial interpolation techniques such as kriging and inverse distance weighted interpolation these commonly used spatial interpolation methods usually work well in areas where point data are evenly distributed across a study area with a good density however they are challenged when used for prediction in data sparse regions especially where most data are clustered in only parts of the study area machine learning ml algorithms have shown great promise in learning complex patterns for spatial temporal data prediction using multiple environmental data sources reichstein et al 2019 xu and liang 2021 li and heap 2014 and li et al 2011 provided a comprehensive comparison between popular machine learning methods e g random forest and support vector machine and more traditional spatial data analysis techniques when they were used for spatial interpolations multiple existing studies have applied machine techniques to predict groundwater level change in different hydrogeologic settings sahoo et al 2017 chen et al 2020 rajaee et al 2019 rahman et al 2020 jiang et al 2019 used deep neural networks to identify palaeovalley location using geophysical data however to the best knowledge of the authors they have not been used in the groundwater literature to construct interpolated surfaces for groundwater salinity by integrating information from multiple data sources such as data from digital elevation model dem and airborne electromagnetics aem although various ml algorithms are available gaussian processes gps were chosen in this study because of their close connection to kriging which is commonly used in geosciences gps are also able to generate predictive uncertainty required for risk based decision making without the extra effort mathematically gps can be considered a generalization of kriging into a higher dimensional space rasmussen and williams 2006 diggle and ribeiro 2007 however the usage of terminology for kriging and gps is not consistent in the literature kriging is used in most geology related studies while gps are preferred in machine learning publications some other studies use them interchangeably erickson et al 2018 in the current study we distinguish gps from kriging by the listed characteristics in table 1 despite their mathematical connection although gps are not a new modelling approach their application in groundwater science is very limited as a bayesian inference technique excepted for a few studies using gp as a surrogate model of process based models rajabi and ketabchi 2017 cui et al 2018 we demonstrate the utility of gps by applying them to predict groundwater salinity in an area of 250 180 km in the tectonic musgrave province in the northwestern part of south australia fig 1 the area is a crystalline basement covered by sand dunes desert sediments calcrete palaeovalley sediments and quaternary fluvial outwash the area has a semi arid climate with an unreliable precipitation about 230 mm year varma 2012 while the average annual evaporation is above 3500 mm as a result there is no perennial surface water features in the study area it is populated by several aboriginal communities water supply for these communities 382 ml year mainly relies on groundwater the area is considered a priority zone for future mining development however the development of any potential projects is seriously constrained by limited water resources another goal of the current study is to evaluate the data worth of the aem data for salinity mapping when used in a machine learning framework aem resistivity data have been used in multiple studies to assess groundwater salinity in various contexts particularly for seawater intrusion ball et al 2020 delsman et al 2018 however the previous applications mainly rely on petrophysical relations or linear regression to derive salinity values categories aem surveys have been recently conducted in the study area to enhance knowledge of the available groundwater resources in the area soerensen et al 2016 jiang et al 2019 the effectiveness of the proposed method will also be benchmarked against kriging cokriging 2 method gps are nonparametric models having a finite but unfixed number of parameters that grow with the volume of data this section provides a brief introduction to the fundamental theory of gps for a more comprehensive discussion readers are referred to rasmussen and williams 2006 and schulz et al 2018 a visual and interactive explanation is provided by görtler et al 2019 gps can be considered an extension of multivariate gaussian distribution over an infinite number of jointly gaussian variables a gp is defined as a distribution over a space of continuous functions and is characterised by a mean function m x and a covariance function k x x a sample from gps is a function with its values at any location being distributed according to a gaussian distribution let f x denote a target variable of interest e g groundwater salinity in the current study 1 f x g p m x k x x where x r d is a d dimensional column vector at a particular location in the parameter space x i x i 1 x i 2 x id t where x id is the value of a predictor such as spatial coordinates and bore depths in gps spatial coordinates and other predictors form a high dimensional parameter space interpolation is conducted in this high dimensional parameter space the subscript i in x i indicates sample observation location index in the parameter space the m x and k x x are the mean function and positive definite covariance function respectively for all possible combinations in the parameter space in a bayesian framework gps are viewed as a prior over the function space as observations are taken the prior can be sequentially updated to a posterior distribution over the true function that represents the target variable of interest this posterior distribution allows predictions to be made over the parameter space formed by all the predictors including geographic locations and also other features when the function is continuous over a particular domain the posterior represents our best prediction of the target variable at uncountably locations in the parameter space this sounds daunting but in reality we only need to define the distribution at a finite set of locations we use x to denote a finite collection of inputs at n locations 2 x x 1 x 2 x n the corresponding function values f x f x 1 f x 2 f x n t are a joint multivariate gaussian normal distribution 3 f x n μ k x x where n is the normal distribution symbol μ is the mean vector and the covariance matrix k x x can be denoted by now suppose that we have training data at n 1 locations and we want to make predictions at another n 2 locations for a gp we can write these quantities in terms of conditional gaussian distributions 4 p y 2 y 1 x 1 x 2 where y 1 f x 1 and y 2 f x 2 based on the definition gp y 1 and y 2 are jointly gaussian 5 y 1 y 2 n μ 1 μ 2 σ 11 σ 12 σ 21 σ 22 where 6 μ 1 m x 1 a n 1 1 vector μ 2 m x 2 a n 2 1 vector σ 11 k x 1 x 1 a n 1 n 1 matrix σ 22 k x 2 x 2 a n 2 n 2 matrix σ 12 k x 1 x 2 σ 21 a n 1 n 2 matrix then the conditional distribution of y 2 can be calculated as 7 p y 2 y 1 x 1 x 2 n μ 2 1 σ 2 1 μ 2 1 μ 2 σ 21 σ 11 1 y 1 μ 1 σ 21 σ 11 1 y 1 a s s u m i n g a p r i o r m e a n w i t h μ 0 σ 2 1 σ 22 σ 21 σ 11 1 σ 12 this can be rearranged as 8 μ 2 1 σ 21 σ 11 1 y 1 σ 11 1 σ 12 y 1 9 σ 2 1 σ 22 σ 21 σ 11 1 σ 12 σ 22 σ 11 1 σ 12 σ 12 the expected value and covariance matrix of the vector y 2 with input samples contained in x2 can then be calculated from the above equations note that a noise term should be added to σ 11 when noise is considered in the above equation σ 11 and y 1 are directly derived from the training dataset other matrices need to be filled through a covariance function that is learned fitted from σ 11 in machine learning covariance functions are usually referred to as kernel functions a covariance matrix is calculated through the kernel by plugging in the observation data a simple example is provided in the supporting information to demonstrate how to use a radial basis function rbf kernel to populate a covariance matrix therefore the selection of kernel functions plays a critical role in gp machine learning for example one of the most commonly used kernels is the rbf kernel 10 k r b f x x σ r b f 2 exp 1 2 l 2 x x 2 where σ r b f 2 is a scale hyperparameter and l is a length hyperparameter to be learned from the training dataset hyperparameters are determined by maximizing the marginal likelihood p y 1 x 1 θ of the gp 11 θ ˆ a r g m a x θ p y 1 x 1 θ 12 log p y 1 x 1 θ 1 2 y 1 t k y 1 1 y 1 1 2 log k y 1 1 n 2 log 2 π where θ is a vector containing the required kernel hyperparameters the scale and length parameters for k r b f x x and k y 1 is the covariance matrix for measurements y 1 the first component in equation 12 measures the data fitting performance the second component depends on the model complexity and the third is a normalization term an automatic trade off between model fitting and model complexity is achieved by minimizing the negative log marginal likelihood rasmussen and williams 2006 provided a comprehensive discussion with examples about this unique property in their book chapter 5 as a result some researchers considered gps as an occam s razor implementation via regularization and bayesian model selection rasmussen 2003 gps have been implemented in different scientific computing environments such as gpml for matlab and octave gptk and lagp for r and scikit learn gpy gpflow and gpytorch for python as examples a good review of existing gp software is provided by erickson et al 2018 the selection of software can be driven by multiple factors such as the researchers experience and the size of the problem gpy was used in the present study gpy 2012 3 workflow and data the groundwater bore data including locations screen depth and salinity were sourced from waterconnect https www waterconnect sa gov au south australia s water information sharing portal the aem data were based on two recent regional scale aem surveys 2 km line spacing in the study area using the tempst system and the skytem system the electrical conductivity data were compiled and processed by soerensen et al 2016 using the aarhusinv 1d processing and inversion code more information can be found in the corresponding reference the data analysis employed for our application generally follows a standard machine learning workflow starting with problem definition and then proceeding through stages of data collection and cleaning data analysis and feature engineering and finally model training and prediction fig 2 a critical step is to split the dataset into a training set and a test set at the early stage of the analysis prior to exploratory analysis to ensure that the test data is able to provide an independent assessment of model performance due to the fact that most data are located in the vicinity of the northeastern border of the study area the aem extent it is also critical to include representative test bores from the western side of the study area stratified sampling was conducted to ensure that the test data were spatially representative overall 5 of the data were set aside for test and not used for training the gp model in addition to ensure that the model did indeed provide stable predictions a leave one out cross validation analysis for 30 random samples was also undertaken on the training data the predictors or features used for modelling groundwater salinity include the spatial locations dem aem electrical conductivity distance to streams and depth of groundwater bores fig 3 the features have very different value ranges spanning orders of magnitude they were standardized during the gp modelling the dem of the study area is shown in fig 1 the aem electrical conductivity data are available for the study area at 10 m intervals from the ground surface down to a depth of 200 m with a cell size of 400 m the conductivity data at two representative depth of 50 and 100 m are presented in fig 4 surface water features typically impact shallow groundwater quality through surface water groundwater interactions thus the euclidean distance to the nearest stream was also derived as a spatial predictor of groundwater salinity locations of streams can be found in fig 1 groundwater salinity data total dissolved solids were available for 384 bores in the aem covered area fig 1 when multiple salinity measurements were available for the same location the mean was used to ensure no location is over weighted the bottom of the bore was used to approximate the depth where the water sample was obtained the exploratory analysis found that the salinity is approximately log normally distributed thus the prediction uses the log of salinity mg l with base 10 aem data is log transformed because the raw values are seriously skewed with most mass is close to the lower conductivity bound based on a pearson correlation analysis with the logged salinity locations dem and aem show a moderate linear correlation around 0 3 in contrast the distance to streams and drilling depths only showed a minor linear correlation around 0 1 this can be an indicator of a weak gw sw interaction which is consistent with the insignificant recharge estimate for most of the study area leaney et al 2013 the prediction was conducted using a grid based method once the gps were trained a cell size of 500 m was chosen to generate regional scale salinity map to identify low salinity zones for further exploration the trained model was able to provide depth dependent maps however because the depth data were largely uncertain and only showed a minor correlation with available salinity data in the study area only maps for a representative depth were presented to demonstrate the prediction performance the representative depth was chosen as the middle point depth of the paleo valley in areas where the paleo valley was thicker than 60 m while 30 m was used elsewhere to explore data worth and benchmark the results against kriging salinity maps were created for seven scenarios table 2 4 results and discussion 4 1 kernel selection and optimization as mentioned previously kernel selection is a critical step in building a gp model because it controls the similarity between locations the selection not only requires a good understanding of the data but also the characteristics of various kernels conceptually we need to find the right kernel function to best explain the covariance in the observation data although there are some attempts to automatic this process abdessalem et al 2017 it largely remains a trial and error process depending on experience and domain knowledge a good reference for the characteristics of common kernels is the kernel cookbook by david duvenaud https www cs toronto edu duvenaud cookbook based on an exploratory data analysis fig 5 a mixed kernel of the rbf kernel and the linear kernel and the white kernel was deemed suitable for our data the linear kernel component was included to reflect the weak linear correlation between the logarithm of tds and some predictors the rbf kernel was included to allow more flexible wiggly change the defined kernels for the tested gp models are presented below note that the default gpy setting includes a gaussian noise in the likelihood function therefore we did not define a white noise kernel explicitly for other implementation users may need to define the white kernel explicitly case 4 s kernel and model were defined as image 1 case 5 s kernel and model were defined as image 2 case 6 s kernel and model were defined as image 3 case 7 s kernel and model were defined as image 4 before a gp model is used for prediction the kernel hyperparameters need to be optimized this was conducted in the study by minimizing the negative log marginal likelihood through the l bfgs limited memory broyden fletcher goldfarb shanno algorithm morales and nocedal 2011 although l bfgs is an efficient quasi newton method that uses both first and second derivatives to find local extrema of functions the gradient based method still likely converges to a local minimum for complex models the l bfgs was enhanced by running the optimization multiple times from different and random starting locations to avoid local optima although it is a standard practice to use gradient based optimization for gp modelling more sophisticated global optimization techniques such as evolutionary algorithms petelin et al 2011 and bayesian optimization joy et al 2016 can be used the optimization performance is also related to the prior distributions and initial values of the hyperparameters a useful heuristic approach is to standardize the input data subtract the mean and divide by the standard deviation the standardization makes it easier to compare the correlation distance for different predictors without considering the unit difference after this transformation a good starting parameter for lengthscale hyperparameters is one we applied a prior range of 0 5 3 in our gp models an alternative approach is to initialize the lengthscale hyperparameters using the media distance among all the training locations another challenge is to determine the relative contribution of the signal and noise variances when data are sparse or highly uncertain the variations of the training targets can be explained either by the signal variance or the noise variance the tds data were standardized in the current study thus a total variance of one is assumed as defined in the models above a prior range of 0 01 0 3 was used for the noise variance the signal variance was not constrained explicitly a common practice is to ensure that the signal to noise ratio is larger than one if we believe that the training targets are controlled by a hidden process rather than random noise when multiple kernels or kernel configurations are available and cannot be ranked based on expert knowledge a formal model selection strategy is required such as bayesian model selection and statistics of generalization errors chapter 5 of rasmussen and williams 2006 provided an excellent discussion for gp model selection a combination of linear and rbf kernels was chosen based on the authors experience in the present study furthermore we rely on cross validation statistics to determine the relevance of different axes one way is to allow a correlated kernel in the parameter space formed by the easting and northing locations kernels along other axes are then added to this correlated kernel one by one see the kernel definition for case 6 above another way is to implement a more flexible anisotropic kernel in the 6d space case 7 rmse and r2 of the leave one out cross validation analysis indicate that the kernel in case 6 is a better choice thus case 6 is considered as the preferred model for salinity mapping in the current study with the optimized hyperparameter set the model performance of case 6 on the validation data is shown in fig 6 for case 1 among the 23 test bores only the salinity of one bore lay outside the 95 prediction interval of the gp the one bore outside the 95 prediction intervals is at the higher end of the salinity range this means that the trained gp has lower prediction reliability at the high salinity locations above 8000 mg l because there is less training data when it is close to the high salinity area this is not ideal but the interest in the study area is more focused on the low salinity groundwater rather than the high salinity one to meet the future water supply requirement the validation also suggests wide prediction intervals for a large proportion of the test locations this should be improved by collecting more better quality data especially from the southwest region 4 2 salinity interpolation when aem data were not used case 5 the analysis showed a salinity distribution that generally followed the topography elevation fig 7 c low salinity was predicted along the northern boundary of the study area near the basement outcrops where recharge is expected to occur the salinity gradually increases towards the south towards the low lying areas meanwhile the distance to streams also shows some local impact on the salinity although there is no simple linear relationship between salinity and the distance to streams in the collected data there is a weak trend of slight salinity decreasing as we move away from an ephemeral stream and when the distance is less than 10 km fig 5 this can be observed in the eastern part of the prediction map when aem data were included in the training data case 6 the cross validation rmse was reduced to 0 221 from 0 264 while the coefficient of determination r2 was increased to 0 608 from 0 527 although the changes of rmse and r2 are below 20 the spatial distribution presents a different pattern with more identifiable local details although the influence of dem is obvious at the regional scale local patches of low high salinity were more significantly influenced by the aem data the results indicate four low salinity zones in the study area fig 7e zone 2 is not obvious when aem data were not included zone 3 and zone 4 can also be observed in case 5 however the salinity between the two zones is higher in case 6 these zones can be prioritized for further exploration to have a more comprehensive understanding of their local characteristics such as sustainable yield and sediment properties the four zones are generally located in the relatively low uncertainty area fig 7f subplots g and h in fig 7 case 7 provide a good example to show the influence of kernel selection and configuration as stated in the previous section both case 6 and case 7 include six predictors they are only different in the strategy we used to combine kernels along different axes the kernel of case 7 leads to higher variances for the liner and noise components in the combined kernel therefore the gp model of case 7 has a limited generalization capacity resulting rapid uncertainty increase when it is away from the training data fig 7h 4 3 kriging cokriging results to ensure that gps can generate comparable results with the more traditional spatial interpolation techniques three kriging cokriging exercises were conducted for comparison ordinary kriging and ordinary cokriging was used for case 1 and cases 2 and 3 respectively the linear model of coregionalization lmc was used for variogram modelling when cokriging was conducted it is beyond the scope of the current paper to describe the geostatistical theory of kriging cokriging many good references for geostatistics are available for interested readers such as oliver and webster 2014 kitanidis 1997 and pyrcz and deutsch 2014 the gp model in case 4 fig 7a only includes the location predictors to allow a direct comparison with ordinary kriging in case 1 fig 8 a the kernel of the gp model for case 4 is defined in the previous section a spherical variogram model with a nugget effect of 0 08 was fitted by eyeballing for case 1 the two cases show a similar trend in terms of the groundwater salinity variation although with some different local details predictions from the gp model have a higher standard deviation fig 7b than the kriging model fig 8b this is because that the gp learning tend to prefer a model with a noise variance that is higher than the nugget effect of the kriging model the dem and aem data were selected as the secondary variable in case 2 and case 3 respectively fig 8c and e as expected the introduction of a more exhaustively sampled secondary variable adds local details to the interpolation cokriging with secondary variables generates more extreme predictions indicated by the dark blue and red locations in fig 8c and e the cokriging interpolations of case 2 and case 3 also reveal the four low salinity zones in case 6 although they are generally less obvious the structure pattern of the second variable is obvious when cokriging is used for example it is easy to observe the impact of the aem structure when we compare fig 8e with fig 8a the results of gp prediction case 6 can be considered a weighted mixture of the 6 predictors although there is no limitation for the number of secondary variables for cokriging theoretically its actual application is often limited to one secondary variable due to the complexity to fit multiple variograms and cross variograms furthermore the multiple variograms cannot be modelled independently due to the positive definite requirement of the cross covariance matrix however the application of gps does not have such limitation as shown in the study it is much simpler to add or remove a predictor from the training data in gp machine learning 5 conclusions the flexibility and capability of gaussian processes gps were demonstrated by applying it to generate groundwater salinity maps in the data sparse musgrave province in australia our results suggest that nonpoint data such as aem electrical conductivity and dem data are valuable for hydraulic property estimation particularly when local details and heterogeneity are considered critical although these data can also be taken into account as secondary variables in cokriging the modern implementation of gps makes this process much more straightforward when data for variables of interest are sparse and noisy or clustered to only parts of the study area multiple data sources even when they are only weakly related to the target variables can add precious information to the interpolation under this type of scenario modelling with gps provides a flexible and systematic bayesian framework to generate unbiased prediction using all available covariates with a rigorous quantification of uncertainty to support risk based decision making the use of gps for modelling spatial datasets with multiple covariates should be further encouraged in groundwater science and in earth and environmental science more broadly we conclude the paper by reminding the readers to be aware of the computing limitation of gps standard gps use the entire dataset to perform the prediction resulting a computational complexity o n 3 due to the inversion of a n n matrix where n is the size of the training sample their efficiency can rapidly decrease with the increase of training data when the efficiency becomes a concern sparse or approximate gps should be considered gramacy and apley 2015 liu et al 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the study was funded by goyder institute for water research and csiro 
25773,gaussian processes gps provide statistically optimal predictions in the sense of unbiasedness and maximal precision although the modern implementation of gps as a machine learning technique is more capable and flexible than kriging their employment in environmental science is less routine their flexibility and capability as a spatial data interpolation technique are demonstrated by applying them to groundwater salinity prediction in a data sparse region in australia by learning from multiple data sources including aem and dem data gps have generated groundwater salinity maps with rich local details and quantified uncertainty to support risk based decision making the results demonstrate the great worth of nonpoint data with regional spatial coverage to provide more realistic heterogeneity in aquifer properties that are critical for many studies such as contaminant transport gps should be further encouraged in groundwater science for data interpolation and prediction especially when point measurements are sparse and multiple predictors are available keywords groundwater salinity airborne electromagnetic aem musgrave province australia cokriging 1 introduction salinity is a critical parameter that must be taken into consideration for groundwater exploration and management to locate a water supply bore the salinity needs to meet a minimum requirement depending on the purpose of the water supply salinity is among the standard water properties to be reported when a groundwater sample is analyzed however it is still a challenge to compile a regional salinity map due to its complex interactions with multiple factors such as irrigation climate recharge and rock properties the salinity interpolation is even more difficult in a data sparse region where measurements are only available in limited locations or data density is low traditionally groundwater salinity maps are often compiled based on point samples using spatial interpolation techniques such as kriging and inverse distance weighted interpolation these commonly used spatial interpolation methods usually work well in areas where point data are evenly distributed across a study area with a good density however they are challenged when used for prediction in data sparse regions especially where most data are clustered in only parts of the study area machine learning ml algorithms have shown great promise in learning complex patterns for spatial temporal data prediction using multiple environmental data sources reichstein et al 2019 xu and liang 2021 li and heap 2014 and li et al 2011 provided a comprehensive comparison between popular machine learning methods e g random forest and support vector machine and more traditional spatial data analysis techniques when they were used for spatial interpolations multiple existing studies have applied machine techniques to predict groundwater level change in different hydrogeologic settings sahoo et al 2017 chen et al 2020 rajaee et al 2019 rahman et al 2020 jiang et al 2019 used deep neural networks to identify palaeovalley location using geophysical data however to the best knowledge of the authors they have not been used in the groundwater literature to construct interpolated surfaces for groundwater salinity by integrating information from multiple data sources such as data from digital elevation model dem and airborne electromagnetics aem although various ml algorithms are available gaussian processes gps were chosen in this study because of their close connection to kriging which is commonly used in geosciences gps are also able to generate predictive uncertainty required for risk based decision making without the extra effort mathematically gps can be considered a generalization of kriging into a higher dimensional space rasmussen and williams 2006 diggle and ribeiro 2007 however the usage of terminology for kriging and gps is not consistent in the literature kriging is used in most geology related studies while gps are preferred in machine learning publications some other studies use them interchangeably erickson et al 2018 in the current study we distinguish gps from kriging by the listed characteristics in table 1 despite their mathematical connection although gps are not a new modelling approach their application in groundwater science is very limited as a bayesian inference technique excepted for a few studies using gp as a surrogate model of process based models rajabi and ketabchi 2017 cui et al 2018 we demonstrate the utility of gps by applying them to predict groundwater salinity in an area of 250 180 km in the tectonic musgrave province in the northwestern part of south australia fig 1 the area is a crystalline basement covered by sand dunes desert sediments calcrete palaeovalley sediments and quaternary fluvial outwash the area has a semi arid climate with an unreliable precipitation about 230 mm year varma 2012 while the average annual evaporation is above 3500 mm as a result there is no perennial surface water features in the study area it is populated by several aboriginal communities water supply for these communities 382 ml year mainly relies on groundwater the area is considered a priority zone for future mining development however the development of any potential projects is seriously constrained by limited water resources another goal of the current study is to evaluate the data worth of the aem data for salinity mapping when used in a machine learning framework aem resistivity data have been used in multiple studies to assess groundwater salinity in various contexts particularly for seawater intrusion ball et al 2020 delsman et al 2018 however the previous applications mainly rely on petrophysical relations or linear regression to derive salinity values categories aem surveys have been recently conducted in the study area to enhance knowledge of the available groundwater resources in the area soerensen et al 2016 jiang et al 2019 the effectiveness of the proposed method will also be benchmarked against kriging cokriging 2 method gps are nonparametric models having a finite but unfixed number of parameters that grow with the volume of data this section provides a brief introduction to the fundamental theory of gps for a more comprehensive discussion readers are referred to rasmussen and williams 2006 and schulz et al 2018 a visual and interactive explanation is provided by görtler et al 2019 gps can be considered an extension of multivariate gaussian distribution over an infinite number of jointly gaussian variables a gp is defined as a distribution over a space of continuous functions and is characterised by a mean function m x and a covariance function k x x a sample from gps is a function with its values at any location being distributed according to a gaussian distribution let f x denote a target variable of interest e g groundwater salinity in the current study 1 f x g p m x k x x where x r d is a d dimensional column vector at a particular location in the parameter space x i x i 1 x i 2 x id t where x id is the value of a predictor such as spatial coordinates and bore depths in gps spatial coordinates and other predictors form a high dimensional parameter space interpolation is conducted in this high dimensional parameter space the subscript i in x i indicates sample observation location index in the parameter space the m x and k x x are the mean function and positive definite covariance function respectively for all possible combinations in the parameter space in a bayesian framework gps are viewed as a prior over the function space as observations are taken the prior can be sequentially updated to a posterior distribution over the true function that represents the target variable of interest this posterior distribution allows predictions to be made over the parameter space formed by all the predictors including geographic locations and also other features when the function is continuous over a particular domain the posterior represents our best prediction of the target variable at uncountably locations in the parameter space this sounds daunting but in reality we only need to define the distribution at a finite set of locations we use x to denote a finite collection of inputs at n locations 2 x x 1 x 2 x n the corresponding function values f x f x 1 f x 2 f x n t are a joint multivariate gaussian normal distribution 3 f x n μ k x x where n is the normal distribution symbol μ is the mean vector and the covariance matrix k x x can be denoted by now suppose that we have training data at n 1 locations and we want to make predictions at another n 2 locations for a gp we can write these quantities in terms of conditional gaussian distributions 4 p y 2 y 1 x 1 x 2 where y 1 f x 1 and y 2 f x 2 based on the definition gp y 1 and y 2 are jointly gaussian 5 y 1 y 2 n μ 1 μ 2 σ 11 σ 12 σ 21 σ 22 where 6 μ 1 m x 1 a n 1 1 vector μ 2 m x 2 a n 2 1 vector σ 11 k x 1 x 1 a n 1 n 1 matrix σ 22 k x 2 x 2 a n 2 n 2 matrix σ 12 k x 1 x 2 σ 21 a n 1 n 2 matrix then the conditional distribution of y 2 can be calculated as 7 p y 2 y 1 x 1 x 2 n μ 2 1 σ 2 1 μ 2 1 μ 2 σ 21 σ 11 1 y 1 μ 1 σ 21 σ 11 1 y 1 a s s u m i n g a p r i o r m e a n w i t h μ 0 σ 2 1 σ 22 σ 21 σ 11 1 σ 12 this can be rearranged as 8 μ 2 1 σ 21 σ 11 1 y 1 σ 11 1 σ 12 y 1 9 σ 2 1 σ 22 σ 21 σ 11 1 σ 12 σ 22 σ 11 1 σ 12 σ 12 the expected value and covariance matrix of the vector y 2 with input samples contained in x2 can then be calculated from the above equations note that a noise term should be added to σ 11 when noise is considered in the above equation σ 11 and y 1 are directly derived from the training dataset other matrices need to be filled through a covariance function that is learned fitted from σ 11 in machine learning covariance functions are usually referred to as kernel functions a covariance matrix is calculated through the kernel by plugging in the observation data a simple example is provided in the supporting information to demonstrate how to use a radial basis function rbf kernel to populate a covariance matrix therefore the selection of kernel functions plays a critical role in gp machine learning for example one of the most commonly used kernels is the rbf kernel 10 k r b f x x σ r b f 2 exp 1 2 l 2 x x 2 where σ r b f 2 is a scale hyperparameter and l is a length hyperparameter to be learned from the training dataset hyperparameters are determined by maximizing the marginal likelihood p y 1 x 1 θ of the gp 11 θ ˆ a r g m a x θ p y 1 x 1 θ 12 log p y 1 x 1 θ 1 2 y 1 t k y 1 1 y 1 1 2 log k y 1 1 n 2 log 2 π where θ is a vector containing the required kernel hyperparameters the scale and length parameters for k r b f x x and k y 1 is the covariance matrix for measurements y 1 the first component in equation 12 measures the data fitting performance the second component depends on the model complexity and the third is a normalization term an automatic trade off between model fitting and model complexity is achieved by minimizing the negative log marginal likelihood rasmussen and williams 2006 provided a comprehensive discussion with examples about this unique property in their book chapter 5 as a result some researchers considered gps as an occam s razor implementation via regularization and bayesian model selection rasmussen 2003 gps have been implemented in different scientific computing environments such as gpml for matlab and octave gptk and lagp for r and scikit learn gpy gpflow and gpytorch for python as examples a good review of existing gp software is provided by erickson et al 2018 the selection of software can be driven by multiple factors such as the researchers experience and the size of the problem gpy was used in the present study gpy 2012 3 workflow and data the groundwater bore data including locations screen depth and salinity were sourced from waterconnect https www waterconnect sa gov au south australia s water information sharing portal the aem data were based on two recent regional scale aem surveys 2 km line spacing in the study area using the tempst system and the skytem system the electrical conductivity data were compiled and processed by soerensen et al 2016 using the aarhusinv 1d processing and inversion code more information can be found in the corresponding reference the data analysis employed for our application generally follows a standard machine learning workflow starting with problem definition and then proceeding through stages of data collection and cleaning data analysis and feature engineering and finally model training and prediction fig 2 a critical step is to split the dataset into a training set and a test set at the early stage of the analysis prior to exploratory analysis to ensure that the test data is able to provide an independent assessment of model performance due to the fact that most data are located in the vicinity of the northeastern border of the study area the aem extent it is also critical to include representative test bores from the western side of the study area stratified sampling was conducted to ensure that the test data were spatially representative overall 5 of the data were set aside for test and not used for training the gp model in addition to ensure that the model did indeed provide stable predictions a leave one out cross validation analysis for 30 random samples was also undertaken on the training data the predictors or features used for modelling groundwater salinity include the spatial locations dem aem electrical conductivity distance to streams and depth of groundwater bores fig 3 the features have very different value ranges spanning orders of magnitude they were standardized during the gp modelling the dem of the study area is shown in fig 1 the aem electrical conductivity data are available for the study area at 10 m intervals from the ground surface down to a depth of 200 m with a cell size of 400 m the conductivity data at two representative depth of 50 and 100 m are presented in fig 4 surface water features typically impact shallow groundwater quality through surface water groundwater interactions thus the euclidean distance to the nearest stream was also derived as a spatial predictor of groundwater salinity locations of streams can be found in fig 1 groundwater salinity data total dissolved solids were available for 384 bores in the aem covered area fig 1 when multiple salinity measurements were available for the same location the mean was used to ensure no location is over weighted the bottom of the bore was used to approximate the depth where the water sample was obtained the exploratory analysis found that the salinity is approximately log normally distributed thus the prediction uses the log of salinity mg l with base 10 aem data is log transformed because the raw values are seriously skewed with most mass is close to the lower conductivity bound based on a pearson correlation analysis with the logged salinity locations dem and aem show a moderate linear correlation around 0 3 in contrast the distance to streams and drilling depths only showed a minor linear correlation around 0 1 this can be an indicator of a weak gw sw interaction which is consistent with the insignificant recharge estimate for most of the study area leaney et al 2013 the prediction was conducted using a grid based method once the gps were trained a cell size of 500 m was chosen to generate regional scale salinity map to identify low salinity zones for further exploration the trained model was able to provide depth dependent maps however because the depth data were largely uncertain and only showed a minor correlation with available salinity data in the study area only maps for a representative depth were presented to demonstrate the prediction performance the representative depth was chosen as the middle point depth of the paleo valley in areas where the paleo valley was thicker than 60 m while 30 m was used elsewhere to explore data worth and benchmark the results against kriging salinity maps were created for seven scenarios table 2 4 results and discussion 4 1 kernel selection and optimization as mentioned previously kernel selection is a critical step in building a gp model because it controls the similarity between locations the selection not only requires a good understanding of the data but also the characteristics of various kernels conceptually we need to find the right kernel function to best explain the covariance in the observation data although there are some attempts to automatic this process abdessalem et al 2017 it largely remains a trial and error process depending on experience and domain knowledge a good reference for the characteristics of common kernels is the kernel cookbook by david duvenaud https www cs toronto edu duvenaud cookbook based on an exploratory data analysis fig 5 a mixed kernel of the rbf kernel and the linear kernel and the white kernel was deemed suitable for our data the linear kernel component was included to reflect the weak linear correlation between the logarithm of tds and some predictors the rbf kernel was included to allow more flexible wiggly change the defined kernels for the tested gp models are presented below note that the default gpy setting includes a gaussian noise in the likelihood function therefore we did not define a white noise kernel explicitly for other implementation users may need to define the white kernel explicitly case 4 s kernel and model were defined as image 1 case 5 s kernel and model were defined as image 2 case 6 s kernel and model were defined as image 3 case 7 s kernel and model were defined as image 4 before a gp model is used for prediction the kernel hyperparameters need to be optimized this was conducted in the study by minimizing the negative log marginal likelihood through the l bfgs limited memory broyden fletcher goldfarb shanno algorithm morales and nocedal 2011 although l bfgs is an efficient quasi newton method that uses both first and second derivatives to find local extrema of functions the gradient based method still likely converges to a local minimum for complex models the l bfgs was enhanced by running the optimization multiple times from different and random starting locations to avoid local optima although it is a standard practice to use gradient based optimization for gp modelling more sophisticated global optimization techniques such as evolutionary algorithms petelin et al 2011 and bayesian optimization joy et al 2016 can be used the optimization performance is also related to the prior distributions and initial values of the hyperparameters a useful heuristic approach is to standardize the input data subtract the mean and divide by the standard deviation the standardization makes it easier to compare the correlation distance for different predictors without considering the unit difference after this transformation a good starting parameter for lengthscale hyperparameters is one we applied a prior range of 0 5 3 in our gp models an alternative approach is to initialize the lengthscale hyperparameters using the media distance among all the training locations another challenge is to determine the relative contribution of the signal and noise variances when data are sparse or highly uncertain the variations of the training targets can be explained either by the signal variance or the noise variance the tds data were standardized in the current study thus a total variance of one is assumed as defined in the models above a prior range of 0 01 0 3 was used for the noise variance the signal variance was not constrained explicitly a common practice is to ensure that the signal to noise ratio is larger than one if we believe that the training targets are controlled by a hidden process rather than random noise when multiple kernels or kernel configurations are available and cannot be ranked based on expert knowledge a formal model selection strategy is required such as bayesian model selection and statistics of generalization errors chapter 5 of rasmussen and williams 2006 provided an excellent discussion for gp model selection a combination of linear and rbf kernels was chosen based on the authors experience in the present study furthermore we rely on cross validation statistics to determine the relevance of different axes one way is to allow a correlated kernel in the parameter space formed by the easting and northing locations kernels along other axes are then added to this correlated kernel one by one see the kernel definition for case 6 above another way is to implement a more flexible anisotropic kernel in the 6d space case 7 rmse and r2 of the leave one out cross validation analysis indicate that the kernel in case 6 is a better choice thus case 6 is considered as the preferred model for salinity mapping in the current study with the optimized hyperparameter set the model performance of case 6 on the validation data is shown in fig 6 for case 1 among the 23 test bores only the salinity of one bore lay outside the 95 prediction interval of the gp the one bore outside the 95 prediction intervals is at the higher end of the salinity range this means that the trained gp has lower prediction reliability at the high salinity locations above 8000 mg l because there is less training data when it is close to the high salinity area this is not ideal but the interest in the study area is more focused on the low salinity groundwater rather than the high salinity one to meet the future water supply requirement the validation also suggests wide prediction intervals for a large proportion of the test locations this should be improved by collecting more better quality data especially from the southwest region 4 2 salinity interpolation when aem data were not used case 5 the analysis showed a salinity distribution that generally followed the topography elevation fig 7 c low salinity was predicted along the northern boundary of the study area near the basement outcrops where recharge is expected to occur the salinity gradually increases towards the south towards the low lying areas meanwhile the distance to streams also shows some local impact on the salinity although there is no simple linear relationship between salinity and the distance to streams in the collected data there is a weak trend of slight salinity decreasing as we move away from an ephemeral stream and when the distance is less than 10 km fig 5 this can be observed in the eastern part of the prediction map when aem data were included in the training data case 6 the cross validation rmse was reduced to 0 221 from 0 264 while the coefficient of determination r2 was increased to 0 608 from 0 527 although the changes of rmse and r2 are below 20 the spatial distribution presents a different pattern with more identifiable local details although the influence of dem is obvious at the regional scale local patches of low high salinity were more significantly influenced by the aem data the results indicate four low salinity zones in the study area fig 7e zone 2 is not obvious when aem data were not included zone 3 and zone 4 can also be observed in case 5 however the salinity between the two zones is higher in case 6 these zones can be prioritized for further exploration to have a more comprehensive understanding of their local characteristics such as sustainable yield and sediment properties the four zones are generally located in the relatively low uncertainty area fig 7f subplots g and h in fig 7 case 7 provide a good example to show the influence of kernel selection and configuration as stated in the previous section both case 6 and case 7 include six predictors they are only different in the strategy we used to combine kernels along different axes the kernel of case 7 leads to higher variances for the liner and noise components in the combined kernel therefore the gp model of case 7 has a limited generalization capacity resulting rapid uncertainty increase when it is away from the training data fig 7h 4 3 kriging cokriging results to ensure that gps can generate comparable results with the more traditional spatial interpolation techniques three kriging cokriging exercises were conducted for comparison ordinary kriging and ordinary cokriging was used for case 1 and cases 2 and 3 respectively the linear model of coregionalization lmc was used for variogram modelling when cokriging was conducted it is beyond the scope of the current paper to describe the geostatistical theory of kriging cokriging many good references for geostatistics are available for interested readers such as oliver and webster 2014 kitanidis 1997 and pyrcz and deutsch 2014 the gp model in case 4 fig 7a only includes the location predictors to allow a direct comparison with ordinary kriging in case 1 fig 8 a the kernel of the gp model for case 4 is defined in the previous section a spherical variogram model with a nugget effect of 0 08 was fitted by eyeballing for case 1 the two cases show a similar trend in terms of the groundwater salinity variation although with some different local details predictions from the gp model have a higher standard deviation fig 7b than the kriging model fig 8b this is because that the gp learning tend to prefer a model with a noise variance that is higher than the nugget effect of the kriging model the dem and aem data were selected as the secondary variable in case 2 and case 3 respectively fig 8c and e as expected the introduction of a more exhaustively sampled secondary variable adds local details to the interpolation cokriging with secondary variables generates more extreme predictions indicated by the dark blue and red locations in fig 8c and e the cokriging interpolations of case 2 and case 3 also reveal the four low salinity zones in case 6 although they are generally less obvious the structure pattern of the second variable is obvious when cokriging is used for example it is easy to observe the impact of the aem structure when we compare fig 8e with fig 8a the results of gp prediction case 6 can be considered a weighted mixture of the 6 predictors although there is no limitation for the number of secondary variables for cokriging theoretically its actual application is often limited to one secondary variable due to the complexity to fit multiple variograms and cross variograms furthermore the multiple variograms cannot be modelled independently due to the positive definite requirement of the cross covariance matrix however the application of gps does not have such limitation as shown in the study it is much simpler to add or remove a predictor from the training data in gp machine learning 5 conclusions the flexibility and capability of gaussian processes gps were demonstrated by applying it to generate groundwater salinity maps in the data sparse musgrave province in australia our results suggest that nonpoint data such as aem electrical conductivity and dem data are valuable for hydraulic property estimation particularly when local details and heterogeneity are considered critical although these data can also be taken into account as secondary variables in cokriging the modern implementation of gps makes this process much more straightforward when data for variables of interest are sparse and noisy or clustered to only parts of the study area multiple data sources even when they are only weakly related to the target variables can add precious information to the interpolation under this type of scenario modelling with gps provides a flexible and systematic bayesian framework to generate unbiased prediction using all available covariates with a rigorous quantification of uncertainty to support risk based decision making the use of gps for modelling spatial datasets with multiple covariates should be further encouraged in groundwater science and in earth and environmental science more broadly we conclude the paper by reminding the readers to be aware of the computing limitation of gps standard gps use the entire dataset to perform the prediction resulting a computational complexity o n 3 due to the inversion of a n n matrix where n is the size of the training sample their efficiency can rapidly decrease with the increase of training data when the efficiency becomes a concern sparse or approximate gps should be considered gramacy and apley 2015 liu et al 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the study was funded by goyder institute for water research and csiro 
25774,in environmental participatory modeling pm both computer and non computer based modeling techniques are used to aid participatory problem description solution and decision making actions in environmental contexts although many pm case studies have been published few efforts have sought to systematically describe and understand dominant pm processes or establish best practices for pm as a first step we have reviewed a random sample of environmental pm case study articles n 60 using a novel pm process evaluation instrument we found that significant work likely remains for pm to fully support participatory and integrated planning processes while pm reports systematically address knowledge integration and learning they often neglect the facilitation of a multi value perspective within a democratic process and the integration across organizations within a governance system if not reported we suspect these aspects are also neglected in practice we conclude with key research and practice issues for improving pm as an approach for real world participatory planning and governance keywords participatory modeling review integrated planning planning process democratic process resilience 1 introduction participatory modeling for environmental planning and decision making over the past few decades environmental governance has been shifting to include more public participation throughout the decision making process with collaborative approaches called for in newer policy and regulatory directives leong et al 2011 reed 2008 sterling et al 2019 benefits of participation include increased public trust transformation of adversarial relationships social learning and higher quality and more durable decisions see reed 2008 for review a large body of literature describes what successful participatory processes involve but it also recounts the difficulties and problems that may arise from these efforts e g ansell and gash 2008 schuett et al 2001 susskind et al 2012 for example groups of stakeholders may be over or under represented in participatory processes leading to democratic problems relatedly power imbalances among the participants can make some participant s voices heard at the expense of others furthermore the complexity of environmental issues makes it crucial to coordinate decision making across geographical scales organizational boundaries and policy fields to achieve effective coordination and cooperation between public authorities in complex decision making contexts has proven difficult see e g hedelin 2017 so successfully engaging public participants in such nested decision making processes is a major challenge dietz et al 2003 perhaps as a consequence of that in practice one way activities for public input such as public hearings and public notice and comment have become standard while collaborative approaches such as co management and community science remain underutilized innes and booher 2004 jacobsen et al 2012 leong et al 2011 within the large and multifaceted research area that concerns methods and tools for planning and decision making 1 1 this area can be structured and described in different ways depending on the entrence points choosen such as area of application type of tools in focus and the aim of the research examples of previous overviews are huang et al 2011 johnson et al 2018 lamé et al 2020 rouwette and vennix 2006 a field that specifically refers to the concept of participatory modeling pm has developed during the last couple of decades it includes various methods and tools that can support collaboration and stakeholder involvement throughout an environmental planning process from problem formulation to generating knowledge about system dynamics to developing and evaluating decision alternatives and implementation although extensive and systematic reviews of environmental pm literature are lacking pm has been widely touted as a useful approach for understanding complex socio environmental problems by improving social learning and integrating expert and stakeholder knowledge davies et al 2015 pahl wostl et al 2007 zellner et al 2012b we 2 2 the us national science foundation s national socio environmental synthesis center sesync funded an interdisciplinary team of social scientists biophysical scientists software developers and participatory modeling practitioners to discuss the processes products and outcomes associated with participatory modeling and its approaches the group of authors was part of this team for a complete list see gray voinov paolisso and jordan participatory modeling https www sesync org project enhancing socio environmental research education participatory modeling therefore describe pm as a purposeful learning process for action that engages the implicit and explicit stakeholder knowledge to create formalized and shared representations of reality voinov et al 2018 further pm has also been shown to support conflict resolution trust building and collectively identifying and agreeing on problem solutions becu et al 2008 smajgl and ward 2013a the growing popularity of pm studies seidl 2015 has been attributed to the shifting norms for stakeholder engagement coupled with improvements in cyberinfrastructure that have surfaced novel ways to engage stakeholders in collaborative representations of complex socio environmental problems gray et al 2018 seidl 2015 sterling et al 2019 given the inherent abilities of modeling methods to structure complex problems to explicate the relationships between system components and to demonstrate the trade offs between key values pm has important potential to support the difficulties of participatory planning although there are rich literatures on both participatory planning and pm with respect to environmental issues for overviews see nared and bole 2020 voinov et al 2016 little work has explicitly integrated both domains while pm has shown clear contributions and great promise to improve public participation studies indicate a need within the pm community to better understand the complexities of participatory processes and to investigate how best to select and implement methods and tools to achieve desired benefits gray et al 2018 hedelin et al 2017a jordan et al 2018 seidl 2015 voinov et al 2018 another area of development is the need to understand pm in relation to the real world complex decision making contexts of environmental problems which generally span diverse legislations and policy fields at multiple geographical scales administrative levels and actors hedelin et al 2017a voinov et al 2018 furthermore theoretical and critical studies of pm are rare such studies can provide increased understanding of issues such as whether certain types of participants influence decision making in pm more than others issues of power if and how knowledge generated in pm may become accessible by others than those directly involved issues of efficiency and how decision making in pm can be related to the overall democratic system issues of democracy a planning perspective on pm can help to further develop pm with respect to these issues and would at the same time open avenues for transferring methods and tools of pm to the participatory planning field to support the development of best practices of pm with a view to adopt and implement pm into real world planning and decision making we will here explicitly study pm from a participatory planning perspective we ask what s left to do before pm can provide full support to real world participatory planning as a first step towards answering that question we systematically review the field of environmental pm according to ideals for participatory planning in complex governance settings to lay the basis for further studies of the complexities of participatory processes we focus on procedural aspects to do this we firstly draw a random sample n 60 of environmental pm case articles from a larger pool of pm articles that represents a near census of peer reviewed pm studies as of mid 2017 the sampled articles are reviewed with a novel evaluation instrument that structures the review and provides the participatory planning lens for the study the instrument is based on two complementary participatory planning frameworks one focuses on descriptive criteria e g number of participants and the other is theoretical and focuses on prescriptive process criteria e g how participants are selected see section 2 after presenting our findings we conclude with key research and practice issues for improving pm as an approach for real world participatory planning and governance 2 method and theory 2 1 data collection our review is based on 60 published case study articles listed in appendix c these were randomly selected from a pool of 212 articles compiled by searches on the academic databases web of science and science direct using the search terms participatory modeling collaborative modeling and companion modeling articles were further narrowed through filtering for environmental applications one of this paper s authors to ensure consistency was devoted to scanning each compiled abstract and removing it if it was not an environmentally focused pm case study case studies for the purposes of this review were limited to english language peer reviewed scholarly journals choices of methods will always limit the study a main limitation follows from the choices of databases search terms and application areas for example research of environmental applications that do not use the selected search terms such as operations research multi criteria decision making and decision theory are not included furthermore the use of scientific papers as data limits our possibilities to make statements about the performed studies as not everything about the study is reported in a scientific paper e g due to length restrictions there may also be an overrepresentation in the reports of what the authors see as successful cases processes and activities 2 2 evaluation instrument to evaluate the selected pm case papers we developed a novel 21 question evaluation instrument see appendix a the review questions are hereafter referred to as q1 q2 etc this instrument is based on two complementary frameworks for participatory planning processes the first framework the comparison of participatory processes copp hassenforder et al 2015 informed review questions about the case context e g the geographic scale the problem inspiring the pm process the pm process initiator the goals of the pm process and the number and type of participants in different steps of the process q2 10 the second framework the sustainable procedure framework spf hedelin 2007 2015 was used as the theoretical and normative basis for evaluating procedural aspects of the cases e g how participants were selected how power imbalances were managed what resources the process required and how the pm process was related to the surrounding decision making system q12 21 section 2 3 outlines the spf question 11 asked about specific tools and methods used in the case study it was also used to complement a survey of pm practitioners voinov et al 2018 our pm process evaluation instrument was elaborated through a stepwise and iterative procedure engaging the whole group of authors 2 the process started with a smaller group who developed pre tested and revised an initial set of questions against a subset of case articles these pre tests and revisions were repeated several times to integrate collective knowledge and experiences of the pm field in the final version of the instrument the whole group of authors iteratively discussed and refined questions and answer categories we also focused on making this instrument useful to those beyond our author team so we have posted it online for sharing 3 3 the digital evaluation instrument is available via the pm website https participatorymodeling org pm for participatory planning and decision making a review tool the questions were separately loaded into a web based questionnaire to assemble the review data and to generate basic reports including tables and diagrams each paper was double coded weeks apart by the same coder ensuring consistency in evaluations across the papers to further support data collection and analysis a generic process framework was used based on evers et al 2012 and alkan olsson et al 2011 it includes six steps covering the main topics that can be handled in a pm process from a planning perspective although not all steps were described in all case study articles fig 1 2 3 theoretical basis implications of participatory and integrated planning the theoretical basis of this study is operationalized by the sustainable procedure framework spf the spf prescribes what a planning process needs to include support or promote to be both integrative and participatory table 1 to that end it synthesizes relevant theories and research from a broad review of literature as well as from interviews with senior researchers in planning public administration economics political science resilience studies adaptive governance deliberative democracy integrated management and ecological economics for a full explanation of the spf and how it was derived see hedelin 2007 2015 the 16 spf criteria stem from the two concepts of integration 4 4 integration here means that efforts are being made to include and combine all the key aspects of a certain issue including an understanding of their relationships thus an inclusive and at the same time reductionist perspective is necessary to apply because the simultaneous inclusion of all aspects of an issue will make understanding and management of that issue hard and participation and are structured by five themes which together support a structured and theory based analysis of any participatory planning process in this review those using pm there are a number of participatory frameworks and best practice guidelines for participation and pm in literature such as barreteau et al 2010 hassenforder et al 2015 perez et al 2014 smajgl and ward 2013b the spf is used as basis in this study because it allows us to simultaneously perform a focus on procedure compared to a focus on output e g a management plan an implemented measure a developed model 5 5 the process and its outputs are highly dependent as the value and function of the resulting plan or decision depends on the quality of the process a theory based analysis a critical perspective based on the deductive and normative character of the spf and a governance perspective on pm due to inclusion of issues such as organizational integration and stakeholder representation which stems mainly from the principle of integration the use of the spf has provided deeper understanding and practicable advice related to several cases of planning and planning tools already see hedelin 2017 for illustration of the spf as analysis tool for 5 p m cases see hedelin and lindh 2008 and hedelin 2015a for examples of analyses of planning processes and see hedelin 2008 for an analysis of planning legislation 3 results we begin by summarizing a number of contextual features of pm studies including the number of publications per year subject focus areas of the studies their geographical distribution and other dimensions after that we present our analysis around the more process oriented spf themes 1 integration across disciplines 2 integration across values 3 integration across organizations 4 participation contributing to the process and 5 participation generating commitment legitimacy or acceptance see table 1 lastly we explore characteristics around evaluation and theoretical connection of the case studies appendix b includes complementary graphical summaries of the results referred to as figure b1 b2 etc 3 1 contextual features of the case studies the 60 reviewed papers in our sample were published between 2003 and the summer of 2017 when the literature search was finalized more than half of the reviewed papers were published during or after 2012 figure b1 among our case studies agriculture n 13 22 and river basin management n 11 18 were the most common focus areas the issues of food availability and climate change were not studied in any case studies reviewed figure b2 q2 the reviewed studies were conducted in all permanently populated continents with most studies in europe n 17 28 and fewest in south america n 1 2 figure b3 q3 almost all of the studies had a regional n 42 71 or local n 16 27 scale of interest with just one case study explicitly including a range of geographic scales figure b4 q4 project initiators were described in just over 60 of the cases n 36 of those 30 n 11 identified multiple types of initiators governmental agencies were the most commonly listed followed by scientists and ngos local community members were not identified as project initiators in any of the studies for studies that did not explicitly describe who initiated the study or project we inferred who initiated the study in almost all cases our review indicated that scientists had initiated the study suggesting they have the strongest role in initiating pm projects and case studies figure b5 q5 scientists also had a very strong role in leading the pm processes most papers report the type of actor that leads the pm process n 40 67 of those studies 85 n 34 were led by scientists and none were led by a local community member figure b6 q6 for most studies the number of participants was either listed or could be inferred the smallest number of participants listed explicitly was 6 and the largest was 602 30 of the studies had 25 or fewer participants 20 had 26 49 35 had more than 50 participants and for 15 of the studies the number of participants could not be determined figure b7 q7 a detailed examination however revealed that over 90 of the cases involved less than 65 participants the two largest studies involving 373 and 602 participants respectively both relied on online participation question 8 queried the cases process frameworks by which we mean several explicitly described steps including their relations which provide an overview and a structure for the pm process a majority of the studies n 33 55 reported some form of such a defined process framework there is great variation and lack of consistency of how process frameworks are described in the pm papers only two frameworks companion modeling gurung tayan et al 2006 and group model building vennix 1999 were mentioned in more than one case the pm studies were undertaken for a variety of reasons the most common of our listed purposes stated by 26 studies 43 concerns the development or application of a pm process framework no other purpose was cited in more than 12 of the cases 20 figure b8 in contrast to the diversity of purposes there was much more consistency in the types of reported results at least 80 of the studies reported that gathering knowledge from stakeholders all studies developing modeling method n 57 developing and applying a pm processes n 53 and stakeholder learning n 52 were important results figure b9 q9 the case studies drew on a range of methods and tools which were used for a variety of purposes figure b10 q11 here we define tools as specific software types for example agent based modeling system dynamics modeling decision trees etc and we define methods as process oriented approaches e g brainstorming scenario planning which can use a range of tools although in environmental pm there is often some degree of fluidity between the two voinov et al 2018 we recorded both methods and tools used in three phases of pm problem identification data collection about the problem and problem analysis the cases often used multiple methods and tools both for problem identification and for data knowledge collection the most commonly used methods and tools were interviews n 43 and expert elicitation panels n 44 followed by surveys questionnaires and polls n 35 and scenario building n 30 and focus groups n 27 most cases used only one kind of method or tool for problem analysis but no single tool dominated across our case study sample most popular were system dynamics modeling n 24 and geographic information systems gis n 22 3 2 procedural aspects of the case studies 3 2 1 integration across disciplines 3 2 1 1 integration of knowledge criterion a we reviewed case studies for whether and how they included procedures to identify the knowledge necessary for the pm process including the tools used for that purpose q12 different types of knowledge were targeted such as scientific expert and layman knowledges nearly every study 55 92 described a systematic procedure for addressing this issue a systematic procedure means that comprehensive and explicit procedures are used to identify the main pieces of knowledge needed in the process including justification of what knowledge to include fig 2 as detailed in table 2 participants e g experts stakeholders community members were the main source of information used to identify knowledge needs interviews were performed in 39 cases 65 expert elicitations were used in 25 cases 42 and brainstorming occurred in 12 cases 20 while no additional process to structure knowledge was mentioned in most studies we found that 15 studies engaged in a form of qualitative mapping 25 these forms of mapping differed significantly ranging from causal maps e g as a final product or as a step towards a system dynamics model to fuzzy cognitive maps the use of previous studies was less common as seen in 9 cases 15 and only 6 studies 10 collected or used data other than qualitative transcripts of interviews and focus groups e g administrative data spatial data many of the methods and tools applied in the cases figure b10 q11 may also support integration of knowledge examples of such tools frequently used include scenario building n 30 50 system dynamics modeling n 24 40 focused group discussions interviews n 27 45 brainstorming n 23 38 gis n 23 38 and cognitive concept mapping n 20 33 3 2 1 2 handling different views of knowledge e g positivist relativist criterion b we did not code the papers for this criterion explicitly as our early pilot efforts to create and apply our evaluation instrument along with the authors collective prior experience determined that this issue is rarely if ever handled explicitly in pm processes and even less so in pm case study applications a more in depth review approach is necessary to target this issue 3 2 1 3 handling different kinds of uncertainty criterion c of the reviewed case studies 37 cases 62 report a systematic approach towards uncertainties fig 2 q12 the rest of the papers report a limited approach towards uncertainties n 14 23 or do not report any uncertainty handling n 9 15 a range of methods and tools are used for those that report using systematic approaches the use of scenarios is the most common approach used n 15 other systematic approaches used are agent based models n 6 multi criteria analyses n 4 and conditional probability tables n 3 see table 2 we also coded the cases for the types of uncertainties addressed q13 the most common type concerns uncertainties in the system at hand such as alternative system functions n 40 74 the least common type is related to the proper formulation and representation of a conceptual model n 24 43 the other uncertainty types addressed in the reviewed papers are distributed within that range n 25 32 figure b11 most 79 of the papers that report uncertainties address more than one type of uncertainty one fifth of the cases concurrently addressed all five types of uncertainty that we evauated 3 2 2 integration across values 3 2 2 1 identification of and rational argumentation based on relevant values criteria d and e the main questions for examining these criteria concern how studies reported handling the main values involved and which methods and tools were used to support those efforts q12 just over 40 n 25 of the cases reported systematic approaches to values fig 2 meaning that a comprehensive and explicit approach was applied to identify the main values and a justified decision was made about which values to include and how to adjust the process accordingly there was a broad diversity of methods and tools used by cases that took a systematic approach to the issue table 2 out of the 25 cases using systematic approaches 7 cases used interviews and 3 used a multi criteria approach some cases did not include the important values focused process steps of formulating and evaluating alternatives or assessing of proposed decisions for those that did 64 n 32 made clear within the process how values were affected by the key alternatives considered such as alternative policies plans and measures the rest of the relevant cases n 18 36 did not report how the involved values were affected q12b the most used methods for analysis cited in the case studies were system dynamics modeling and gis figure b10 q11 while these can be strong tools for analyzing how some values are affected by alternative decisions we cannot tell from our analysis whether these tools were also used to analyze effects on a set of identified main values in comparison multi criteria analysis used in 9 cases 17 is a method that provides strong support for explicitly deliberating about how alternatives affect a set of selected values likewise cost benefit analysis is another tool for communication around values but was used in only 1 case 2 3 2 3 integration across organization 3 2 3 1 organizational learning criterion f by organizational learning we mean learning that reaches beyond the set of people directly involved in the pm process non participant learning examples of such approaches that we looked for included process evaluation and documentation the establishment of shared databases and the development of institutions e g meeting routines of the organizations represented by individuals in the process just over 30 of the cases n 19 reported on the issue of organizational learning and only 15 reported explicitly on this issue meaning that activities that might support organizational learning and discussions of organizational learning were both included in the paper fig 3 q16 the main way to accomplish organizational learning reported was to trust those directly involved to disseminate knowledge gained during the pm process to their home organization village family etc other ways included the distribution of maps and oral presentations structural or institutional approaches to organizational learning with long term continuation established as part of the process were not reported 3 2 3 2 handling of formal planning context criterion g a systematic way of fitting the pm process into the formal planning context including mapping of the formal context and a strategy for handling the formal context was reported in 15 of the reviewed cases n 9 nearly 60 did not report the issue at all n 35 fig 2 q12 only 3 cases reported that explicit communication around participants roles and mandates occurred as part of the process 5 fig 6 q18 no method or tool clearly emerges as common practice among the nine cases that used a systematic approach to the planning context issue although approaches are different we note that four of the studies are unified in their reliance on participants to fit the pm process into the formal decision making context table 2 3 2 3 3 handling of incentives including resources and efficiency removal of thresholds criterion h the issue of resources and efficiency is fundamental for the implementation of pm processes in practice we evaluated case studies for how they described the resources required for carrying out the pm process e g money time expert skills and data only 10 n 6 of the cases reported something about the resources required q14 among the few cases that did report resources we wanted to know if the trade offs involved could be understood i e what was the potential increased value that accompanied the added cost of the pm process and what can be learned from the case about efficient pm process designs these cases provided only sparse accounts which did not enable cross case comparisons of alternative process designs the cases mainly accounted for issues of time either the time required to complete the pm process as such or that the process required more time than an alternative non pm process two cases discussed monetary costs of the process in general terms and only one case reported on the total project budget four cases discussed resources in relation to process outcomes 3 2 3 4 handling human aspects e g trust engagement conflict management criterion i over half of the cases did not report taking any actions to handle the human aspects of the process such as trust engagement and inter personal conflict n 33 fig 4 q17 the most common action applied by almost one third of the cases was to engage a skilled process leader n 19 almost one third of the cases made the pm process transparent e g by documenting the reasons behind the decisions made only 3 5 of the cases took actions to understand the social relationships of the participants before the process e g value positions disputes and conflicts among the group of participants 3 2 4 participation contributing to the process 3 2 4 1 inclusion of knowledge owned by relevant actors criterion j as stated above more than 90 of the cases reported using a systematic approach to address the issue of knowledge identification n 55 table 2 q12 to include the knowledge owned by relevant actors identification and engagement of these actors in the process is also a prerequisite however less than 20 of the cases reported using systematic procedures for identifying and selecting participants n 11 table 2 q12 on the positive side only a few cases n 2 3 did not report this issue at all furthermore among the cases that did describe participant identification systematically or not n 58 all cases 100 reported that their main reason for selecting and inviting participants was that they own knowledge that was needed in the pm process q12a 3 2 4 2 inclusion of the ideological orientations represented by relevant actors criterion k just over 40 of the cases reported systematic procedures for identifying the main values related to the issue at hand n 25 but as we just saw less than 20 of the cases reported using systematic procedures for identifying and selecting participants n 11 table 2 3 2 4 3 participation in the most critical phase s of the process criterion l we reviewed case studies with respect to each of the generic pm steps shown in fig 1 first as to whether each step was explicitly described and then if so which types of actors participated in the step fig 5 q10 almost all papers included a description of the work for achieving the baseline understanding of the case system n 58 and nearly all case studies included a discussion of model development and use n 52 the outcome of the process such as a decision plan proposed measure or an agreement was the step least described less than 30 of the cases n 17 the rest of the planning steps process preparation and setup formulation of objectives and formulation and evaluation of alternatives were included and described in 70 80 of the cases considering the stepwise involvement of actors there is a somewhat unbalanced focus among the case studies towards the more knowledge based process steps compared to those of more managerial value oriented and operational characteristics process preparation objectives alternatives evaluation and outcome respectively fig 5 shows who was involved in each phase of the reported pm processes scientists were the type of participant reported to be involved most frequently in pm planning steps this is in line with our earlier finding that scientists were leading most of the pm processes in the preparation step scientists were involved more than twice as often as any of the other participant types however when it comes to the pm process outcome local community members were involved the most non governmental organizations ngos and local community members were rarely involved in the process preparation step furthermore ngos had the lowest involvement in all steps except for preparation 3 2 5 participation generating commitment legitimacy and acceptance 3 2 5 1 a procedure for defining the actors that should be involved criterion m which actors participate has fundamental and cross cutting impacts on the quality of the process it directly affects the fulfilment of criteria j and k and indirectly affects most of the other criteria less than 20 n 11 of the cases reported systematic procedures for addressing the issue of identification of participants i e the participants were identified and selected by a comprehensive and justified approach fig 2 q12 we noted four different approaches among these 11 studies with systematic procedures table 2 with sampling being the most common approach 6 cases the remaining cases used some sort of pre established selection criteria 2 following the nominations of a local group or committee 2 or inviting all possible participants 1 on the positive side only a few cases did not report this issue at all n 2 the participant identification and selection criteria that we coded q12a were applied by most of the cases that reported on this issue systematically or not participants who may support implementation n 47 who own valuable knowledge and perspectives n 58 and those who are affected by the process n 52 only 5 cases applied other participant selection criteria 3 2 5 2 handling power asymmetries criterion n and procedures that ensure that ideological orientations are not suppressed criterion o half of the cases did not report any activities for managing power asymmetries fig 6 q18 for the rest three approaches were applied most commonly 1 separated meetings 2 structured communication procedures to obtain input from each party each by 30 n 18 and 3 documentation to increase transparency just over 20 n 13 only 5 of cases reported the use of telecommunication e g via a webpage as a way of handling power issues and only 5 reported that they clarified the roles and mandates of the participants involved in the process because knowledge is a resource that brings power an additional and important way to manage power is to support learning which takes us to spf criterion p 3 2 5 3 stakeholder learning criterion p the review focuses on learning processes rather than on learning outputs i e whether and how much the participants learned from the pm processes by q15 over 80 n 49 of the cases described activities that may support stakeholder learning fig 7 just over half of the cases however reported explicitly on how they supported learning meaning that activities that supported learning were described and learning as such was discussed in the paper this does not mean that an evaluation of the learning was reported which could show if the process activities indeed have resulted in learning see section 3 3 among the cases that reported explicitly on learning participant interactions and discussions of various forms such as workshops modeling and field trips were reported as activities aimed to support learning 3 3 evaluation and theory connection of the case studies although all reviewed papers were published in scientific journals over 60 n 37 did not include any kind of evaluation of their project q20 of the evaluated cases 30 lacked a description or justification of the evaluation n 7 of the cases that included evaluations most cases used data on the participants understanding of the process using interviews or questionnaires n 13 the second most commonly used approach was to use process data other than the participants self reported experiences such as observations of process activities or interview with the process leader only one case used theory to support their evaluation see fig 7 and fig 8 we also reviewed cases for whether the outcomes of the planning processes were recognized in any decisions or actions outside the pm effort itself q21 almost 90 of the cases lacked such a recognition we categorized the seven cases where outcomes of the pm effort had been recognized recognition by laws or regulations n 1 by a management action measure n 6 by an institutional arrangement n 0 or by other means n 1 4 discussion learning from pm research pm research is generally carried out in collaboration with public authorities organizations and persons who are engaged in managing their real world problems as seen above many of the studies we evaluated aimed to support learning whether about the socio ecological problem at hand about different ways of understanding it or about alternative ways forward however one can also learn about the pm process e g the characteristics and function of methods and tools employed process designs process leadership and conflict management etc as pm researchers it is mainly this kind of knowledge that we seek to contribute to a growing body of shared and collectively reflected knowledge of pm processes our main strategy has been to report our studies in scientific papers these papers became our data for this review 6 6 as noted in the method section the use of academic articles has its limitations it may for example be that systematic procedures are being used to identify participants criteria m in most cases but there are systematic gaps in the reporting of this and potentially other criteria in the pm literature in other words more focus is given to the outputs and descriptions of modeling methods and tools rather than other process issues complementary studies would have to be made to study that they generally describe pm processes and their contexts focusing on the specific study objective which as we have learned can vary considerably figure b8 to this end gray et al 2018 suggest a procedure to standardize the reporting of pm studies to allow for better systematic review across studies and thus to better understand and inform the evolution of this field the 4ps of pm are purpose process partnerships and products these 4ps should be addressed explicitly in all publications whether peer reviewed or not to ensure their contribution to generalizable knowledge about how to conduct pm exercises towards impactful outcomes in addition while project evaluation should also be regarded as an important part of a regular research paper we have found that most papers do not include an evaluation moreover for those that do include evaluations reporting standards are low lacking justification such as a detailed description of how the evaluation was performed and clear evaluation criteria for the papers that include a justified evaluation n 7 most are based on participants perceptions of the pm process and only one case used a specific theory to support evaluation this decreases the usefulness of the evaluation performed because it cannot easily be understood by anyone outside the case what standards are the assessment being made against whose standards furthermore whilst this review focuses on process evaluation of both process and outcome criteria are important to include if we want to contribute to a complete comprehension of pm our study shows that currently case study reports seldom include evidence of outcomes in this study we have purposely chosen to be very specific about our basis of assessment because it allows us to compare pm against a current framework of knowledge about what a participatory and integrative planning process should imply the spf a summary interpreting the results of our analysis is provided in table 3 because the criteria are related to each other in various ways the criteria wise result can be summarized across several lines we have identified three cross cutting patterns that merit discussion 1 knowledge integration and learning 2 values and democracy and 3 integration across organizations 4 1 knowledge integration and learning spf criteria a c j m p 4 1 1 knowledge identified and often integrated the spf states that a fundamental characteristic of a participatory planning and decision making process is that it integrates the main pieces of knowledge related to the issue at hand spf criteria a and j for river basin management in a regulated river for example knowledge of river ecology flow regulation alternative hydro power technologies would be key as would knowledge of policy law economy and planning depending on other case specifics knowledge of tourism recreation land preservation heritage flood management irrigation technology and more could be likewise important knowledge comes in various forms such as disciplinary knowledge criterion a from experts databases and reports and also more local and contextual knowledge criterion j from local stakeholders ngos and public authorities which may not be documented as text or numbers examples of the latter include local farmers harvesting and fertilizing routines fishers observations of fish stock dynamics a regional authority s current plans for the infrastructure system and local attitudes towards a specific policy or measure engaging experts and local actors who can represent all the pieces of relevant knowledge may be very expensive but missing out on a key piece of knowledge could have devastating consequences therefore it is important to prioritize knowledge carefully and to be open to reassessing knowledge needs as the pm process evolves to rely solely upon the expert judgement of a few persons is not sufficient glaas et al 2010 in great part because experts are likely to emphasize the knowledge that lies close to their own expertise we do not know of the knowledge that we do not know glaas et al 2010 hedelin and lindh 2008 instead a more comprehensive transparent and justified procedure is necessary our finding that over 90 of reviewed cases apply such approaches to knowledge identification table 2 is gratifying furthermore the frequent use of methods and tools that can integrate knowledge such as scenario building system dynamics modeling agent based modeling gis and cognitive mapping is also a very strong feature figure b10 which implies that the pm field has an important contribution to make to environmental planning and decision making when it comes to knowledge integration note however that the application of these methods and tools is no guarantee that the knowledges have been integrated effectively for example focus group discussions that are poorly led will not function well and any approach that neglects actors with key knowledge will fall short we have coded the cases for issues such as representation power imbalances and conflict management which may indicate whether the methods and tools applied function well see result for criteria m n o and i difficulties with social power and tool use are interwoven with the different ways participants understand knowledge and its connection to values i e their different epistemologies halpern and o rourke 2020 o rourke et al 2019 for instance can some knowledge be considered as objective while other knowledge needs to be treated as depending on individual experiences and preferences not only are epistemologies usually different but also depending on their disciplinary or professional training life experiences and other personal characteristics participants of the pm process will be differentially equipped for managing epistemological differences the pm process including leadership resources methods and tools will affect how these dual differences of content and capacity play out hedelin et al 2017a process features can become obstacles for knowledge integration or they can enrich the process by supporting a better understanding of the complex planning issue at hand spf criterion b we did not code the papers for spf criterion b explicitly because this issue is hardly ever reported in pm papers see voinov et al 2018 for guidance on how conceptual modeling methods such as fuzzy cognitive mapping fcm cognitive mapping can complement quantitative methods to avoid that kind of problem more studies are needed however to explore the issues of epistemology in participatory processes and of how pm may provide support for example how does the epistemology of the process leader affect the design and orchestration of a participatory pm process when it comes to knowledge integration learning and management of alternatives how aware of our epistemological positions are we and of how they manifest in our behaviors and actions as pm process leaders as pm researchers or as modeling tool developers how can methods and tools in pm help to expose and manage alternative views of knowledge so that such differences do not hamper the participatory process such studies could raise of awareness of this issue among pm researchers and hopefully epistemological aspects will become explicit in pm studies that focus on knowledge integration and learning for instance in their analysis of 5 p m cases hedelin et al 2017b showed that methods and tools regularly used in pm have the potential to act as learning platforms supporting discussions among the involved researchers and experts of ways of understanding both reality and knowledge which are fundamentally connected however sometimes the methods and tools used can hamper discussions of knowledge views and integration of knowledge hedelin et al 2017b for example because of the time and resources that extensive quantitative simulation models rely on it may be difficult to reconsider the foundations or components of the model in accordance to the understanding gained from an interdisciplinary learning process since this may not be included in the project time plan and budget hedelin et al 2017b 4 1 2 understanding uncertainty in the context of knowledge and learning the issue of uncertainties what we don t know is critical bammer 2013 the complexity inherent in the socio environmental systems targeted by planning efforts makes it impossible to eliminate uncertainty no matter how much time and resources are spent on increasing the level of factual knowledge bradshaw and jeffrey 2000 zellner 2008 thus uncertainty will be a regular feature of the decision making process and it may have large impact on the various decision alternatives at hand the potential outcome of the process logically an effective approach towards handling uncertainties must involve understanding which uncertainties are the most important in terms of how they affect the outcomes of alternative decisions clear understanding of the relative importance of different uncertainties should guide where evaluation and research effort should focus an open comprehensive and systematic approach towards the full range of types of uncertainties is therefore imperative by our results we can see that knowledge and practice concerning the management of uncertainty is relatively well established within the pm field over 60 of the cases have handled this issue by systematic procedures comprehensive and justified fig 2 even though there are additional types of uncertainty e g of the modeler s skills the types that we reviewed are covered quite well by the cases which signals that none of these broad types are being completely overlooked figure b11 4 1 3 whose knowledge integration of knowledge depends on more than identifying knowledge and uncertainties knowledge is held by actual groups and persons so appropriately broad representation of stakeholders is also essential to knowledge integration the spf criterion of carefully defining the actors that need to be involved m is fundamental for the fulfillment of inclusion of actors knowledge j and likewise important for many other key process criteria such as inclusion of actors ideological orientations k and for establishing a democratic process to meet these criteria the most relevant actors those directly or indirectly affected by the planning issue at hand need to be identified and involved in the process directly or by representation dryzek 2013 lidskog 2005 unfortunately we found that less than 20 of cases described systematic procedures for participant selection fig 2 4 1 4 stakeholder learning is becoming central to pm process the final key knowledge integration criterion concerns stakeholder learning p socio environmental problems are generally complex both technically and socially defries and nagendra 2017 local actors who are directly concerned by the problem at stake can provide important knowledge to the process that complements expert knowledge and their participation may also generate commitment legitimacy and acceptance for the resulting decision and facilitate implementation bryan 2004 however if stakeholders including experts are going to take part in the decision making process they need to understand the problem that the decision concerns sterling et al 2019 stakeholder learning is therefore a fundamental component of participatory processes both for reasons of knowledge integration and democracy we return to the issue of democracy in the next section we have shown that over half of the pm cases reported explicitly on stakeholder learning including how they intended to support it through process methods and procedures another 30 reported on methods and tools that may support learning such as workshops modeling activities and field trips nevertheless our judgement is that although the field has come far in supporting stakeholder learning progress can still be made especially when it comes to the question of whose knowledge is shared and whose is represented in analytic models another urgent aspect to further develop pm in this respect is to increase the number and quality of standards for evaluating learning see section 3 3 about evaluation 4 2 values and democracy spf criteria d e k o our values influence the way we understand a problem and how we are likely to respond to it the issue of how to deal with values is therefore a fundamental question for establishing a good planning process and the planning literature is much concerned with it e g the separation of knowledge and values the role of the planner process leader and how to deal with the issue practically for overview and examples see allmendinger 2002 thomas 2012 wallace et al 2020 practically the way forward is to identify the most important values connected to the problem at hand spf criterion d and to use these as the basis for rational argumentation around alternative choices that needs to be made during the planning process spf criterion e 4 2 1 which values just over 40 of the cases reviewed describe systematic approaches for identification of the most important values fig 2 so for a majority of the cases one cannot be sure that the values considered are the ones most affected by the decision at hand for example if a pm process concerns a plan for a regional agricultural district it might be that the management alternatives are analyzed for their impact on water quality bio diversity and farmers income levels but not for climate or for securing food supply key values can be excluded due to organizational administrative or practical reasons such as the agenda of the funding agency the mandate of the initiating authority or the formulation of regulations that protect a certain value other possible reasons are ignorance inattention and the influence of unbalanced power relations 4 2 2 whose values generating a comprehensive understanding of a problem and possible solutions requires input from those actors who represent the array of ideological orientations connected with the issue hemmati et al 2002 express this nicely by stating that participation aim s at multi subjectivity rather than objectivity which is a claim based on the view that everyone has a subjective understanding of an issue and can therefore only contribute with parts of the overall picture hemmati et al 2002 pp 44 and 300 but in our review over 80 of the studied cases did not describe participant selection procedures that could secure inclusion of the full spectrum of ideological orientations related to the issue at stake fig 2 seventy percent of the studied processes took place at the regional scale figure b4 there are various ways for capturing the full array of ideological orientations at this scale in addition to a comprehensive and justified participant selection approach spf criterion m such approaches need to include practical means of communication one strategy can be to open up the process for a large number of participants direct involvement using some form of web portal for distanced communication only four cases among the studies we reviewed used a web portal however and as the median number of participants in the studies cases was 25 it appears that this strategy has not yet been applied much within the pm field another way to capture the values of many persons could be to use some form of crowd sourcing but here too the pm field has not yet engaged much only two cases use a crowd sourcing technique figure b10 yet another main strategy to gather information on people s values is to seek representation of a broad set of views by carefully selecting persons and or interest organizations representing different views currently this seems to be the most common strategy although again most cases do not use a systematic selection procedure the advantage here is that it allows for a close involvement of the actors which gets more difficult as the group of participants grows larger for selecting persons that can represent main ideological orientations at a regional scale one could expect ngos to be a key type of participant instead of finding and justifying the selection of several unorganized persons that can represent the key values or interest groups at that regional scale it would certainly be more feasible to commit a number of ngo leaders who are already organized to represent key values at larger scales than individual surprisingly however ngos are the participant type with lowest involvement in most of the planning steps fig 5 connected to values and representation it is also important to consider which planning steps are in focus and which types of participants are involved in the different steps spf criterion l the cases reviewed generally described several of the steps in our 6 step model fig 1 but there was an unbalanced focus towards the more knowledge based process steps compared to the more managerial value oriented and operational steps process preparation objectives alternatives evaluation and outcome respectively furthermore ngos and local community members had relatively low rates of involvement in the value oriented steps such as objective setting and evaluation where one could expect these types of participants to play a key role 4 2 3 power and rationality power can come in different forms knowledge social skills monetary resources and formal legal mandates it is generally accepted that power imbalances exist and that they have a negative effect on the conduct of a democratic participatory process of rational deliberation e g flyvbjerg 1998 and allmendinger 2002 power asymmetries can be detrimental to democratic outcomes but only if we ignore them and allow them to work against transparency and rationality spf criterion n arnstein 2019 original publ 1969 flyvbjerg 2002 deliberation where the various voices of the affected actors will interact and transform rather than simply be aggregated in an unchallenged manner is actually one way to account for and handle power imbalances roberts 2004 participatory processes are often tightly related to the idea of consensus identification of common interests on which all involved agree because of power imbalances however critics warn that social groups or ideological orientations might be systematically excluded from decision making if it is based on such a consensus ideal mcguirk 2001 therefore consensus based processes specifically need to include measures that ensure that this does not happen criterion o one way is to design the process as compromise seeking rather than consensus building zellner et al 2020 we found that half of the reviewed cases did not report any activities to manage power imbalances such as separated or structured meetings documentation anonymous involvement or clarification of roles and mandates fig 6 this is troublesome because it will favor the solutions brought forward by those with the greatest power rather than solutions that are democratically defined various methods and tools used in the reviewed pm cases however have a potential to support management of power e g by providing structure clarity and transparence to the process by learning by documentation of a discussion and the reasons behind a decision and by enabling electronic voting and communication furthermore several methods and tools can also clarify how alternative measures decisions plans specifically affect the values included in the process good examples are gis multi criteria analysis and cost benefit analysis bendor and scheffran 2019 such tools can support rational argumentation based on both knowledge and values which is a fundamental feature of a democratic participatory process spf criterion e out of the cases that include the planning step of evaluation of alternatives 64 used tools to show explicitly how the included values were affected by the main alternatives considered in the process this is an auspicious result showing that knowledge and tools related to this issue are well established in the pm field we must continue to raise awareness within the pm community that the values of concerned actors just as much as their knowledge need to be systematically integrated in the pm process to summarize the potential within the field is strong but the issues of representation and power need to attain a much greater focus before pm processes can be commonly regarded as democratic 4 3 integration across organizations pm in the governance system spf criteria f i l planning in the context of complex socio environmental issues is generally characterized by equally complex organizational and structural settings dietz et al 2003 this includes organizations of different types public authorities companies ngos and stakeholder networks operating at different geographic scales ranging from multi national to highly localized these organizations have different roles relationships mandates responsibilities and powers and typically none have the full capacity or authority to unilaterally manage the problem at hand glaas et al 2010 this is a fundamental reason why an effective planning process needs to be integrated across the main organizations related to the issue at hand and embedded in its planning context dietz et al 2003 on this important dimension of participatory planning we find that pm falls short of its promise 4 3 1 the planning context pm disconnected an important part of the planning context of a pm case is formal including legislation authorities binding decision making mandates and ongoing planning and decision making processes that relate to the pm issue at hand in various ways it generally includes multiple administrative scales and sectors the need to understand and handle these complex institutional and organizational settings of cross sectorial issues relates to the fact that different actors have different formal mandates and capacities glaas et al 2010 prager 2010 for example a central authority may be formally responsible for developing a plan but may not have a mandate to implement it at the local level where implementation depends on decisions made by local governments and the general public and on investments and know how from private organizations a basis for the successful management of cross sectoral issues is therefore to firstly generate a broad understanding of these interdependencies and of the structural prerequisites of the planning process among the involved organizations and secondly to manage this in the planning process spf criterion g how does the pm process relate to the representative democratic system around it what is the formal role and mandate of the process leader what can actually be managed by the participatory process and hence how should the objective of the process be formulated to effectively adjust to that what other processes and decisions need the process be coordinated to address and who needs to be involved to that end our results on this topic are alarming because only 15 of the cases reported satisfactory approaches to the issue of cross organizational integration and almost 60 did not report addressing it at all furthermore a large portion of the papers did not report who the project leader was and who initiated the process only 5 of the cases reported explicit communication around roles and mandates having taken place as part of the process fig 6 this indicates a lack of understanding of the importance of clarity around formal roles and mandates of the participants what mandates do the initiator and leader really have what are the reasons for establishing the process what will the output of the process really mean when it comes to implementation most pm cases 70 took place at the regional scale a scale that commonly requires coordination with both larger and smaller scales only one study however reported an explicit strategy to connect scales in the planning process this lack of connection is troubling for example a pm process at a regional scale might establish a plan that disrupts the implementation of a local decision made by democratically elected politicians clarity of roles and processes are critical with regard to decision implementation to the socio ecological problems at stake to the local representative democracy and to the purpose of the regional pm process within political science these kinds of problems related to the fuzziness of regions as political entities and the need of many issues to be coordinated at the regional scale have even been termed the regional mess allen and cochrane 2007 mccallion 2008 we believe that pm has a great potential to help manage this so called mess through the use of tools and methods that can clarify complex system behaviors and aid the establishment of well founded strategies and plans however to make this happen and to prevent pm from actually adding to the mess pm processes must have a strong self awareness and must be understood by and embedded within the surrounding governance system other results further underline this point almost 90 of the cases do not present planning outcomes that are recognized in any decisions or actions outside the scope of the pm processes moreover we did not find a single case that addressed climate change mitigation or food availability in our randomized case pool we hypothesize that one reason may be that these areas clearly require connection among global to local scales and actors and that pm practitioners are not yet comfortable or knowledgeable about how to manage those connections considering the inherent potential of pm to connect scales and actors we urgently need to develop our tools and procedures to handle this difficulty 4 3 2 why participate as described above a foundational reason to integrate a pm process with its planning context is to involve all the actors that together own the capacity to handle the problem at stake once these actors are defined however there are still several barriers that can disrupt their collaboration to overcome these the issue of incentives including resources and efficiency is fundamental h ansell and gash 2008 why do participants get involved what thresholds do they need to pass do authority mandates legislation budgets and schedules make participation feasible how can the process be set up to increase its efficiency and to decrease the time and money required to participate what alternative set ups exist and what are their pros and cons in respect of resources and efficiency incentives and resources cuts through many focal issues of participatory planning such as knowledge integration and learning values and democracy and collaboration and coordination if there are no incentives there will be no participation despite this the issue of resources is the most neglected one in our review 90 of the cases did not report anything related to it the few cases that did address resources did so in a limited manner that focused on time and did not provide any grounds for cross case comparison of methods or process designs furthermore we know from our review that participation is spread over many steps in the processes fig 5 the question is therefore have the resources participants time and engagement been spent efficiently and effectively or will the experience of the process rather make future engagement unlikely because the participants found it too costly generally participation needs to be focused to the phases of the planning process where it is the most useful given its objectives spf criterion l the initial stages of a process generally have a greater influence on the process outcome than later stages and involving the participants in the early steps such as process set up and formulation of process objectives is also a good way to create transparency and a sense of ownership e g spf criteria i n and o different groups of participants may need to be involved for different purposes and hence at different phases because this issue is fundamental for implementing pm on a broad scale further studies are needed to investigate the state of knowledge here in more detail incentives can also come in less tangible forms and be connected to so called human aspects such as trust engagement and conflict management spf criterion i these pyscho social aspects depend on deep rooted behaviors power relationships organizational cultures history and more and their importance for establishing a successful participatory process where people share their knowledge and perspectives is often underestimated trust is fundamental for making collaboration happen and social carrots are needed to make it work well zellner et al 2012a for example in order for people to make room in their already stressful working schedules it is key that participating persons need to feel warmly welcomed to meetings that their ideas and opinions are received with respect that initiatives are encouraged that critical views are allowed among others another key is to establish a sense of ownership as when it is made clear that all of the involved actors are valuable in terms of the overall capacity to manage the issue at hand reed 2008 furthermore sometimes personal conflicts or controversial issues obstruct productive and evidence based reasoning and deliberation weiss and hughes 2005 to manage all of this competent process leadership including preparedness to handle conflicts is fundamental as are measures such as process transparence managing power imbalances and setting up ground rules for participant interaction etc milz 2018 müller seitz 2012 our results show that compared to the more tangible incentives spf criterion h human aspects spf criterion i are better managed there is still much left to hope for however because more than half of the cases do not report any activities to address this issue fig 4 4 3 3 learning only for those directly involved as we have discussed learning is widely seen as key in participatory planning spf criterion p we have also seen that within the pm field most of the cases apply methods and tools that aim to support stakeholder learning figure b15 when it comes to supporting integration across organizations a vital aspect is the need to establish learning structures within the participating actor groups and organizations glaas et al 2010 a pm process can support this by making time to discuss it with the participants and suggesting alternative ways of establishing learning structures for example connecting to organization and meeting routines at the participants home organizations establishing ways to share data and creating shared communication platforms a lack of such structures can result in an unsufficiently used project report with some participants having learned something but without real connections being formed between the process and the collaborating organizations and actor groups the collaboration including its learning processes then becomes a bubble outside of the organizations and groups that they should represent which will at best work as long as the concerned individuals stay with their organization rashman et al 2009 because consultants or short term employment positions are common in many organizations this issue remains an important obstacle for integrated planning to be effective on a long term basis the institutionalization and long term continuance of learning learning being built into the participating organizations is key see spf criterion f unfortunately understanding the importance of supporting organizational learning in the process seem far from universal in the pm field none of the cases reported efforts to establish any type of structures to support learning between process participants and the respective organizations that they represent 5 conclusions and key research issues pm has great potential for supporting planning and decision making processes in the governance of complex socio environmental systems such governance urgently needs innovative and efficient participatory processes that can be implemented in the real world overall however our assessment suggests that significant work remains for pm to be fully effective in supporting participatory planning while the papers we reviewed indicated that environmental pm is very effective at promoting knowledge integration and learning among participants our case studies also handled 11 out of 15 spf criteria poorly or very poorly table 3 judging by their presentations these studies often fell short of facilitating a multi value perspective within a democratic process and in integrating across organizations within a governance system a main underlying reason may be that the studies systematically and purposely leave certain aspects of their pm interventions out of the papers describing their work 7 7 the most common objective among our pm case studies however was the development of a pm process framework 43 which implies that many of the studies had a broad focus to implement pm within planning and decision making in the real world however including the whole range of spf criteria is a high but important standard to meet to establish truly participatory and integrative processes for implementation future study designs and research reports need to adjust to reflect that end new forms of funding of building structures within and between research institutions and practice and of publishing scientific work can support this development therefore several research questions are of key importance and need prompt investments and engagement to pursue the field s potential for questions of knowledge integration and learning the pm field can already make important contributions to participatory planning and decision making especially when it comes to approaches to knowledge identification and tools that integrate knowledge approaches for managing uncertainty and methods and tools that support learning processes among the involved stakeholders are already well developed however some vital improvements are still needed especially related to the question of whose knowledge is represented are there some types of knowledge that are generally included while other types are excluded for example is expert knowledge more likely to be included than lay knowledge or is knowledge in the form of data and numbers more likely to be included than what exists in text and other non numeric forms how can procedures for participant identification and selection be developed and improved to ensure that all the main pieces of knowledge are included questions of values and democracy have been given too little attention in the practice of pm while there are a variety of methods and tools within the field that could make important strides in addressing these aspects of planning processes most pm processes cannot currently be set as a standard for value based and democratic participatory processes efforts to address several research questions could help close that gap which include what types of values are currently included in pm processes are there patterns of value types that are commonly included or excluded intentionally or otherwise for example are values that are more difficult to capture by available tools simply left unacknowledged are the values of participants who are less process oriented less analytical or simply less well organized left out are certain types of actors systematically over or under represented in pm efforts do we need new procedures methods or tools for value identification selection of participants and management of power what knowledge tools and approaches for expanding these aspects of pm can be integrated from other disciplines and fields e g public administration social work urban planning operational research and multi criteria decision analysis valuable insights into such methods and tools fields are provided by for example huang et al 2011 johnson et al 2018 lamé et al 2020 rouwette and vennix 2006 lastly there remain important questions of organizational integration and governance the understanding that participatory processes need to be understood by and connected to their surrounding governance system is not well established in the pm field without advancements in this area we believe pm faces significant limits in practice addressing this problem requires prompt and extensive research efforts that confront broad questions including what procedures need to be added to pm processes to ensure that they are sufficiently coordinated with the planning and decision making context that surrounds it how can pm processes be developed that incentivize participation among all relevant actors how can efficient and effective pm processes be designed for example at what planning stages should different types of participants most efficiently and effectively be involved what are the participatory costs and benefits related to alternative pm process designs what structures and institutions could facilitate accurate spread of knowledge created in pm process beyond those actors who were directly engaged by this study we hope to inspire and support pm leaders and practitioners in their planning and reflection upon past future and current pm processes we also highlight important questions that we hope will guide our field into the future to vest our support we have adapted our review instrument to facilitate practitioner evaluation of their own pm processes on the collaborative pm website https participatorymodeling org pm for participatory planning and decision making a review tool the instrument may also support design and documentation of pm studies declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding for this study was provided by the us national science foundation through the socio environmental synthesis center sesync in annapolis maryland usa we would like to thank david hawthorne and gabrielle bammer for their involvement and support funding was also given by formas the swedish national research council for sustainable development grant no 2016 01432 and the swedish national contingencies agency project societal resilience governance social networks and learning we also want to thank dan milz for insights into portions of the manuscript appendix a c supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105073 
25774,in environmental participatory modeling pm both computer and non computer based modeling techniques are used to aid participatory problem description solution and decision making actions in environmental contexts although many pm case studies have been published few efforts have sought to systematically describe and understand dominant pm processes or establish best practices for pm as a first step we have reviewed a random sample of environmental pm case study articles n 60 using a novel pm process evaluation instrument we found that significant work likely remains for pm to fully support participatory and integrated planning processes while pm reports systematically address knowledge integration and learning they often neglect the facilitation of a multi value perspective within a democratic process and the integration across organizations within a governance system if not reported we suspect these aspects are also neglected in practice we conclude with key research and practice issues for improving pm as an approach for real world participatory planning and governance keywords participatory modeling review integrated planning planning process democratic process resilience 1 introduction participatory modeling for environmental planning and decision making over the past few decades environmental governance has been shifting to include more public participation throughout the decision making process with collaborative approaches called for in newer policy and regulatory directives leong et al 2011 reed 2008 sterling et al 2019 benefits of participation include increased public trust transformation of adversarial relationships social learning and higher quality and more durable decisions see reed 2008 for review a large body of literature describes what successful participatory processes involve but it also recounts the difficulties and problems that may arise from these efforts e g ansell and gash 2008 schuett et al 2001 susskind et al 2012 for example groups of stakeholders may be over or under represented in participatory processes leading to democratic problems relatedly power imbalances among the participants can make some participant s voices heard at the expense of others furthermore the complexity of environmental issues makes it crucial to coordinate decision making across geographical scales organizational boundaries and policy fields to achieve effective coordination and cooperation between public authorities in complex decision making contexts has proven difficult see e g hedelin 2017 so successfully engaging public participants in such nested decision making processes is a major challenge dietz et al 2003 perhaps as a consequence of that in practice one way activities for public input such as public hearings and public notice and comment have become standard while collaborative approaches such as co management and community science remain underutilized innes and booher 2004 jacobsen et al 2012 leong et al 2011 within the large and multifaceted research area that concerns methods and tools for planning and decision making 1 1 this area can be structured and described in different ways depending on the entrence points choosen such as area of application type of tools in focus and the aim of the research examples of previous overviews are huang et al 2011 johnson et al 2018 lamé et al 2020 rouwette and vennix 2006 a field that specifically refers to the concept of participatory modeling pm has developed during the last couple of decades it includes various methods and tools that can support collaboration and stakeholder involvement throughout an environmental planning process from problem formulation to generating knowledge about system dynamics to developing and evaluating decision alternatives and implementation although extensive and systematic reviews of environmental pm literature are lacking pm has been widely touted as a useful approach for understanding complex socio environmental problems by improving social learning and integrating expert and stakeholder knowledge davies et al 2015 pahl wostl et al 2007 zellner et al 2012b we 2 2 the us national science foundation s national socio environmental synthesis center sesync funded an interdisciplinary team of social scientists biophysical scientists software developers and participatory modeling practitioners to discuss the processes products and outcomes associated with participatory modeling and its approaches the group of authors was part of this team for a complete list see gray voinov paolisso and jordan participatory modeling https www sesync org project enhancing socio environmental research education participatory modeling therefore describe pm as a purposeful learning process for action that engages the implicit and explicit stakeholder knowledge to create formalized and shared representations of reality voinov et al 2018 further pm has also been shown to support conflict resolution trust building and collectively identifying and agreeing on problem solutions becu et al 2008 smajgl and ward 2013a the growing popularity of pm studies seidl 2015 has been attributed to the shifting norms for stakeholder engagement coupled with improvements in cyberinfrastructure that have surfaced novel ways to engage stakeholders in collaborative representations of complex socio environmental problems gray et al 2018 seidl 2015 sterling et al 2019 given the inherent abilities of modeling methods to structure complex problems to explicate the relationships between system components and to demonstrate the trade offs between key values pm has important potential to support the difficulties of participatory planning although there are rich literatures on both participatory planning and pm with respect to environmental issues for overviews see nared and bole 2020 voinov et al 2016 little work has explicitly integrated both domains while pm has shown clear contributions and great promise to improve public participation studies indicate a need within the pm community to better understand the complexities of participatory processes and to investigate how best to select and implement methods and tools to achieve desired benefits gray et al 2018 hedelin et al 2017a jordan et al 2018 seidl 2015 voinov et al 2018 another area of development is the need to understand pm in relation to the real world complex decision making contexts of environmental problems which generally span diverse legislations and policy fields at multiple geographical scales administrative levels and actors hedelin et al 2017a voinov et al 2018 furthermore theoretical and critical studies of pm are rare such studies can provide increased understanding of issues such as whether certain types of participants influence decision making in pm more than others issues of power if and how knowledge generated in pm may become accessible by others than those directly involved issues of efficiency and how decision making in pm can be related to the overall democratic system issues of democracy a planning perspective on pm can help to further develop pm with respect to these issues and would at the same time open avenues for transferring methods and tools of pm to the participatory planning field to support the development of best practices of pm with a view to adopt and implement pm into real world planning and decision making we will here explicitly study pm from a participatory planning perspective we ask what s left to do before pm can provide full support to real world participatory planning as a first step towards answering that question we systematically review the field of environmental pm according to ideals for participatory planning in complex governance settings to lay the basis for further studies of the complexities of participatory processes we focus on procedural aspects to do this we firstly draw a random sample n 60 of environmental pm case articles from a larger pool of pm articles that represents a near census of peer reviewed pm studies as of mid 2017 the sampled articles are reviewed with a novel evaluation instrument that structures the review and provides the participatory planning lens for the study the instrument is based on two complementary participatory planning frameworks one focuses on descriptive criteria e g number of participants and the other is theoretical and focuses on prescriptive process criteria e g how participants are selected see section 2 after presenting our findings we conclude with key research and practice issues for improving pm as an approach for real world participatory planning and governance 2 method and theory 2 1 data collection our review is based on 60 published case study articles listed in appendix c these were randomly selected from a pool of 212 articles compiled by searches on the academic databases web of science and science direct using the search terms participatory modeling collaborative modeling and companion modeling articles were further narrowed through filtering for environmental applications one of this paper s authors to ensure consistency was devoted to scanning each compiled abstract and removing it if it was not an environmentally focused pm case study case studies for the purposes of this review were limited to english language peer reviewed scholarly journals choices of methods will always limit the study a main limitation follows from the choices of databases search terms and application areas for example research of environmental applications that do not use the selected search terms such as operations research multi criteria decision making and decision theory are not included furthermore the use of scientific papers as data limits our possibilities to make statements about the performed studies as not everything about the study is reported in a scientific paper e g due to length restrictions there may also be an overrepresentation in the reports of what the authors see as successful cases processes and activities 2 2 evaluation instrument to evaluate the selected pm case papers we developed a novel 21 question evaluation instrument see appendix a the review questions are hereafter referred to as q1 q2 etc this instrument is based on two complementary frameworks for participatory planning processes the first framework the comparison of participatory processes copp hassenforder et al 2015 informed review questions about the case context e g the geographic scale the problem inspiring the pm process the pm process initiator the goals of the pm process and the number and type of participants in different steps of the process q2 10 the second framework the sustainable procedure framework spf hedelin 2007 2015 was used as the theoretical and normative basis for evaluating procedural aspects of the cases e g how participants were selected how power imbalances were managed what resources the process required and how the pm process was related to the surrounding decision making system q12 21 section 2 3 outlines the spf question 11 asked about specific tools and methods used in the case study it was also used to complement a survey of pm practitioners voinov et al 2018 our pm process evaluation instrument was elaborated through a stepwise and iterative procedure engaging the whole group of authors 2 the process started with a smaller group who developed pre tested and revised an initial set of questions against a subset of case articles these pre tests and revisions were repeated several times to integrate collective knowledge and experiences of the pm field in the final version of the instrument the whole group of authors iteratively discussed and refined questions and answer categories we also focused on making this instrument useful to those beyond our author team so we have posted it online for sharing 3 3 the digital evaluation instrument is available via the pm website https participatorymodeling org pm for participatory planning and decision making a review tool the questions were separately loaded into a web based questionnaire to assemble the review data and to generate basic reports including tables and diagrams each paper was double coded weeks apart by the same coder ensuring consistency in evaluations across the papers to further support data collection and analysis a generic process framework was used based on evers et al 2012 and alkan olsson et al 2011 it includes six steps covering the main topics that can be handled in a pm process from a planning perspective although not all steps were described in all case study articles fig 1 2 3 theoretical basis implications of participatory and integrated planning the theoretical basis of this study is operationalized by the sustainable procedure framework spf the spf prescribes what a planning process needs to include support or promote to be both integrative and participatory table 1 to that end it synthesizes relevant theories and research from a broad review of literature as well as from interviews with senior researchers in planning public administration economics political science resilience studies adaptive governance deliberative democracy integrated management and ecological economics for a full explanation of the spf and how it was derived see hedelin 2007 2015 the 16 spf criteria stem from the two concepts of integration 4 4 integration here means that efforts are being made to include and combine all the key aspects of a certain issue including an understanding of their relationships thus an inclusive and at the same time reductionist perspective is necessary to apply because the simultaneous inclusion of all aspects of an issue will make understanding and management of that issue hard and participation and are structured by five themes which together support a structured and theory based analysis of any participatory planning process in this review those using pm there are a number of participatory frameworks and best practice guidelines for participation and pm in literature such as barreteau et al 2010 hassenforder et al 2015 perez et al 2014 smajgl and ward 2013b the spf is used as basis in this study because it allows us to simultaneously perform a focus on procedure compared to a focus on output e g a management plan an implemented measure a developed model 5 5 the process and its outputs are highly dependent as the value and function of the resulting plan or decision depends on the quality of the process a theory based analysis a critical perspective based on the deductive and normative character of the spf and a governance perspective on pm due to inclusion of issues such as organizational integration and stakeholder representation which stems mainly from the principle of integration the use of the spf has provided deeper understanding and practicable advice related to several cases of planning and planning tools already see hedelin 2017 for illustration of the spf as analysis tool for 5 p m cases see hedelin and lindh 2008 and hedelin 2015a for examples of analyses of planning processes and see hedelin 2008 for an analysis of planning legislation 3 results we begin by summarizing a number of contextual features of pm studies including the number of publications per year subject focus areas of the studies their geographical distribution and other dimensions after that we present our analysis around the more process oriented spf themes 1 integration across disciplines 2 integration across values 3 integration across organizations 4 participation contributing to the process and 5 participation generating commitment legitimacy or acceptance see table 1 lastly we explore characteristics around evaluation and theoretical connection of the case studies appendix b includes complementary graphical summaries of the results referred to as figure b1 b2 etc 3 1 contextual features of the case studies the 60 reviewed papers in our sample were published between 2003 and the summer of 2017 when the literature search was finalized more than half of the reviewed papers were published during or after 2012 figure b1 among our case studies agriculture n 13 22 and river basin management n 11 18 were the most common focus areas the issues of food availability and climate change were not studied in any case studies reviewed figure b2 q2 the reviewed studies were conducted in all permanently populated continents with most studies in europe n 17 28 and fewest in south america n 1 2 figure b3 q3 almost all of the studies had a regional n 42 71 or local n 16 27 scale of interest with just one case study explicitly including a range of geographic scales figure b4 q4 project initiators were described in just over 60 of the cases n 36 of those 30 n 11 identified multiple types of initiators governmental agencies were the most commonly listed followed by scientists and ngos local community members were not identified as project initiators in any of the studies for studies that did not explicitly describe who initiated the study or project we inferred who initiated the study in almost all cases our review indicated that scientists had initiated the study suggesting they have the strongest role in initiating pm projects and case studies figure b5 q5 scientists also had a very strong role in leading the pm processes most papers report the type of actor that leads the pm process n 40 67 of those studies 85 n 34 were led by scientists and none were led by a local community member figure b6 q6 for most studies the number of participants was either listed or could be inferred the smallest number of participants listed explicitly was 6 and the largest was 602 30 of the studies had 25 or fewer participants 20 had 26 49 35 had more than 50 participants and for 15 of the studies the number of participants could not be determined figure b7 q7 a detailed examination however revealed that over 90 of the cases involved less than 65 participants the two largest studies involving 373 and 602 participants respectively both relied on online participation question 8 queried the cases process frameworks by which we mean several explicitly described steps including their relations which provide an overview and a structure for the pm process a majority of the studies n 33 55 reported some form of such a defined process framework there is great variation and lack of consistency of how process frameworks are described in the pm papers only two frameworks companion modeling gurung tayan et al 2006 and group model building vennix 1999 were mentioned in more than one case the pm studies were undertaken for a variety of reasons the most common of our listed purposes stated by 26 studies 43 concerns the development or application of a pm process framework no other purpose was cited in more than 12 of the cases 20 figure b8 in contrast to the diversity of purposes there was much more consistency in the types of reported results at least 80 of the studies reported that gathering knowledge from stakeholders all studies developing modeling method n 57 developing and applying a pm processes n 53 and stakeholder learning n 52 were important results figure b9 q9 the case studies drew on a range of methods and tools which were used for a variety of purposes figure b10 q11 here we define tools as specific software types for example agent based modeling system dynamics modeling decision trees etc and we define methods as process oriented approaches e g brainstorming scenario planning which can use a range of tools although in environmental pm there is often some degree of fluidity between the two voinov et al 2018 we recorded both methods and tools used in three phases of pm problem identification data collection about the problem and problem analysis the cases often used multiple methods and tools both for problem identification and for data knowledge collection the most commonly used methods and tools were interviews n 43 and expert elicitation panels n 44 followed by surveys questionnaires and polls n 35 and scenario building n 30 and focus groups n 27 most cases used only one kind of method or tool for problem analysis but no single tool dominated across our case study sample most popular were system dynamics modeling n 24 and geographic information systems gis n 22 3 2 procedural aspects of the case studies 3 2 1 integration across disciplines 3 2 1 1 integration of knowledge criterion a we reviewed case studies for whether and how they included procedures to identify the knowledge necessary for the pm process including the tools used for that purpose q12 different types of knowledge were targeted such as scientific expert and layman knowledges nearly every study 55 92 described a systematic procedure for addressing this issue a systematic procedure means that comprehensive and explicit procedures are used to identify the main pieces of knowledge needed in the process including justification of what knowledge to include fig 2 as detailed in table 2 participants e g experts stakeholders community members were the main source of information used to identify knowledge needs interviews were performed in 39 cases 65 expert elicitations were used in 25 cases 42 and brainstorming occurred in 12 cases 20 while no additional process to structure knowledge was mentioned in most studies we found that 15 studies engaged in a form of qualitative mapping 25 these forms of mapping differed significantly ranging from causal maps e g as a final product or as a step towards a system dynamics model to fuzzy cognitive maps the use of previous studies was less common as seen in 9 cases 15 and only 6 studies 10 collected or used data other than qualitative transcripts of interviews and focus groups e g administrative data spatial data many of the methods and tools applied in the cases figure b10 q11 may also support integration of knowledge examples of such tools frequently used include scenario building n 30 50 system dynamics modeling n 24 40 focused group discussions interviews n 27 45 brainstorming n 23 38 gis n 23 38 and cognitive concept mapping n 20 33 3 2 1 2 handling different views of knowledge e g positivist relativist criterion b we did not code the papers for this criterion explicitly as our early pilot efforts to create and apply our evaluation instrument along with the authors collective prior experience determined that this issue is rarely if ever handled explicitly in pm processes and even less so in pm case study applications a more in depth review approach is necessary to target this issue 3 2 1 3 handling different kinds of uncertainty criterion c of the reviewed case studies 37 cases 62 report a systematic approach towards uncertainties fig 2 q12 the rest of the papers report a limited approach towards uncertainties n 14 23 or do not report any uncertainty handling n 9 15 a range of methods and tools are used for those that report using systematic approaches the use of scenarios is the most common approach used n 15 other systematic approaches used are agent based models n 6 multi criteria analyses n 4 and conditional probability tables n 3 see table 2 we also coded the cases for the types of uncertainties addressed q13 the most common type concerns uncertainties in the system at hand such as alternative system functions n 40 74 the least common type is related to the proper formulation and representation of a conceptual model n 24 43 the other uncertainty types addressed in the reviewed papers are distributed within that range n 25 32 figure b11 most 79 of the papers that report uncertainties address more than one type of uncertainty one fifth of the cases concurrently addressed all five types of uncertainty that we evauated 3 2 2 integration across values 3 2 2 1 identification of and rational argumentation based on relevant values criteria d and e the main questions for examining these criteria concern how studies reported handling the main values involved and which methods and tools were used to support those efforts q12 just over 40 n 25 of the cases reported systematic approaches to values fig 2 meaning that a comprehensive and explicit approach was applied to identify the main values and a justified decision was made about which values to include and how to adjust the process accordingly there was a broad diversity of methods and tools used by cases that took a systematic approach to the issue table 2 out of the 25 cases using systematic approaches 7 cases used interviews and 3 used a multi criteria approach some cases did not include the important values focused process steps of formulating and evaluating alternatives or assessing of proposed decisions for those that did 64 n 32 made clear within the process how values were affected by the key alternatives considered such as alternative policies plans and measures the rest of the relevant cases n 18 36 did not report how the involved values were affected q12b the most used methods for analysis cited in the case studies were system dynamics modeling and gis figure b10 q11 while these can be strong tools for analyzing how some values are affected by alternative decisions we cannot tell from our analysis whether these tools were also used to analyze effects on a set of identified main values in comparison multi criteria analysis used in 9 cases 17 is a method that provides strong support for explicitly deliberating about how alternatives affect a set of selected values likewise cost benefit analysis is another tool for communication around values but was used in only 1 case 2 3 2 3 integration across organization 3 2 3 1 organizational learning criterion f by organizational learning we mean learning that reaches beyond the set of people directly involved in the pm process non participant learning examples of such approaches that we looked for included process evaluation and documentation the establishment of shared databases and the development of institutions e g meeting routines of the organizations represented by individuals in the process just over 30 of the cases n 19 reported on the issue of organizational learning and only 15 reported explicitly on this issue meaning that activities that might support organizational learning and discussions of organizational learning were both included in the paper fig 3 q16 the main way to accomplish organizational learning reported was to trust those directly involved to disseminate knowledge gained during the pm process to their home organization village family etc other ways included the distribution of maps and oral presentations structural or institutional approaches to organizational learning with long term continuation established as part of the process were not reported 3 2 3 2 handling of formal planning context criterion g a systematic way of fitting the pm process into the formal planning context including mapping of the formal context and a strategy for handling the formal context was reported in 15 of the reviewed cases n 9 nearly 60 did not report the issue at all n 35 fig 2 q12 only 3 cases reported that explicit communication around participants roles and mandates occurred as part of the process 5 fig 6 q18 no method or tool clearly emerges as common practice among the nine cases that used a systematic approach to the planning context issue although approaches are different we note that four of the studies are unified in their reliance on participants to fit the pm process into the formal decision making context table 2 3 2 3 3 handling of incentives including resources and efficiency removal of thresholds criterion h the issue of resources and efficiency is fundamental for the implementation of pm processes in practice we evaluated case studies for how they described the resources required for carrying out the pm process e g money time expert skills and data only 10 n 6 of the cases reported something about the resources required q14 among the few cases that did report resources we wanted to know if the trade offs involved could be understood i e what was the potential increased value that accompanied the added cost of the pm process and what can be learned from the case about efficient pm process designs these cases provided only sparse accounts which did not enable cross case comparisons of alternative process designs the cases mainly accounted for issues of time either the time required to complete the pm process as such or that the process required more time than an alternative non pm process two cases discussed monetary costs of the process in general terms and only one case reported on the total project budget four cases discussed resources in relation to process outcomes 3 2 3 4 handling human aspects e g trust engagement conflict management criterion i over half of the cases did not report taking any actions to handle the human aspects of the process such as trust engagement and inter personal conflict n 33 fig 4 q17 the most common action applied by almost one third of the cases was to engage a skilled process leader n 19 almost one third of the cases made the pm process transparent e g by documenting the reasons behind the decisions made only 3 5 of the cases took actions to understand the social relationships of the participants before the process e g value positions disputes and conflicts among the group of participants 3 2 4 participation contributing to the process 3 2 4 1 inclusion of knowledge owned by relevant actors criterion j as stated above more than 90 of the cases reported using a systematic approach to address the issue of knowledge identification n 55 table 2 q12 to include the knowledge owned by relevant actors identification and engagement of these actors in the process is also a prerequisite however less than 20 of the cases reported using systematic procedures for identifying and selecting participants n 11 table 2 q12 on the positive side only a few cases n 2 3 did not report this issue at all furthermore among the cases that did describe participant identification systematically or not n 58 all cases 100 reported that their main reason for selecting and inviting participants was that they own knowledge that was needed in the pm process q12a 3 2 4 2 inclusion of the ideological orientations represented by relevant actors criterion k just over 40 of the cases reported systematic procedures for identifying the main values related to the issue at hand n 25 but as we just saw less than 20 of the cases reported using systematic procedures for identifying and selecting participants n 11 table 2 3 2 4 3 participation in the most critical phase s of the process criterion l we reviewed case studies with respect to each of the generic pm steps shown in fig 1 first as to whether each step was explicitly described and then if so which types of actors participated in the step fig 5 q10 almost all papers included a description of the work for achieving the baseline understanding of the case system n 58 and nearly all case studies included a discussion of model development and use n 52 the outcome of the process such as a decision plan proposed measure or an agreement was the step least described less than 30 of the cases n 17 the rest of the planning steps process preparation and setup formulation of objectives and formulation and evaluation of alternatives were included and described in 70 80 of the cases considering the stepwise involvement of actors there is a somewhat unbalanced focus among the case studies towards the more knowledge based process steps compared to those of more managerial value oriented and operational characteristics process preparation objectives alternatives evaluation and outcome respectively fig 5 shows who was involved in each phase of the reported pm processes scientists were the type of participant reported to be involved most frequently in pm planning steps this is in line with our earlier finding that scientists were leading most of the pm processes in the preparation step scientists were involved more than twice as often as any of the other participant types however when it comes to the pm process outcome local community members were involved the most non governmental organizations ngos and local community members were rarely involved in the process preparation step furthermore ngos had the lowest involvement in all steps except for preparation 3 2 5 participation generating commitment legitimacy and acceptance 3 2 5 1 a procedure for defining the actors that should be involved criterion m which actors participate has fundamental and cross cutting impacts on the quality of the process it directly affects the fulfilment of criteria j and k and indirectly affects most of the other criteria less than 20 n 11 of the cases reported systematic procedures for addressing the issue of identification of participants i e the participants were identified and selected by a comprehensive and justified approach fig 2 q12 we noted four different approaches among these 11 studies with systematic procedures table 2 with sampling being the most common approach 6 cases the remaining cases used some sort of pre established selection criteria 2 following the nominations of a local group or committee 2 or inviting all possible participants 1 on the positive side only a few cases did not report this issue at all n 2 the participant identification and selection criteria that we coded q12a were applied by most of the cases that reported on this issue systematically or not participants who may support implementation n 47 who own valuable knowledge and perspectives n 58 and those who are affected by the process n 52 only 5 cases applied other participant selection criteria 3 2 5 2 handling power asymmetries criterion n and procedures that ensure that ideological orientations are not suppressed criterion o half of the cases did not report any activities for managing power asymmetries fig 6 q18 for the rest three approaches were applied most commonly 1 separated meetings 2 structured communication procedures to obtain input from each party each by 30 n 18 and 3 documentation to increase transparency just over 20 n 13 only 5 of cases reported the use of telecommunication e g via a webpage as a way of handling power issues and only 5 reported that they clarified the roles and mandates of the participants involved in the process because knowledge is a resource that brings power an additional and important way to manage power is to support learning which takes us to spf criterion p 3 2 5 3 stakeholder learning criterion p the review focuses on learning processes rather than on learning outputs i e whether and how much the participants learned from the pm processes by q15 over 80 n 49 of the cases described activities that may support stakeholder learning fig 7 just over half of the cases however reported explicitly on how they supported learning meaning that activities that supported learning were described and learning as such was discussed in the paper this does not mean that an evaluation of the learning was reported which could show if the process activities indeed have resulted in learning see section 3 3 among the cases that reported explicitly on learning participant interactions and discussions of various forms such as workshops modeling and field trips were reported as activities aimed to support learning 3 3 evaluation and theory connection of the case studies although all reviewed papers were published in scientific journals over 60 n 37 did not include any kind of evaluation of their project q20 of the evaluated cases 30 lacked a description or justification of the evaluation n 7 of the cases that included evaluations most cases used data on the participants understanding of the process using interviews or questionnaires n 13 the second most commonly used approach was to use process data other than the participants self reported experiences such as observations of process activities or interview with the process leader only one case used theory to support their evaluation see fig 7 and fig 8 we also reviewed cases for whether the outcomes of the planning processes were recognized in any decisions or actions outside the pm effort itself q21 almost 90 of the cases lacked such a recognition we categorized the seven cases where outcomes of the pm effort had been recognized recognition by laws or regulations n 1 by a management action measure n 6 by an institutional arrangement n 0 or by other means n 1 4 discussion learning from pm research pm research is generally carried out in collaboration with public authorities organizations and persons who are engaged in managing their real world problems as seen above many of the studies we evaluated aimed to support learning whether about the socio ecological problem at hand about different ways of understanding it or about alternative ways forward however one can also learn about the pm process e g the characteristics and function of methods and tools employed process designs process leadership and conflict management etc as pm researchers it is mainly this kind of knowledge that we seek to contribute to a growing body of shared and collectively reflected knowledge of pm processes our main strategy has been to report our studies in scientific papers these papers became our data for this review 6 6 as noted in the method section the use of academic articles has its limitations it may for example be that systematic procedures are being used to identify participants criteria m in most cases but there are systematic gaps in the reporting of this and potentially other criteria in the pm literature in other words more focus is given to the outputs and descriptions of modeling methods and tools rather than other process issues complementary studies would have to be made to study that they generally describe pm processes and their contexts focusing on the specific study objective which as we have learned can vary considerably figure b8 to this end gray et al 2018 suggest a procedure to standardize the reporting of pm studies to allow for better systematic review across studies and thus to better understand and inform the evolution of this field the 4ps of pm are purpose process partnerships and products these 4ps should be addressed explicitly in all publications whether peer reviewed or not to ensure their contribution to generalizable knowledge about how to conduct pm exercises towards impactful outcomes in addition while project evaluation should also be regarded as an important part of a regular research paper we have found that most papers do not include an evaluation moreover for those that do include evaluations reporting standards are low lacking justification such as a detailed description of how the evaluation was performed and clear evaluation criteria for the papers that include a justified evaluation n 7 most are based on participants perceptions of the pm process and only one case used a specific theory to support evaluation this decreases the usefulness of the evaluation performed because it cannot easily be understood by anyone outside the case what standards are the assessment being made against whose standards furthermore whilst this review focuses on process evaluation of both process and outcome criteria are important to include if we want to contribute to a complete comprehension of pm our study shows that currently case study reports seldom include evidence of outcomes in this study we have purposely chosen to be very specific about our basis of assessment because it allows us to compare pm against a current framework of knowledge about what a participatory and integrative planning process should imply the spf a summary interpreting the results of our analysis is provided in table 3 because the criteria are related to each other in various ways the criteria wise result can be summarized across several lines we have identified three cross cutting patterns that merit discussion 1 knowledge integration and learning 2 values and democracy and 3 integration across organizations 4 1 knowledge integration and learning spf criteria a c j m p 4 1 1 knowledge identified and often integrated the spf states that a fundamental characteristic of a participatory planning and decision making process is that it integrates the main pieces of knowledge related to the issue at hand spf criteria a and j for river basin management in a regulated river for example knowledge of river ecology flow regulation alternative hydro power technologies would be key as would knowledge of policy law economy and planning depending on other case specifics knowledge of tourism recreation land preservation heritage flood management irrigation technology and more could be likewise important knowledge comes in various forms such as disciplinary knowledge criterion a from experts databases and reports and also more local and contextual knowledge criterion j from local stakeholders ngos and public authorities which may not be documented as text or numbers examples of the latter include local farmers harvesting and fertilizing routines fishers observations of fish stock dynamics a regional authority s current plans for the infrastructure system and local attitudes towards a specific policy or measure engaging experts and local actors who can represent all the pieces of relevant knowledge may be very expensive but missing out on a key piece of knowledge could have devastating consequences therefore it is important to prioritize knowledge carefully and to be open to reassessing knowledge needs as the pm process evolves to rely solely upon the expert judgement of a few persons is not sufficient glaas et al 2010 in great part because experts are likely to emphasize the knowledge that lies close to their own expertise we do not know of the knowledge that we do not know glaas et al 2010 hedelin and lindh 2008 instead a more comprehensive transparent and justified procedure is necessary our finding that over 90 of reviewed cases apply such approaches to knowledge identification table 2 is gratifying furthermore the frequent use of methods and tools that can integrate knowledge such as scenario building system dynamics modeling agent based modeling gis and cognitive mapping is also a very strong feature figure b10 which implies that the pm field has an important contribution to make to environmental planning and decision making when it comes to knowledge integration note however that the application of these methods and tools is no guarantee that the knowledges have been integrated effectively for example focus group discussions that are poorly led will not function well and any approach that neglects actors with key knowledge will fall short we have coded the cases for issues such as representation power imbalances and conflict management which may indicate whether the methods and tools applied function well see result for criteria m n o and i difficulties with social power and tool use are interwoven with the different ways participants understand knowledge and its connection to values i e their different epistemologies halpern and o rourke 2020 o rourke et al 2019 for instance can some knowledge be considered as objective while other knowledge needs to be treated as depending on individual experiences and preferences not only are epistemologies usually different but also depending on their disciplinary or professional training life experiences and other personal characteristics participants of the pm process will be differentially equipped for managing epistemological differences the pm process including leadership resources methods and tools will affect how these dual differences of content and capacity play out hedelin et al 2017a process features can become obstacles for knowledge integration or they can enrich the process by supporting a better understanding of the complex planning issue at hand spf criterion b we did not code the papers for spf criterion b explicitly because this issue is hardly ever reported in pm papers see voinov et al 2018 for guidance on how conceptual modeling methods such as fuzzy cognitive mapping fcm cognitive mapping can complement quantitative methods to avoid that kind of problem more studies are needed however to explore the issues of epistemology in participatory processes and of how pm may provide support for example how does the epistemology of the process leader affect the design and orchestration of a participatory pm process when it comes to knowledge integration learning and management of alternatives how aware of our epistemological positions are we and of how they manifest in our behaviors and actions as pm process leaders as pm researchers or as modeling tool developers how can methods and tools in pm help to expose and manage alternative views of knowledge so that such differences do not hamper the participatory process such studies could raise of awareness of this issue among pm researchers and hopefully epistemological aspects will become explicit in pm studies that focus on knowledge integration and learning for instance in their analysis of 5 p m cases hedelin et al 2017b showed that methods and tools regularly used in pm have the potential to act as learning platforms supporting discussions among the involved researchers and experts of ways of understanding both reality and knowledge which are fundamentally connected however sometimes the methods and tools used can hamper discussions of knowledge views and integration of knowledge hedelin et al 2017b for example because of the time and resources that extensive quantitative simulation models rely on it may be difficult to reconsider the foundations or components of the model in accordance to the understanding gained from an interdisciplinary learning process since this may not be included in the project time plan and budget hedelin et al 2017b 4 1 2 understanding uncertainty in the context of knowledge and learning the issue of uncertainties what we don t know is critical bammer 2013 the complexity inherent in the socio environmental systems targeted by planning efforts makes it impossible to eliminate uncertainty no matter how much time and resources are spent on increasing the level of factual knowledge bradshaw and jeffrey 2000 zellner 2008 thus uncertainty will be a regular feature of the decision making process and it may have large impact on the various decision alternatives at hand the potential outcome of the process logically an effective approach towards handling uncertainties must involve understanding which uncertainties are the most important in terms of how they affect the outcomes of alternative decisions clear understanding of the relative importance of different uncertainties should guide where evaluation and research effort should focus an open comprehensive and systematic approach towards the full range of types of uncertainties is therefore imperative by our results we can see that knowledge and practice concerning the management of uncertainty is relatively well established within the pm field over 60 of the cases have handled this issue by systematic procedures comprehensive and justified fig 2 even though there are additional types of uncertainty e g of the modeler s skills the types that we reviewed are covered quite well by the cases which signals that none of these broad types are being completely overlooked figure b11 4 1 3 whose knowledge integration of knowledge depends on more than identifying knowledge and uncertainties knowledge is held by actual groups and persons so appropriately broad representation of stakeholders is also essential to knowledge integration the spf criterion of carefully defining the actors that need to be involved m is fundamental for the fulfillment of inclusion of actors knowledge j and likewise important for many other key process criteria such as inclusion of actors ideological orientations k and for establishing a democratic process to meet these criteria the most relevant actors those directly or indirectly affected by the planning issue at hand need to be identified and involved in the process directly or by representation dryzek 2013 lidskog 2005 unfortunately we found that less than 20 of cases described systematic procedures for participant selection fig 2 4 1 4 stakeholder learning is becoming central to pm process the final key knowledge integration criterion concerns stakeholder learning p socio environmental problems are generally complex both technically and socially defries and nagendra 2017 local actors who are directly concerned by the problem at stake can provide important knowledge to the process that complements expert knowledge and their participation may also generate commitment legitimacy and acceptance for the resulting decision and facilitate implementation bryan 2004 however if stakeholders including experts are going to take part in the decision making process they need to understand the problem that the decision concerns sterling et al 2019 stakeholder learning is therefore a fundamental component of participatory processes both for reasons of knowledge integration and democracy we return to the issue of democracy in the next section we have shown that over half of the pm cases reported explicitly on stakeholder learning including how they intended to support it through process methods and procedures another 30 reported on methods and tools that may support learning such as workshops modeling activities and field trips nevertheless our judgement is that although the field has come far in supporting stakeholder learning progress can still be made especially when it comes to the question of whose knowledge is shared and whose is represented in analytic models another urgent aspect to further develop pm in this respect is to increase the number and quality of standards for evaluating learning see section 3 3 about evaluation 4 2 values and democracy spf criteria d e k o our values influence the way we understand a problem and how we are likely to respond to it the issue of how to deal with values is therefore a fundamental question for establishing a good planning process and the planning literature is much concerned with it e g the separation of knowledge and values the role of the planner process leader and how to deal with the issue practically for overview and examples see allmendinger 2002 thomas 2012 wallace et al 2020 practically the way forward is to identify the most important values connected to the problem at hand spf criterion d and to use these as the basis for rational argumentation around alternative choices that needs to be made during the planning process spf criterion e 4 2 1 which values just over 40 of the cases reviewed describe systematic approaches for identification of the most important values fig 2 so for a majority of the cases one cannot be sure that the values considered are the ones most affected by the decision at hand for example if a pm process concerns a plan for a regional agricultural district it might be that the management alternatives are analyzed for their impact on water quality bio diversity and farmers income levels but not for climate or for securing food supply key values can be excluded due to organizational administrative or practical reasons such as the agenda of the funding agency the mandate of the initiating authority or the formulation of regulations that protect a certain value other possible reasons are ignorance inattention and the influence of unbalanced power relations 4 2 2 whose values generating a comprehensive understanding of a problem and possible solutions requires input from those actors who represent the array of ideological orientations connected with the issue hemmati et al 2002 express this nicely by stating that participation aim s at multi subjectivity rather than objectivity which is a claim based on the view that everyone has a subjective understanding of an issue and can therefore only contribute with parts of the overall picture hemmati et al 2002 pp 44 and 300 but in our review over 80 of the studied cases did not describe participant selection procedures that could secure inclusion of the full spectrum of ideological orientations related to the issue at stake fig 2 seventy percent of the studied processes took place at the regional scale figure b4 there are various ways for capturing the full array of ideological orientations at this scale in addition to a comprehensive and justified participant selection approach spf criterion m such approaches need to include practical means of communication one strategy can be to open up the process for a large number of participants direct involvement using some form of web portal for distanced communication only four cases among the studies we reviewed used a web portal however and as the median number of participants in the studies cases was 25 it appears that this strategy has not yet been applied much within the pm field another way to capture the values of many persons could be to use some form of crowd sourcing but here too the pm field has not yet engaged much only two cases use a crowd sourcing technique figure b10 yet another main strategy to gather information on people s values is to seek representation of a broad set of views by carefully selecting persons and or interest organizations representing different views currently this seems to be the most common strategy although again most cases do not use a systematic selection procedure the advantage here is that it allows for a close involvement of the actors which gets more difficult as the group of participants grows larger for selecting persons that can represent main ideological orientations at a regional scale one could expect ngos to be a key type of participant instead of finding and justifying the selection of several unorganized persons that can represent the key values or interest groups at that regional scale it would certainly be more feasible to commit a number of ngo leaders who are already organized to represent key values at larger scales than individual surprisingly however ngos are the participant type with lowest involvement in most of the planning steps fig 5 connected to values and representation it is also important to consider which planning steps are in focus and which types of participants are involved in the different steps spf criterion l the cases reviewed generally described several of the steps in our 6 step model fig 1 but there was an unbalanced focus towards the more knowledge based process steps compared to the more managerial value oriented and operational steps process preparation objectives alternatives evaluation and outcome respectively furthermore ngos and local community members had relatively low rates of involvement in the value oriented steps such as objective setting and evaluation where one could expect these types of participants to play a key role 4 2 3 power and rationality power can come in different forms knowledge social skills monetary resources and formal legal mandates it is generally accepted that power imbalances exist and that they have a negative effect on the conduct of a democratic participatory process of rational deliberation e g flyvbjerg 1998 and allmendinger 2002 power asymmetries can be detrimental to democratic outcomes but only if we ignore them and allow them to work against transparency and rationality spf criterion n arnstein 2019 original publ 1969 flyvbjerg 2002 deliberation where the various voices of the affected actors will interact and transform rather than simply be aggregated in an unchallenged manner is actually one way to account for and handle power imbalances roberts 2004 participatory processes are often tightly related to the idea of consensus identification of common interests on which all involved agree because of power imbalances however critics warn that social groups or ideological orientations might be systematically excluded from decision making if it is based on such a consensus ideal mcguirk 2001 therefore consensus based processes specifically need to include measures that ensure that this does not happen criterion o one way is to design the process as compromise seeking rather than consensus building zellner et al 2020 we found that half of the reviewed cases did not report any activities to manage power imbalances such as separated or structured meetings documentation anonymous involvement or clarification of roles and mandates fig 6 this is troublesome because it will favor the solutions brought forward by those with the greatest power rather than solutions that are democratically defined various methods and tools used in the reviewed pm cases however have a potential to support management of power e g by providing structure clarity and transparence to the process by learning by documentation of a discussion and the reasons behind a decision and by enabling electronic voting and communication furthermore several methods and tools can also clarify how alternative measures decisions plans specifically affect the values included in the process good examples are gis multi criteria analysis and cost benefit analysis bendor and scheffran 2019 such tools can support rational argumentation based on both knowledge and values which is a fundamental feature of a democratic participatory process spf criterion e out of the cases that include the planning step of evaluation of alternatives 64 used tools to show explicitly how the included values were affected by the main alternatives considered in the process this is an auspicious result showing that knowledge and tools related to this issue are well established in the pm field we must continue to raise awareness within the pm community that the values of concerned actors just as much as their knowledge need to be systematically integrated in the pm process to summarize the potential within the field is strong but the issues of representation and power need to attain a much greater focus before pm processes can be commonly regarded as democratic 4 3 integration across organizations pm in the governance system spf criteria f i l planning in the context of complex socio environmental issues is generally characterized by equally complex organizational and structural settings dietz et al 2003 this includes organizations of different types public authorities companies ngos and stakeholder networks operating at different geographic scales ranging from multi national to highly localized these organizations have different roles relationships mandates responsibilities and powers and typically none have the full capacity or authority to unilaterally manage the problem at hand glaas et al 2010 this is a fundamental reason why an effective planning process needs to be integrated across the main organizations related to the issue at hand and embedded in its planning context dietz et al 2003 on this important dimension of participatory planning we find that pm falls short of its promise 4 3 1 the planning context pm disconnected an important part of the planning context of a pm case is formal including legislation authorities binding decision making mandates and ongoing planning and decision making processes that relate to the pm issue at hand in various ways it generally includes multiple administrative scales and sectors the need to understand and handle these complex institutional and organizational settings of cross sectorial issues relates to the fact that different actors have different formal mandates and capacities glaas et al 2010 prager 2010 for example a central authority may be formally responsible for developing a plan but may not have a mandate to implement it at the local level where implementation depends on decisions made by local governments and the general public and on investments and know how from private organizations a basis for the successful management of cross sectoral issues is therefore to firstly generate a broad understanding of these interdependencies and of the structural prerequisites of the planning process among the involved organizations and secondly to manage this in the planning process spf criterion g how does the pm process relate to the representative democratic system around it what is the formal role and mandate of the process leader what can actually be managed by the participatory process and hence how should the objective of the process be formulated to effectively adjust to that what other processes and decisions need the process be coordinated to address and who needs to be involved to that end our results on this topic are alarming because only 15 of the cases reported satisfactory approaches to the issue of cross organizational integration and almost 60 did not report addressing it at all furthermore a large portion of the papers did not report who the project leader was and who initiated the process only 5 of the cases reported explicit communication around roles and mandates having taken place as part of the process fig 6 this indicates a lack of understanding of the importance of clarity around formal roles and mandates of the participants what mandates do the initiator and leader really have what are the reasons for establishing the process what will the output of the process really mean when it comes to implementation most pm cases 70 took place at the regional scale a scale that commonly requires coordination with both larger and smaller scales only one study however reported an explicit strategy to connect scales in the planning process this lack of connection is troubling for example a pm process at a regional scale might establish a plan that disrupts the implementation of a local decision made by democratically elected politicians clarity of roles and processes are critical with regard to decision implementation to the socio ecological problems at stake to the local representative democracy and to the purpose of the regional pm process within political science these kinds of problems related to the fuzziness of regions as political entities and the need of many issues to be coordinated at the regional scale have even been termed the regional mess allen and cochrane 2007 mccallion 2008 we believe that pm has a great potential to help manage this so called mess through the use of tools and methods that can clarify complex system behaviors and aid the establishment of well founded strategies and plans however to make this happen and to prevent pm from actually adding to the mess pm processes must have a strong self awareness and must be understood by and embedded within the surrounding governance system other results further underline this point almost 90 of the cases do not present planning outcomes that are recognized in any decisions or actions outside the scope of the pm processes moreover we did not find a single case that addressed climate change mitigation or food availability in our randomized case pool we hypothesize that one reason may be that these areas clearly require connection among global to local scales and actors and that pm practitioners are not yet comfortable or knowledgeable about how to manage those connections considering the inherent potential of pm to connect scales and actors we urgently need to develop our tools and procedures to handle this difficulty 4 3 2 why participate as described above a foundational reason to integrate a pm process with its planning context is to involve all the actors that together own the capacity to handle the problem at stake once these actors are defined however there are still several barriers that can disrupt their collaboration to overcome these the issue of incentives including resources and efficiency is fundamental h ansell and gash 2008 why do participants get involved what thresholds do they need to pass do authority mandates legislation budgets and schedules make participation feasible how can the process be set up to increase its efficiency and to decrease the time and money required to participate what alternative set ups exist and what are their pros and cons in respect of resources and efficiency incentives and resources cuts through many focal issues of participatory planning such as knowledge integration and learning values and democracy and collaboration and coordination if there are no incentives there will be no participation despite this the issue of resources is the most neglected one in our review 90 of the cases did not report anything related to it the few cases that did address resources did so in a limited manner that focused on time and did not provide any grounds for cross case comparison of methods or process designs furthermore we know from our review that participation is spread over many steps in the processes fig 5 the question is therefore have the resources participants time and engagement been spent efficiently and effectively or will the experience of the process rather make future engagement unlikely because the participants found it too costly generally participation needs to be focused to the phases of the planning process where it is the most useful given its objectives spf criterion l the initial stages of a process generally have a greater influence on the process outcome than later stages and involving the participants in the early steps such as process set up and formulation of process objectives is also a good way to create transparency and a sense of ownership e g spf criteria i n and o different groups of participants may need to be involved for different purposes and hence at different phases because this issue is fundamental for implementing pm on a broad scale further studies are needed to investigate the state of knowledge here in more detail incentives can also come in less tangible forms and be connected to so called human aspects such as trust engagement and conflict management spf criterion i these pyscho social aspects depend on deep rooted behaviors power relationships organizational cultures history and more and their importance for establishing a successful participatory process where people share their knowledge and perspectives is often underestimated trust is fundamental for making collaboration happen and social carrots are needed to make it work well zellner et al 2012a for example in order for people to make room in their already stressful working schedules it is key that participating persons need to feel warmly welcomed to meetings that their ideas and opinions are received with respect that initiatives are encouraged that critical views are allowed among others another key is to establish a sense of ownership as when it is made clear that all of the involved actors are valuable in terms of the overall capacity to manage the issue at hand reed 2008 furthermore sometimes personal conflicts or controversial issues obstruct productive and evidence based reasoning and deliberation weiss and hughes 2005 to manage all of this competent process leadership including preparedness to handle conflicts is fundamental as are measures such as process transparence managing power imbalances and setting up ground rules for participant interaction etc milz 2018 müller seitz 2012 our results show that compared to the more tangible incentives spf criterion h human aspects spf criterion i are better managed there is still much left to hope for however because more than half of the cases do not report any activities to address this issue fig 4 4 3 3 learning only for those directly involved as we have discussed learning is widely seen as key in participatory planning spf criterion p we have also seen that within the pm field most of the cases apply methods and tools that aim to support stakeholder learning figure b15 when it comes to supporting integration across organizations a vital aspect is the need to establish learning structures within the participating actor groups and organizations glaas et al 2010 a pm process can support this by making time to discuss it with the participants and suggesting alternative ways of establishing learning structures for example connecting to organization and meeting routines at the participants home organizations establishing ways to share data and creating shared communication platforms a lack of such structures can result in an unsufficiently used project report with some participants having learned something but without real connections being formed between the process and the collaborating organizations and actor groups the collaboration including its learning processes then becomes a bubble outside of the organizations and groups that they should represent which will at best work as long as the concerned individuals stay with their organization rashman et al 2009 because consultants or short term employment positions are common in many organizations this issue remains an important obstacle for integrated planning to be effective on a long term basis the institutionalization and long term continuance of learning learning being built into the participating organizations is key see spf criterion f unfortunately understanding the importance of supporting organizational learning in the process seem far from universal in the pm field none of the cases reported efforts to establish any type of structures to support learning between process participants and the respective organizations that they represent 5 conclusions and key research issues pm has great potential for supporting planning and decision making processes in the governance of complex socio environmental systems such governance urgently needs innovative and efficient participatory processes that can be implemented in the real world overall however our assessment suggests that significant work remains for pm to be fully effective in supporting participatory planning while the papers we reviewed indicated that environmental pm is very effective at promoting knowledge integration and learning among participants our case studies also handled 11 out of 15 spf criteria poorly or very poorly table 3 judging by their presentations these studies often fell short of facilitating a multi value perspective within a democratic process and in integrating across organizations within a governance system a main underlying reason may be that the studies systematically and purposely leave certain aspects of their pm interventions out of the papers describing their work 7 7 the most common objective among our pm case studies however was the development of a pm process framework 43 which implies that many of the studies had a broad focus to implement pm within planning and decision making in the real world however including the whole range of spf criteria is a high but important standard to meet to establish truly participatory and integrative processes for implementation future study designs and research reports need to adjust to reflect that end new forms of funding of building structures within and between research institutions and practice and of publishing scientific work can support this development therefore several research questions are of key importance and need prompt investments and engagement to pursue the field s potential for questions of knowledge integration and learning the pm field can already make important contributions to participatory planning and decision making especially when it comes to approaches to knowledge identification and tools that integrate knowledge approaches for managing uncertainty and methods and tools that support learning processes among the involved stakeholders are already well developed however some vital improvements are still needed especially related to the question of whose knowledge is represented are there some types of knowledge that are generally included while other types are excluded for example is expert knowledge more likely to be included than lay knowledge or is knowledge in the form of data and numbers more likely to be included than what exists in text and other non numeric forms how can procedures for participant identification and selection be developed and improved to ensure that all the main pieces of knowledge are included questions of values and democracy have been given too little attention in the practice of pm while there are a variety of methods and tools within the field that could make important strides in addressing these aspects of planning processes most pm processes cannot currently be set as a standard for value based and democratic participatory processes efforts to address several research questions could help close that gap which include what types of values are currently included in pm processes are there patterns of value types that are commonly included or excluded intentionally or otherwise for example are values that are more difficult to capture by available tools simply left unacknowledged are the values of participants who are less process oriented less analytical or simply less well organized left out are certain types of actors systematically over or under represented in pm efforts do we need new procedures methods or tools for value identification selection of participants and management of power what knowledge tools and approaches for expanding these aspects of pm can be integrated from other disciplines and fields e g public administration social work urban planning operational research and multi criteria decision analysis valuable insights into such methods and tools fields are provided by for example huang et al 2011 johnson et al 2018 lamé et al 2020 rouwette and vennix 2006 lastly there remain important questions of organizational integration and governance the understanding that participatory processes need to be understood by and connected to their surrounding governance system is not well established in the pm field without advancements in this area we believe pm faces significant limits in practice addressing this problem requires prompt and extensive research efforts that confront broad questions including what procedures need to be added to pm processes to ensure that they are sufficiently coordinated with the planning and decision making context that surrounds it how can pm processes be developed that incentivize participation among all relevant actors how can efficient and effective pm processes be designed for example at what planning stages should different types of participants most efficiently and effectively be involved what are the participatory costs and benefits related to alternative pm process designs what structures and institutions could facilitate accurate spread of knowledge created in pm process beyond those actors who were directly engaged by this study we hope to inspire and support pm leaders and practitioners in their planning and reflection upon past future and current pm processes we also highlight important questions that we hope will guide our field into the future to vest our support we have adapted our review instrument to facilitate practitioner evaluation of their own pm processes on the collaborative pm website https participatorymodeling org pm for participatory planning and decision making a review tool the instrument may also support design and documentation of pm studies declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding for this study was provided by the us national science foundation through the socio environmental synthesis center sesync in annapolis maryland usa we would like to thank david hawthorne and gabrielle bammer for their involvement and support funding was also given by formas the swedish national research council for sustainable development grant no 2016 01432 and the swedish national contingencies agency project societal resilience governance social networks and learning we also want to thank dan milz for insights into portions of the manuscript appendix a c supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105073 
