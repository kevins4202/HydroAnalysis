index,text
26390,sophisticated methods for uncertainty quantification have been proposed for overcoming the pitfalls of simple statistical inference in hydrology the implementation of such methods is conceptually and computationally challenging however especially for large scale models here we explore whether there are circumstances in which simple approaches such as least squares produce comparably accurate and reliable predictions we do so using three case studies with two involving a small sewer catchment with limited calibration data and one an agricultural river basin with rich calibration data we also review additional published case studies we find that least squares performs similarly to more sophisticated approaches such as a bayesian autoregressive error model in terms of both accuracy and reliability if calibration periods are long or if the input data and the model have minimal bias overall we find that when mindfully applied simple statistical methods such as least squares can still be useful for uncertainty quantification keywords uncertainty assessment mechanistic modeling surface hydrology water quality least squares statistical inference 1 introduction statistical model calibration and uncertainty quantification uq have recently received substantial attention in surface hydrology and water quality research several studies have stressed the importance of more realistically describing the behavior of calibration errors a k a residuals and thus moving beyond least squares ls calibration assumption of independent and normally distributed residuals e g reichert and mieleitner 2009 renard et al 2011 honti et al 2013 in particular it has been suggested that by using error models that explicitly consider the heteroscedasticity i e non constant variance and autocorrelation of the calibration errors parameter estimation and subsequent predictive uncertainty assessment can be improved in a relatively straightforward manner sorooshian and dracup 1980 yang et al 2007b schoups and vrugt 2010 del giudice et al 2015a however while such sophisticated approaches have been shown to be helpful in the specific situations where they were tested this does not necessarily imply that simpler statistical techniques such as least squares calibration are never useful therefore there is a critical knowledge gap in hydrologic and environmental modeling regarding when simple calibration approaches are acceptable versus when more sophisticated ones are needed understanding the domain of applicability of simple least squares along with its limitations is essential indeed we argue that the presupposition that statistical inference always has to be conducted with conceptually and computationally burdensome methods might be inducing modelers to eschew uq altogether e g bosch et al 2011 coutu et al 2012 razavi and tolson 2013 or use pseudo statistical methods with unclear probabilistic interpretation e g freni et al 2009 beven and smith 2015 in the context of predictive uq we therefore address an important yet so far unanswered question are there cases in which a simple method such as least squares yields predictions with precision and accuracy that are on par with state of the art approaches that account for error autocorrelation and parameter uncertainty if so are there features that make a particular case study a better candidate for simple approaches to uncertainty quantification here we contrast three case studies with different degrees of data availability and model discrepancy to answer these questions by also drawing on published hydrologic and water quality case studies we argue that ls can be used provided that some criteria are met and that the method is applied with some caution we additionally shed light upon the specific cases in which more sophisticated statistical methods are needed to deliver useful parameter estimates and uncertainty intervals 2 study areas and models we investigate the suitability of ls calibration and subsequent uncertainty propagation in two catchments that differ substantially in terms of geographic domain availability of calibration data quality of input data type and complexity of hydrological model variables predicted and types of systematic errors for one catchment we consider two cases one with low systematic deviations between model results and output data and one with high bias induced by forcing the model with less accurate input estimates in each of the three cases we split the recorded time series into a calibration period where output data are used for parameter estimation and a validation period where output data are used to corroborate model predictive abilities 2 1 case studies 1 cs1 and cs1 watershed with limited data cs1 and cs1 involve the same small partially combined sewer network located in adliswil zurich canton switzerland fig s1 the watershed has an area of 28 6 ha only a fraction of which contributes to the sewer outflow the effective contributing area of the watershed is indeed a calibration parameter see below the area is characterized by medium density residential development and a slope of about 8 7 the site was monitored in 2013 to quantify the occurrence of sewer overflows and to understand the impact of the location of precipitation measurements on discharge predictions for calibration we use a discharge q l s time series of an event including 97 observations recorded every 4 min fig 1 this calibration period includes two storm events of duration greater than the catchment response time which is on the order of minutes for validation we use a subsequent event that occurred 80 days later and included 179 observations such short time series are typical in hydrological modeling of urban catchments e g freni et al 2009 coutu et al 2012 for cs1 input data were recorded by a pluviometer from the swiss meteorological office 1 1 www hw zh ch hochwasser foto db 20sma pdf located circa 7 5 km northeast of the catchment fig s1 the second version of this case study cs1 uses more accurate input obtained by averaging data from two pluviometers located within the catchment area itself a comparison of the precipitation records from these pluviometers reveals that the precipitation input data used in cs1 has substantial systematic errors fig s3 the time series of sewer runoff at the outlet of the catchment is modeled using a lumped linear reservoir model with a harmonic function describing the wastewater oscillations see del giudice et al 2016 for further details about the catchment and the model in this investigation we calibrate the three model parameters related to rainfall runoff namely a m2 the area contributing to the stormwater outflow k hr the mean residence time in the virtual reservoir representing the catchment and x g w l s the baseflow 2 2 case study 2 cs2 watershed with abundant data cs2 is the river raisin basin which has an area of 2784 km2 and is primarily rural 72 and forested 16 the variables of interest are river discharge q m3 s and soluble reactive phosphorus load s r p kg d the calibration period contains 1095 discharge observations and 1095 s r p load observations at daily resolution fig 3 this calibration period includes numerous storm events of duration longer than the catchment response time which is on the order of days the validation period immediately follows the calibration period and includes 366 discharge observations and 335 s r p load observations the watershed dynamics are simulated using the soil and water assessment tool swat arnold et al 1998 swat is a hydrologic transport model that operates at catchment scale it is both more complex due to its more explicit representation of spatial heterogeneity and watershed processes and more computationally demanding than the simple reservoir model used in cs1 and cs1 the river raisin basin and model are well studied in the context of furthering the understanding of the dynamics of nutrient loading from agricultural areas bosch et al 2011 the swat model used here includes all the same process parameterizations inputs and management details as in muenich et al 2017 the model is driven by daily precipitation and temperature observations from nine noaa ghcn land surface stations menne et al 2012 most of which are located within the catchment area fig s2 daily discharge and s r p observations used for calibration and validation are obtained from heidelberg university ncwqr 2015 in the current application we calibrate three model parameters c n 2 the runoff curve number for moisture condition ii s m t m p c the snow melt base temperature and p h o s k d the phosphorus soil partitioning coefficient these parameters are selected because they are primary controls on three key processes namely rainfall runoff snowmelt and biochemical reaction and the output variables of interest are sensitive to them 3 methods 3 1 simple method frequentist least squares ls the least squares method ls is a classic statistical approach for calibrating model parameters estimating model output errors and thus producing prediction intervals wooldridge 2015 ls is generally adopted as the basic technique against which new methods for uncertainty quantification are tested sorooshian and dracup 1980 schoups and vrugt 2010 renard et al 2011 honti et al 2013 del giudice et al 2016 the simplest application of ls is within a frequentist framework in which model parameters are assumed to have one true yet unknown value consequently model parameters are estimated by minimizing an objective function and neither prior nor posterior model parameter uncertainties are explicitly considered because model residuals in hydrology are typically heteroskedastic and non normal wang et al 2012 del giudice et al 2013 here we apply ls after having transformed the observed y o and modeled y output using a non linear monotonic function g see supporting material the objective function used for calibration is the sum of the squares of the errors 1 s s e n y o y 2 where tilde represents the transformed output and n is the number of data points in the calibration dataset i e the length of y o a vector possibly including multiple outputs numerically we use an adaptive markov chain monte carlo algorithm as in del giudice et al 2013 to find the parameter set that minimizes s s e after model calibration model predictions for the validation period are obtained by running the model with the best estimates of the parameters and 95 uncertainty intervals a k a prediction intervals for new observations are approximated as plus minus two times the root mean squared error observed during the calibration period wooldridge 2015 the upper and lower 95 uncertainty intervals for each output k here being q or s r p are then back transformed using a function g 1 to obtain the 95 interquantile range i q r in the original space 2 i q r 95 k g 1 y v a l k 2 1 n k n k y o k y k 2 where y v a l k represents the transformed model output of type k in the validation period and n k is the number of data points in the calibration dataset for the output of type k 3 2 complex method bayesian autoregressive error model arem as a prototypical example of a more complex statistical technique we here choose a bayesian approach that represents model discrepancy or bias as a gaussian process we call this method autoregressive error model arem the key difference between arem and ls consists in the consideration of output error autocorrelation arem derives from applied statistics kennedy and o hagan 2001 bayarri et al 2007 reichert and schuwirth 2012 and here we adopt an implementation that was recently proposed in the hydrological literature del giudice et al 2013 the approach considers the autocorrelation in residuals resulting from model structural deficits linked to oversimplifications in the process description and insufficient spatial discretization reichert and mieleitner 2009 and input errors due to imperfect estimation of time varying inputs such as precipitation del giudice et al 2016 as discussed earlier the concept of stochastically describing error autocorrelation has helped improve parameter estimation and model predictions in several hydrological contexts sorooshian and dracup 1980 yang et al 2007a schoups and vrugt 2010 del giudice et al 2015a sikorska et al 2015 besides its popularity arem has the advantage of realistically describing predictive uncertainty even in the presence of model bias while still being applicable in conjunction with computationally expensive aquatic models del giudice et al 2015b dietzel and reichert 2014 model calibration involves characterizing the posterior distribution of both hydrological model and error model parameters 3 f θ y o f θ f y o θ f θ f y o θ d θ where f θ represents the prior parameter distribution and f y o θ is the likelihood function that represents the errors as a sum of white noise and a gauss markov process the gauss markov process is the continuous time equivalent of an autoregressive process of order one a parametrization used in other applications of arem e g evin et al 2014 the likelihood function embodies assumptions about the output distribution and in a bayesian framework it enables us to extract information from the data y o about parameters θ of the deterministic model and of the error model for arem the likelihood function f y o θ is gaussian and centered at the deterministic model output y θ in a transformed space del giudice et al 2015b sikorska et al 2015 4 f y o θ 2 π n det σ θ exp 1 2 y o y θ t σ θ 1 y o y θ n i 1 d g d y y o i where g represents the transformation of the output of length n while in cs1 there is only one output q in cs2 there are two outputs q and s r p in the latter case the joint likelihood function is given by 5 f y o q y o s r p θ f y o q θ f y o s r p θ the covariance σ is a square matrix of order n 6 σ θ i j σ b 2 exp t i t j τ δ i j σ e 2 where σ b 2 and σ e 2 are parameters representing the variance of the markov bias process and of the white noise process respectively τ is the correlation length of the markov bias process i and j are subscripts spanning over the calibration time domain and δ represents the kronecker delta while the markov bias process describes the autocorrelated output errors deriving form a combination of time dependent input errors and model structural deficits the white noise process describes the uncorrelated output errors note that in cs2 each output variable has a different σ e and σ b table 1 viewed from a bayesian perspective the calibration framework in sec 3 1 is equivalent to maximizing 4 with noninformative priors f θ 1 and diagonal covariance σ θ i j δ i j σ e 2 borsuk et al 2002 therefore model calibration is conducted iteratively using the same adaptive markov chain monte carlo algorithm as in section 3 1 with the difference that here the full representation of the posterior distribution is of interest rather than just its mode model predictions are obtained by propagating a large sample from the posterior parameter distribution through the hydrological model and the error model eqs 26 and 27 in del giudice et al 2016 the 95 uncertainty intervals for new observations are derived by empirically calculating the 2 5th and 97 5th quantiles at each time point and then transforming those back to the real space via the function g 1 3 3 prior distribution of hydrological model and error model parameters as a bayesian method arem can accommodate prior knowledge about hydrological model and error model parameters this prior information is typically based on experience with similar models and datasets as those being investigated following del giudice et al 2016 for cs1 and cs1 we define the priors of model parameters as lognormal distributions for cs2 we instead select normal distributions centered at default swat values and truncated at values suggested in previous investigations muenich et al 2017 we use lognormal prior distributions for the error model parameters σ e and τ and a truncated normal distribution for σ b as suggested by del giudice et al 2015b the latter has zero both as the mean before truncation and the lower bound indicating the preference for having most of the variability in the data explained by the model rather than by the bias process more details on prior parameters are given in table 1 and prior distributions are shown in figs 1 and 3 4 results 4 1 model calibration for cs1 we observe that the model fits the calibration data substantially better when parameters are optimized using ls rather than arem fig 1 this is evident by observing the red line which is closer to the peak discharge observations and the higher ns which indicates the ability of the ls calibrated model to match high value output data more closely the difference in predictive performances between the methods is attributable primarily to differences in the calibrated parameter estimates between ls and arem for cs1 a substantial difference is evident between the parameter estimates obtained with the two methods fig 1 arem which incorporates existing knowledge on model parameters yields posterior distributions similar to the priors the only parameter that is substantially informed by the calibration process is σ b q which represents the amount of bias identified ls parameter estimates which are not informed by priors are instead dramatically different from both the priors and the arem estimates interestingly ls and arem perform almost identically for the modified version of the case study cs1 wherein the precipitation input data with systematic errors is replaced with more accurate data the model in cs1 exhibits low bias as demonstrated by visual inspection high nash sutcliffe efficiency ns relatively small standard deviation of the bias term σ b q σ e q fig 2 and low residual autocorrelation fig s5 for cs2 the two approaches yield almost identical model fits showing higher accuracy for discharge than for s r p fig 3 interestingly even though ls does not make use of prior information it infers hydrologic parameters comparable to the prior estimates overall ls involved 10 3 model simulations whereas arem required 10 4 model runs this represents a substantial difference in the calibration cost between the two methods the higher computational cost associated with arem is attributable to the need to obtain a representative sample of the full posterior distribution and to the presence of more error model parameters to estimate 4 2 model predictions for the validation period the predictive performance of each method is assessed by comparing the median and the 95 uncertainty intervals for a validation period with an independent dataset of measured outputs besides relying on visual assessment as in previous studies e g reichert and mieleitner 2009 sikorska et al 2015 del giudice et al 2016 we use the nash sutcliffe efficiency index to measure predictive accuracy and the percentage of data falling within the 95 prediction intervals to measure the reliability of the uncertainty bands reliability increasing the closer to 95 the actual coverage of validation data is and precision increasing with narrower uncertainty bands of the 95 prediction intervals are also simultaneously assessed by the negative of the interval skill score supporting material the better the quality of the predictions the closer to 0 this statistic is the value of these metrics for our experiments are given in table 2 while in the calibration period we observe that ls produces more accurate predictions than arem for cs1 the situation is reversed during the validation period the limited calibration data and high systematic precipitation errors lead ls to overfit observations and converge on erroneous parameter estimates while arem minimizes this undesirable effect besides being more accurate predictions with arem are also more reliable in that the uncertainty bounds are more representative of the true uncertainty fig 4 when reducing the systematic precipitation errors in cs1 i e cs1 ls again yields results on par with arem fig 5 in other words even with limited calibration data ls performs well as long as systematic input errors are minimal this is true both in terms of prediction accuracy and reliability however arem uncertainty intervals account for parameter uncertainty making them wider during high flows and thus potentially making model predictions even more reliable especially in those crucial periods in cs2 the situation is different from cs1 but similar to cs1 as for the calibration phase model performance during validation is comparable for the two methods with arem having slightly higher predictive accuracy predictive uncertainty is also very similar for the two methods with ls having slightly more reliable and precise confidence intervals fig 6 5 discussion and conclusions 5 1 factors favoring the application of least squares calibration the results of the three case studies presented here are consistent with published studies where ls was implemented for prediction during a validation period table 3 or only during a calibration period e g borsuk et al 2002 vrugt et al 2003 freni et al 2009 forrest et al 2011 wang et al 2012 jiang et al 2015 these studies span a variety of landscapes aquatic systems and models ranging from conceptual lumped models to physically based semi distributed models ls is implemented in a variety of ways across the published studies for instance by assuming the residuals to have a constant variance e g reichert and schuwirth 2012 a constant variance after variable transformation e g dotto et al 2011 or a variance linearly dependent on the output magnitude e g westra et al 2014 a consistent assumption however is that the output errors during the calibration period are independent in time and normally distributed wooldridge 2015 the case studies examined here together with those published previously point to two key characteristics that favor the use of simple approaches such as ls both of these characteristics are indicative of high information content in the training dataset making it possible to estimate parameters of both the physical model and the error model in a manner that will likely yield accurate and reliable predictions kavetski et al 2011 razavi and tolson 2013 westra et al 2014 beven and smith 2015 5 1 1 long calibration time series in the case studies using long calibration time series e g cs2 table 3 borsuk et al 2002 vrugt et al 2003 wang et al 2012 jiang et al 2015 model predictions obtained with ls are usually accurate referring to the median and reliable referring to the uncertainty in the context of watershed modeling long calibration periods refer to periods that include numerous runoff events and span a variety of hydrologic conditions typically over the course of multiple months or even years razavi and tolson 2013 it has previously been shown that ls in conjunction with a long calibration period can produce robust estimates of model parameters and thus accurate predictions e g yapo et al 1996 bosch et al 2011 razavi and tolson 2013 while these earlier studies had not investigated the ability of ls to produce reliable uncertainty intervals they did show that long calibration datasets make it more likely that a variety of environmental conditions will be captured by the calibration data which is critical for calibrating model parameters additionally long calibration time series reduce the impact of any periods that may be less representative which have also been referred to as misleading short periods razavi and tolson 2013 or disinformative events beven and smith 2015 these considerations are also consistent with honti et al 2013 who having calibrated a hydrological model over a period of almost eight years found that even in presence of model bias predictive accuracy with ls and arem was equally high in terms of predictive uncertainty having longer calibration datasets also makes it possible to better assess the variance of the predictive errors kavetski et al 2011 wooldridge 2015 overall this finding also highlights the importance of long term monitoring programs for improved hydrologic predictions such as the national center for water quality research heidelberg university ncwqr 2015 from which this study benefited 5 1 2 low systematic model errors during calibration period it is difficult to set a quantitative threshold for low model bias and several studies therefore mainly discuss the visual observation that model results and calibration data show minimal systematic discrepancies e g bayarri et al 2007 reichert and schuwirth 2012 however in addition to visual inspection we also consider model bias to be low when ns is close to 1 gupta et al 2009 the identified bias is within the range of the uncorrelated output errors σ b q σ e q del giudice et al 2016 and residuals show negligible autocorrelation yang et al 2007b in general as shown when comparing cs1 and cs1 and the two cases from reichert and schuwirth 2012 in table 3 ls performs better when model bias due to systematic errors in input data or to structural deficits in the hydrologic water quality model itself is low this is true both in terms of prediction accuracy and reliability the lack of systematic errors can in fact compensate for short calibration time series this effect is linked to the fact that when model bias is negligible ls is meant to estimate unbiased model parameters even with a small sample size wooldridge 2015 i e a short calibration period however because surface hydrology and pollutant transport models can exhibit substantial systematic deviations from monitoring data over the short calibration periods available much attention has been devoted to identifying representative time series that can be used for more robust parameter estimation for instance razavi and tolson 2013 showed that a short yet representative calibration period can produce ls calibration results as useful for predictions as those produced using long calibration periods for short representative periods it is important that the model show low output bias and that the input data include sufficient variability to identify parameters representative of both high and low flow conditions this is the reason for which in rainfall runoff modeling it is usually recommended that calibration be performed during wet conditions when the dynamic inputs vary sufficiently to enable appropriate parameter sensitivity and identification yapo et al 1996 additionally as discussed by beven and smith 2015 it is best to avoid periods where the relationship between input and output data is unusual or inconsistent with typical conditions as this may be indicative of a period with substantial input errors 5 1 3 caveats overall we find that in the presence of long calibration time series or even with short time series with low systematic input and model errors ls calibration is likely to lead to accurate and reliable predictions at the same time we acknowledge that even when one of these conditions is fulfilled there might be particular situations that require caution for instance arem can provide more reliable predictions than ls in the special case of wanting to quantify the uncertainties at aggregated scales after model calibration has been performed as finer scales evin et al 2014 the reason is that assessing uncertainty at aggregated scales required a quantification of error covariances in addition to variances also arem can provide more accurate and precise predictions in short term forecasting mode del giudice et al 2015a when such periods are on the order of the autocorrelation timescale of the prediction errors because it considers residual autocorrelation evin et al 2014 finally we emphasize the importance of considering error heteroscedasticity by implementing ls calibration as a weighted least squares wls approach kavetski et al 2011 wooldridge 2015 while for some aquatic systems the error variance may not depend substantially on the magnitude of the output variable e g borsuk et al 2002 reichert and schuwirth 2012 jiang et al 2015 most studies focusing on streamflow predictions do report errors that vary with output magnitude sorooshian and dracup 1980 honti et al 2013 sikorska et al 2015 weighted ls is still a very simple approach and can be implemented either by using a data transformation as done here and elsewhere wang et al 2012 del giudice et al 2013 or a linear heteroscedastic model evin et al 2014 westra et al 2014 several studies underscoring the importance of using more sophisticated error models have actually done so by comparing the performances of ordinary ls against those of approaches that simultaneously account for autocorrelation and heteroscedasticity e g schoups and vrugt 2010 del giudice et al 2015a in these cases it is therefore not clear whether the suboptimal predictions based on ls were simply due to a lack of weighting of the output indeed other studies have discussed how results of ordinary ls can be substantially improved when heteroscedasticity is appropriately accounted for e g kavetski et al 2011 5 2 summary and outlook in this study we have analyzed the usefulness of least squares calibration for obtaining accurate and reliable predictions of hydrologic time series the need for such an analysis has recently arisen in response to investigations suggesting that more sophisticated methods are required for uq we have therefore addressed this gap by performing calibration and validation on three case studies and reviewing numerous published studies on the topic we find that ls produces satisfactory predictions in cases where the calibration period is either long e g includes numerous runoff events or short but displays low systematic errors provided that heteroscedasticity is appropriately taken into account under these circumstaces a more complex methods such as arem provide results that are very similar to ls at the same time we have also shown that arem can be helpful when the model is biased and calibration time series are short we acknowledge that for particular applications such as when the goal is to disentangle and reduce the sources of total predictive uncertainty stochastic methods of even higher complexity than arem e g reichert and mieleitner 2009 renard et al 2011 del giudice et al 2016 may be required however if the focus is on accurate and reliable predictions rather than on apportioning total uncertainty we find that ls can perform well the guidelines provided here can be particularly beneficial when considered prior to conducting model calibration and uncertainty propagation besides providing useful guidance for uncertainty analysis in a statistical framework we hope our study will foster further research focusing on i the formulation of quantitative metrics for defining a priori the information content and representativeness of a calibration period ii understanding the role of prior information about model parameters residual heteroscedasticity and residual autocorrelation and iii further understanding how methods of different complexity perform in the presence of non stationary errors acknowledgements the data used are available upon request from ddelgiu ncsu edu this material is based upon work supported by the national science foundation under grant no 1313897 additional funding for m kalcic was provided by epa under grant no gl 00e0461 0 we are grateful to carlo albert for his thoughts on the initial part of this work and wolfgang nowak jasper a vrugt mary c hill and an anonymous reviewer for their feedback on the manuscript discharge and loading data for the river raisin is available from the heidelberg university national center for water quality research weather data for the same watershed were obtained from the national oceanic and atmospheric association global historical climatology network data for the adliswil catchment were from del giudice et al 2016 precipitation data for cs1 were obtained from meteoswiss appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 009 
26390,sophisticated methods for uncertainty quantification have been proposed for overcoming the pitfalls of simple statistical inference in hydrology the implementation of such methods is conceptually and computationally challenging however especially for large scale models here we explore whether there are circumstances in which simple approaches such as least squares produce comparably accurate and reliable predictions we do so using three case studies with two involving a small sewer catchment with limited calibration data and one an agricultural river basin with rich calibration data we also review additional published case studies we find that least squares performs similarly to more sophisticated approaches such as a bayesian autoregressive error model in terms of both accuracy and reliability if calibration periods are long or if the input data and the model have minimal bias overall we find that when mindfully applied simple statistical methods such as least squares can still be useful for uncertainty quantification keywords uncertainty assessment mechanistic modeling surface hydrology water quality least squares statistical inference 1 introduction statistical model calibration and uncertainty quantification uq have recently received substantial attention in surface hydrology and water quality research several studies have stressed the importance of more realistically describing the behavior of calibration errors a k a residuals and thus moving beyond least squares ls calibration assumption of independent and normally distributed residuals e g reichert and mieleitner 2009 renard et al 2011 honti et al 2013 in particular it has been suggested that by using error models that explicitly consider the heteroscedasticity i e non constant variance and autocorrelation of the calibration errors parameter estimation and subsequent predictive uncertainty assessment can be improved in a relatively straightforward manner sorooshian and dracup 1980 yang et al 2007b schoups and vrugt 2010 del giudice et al 2015a however while such sophisticated approaches have been shown to be helpful in the specific situations where they were tested this does not necessarily imply that simpler statistical techniques such as least squares calibration are never useful therefore there is a critical knowledge gap in hydrologic and environmental modeling regarding when simple calibration approaches are acceptable versus when more sophisticated ones are needed understanding the domain of applicability of simple least squares along with its limitations is essential indeed we argue that the presupposition that statistical inference always has to be conducted with conceptually and computationally burdensome methods might be inducing modelers to eschew uq altogether e g bosch et al 2011 coutu et al 2012 razavi and tolson 2013 or use pseudo statistical methods with unclear probabilistic interpretation e g freni et al 2009 beven and smith 2015 in the context of predictive uq we therefore address an important yet so far unanswered question are there cases in which a simple method such as least squares yields predictions with precision and accuracy that are on par with state of the art approaches that account for error autocorrelation and parameter uncertainty if so are there features that make a particular case study a better candidate for simple approaches to uncertainty quantification here we contrast three case studies with different degrees of data availability and model discrepancy to answer these questions by also drawing on published hydrologic and water quality case studies we argue that ls can be used provided that some criteria are met and that the method is applied with some caution we additionally shed light upon the specific cases in which more sophisticated statistical methods are needed to deliver useful parameter estimates and uncertainty intervals 2 study areas and models we investigate the suitability of ls calibration and subsequent uncertainty propagation in two catchments that differ substantially in terms of geographic domain availability of calibration data quality of input data type and complexity of hydrological model variables predicted and types of systematic errors for one catchment we consider two cases one with low systematic deviations between model results and output data and one with high bias induced by forcing the model with less accurate input estimates in each of the three cases we split the recorded time series into a calibration period where output data are used for parameter estimation and a validation period where output data are used to corroborate model predictive abilities 2 1 case studies 1 cs1 and cs1 watershed with limited data cs1 and cs1 involve the same small partially combined sewer network located in adliswil zurich canton switzerland fig s1 the watershed has an area of 28 6 ha only a fraction of which contributes to the sewer outflow the effective contributing area of the watershed is indeed a calibration parameter see below the area is characterized by medium density residential development and a slope of about 8 7 the site was monitored in 2013 to quantify the occurrence of sewer overflows and to understand the impact of the location of precipitation measurements on discharge predictions for calibration we use a discharge q l s time series of an event including 97 observations recorded every 4 min fig 1 this calibration period includes two storm events of duration greater than the catchment response time which is on the order of minutes for validation we use a subsequent event that occurred 80 days later and included 179 observations such short time series are typical in hydrological modeling of urban catchments e g freni et al 2009 coutu et al 2012 for cs1 input data were recorded by a pluviometer from the swiss meteorological office 1 1 www hw zh ch hochwasser foto db 20sma pdf located circa 7 5 km northeast of the catchment fig s1 the second version of this case study cs1 uses more accurate input obtained by averaging data from two pluviometers located within the catchment area itself a comparison of the precipitation records from these pluviometers reveals that the precipitation input data used in cs1 has substantial systematic errors fig s3 the time series of sewer runoff at the outlet of the catchment is modeled using a lumped linear reservoir model with a harmonic function describing the wastewater oscillations see del giudice et al 2016 for further details about the catchment and the model in this investigation we calibrate the three model parameters related to rainfall runoff namely a m2 the area contributing to the stormwater outflow k hr the mean residence time in the virtual reservoir representing the catchment and x g w l s the baseflow 2 2 case study 2 cs2 watershed with abundant data cs2 is the river raisin basin which has an area of 2784 km2 and is primarily rural 72 and forested 16 the variables of interest are river discharge q m3 s and soluble reactive phosphorus load s r p kg d the calibration period contains 1095 discharge observations and 1095 s r p load observations at daily resolution fig 3 this calibration period includes numerous storm events of duration longer than the catchment response time which is on the order of days the validation period immediately follows the calibration period and includes 366 discharge observations and 335 s r p load observations the watershed dynamics are simulated using the soil and water assessment tool swat arnold et al 1998 swat is a hydrologic transport model that operates at catchment scale it is both more complex due to its more explicit representation of spatial heterogeneity and watershed processes and more computationally demanding than the simple reservoir model used in cs1 and cs1 the river raisin basin and model are well studied in the context of furthering the understanding of the dynamics of nutrient loading from agricultural areas bosch et al 2011 the swat model used here includes all the same process parameterizations inputs and management details as in muenich et al 2017 the model is driven by daily precipitation and temperature observations from nine noaa ghcn land surface stations menne et al 2012 most of which are located within the catchment area fig s2 daily discharge and s r p observations used for calibration and validation are obtained from heidelberg university ncwqr 2015 in the current application we calibrate three model parameters c n 2 the runoff curve number for moisture condition ii s m t m p c the snow melt base temperature and p h o s k d the phosphorus soil partitioning coefficient these parameters are selected because they are primary controls on three key processes namely rainfall runoff snowmelt and biochemical reaction and the output variables of interest are sensitive to them 3 methods 3 1 simple method frequentist least squares ls the least squares method ls is a classic statistical approach for calibrating model parameters estimating model output errors and thus producing prediction intervals wooldridge 2015 ls is generally adopted as the basic technique against which new methods for uncertainty quantification are tested sorooshian and dracup 1980 schoups and vrugt 2010 renard et al 2011 honti et al 2013 del giudice et al 2016 the simplest application of ls is within a frequentist framework in which model parameters are assumed to have one true yet unknown value consequently model parameters are estimated by minimizing an objective function and neither prior nor posterior model parameter uncertainties are explicitly considered because model residuals in hydrology are typically heteroskedastic and non normal wang et al 2012 del giudice et al 2013 here we apply ls after having transformed the observed y o and modeled y output using a non linear monotonic function g see supporting material the objective function used for calibration is the sum of the squares of the errors 1 s s e n y o y 2 where tilde represents the transformed output and n is the number of data points in the calibration dataset i e the length of y o a vector possibly including multiple outputs numerically we use an adaptive markov chain monte carlo algorithm as in del giudice et al 2013 to find the parameter set that minimizes s s e after model calibration model predictions for the validation period are obtained by running the model with the best estimates of the parameters and 95 uncertainty intervals a k a prediction intervals for new observations are approximated as plus minus two times the root mean squared error observed during the calibration period wooldridge 2015 the upper and lower 95 uncertainty intervals for each output k here being q or s r p are then back transformed using a function g 1 to obtain the 95 interquantile range i q r in the original space 2 i q r 95 k g 1 y v a l k 2 1 n k n k y o k y k 2 where y v a l k represents the transformed model output of type k in the validation period and n k is the number of data points in the calibration dataset for the output of type k 3 2 complex method bayesian autoregressive error model arem as a prototypical example of a more complex statistical technique we here choose a bayesian approach that represents model discrepancy or bias as a gaussian process we call this method autoregressive error model arem the key difference between arem and ls consists in the consideration of output error autocorrelation arem derives from applied statistics kennedy and o hagan 2001 bayarri et al 2007 reichert and schuwirth 2012 and here we adopt an implementation that was recently proposed in the hydrological literature del giudice et al 2013 the approach considers the autocorrelation in residuals resulting from model structural deficits linked to oversimplifications in the process description and insufficient spatial discretization reichert and mieleitner 2009 and input errors due to imperfect estimation of time varying inputs such as precipitation del giudice et al 2016 as discussed earlier the concept of stochastically describing error autocorrelation has helped improve parameter estimation and model predictions in several hydrological contexts sorooshian and dracup 1980 yang et al 2007a schoups and vrugt 2010 del giudice et al 2015a sikorska et al 2015 besides its popularity arem has the advantage of realistically describing predictive uncertainty even in the presence of model bias while still being applicable in conjunction with computationally expensive aquatic models del giudice et al 2015b dietzel and reichert 2014 model calibration involves characterizing the posterior distribution of both hydrological model and error model parameters 3 f θ y o f θ f y o θ f θ f y o θ d θ where f θ represents the prior parameter distribution and f y o θ is the likelihood function that represents the errors as a sum of white noise and a gauss markov process the gauss markov process is the continuous time equivalent of an autoregressive process of order one a parametrization used in other applications of arem e g evin et al 2014 the likelihood function embodies assumptions about the output distribution and in a bayesian framework it enables us to extract information from the data y o about parameters θ of the deterministic model and of the error model for arem the likelihood function f y o θ is gaussian and centered at the deterministic model output y θ in a transformed space del giudice et al 2015b sikorska et al 2015 4 f y o θ 2 π n det σ θ exp 1 2 y o y θ t σ θ 1 y o y θ n i 1 d g d y y o i where g represents the transformation of the output of length n while in cs1 there is only one output q in cs2 there are two outputs q and s r p in the latter case the joint likelihood function is given by 5 f y o q y o s r p θ f y o q θ f y o s r p θ the covariance σ is a square matrix of order n 6 σ θ i j σ b 2 exp t i t j τ δ i j σ e 2 where σ b 2 and σ e 2 are parameters representing the variance of the markov bias process and of the white noise process respectively τ is the correlation length of the markov bias process i and j are subscripts spanning over the calibration time domain and δ represents the kronecker delta while the markov bias process describes the autocorrelated output errors deriving form a combination of time dependent input errors and model structural deficits the white noise process describes the uncorrelated output errors note that in cs2 each output variable has a different σ e and σ b table 1 viewed from a bayesian perspective the calibration framework in sec 3 1 is equivalent to maximizing 4 with noninformative priors f θ 1 and diagonal covariance σ θ i j δ i j σ e 2 borsuk et al 2002 therefore model calibration is conducted iteratively using the same adaptive markov chain monte carlo algorithm as in section 3 1 with the difference that here the full representation of the posterior distribution is of interest rather than just its mode model predictions are obtained by propagating a large sample from the posterior parameter distribution through the hydrological model and the error model eqs 26 and 27 in del giudice et al 2016 the 95 uncertainty intervals for new observations are derived by empirically calculating the 2 5th and 97 5th quantiles at each time point and then transforming those back to the real space via the function g 1 3 3 prior distribution of hydrological model and error model parameters as a bayesian method arem can accommodate prior knowledge about hydrological model and error model parameters this prior information is typically based on experience with similar models and datasets as those being investigated following del giudice et al 2016 for cs1 and cs1 we define the priors of model parameters as lognormal distributions for cs2 we instead select normal distributions centered at default swat values and truncated at values suggested in previous investigations muenich et al 2017 we use lognormal prior distributions for the error model parameters σ e and τ and a truncated normal distribution for σ b as suggested by del giudice et al 2015b the latter has zero both as the mean before truncation and the lower bound indicating the preference for having most of the variability in the data explained by the model rather than by the bias process more details on prior parameters are given in table 1 and prior distributions are shown in figs 1 and 3 4 results 4 1 model calibration for cs1 we observe that the model fits the calibration data substantially better when parameters are optimized using ls rather than arem fig 1 this is evident by observing the red line which is closer to the peak discharge observations and the higher ns which indicates the ability of the ls calibrated model to match high value output data more closely the difference in predictive performances between the methods is attributable primarily to differences in the calibrated parameter estimates between ls and arem for cs1 a substantial difference is evident between the parameter estimates obtained with the two methods fig 1 arem which incorporates existing knowledge on model parameters yields posterior distributions similar to the priors the only parameter that is substantially informed by the calibration process is σ b q which represents the amount of bias identified ls parameter estimates which are not informed by priors are instead dramatically different from both the priors and the arem estimates interestingly ls and arem perform almost identically for the modified version of the case study cs1 wherein the precipitation input data with systematic errors is replaced with more accurate data the model in cs1 exhibits low bias as demonstrated by visual inspection high nash sutcliffe efficiency ns relatively small standard deviation of the bias term σ b q σ e q fig 2 and low residual autocorrelation fig s5 for cs2 the two approaches yield almost identical model fits showing higher accuracy for discharge than for s r p fig 3 interestingly even though ls does not make use of prior information it infers hydrologic parameters comparable to the prior estimates overall ls involved 10 3 model simulations whereas arem required 10 4 model runs this represents a substantial difference in the calibration cost between the two methods the higher computational cost associated with arem is attributable to the need to obtain a representative sample of the full posterior distribution and to the presence of more error model parameters to estimate 4 2 model predictions for the validation period the predictive performance of each method is assessed by comparing the median and the 95 uncertainty intervals for a validation period with an independent dataset of measured outputs besides relying on visual assessment as in previous studies e g reichert and mieleitner 2009 sikorska et al 2015 del giudice et al 2016 we use the nash sutcliffe efficiency index to measure predictive accuracy and the percentage of data falling within the 95 prediction intervals to measure the reliability of the uncertainty bands reliability increasing the closer to 95 the actual coverage of validation data is and precision increasing with narrower uncertainty bands of the 95 prediction intervals are also simultaneously assessed by the negative of the interval skill score supporting material the better the quality of the predictions the closer to 0 this statistic is the value of these metrics for our experiments are given in table 2 while in the calibration period we observe that ls produces more accurate predictions than arem for cs1 the situation is reversed during the validation period the limited calibration data and high systematic precipitation errors lead ls to overfit observations and converge on erroneous parameter estimates while arem minimizes this undesirable effect besides being more accurate predictions with arem are also more reliable in that the uncertainty bounds are more representative of the true uncertainty fig 4 when reducing the systematic precipitation errors in cs1 i e cs1 ls again yields results on par with arem fig 5 in other words even with limited calibration data ls performs well as long as systematic input errors are minimal this is true both in terms of prediction accuracy and reliability however arem uncertainty intervals account for parameter uncertainty making them wider during high flows and thus potentially making model predictions even more reliable especially in those crucial periods in cs2 the situation is different from cs1 but similar to cs1 as for the calibration phase model performance during validation is comparable for the two methods with arem having slightly higher predictive accuracy predictive uncertainty is also very similar for the two methods with ls having slightly more reliable and precise confidence intervals fig 6 5 discussion and conclusions 5 1 factors favoring the application of least squares calibration the results of the three case studies presented here are consistent with published studies where ls was implemented for prediction during a validation period table 3 or only during a calibration period e g borsuk et al 2002 vrugt et al 2003 freni et al 2009 forrest et al 2011 wang et al 2012 jiang et al 2015 these studies span a variety of landscapes aquatic systems and models ranging from conceptual lumped models to physically based semi distributed models ls is implemented in a variety of ways across the published studies for instance by assuming the residuals to have a constant variance e g reichert and schuwirth 2012 a constant variance after variable transformation e g dotto et al 2011 or a variance linearly dependent on the output magnitude e g westra et al 2014 a consistent assumption however is that the output errors during the calibration period are independent in time and normally distributed wooldridge 2015 the case studies examined here together with those published previously point to two key characteristics that favor the use of simple approaches such as ls both of these characteristics are indicative of high information content in the training dataset making it possible to estimate parameters of both the physical model and the error model in a manner that will likely yield accurate and reliable predictions kavetski et al 2011 razavi and tolson 2013 westra et al 2014 beven and smith 2015 5 1 1 long calibration time series in the case studies using long calibration time series e g cs2 table 3 borsuk et al 2002 vrugt et al 2003 wang et al 2012 jiang et al 2015 model predictions obtained with ls are usually accurate referring to the median and reliable referring to the uncertainty in the context of watershed modeling long calibration periods refer to periods that include numerous runoff events and span a variety of hydrologic conditions typically over the course of multiple months or even years razavi and tolson 2013 it has previously been shown that ls in conjunction with a long calibration period can produce robust estimates of model parameters and thus accurate predictions e g yapo et al 1996 bosch et al 2011 razavi and tolson 2013 while these earlier studies had not investigated the ability of ls to produce reliable uncertainty intervals they did show that long calibration datasets make it more likely that a variety of environmental conditions will be captured by the calibration data which is critical for calibrating model parameters additionally long calibration time series reduce the impact of any periods that may be less representative which have also been referred to as misleading short periods razavi and tolson 2013 or disinformative events beven and smith 2015 these considerations are also consistent with honti et al 2013 who having calibrated a hydrological model over a period of almost eight years found that even in presence of model bias predictive accuracy with ls and arem was equally high in terms of predictive uncertainty having longer calibration datasets also makes it possible to better assess the variance of the predictive errors kavetski et al 2011 wooldridge 2015 overall this finding also highlights the importance of long term monitoring programs for improved hydrologic predictions such as the national center for water quality research heidelberg university ncwqr 2015 from which this study benefited 5 1 2 low systematic model errors during calibration period it is difficult to set a quantitative threshold for low model bias and several studies therefore mainly discuss the visual observation that model results and calibration data show minimal systematic discrepancies e g bayarri et al 2007 reichert and schuwirth 2012 however in addition to visual inspection we also consider model bias to be low when ns is close to 1 gupta et al 2009 the identified bias is within the range of the uncorrelated output errors σ b q σ e q del giudice et al 2016 and residuals show negligible autocorrelation yang et al 2007b in general as shown when comparing cs1 and cs1 and the two cases from reichert and schuwirth 2012 in table 3 ls performs better when model bias due to systematic errors in input data or to structural deficits in the hydrologic water quality model itself is low this is true both in terms of prediction accuracy and reliability the lack of systematic errors can in fact compensate for short calibration time series this effect is linked to the fact that when model bias is negligible ls is meant to estimate unbiased model parameters even with a small sample size wooldridge 2015 i e a short calibration period however because surface hydrology and pollutant transport models can exhibit substantial systematic deviations from monitoring data over the short calibration periods available much attention has been devoted to identifying representative time series that can be used for more robust parameter estimation for instance razavi and tolson 2013 showed that a short yet representative calibration period can produce ls calibration results as useful for predictions as those produced using long calibration periods for short representative periods it is important that the model show low output bias and that the input data include sufficient variability to identify parameters representative of both high and low flow conditions this is the reason for which in rainfall runoff modeling it is usually recommended that calibration be performed during wet conditions when the dynamic inputs vary sufficiently to enable appropriate parameter sensitivity and identification yapo et al 1996 additionally as discussed by beven and smith 2015 it is best to avoid periods where the relationship between input and output data is unusual or inconsistent with typical conditions as this may be indicative of a period with substantial input errors 5 1 3 caveats overall we find that in the presence of long calibration time series or even with short time series with low systematic input and model errors ls calibration is likely to lead to accurate and reliable predictions at the same time we acknowledge that even when one of these conditions is fulfilled there might be particular situations that require caution for instance arem can provide more reliable predictions than ls in the special case of wanting to quantify the uncertainties at aggregated scales after model calibration has been performed as finer scales evin et al 2014 the reason is that assessing uncertainty at aggregated scales required a quantification of error covariances in addition to variances also arem can provide more accurate and precise predictions in short term forecasting mode del giudice et al 2015a when such periods are on the order of the autocorrelation timescale of the prediction errors because it considers residual autocorrelation evin et al 2014 finally we emphasize the importance of considering error heteroscedasticity by implementing ls calibration as a weighted least squares wls approach kavetski et al 2011 wooldridge 2015 while for some aquatic systems the error variance may not depend substantially on the magnitude of the output variable e g borsuk et al 2002 reichert and schuwirth 2012 jiang et al 2015 most studies focusing on streamflow predictions do report errors that vary with output magnitude sorooshian and dracup 1980 honti et al 2013 sikorska et al 2015 weighted ls is still a very simple approach and can be implemented either by using a data transformation as done here and elsewhere wang et al 2012 del giudice et al 2013 or a linear heteroscedastic model evin et al 2014 westra et al 2014 several studies underscoring the importance of using more sophisticated error models have actually done so by comparing the performances of ordinary ls against those of approaches that simultaneously account for autocorrelation and heteroscedasticity e g schoups and vrugt 2010 del giudice et al 2015a in these cases it is therefore not clear whether the suboptimal predictions based on ls were simply due to a lack of weighting of the output indeed other studies have discussed how results of ordinary ls can be substantially improved when heteroscedasticity is appropriately accounted for e g kavetski et al 2011 5 2 summary and outlook in this study we have analyzed the usefulness of least squares calibration for obtaining accurate and reliable predictions of hydrologic time series the need for such an analysis has recently arisen in response to investigations suggesting that more sophisticated methods are required for uq we have therefore addressed this gap by performing calibration and validation on three case studies and reviewing numerous published studies on the topic we find that ls produces satisfactory predictions in cases where the calibration period is either long e g includes numerous runoff events or short but displays low systematic errors provided that heteroscedasticity is appropriately taken into account under these circumstaces a more complex methods such as arem provide results that are very similar to ls at the same time we have also shown that arem can be helpful when the model is biased and calibration time series are short we acknowledge that for particular applications such as when the goal is to disentangle and reduce the sources of total predictive uncertainty stochastic methods of even higher complexity than arem e g reichert and mieleitner 2009 renard et al 2011 del giudice et al 2016 may be required however if the focus is on accurate and reliable predictions rather than on apportioning total uncertainty we find that ls can perform well the guidelines provided here can be particularly beneficial when considered prior to conducting model calibration and uncertainty propagation besides providing useful guidance for uncertainty analysis in a statistical framework we hope our study will foster further research focusing on i the formulation of quantitative metrics for defining a priori the information content and representativeness of a calibration period ii understanding the role of prior information about model parameters residual heteroscedasticity and residual autocorrelation and iii further understanding how methods of different complexity perform in the presence of non stationary errors acknowledgements the data used are available upon request from ddelgiu ncsu edu this material is based upon work supported by the national science foundation under grant no 1313897 additional funding for m kalcic was provided by epa under grant no gl 00e0461 0 we are grateful to carlo albert for his thoughts on the initial part of this work and wolfgang nowak jasper a vrugt mary c hill and an anonymous reviewer for their feedback on the manuscript discharge and loading data for the river raisin is available from the heidelberg university national center for water quality research weather data for the same watershed were obtained from the national oceanic and atmospheric association global historical climatology network data for the adliswil catchment were from del giudice et al 2016 precipitation data for cs1 were obtained from meteoswiss appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 009 
26391,the prediction of fire propagation across landscapes is necessary for safe and effective fire management we analyzed the predictive accuracy of models currently used operationally in australia for predicting fire spread rates in five different fuel types grasslands temperate and semi arid shrublands dry eucalypt and conifer forests compared to their previous counterparts we calculated error statistics and contrasted model predictions against observed spread rates of field observations of wildfires and prescribed fires we then compared the changes in error metrics of older models to newer ones evaluation results show newer models to have improved prediction accuracy mean absolute errors were reduced by 56 68 and 70 in dry eucalypt forests grasslands and crown fires in conifer forests respectively the most significant improvement was the reversion of under prediction bias achieved with newer models this study has highlighted the value of continuous improvement when it comes to developing operational wildland fire spread models keywords crown fire fire behaviour fire propagation fire weather fuel type model error 1 introduction a wildland fire is a free moving combustion reaction spreading across the landscape fig 1 as determined by fuel availability topography and atmospheric variables barrows 1961 under favourable environmental conditions fast spreading high intensity wildfires can be responsible for large burned areas stocks and walker 1973 cheney 1976 alexander and cruz 2016 widespread destruction of life and property harris et al 2012 blanchi et al 2014 alexander et al 2017 and at times undesirable ecological and environmental consequences mobley 1974 free burning wildland fires are characterized by energy release rates spanning several orders of magnitude from 10 kw m 1 an approximate lower limit for sustainable fire propagation upwards to around 100 000 kw m 1 byram 1959 only observable under the most extreme burning conditions anderson 1968 kiil and grigel 1969 cruz et al 2012 within this range direct fire suppression action is only effective in halting a fire in the lower 4 5 portion i e less than 4000 5000 kw m 1 of the spectrum cheney 1991 fires that become large will only stop when a change in fuels weather and or topographic conditions leads to a decrease in its intensity to suppressible levels or via self extinguishment schaefer 1957 underwood et al 1985 salazar and gonzález cabán 1987 the ability to predict the direction and spread rate of a wildland fire and its associated flame front characteristics allows land and fire managers to develop and implement safe and effective suppression strategies and to release timely and effective public warnings and evacuation orders scott et al 2014 cruz et al 2015b mathematical models aimed at describing the behaviour of wildland fires namely the forward or head fire rate of spread arguably the most important fire behaviour characteristic from an operational standpoint alexander 2000 have been under development for the past 80 years scott et al 2014 weise and fons 2014 models have been developed with a range of purposes from improving our theoretical understanding of particular aspects of fire behaviour to specifically addressing an applied research need such as modelling how fast a fire with an idealised flame front will move across the landscape or how tall the flames will be under a given set of fuel weather and topographic conditions albini 1984 a question one might ask when examining the accumulated body of research e g weber 1991 pastor et al 2003 sullivan 2009b is has the capability of fire spread models to accurately predict fire propagation improved in the last couple of decades or alternatively are the newer fire spread models substantially better than the older models at quantifying the effect of the environmental variables controlling the spread rate of wildland fires this line of questioning is especially pertinent to global wildfire science and management in regions of the world where wildfires threaten life and property and fire spread models are used operationally to support wildland fire control and fuel management in the particular case of australia wildland fire behaviour research has been continuously conducted to develop operationally applicable models that would overcome well documented burrows et al 1991 mccaw et al 2008 or perceived cheney and gould 1995 catchpole et al 1998a limitations of existent fire spread models cruz and alexander 2013 conducted a comprehensive survey of studies where rate of fire spread models were evaluated against independent data derived from field observations of wildfires prescribed fires and experimental fires fig 1 using this database comprised of 49 data sets corresponding to 1278 paired observations vs model predictions they were able to characterise the error metrics for several wildland fire rate of spread models they did not however conduct a temporal analysis of how the errors had changed over the years with the evolution of fire behaviour models data recently published by anderson et al 2015 and made available by kilinc et al 2012 have presented an opportunity to assess model performance and look at how newer operational models perform relative to their previous older counterparts in this paper we aim to analyse the predictive performance of a number of fire rate of spread models used operationally in australia and quantify the improvements or lack thereof in model accuracy resulting from ongoing model developments over the past 30 years or so fig 1a and b this was done using commonly accepted error statistics based on an analysis that relied upon independent datasets of wildfire fig 1 c and to a lesser extent prescribed fire observations 2 methods 2 1 model evaluation data we compiled all fire spread model evaluation studies known to us where there was a direct comparison between model predictions and field observations and where the evaluation datasets were independent from the model development process this included the 49 studies used in cruz and alexander 2013 2014 complemented with additional data from kilinc et al 2012 and anderson et al 2015 data origins included wildfires prescribed fires and experimental fires purposely conducted to develop or evaluate fire spread models we then scrutinised the available data pool for studies that allowed for the evaluation of both older and newer fire spread models or provided information from which simulations for models not considered could be undertaken it was assumed that a fire run occurred in a single general fuel type and that the fire had reached a quasi steady state spread condition i e fire was not spreading in its build up phase mcarthur 1968 to the best of our knowledge the selected wildfire runs represented a free burning linear flame front unaffected by fire suppression action and in the case of prescribed fires by their ignition pattern johansen 1987 each evaluation study selected included a minimum of five fire spread rate observations 2 2 models examined an initial examination of the data available revealed five pairs older vs newer of fuel type specific fire spread models identified as suitable for analyses table 1 this included models for four common fuel types found in australia namely grasslands mcarthur 1966 vs cheney et al 1998 temperate shrublands catchpole et al 1998a vs anderson et al 2015 semi arid shrublands mccaw 1997 vs cruz et al 2013 and dry eucalypt forests mcarthur 1967 vs cheney et al 2012 in addition to a pair of models for predicting crown fire spread in north american conifer forests rothermel 1991 vs cruz et al 2005 this latter model has been applied to australia s exotic pine plantations cruz et al 2008 all of the newer or most current models listed by fuel type in table 1 are presently used to predict fire propagation operationally in australia plucinski et al 2017 a more in depth description of these models can be found in cruz et al 2015a b as a benchmark for the analyses carried out on the models referred to in table 1 we also incorporated a similar assessment of studies evaluating the rothermel 1972 semi empirical surface fire spread model this model with additions devised by albini 1976 is globally the most widely used fire spread model either through the behaveplus fire modelling system andrews 2010 2014 or through different implementations in spatially explicit fire spread simulators see sullivan 2009c for a review the rothermel 1972 surface fire spread model consists of a mathematical algorithm developed by coupling physical heat transfer theory and statistical correlations obtained from laboratory fires and to a much lesser extent wildfire observations andrews et al 2013 the fuel inputs include fuel bed load and depth fuel particle density surface area to volume ratio heat content mineral content and the moisture content at which extinction can be expected expressed as either stylized anderson 1982 scott and burgan 2005 or custom burgan 1987 fuel models the environmental inputs are wind speed at mid flame height andrews 2012 the moisture content of fuels and slope steepness the ability of users to customize the fuel description from estimated or measured fuel data explains in part its widespread appeal and use around the world e g van wilgen et al 1985 gould 1991 lopes et al 2002 athanasiou and xanthopoulos 2010 2 3 model evaluation statistics the statistics used to analyse the error in model predictions were the mean absolute error mae the mean bias error mbe the mean absolute percent error mape and the ratio between the mbe and mae which we call the mean bias percent error mbpe willmott 1982 the equations for calculating these four error statistics are given in table 2 residuals were calculated as residual predicted observed a negative residual indicates an under prediction while a positive residual indicates an over prediction residuals were visually tested for normality and paired t test were conducted for each fuel type specific model pair to evaluate the significance of changes in residual error obtained with newer models over their older counterparts we used the statistics published in the original sources where possible in cases where they were not reported we computed them from the data contained in the associated publication or by contacting the author s directly for the raw data 3 results 3 1 operational rate of fire spread models the studies selected for undertaking the paired older vs newer comparisons of model accuracy and error their sample size e g number of independent fire spread periods and range in observed rates of fire spread are summarized in table 3 the datasets included primarily wildfire observations fig 1c prescribed fires were limited to part of the semi arid shrubland fuel type evaluation dataset a relevant aspect of the model evaluation results was that the error statistics for the grassland temperate shrubland dry eucalypt forest and conifer forest datasets were based on evaluation studies where the range in fire behaviour and certain environmental conditions e g fine dead fuel moisture content and wind speed were beyond the range of the data used in model development tables 1 and 3 this was notable for the grassland temperate shrubland and conifer forest datasets where the models were developed with a maximum rate of fire spread of 383 60 and 51 m min 1 respectively whereas the respective maxima in the evaluation datasets were 560 100 and 107 m min 1 a 10 fold difference was found in the dry eucalypt forest model analysis 22 7 compared to 260 m min 1 fig 2 is a scatterplot of model predictions against observed rates of fire spread these plots show a notorious bias to be present in the older models namely an over prediction for the grasslands and under prediction for the remaining fuel types in particular the older dry eucalypt forest and conifer forest crown fire models show a strong under prediction bias the newer models show an improvement over their older counterparts the results show a significant reduction in the bias with the data generally following the line of perfect agreement note the considerable scatter observable in these graphs the level of scatter observed is characteristic of the wildfire data due to uncertainties and natural variability in the temporal and spatial distribution of the model input conditions table 4 presents the residual mean and standard deviation for the models examined the results show the newer models reduce the magnitude of residual error paired t test of residuals showed this reduction to be significant p 0 001 for all fuel types fig 3 summarises the change in the mae mbe mape and mbpe for the various fire spread model combinations considered table a1 the results show substantial reductions in the mae for all model combinations except for the semi arid shrublands fig 3a considering the changes in the mae statistic using the newer model results as the benchmark i e change in error older model error newer model error newer model error 100 the most notable reductions were obtained for grasslands mcarthur 1966 vs cheney et al 1998 with a 68 reduction in error and crowning wildfires in conifer forests rothermel 1991 vs cruz et al 2005 with a 70 reduction in error a 56 and 52 reduction in mae was also observed in temperate shrublands catchpole et al 1998a vs anderson et al 2015 and dry eucalypt forests mcarthur 1967 vs cheney et al 2012 respectively a minor increase in mae was noted for the semi arid shrublands mccaw 1997 vs cruz et al 2013 from 18 2 to 18 7 m min 1 i e a 3 increase more important than the changes in mae is the reversion of the error type as expressed by the mbe fig 3b four out of the five older models had a preponderant under prediction bias an error type of potentially disastrous consequences in wildland fire behaviour prediction cheney 1981 the newer models had a reduced bias with three of the models having a positive bias error i e 15 3 1 and 6 6 m min 1 for grasslands semi arid shrublands and conifer forest crown fires respectively the other two models had a relatively small under prediction bias i e 1 5 and 0 7 m min 1 for temperate shrublands and dry eucalypt forests respectively the changes in mape requires careful interpretation fig 3c does not show a clear reduction in mape with the newer models this is because the mape is constrained to 100 for under predictions but is open ended for over predictions as such mape changes should only be compared for unbiased models or for models showing the same under prediction or over prediction trend this limitation of the mape is observed for the dry eucalypt forest and conifer forest evaluations where under prediction trends for the older models result in the bias to be 97 and 100 of the mean error respectively fig 3d although the newer models show a notable reduction in the mae and mbe the mape shows only a moderate improvement for the conifer forest fuel type and an undesirable increase for the dry eucalypt forest fuel type for the results presented in fig 3c only the grasslands had a decreased mape i e from 124 to 80 the analysis of changes in mbpe the bias expressed as a fraction of the mean error further highlighted the changes in error type most of the older models had a large under prediction bias fig 3d for the temperate shrubland dry eucalypt forest and conifer forest datasets this bias was close or equal to the mae indicating an very high proportion of simulations with an under prediction error the bias error of newer models was within 25 of the mean error for four of the five independent datasets the largest mpbe i e 44 for these models was found for the cruz et al 2005 model evaluation against the conifer forest dataset for crowning wildfires given in alexander and cruz 2006 3 2 rothermel 1972 surface fire spread model we found the rothermel 1972 model to be the most widely evaluated fire spread model in use around the world having been evaluated against 29 independent datasets out of a total of 49 cruz and alexander 2013 the model was evaluated over a reasonably wide range of fuel weather and topographic conditions table 5 the data available to evaluate the rothermel 1972 model were different to the datasets table 3 associated with the evaluation of the models listed in table 1 of the 29 published studies evaluating the rothermel 1972 model 18 were based entirely on experimental fires 8 relied on prescribed fires two used wildfire data and one remaining study involved a mix of both experimental fires and wildfires table 5 overall the model produced the best predictions for grassland and shrubland fuel types both with mape values of 49 and a residual bias of 7 and 14 of the mean error table 6 the lowest mae values 1 6 and 4 0 m min 1 were found for the forest understory and logging slash fuel types respectively table 6 but these errors in turn corresponded to the largest mape 76 and 61 and mbpe 56 and 79 values respectively this apparent contradiction results from the magnitude of the mae being dependent on the range of the model evaluation dataset the range of observed rate of fire spread values in the forest understory and logging slash fuel types were substantially lower 20 and 38 m min 1 respectively than found for the grasslands and shrublands fuel types 160 and 89 m min 1 respectively 4 discussion and conclusions the results of the evaluation of the fire spread rate models used operationally in australia showed a consistent improvement in prediction accuracy of newer models in comparison to their older counterparts mean errors from newer models were substantially reduced when compared with the older models and under prediction biases were eliminated in four of the five model pairs tested we believe the improvement in the predictive accuracy of the newer models is not fortuitous but the result of more robust experimental designs model development datasets covering wider range in fire behaviour and regression analysis based on functional forms that are known to represent the bulk effect of the chosen environment variables influencing fire spread rates rather than a form driven solely by statistical fitting cruz et al 2017 the improvement in model accuracy is especially relevant to the australian fire environment where the periodic occurrence of severe burning conditions causes high impact bushfires with significant loss of life and property cruz et al 2012 harris et al 2012 blanchi et al 2014 nevertheless it is noteworthy that there is still a substantial error associated with the model predictions with mape values varying between 33 in temperate shrublands to 122 in dry eucalypt forests these models tend to show mape values between 30 and 50 when evaluated against their original experimental datasets cheney et al 1998 2012 cruz et al 2005 anderson et al 2015 it is expected that a significant proportion of the model error observed in these results arises from uncertainties in the wildfire datasets used in the evaluation wildfire data is notoriously coarse with uncertainties arising from issues such as the use of long fire runs encompassing at times variations in fuel types the estimation of fuel characteristics across the landscape and the averaging of wind speed over broad spatial and temporal scales alexander and cruz 2006 cheney et al 2012 in their modelling evaluation conducted against independent data cheney et al 2012 showed that mae values for low reliability data typically wildfires were more than double those found for more reliable data the high error associated with wildfires reflect not only the data used but the typical situation when these models are used for operational near real time prediction of wildfire propagation as opposed to say prescribed burning a further source of error in field based models is the simplistic nature of the empirically derived model relationships with only a few fuel and weather variables influencing the model output cruz and gould 2009 it is worth noting that methods used in operational prediction of wildfire spread based on an empirical understanding of fire environment dynamics acknowledgement of model limitations and simple ad hoc calibration methods rothermel and rinehart 1983 stratton 2006 alexander and cruz 2013 allows experienced users to reduce these errors to acceptable levels the benchmark evaluation results for the rothermel 1972 surface fire spread model are elucidative of the model s capabilities and limitations the moderate mape and low bias for the grassland and shrubland fuel types illustrate the adequacy of the model for these fuel types the larger mape values and under prediction bias determined for logging slash and forest understorey fuel types highlight a long known over sensitivity of the model to fuel bed depth and compactness catchpole et al 1993 1998b the results from the rothermel 1972 model evaluations are not easily compared with those of the current operational fire spread models used in australia the magnitude of the error statistics was in general lower for the rothermel 1972 model than found for the australian models this can be attributed to differences in the evaluation datasets the datasets used to evaluate the rothermel 1972 fires spread model had smaller ranges in observed fire behaviour and were based on experimental fires and to a lesser extent prescribed fire data with locally measured inputs whereas the australian models as mentioned previously were largely evaluated against wildfire data with coarsely estimated inputs and significant uncertainty pertinent to the present discussion was the approach taken by australian fire researchers not to assume existent models as necessarily satisfactory and endeavour to develop new improved ones in light of perceived and identified model limitations burrows and sneewjagt 1991 mccaw et al 2008 this in turn resulted in substantial improvements in model accuracy and is in keeping with a long established belief of periodically carrying out model revisions mcarthur 1977 luke and mcarthur 1978 in contrast no such improvement in fire rate of spread models for operational application based on quantified model performance has been observed elsewhere in the world in recent years attempts to improve individual components of the rothermel 1972 model e g wilson 1985 1990 sandberg et al 2007 or develop a replacement catchpole et al 1993 catchpole and catchpole 2000 have occurred in the past but the perceived improvements in model behaviour were not considered enough to justify changes in the fire behaviour prediction systems andrews 2014 improvements in the capability to predict fire behaviour has arisen mostly from other factors such as better mapping of fuel characteristics and moisture sullivan and matthews 2013 keane 2015 pimont et al 2016 and weather forecasting including the capability to model wind speed and direction over complex topography forthofer et al 2014 lopes et al 2017 and the integration of fire environment and fire behaviour models into software packages finney 2004 tymstra et al 2010 plucinski et al 2017 considering the pressing needs of fire management agencies that use models for day to day fire behaviour predictions that form the basis of emergency warnings for the general public and in support of safe and effective firefighting strategies and tactics the question remains what research approach should be pursued to reduce the error associated with current operational models the design of well crafted field experiments e g johansen 1987 wotton et al 1999 cruz et al 2015c a reanalysis of the wealth of existent fire spread data e g andrews et al 2013 gould 2016 and an increased emphasis on wildfire and prescribed fire case study documentation alexander et al 2014 perrakis et al 2014 would undoubtedly constitute a pragmatic strategy at least for the interim to collect specific data that would allow us to improve our current description of fire spread by empirical modelling while not satisfying our scientific curiosity to determine the fundamental mechanisms driving fire propagation e g finney et al 2013 it would nevertheless meet the foreseeable needs of land and fire management agencies for somewhat more accurate fire behaviour forecasts and warnings in the longer term a marriage of fundamental fire propagation mechanics with empirically derived sub modelling producing hybrid fire spread models sullivan 2009a could achieve mutual satisfaction for all concerned acknowledgements the authors are indebted to drs matt plucinski and jen beverly for their helpful comments on an earlier version of this paper and to three anonymous reviewers appendix table a1 summary of evaluation statistics for the older and newer fuel type specific fire rate of spread ros models used in australia mae mean absolute error mbe mean bias error mape mean absolute percentage error and mbpe mean bias percent error table a1 fuel types ros models mae m min 1 mbe m min 1 mape mbpe grasslands mcarthur 1966 95 66 124 68 cheney et al 1998 57 15 80 26 temperate shrublands catchpole et al 1998a 14 2 12 3 41 87 anderson et al 2015 9 1 1 5 33 17 semi arid shrublands mccaw 1997 18 2 9 5 49 50 cruz et al 2013 18 7 3 1 53 17 dry eucalypt forests mcarthur 1967 38 37 86 97 cheney et al 2012 25 1 122 3 conifer forests rothermel 1991 25 3 25 3 59 100 cruz et al 2005 14 9 6 6 52 44 
26391,the prediction of fire propagation across landscapes is necessary for safe and effective fire management we analyzed the predictive accuracy of models currently used operationally in australia for predicting fire spread rates in five different fuel types grasslands temperate and semi arid shrublands dry eucalypt and conifer forests compared to their previous counterparts we calculated error statistics and contrasted model predictions against observed spread rates of field observations of wildfires and prescribed fires we then compared the changes in error metrics of older models to newer ones evaluation results show newer models to have improved prediction accuracy mean absolute errors were reduced by 56 68 and 70 in dry eucalypt forests grasslands and crown fires in conifer forests respectively the most significant improvement was the reversion of under prediction bias achieved with newer models this study has highlighted the value of continuous improvement when it comes to developing operational wildland fire spread models keywords crown fire fire behaviour fire propagation fire weather fuel type model error 1 introduction a wildland fire is a free moving combustion reaction spreading across the landscape fig 1 as determined by fuel availability topography and atmospheric variables barrows 1961 under favourable environmental conditions fast spreading high intensity wildfires can be responsible for large burned areas stocks and walker 1973 cheney 1976 alexander and cruz 2016 widespread destruction of life and property harris et al 2012 blanchi et al 2014 alexander et al 2017 and at times undesirable ecological and environmental consequences mobley 1974 free burning wildland fires are characterized by energy release rates spanning several orders of magnitude from 10 kw m 1 an approximate lower limit for sustainable fire propagation upwards to around 100 000 kw m 1 byram 1959 only observable under the most extreme burning conditions anderson 1968 kiil and grigel 1969 cruz et al 2012 within this range direct fire suppression action is only effective in halting a fire in the lower 4 5 portion i e less than 4000 5000 kw m 1 of the spectrum cheney 1991 fires that become large will only stop when a change in fuels weather and or topographic conditions leads to a decrease in its intensity to suppressible levels or via self extinguishment schaefer 1957 underwood et al 1985 salazar and gonzález cabán 1987 the ability to predict the direction and spread rate of a wildland fire and its associated flame front characteristics allows land and fire managers to develop and implement safe and effective suppression strategies and to release timely and effective public warnings and evacuation orders scott et al 2014 cruz et al 2015b mathematical models aimed at describing the behaviour of wildland fires namely the forward or head fire rate of spread arguably the most important fire behaviour characteristic from an operational standpoint alexander 2000 have been under development for the past 80 years scott et al 2014 weise and fons 2014 models have been developed with a range of purposes from improving our theoretical understanding of particular aspects of fire behaviour to specifically addressing an applied research need such as modelling how fast a fire with an idealised flame front will move across the landscape or how tall the flames will be under a given set of fuel weather and topographic conditions albini 1984 a question one might ask when examining the accumulated body of research e g weber 1991 pastor et al 2003 sullivan 2009b is has the capability of fire spread models to accurately predict fire propagation improved in the last couple of decades or alternatively are the newer fire spread models substantially better than the older models at quantifying the effect of the environmental variables controlling the spread rate of wildland fires this line of questioning is especially pertinent to global wildfire science and management in regions of the world where wildfires threaten life and property and fire spread models are used operationally to support wildland fire control and fuel management in the particular case of australia wildland fire behaviour research has been continuously conducted to develop operationally applicable models that would overcome well documented burrows et al 1991 mccaw et al 2008 or perceived cheney and gould 1995 catchpole et al 1998a limitations of existent fire spread models cruz and alexander 2013 conducted a comprehensive survey of studies where rate of fire spread models were evaluated against independent data derived from field observations of wildfires prescribed fires and experimental fires fig 1 using this database comprised of 49 data sets corresponding to 1278 paired observations vs model predictions they were able to characterise the error metrics for several wildland fire rate of spread models they did not however conduct a temporal analysis of how the errors had changed over the years with the evolution of fire behaviour models data recently published by anderson et al 2015 and made available by kilinc et al 2012 have presented an opportunity to assess model performance and look at how newer operational models perform relative to their previous older counterparts in this paper we aim to analyse the predictive performance of a number of fire rate of spread models used operationally in australia and quantify the improvements or lack thereof in model accuracy resulting from ongoing model developments over the past 30 years or so fig 1a and b this was done using commonly accepted error statistics based on an analysis that relied upon independent datasets of wildfire fig 1 c and to a lesser extent prescribed fire observations 2 methods 2 1 model evaluation data we compiled all fire spread model evaluation studies known to us where there was a direct comparison between model predictions and field observations and where the evaluation datasets were independent from the model development process this included the 49 studies used in cruz and alexander 2013 2014 complemented with additional data from kilinc et al 2012 and anderson et al 2015 data origins included wildfires prescribed fires and experimental fires purposely conducted to develop or evaluate fire spread models we then scrutinised the available data pool for studies that allowed for the evaluation of both older and newer fire spread models or provided information from which simulations for models not considered could be undertaken it was assumed that a fire run occurred in a single general fuel type and that the fire had reached a quasi steady state spread condition i e fire was not spreading in its build up phase mcarthur 1968 to the best of our knowledge the selected wildfire runs represented a free burning linear flame front unaffected by fire suppression action and in the case of prescribed fires by their ignition pattern johansen 1987 each evaluation study selected included a minimum of five fire spread rate observations 2 2 models examined an initial examination of the data available revealed five pairs older vs newer of fuel type specific fire spread models identified as suitable for analyses table 1 this included models for four common fuel types found in australia namely grasslands mcarthur 1966 vs cheney et al 1998 temperate shrublands catchpole et al 1998a vs anderson et al 2015 semi arid shrublands mccaw 1997 vs cruz et al 2013 and dry eucalypt forests mcarthur 1967 vs cheney et al 2012 in addition to a pair of models for predicting crown fire spread in north american conifer forests rothermel 1991 vs cruz et al 2005 this latter model has been applied to australia s exotic pine plantations cruz et al 2008 all of the newer or most current models listed by fuel type in table 1 are presently used to predict fire propagation operationally in australia plucinski et al 2017 a more in depth description of these models can be found in cruz et al 2015a b as a benchmark for the analyses carried out on the models referred to in table 1 we also incorporated a similar assessment of studies evaluating the rothermel 1972 semi empirical surface fire spread model this model with additions devised by albini 1976 is globally the most widely used fire spread model either through the behaveplus fire modelling system andrews 2010 2014 or through different implementations in spatially explicit fire spread simulators see sullivan 2009c for a review the rothermel 1972 surface fire spread model consists of a mathematical algorithm developed by coupling physical heat transfer theory and statistical correlations obtained from laboratory fires and to a much lesser extent wildfire observations andrews et al 2013 the fuel inputs include fuel bed load and depth fuel particle density surface area to volume ratio heat content mineral content and the moisture content at which extinction can be expected expressed as either stylized anderson 1982 scott and burgan 2005 or custom burgan 1987 fuel models the environmental inputs are wind speed at mid flame height andrews 2012 the moisture content of fuels and slope steepness the ability of users to customize the fuel description from estimated or measured fuel data explains in part its widespread appeal and use around the world e g van wilgen et al 1985 gould 1991 lopes et al 2002 athanasiou and xanthopoulos 2010 2 3 model evaluation statistics the statistics used to analyse the error in model predictions were the mean absolute error mae the mean bias error mbe the mean absolute percent error mape and the ratio between the mbe and mae which we call the mean bias percent error mbpe willmott 1982 the equations for calculating these four error statistics are given in table 2 residuals were calculated as residual predicted observed a negative residual indicates an under prediction while a positive residual indicates an over prediction residuals were visually tested for normality and paired t test were conducted for each fuel type specific model pair to evaluate the significance of changes in residual error obtained with newer models over their older counterparts we used the statistics published in the original sources where possible in cases where they were not reported we computed them from the data contained in the associated publication or by contacting the author s directly for the raw data 3 results 3 1 operational rate of fire spread models the studies selected for undertaking the paired older vs newer comparisons of model accuracy and error their sample size e g number of independent fire spread periods and range in observed rates of fire spread are summarized in table 3 the datasets included primarily wildfire observations fig 1c prescribed fires were limited to part of the semi arid shrubland fuel type evaluation dataset a relevant aspect of the model evaluation results was that the error statistics for the grassland temperate shrubland dry eucalypt forest and conifer forest datasets were based on evaluation studies where the range in fire behaviour and certain environmental conditions e g fine dead fuel moisture content and wind speed were beyond the range of the data used in model development tables 1 and 3 this was notable for the grassland temperate shrubland and conifer forest datasets where the models were developed with a maximum rate of fire spread of 383 60 and 51 m min 1 respectively whereas the respective maxima in the evaluation datasets were 560 100 and 107 m min 1 a 10 fold difference was found in the dry eucalypt forest model analysis 22 7 compared to 260 m min 1 fig 2 is a scatterplot of model predictions against observed rates of fire spread these plots show a notorious bias to be present in the older models namely an over prediction for the grasslands and under prediction for the remaining fuel types in particular the older dry eucalypt forest and conifer forest crown fire models show a strong under prediction bias the newer models show an improvement over their older counterparts the results show a significant reduction in the bias with the data generally following the line of perfect agreement note the considerable scatter observable in these graphs the level of scatter observed is characteristic of the wildfire data due to uncertainties and natural variability in the temporal and spatial distribution of the model input conditions table 4 presents the residual mean and standard deviation for the models examined the results show the newer models reduce the magnitude of residual error paired t test of residuals showed this reduction to be significant p 0 001 for all fuel types fig 3 summarises the change in the mae mbe mape and mbpe for the various fire spread model combinations considered table a1 the results show substantial reductions in the mae for all model combinations except for the semi arid shrublands fig 3a considering the changes in the mae statistic using the newer model results as the benchmark i e change in error older model error newer model error newer model error 100 the most notable reductions were obtained for grasslands mcarthur 1966 vs cheney et al 1998 with a 68 reduction in error and crowning wildfires in conifer forests rothermel 1991 vs cruz et al 2005 with a 70 reduction in error a 56 and 52 reduction in mae was also observed in temperate shrublands catchpole et al 1998a vs anderson et al 2015 and dry eucalypt forests mcarthur 1967 vs cheney et al 2012 respectively a minor increase in mae was noted for the semi arid shrublands mccaw 1997 vs cruz et al 2013 from 18 2 to 18 7 m min 1 i e a 3 increase more important than the changes in mae is the reversion of the error type as expressed by the mbe fig 3b four out of the five older models had a preponderant under prediction bias an error type of potentially disastrous consequences in wildland fire behaviour prediction cheney 1981 the newer models had a reduced bias with three of the models having a positive bias error i e 15 3 1 and 6 6 m min 1 for grasslands semi arid shrublands and conifer forest crown fires respectively the other two models had a relatively small under prediction bias i e 1 5 and 0 7 m min 1 for temperate shrublands and dry eucalypt forests respectively the changes in mape requires careful interpretation fig 3c does not show a clear reduction in mape with the newer models this is because the mape is constrained to 100 for under predictions but is open ended for over predictions as such mape changes should only be compared for unbiased models or for models showing the same under prediction or over prediction trend this limitation of the mape is observed for the dry eucalypt forest and conifer forest evaluations where under prediction trends for the older models result in the bias to be 97 and 100 of the mean error respectively fig 3d although the newer models show a notable reduction in the mae and mbe the mape shows only a moderate improvement for the conifer forest fuel type and an undesirable increase for the dry eucalypt forest fuel type for the results presented in fig 3c only the grasslands had a decreased mape i e from 124 to 80 the analysis of changes in mbpe the bias expressed as a fraction of the mean error further highlighted the changes in error type most of the older models had a large under prediction bias fig 3d for the temperate shrubland dry eucalypt forest and conifer forest datasets this bias was close or equal to the mae indicating an very high proportion of simulations with an under prediction error the bias error of newer models was within 25 of the mean error for four of the five independent datasets the largest mpbe i e 44 for these models was found for the cruz et al 2005 model evaluation against the conifer forest dataset for crowning wildfires given in alexander and cruz 2006 3 2 rothermel 1972 surface fire spread model we found the rothermel 1972 model to be the most widely evaluated fire spread model in use around the world having been evaluated against 29 independent datasets out of a total of 49 cruz and alexander 2013 the model was evaluated over a reasonably wide range of fuel weather and topographic conditions table 5 the data available to evaluate the rothermel 1972 model were different to the datasets table 3 associated with the evaluation of the models listed in table 1 of the 29 published studies evaluating the rothermel 1972 model 18 were based entirely on experimental fires 8 relied on prescribed fires two used wildfire data and one remaining study involved a mix of both experimental fires and wildfires table 5 overall the model produced the best predictions for grassland and shrubland fuel types both with mape values of 49 and a residual bias of 7 and 14 of the mean error table 6 the lowest mae values 1 6 and 4 0 m min 1 were found for the forest understory and logging slash fuel types respectively table 6 but these errors in turn corresponded to the largest mape 76 and 61 and mbpe 56 and 79 values respectively this apparent contradiction results from the magnitude of the mae being dependent on the range of the model evaluation dataset the range of observed rate of fire spread values in the forest understory and logging slash fuel types were substantially lower 20 and 38 m min 1 respectively than found for the grasslands and shrublands fuel types 160 and 89 m min 1 respectively 4 discussion and conclusions the results of the evaluation of the fire spread rate models used operationally in australia showed a consistent improvement in prediction accuracy of newer models in comparison to their older counterparts mean errors from newer models were substantially reduced when compared with the older models and under prediction biases were eliminated in four of the five model pairs tested we believe the improvement in the predictive accuracy of the newer models is not fortuitous but the result of more robust experimental designs model development datasets covering wider range in fire behaviour and regression analysis based on functional forms that are known to represent the bulk effect of the chosen environment variables influencing fire spread rates rather than a form driven solely by statistical fitting cruz et al 2017 the improvement in model accuracy is especially relevant to the australian fire environment where the periodic occurrence of severe burning conditions causes high impact bushfires with significant loss of life and property cruz et al 2012 harris et al 2012 blanchi et al 2014 nevertheless it is noteworthy that there is still a substantial error associated with the model predictions with mape values varying between 33 in temperate shrublands to 122 in dry eucalypt forests these models tend to show mape values between 30 and 50 when evaluated against their original experimental datasets cheney et al 1998 2012 cruz et al 2005 anderson et al 2015 it is expected that a significant proportion of the model error observed in these results arises from uncertainties in the wildfire datasets used in the evaluation wildfire data is notoriously coarse with uncertainties arising from issues such as the use of long fire runs encompassing at times variations in fuel types the estimation of fuel characteristics across the landscape and the averaging of wind speed over broad spatial and temporal scales alexander and cruz 2006 cheney et al 2012 in their modelling evaluation conducted against independent data cheney et al 2012 showed that mae values for low reliability data typically wildfires were more than double those found for more reliable data the high error associated with wildfires reflect not only the data used but the typical situation when these models are used for operational near real time prediction of wildfire propagation as opposed to say prescribed burning a further source of error in field based models is the simplistic nature of the empirically derived model relationships with only a few fuel and weather variables influencing the model output cruz and gould 2009 it is worth noting that methods used in operational prediction of wildfire spread based on an empirical understanding of fire environment dynamics acknowledgement of model limitations and simple ad hoc calibration methods rothermel and rinehart 1983 stratton 2006 alexander and cruz 2013 allows experienced users to reduce these errors to acceptable levels the benchmark evaluation results for the rothermel 1972 surface fire spread model are elucidative of the model s capabilities and limitations the moderate mape and low bias for the grassland and shrubland fuel types illustrate the adequacy of the model for these fuel types the larger mape values and under prediction bias determined for logging slash and forest understorey fuel types highlight a long known over sensitivity of the model to fuel bed depth and compactness catchpole et al 1993 1998b the results from the rothermel 1972 model evaluations are not easily compared with those of the current operational fire spread models used in australia the magnitude of the error statistics was in general lower for the rothermel 1972 model than found for the australian models this can be attributed to differences in the evaluation datasets the datasets used to evaluate the rothermel 1972 fires spread model had smaller ranges in observed fire behaviour and were based on experimental fires and to a lesser extent prescribed fire data with locally measured inputs whereas the australian models as mentioned previously were largely evaluated against wildfire data with coarsely estimated inputs and significant uncertainty pertinent to the present discussion was the approach taken by australian fire researchers not to assume existent models as necessarily satisfactory and endeavour to develop new improved ones in light of perceived and identified model limitations burrows and sneewjagt 1991 mccaw et al 2008 this in turn resulted in substantial improvements in model accuracy and is in keeping with a long established belief of periodically carrying out model revisions mcarthur 1977 luke and mcarthur 1978 in contrast no such improvement in fire rate of spread models for operational application based on quantified model performance has been observed elsewhere in the world in recent years attempts to improve individual components of the rothermel 1972 model e g wilson 1985 1990 sandberg et al 2007 or develop a replacement catchpole et al 1993 catchpole and catchpole 2000 have occurred in the past but the perceived improvements in model behaviour were not considered enough to justify changes in the fire behaviour prediction systems andrews 2014 improvements in the capability to predict fire behaviour has arisen mostly from other factors such as better mapping of fuel characteristics and moisture sullivan and matthews 2013 keane 2015 pimont et al 2016 and weather forecasting including the capability to model wind speed and direction over complex topography forthofer et al 2014 lopes et al 2017 and the integration of fire environment and fire behaviour models into software packages finney 2004 tymstra et al 2010 plucinski et al 2017 considering the pressing needs of fire management agencies that use models for day to day fire behaviour predictions that form the basis of emergency warnings for the general public and in support of safe and effective firefighting strategies and tactics the question remains what research approach should be pursued to reduce the error associated with current operational models the design of well crafted field experiments e g johansen 1987 wotton et al 1999 cruz et al 2015c a reanalysis of the wealth of existent fire spread data e g andrews et al 2013 gould 2016 and an increased emphasis on wildfire and prescribed fire case study documentation alexander et al 2014 perrakis et al 2014 would undoubtedly constitute a pragmatic strategy at least for the interim to collect specific data that would allow us to improve our current description of fire spread by empirical modelling while not satisfying our scientific curiosity to determine the fundamental mechanisms driving fire propagation e g finney et al 2013 it would nevertheless meet the foreseeable needs of land and fire management agencies for somewhat more accurate fire behaviour forecasts and warnings in the longer term a marriage of fundamental fire propagation mechanics with empirically derived sub modelling producing hybrid fire spread models sullivan 2009a could achieve mutual satisfaction for all concerned acknowledgements the authors are indebted to drs matt plucinski and jen beverly for their helpful comments on an earlier version of this paper and to three anonymous reviewers appendix table a1 summary of evaluation statistics for the older and newer fuel type specific fire rate of spread ros models used in australia mae mean absolute error mbe mean bias error mape mean absolute percentage error and mbpe mean bias percent error table a1 fuel types ros models mae m min 1 mbe m min 1 mape mbpe grasslands mcarthur 1966 95 66 124 68 cheney et al 1998 57 15 80 26 temperate shrublands catchpole et al 1998a 14 2 12 3 41 87 anderson et al 2015 9 1 1 5 33 17 semi arid shrublands mccaw 1997 18 2 9 5 49 50 cruz et al 2013 18 7 3 1 53 17 dry eucalypt forests mcarthur 1967 38 37 86 97 cheney et al 2012 25 1 122 3 conifer forests rothermel 1991 25 3 25 3 59 100 cruz et al 2005 14 9 6 6 52 44 
26392,an efficient bayesian analytical framework was developed to address the challenges of uncertainty analysis and assess the parameter identification problems of complex water quality models with high dimensional parameter space the inclusion of a multi chain markov chain monte carlo method and comprehensive global sensitive analysis gsa guarantees the results to be robust a high frequency synthetic data case study was conducted in the efdc water quality module including 54 parameters the comprehensive gsa identified 39 completely or partially sensitive parameters for reducing dimensionality among which only nine were identifiable without significant bias the fundamental causes of the parameter identification problem could be traced to the cognitive limitations of the real water quality assessment process instead of data scarcity the framework is powerful for exploring these limitations generating reminders for model users to use bayesian estimates in future forecasts and providing directions for model developers to perfect a model in future work keywords water quality model parameter high dimension sensitivity uncertainty identifiability 1 introduction with rapid urbanization and economic development water quality deterioration has become a global concern serious problems necessitate the prevention and control of water pollution as well as aquatic ecosystem management water quality models are powerful mathematical tools for water quality assessment pollution control emergency preparedness and response and aquatic environmental planning mirchi and watkins 2012 melching et al 2013 xiao et al 2015 water quality models are established based on an understanding of the relevant hydrodynamic chemical and biochemical pollutant migration and transformation processes in aquatic ecosystems as well as the hypotheses on inaccessible behaviors beck 1987 melching et al 1990 walker et al 2003 lindenschmidt et al 2007 these inaccessible but complicated behaviors are often parameterized based on different state variables e g the monod model uses two parameters to describe the microbial usage of bod5 with deepening insight into such mechanisms and related processes water quality models have become increasingly complex on the one hand non monotonic and non linear relationships among state variables have replaced the initial monotonic and linear relationships yielding extensive local optima on the other hand dimensions of the parameter space have increased dramatically and redundant relationships have simultaneously arisen refsgaard et al 2006 freni et al 2011 causing extensive equifinality i e multiple optimal parameter vectors that yield similar goodness of fit parameter identifiability is the possibility of learning the true values of underlying parameters with an infinite experimental dataset raue et al 2009 parameter identification for complex water quality models is inevitably challenging and parameter true values are often not learned because of the increased computation cycles and the aforementioned factors against such serious parameter identification problems omlin et al 2001 müller et al 2002 brun et al 2002 raue et al 2009 an efficient and robust uncertainty analysis ua will aid both model users and developers to assess parameter identifiability and eventually determine the cognitive limitations of the real behaviors of an aquatic ecosystem in turn targeted control of model imperfections can mitigate the adverse impacts of non identifiability and strengthen the reliability of simulation results thereby preventing decision making errors wagener and kollat 2007 ascough et al 2008 jiang et al 2017 the bayesian method represents a modern branch of ua techniques developed based on bayes theorem eq 1 this method describes parameter uncertainty by deriving the posterior parameter distribution π θ x from a combination of prior parameter distribution π θ and the likelihood function p x θ in which empirical knowledge such as past research experience previous comparable experiments and even intuition or belief and sampling information are encoded respectively 1 π θ x π θ p x θ in general the explicit functional form of the posterior distribution is unlikely to be derived analytically therefore sampling is indispensable the markov chain monte carlo mcmc method provides a series of efficient sampling algorithms to obtain the posterior parameter distribution tierney 2015 multi chain mcmc methods represented by the differential evolution adaptive metropolis dream algorithm vrugt 2016 are a substantially competitive branch multi chain mcmc methods are based on a genetic algorithm integrated with the population concept each markov chain uses a randomly sampled point from the prior parameter space as the initial population to initiate an evolution multiple markov chains interact with one another to co generate the transition kernel ter braak 2006 ter braak and vrugt 2008 different from single chain mcmc methods hence multi chain mcmc methods have been used to improve sampling and searching capabilities as well as to prevent premature convergence i e being trapped in local optimum in high dimensional parameter spaces vrugt et al 2009 these advantages resulted in the current popularity of multi chain mcmc methods in hydrological research keating et al 2010 he et al 2011 harrison et al 2012 joseph and guillaume 2013 however relevant applications in complex water quality models still remain insufficient to further increase the efficiency of ua dimensionality reduction is often initially required for complex water quality models sensitivity analysis sa which assesses the degree to which parameter uncertainty causes output variations plays an important role in dimensionality reduction via parameter prioritization and fixing saltelli et al 2008 bilotta et al 2012 sun et al 2012 ganji et al 2016 sa techniques are often categorized into local lsa and global sensitivity analysis gsa methods lsa is a partial derivative based method to investigate the response of a small disturbance of each parameter around a specific location in parameter space on model output matott et al 2009 baroni and tarantola 2014 a common method of conducting lsa is to utilize the one factor at a time oat method yang 2011 although lsa is computationally economical and popular luo and zhang 2009 jia et al 2015 abdul aziz and al amin 2016 it is not suitable for reducing the dimensionality of complex water quality models because of its location dependence as well as the lack of knowledge on the suitable location i e the parameter true value gsa investigates the effect of the variations over the entire prior parameter space on the model output saltelli et al 2008 zhan and zhang 2013 pianosi et al 2016 gsa does not rely on a pre known suitable location thus it overcomes the limitations of lsa common gsa methods can be classified into four categories i variance based methods such as the sobol s method ii entropy based methods such as kullback leibler kl entropy iii derivative based methods such as the morris screening method and iv regression based methods such as standardized regression coefficients src due to the ambiguous definition of global sensitivity different gsa methods reveal different relationships between the parameters and model responses razavi and gupta 2015 which lead to varying results of gsa razavi and gupta 2015 2016 introduced important characteristics i e local sensitivities and their global distribution global distribution of model responses and structural organization of the response s surface to interpret global sensitivity and indicated that existing gsa methods such as the sobol s and morris screening methods only focus on one or a few of these characteristics while not considering the others this indicates the strengths and weaknesses of a single gsa method in addition to razavi and gupta 2015 2016 several other researchers have also recommended comprehensive and complementary use of different gsa techniques for robust sa cloke et al 2008 pappenberger et al 2008 mishra et al 2009 neumann 2012 cosenza et al 2013 gamerith et al 2013 wainwright et al 2013 gan et al 2014 vanrolleghem et al 2015 sarrazin et al 2016 peer reviewed literature remains insufficient for proposing an efficient and robust ua parameter framework to assess the parameter identification problem i e model imperfection of complex water quality models efficient refers to the computational frugality of the ua process as well as the achievement of convergence with infinite iterations which can be realized by reducing the dimensionality robust indicates that the framework will correctly evaluate whether the true value of a certain parameter can be learned with this background this study organically combines the comprehensive gsa multi chain mcmc method and bayesian estimation to establish a new analytical framework with this framework the environmental fluid dynamics code efdc water quality module with a built in case for the lower charles river basin hamrick 1992 is used as a synthetic data case study to assess the parameter identification problem caused by the cognitive limitations 2 methods and materials 2 1 analytical framework the proposed analytical framework fig 1 aims to address the challenge of ua and to assess the rationality of the parameterized processes in complex water quality models at first the prior distribution of each parameter should be specified by combining past research experience previous comparable experiments and intuition or belief where subjective assumptions are sometimes inevitable the purpose of using a comprehensive gsa on the entire prior parameter hyperspace is to classify the parameters as important or unimportant insensitive if the contribution of a parameter to the model output uncertainty i e the sensitivity index si exceeds a predefined threshold α in at least one gsa technique the parameter is labelled as important otherwise the parameter is labelled as unimportant the si cut off α is determined from cumulative sis the venn diagrams in fig 1 will help researchers to intuitively understand the classification rules i e a union of sensitive parameters screened by each gsa technique independently represents an important ensemble in the ua process unimportant parameters are fixed at their true values table 2 i e parameter fixing and only a limited number of important parameters are allowed to vary i e parameter prioritization after the likelihood function that measures the discrepancies between the observed and predicted water quality time series is properly defined a multi chain mcmc method is applied to generate the posterior parameter distributions of the important ensemble according to the previous definition of parameter identifiability if the posterior distribution is not represented unimodally i e non peak distribution indicates impossibility of learning the true value whereas multimodal posterior distribution indicates that at least two nonadjacent high density zones lead to similar goodness of fit values but only one may contain the true value or is not represented with a diminished range then a parameter is not identifiable otherwise bayesian estimation be is used to assess whether the parameter can be correctly identified if the bayesian estimate is unbiased compared to the true value θ then the parameter is regarded as correctly identified otherwise the parameter is labelled as biasedly identified in addition some exceptional unidentifiable parameters could be indirectly identified through pairwise correlation analysis particularly if significant correlations are determined with a certain identifiable parameter the parameter true values are critical to the proposed framework in real data cases the parameter true values are often unknown making it difficult to discuss the parameter identification problem whereas for a synthetic dataset the parameter true values can be reasonably designated in addition the use of field monitoring datasets involves model input uncertainty and data scarcity due to current limitations of monitoring techniques which will interfere with analysis of parameter identifiability therefore a synthetic dataset would be an appropriate solution schoups and vrugt 2010 evin et al 2014 smith et al 2015 especially when the purpose of the research is to assess parameter identifiability i e to determine the rationality of the constituent hypothesis regarding water quality processes as is in this study for new cases without substantial previous work parameter true values are not readily available in such situations an appropriate value from the prior parameter range could be selected as the true value based on the parameter true values the observation dataset would then be generated from a simulated time series with corrupting gaussian noise that has a mean of zero and an independent constant deviation i e n 0 σ 2 a few complex residual structures have been reviewed by smith et al 2015 but a simple assumption for illustrative purposes will suffice here 2 2 comprehensive gsa previous researches campolongo and saltelli 1997 yang 2011 gamerith et al 2013 li et al 2013 zhan and zhang 2013 esmaeili et al 2014 gan et al 2014 vanuytrecht et al 2014 vanrolleghem et al 2015 sarrazin et al 2016 indicated that the sobol s variance based morris screening derivative based and src methods regression based were the three most frequently applied gsa methods to date these three methods have complementary advantages thereby recommended for comprehensive gsa in the analytical framework the sobol s method sobol 2001 decomposes the unconditional variance v of the model output into n th order conditional variances v i j eq 2 the proportion to the unconditional variance v i j v represents the contribution of the corresponding parameter or parameter interaction to model output uncertainty 2 v i v i i j i v i j v 12 n the morris screening method morris 1991 is based on the reiterative one factor at a time oat method first multiple trajectories are generated in the prior parameter space via latin hypercube sampling lhs ye 1998 for each trajectory the parameter is screened consecutively from θ 1 to θ m for each screened parameter oat method is then applied to calculate the elementary effect e e i expressed as eq 3 where δ i is a predefined slight disturbance of parameter θ i then each trajectory presents m 1 parameter realizations the mean value of ee i with all the trajectories is considered as the si for parameter θ i 3 e e i f θ 1 θ i δ i θ m f θ 1 θ i θ m δ i the src method saltelli et al 2004 is based on the assumed multivariate linear relationship between the model output and the parameter vector θ such that the model can be rewritten as eq 4 the regression coefficient b i is estimated via the least squares method lsm then a normal standardization is applied to make the coefficients comparable expressed as eq 5 the standardized regression coefficient s r c i in eq 5 is considered the si for parameter θ i 4 y i 1 n b i θ i b 0 5 s r c i b i ˆ σ θ i σ y 2 3 ua and parameter identification a multi chain mcmc method namely the dream was applied in this study this algorithm was introduced by jasper a vrugt and his colleagues in 2009 vrugt et al 2009 the matlab code is available on http faculty sites uci edu jasper software the likelihood function in eq 1 corresponding to the assumed gaussian homoscedastic and independent residual errors can be defined as eq 6 marshall et al 2004 liu et al 2008 smith et al 2015 considering zero observation of pollutant indicators could scarcely be monitored in reality zero inflation should not necessarily be included 6 p x θ j k 2 πσ 2 t 2 t exp x t j k o t j k 2 2 σ 2 where x t j k and o t j k are the simulated and observed time series of the j th state variable at the k th monitoring site respectively and t is the recording frequency the transition kernel of the dream consists of two components one component is the evolution of the candidate parameter vector liang et al 2016 7 z m θ m 1 e γ δ d i 1 δ θ r 1 i j 1 δ θ r 2 j ε where z is the candidate parameter vector m is the index of the chain θ is the present parameter vector δ is the number of pairs used to generate the proposed vector d is the dimensionality of the parameter hyperspace r 1 i r 2 j 1 n where n is the number of chains and r 1 i r 2 j m for i j 1 δ e is a random vector drawn from a uniform distribution on b b with b 1 ε is a random vector drawn from a normal distribution with a mean value of 0 and a standard deviation substantially smaller than the width of the target distribution and γ is the value of jump size that depends on δ and d 2 38 2 δd in eq 7 the markov chains are interactive with one another in co generating the candidate parameter vector if one chain falls into local optimum fortunately the other n 1 chains will drag it out of the trap therefore such a complicated transition kernel avoids premature convergence vrugt et al 2009 the second component of the dream transition kernel corresponds to the acceptance probability metropolis et al 1953 8 p θ t 1 m z m min p x z m π z m p x θ t 1 m π θ t 1 m 1 in eq 8 if the candidate parameter vector has a higher posterior probability to approximate the real behavior and conform to prior knowledge compared to the present parameter vector then the markov chain will step forward to the candidate position otherwise the candidate parameter has an acceptance probability equal to the posterior probability ratio of replacing the present parameter vector in the markov chain a detailed description of the dream can be found in the literature vrugt et al 2009 as is known parameter uncertainty is inevitable similar to other ua approaches results of the dream are posterior distributions rather than unique optimal values a unimodal posterior distribution is the best state for an uncertain parameter thereby qualitatively defined as small uncertainty the others are defined as large uncertainty if the posterior parameter distribution is unimodal with a diminished range then be will be applied otherwise be is not necessary the two most common bes are the posterior expectation θ ˆ e that corresponds to the minimum mean square error loss function and the posterior median θ ˆ md that corresponds to the least absolute value loss function eq 9 these two estimates often have similar results but the posterior expectation is susceptible to the extreme value hence the posterior median is recommended a parameter can be categorized as either correctly identified or biasedly identified by comparing the bayesian estimate and the true value whether they are located in the same high density zone 9 θ ˆ θ ˆ md l θ ˆ e θ ˆ θ θ ˆ e l θ ˆ e θ ˆ θ 2 2 4 brief introduction of the efdc water quality module the aforementioned methodology would be applied to a sufficiently complex water quality model namely efdc the efdc is a 3d multifunctional surface water modeling system that consists of hydrodynamic and mass transport sediment and contaminant transport and water quality modules hamrick 1992 this study focuses on the efdc water quality module which has been increasingly and extensively applied to assess water quality variations in surface water systems liang et al 2016 yi et al 2016 zhu et al 2016 the structure of the water quality module is illustrated in fig 2 in which 22 water quality state variables and their relationships are also depicted table 1 lists the definitions of each state variable the governing equation of the efdc water quality module with respect to mass conservation is expressed as eq 10 which is workable for all water quality state variables 10 m x m y h c t m y h u c x m x h v c y m x m y w c z x m y h a x m x c x y m x h a y m y c y z m x m y a z h c z m x m y h s c where u v and w are velocity components in the curvilinear orthogonal coordinates x y and z respectively a x a y and a z are turbulent diffusivities in x y and z directions h represents water column depth c is concentration of a water quality state variable s c is internal and external source sink term per unit volume of a water quality state variable m x and m y are square roots of the diagonal components of the metric tensor and m x m y is the jacobian or square root of the metric tensor determinant nine efdc water quality state variables were selected for ua they were bc bd bg nh4 no23 toc tp ton and do where toc is the sum of doc lpoc and rpoc tp is the sum of dop lpop rpop and po4t and ton is the sum of don lpon and rpon see table 1 for their corrupting gaussian noise a total of 54 parameters were involved as listed in table 2 2 5 synthetic data case study the study site is located in the lower charles river basin boston massachusetts from watertown dam to boston harbor fig 3 a this is a built in efdc case much effort has been expended to explore the performance of efdc using this case especially for model calibration and verification tetra tech inc 2005 the available information about model settings was instructive for this synthetic data case and the default parameter values were regarded as true values a few other efdc cases see appendix were also consulted to determine the prior parameter ranges as the ranges are bounded and no more statistical results about the site specific prior parameter distribution are accessible bayes hypothesis priors i e uniform distribution were assumed for illustrative purposes freni and mannina 2010 the results are listed in table 2 the grid is the basis of numerical computation in efdc fig 3 b and c depict the generalized grids of the studied river and the grid transformation from a curvilinear orthogonal coordinate system to a cartesian coordinate system before generating numerical solutions respectively the monitoring sites were located within the five grids marked in fig 3 c the default simulation time step of 30 s was used by substituting the parameter true values into the model the simulated time series of the state variables recorded at 0 3 h 18 min frequency from april 1st 1998 to june 30th 1998 a total of 7280 points in each time series excluding the warm up period were taken as the hypothetical observation dataset 3 results and discussion 3 1 summary of results fig 4 illustrates the parameter identification results after implementing the proposed analytical framework the sis of the sobol s morris screening and src methods were calculated following the methods of previous studies sobol 2001 morris 1991 saltelli et al 2004 for the sobol s and src methods convergence was assumed after 5000 bootstrap samplings efron 1981 the convergence of the morris screening method was assumed when the parameter space was gridded into a 10054 hypercube and lhs was repeated 100 times with an si cut off α of 1 for bc bd bg nh4 no3 and do the cumulative sis of the important parameters i e si 1 exceeded 95 and for toc tp and ton the cumulative sis of the important parameters exceeded 90 whereas the sis of the unimportant parameters i e si 1 were all below 0 5 therefore the si cut off α of 1 was reasonable a total of 39 important parameters were identified as sensitive by at least one gsa method the important parameters determined by all three gsa methods were referred to as completely sensitive labelled in the red dashed rectangles in fig 5 while the other important parameters determined by one or two gsa methods were referred to as partially sensitive the stacked bars in fig 5 represent the individual and cumulative contributions of each important parameter to the uncertainty of the corresponding state variable before running the dream the nine state variables were separated into four subsystems bc bd and bg toc tp and ton nh4 and no3 and do based on the distinct si patterns presented by the sobol s morris screening and src methods which will be discussed in the following sections moreover no two groups shared important parameters hence ua could be conducted separately for each subsystem during the ua process the important parameters screened out by the comprehensive gsa evolved according to the transition kernel of the dream while the unimportant parameters were fixed at their true values table 2 for example in the bc bd bg subsystem a total of 12 out of 54 important parameters including cchlx prrx wsx and trx varied while 42 out of 54 parameters remained constant the numerical experiments were repeated four times for each subsystem subsequently for each experiment eight markov chains were applied to the dream algorithm after 750 evolutions the mean and variance of each parameter of interest were assumed to be constant indicating that the dream has achieved convergence furthermore the subsequent 750 consecutive samples for each markov chain and experiment i e a total of 24 000 samples were used to plot the posterior parameter distributions a total of 18 parameters presented a unimodal posterior distribution with a diminished range fig 6 further classified as nine correctly and nine biasedly identified parameters through be the remaining 21 important parameters were classified as unidentifiable because of their multimodal or undiminished posterior distributions fig 7 presents three examples of this classification in addition no indirectly identified parameter was found in the pairwise correlation analysis 3 2 discussion of the gsa results 3 2 1 a mechanism perspective from a biochemical or physicochemical perspective most of the important parameters were directly related to the generation and extinction of the corresponding state variables the most complex subsystem the toc tp and ton subsystem was taken as an example and a similar analysis could be conducted for the other subsystems for toc four completely sensitive parameters were determined trhdr kthdr krc and klc table 3 these findings indicated that krc and klc excluding kdc were the leading factors that determined the concentration of toc remaining in the water body this scenario can logically be explained by biochemical mechanisms among the three forms of organic carbon dissolved refractory particulate and liable particulate forms dissolved organic carbon is easily utilized in respiration by heterotrophic microorganisms thus dissolved organic carbon is mineralized for hydrolysis more readily than particulate organic carbon therefore the hydrolysis of particulate organic carbon is a rate determining step characterized by the krc and klc parameters similar to toc all the three gsa methods identified the two temperature related parameters trhdr and kthdr as being sensitive to tp and ton this finding is attributed to the governing equation of the same form that controls the hydrolysis process expressed as eq 11 11 k h k d p i k a p i e k t h t t r h where k h is the actual hydrolysis rate of particulate organic matter k d p is the minimum dissolution rate of particulate organic matter k a p i is a constant that relates the hydrolysis of particulate organic matter to algal i biomass k t h i e kthdr measures the effect of water temperature on hydrolysis t is water temperature and t r h i e trhdr is the reference temperature for hydrolysis refer to tetra tech inc 2007 p34 for details in addition to the temperature and hydrolysis rate parameters other factors related to algae such as khpx and ancx also significantly influenced tp and ton concentrations toc was excluded for ton ancc and ancg had importance rankings higher than kln and krn and were also identified as completely sensitive parameters while kln and krn were insensitive in the morris screening method the reason for the screening out of a few algae related parameters was that phosphorus and nitrogen could be directly utilized by algae rather than organic carbon because they are autotrophic microorganisms 3 2 2 comparisons among different gsa methods for each state variable at least one completely important parameter was identified by the venn diagrams fig 1 detailed information on these parameters is listed in table 3 distinct patterns were presented for each state variable by the sobol s morris screening and src methods fig 5 these patterns included the si contributed by the most sensitive parameter the cumulative sis of completely sensitive parameters the partially sensitive parameters that were screened out and the importance rankings however a few common points were noticeable when exploring these distinct patterns the morris screening method was the most rigorous and conservative of the three gsa methods this method highlighted the importance of the most sensitive parameter and was less prone to type ii errors with the exception of tp and do the morris screening method yielded the highest cumulative sis of the completely sensitive parameters for each state variable and excluding nh4 and no3 the morris screening method yielded the highest si for each state variable see the red bold digits in table 3 however this method was incapable of identifying partially sensitive parameters therefore the morris screening method led to the complementary results of the sobol s and src methods compared with the morris screening method the sobol s and src methods exhibited better performance at identifying partially sensitive parameters thus they were less likely to make a type i error moreover the results of the sobol s and src methods were often in good agreement with one another in terms of ranking the important parameters similar patterns i e almost the same important parameters appeared in the stacked histogram the same colored bars had similar lengths and important parameters were ranked in similar orders fig 5 were presented by these two gsa methods especially for toc tp and ton this indicates that the src method could be substituted for the sobol s method in some situations particularly when computational efficiency is considered as the sobol s method is more computationally taxing than the src method 3 3 discussion of the ua and parameter identification results 3 3 1 overview of parameter identification problem among the 54 parameters only eighteen i e 1 3 parameters presented a unimodal posterior distribution with a diminished range nine of which i e 1 2 were correctly identified indicating serious parameter identification problems the first issue is the be bias a typical example is the kro parameter as it presented a substantially exaggerated bias i e the bayesian estimate 4 53 is almost three times as large as the true value 1 50 fig 6 compared with the ktr parameter in the same do subsystem according to eq 12 the parameters ktr and kro jointly control the entire reaeration procedure during the simulation period the water temperature mostly remained below 20 c thus k r is a rigorously increasing function of kro and a rigorously decreasing function of ktr to maintain a constant k r an increase in ktr must accompany an increase in kro considering that ktr accounted for over 90 of the do variance and was much more sensitive than kro fig 5 a ktr estimate 1 064 that was slightly larger than the true value 1 024 would therefore multiply kro to counteract the increasing k r trend 12 k r k r o u e q h e q w r e a k t r t 20 where k r is the actual reaeration rate of do t is the real time water temperature u e q and h e q are the weighted velocity and weighted depth over a cross section respectively and w r e a describes the wind induced reaeration which can be estimated using wind speed refer to tetra tech inc 2007 p49 for details similar logic could be applied to the parameters trhdr and kthdr the only difference is that these two parameters were almost equally sensitive and had a relatively small bias compared to ktr and kro both trhdr and kthdr tended to be overestimated fig 6 according to eq 11 a larger kthdr k t h tends to result in a lower particulate organic matter hydrolysis rate whereas a larger trhdr t r h tends to result in a higher hydrolysis rate in the synthetic data case the effects of the biased estimates coincidentally offset each other yielding consistent outputs the other biasedly identified parameters could be explained in the same way as the two examples presented in the previous paragraphs although explaining certain parameters would be relatively laborious however the scenario in which the effects of a group of biased estimates offset one another cannot be guaranteed when applying bayesian estimates for forecasting a future state moreover because model users cannot always know the parameter true values in advance whether the concentrations of state variables will be overestimated or underestimated is unpredictable the second issue is the multimodal posterior distributions such as those of the parameters trc krc and tnit this situation would lead to a bayesian estimate located outside the high density zones fig 7 and raise a choice dilemma for model users a few studies freni and mannina 2010 tang et al 2016 indicated that the selection of prior knowledge including prior distributions and value ranges can impact the posterior distributions of sensitive parameters although prior knowledge may be less important if the available data are sufficient given that the prior distributions were uniform in this synthetic data case freni and mannina 2010 the multimodal posterior distributions must result from the likelihood function which had a direct relationship with the model structure and was non monotonic with respect to these parameters as several locally optimal points existed within the prior value ranges in that case the selection of appropriate prior value ranges i e no existing locally optimal values for these parameters is important for model users unfortunately further narrowing the prior value ranges would not be easy because it requires additional field experiments and parameter investigation 3 3 2 causes of parameter identification problem apart from the cognitive limitations many studies beck 1985 müller et al 2002 swameye et al 2003 have cited data problems including model input uncertainties and data scarcity as one of the reasons why parameter identification is difficult for model input uncertainties monitoring data from different sources e g meteorological data pollution source data and water quality data have different scales of uncertainties by the synthesis of output time series these uncertainties could be eliminated for the data scarcity problem at the temporal scale the case of the lower charles river basin used high frequency time series of multiple state variables for analysis which is almost impossible to obtain through field monitoring at the spatial scale the synthetic data case monitored the average concentration of a pollutant indicator throughout the entire grid while during real monitoring only one or a few points were monitored to represent the states of the entire grid because of these two reasons data scarcity was considerably weaker in the synthetic data case than in any real data cases this study added further two hypothetical monitoring sites grids 3 33 and 3 40 in fig 3 c increased the recording frequency from 0 3 h to 0 1 h and retested parameter identifiability with respect to the nh4 and no3 subsystem as a supplement unfortunately the parameters tnit and knit2 still did not present unimodal posterior distributions in this respect when observation data are extremely scarce a potential approach to address the parameter identification problem is to intensify the monitoring frequency however when the monitoring intensity is enhanced to a certain degree data scarcity is no longer the primary cause of difficulty in parameter identification because synthetic datasets were able to eliminate or well control the impacts of data problems the fundamental causes of the parameter identification problem could be entirely traced to the cognitive limitations of the modelers regarding water quality processes in this study the framework outlined in this study is then able to identify these cognitive limitations indicating a direction of model improvement for model developers 3 3 3 tri variate relationships among sensitivity uncertainty and identifiability tri variate relationships among sensitivity uncertainty and identifiability were gradually elucidated through exploring the ua results fig 8 the analytical framework established based on these relationships is therefore logically reasonable first a sufficient but unnecessary condition is given for low sensitivity to be large uncertainty low sensitivity suggests that parameter variation has almost no effects on model output resulting in extensive equifinality consequently the posterior parameter distribution will be almost uniform which further causes large uncertainty however low sensitivity cannot be determined from large uncertainty the parameters shown in fig 7 represent good counterexamples all of these parameters have multimodal posterior distributions the gsa categorized them as completely sensitive parameters yielding the peaks in their posterior distributions but the ua regarded them as largely uncertain ones because of multiple locally optimal points existing in the likelihood functions which yielded similar goodness of fit razavi and gupta 2015 have provided a few more illustrative examples on this issue the second proposition is that large uncertainty is a sufficient but unnecessary condition for poor identifiability in addition to the unimodal posterior distribution with a diminished range a correctly identified parameter requires an unbiased bayesian estimate therefore if a parameter presents large uncertainty i e the posterior parameter distribution is not unimodal it is poorly identifiable in turn if the bayesian estimate is significantly biased from the true value a parameter with small uncertainty will be regarded as biasedly identified the aforementioned two propositions yield the third proposition low sensitivity is a sufficient but unnecessary condition for poor identifiability consistent with the connotations underlying the definitions of identifiability omlin et al 2001 brun et al 2002 fig 8 b is the inverse negative proposition of fig 8 a thus the tri variate relationships illustrated in fig 8 b are also maintained 3 3 4 robustness of the bayesian framework robustness means that the framework will form a correct judgment on whether the parameter true values can be learned the reasons that the outlined framework here is robust lie in the following aspects the first aspect corresponds to the integration of comprehensive gsa into dimensionality reduction for example the parameters prrx and wsx as illustrative examples were partially sensitive i e simorris 1 fig 5 however they presented unimodal posterior distributions despite being biasedly identified fig 6 indicating small uncertainty if these two parameters were fixed at their default values during the ua process after only morris screening method was applied as simorris 1 then the posterior distributions of the other important parameters in the same subsystem would be distorted in fact the parameters prrx and wsx should be sensitive although morris screening method filtered them out based on the relationships between sensitivity and uncertainty in contrast if the values of an insensitive parameter were random during the ua process then the ua results would not significantly vary thus this study presumed that admitting a partially sensitive parameter into the ua process would be better than neglecting an important parameter this is another reason for using comprehensive gsa which has not yet been mentioned in other sa studies the second aspect is related to the application of the multi chain mcmc method local optimum is a common issue in complex water quality models if the markov chain is trapped into the local optimum during its evolution then a few unidentifiable parameters that should present multimodal posterior distributions e g fig 7 may present unimodal distributions and thus be mistakenly categorized as correctly or biasedly identified ones this situation would cause modelers to mistakenly believe that the water quality processes characterized by these parameters are reasonable and relevant improvements are not necessary fortunately multi chain mcmc methods can avoid premature convergence as revealed by vrugt et al 2009 4 conclusions considering parameter identification problems of complex water quality models derived from the cognitive limitations regarding water quality processes i e model imperfections a bayesian analytical framework was developed based on the tri variate relationships among sensitivity uncertainty and identifiability to assess parameter identifiability in a high dimensional parameter space framework efficiency was achieved through dimensionality reduction i e gsa the involvement of comprehensive gsa and dream guarantees the framework to yield correct posterior distributions for further assessment of parameter identification problems a high frequency synthetic efdc dataset in which model input uncertainty and data scarcity problem could be eliminated i e the fundamental causes of parameter identification problems could be entirely traced to cognitive limitations was used to illustrate how the framework discerned model imperfections among the total 54 parameters in the efdc water quality module only nine important parameters were identified without significant bias therefore efdc users should treat bayesian estimates with extreme caution when applying these results in future forecasts in the charles river case for example if researchers aim to predict the variations in toc top ton concentrations from april to june in the following years the bayesian estimates of trhdr and kthdr obtained from field monitoring data should be adjusted to be smaller because both the parameters trhdr and kthdr tended to be overestimated within the same period the outlined analytical framework is a powerful tool to determine the potential cognitive limitations of real behaviors in aquatic ecosystems specifically model imperfections exist in the water quality process that is characterized by the parameters that cannot be correctly identified e g the model may include redundant relationships model imperfections encourage the modellers to perfect their models with the help of the framework for the model users it is recommended using the framework to conduct an initial synthetic data case study in the research site before parameter estimations in real world settings the following are two potential applications 1 to assess whether each of the important parameters will be overestimated or underestimated in the synthetic data case and then to seek an acceptable adjustment of be results in the real data case 2 to assess whether each of the important parameters will present a multimodal posterior distribution in the synthetic data case and then to decide whether a corrected prior distribution should be included in ua in the real data case although current lack of online monitored high frequency water quality datasets restricts the second step application in real world settings future studies are promising due to continuous improvement of monitoring techniques e g many water quality online monitoring stations are being constructed during the 13th five year plan in china analysis of parameter sensitivity uncertainty and identifiability is always an interesting topic in the field of environmental modeling especially for complex water quality models because we still need to address the challenges resulting from parameter identification problems although this study has tried to clarify the tri variate relationship among parameter sensitivity uncertainty and identifiability within their qualitative definitions it is still not intuitive for researchers to understand the differences among these three terms by only using the definitions or by giving a few counterexamples future work may develop some indices to quantitatively characterize their common points and differences to facilitate further research on this topic acknowledgements this research was supported by the national water pollution control special project no 2017zx07205003 and tsinghua fudaoyuan research fund the authors greatly thank the editors and reviewers for providing valuable suggestions on perfection of the methodology appendix this study has consulted 19 papers for determining prior parameter value ranges cerco and cole 1993 in chesapeake bay odeq 1996 in lake fort cobb tetra tech inc 1998 in peconic estuary park et al 2005 in kwang yang bay tetra tech inc 2005 in charles river jin et al 2006 in lake okeechobee lin et al 2007 in cape fear river xia et al 2010 in caloosahatchee river estuary lin and li 2011 in neuse reservoir xia et al 2011 in perdido bay wu and xu 2011 in daoxiang lake wan et al 2012 in st lucie estuary kang and jang 2013 in hwaseong reservoir tang et al 2014 in shenzhen reservoir wang et al 2014 in miyun reservoir chen et al 2016 in danjiangkou reservoir gong et al 2016 in tianyinhu lake liang et al 2016 in miyun reservoir and yi et al 2016 in lake dianchi 
26392,an efficient bayesian analytical framework was developed to address the challenges of uncertainty analysis and assess the parameter identification problems of complex water quality models with high dimensional parameter space the inclusion of a multi chain markov chain monte carlo method and comprehensive global sensitive analysis gsa guarantees the results to be robust a high frequency synthetic data case study was conducted in the efdc water quality module including 54 parameters the comprehensive gsa identified 39 completely or partially sensitive parameters for reducing dimensionality among which only nine were identifiable without significant bias the fundamental causes of the parameter identification problem could be traced to the cognitive limitations of the real water quality assessment process instead of data scarcity the framework is powerful for exploring these limitations generating reminders for model users to use bayesian estimates in future forecasts and providing directions for model developers to perfect a model in future work keywords water quality model parameter high dimension sensitivity uncertainty identifiability 1 introduction with rapid urbanization and economic development water quality deterioration has become a global concern serious problems necessitate the prevention and control of water pollution as well as aquatic ecosystem management water quality models are powerful mathematical tools for water quality assessment pollution control emergency preparedness and response and aquatic environmental planning mirchi and watkins 2012 melching et al 2013 xiao et al 2015 water quality models are established based on an understanding of the relevant hydrodynamic chemical and biochemical pollutant migration and transformation processes in aquatic ecosystems as well as the hypotheses on inaccessible behaviors beck 1987 melching et al 1990 walker et al 2003 lindenschmidt et al 2007 these inaccessible but complicated behaviors are often parameterized based on different state variables e g the monod model uses two parameters to describe the microbial usage of bod5 with deepening insight into such mechanisms and related processes water quality models have become increasingly complex on the one hand non monotonic and non linear relationships among state variables have replaced the initial monotonic and linear relationships yielding extensive local optima on the other hand dimensions of the parameter space have increased dramatically and redundant relationships have simultaneously arisen refsgaard et al 2006 freni et al 2011 causing extensive equifinality i e multiple optimal parameter vectors that yield similar goodness of fit parameter identifiability is the possibility of learning the true values of underlying parameters with an infinite experimental dataset raue et al 2009 parameter identification for complex water quality models is inevitably challenging and parameter true values are often not learned because of the increased computation cycles and the aforementioned factors against such serious parameter identification problems omlin et al 2001 müller et al 2002 brun et al 2002 raue et al 2009 an efficient and robust uncertainty analysis ua will aid both model users and developers to assess parameter identifiability and eventually determine the cognitive limitations of the real behaviors of an aquatic ecosystem in turn targeted control of model imperfections can mitigate the adverse impacts of non identifiability and strengthen the reliability of simulation results thereby preventing decision making errors wagener and kollat 2007 ascough et al 2008 jiang et al 2017 the bayesian method represents a modern branch of ua techniques developed based on bayes theorem eq 1 this method describes parameter uncertainty by deriving the posterior parameter distribution π θ x from a combination of prior parameter distribution π θ and the likelihood function p x θ in which empirical knowledge such as past research experience previous comparable experiments and even intuition or belief and sampling information are encoded respectively 1 π θ x π θ p x θ in general the explicit functional form of the posterior distribution is unlikely to be derived analytically therefore sampling is indispensable the markov chain monte carlo mcmc method provides a series of efficient sampling algorithms to obtain the posterior parameter distribution tierney 2015 multi chain mcmc methods represented by the differential evolution adaptive metropolis dream algorithm vrugt 2016 are a substantially competitive branch multi chain mcmc methods are based on a genetic algorithm integrated with the population concept each markov chain uses a randomly sampled point from the prior parameter space as the initial population to initiate an evolution multiple markov chains interact with one another to co generate the transition kernel ter braak 2006 ter braak and vrugt 2008 different from single chain mcmc methods hence multi chain mcmc methods have been used to improve sampling and searching capabilities as well as to prevent premature convergence i e being trapped in local optimum in high dimensional parameter spaces vrugt et al 2009 these advantages resulted in the current popularity of multi chain mcmc methods in hydrological research keating et al 2010 he et al 2011 harrison et al 2012 joseph and guillaume 2013 however relevant applications in complex water quality models still remain insufficient to further increase the efficiency of ua dimensionality reduction is often initially required for complex water quality models sensitivity analysis sa which assesses the degree to which parameter uncertainty causes output variations plays an important role in dimensionality reduction via parameter prioritization and fixing saltelli et al 2008 bilotta et al 2012 sun et al 2012 ganji et al 2016 sa techniques are often categorized into local lsa and global sensitivity analysis gsa methods lsa is a partial derivative based method to investigate the response of a small disturbance of each parameter around a specific location in parameter space on model output matott et al 2009 baroni and tarantola 2014 a common method of conducting lsa is to utilize the one factor at a time oat method yang 2011 although lsa is computationally economical and popular luo and zhang 2009 jia et al 2015 abdul aziz and al amin 2016 it is not suitable for reducing the dimensionality of complex water quality models because of its location dependence as well as the lack of knowledge on the suitable location i e the parameter true value gsa investigates the effect of the variations over the entire prior parameter space on the model output saltelli et al 2008 zhan and zhang 2013 pianosi et al 2016 gsa does not rely on a pre known suitable location thus it overcomes the limitations of lsa common gsa methods can be classified into four categories i variance based methods such as the sobol s method ii entropy based methods such as kullback leibler kl entropy iii derivative based methods such as the morris screening method and iv regression based methods such as standardized regression coefficients src due to the ambiguous definition of global sensitivity different gsa methods reveal different relationships between the parameters and model responses razavi and gupta 2015 which lead to varying results of gsa razavi and gupta 2015 2016 introduced important characteristics i e local sensitivities and their global distribution global distribution of model responses and structural organization of the response s surface to interpret global sensitivity and indicated that existing gsa methods such as the sobol s and morris screening methods only focus on one or a few of these characteristics while not considering the others this indicates the strengths and weaknesses of a single gsa method in addition to razavi and gupta 2015 2016 several other researchers have also recommended comprehensive and complementary use of different gsa techniques for robust sa cloke et al 2008 pappenberger et al 2008 mishra et al 2009 neumann 2012 cosenza et al 2013 gamerith et al 2013 wainwright et al 2013 gan et al 2014 vanrolleghem et al 2015 sarrazin et al 2016 peer reviewed literature remains insufficient for proposing an efficient and robust ua parameter framework to assess the parameter identification problem i e model imperfection of complex water quality models efficient refers to the computational frugality of the ua process as well as the achievement of convergence with infinite iterations which can be realized by reducing the dimensionality robust indicates that the framework will correctly evaluate whether the true value of a certain parameter can be learned with this background this study organically combines the comprehensive gsa multi chain mcmc method and bayesian estimation to establish a new analytical framework with this framework the environmental fluid dynamics code efdc water quality module with a built in case for the lower charles river basin hamrick 1992 is used as a synthetic data case study to assess the parameter identification problem caused by the cognitive limitations 2 methods and materials 2 1 analytical framework the proposed analytical framework fig 1 aims to address the challenge of ua and to assess the rationality of the parameterized processes in complex water quality models at first the prior distribution of each parameter should be specified by combining past research experience previous comparable experiments and intuition or belief where subjective assumptions are sometimes inevitable the purpose of using a comprehensive gsa on the entire prior parameter hyperspace is to classify the parameters as important or unimportant insensitive if the contribution of a parameter to the model output uncertainty i e the sensitivity index si exceeds a predefined threshold α in at least one gsa technique the parameter is labelled as important otherwise the parameter is labelled as unimportant the si cut off α is determined from cumulative sis the venn diagrams in fig 1 will help researchers to intuitively understand the classification rules i e a union of sensitive parameters screened by each gsa technique independently represents an important ensemble in the ua process unimportant parameters are fixed at their true values table 2 i e parameter fixing and only a limited number of important parameters are allowed to vary i e parameter prioritization after the likelihood function that measures the discrepancies between the observed and predicted water quality time series is properly defined a multi chain mcmc method is applied to generate the posterior parameter distributions of the important ensemble according to the previous definition of parameter identifiability if the posterior distribution is not represented unimodally i e non peak distribution indicates impossibility of learning the true value whereas multimodal posterior distribution indicates that at least two nonadjacent high density zones lead to similar goodness of fit values but only one may contain the true value or is not represented with a diminished range then a parameter is not identifiable otherwise bayesian estimation be is used to assess whether the parameter can be correctly identified if the bayesian estimate is unbiased compared to the true value θ then the parameter is regarded as correctly identified otherwise the parameter is labelled as biasedly identified in addition some exceptional unidentifiable parameters could be indirectly identified through pairwise correlation analysis particularly if significant correlations are determined with a certain identifiable parameter the parameter true values are critical to the proposed framework in real data cases the parameter true values are often unknown making it difficult to discuss the parameter identification problem whereas for a synthetic dataset the parameter true values can be reasonably designated in addition the use of field monitoring datasets involves model input uncertainty and data scarcity due to current limitations of monitoring techniques which will interfere with analysis of parameter identifiability therefore a synthetic dataset would be an appropriate solution schoups and vrugt 2010 evin et al 2014 smith et al 2015 especially when the purpose of the research is to assess parameter identifiability i e to determine the rationality of the constituent hypothesis regarding water quality processes as is in this study for new cases without substantial previous work parameter true values are not readily available in such situations an appropriate value from the prior parameter range could be selected as the true value based on the parameter true values the observation dataset would then be generated from a simulated time series with corrupting gaussian noise that has a mean of zero and an independent constant deviation i e n 0 σ 2 a few complex residual structures have been reviewed by smith et al 2015 but a simple assumption for illustrative purposes will suffice here 2 2 comprehensive gsa previous researches campolongo and saltelli 1997 yang 2011 gamerith et al 2013 li et al 2013 zhan and zhang 2013 esmaeili et al 2014 gan et al 2014 vanuytrecht et al 2014 vanrolleghem et al 2015 sarrazin et al 2016 indicated that the sobol s variance based morris screening derivative based and src methods regression based were the three most frequently applied gsa methods to date these three methods have complementary advantages thereby recommended for comprehensive gsa in the analytical framework the sobol s method sobol 2001 decomposes the unconditional variance v of the model output into n th order conditional variances v i j eq 2 the proportion to the unconditional variance v i j v represents the contribution of the corresponding parameter or parameter interaction to model output uncertainty 2 v i v i i j i v i j v 12 n the morris screening method morris 1991 is based on the reiterative one factor at a time oat method first multiple trajectories are generated in the prior parameter space via latin hypercube sampling lhs ye 1998 for each trajectory the parameter is screened consecutively from θ 1 to θ m for each screened parameter oat method is then applied to calculate the elementary effect e e i expressed as eq 3 where δ i is a predefined slight disturbance of parameter θ i then each trajectory presents m 1 parameter realizations the mean value of ee i with all the trajectories is considered as the si for parameter θ i 3 e e i f θ 1 θ i δ i θ m f θ 1 θ i θ m δ i the src method saltelli et al 2004 is based on the assumed multivariate linear relationship between the model output and the parameter vector θ such that the model can be rewritten as eq 4 the regression coefficient b i is estimated via the least squares method lsm then a normal standardization is applied to make the coefficients comparable expressed as eq 5 the standardized regression coefficient s r c i in eq 5 is considered the si for parameter θ i 4 y i 1 n b i θ i b 0 5 s r c i b i ˆ σ θ i σ y 2 3 ua and parameter identification a multi chain mcmc method namely the dream was applied in this study this algorithm was introduced by jasper a vrugt and his colleagues in 2009 vrugt et al 2009 the matlab code is available on http faculty sites uci edu jasper software the likelihood function in eq 1 corresponding to the assumed gaussian homoscedastic and independent residual errors can be defined as eq 6 marshall et al 2004 liu et al 2008 smith et al 2015 considering zero observation of pollutant indicators could scarcely be monitored in reality zero inflation should not necessarily be included 6 p x θ j k 2 πσ 2 t 2 t exp x t j k o t j k 2 2 σ 2 where x t j k and o t j k are the simulated and observed time series of the j th state variable at the k th monitoring site respectively and t is the recording frequency the transition kernel of the dream consists of two components one component is the evolution of the candidate parameter vector liang et al 2016 7 z m θ m 1 e γ δ d i 1 δ θ r 1 i j 1 δ θ r 2 j ε where z is the candidate parameter vector m is the index of the chain θ is the present parameter vector δ is the number of pairs used to generate the proposed vector d is the dimensionality of the parameter hyperspace r 1 i r 2 j 1 n where n is the number of chains and r 1 i r 2 j m for i j 1 δ e is a random vector drawn from a uniform distribution on b b with b 1 ε is a random vector drawn from a normal distribution with a mean value of 0 and a standard deviation substantially smaller than the width of the target distribution and γ is the value of jump size that depends on δ and d 2 38 2 δd in eq 7 the markov chains are interactive with one another in co generating the candidate parameter vector if one chain falls into local optimum fortunately the other n 1 chains will drag it out of the trap therefore such a complicated transition kernel avoids premature convergence vrugt et al 2009 the second component of the dream transition kernel corresponds to the acceptance probability metropolis et al 1953 8 p θ t 1 m z m min p x z m π z m p x θ t 1 m π θ t 1 m 1 in eq 8 if the candidate parameter vector has a higher posterior probability to approximate the real behavior and conform to prior knowledge compared to the present parameter vector then the markov chain will step forward to the candidate position otherwise the candidate parameter has an acceptance probability equal to the posterior probability ratio of replacing the present parameter vector in the markov chain a detailed description of the dream can be found in the literature vrugt et al 2009 as is known parameter uncertainty is inevitable similar to other ua approaches results of the dream are posterior distributions rather than unique optimal values a unimodal posterior distribution is the best state for an uncertain parameter thereby qualitatively defined as small uncertainty the others are defined as large uncertainty if the posterior parameter distribution is unimodal with a diminished range then be will be applied otherwise be is not necessary the two most common bes are the posterior expectation θ ˆ e that corresponds to the minimum mean square error loss function and the posterior median θ ˆ md that corresponds to the least absolute value loss function eq 9 these two estimates often have similar results but the posterior expectation is susceptible to the extreme value hence the posterior median is recommended a parameter can be categorized as either correctly identified or biasedly identified by comparing the bayesian estimate and the true value whether they are located in the same high density zone 9 θ ˆ θ ˆ md l θ ˆ e θ ˆ θ θ ˆ e l θ ˆ e θ ˆ θ 2 2 4 brief introduction of the efdc water quality module the aforementioned methodology would be applied to a sufficiently complex water quality model namely efdc the efdc is a 3d multifunctional surface water modeling system that consists of hydrodynamic and mass transport sediment and contaminant transport and water quality modules hamrick 1992 this study focuses on the efdc water quality module which has been increasingly and extensively applied to assess water quality variations in surface water systems liang et al 2016 yi et al 2016 zhu et al 2016 the structure of the water quality module is illustrated in fig 2 in which 22 water quality state variables and their relationships are also depicted table 1 lists the definitions of each state variable the governing equation of the efdc water quality module with respect to mass conservation is expressed as eq 10 which is workable for all water quality state variables 10 m x m y h c t m y h u c x m x h v c y m x m y w c z x m y h a x m x c x y m x h a y m y c y z m x m y a z h c z m x m y h s c where u v and w are velocity components in the curvilinear orthogonal coordinates x y and z respectively a x a y and a z are turbulent diffusivities in x y and z directions h represents water column depth c is concentration of a water quality state variable s c is internal and external source sink term per unit volume of a water quality state variable m x and m y are square roots of the diagonal components of the metric tensor and m x m y is the jacobian or square root of the metric tensor determinant nine efdc water quality state variables were selected for ua they were bc bd bg nh4 no23 toc tp ton and do where toc is the sum of doc lpoc and rpoc tp is the sum of dop lpop rpop and po4t and ton is the sum of don lpon and rpon see table 1 for their corrupting gaussian noise a total of 54 parameters were involved as listed in table 2 2 5 synthetic data case study the study site is located in the lower charles river basin boston massachusetts from watertown dam to boston harbor fig 3 a this is a built in efdc case much effort has been expended to explore the performance of efdc using this case especially for model calibration and verification tetra tech inc 2005 the available information about model settings was instructive for this synthetic data case and the default parameter values were regarded as true values a few other efdc cases see appendix were also consulted to determine the prior parameter ranges as the ranges are bounded and no more statistical results about the site specific prior parameter distribution are accessible bayes hypothesis priors i e uniform distribution were assumed for illustrative purposes freni and mannina 2010 the results are listed in table 2 the grid is the basis of numerical computation in efdc fig 3 b and c depict the generalized grids of the studied river and the grid transformation from a curvilinear orthogonal coordinate system to a cartesian coordinate system before generating numerical solutions respectively the monitoring sites were located within the five grids marked in fig 3 c the default simulation time step of 30 s was used by substituting the parameter true values into the model the simulated time series of the state variables recorded at 0 3 h 18 min frequency from april 1st 1998 to june 30th 1998 a total of 7280 points in each time series excluding the warm up period were taken as the hypothetical observation dataset 3 results and discussion 3 1 summary of results fig 4 illustrates the parameter identification results after implementing the proposed analytical framework the sis of the sobol s morris screening and src methods were calculated following the methods of previous studies sobol 2001 morris 1991 saltelli et al 2004 for the sobol s and src methods convergence was assumed after 5000 bootstrap samplings efron 1981 the convergence of the morris screening method was assumed when the parameter space was gridded into a 10054 hypercube and lhs was repeated 100 times with an si cut off α of 1 for bc bd bg nh4 no3 and do the cumulative sis of the important parameters i e si 1 exceeded 95 and for toc tp and ton the cumulative sis of the important parameters exceeded 90 whereas the sis of the unimportant parameters i e si 1 were all below 0 5 therefore the si cut off α of 1 was reasonable a total of 39 important parameters were identified as sensitive by at least one gsa method the important parameters determined by all three gsa methods were referred to as completely sensitive labelled in the red dashed rectangles in fig 5 while the other important parameters determined by one or two gsa methods were referred to as partially sensitive the stacked bars in fig 5 represent the individual and cumulative contributions of each important parameter to the uncertainty of the corresponding state variable before running the dream the nine state variables were separated into four subsystems bc bd and bg toc tp and ton nh4 and no3 and do based on the distinct si patterns presented by the sobol s morris screening and src methods which will be discussed in the following sections moreover no two groups shared important parameters hence ua could be conducted separately for each subsystem during the ua process the important parameters screened out by the comprehensive gsa evolved according to the transition kernel of the dream while the unimportant parameters were fixed at their true values table 2 for example in the bc bd bg subsystem a total of 12 out of 54 important parameters including cchlx prrx wsx and trx varied while 42 out of 54 parameters remained constant the numerical experiments were repeated four times for each subsystem subsequently for each experiment eight markov chains were applied to the dream algorithm after 750 evolutions the mean and variance of each parameter of interest were assumed to be constant indicating that the dream has achieved convergence furthermore the subsequent 750 consecutive samples for each markov chain and experiment i e a total of 24 000 samples were used to plot the posterior parameter distributions a total of 18 parameters presented a unimodal posterior distribution with a diminished range fig 6 further classified as nine correctly and nine biasedly identified parameters through be the remaining 21 important parameters were classified as unidentifiable because of their multimodal or undiminished posterior distributions fig 7 presents three examples of this classification in addition no indirectly identified parameter was found in the pairwise correlation analysis 3 2 discussion of the gsa results 3 2 1 a mechanism perspective from a biochemical or physicochemical perspective most of the important parameters were directly related to the generation and extinction of the corresponding state variables the most complex subsystem the toc tp and ton subsystem was taken as an example and a similar analysis could be conducted for the other subsystems for toc four completely sensitive parameters were determined trhdr kthdr krc and klc table 3 these findings indicated that krc and klc excluding kdc were the leading factors that determined the concentration of toc remaining in the water body this scenario can logically be explained by biochemical mechanisms among the three forms of organic carbon dissolved refractory particulate and liable particulate forms dissolved organic carbon is easily utilized in respiration by heterotrophic microorganisms thus dissolved organic carbon is mineralized for hydrolysis more readily than particulate organic carbon therefore the hydrolysis of particulate organic carbon is a rate determining step characterized by the krc and klc parameters similar to toc all the three gsa methods identified the two temperature related parameters trhdr and kthdr as being sensitive to tp and ton this finding is attributed to the governing equation of the same form that controls the hydrolysis process expressed as eq 11 11 k h k d p i k a p i e k t h t t r h where k h is the actual hydrolysis rate of particulate organic matter k d p is the minimum dissolution rate of particulate organic matter k a p i is a constant that relates the hydrolysis of particulate organic matter to algal i biomass k t h i e kthdr measures the effect of water temperature on hydrolysis t is water temperature and t r h i e trhdr is the reference temperature for hydrolysis refer to tetra tech inc 2007 p34 for details in addition to the temperature and hydrolysis rate parameters other factors related to algae such as khpx and ancx also significantly influenced tp and ton concentrations toc was excluded for ton ancc and ancg had importance rankings higher than kln and krn and were also identified as completely sensitive parameters while kln and krn were insensitive in the morris screening method the reason for the screening out of a few algae related parameters was that phosphorus and nitrogen could be directly utilized by algae rather than organic carbon because they are autotrophic microorganisms 3 2 2 comparisons among different gsa methods for each state variable at least one completely important parameter was identified by the venn diagrams fig 1 detailed information on these parameters is listed in table 3 distinct patterns were presented for each state variable by the sobol s morris screening and src methods fig 5 these patterns included the si contributed by the most sensitive parameter the cumulative sis of completely sensitive parameters the partially sensitive parameters that were screened out and the importance rankings however a few common points were noticeable when exploring these distinct patterns the morris screening method was the most rigorous and conservative of the three gsa methods this method highlighted the importance of the most sensitive parameter and was less prone to type ii errors with the exception of tp and do the morris screening method yielded the highest cumulative sis of the completely sensitive parameters for each state variable and excluding nh4 and no3 the morris screening method yielded the highest si for each state variable see the red bold digits in table 3 however this method was incapable of identifying partially sensitive parameters therefore the morris screening method led to the complementary results of the sobol s and src methods compared with the morris screening method the sobol s and src methods exhibited better performance at identifying partially sensitive parameters thus they were less likely to make a type i error moreover the results of the sobol s and src methods were often in good agreement with one another in terms of ranking the important parameters similar patterns i e almost the same important parameters appeared in the stacked histogram the same colored bars had similar lengths and important parameters were ranked in similar orders fig 5 were presented by these two gsa methods especially for toc tp and ton this indicates that the src method could be substituted for the sobol s method in some situations particularly when computational efficiency is considered as the sobol s method is more computationally taxing than the src method 3 3 discussion of the ua and parameter identification results 3 3 1 overview of parameter identification problem among the 54 parameters only eighteen i e 1 3 parameters presented a unimodal posterior distribution with a diminished range nine of which i e 1 2 were correctly identified indicating serious parameter identification problems the first issue is the be bias a typical example is the kro parameter as it presented a substantially exaggerated bias i e the bayesian estimate 4 53 is almost three times as large as the true value 1 50 fig 6 compared with the ktr parameter in the same do subsystem according to eq 12 the parameters ktr and kro jointly control the entire reaeration procedure during the simulation period the water temperature mostly remained below 20 c thus k r is a rigorously increasing function of kro and a rigorously decreasing function of ktr to maintain a constant k r an increase in ktr must accompany an increase in kro considering that ktr accounted for over 90 of the do variance and was much more sensitive than kro fig 5 a ktr estimate 1 064 that was slightly larger than the true value 1 024 would therefore multiply kro to counteract the increasing k r trend 12 k r k r o u e q h e q w r e a k t r t 20 where k r is the actual reaeration rate of do t is the real time water temperature u e q and h e q are the weighted velocity and weighted depth over a cross section respectively and w r e a describes the wind induced reaeration which can be estimated using wind speed refer to tetra tech inc 2007 p49 for details similar logic could be applied to the parameters trhdr and kthdr the only difference is that these two parameters were almost equally sensitive and had a relatively small bias compared to ktr and kro both trhdr and kthdr tended to be overestimated fig 6 according to eq 11 a larger kthdr k t h tends to result in a lower particulate organic matter hydrolysis rate whereas a larger trhdr t r h tends to result in a higher hydrolysis rate in the synthetic data case the effects of the biased estimates coincidentally offset each other yielding consistent outputs the other biasedly identified parameters could be explained in the same way as the two examples presented in the previous paragraphs although explaining certain parameters would be relatively laborious however the scenario in which the effects of a group of biased estimates offset one another cannot be guaranteed when applying bayesian estimates for forecasting a future state moreover because model users cannot always know the parameter true values in advance whether the concentrations of state variables will be overestimated or underestimated is unpredictable the second issue is the multimodal posterior distributions such as those of the parameters trc krc and tnit this situation would lead to a bayesian estimate located outside the high density zones fig 7 and raise a choice dilemma for model users a few studies freni and mannina 2010 tang et al 2016 indicated that the selection of prior knowledge including prior distributions and value ranges can impact the posterior distributions of sensitive parameters although prior knowledge may be less important if the available data are sufficient given that the prior distributions were uniform in this synthetic data case freni and mannina 2010 the multimodal posterior distributions must result from the likelihood function which had a direct relationship with the model structure and was non monotonic with respect to these parameters as several locally optimal points existed within the prior value ranges in that case the selection of appropriate prior value ranges i e no existing locally optimal values for these parameters is important for model users unfortunately further narrowing the prior value ranges would not be easy because it requires additional field experiments and parameter investigation 3 3 2 causes of parameter identification problem apart from the cognitive limitations many studies beck 1985 müller et al 2002 swameye et al 2003 have cited data problems including model input uncertainties and data scarcity as one of the reasons why parameter identification is difficult for model input uncertainties monitoring data from different sources e g meteorological data pollution source data and water quality data have different scales of uncertainties by the synthesis of output time series these uncertainties could be eliminated for the data scarcity problem at the temporal scale the case of the lower charles river basin used high frequency time series of multiple state variables for analysis which is almost impossible to obtain through field monitoring at the spatial scale the synthetic data case monitored the average concentration of a pollutant indicator throughout the entire grid while during real monitoring only one or a few points were monitored to represent the states of the entire grid because of these two reasons data scarcity was considerably weaker in the synthetic data case than in any real data cases this study added further two hypothetical monitoring sites grids 3 33 and 3 40 in fig 3 c increased the recording frequency from 0 3 h to 0 1 h and retested parameter identifiability with respect to the nh4 and no3 subsystem as a supplement unfortunately the parameters tnit and knit2 still did not present unimodal posterior distributions in this respect when observation data are extremely scarce a potential approach to address the parameter identification problem is to intensify the monitoring frequency however when the monitoring intensity is enhanced to a certain degree data scarcity is no longer the primary cause of difficulty in parameter identification because synthetic datasets were able to eliminate or well control the impacts of data problems the fundamental causes of the parameter identification problem could be entirely traced to the cognitive limitations of the modelers regarding water quality processes in this study the framework outlined in this study is then able to identify these cognitive limitations indicating a direction of model improvement for model developers 3 3 3 tri variate relationships among sensitivity uncertainty and identifiability tri variate relationships among sensitivity uncertainty and identifiability were gradually elucidated through exploring the ua results fig 8 the analytical framework established based on these relationships is therefore logically reasonable first a sufficient but unnecessary condition is given for low sensitivity to be large uncertainty low sensitivity suggests that parameter variation has almost no effects on model output resulting in extensive equifinality consequently the posterior parameter distribution will be almost uniform which further causes large uncertainty however low sensitivity cannot be determined from large uncertainty the parameters shown in fig 7 represent good counterexamples all of these parameters have multimodal posterior distributions the gsa categorized them as completely sensitive parameters yielding the peaks in their posterior distributions but the ua regarded them as largely uncertain ones because of multiple locally optimal points existing in the likelihood functions which yielded similar goodness of fit razavi and gupta 2015 have provided a few more illustrative examples on this issue the second proposition is that large uncertainty is a sufficient but unnecessary condition for poor identifiability in addition to the unimodal posterior distribution with a diminished range a correctly identified parameter requires an unbiased bayesian estimate therefore if a parameter presents large uncertainty i e the posterior parameter distribution is not unimodal it is poorly identifiable in turn if the bayesian estimate is significantly biased from the true value a parameter with small uncertainty will be regarded as biasedly identified the aforementioned two propositions yield the third proposition low sensitivity is a sufficient but unnecessary condition for poor identifiability consistent with the connotations underlying the definitions of identifiability omlin et al 2001 brun et al 2002 fig 8 b is the inverse negative proposition of fig 8 a thus the tri variate relationships illustrated in fig 8 b are also maintained 3 3 4 robustness of the bayesian framework robustness means that the framework will form a correct judgment on whether the parameter true values can be learned the reasons that the outlined framework here is robust lie in the following aspects the first aspect corresponds to the integration of comprehensive gsa into dimensionality reduction for example the parameters prrx and wsx as illustrative examples were partially sensitive i e simorris 1 fig 5 however they presented unimodal posterior distributions despite being biasedly identified fig 6 indicating small uncertainty if these two parameters were fixed at their default values during the ua process after only morris screening method was applied as simorris 1 then the posterior distributions of the other important parameters in the same subsystem would be distorted in fact the parameters prrx and wsx should be sensitive although morris screening method filtered them out based on the relationships between sensitivity and uncertainty in contrast if the values of an insensitive parameter were random during the ua process then the ua results would not significantly vary thus this study presumed that admitting a partially sensitive parameter into the ua process would be better than neglecting an important parameter this is another reason for using comprehensive gsa which has not yet been mentioned in other sa studies the second aspect is related to the application of the multi chain mcmc method local optimum is a common issue in complex water quality models if the markov chain is trapped into the local optimum during its evolution then a few unidentifiable parameters that should present multimodal posterior distributions e g fig 7 may present unimodal distributions and thus be mistakenly categorized as correctly or biasedly identified ones this situation would cause modelers to mistakenly believe that the water quality processes characterized by these parameters are reasonable and relevant improvements are not necessary fortunately multi chain mcmc methods can avoid premature convergence as revealed by vrugt et al 2009 4 conclusions considering parameter identification problems of complex water quality models derived from the cognitive limitations regarding water quality processes i e model imperfections a bayesian analytical framework was developed based on the tri variate relationships among sensitivity uncertainty and identifiability to assess parameter identifiability in a high dimensional parameter space framework efficiency was achieved through dimensionality reduction i e gsa the involvement of comprehensive gsa and dream guarantees the framework to yield correct posterior distributions for further assessment of parameter identification problems a high frequency synthetic efdc dataset in which model input uncertainty and data scarcity problem could be eliminated i e the fundamental causes of parameter identification problems could be entirely traced to cognitive limitations was used to illustrate how the framework discerned model imperfections among the total 54 parameters in the efdc water quality module only nine important parameters were identified without significant bias therefore efdc users should treat bayesian estimates with extreme caution when applying these results in future forecasts in the charles river case for example if researchers aim to predict the variations in toc top ton concentrations from april to june in the following years the bayesian estimates of trhdr and kthdr obtained from field monitoring data should be adjusted to be smaller because both the parameters trhdr and kthdr tended to be overestimated within the same period the outlined analytical framework is a powerful tool to determine the potential cognitive limitations of real behaviors in aquatic ecosystems specifically model imperfections exist in the water quality process that is characterized by the parameters that cannot be correctly identified e g the model may include redundant relationships model imperfections encourage the modellers to perfect their models with the help of the framework for the model users it is recommended using the framework to conduct an initial synthetic data case study in the research site before parameter estimations in real world settings the following are two potential applications 1 to assess whether each of the important parameters will be overestimated or underestimated in the synthetic data case and then to seek an acceptable adjustment of be results in the real data case 2 to assess whether each of the important parameters will present a multimodal posterior distribution in the synthetic data case and then to decide whether a corrected prior distribution should be included in ua in the real data case although current lack of online monitored high frequency water quality datasets restricts the second step application in real world settings future studies are promising due to continuous improvement of monitoring techniques e g many water quality online monitoring stations are being constructed during the 13th five year plan in china analysis of parameter sensitivity uncertainty and identifiability is always an interesting topic in the field of environmental modeling especially for complex water quality models because we still need to address the challenges resulting from parameter identification problems although this study has tried to clarify the tri variate relationship among parameter sensitivity uncertainty and identifiability within their qualitative definitions it is still not intuitive for researchers to understand the differences among these three terms by only using the definitions or by giving a few counterexamples future work may develop some indices to quantitatively characterize their common points and differences to facilitate further research on this topic acknowledgements this research was supported by the national water pollution control special project no 2017zx07205003 and tsinghua fudaoyuan research fund the authors greatly thank the editors and reviewers for providing valuable suggestions on perfection of the methodology appendix this study has consulted 19 papers for determining prior parameter value ranges cerco and cole 1993 in chesapeake bay odeq 1996 in lake fort cobb tetra tech inc 1998 in peconic estuary park et al 2005 in kwang yang bay tetra tech inc 2005 in charles river jin et al 2006 in lake okeechobee lin et al 2007 in cape fear river xia et al 2010 in caloosahatchee river estuary lin and li 2011 in neuse reservoir xia et al 2011 in perdido bay wu and xu 2011 in daoxiang lake wan et al 2012 in st lucie estuary kang and jang 2013 in hwaseong reservoir tang et al 2014 in shenzhen reservoir wang et al 2014 in miyun reservoir chen et al 2016 in danjiangkou reservoir gong et al 2016 in tianyinhu lake liang et al 2016 in miyun reservoir and yi et al 2016 in lake dianchi 
26393,providing reliable reservoir water level forecasts is a challenge because of the accumulative errors in hydrological and reservoir routing models we present a novel forecasting model that addresses these issues the model consists of a hydrological model to simulate inflow a reservoir routing model to simulate water levels and an autoregressive model for error correction the parameters for the hydrological model were calibrated with the objective of forecasting water levels over multiple lead times while a back fitting algorithm was used to recalibrate the parameters sequentially for the hydrological and autoregressive models the results show that 1 the forecasting performance of effective lead times can be enhanced by minimizing the difference between the forecasted and observed water levels for multiple lead times 2 the most recent errors method is better than the one step ahead recursive prediction method and 3 the back fitting algorithm is superior to the joint inference method keywords flood forecasting lead time objective function xinanjiang model software availability name of software hirrof bf 1 developer xiaojing zhang pan liu contact address state key laboratory of water resources and hydropower engineering science wuhan university wuhan 430 072 china telephone fax and email numbers liupan whu edu cn year first available 2017 hardware required personal computer software required ms windows availability and cost free available at https pan baidu com s 1o7zgvfo program language fortran program size 95 8 mb downloaded zip file 1 introduction reliable reservoir inflow and water level forecasts contribute to efficient reservoir operations which play a key role in flood control hydropower generation and water supply gragne et al 2015 liu et al 2015a however temporal and spatial variations in climate as well as complex physical processes mean that forecasting reservoir water levels remains a complicated and challenging task chang and tsai 2016 data driven and physically based models are two basic approaches for forecasting reservoir water levels the data driven approaches include statistical and artificial intelligence ai models statistical methods include autoregressive ar autoregressive moving average and autoregressive integrated moving average arima models wang et al 2015 these data driven approaches offer easy implementation but ignore the nonlinearity of hydrological series das et al 2016 sun et al 2016 although ai models such as artificial neural networks anns and fuzzy inference gholami et al 2015 seo et al 2015 taormina and chau 2015 can solve the nonlinear problem they do not represent the hydrological process chen et al 2015b but only depend on training data which decreases their credibility as a result physically based models especially conceptual models are widely used because they are both easy to understand and effective fang et al 2017 to forecast reservoir water levels in the context of physically based modeling the inflow should be simulated as the input to the reservoir routing model however the observed reservoir inflow used in hydrological models is generally inaccurate because the discharge from the tributaries of the reservoir is hard to measure deng et al 2015a to address this issue observed water levels can be employed as the fitting target of a forecasting model that integrates hydrological and reservoir routing ihrr models deng et al 2015b the advantage of ihrr models is that the hydrological parameters can be calibrated with more reliable raw data thus improving the forecast accuracy to provide essential information for reservoir real time operation it is very important that accurate and reliable water level forecasts over multiple lead times are obtained however ihrr has a limited ability to meet these requirements because 1 its applicability in forecasting over multiple lead times has not been validated 2 inherent forecast errors are not corrected in real time to improve lead times for forecasting a direct approach is to forecast future rainfall such as typhoon characteristics or remotely sensed variables chang and tsai 2016 jhong et al 2016 lin et al 2010 however in areas without reliable rainfall forecasting an alternative approach is to adopt the forecast streamflow from multiple lead times as the objective function assuming the future rain is zero for the purpose of reducing the forecast errors this improvement is feasible because the hydrological parameters can be adjusted to enhance the efficiency of longer lead times in the calibration procedure the error correction method can mitigate the forecast uncertainty problems van steenbergen et al 2012 yan et al 2012 owing to the difficulties in clarifying the errors from different sources and their interactions it is preferable to consider the overall error deng et al 2015a 2015b i e directly correcting the difference between the forecast and observations the existing literature includes many studies on overall error correction for example xiong and o connor 2002 compared four error forecasting models including an ar autoregressive threshold a fuzzy autoregressive threshold and an ann model they demonstrated that the ar model is efficient despite its simplicity similarly goswami et al 2005 evaluated eight error updating methods while liu et al 2016 compared three error correction techniques these methods are a type of post processor a feasible alternative method is joint inference in which the parameters of the hydrological and error correction models are calibrated simultaneously for example li et al 2016 applied a restricted ar model to normalized errors and jointly calibrated all the parameters using shuffled complex evolution although the joint inference method can theoretically find the optimal global parameters it incurs a heavy computational burden as the back fitting algorithm is a simple iterative procedure for fitting a generalized additive model sorokina et al 2007 the back fitting algorithm only focuses on either the parameter set of the hydrological model or the auto regression model during each recalibration in contrast the joint inference method simultaneously calibrates all the parameters in cases where the number of parameters is very large the back fitting algorithm will significantly reduce the heavy computational burden this study integrates the ihrr with the objective function of minimizing the difference between the simulated and observed water levels over multiple lead times and error correction methods and develops a real time forecasting method for reservoir water levels our aim is to improve the accuracy of multiple step ahead forecasting compared with the ihrr the method has two improvements 1 the forecast streamflow for multiple lead times is derived simultaneously to provide longer flood warnings 2 the back fitting algorithm is used for multiple step ahead error correction to reduce the heavy computational burden this paper is organized as follows in section 2 the details of the methodologies are described and the performance evaluation criteria are provided a case study focused on the shuibuya reservoir is presented in section 3 followed by the results and discussion in sections 4 and 5 finally the conclusions are drawn in section 6 2 methodology as shown in fig 1 the proposed method has two steps 1 the xinanjiang model and reservoir routing model are integrated to simulate the water levels over multiple lead times with the objective function of minimizing the difference between the simulated and observed water levels 2 back fitting based error correction is used to estimate the errors over multiple lead times and recalibrate the parameters to find the overall global optimal parameters for the hydrological and error correction models 2 1 simulating water levels over multiple lead times 2 1 1 integrated hydrological and reservoir routing models the model for simulating the water levels integrates the xinanjiang and reservoir routing models deng et al 2015b the xinanjiang model is used as the rainfall runoff model because it is the most popular conceptual rainfall runoff model in china wu et al 2017 zhao 1992 it has been widely used for streamflow simulation jayawardena and zhou 2000 yao et al 2014 e g in the china national flood forecasting system wmo 2011 a schematic overview of the model is presented in fig 2 the model inputs are precipitation and evaporation and the simulated streamflow is calculated using four main modules 1 evapotranspiration 2 runoff generation 3 runoff separation and 4 flow concentration the 15 parameters in the xinanjiang model are defined in table 1 when evaporation seepage and other water losses are ignored the reservoir routing model based on the water balance equation zhang et al 2016 2017 can be written as 1 v t 1 v t i t 1 o t 1 δ t where δ t is the time interval v t v t 1 are the initial and final reservoir storage volumes at time t 1 respectively and i t o t are the inflow and release of the reservoir during time period t 1 respectively the inflow of the reservoir is obtained by the xinanjiang model and the release which contains flows for electricity generation irrigation water supply and ecological generation is pre determined by reservoir operating rules liu et al 2014 the simulated water levels are then derived via equation 1 and the reservoir stage storage curve liu et al 2015b pan et al 2016 2 1 2 objective function of the ihrr the objective of the conventional water level simulating model is to minimize the sum of squares of the difference between the simulated and observed water levels with the aim of making the simulated series match the observed one 2 min f c o n t 1 n z t z t 2 where z t and z t are the observed and simulated water levels at time t and n is the length of the data series however the forecast water levels for longer lead times are not considered a modified objective function is developed by adopting the difference between the simulated and observed values at longer lead times assuming that the future rainfall is zero this is expressed as follows 3 min f p r o t 1 n z t k z t k t 2 z t k z t k t 1 2 z t k z t k t k 1 2 where z t k t is the simulated water level at time t for time t k and z t k is the observed water level at time t k to further define the variables in equation 3 a schematic illustration is presented in fig 3 chen et al 2016 the columns represent the time at which the simulations were performed and the rows represent the time for which each simulation was made assume that the current time is t and the simulated water levels z t 1 t z t 2 t z t k t are determined simultaneously at t similarly the number of simulated water levels for time t k 1 is k including z t k 1 t 1 z t k 1 t z t k 1 t k 2 we can combine the genetic wang 1991 rosenbrock 1960 and simplex algorithms nelder and mead 1965 to calibrate the model in which the genetic algorithm is used first and the calibrated parameters are subsequently used as initial values for the rosenbrock algorithm the results of the rosenbrock algorithm are treated as initial values for simplex algorithm and the final parameter values are optimized by the simplex algorithm li et al 2010 liu et al 2016 yan et al 2014 the similar combined optimization algorithms have been used in literature and is proved to be an effective tool for parameter calibration hartnett and diamond 1997 moore et al 2008 the population number and the number of function evaluations are 100 and 3500 in the genetic algorithm optimization respectively the number of function evaluations is 3500 in the rosenbrock optimization the number of function evaluations is 3500 in the simplex optimization 2 2 error updating based on the back fitting algorithm 2 2 1 error estimation procedure one of the most general features of forecast errors is autocorrelation the description of autocorrelation in the error estimation model is both justified and necessary evin et al 2014 a substantial number of investigations have indicated that ar modeling is an effective approach for addressing autocorrelation and improving forecast accuracy chen et al 2015a wu et al 2012 xiong and o connor 2002 schaefli et al 2012 and bennett et al 2016 indicated that the simple ar model is sufficient hence the ar model is adopted in the study and depends on the autocorrelation of the antecedent forecast errors in multistep ahead error correction for longer lead times this begs the question of which antecedent forecast errors should be used a strong correlation was generated between the errors for a forecast with a lead time of k h issued 1 h ago and the errors for a forecast with a lead time of k hours issued now nevertheless the actual errors for a forecast with a lead time of k h that was issued 1 h ago are hard to obtain the feasible alternative is to use the estimated errors however deviations between the estimated and actual errors are inevitable and will be significant as the lead times increase another feasible alternative is to use the error for a forecast with a lead time of k h that was issued k h ago the error that was not the most strongly correlated but the most recently known these two error estimation procedures are considered in this study it should be noted that owing to involving different information from antecedent errors autoregressive parameters differ between two error estimation procedures i e using recursively estimated errors and the most recently known error respectively 2 2 1 1 using the most recently known error to take advantage of the most recent observations and simulations the ar model for longer lead times can be expressed as 4 ε t k t ϕ k 1 ε t t k ϕ k 2 ε t k t 2 k ϕ k p ε t p 1 k t p k where ε t k t is the estimated error of forecast z t k t and ε t t k is the actual error of the antecedent forecast water level z t t k ϕ k 1 ϕ k 2 ϕ k p are the ar model parameters for lead k error estimation p is the order of the ar model with low order ar more applicable to short term forecasts lundberg 1982 wu et al 2012 hence p is assumed to be of order 1 in this study as shown in fig 4 a the current time is set as t and then the error ε t k t of the simulated water level at time t for time t k is estimated because the most recent observation is z t the most recently known errors are ε t t 1 ε t t 2 ε t t k instead of using the estimated value the error that is most recent and closest to ε t k t is adopted in the ar model namely the actual error ε t t k the estimated errors are then added to the simulations to obtain the corrected forecast water level 2 2 1 2 using the recursively estimated errors regarding the lack of real time observed values the ar model when k is greater than 1 using the recursively estimated errors can be written as 5 ε t k t φ k 1 k ε t k 1 t 1 φ k 2 k ε t k 2 t 2 φ k p k ε t k p t p where ε t k t is the estimated error of the simulated water level z t k 1 t and ε t k 1 t 1 is the antecedent estimated error of the simulated water level z t k 1 t 1 φ k 1 k φ k 2 k φ k p k are the ar model parameters for lead k error estimation using the estimated errors ε t k 1 t 1 ε t k 2 t 2 ε t k p t p and p is again assumed to be of order 1 ε t k 2 t 2 ε t k 3 t 3 ε t k p 1 t p 1 can be obtained by recursively using the ar model in a similar fashion to equation 5 in fig 4 b the definitions of the row column t and ε t k t are the same as in fig 4 a the estimated error ε t k 2 t 2 for the simulated water level z t k 2 t 2 is used as the input for estimating the error at the next time step denoted as ε t k 1 t 1 and so on for each estimation in the lead k time the ar model is used to estimate the errors of the simulated water level in the adjacent columns autoregressive parameters can be calibrated by the least squares approach the yule walker method and the maximum likelihood estimation method according to the literature the least squares approach has been proven to be efficient liu et al 2016 and was chosen for the present study however in some applications offline calibration cannot adjust the parameters according to the variation in the errors phuoc khac tien and chua 2012 and so the model is not adaptive to real time forecasts in this study the least squares approach applied to the methods described in sections 2 2 1 1 and 2 2 1 2 is an online technique based on the most recent errors this means the parameters of the ar model can be adjusted along with the updated error time series 2 2 2 recalibration based on the back fitting algorithm in the conventional error correction procedure the hydrological model is firstly calibrated with the observed streamflow then the difference between the simulated and observed is input into the error estimation model however the sum of the simulated streamflow and the estimated error may not be the closest value of the observed streamflow alternatively a joint inference method combines the hydrological model with the ar model and estimates the global optimal parameters of both models li et al 2015 prakash et al 2014 to reduce the computational burden brought by the additional parameters the back fitting algorithm is introduced to the hydrological and ar model the back fitting algorithm buja et al 1989 hastie 1990 is an iterative procedure to estimate the additive components for reducing computations in the back fitting algorithm for an additive model one component is estimated while the other components are fixed for each step the algorithm proceeds component by component until convergence ansley and kohn 1994 the reservoir water level forecasting model has two components water level simulation by the ihrr model and error estimation by the ar model an additive system we therefore used the back fitting algorithm based iterative procedure the procedure requires a starting point the starting point can be set to the first calibration of the ihrr model the recalibration procedure is identical for two different error estimation procedures and consists of two steps 1 hydrological model recalibration assuming that the estimated errors over multiple lead times are determined and fixed but the hydrological parameters are unknown the difference between the observed water levels and the estimated errors can be used as training data for the xinanjiang model the simulated water levels over multiple lead times are then obtained by the recalibrated xinanjiang model 2 ar model recalibration as the recalibrated simulated water levels over multiple lead times have been determined the errors i e the difference between the observed and simulated water levels are known the recalibrated ar model then outputs the estimated errors for multiple lead times as the fixed values in phase 1 the back fitting based recalibration cycles through the two models until the iterations number saying 50 in the study is reached 2 3 model evaluation a multi criteria assessment was implemented to evaluate the model performance although the nash sutcliffe efficiency coefficient nse is widely employed in streamflow forecasting the difference between water level evaluations using the nse is not significant hence three other indexes were adopted let x t be the observed value at time step t and x t t k be the simulated forecast or corrected value issued at time step t k 1 root mean square error rmse lin et al 2009 lin and wu 2011 6 r m s e k 1 n t 1 n x t x t t k 2 where k is the specified lead time and n is the length of the forecast water level data series 2 mean absolute error mae budu 2014 lin et al 2009 7 m a e k 1 n t 1 n x t x t t k 3 probability of the forecast error exceeding a given threshold α shen et al 2015 wu et al 2012 8 p k α p k x t x t t k α 1 n t 1 n φ x t x t t k α 100 where φ y 1 if y is true otherwise φ y 0 the three evaluation criteria are able to test different aspects of the model efficiency both r m s e k and m a e k evaluate the deviation of the forecast errors but r m s e k suffers a greater impact from large errors p k α is the success rate of the forecast for a specified α smaller values of r m s e k and m a e k indicate better forecast accuracy whereas higher values of p k α suggest a more robust model 3 case study a case study was conducted to demonstrate the application of the proposed method first the location and data used in the case study are introduced followed by a description of the seven schemes designed for the experiment finally the results are obtained and analyzed 3 1 location and data the proposed method was used to forecast water levels in the shuibuya reservoir see fig 5 located in hubei province the shuibuya reservoir is the first reservoir in a multi reservoir cascade system in the middle and lower reaches of the qing river the qing river is one of the main tributaries of the yangtze river and has a total length of 432 km and a drainage area of 17 600 km2 li et al 2014 liu et al 2011 because it is the most upstream reservoir shuibuya inflow is not impacted by other reservoirs hence shuibuya reservoir can be used as a typical case for real time reservoir water level forecasts the shuibuya reservoir watershed is covered by evergreen and deciduous broad leaved and mixed forest across humid and semi humid regions controlled by a saturated runoff mechanism deng et al 2015b hence the xinanjiang model is an appropriate choice for simulating streamflow in the application the reservoir has a basin area of 10 860 km2 with 4 58 billion m3 storage capacity the normal water level is 400 m and the corresponding water storage is 4 31 billion m3 fourteen rainfall gauge stations four pan evaporation stations at lichuan yuxiakow xuanen and yeshangwan and one water level station shuibuya spread over the basin the hourly data concerning precipitation evaporation reservoir release and water level were collected from theses gauging stations during february 2011 to february 2012 8760 h areal precipitation was obtained by the thiessen polygon method considering spatial variation huang et al 2016 wu et al 2017 areal evaporation was provided by using the average value of the four measured evaporation stations reservoir release data were pre determined by reservoir operation rules water level data were provided by the shuibya station at the basin outlet the observed water levels are representative ranging from low levels e g 363 44 m to high levels e g 397 65 m since all the data were collected after the reservoir built impacts of the dam is consistent the data from february 2011 to september 2011 were used for model calibration and data from october 2012 to february 2012 were used for model validation due to arbitrary initializing the soil water condition a warm up period was used cole and moore 2008 nicolle et al 2014 the errors are accumulative for the long lead times in the reservoir routing model hence the effective lead time is set as six hours 3 2 description of the experiments fig 6 lists seven schemes considered in the case study besides the conventional model ihrr the variants ihrrof ihrrbf 1 ihrrbf 2 ihrrof bf 1 ihrrof bf 2 and ihrrof ji 1 were used for comparative purposes in these models of denotes a modified objective function bf 1 and bf 2 denote additional error updating based on the back fitting algorithm using the most recently known errors fig 4 a and the recursively simulated errors fig 4 b respectively and ji 1 denotes additional error updating using the conventional joint inference method each of the seven schemes has 16 parameters including 15 in the xinanjiang model and one parameter in the ar model firstly the impact of the objective function on the forecasting for longer lead times is investigated based on the comparison of the ihrr and the ihrrof or the ihrr bf 1 vs ihrrof bf 1 ihrr bf 2 vs ihrrof bf 2 secondly which antecedent errors should be used in ar model is suggested by the comparison of the ihrr bf 1 and the ihrr bf 2 or ihrrof bf 1 vs ihrrof bf 2 and finally the superiority of the bf method to the ji method is studied from comparison of the ihrrof bf 1 and ihrrof ji 1 4 results figs 7 10 compare the forecasts given by the seven schemes at lead times of 1 6 h for both calibration and validation periods in terms of r m s e k m a e k and p k α the given threshold α of p k α was set to 0 1 m or 1 m in the schemes without error updating whereas it was set to 0 01 m or 0 05 m in the schemes with error updating 1 ihrr vs ihrrof the impact of the objective function on the forecasting for longer lead times in the calibration period it can be seen that ihrr outperforms ihrrof for a 1 h lead time whereas it is inferior for 2 6 h lead times this is essentially because the modification of the objective function takes the additional forecasts at 2 6 h lead times into account however the same results are not observed in a comparison of the validation period from the results in figs 7 9 ihrrof outperforms ihrr for all 1 6 h lead times as demonstrated by the lower r m s e k m a e k and higher p k α the results indicate that schemes with the modified objective function are more effective in the validation period fig 11 a illustrates the performance for a 1 h lead time in the calibration period it can be seen that ihrrof tends to overestimate the water level in the validation period the forecasted water level using ihrrof is higher than that of the ihrr however the water levels forecast by both models are smaller than the observed values because of the accumulative errors from the reservoir storage simulation that assumes the future rainfall is zero see fig 11 b 2 ihrrbf 1 vs ihrrbf 2 comparison between the error estimation procedure adopting the recently known errors and that adopting the recursively simulated errors given that the errors usually show intensive autocorrelation the schemes with error updating provide a great improvement over ihrr and ihrrof suggesting the need for integration with error updating methods however the differences between ihrrbf 1 and ihrrbf 2 are small as both schemes include error updating comparing the two it can be seen that ihrrbf 1 generally outperforms ihrrbf 2 with the exception of the forecast for a 1 h lead time because ihrrbf 2 forecasts errors without any estimation at the 1 h lead time instead using the most recent errors furthermore the difference between ihrrbf 1 and ihrrbf 2 increases with the lead time demonstrating that forecast errors in the error updating method of ihrrbf 2 accumulate faster than those of ihrrbf 1 a similar conclusion can be reached in the comparison between ihrrof bf 1 and ihrrof bf 2 which further indicates that using the most recently known errors in the ar model provides a better fit than that of the recursive simulations in both the calibration and validation periods 3 ihrrof bf 1 vs ihrrbf 1 impact of combining the objective function for forecasting longer lead times with back fitting based error correction in the calibration period it can be seen that r m s e k and m a e k for ihrrof bf 1 are actually larger than those of ihrrbf 1 which is in contrast to the comparison results between ihrr and ihrrof this situation persists in a comparison of ihrrbf 2 and ihrrof bf 2 the comparison in terms of p k α shows a similar trend but the schemes with modified objective functions gradually produce better forecasts as the lead time increases as demonstrated by the higher p k α values of ihrrof bf 1 ihrrof bf 2 at 4 6 h lead times however in the validation period ihrrof bf 1 ihrrof bf 2 are superior to ihrrbf 1 ihrrbf 2 at all six lead times which suggests that the modification of the objective function is efficient in terms of improving the water level forecasts for longer lead times it can be inferred that adopting longer lead times in the objective function allows the forecast model to learn from the non rain situation and reduce the inherent errors of ihrr 4 ihrrof bf 1 vs ihrrof ji 1 comparison between the back fitting based and joint inference recalibration or calibration methods it can be seen that ihrrof bf 1 outperforms ihrrof ji 1 for all six lead times during both the calibration and validation periods the difference between ihrrof ji 1 and ihrrof bf 1 lies in their parameter estimation methods ihrrof ji 1 uses the conventional joint inference method whereas ihrrof bf 1 calibrates the parameters of the hydrological and error updating models in sequence which reduces the computational burden of recalibration it can further be inferred that the recurrent recalibration procedures provide more chances to project the parameters into the near optimal region 5 discussion for reservoir water level forecasting relying on reliable raw data ihrr integrates the hydrological model reservoir routing model and the observed water level criteria based on ihrr this study develops a real time forecasting method by augmenting ihrr with the objective function of forecasting for longer lead times and a back fitting based error correction method the proposed method of ihrrof bf 1 gives the best and most robust performance among the seven schemes demonstrating that it can effectively contribute to the real time scheduling of reservoir operations the results suggest that the objective function of forecasting for longer lead times improves the forecasting without error correction procedures once the error correction procedure is carried out the effect of the modified objective function will be diminished because the errors of both ihrrof bf 1 and ihrr bf 1 or ihrrof bf 2 and ihrr bf 2 have significant autocorrelation and can be well estimated with the ar model although the improvement is slight the modified objective still makes gains in forecasting over multiple lead times if rainfall forecasts are available and reliable chau and wu 2010 wu et al 2010 the simulated values of reservoir inflow at longer lead times can be obtained by considering the additional data li et al 2015 noted that the postprocessor method is based on the assumption that errors are independent homoscedastic and gaussian however these conditions cannot be satisfied in most cases the joint inference method can address this problem but has more computational demands the back fitting algorithm introduced in this study is not based on the error assumptions mentioned above and can also relieve the computational burden through recalibration of the hydrological and error models furthermore a study carried out by evin et al 2014 showed the joint inference method and the postprocessor method produced comparable results for the hbv model applied to wet regions but different results for the gr4j model applied to drier regions we are considering future research to evaluate the back fitting based recalibration in drier regions using various hydrological models in addition to the back fitting algorithm based recalibration the two error estimation procedures were evaluated by a comparison of the ihrrof bf 1 and ihrrof bf 2 or ihrr bf 1 and ihrr bf 2 methods phuoc khac tien and chua 2012 noted that the recursive estimation of errors leads to high inaccuracies the error estimation procedure using the most recently known errors can address this problem but has the limitation of weak autocorrelation between the most recently known errors and the errors that need to be estimated both of the error estimation procedures used in the present study performed well for short lead times the forecasting ability for ephemeral catchments and longer lead times should be investigated in further research 6 conclusions this study has developed a reservoir water level forecasting method based on error correction over multiple lead times the proposed method includes 1 the ihrr to simulate the water level dealing with the problem of using inaccurate observed streamflow data 2 the simulated water levels for multiple lead times are adopted in the objective function of the water level forecasting model 3 the back fitting algorithm is introduced for error correction using the most recently known errors to improve the forecast accuracy the main conclusions are as follows 1 the hydrological model with the objective function of minimizing the difference between the simulated and observed water levels over multiple lead times can improve the efficiency of multiple step ahead forecasting 2 the error correction method can be implemented more effectively by using the most recently known errors rather than the recursively estimated errors the back fitting scheme outperforms the joint inference method and thus recalibration based on the back fitting algorithm is an effective method for finding the optimal parameters despite the efforts made in this study some future developments are still needed to further improve the real time forecasting method firstly the proposed model should be additionally validated over longer lead time or at other reservoirs in particular the ephemeral catchments secondly the back fitting based recalibration procedure deserves further investigation with more hydrological models and parameter sensitivity analysis is needed thirdly the artificial intelligence based software or algorithm could be adopted as an alternative such as the data assimilation method ercolani and castelli 2017 xu et al 2017 which is widely used in parameter identification and real time error correction the exploration of the above problems is the subject of on going research acknowledgements this study was supported by the national key research and development program of china 2016yfc0400907 the excellent young scientist foundation of the national natural science foundation of china 51422907 and the national natural science foundation of china 51579180 the authors would like to thank the editor and the anonymous reviewers for their comments which helped to improve the quality of this paper 
26393,providing reliable reservoir water level forecasts is a challenge because of the accumulative errors in hydrological and reservoir routing models we present a novel forecasting model that addresses these issues the model consists of a hydrological model to simulate inflow a reservoir routing model to simulate water levels and an autoregressive model for error correction the parameters for the hydrological model were calibrated with the objective of forecasting water levels over multiple lead times while a back fitting algorithm was used to recalibrate the parameters sequentially for the hydrological and autoregressive models the results show that 1 the forecasting performance of effective lead times can be enhanced by minimizing the difference between the forecasted and observed water levels for multiple lead times 2 the most recent errors method is better than the one step ahead recursive prediction method and 3 the back fitting algorithm is superior to the joint inference method keywords flood forecasting lead time objective function xinanjiang model software availability name of software hirrof bf 1 developer xiaojing zhang pan liu contact address state key laboratory of water resources and hydropower engineering science wuhan university wuhan 430 072 china telephone fax and email numbers liupan whu edu cn year first available 2017 hardware required personal computer software required ms windows availability and cost free available at https pan baidu com s 1o7zgvfo program language fortran program size 95 8 mb downloaded zip file 1 introduction reliable reservoir inflow and water level forecasts contribute to efficient reservoir operations which play a key role in flood control hydropower generation and water supply gragne et al 2015 liu et al 2015a however temporal and spatial variations in climate as well as complex physical processes mean that forecasting reservoir water levels remains a complicated and challenging task chang and tsai 2016 data driven and physically based models are two basic approaches for forecasting reservoir water levels the data driven approaches include statistical and artificial intelligence ai models statistical methods include autoregressive ar autoregressive moving average and autoregressive integrated moving average arima models wang et al 2015 these data driven approaches offer easy implementation but ignore the nonlinearity of hydrological series das et al 2016 sun et al 2016 although ai models such as artificial neural networks anns and fuzzy inference gholami et al 2015 seo et al 2015 taormina and chau 2015 can solve the nonlinear problem they do not represent the hydrological process chen et al 2015b but only depend on training data which decreases their credibility as a result physically based models especially conceptual models are widely used because they are both easy to understand and effective fang et al 2017 to forecast reservoir water levels in the context of physically based modeling the inflow should be simulated as the input to the reservoir routing model however the observed reservoir inflow used in hydrological models is generally inaccurate because the discharge from the tributaries of the reservoir is hard to measure deng et al 2015a to address this issue observed water levels can be employed as the fitting target of a forecasting model that integrates hydrological and reservoir routing ihrr models deng et al 2015b the advantage of ihrr models is that the hydrological parameters can be calibrated with more reliable raw data thus improving the forecast accuracy to provide essential information for reservoir real time operation it is very important that accurate and reliable water level forecasts over multiple lead times are obtained however ihrr has a limited ability to meet these requirements because 1 its applicability in forecasting over multiple lead times has not been validated 2 inherent forecast errors are not corrected in real time to improve lead times for forecasting a direct approach is to forecast future rainfall such as typhoon characteristics or remotely sensed variables chang and tsai 2016 jhong et al 2016 lin et al 2010 however in areas without reliable rainfall forecasting an alternative approach is to adopt the forecast streamflow from multiple lead times as the objective function assuming the future rain is zero for the purpose of reducing the forecast errors this improvement is feasible because the hydrological parameters can be adjusted to enhance the efficiency of longer lead times in the calibration procedure the error correction method can mitigate the forecast uncertainty problems van steenbergen et al 2012 yan et al 2012 owing to the difficulties in clarifying the errors from different sources and their interactions it is preferable to consider the overall error deng et al 2015a 2015b i e directly correcting the difference between the forecast and observations the existing literature includes many studies on overall error correction for example xiong and o connor 2002 compared four error forecasting models including an ar autoregressive threshold a fuzzy autoregressive threshold and an ann model they demonstrated that the ar model is efficient despite its simplicity similarly goswami et al 2005 evaluated eight error updating methods while liu et al 2016 compared three error correction techniques these methods are a type of post processor a feasible alternative method is joint inference in which the parameters of the hydrological and error correction models are calibrated simultaneously for example li et al 2016 applied a restricted ar model to normalized errors and jointly calibrated all the parameters using shuffled complex evolution although the joint inference method can theoretically find the optimal global parameters it incurs a heavy computational burden as the back fitting algorithm is a simple iterative procedure for fitting a generalized additive model sorokina et al 2007 the back fitting algorithm only focuses on either the parameter set of the hydrological model or the auto regression model during each recalibration in contrast the joint inference method simultaneously calibrates all the parameters in cases where the number of parameters is very large the back fitting algorithm will significantly reduce the heavy computational burden this study integrates the ihrr with the objective function of minimizing the difference between the simulated and observed water levels over multiple lead times and error correction methods and develops a real time forecasting method for reservoir water levels our aim is to improve the accuracy of multiple step ahead forecasting compared with the ihrr the method has two improvements 1 the forecast streamflow for multiple lead times is derived simultaneously to provide longer flood warnings 2 the back fitting algorithm is used for multiple step ahead error correction to reduce the heavy computational burden this paper is organized as follows in section 2 the details of the methodologies are described and the performance evaluation criteria are provided a case study focused on the shuibuya reservoir is presented in section 3 followed by the results and discussion in sections 4 and 5 finally the conclusions are drawn in section 6 2 methodology as shown in fig 1 the proposed method has two steps 1 the xinanjiang model and reservoir routing model are integrated to simulate the water levels over multiple lead times with the objective function of minimizing the difference between the simulated and observed water levels 2 back fitting based error correction is used to estimate the errors over multiple lead times and recalibrate the parameters to find the overall global optimal parameters for the hydrological and error correction models 2 1 simulating water levels over multiple lead times 2 1 1 integrated hydrological and reservoir routing models the model for simulating the water levels integrates the xinanjiang and reservoir routing models deng et al 2015b the xinanjiang model is used as the rainfall runoff model because it is the most popular conceptual rainfall runoff model in china wu et al 2017 zhao 1992 it has been widely used for streamflow simulation jayawardena and zhou 2000 yao et al 2014 e g in the china national flood forecasting system wmo 2011 a schematic overview of the model is presented in fig 2 the model inputs are precipitation and evaporation and the simulated streamflow is calculated using four main modules 1 evapotranspiration 2 runoff generation 3 runoff separation and 4 flow concentration the 15 parameters in the xinanjiang model are defined in table 1 when evaporation seepage and other water losses are ignored the reservoir routing model based on the water balance equation zhang et al 2016 2017 can be written as 1 v t 1 v t i t 1 o t 1 δ t where δ t is the time interval v t v t 1 are the initial and final reservoir storage volumes at time t 1 respectively and i t o t are the inflow and release of the reservoir during time period t 1 respectively the inflow of the reservoir is obtained by the xinanjiang model and the release which contains flows for electricity generation irrigation water supply and ecological generation is pre determined by reservoir operating rules liu et al 2014 the simulated water levels are then derived via equation 1 and the reservoir stage storage curve liu et al 2015b pan et al 2016 2 1 2 objective function of the ihrr the objective of the conventional water level simulating model is to minimize the sum of squares of the difference between the simulated and observed water levels with the aim of making the simulated series match the observed one 2 min f c o n t 1 n z t z t 2 where z t and z t are the observed and simulated water levels at time t and n is the length of the data series however the forecast water levels for longer lead times are not considered a modified objective function is developed by adopting the difference between the simulated and observed values at longer lead times assuming that the future rainfall is zero this is expressed as follows 3 min f p r o t 1 n z t k z t k t 2 z t k z t k t 1 2 z t k z t k t k 1 2 where z t k t is the simulated water level at time t for time t k and z t k is the observed water level at time t k to further define the variables in equation 3 a schematic illustration is presented in fig 3 chen et al 2016 the columns represent the time at which the simulations were performed and the rows represent the time for which each simulation was made assume that the current time is t and the simulated water levels z t 1 t z t 2 t z t k t are determined simultaneously at t similarly the number of simulated water levels for time t k 1 is k including z t k 1 t 1 z t k 1 t z t k 1 t k 2 we can combine the genetic wang 1991 rosenbrock 1960 and simplex algorithms nelder and mead 1965 to calibrate the model in which the genetic algorithm is used first and the calibrated parameters are subsequently used as initial values for the rosenbrock algorithm the results of the rosenbrock algorithm are treated as initial values for simplex algorithm and the final parameter values are optimized by the simplex algorithm li et al 2010 liu et al 2016 yan et al 2014 the similar combined optimization algorithms have been used in literature and is proved to be an effective tool for parameter calibration hartnett and diamond 1997 moore et al 2008 the population number and the number of function evaluations are 100 and 3500 in the genetic algorithm optimization respectively the number of function evaluations is 3500 in the rosenbrock optimization the number of function evaluations is 3500 in the simplex optimization 2 2 error updating based on the back fitting algorithm 2 2 1 error estimation procedure one of the most general features of forecast errors is autocorrelation the description of autocorrelation in the error estimation model is both justified and necessary evin et al 2014 a substantial number of investigations have indicated that ar modeling is an effective approach for addressing autocorrelation and improving forecast accuracy chen et al 2015a wu et al 2012 xiong and o connor 2002 schaefli et al 2012 and bennett et al 2016 indicated that the simple ar model is sufficient hence the ar model is adopted in the study and depends on the autocorrelation of the antecedent forecast errors in multistep ahead error correction for longer lead times this begs the question of which antecedent forecast errors should be used a strong correlation was generated between the errors for a forecast with a lead time of k h issued 1 h ago and the errors for a forecast with a lead time of k hours issued now nevertheless the actual errors for a forecast with a lead time of k h that was issued 1 h ago are hard to obtain the feasible alternative is to use the estimated errors however deviations between the estimated and actual errors are inevitable and will be significant as the lead times increase another feasible alternative is to use the error for a forecast with a lead time of k h that was issued k h ago the error that was not the most strongly correlated but the most recently known these two error estimation procedures are considered in this study it should be noted that owing to involving different information from antecedent errors autoregressive parameters differ between two error estimation procedures i e using recursively estimated errors and the most recently known error respectively 2 2 1 1 using the most recently known error to take advantage of the most recent observations and simulations the ar model for longer lead times can be expressed as 4 ε t k t ϕ k 1 ε t t k ϕ k 2 ε t k t 2 k ϕ k p ε t p 1 k t p k where ε t k t is the estimated error of forecast z t k t and ε t t k is the actual error of the antecedent forecast water level z t t k ϕ k 1 ϕ k 2 ϕ k p are the ar model parameters for lead k error estimation p is the order of the ar model with low order ar more applicable to short term forecasts lundberg 1982 wu et al 2012 hence p is assumed to be of order 1 in this study as shown in fig 4 a the current time is set as t and then the error ε t k t of the simulated water level at time t for time t k is estimated because the most recent observation is z t the most recently known errors are ε t t 1 ε t t 2 ε t t k instead of using the estimated value the error that is most recent and closest to ε t k t is adopted in the ar model namely the actual error ε t t k the estimated errors are then added to the simulations to obtain the corrected forecast water level 2 2 1 2 using the recursively estimated errors regarding the lack of real time observed values the ar model when k is greater than 1 using the recursively estimated errors can be written as 5 ε t k t φ k 1 k ε t k 1 t 1 φ k 2 k ε t k 2 t 2 φ k p k ε t k p t p where ε t k t is the estimated error of the simulated water level z t k 1 t and ε t k 1 t 1 is the antecedent estimated error of the simulated water level z t k 1 t 1 φ k 1 k φ k 2 k φ k p k are the ar model parameters for lead k error estimation using the estimated errors ε t k 1 t 1 ε t k 2 t 2 ε t k p t p and p is again assumed to be of order 1 ε t k 2 t 2 ε t k 3 t 3 ε t k p 1 t p 1 can be obtained by recursively using the ar model in a similar fashion to equation 5 in fig 4 b the definitions of the row column t and ε t k t are the same as in fig 4 a the estimated error ε t k 2 t 2 for the simulated water level z t k 2 t 2 is used as the input for estimating the error at the next time step denoted as ε t k 1 t 1 and so on for each estimation in the lead k time the ar model is used to estimate the errors of the simulated water level in the adjacent columns autoregressive parameters can be calibrated by the least squares approach the yule walker method and the maximum likelihood estimation method according to the literature the least squares approach has been proven to be efficient liu et al 2016 and was chosen for the present study however in some applications offline calibration cannot adjust the parameters according to the variation in the errors phuoc khac tien and chua 2012 and so the model is not adaptive to real time forecasts in this study the least squares approach applied to the methods described in sections 2 2 1 1 and 2 2 1 2 is an online technique based on the most recent errors this means the parameters of the ar model can be adjusted along with the updated error time series 2 2 2 recalibration based on the back fitting algorithm in the conventional error correction procedure the hydrological model is firstly calibrated with the observed streamflow then the difference between the simulated and observed is input into the error estimation model however the sum of the simulated streamflow and the estimated error may not be the closest value of the observed streamflow alternatively a joint inference method combines the hydrological model with the ar model and estimates the global optimal parameters of both models li et al 2015 prakash et al 2014 to reduce the computational burden brought by the additional parameters the back fitting algorithm is introduced to the hydrological and ar model the back fitting algorithm buja et al 1989 hastie 1990 is an iterative procedure to estimate the additive components for reducing computations in the back fitting algorithm for an additive model one component is estimated while the other components are fixed for each step the algorithm proceeds component by component until convergence ansley and kohn 1994 the reservoir water level forecasting model has two components water level simulation by the ihrr model and error estimation by the ar model an additive system we therefore used the back fitting algorithm based iterative procedure the procedure requires a starting point the starting point can be set to the first calibration of the ihrr model the recalibration procedure is identical for two different error estimation procedures and consists of two steps 1 hydrological model recalibration assuming that the estimated errors over multiple lead times are determined and fixed but the hydrological parameters are unknown the difference between the observed water levels and the estimated errors can be used as training data for the xinanjiang model the simulated water levels over multiple lead times are then obtained by the recalibrated xinanjiang model 2 ar model recalibration as the recalibrated simulated water levels over multiple lead times have been determined the errors i e the difference between the observed and simulated water levels are known the recalibrated ar model then outputs the estimated errors for multiple lead times as the fixed values in phase 1 the back fitting based recalibration cycles through the two models until the iterations number saying 50 in the study is reached 2 3 model evaluation a multi criteria assessment was implemented to evaluate the model performance although the nash sutcliffe efficiency coefficient nse is widely employed in streamflow forecasting the difference between water level evaluations using the nse is not significant hence three other indexes were adopted let x t be the observed value at time step t and x t t k be the simulated forecast or corrected value issued at time step t k 1 root mean square error rmse lin et al 2009 lin and wu 2011 6 r m s e k 1 n t 1 n x t x t t k 2 where k is the specified lead time and n is the length of the forecast water level data series 2 mean absolute error mae budu 2014 lin et al 2009 7 m a e k 1 n t 1 n x t x t t k 3 probability of the forecast error exceeding a given threshold α shen et al 2015 wu et al 2012 8 p k α p k x t x t t k α 1 n t 1 n φ x t x t t k α 100 where φ y 1 if y is true otherwise φ y 0 the three evaluation criteria are able to test different aspects of the model efficiency both r m s e k and m a e k evaluate the deviation of the forecast errors but r m s e k suffers a greater impact from large errors p k α is the success rate of the forecast for a specified α smaller values of r m s e k and m a e k indicate better forecast accuracy whereas higher values of p k α suggest a more robust model 3 case study a case study was conducted to demonstrate the application of the proposed method first the location and data used in the case study are introduced followed by a description of the seven schemes designed for the experiment finally the results are obtained and analyzed 3 1 location and data the proposed method was used to forecast water levels in the shuibuya reservoir see fig 5 located in hubei province the shuibuya reservoir is the first reservoir in a multi reservoir cascade system in the middle and lower reaches of the qing river the qing river is one of the main tributaries of the yangtze river and has a total length of 432 km and a drainage area of 17 600 km2 li et al 2014 liu et al 2011 because it is the most upstream reservoir shuibuya inflow is not impacted by other reservoirs hence shuibuya reservoir can be used as a typical case for real time reservoir water level forecasts the shuibuya reservoir watershed is covered by evergreen and deciduous broad leaved and mixed forest across humid and semi humid regions controlled by a saturated runoff mechanism deng et al 2015b hence the xinanjiang model is an appropriate choice for simulating streamflow in the application the reservoir has a basin area of 10 860 km2 with 4 58 billion m3 storage capacity the normal water level is 400 m and the corresponding water storage is 4 31 billion m3 fourteen rainfall gauge stations four pan evaporation stations at lichuan yuxiakow xuanen and yeshangwan and one water level station shuibuya spread over the basin the hourly data concerning precipitation evaporation reservoir release and water level were collected from theses gauging stations during february 2011 to february 2012 8760 h areal precipitation was obtained by the thiessen polygon method considering spatial variation huang et al 2016 wu et al 2017 areal evaporation was provided by using the average value of the four measured evaporation stations reservoir release data were pre determined by reservoir operation rules water level data were provided by the shuibya station at the basin outlet the observed water levels are representative ranging from low levels e g 363 44 m to high levels e g 397 65 m since all the data were collected after the reservoir built impacts of the dam is consistent the data from february 2011 to september 2011 were used for model calibration and data from october 2012 to february 2012 were used for model validation due to arbitrary initializing the soil water condition a warm up period was used cole and moore 2008 nicolle et al 2014 the errors are accumulative for the long lead times in the reservoir routing model hence the effective lead time is set as six hours 3 2 description of the experiments fig 6 lists seven schemes considered in the case study besides the conventional model ihrr the variants ihrrof ihrrbf 1 ihrrbf 2 ihrrof bf 1 ihrrof bf 2 and ihrrof ji 1 were used for comparative purposes in these models of denotes a modified objective function bf 1 and bf 2 denote additional error updating based on the back fitting algorithm using the most recently known errors fig 4 a and the recursively simulated errors fig 4 b respectively and ji 1 denotes additional error updating using the conventional joint inference method each of the seven schemes has 16 parameters including 15 in the xinanjiang model and one parameter in the ar model firstly the impact of the objective function on the forecasting for longer lead times is investigated based on the comparison of the ihrr and the ihrrof or the ihrr bf 1 vs ihrrof bf 1 ihrr bf 2 vs ihrrof bf 2 secondly which antecedent errors should be used in ar model is suggested by the comparison of the ihrr bf 1 and the ihrr bf 2 or ihrrof bf 1 vs ihrrof bf 2 and finally the superiority of the bf method to the ji method is studied from comparison of the ihrrof bf 1 and ihrrof ji 1 4 results figs 7 10 compare the forecasts given by the seven schemes at lead times of 1 6 h for both calibration and validation periods in terms of r m s e k m a e k and p k α the given threshold α of p k α was set to 0 1 m or 1 m in the schemes without error updating whereas it was set to 0 01 m or 0 05 m in the schemes with error updating 1 ihrr vs ihrrof the impact of the objective function on the forecasting for longer lead times in the calibration period it can be seen that ihrr outperforms ihrrof for a 1 h lead time whereas it is inferior for 2 6 h lead times this is essentially because the modification of the objective function takes the additional forecasts at 2 6 h lead times into account however the same results are not observed in a comparison of the validation period from the results in figs 7 9 ihrrof outperforms ihrr for all 1 6 h lead times as demonstrated by the lower r m s e k m a e k and higher p k α the results indicate that schemes with the modified objective function are more effective in the validation period fig 11 a illustrates the performance for a 1 h lead time in the calibration period it can be seen that ihrrof tends to overestimate the water level in the validation period the forecasted water level using ihrrof is higher than that of the ihrr however the water levels forecast by both models are smaller than the observed values because of the accumulative errors from the reservoir storage simulation that assumes the future rainfall is zero see fig 11 b 2 ihrrbf 1 vs ihrrbf 2 comparison between the error estimation procedure adopting the recently known errors and that adopting the recursively simulated errors given that the errors usually show intensive autocorrelation the schemes with error updating provide a great improvement over ihrr and ihrrof suggesting the need for integration with error updating methods however the differences between ihrrbf 1 and ihrrbf 2 are small as both schemes include error updating comparing the two it can be seen that ihrrbf 1 generally outperforms ihrrbf 2 with the exception of the forecast for a 1 h lead time because ihrrbf 2 forecasts errors without any estimation at the 1 h lead time instead using the most recent errors furthermore the difference between ihrrbf 1 and ihrrbf 2 increases with the lead time demonstrating that forecast errors in the error updating method of ihrrbf 2 accumulate faster than those of ihrrbf 1 a similar conclusion can be reached in the comparison between ihrrof bf 1 and ihrrof bf 2 which further indicates that using the most recently known errors in the ar model provides a better fit than that of the recursive simulations in both the calibration and validation periods 3 ihrrof bf 1 vs ihrrbf 1 impact of combining the objective function for forecasting longer lead times with back fitting based error correction in the calibration period it can be seen that r m s e k and m a e k for ihrrof bf 1 are actually larger than those of ihrrbf 1 which is in contrast to the comparison results between ihrr and ihrrof this situation persists in a comparison of ihrrbf 2 and ihrrof bf 2 the comparison in terms of p k α shows a similar trend but the schemes with modified objective functions gradually produce better forecasts as the lead time increases as demonstrated by the higher p k α values of ihrrof bf 1 ihrrof bf 2 at 4 6 h lead times however in the validation period ihrrof bf 1 ihrrof bf 2 are superior to ihrrbf 1 ihrrbf 2 at all six lead times which suggests that the modification of the objective function is efficient in terms of improving the water level forecasts for longer lead times it can be inferred that adopting longer lead times in the objective function allows the forecast model to learn from the non rain situation and reduce the inherent errors of ihrr 4 ihrrof bf 1 vs ihrrof ji 1 comparison between the back fitting based and joint inference recalibration or calibration methods it can be seen that ihrrof bf 1 outperforms ihrrof ji 1 for all six lead times during both the calibration and validation periods the difference between ihrrof ji 1 and ihrrof bf 1 lies in their parameter estimation methods ihrrof ji 1 uses the conventional joint inference method whereas ihrrof bf 1 calibrates the parameters of the hydrological and error updating models in sequence which reduces the computational burden of recalibration it can further be inferred that the recurrent recalibration procedures provide more chances to project the parameters into the near optimal region 5 discussion for reservoir water level forecasting relying on reliable raw data ihrr integrates the hydrological model reservoir routing model and the observed water level criteria based on ihrr this study develops a real time forecasting method by augmenting ihrr with the objective function of forecasting for longer lead times and a back fitting based error correction method the proposed method of ihrrof bf 1 gives the best and most robust performance among the seven schemes demonstrating that it can effectively contribute to the real time scheduling of reservoir operations the results suggest that the objective function of forecasting for longer lead times improves the forecasting without error correction procedures once the error correction procedure is carried out the effect of the modified objective function will be diminished because the errors of both ihrrof bf 1 and ihrr bf 1 or ihrrof bf 2 and ihrr bf 2 have significant autocorrelation and can be well estimated with the ar model although the improvement is slight the modified objective still makes gains in forecasting over multiple lead times if rainfall forecasts are available and reliable chau and wu 2010 wu et al 2010 the simulated values of reservoir inflow at longer lead times can be obtained by considering the additional data li et al 2015 noted that the postprocessor method is based on the assumption that errors are independent homoscedastic and gaussian however these conditions cannot be satisfied in most cases the joint inference method can address this problem but has more computational demands the back fitting algorithm introduced in this study is not based on the error assumptions mentioned above and can also relieve the computational burden through recalibration of the hydrological and error models furthermore a study carried out by evin et al 2014 showed the joint inference method and the postprocessor method produced comparable results for the hbv model applied to wet regions but different results for the gr4j model applied to drier regions we are considering future research to evaluate the back fitting based recalibration in drier regions using various hydrological models in addition to the back fitting algorithm based recalibration the two error estimation procedures were evaluated by a comparison of the ihrrof bf 1 and ihrrof bf 2 or ihrr bf 1 and ihrr bf 2 methods phuoc khac tien and chua 2012 noted that the recursive estimation of errors leads to high inaccuracies the error estimation procedure using the most recently known errors can address this problem but has the limitation of weak autocorrelation between the most recently known errors and the errors that need to be estimated both of the error estimation procedures used in the present study performed well for short lead times the forecasting ability for ephemeral catchments and longer lead times should be investigated in further research 6 conclusions this study has developed a reservoir water level forecasting method based on error correction over multiple lead times the proposed method includes 1 the ihrr to simulate the water level dealing with the problem of using inaccurate observed streamflow data 2 the simulated water levels for multiple lead times are adopted in the objective function of the water level forecasting model 3 the back fitting algorithm is introduced for error correction using the most recently known errors to improve the forecast accuracy the main conclusions are as follows 1 the hydrological model with the objective function of minimizing the difference between the simulated and observed water levels over multiple lead times can improve the efficiency of multiple step ahead forecasting 2 the error correction method can be implemented more effectively by using the most recently known errors rather than the recursively estimated errors the back fitting scheme outperforms the joint inference method and thus recalibration based on the back fitting algorithm is an effective method for finding the optimal parameters despite the efforts made in this study some future developments are still needed to further improve the real time forecasting method firstly the proposed model should be additionally validated over longer lead time or at other reservoirs in particular the ephemeral catchments secondly the back fitting based recalibration procedure deserves further investigation with more hydrological models and parameter sensitivity analysis is needed thirdly the artificial intelligence based software or algorithm could be adopted as an alternative such as the data assimilation method ercolani and castelli 2017 xu et al 2017 which is widely used in parameter identification and real time error correction the exploration of the above problems is the subject of on going research acknowledgements this study was supported by the national key research and development program of china 2016yfc0400907 the excellent young scientist foundation of the national natural science foundation of china 51422907 and the national natural science foundation of china 51579180 the authors would like to thank the editor and the anonymous reviewers for their comments which helped to improve the quality of this paper 
26394,rainfall is low and unreliable in much of australia s dryland cropping areas requiring well informed crop management for optimising yield and profit growing season rainfall is usually supplemented by soil water during fallow periods preceding a crop while rainfall is conveniently measured the difficulty of measuring a soil s plant available water paw mm has led to using simulation models for estimating paw here we developed a smartphone application app that simulates soil water balance by accessing weather soil and crop data from databases and on farm records predictions of paw using the howleaky modelling engine were compared with field measurements validation of the simulation engine across sites in australian cropping areas showed good agreement between simulated and measured paw errors in model estimates are compared with variability found within small fields we conclude that estimating paw for paddocks using a simulation model built in a smartphone app is a reliable and adaptable technology keywords decision support soil water water balance monitoring modelling app 1 introduction crop production in australian agriculture is limited by the water supply and water use efficiency wue kg ha mm french and schultz 1984 of farming systems many dryland farmers are familiar with concepts of yield targets based on wue which relates crop yield directly to water supply water stored in the soil at planting plus in crop rainfall wue is simple transparent and well suited to communication with farmers in a study of 334 commercial wheat crops hochman et al 2009 found a wue value of 15 kg ha mm and a threshold value of 67 mm nutritional disorders pests and disease reduce yield below these guideline values of wue and provide evidence of crop disorders cornish and murray 1989 nevertheless the importance of water supply to dryland crops is overarching as summarised by routley 2010 water supply is clearly the factor most limiting the productivity and profitability primary aim of dryland cropping systems maximise the efficient capture storage and use of this limited water in the northern and drier areas of southern australia there is insufficient rainfall during crop growth to achieve economically viable yields so fallows are used to accumulate soil water to supplement in crop rain this dependency on water stored in a fallow varies from 5 in western australia to 60 in central queensland thomas et al 2007 the need for improved soil water management may increase in the future under a changing climate as climate adaptations are likely to have a greater reliance on stored soil water kirkegaard et al 2014 ghahramani et al 2015 major investments in crop production occur at planting time and shortly after when an uncertain water supply makes prediction of yield and financial return difficult financial losses from both under investing and over investing in crop inputs are common but having a robust estimate of soil water at sowing time can reduce uncertainty thomas et al 2007 management options and farm financial risk profiles can be decided by soil moisture status of a paddock a high potential yield attracts greater investment in crop establishment nutrition moeller et al 2009 crop protection and informs marketing decisions on the other hand low yield potential informs a variety of agronomic and business decisions with inputs often being reduced although predicting grain yield before or early in the growing season is challenging applying the wue framework to predict yield is well established french and schultz 1984 moore et al 2011 and is improved by a reliable estimate of plant available water paw near planting paw is water that is available to plants during the crop phase is regarded as safe water as it is mostly immune to evaporation loses due to its depth of storage and sustains crops between rainfall events paw is calculated for each soil layer from the difference between gravimetric water content g g 1 and the soils lower limit ll or wilting point and considering the thickness and bulk density of each soil lawrence et al 2005 plant available water capacity pawc refers to a soil s capacity to store water and is often taken as a soil property although it can be dependent on crop type pawc is calculated from a soil s ll and drained upper limit dul or field capacity dalgliesh and cawthray 2005 estimating paw and pawc is expensive and labour intensive in this paper we explore errors in predicting soil water using a water balance model along with an analysis of spatial variability in relatively small fields it is recognised that there are errors associated with instrument calibration and estimating basic soil properties such as bulk density and ll required in calculating paw dalgliesh et al 2009 because of these errors and high spatial variability in field conditions paw is not a variable to be measured directly in a simple manner by farmers and consultants early simulation models of crop growth and yield were focused on predicting the supply of soil water with a view to managing crop water use and increasing wue e g fitzpatrick and nix 1969 nix and fitzpatrick 1969 the capability to estimate paw within soil and cropping systems models such as howleaky mcclymont et al 2016 and agricultural production systems simulator apsim holzworth et al 2014 is largely inaccessible to practical agronomists and farmers as those models were designed as research tools not as information products decision support tools that do incorporate soil and crop dynamics such as yield prophet https www yieldprophet com au require considerable system specification whereas the app being introduced here aims to provide a robust and rapid estimate of soil water aimed at farmers and consultants as users in developing a smartphone app to provide estimates of paw to farmers and their consultants it was considered prudent to understand the accuracy and reliability of a model based estimate of paw confidence in the performance of models is usually obtained by comparison with field observations for the key variables of interest to the scientist such as runoff erosion and water quality knisel 1980 williams 1983 littleboy et al 1992 or crop biomass and yield carberry et al 2009 while it has largely been assumed that models accurately predict paw such a narrow focus is expected as most components of the water balance are difficult to measure for example runoff is infrequent and unpredictable making it difficult to maitain equipment freebairn et al 1986 while deep drainage is technically difficult to measure and subject to high spatial variability humphreys et al 2003 while evapotranspiration is more spatially homogeneous and accurately measured variable in a water balance analysis calculations of the bowen ratio fritschen 1965 and related methods require advanced instrumentation complex mathematics and are labour intensive and expensive these methods are almost exclusively applied where crops are growing and water flux to the atmosphere is unable to be apportioned to soil evaporation and transpiration the analysis presented here was part of the design of a virtual soil water monitoring system soilwaterapp which is aimed to meet farmer and adviser needs we evaluate the water balance model in howleaky mcclymont et al 2016 used in soilwaterapp to estimate changes in paw also we investigate the ability of a smartphone app to estimate the components of water balance from meteorological soil and crop information providing estimates of paw for improved crop management through system design soilwaterapp is available from the apple store in australia and documented at http www soilwaterapp net au soilwaterapp has some special features fast simulation of the water balance on a smartphone or tablet connection to climate soil and crop databases accept on farm data and sufficiently user friendly to accommodate a wide range of users including farmers and consultants 2 water balance model the water balance model used in the app has evolved from creams knisel 1980 which predicted paw runoff and soil erosion from a combination of rainfall and evaporation data with i the runoff model of williams and laseur 1976 ii the soil evaporation model of ritchie 1972 and iii the usle for soil erosion williams 1983 creams was influential in the development of perfect littleboy et al 1992 and later howleaky mcclymont et al 2016 the latter model uses the williams ritchie water balance model williams and laseur 1976 ritchie 1972 which is a one dimensional mechanistic model with parameterisation strongly based on a wide range of empirical studies littleboy et al 1992 http howleaky net index php library simulation is performed on a daily time step surface runoff is estimated as a function of daily rainfall soil water deficit surface residue and crop cover the model has a cascading bucket structure where infiltration is partitioned into soil layers from the surface filling subsequent layers to total porosity in the model vertical water movement occurs if the layer is wetter than its field capacity and the layer below is drier than its field capacity water flux is limited by the saturated hydraulic conductivity of each layer soil water can be removed from the profile by transpiration soil evaporation and downwards movement from the lowest layer as deep drainage transpiration is a function of pan evaporation a climate input leaf area or percentage green cover and soil moisture soil evaporation removes soil water from the upper two layers the sum of transpiration and soil evaporation evapotranspiration cannot exceed pan evaporation on any day a summary of the soil water balance model in howleaky is presented in the supplementary material s1 3 architecture software and data soilwaterapp has been developed for ios devices using apple s native objective c framework and communicates with a central cloud based server for synchronising both app and user data operating the app involves setting up and monitoring a range of sites with different agro climatic variables selecting a site in the user interface will present an analysis page which automatically updates the soil water results for the latest climate conditions using the howleaky model during this process it will update any outdated climate data and provides the user with a range of input and output infographics that progressively disclose more detail as the user scrolls down fig 1 inputs are presented at the top of the analysis page and are grouped into soil starting conditions fallow crop conditions irrigation local rainfall and soil water sensor options outputs include a summary of predicted paw a time series of recent historical past years and predicted plumes of soil water recent stubble and crop cover a soil moisture profile graph and a water balance summary table the app has been developed with a multithreaded design for parallel processing of data input output and analysis streams it is composed of a range of independent functional modules for data input storage and synchronisation and for running soil water analyses using the howleaky engine fig 2 shows these modules and how they interact with each other and external data sources database operations are handled by a coredata manager module and multiple synchronised database instances known as managed object contexts separate contexts ensure that data integrity is maintained during asynchronous operations of the data synchronisation manager bluetooth manager and running of the howleaky engine temporary view models are used to safely transfer data between the coredata manager and howleaky engine and to provide an undo and redo functionality for user settings changes a climate data manager module facilitates updating local climate data from the silo data server https www longpaddock qld gov au silo while an export manager module allows analysis data and results to be shared with other users the app communicates with several external application programming interfaces apis including a cloud based database a climate data provider and bluetooth rain gauge and soil water sensors key to the software s operation is a data synchronisation manager which synchronises both application and user data between multiple mobile devices and a server data includes soil and vegetation descriptions climate locations project site and simulation data and analytics to track the app s use this allows data to be collected on one device with or without an internet connection and synchronised with the soilwaterapp server and other mobile devices once an internet connection exists it operates in different modes including one way from the server to device two way between server and device and one way between device and server depending on the nature of the data soilwaterapp relies on a range of short and long term time series data that are retrieved and stored using different methodologies long term records of daily historical climate data from the silo patched point database jeffrey et al 2001 www longpaddock qld gov au silo are stored locally as ascii files on the user s device and automatically appended to each time the app is active these files are replaced after 2 weeks as the silo data files are prone to data corrections over time users local short term data such as rain gauge soil water sensor and irrigation data are stored in the database in records spanning three monthly data chunks to facilitate efficient retrieval and synchronisation between device and server 4 model evaluation performance of the water balance model in soilwaterapp was evaluated for three data types fallows with detailed long term observation of gravimetric soil water collected from hydraulically driven soil cores three sets of small catchments daily data for a sequence of fallow and crops at two sites using capacitance probes and a set of bluetooth enabled decagon soil water sensors and a rain gauge integrated with the soilwaterapp comparisons between soil water observations and model estimates were carried out using the howleaky model mcclymont et al 2016 as this was the most efficient process and the app is not suited to model testing the model in the app and howleaky were verified to produce identical outputs when given the same inputs 4 1 fallow in australian dryland cropping systems fallows are instigated to store soil water accumulate soil nitrate and control weeds in preparation for the next crop to reduce the risk of failure and increase yield especially where soils have a high water holding capacity field observations of fallow effect on soil water were available from three long term experimental sites in the state of queensland greenmount 27 44 27 s 151 51 33 e greenwood 27 19 42 s 151 43 47 e wallumbilla 26 34 28 s 149 11 17 e where the focus of each study was to better understand the role of tillage and stubble management on moisture and soil conservation freebairn and wockner 1986 freebairn et al 2009 rainfall runoff and soil water were monitored over periods of 7 17 years equivalent to 170 plot years of data with each plot being sampled at least three times each year start mid and end of each fallow period each soil sampling consisted of nine soil cores taken in each bounded catchment referred to hereafter as a bay at 0 10 10 30 30 60 60 90 90 120 and 120 150 cm depths see fig 3 a for sampling patterns plant available water paw mm was estimated from a field measured lower limit ll cm3 g dalgliesh and foale 1998 and measured soil bulk density g cm3 and is presented in this paper as an average of total profile paw for each sample date field experiments were conducted under two common but contrasting soil management conditions stubble burnt after harvest with little soil cover and zero tillage with 30 80 cover from crop residue measured and predicted change in paw between sample dates is used rather than absolute values of paw to reduce artificial skill in the statistics all related weather agronomy practice and soil descriptions and soil water data are accessible from a database of experimental sites focusing on water balance and water quality across australia http howleaky net index php library 4 2 fallow and crops soilwaterapp s modelling engine was evaluated for its ability to simulate paw in crop fallow sequences at two sites in the state of the victoria youanmite 36 1639os 145 6640ow and hamilton 37 7277os 141 9242ow each site had one sample location with 8 capacitance sensors at 30 40 50 60 70 80 90 and 100 cm depths enviropro ep100g 08 entelechy pty ltd adelaide australia the shallow sample was located to avoid damage by tillage equipment and was sited to avoid wheel tracks the surface 25 cm was not sample while these data are more limited in terms of duration and fallow management practices the daily time series provides more detail on soil water dynamics using measuring equipment similar to that increasingly being installed by farmers and consultants in essence this level of detail is a useful benchmark for evaluating soilwaterapp s ability to track soil water dynamics at a daily time step 4 3 surface water dynamics soilwaterapp accepts data directly from a bluetooth enabled data logger connected to a tipping bucket rain gauge and three decagon devices inc 10hs soil water sensors to test a prototype system we installed a sensor logger system into a bare soil plot at the university of southern queensland toowoomba 27 36 52 0 s 151 56 14 e daily rainfall and soil water content were recorded from june to october 2016 20 weeks in this experiment the surface 10 cm of soil was monitored at 4 cm 6 cm and 8 cm representing a measurement zone of 0 5 cm 5 7 cm and 7 10 cm respectively with a focus on testing the evaporation algorithm within soilwaterapp as well as the robustness of deploying the system into a field setting the soil description reflected the sensed layers for a shallow ferrosol 5 spatial variability in field measurements plant available water capacity pawc is often taken as a soil property and is an important descriptor used in biophysical models we explored the spatial variability in measurements from three long term datasets also used in section 4 1 that represent cropping in southern queensland available at http howleaky net index php library a soil survey with detailed pedology and chemical assessment did not reveal any marked differences in soil type across a 12ha site with soil depth 2 m across the three sites i e soil depth should not be limiting to plant growth at greenmount and greenwood distance between sample areas is relatively short 50 60 m greater variability would be expected at wallumbilla as it was a larger site with sample sites 150 m distance apart on a diverging slope samples displayed greater variation in colour when sampled all three study sites represent areas 10 of a normal paddock in their respective regions 6 operating the app daily rainfall and evaporation data from silo are accessed by soilwaterapp via mobile telephone or wi fi networks once a site is established climate soil type cover and crop dates taking a new user 5 min soil water estimates are immediately updated using weather data up to the day before yesterday each time the app is opened multiple sites can be established and shared between users given the high spatial variability of rainfall and sparse network of gauges in some parts of grain growing regions users can enter local rainfall data manually or use an automated rain gauge that connects to the mobile device via bluetooth low energy ble in addition to the ble rain gauge a small number of ble data loggers have been deployed with soil moisture sensors to monitor paw daily whenever the user is within ble range 20 m a seamless communication and transfer of the soil water and rainfall data from the logger to the app occurs the app and its associated api includes databases of soil parameters for estimating daily runoff deep drainage soil evaporation and pawc similarly a database of crop parameters describing green cover and root depth distributions is required for estimating transpiration and soil water extraction crop residue cover used to modify infiltration and evaporation is specified by the user databases are updated with app use while a system administrator can easily manage the reference databases of soil and vegetation descriptions 7 results and discussions 7 1 modelling capability of the app the performance of the app s modelling engine in simulating changes in paw over fallows and crops for a range of rain fed agricultural systems is evaluated fig 3a presents a comparison of simulated and observed gains in paw for the three long term field studies in the state of queensland greenmount greenwood and wallumbilla there was agreement between simulated and observed values for each practice and soil type the scatter of measured and observed data in fig 3a has a range of r2 values from 0 72 to 0 76 across sites and treatments with an overall r2 of 0 69 indicating reasonable confidence in model estimates across multiple seasons soil types and fallow management conditions regression analyses were carried out using ms excel statistics the pattern of error shown in fig 3a frequently occurs in models of this type due to inadequate representation of biological and physical processes that give rise to low and high values for example regression of observed and apsim predictions in 15 studies of crop yields resulted in low slope 1 in 12 studies and a high intercept 0 in 13 of those studies carberry et al 2009 at youanmite state of victoria daily values of observed and predicted paw are in step during the crop of 2015 that was planted with low paw and subsequently suffered severe water stress fig 3b the disagreement between simulated and measured replenishment of paw over the summer fallow where howleaky overestimated paw accumulation may be attributed to errors in parameters describing surface soil water characteristics or missing rainfall data the flat sections in model estimates are an artefact of soil water data being available for 30 100 cm layers as sensors were buried beneath the tillage depth plotted model estimates ignored layer 1 0 20 cm therefore not showing soil surface water dynamics paw predictions in june and july are in agreement with measurements an important outcome given the app is used to guide inputs during early crop growth results from the hamilton site show good agreement between observations and estimates of paw with both accumulation and depletion of paw predicted well fig 3b these comparisons lead to confidence in predicting gains in paw during fallows with small and generally explainable errors during periods of crop water use an analysis of errors found that unreliable rainfall data and poor specification of soil constraints accounted for most poor predictions of paw unreliable rainfall data came from poorly maintained digital rain gauges it is recognised that field data collection is challenging given environmental extremes while soil specification remains a challenge for site specific applications of models fig 3c shows paw estimated by soilwaterapp and measured values derived from capacitance sensors demonstrating linking local rainfall and soil water data with the ability to customise a soil type for a site fig 3c shows that when soilwaterapp is suitably configured its water balance simulation and field sensors provided comparable estimates of soil water in a simple experiment focusing on evaporation losses this combination of sensors a bluetooth logger and soilwaterapp is a valuable development in bringing a simple and reliable soil water sensing system and modelling to soil scientists agronomists and eventually farmers while this system is not commercially available fig 3c displays the system s simplicity low cost accuracy without calibration and direct link between a monitoring device and a water balance simulation 7 2 why a virtual soil water monitoring tool to compare field variability of soil water observations with errors associated with modelling soil water individual soil core data from the wallumbilla study were analysed in detail for several sample dates at the studies initiation the site was chosen to be representative of a major soil type of the region and to be relatively uniform across the site yet fig 4 a shows variability in pawc 180 280 mm for four adjacent catchments over a 15 ha site see fig 3a pawc is regarded an important soil descriptor in water balance models and is important in calibrating soil sensors fig 4b shows the variability in measured paw for nine sites within a 3 ha bay for a sample date at the end of a summer fallow while nine samples were collected with the aim of providing a robust estimate of paw and changes in paw for the 3 ha catchment it is clear that reliance on a different number of sample locations would produce a different result the standard deviation of gravimetric soil water for any depth is 1 2 which translates into a standard deviation of 50 mm of paw an outcome from these observations might be that users of models might have a more realistic view regarding specification of a model given the natural variability within a small area these results from an intensively sampled experiment over 17 years demonstrate that even in sites selected for their apparent uniformity it is difficult to assume reliable values of soil lower limit ll or wilting point drained upper limit dul or field capacity and consequent pawc even if errors in estimating soil bulk density are ignored it follows that when soil water monitoring equipment is installed that site selection will be challenging as the single sample site might be any one of the points on fig 4a or 4b it is worth putting all these estimates of soil water into a context with where they may be used by decision makers for example farmers estimate paw in numerous ways intuition from rainfall records and simple rules e g a percentage of rainfall stored steel push probes to detect the depth of moist soil observation of crops weather and soil water monitoring systems and models and apps such as presented here table 1 lists some strengths and weaknesses of six methods of estimating paw each approach has a fit in the real world of decision making for farmers putting a financial value estimates of soil water regardless of method is a challenge dependent on enterprise decision makers attitude to risk and application of information to the decision making process and won t be attempted here nevertheless a simple cost comparison of two contrasting approaches to tracking soil water application of soilwaterapp and the installation of a commercial weather and soil monitoring station is presented while the cost of developing soilwaterapp was considerable aud 500 000 this cost can be spread over many users with minimal ongoing costs a weather station with soil water sensors may cost aud 2 10 000 per installation based on this simple cost model when the app is used for 250 sites the cost per site will be less than a physical installation soilwaterapp has 2000 active sites being monitored 18 months after its release more detail is presented in supplementary materials s2 7 3 adoption of soilwaterapp to date while no formal evaluation of soilwaterapp has been completed fig 5 shows locations and accesses in the first 18 months since soilwaterapp s release soilwaterapp is being applied across australia s grain growing regions where the importance of soil water status is well recognised this initial adoption rate sets the scene for more in depth assessment of the value of paw plant available water estimates for grain growers it is noteworthy that the top 200 users by a number of sites comprise 80 of the total sites to date many of these users are consultants who are monitoring multiple sites 8 conclusions we conclude that soilwaterapp s modelling engine can reliably estimate patterns of paw through fallows and crops and that an app is a practical approach to bringing climate weather soil and crop information together for farmers and consultants to estimate paw in their decision making simulation based estimates of paw are shown to be reliable and less expensive than physical measurements an app based estimate of paw is well suited to application across multiple paddocks and soil types model based estimates of soil water are reliable easy to access in a mobile device app acknowledgement soilwaterapp was developed for the grain research and development corporation project new tools to measure and monitor soil water usq 00014 development has benefited from significant contributions of grain growers who acted as beta testers and scientists who shared data for testing the water balance model we thank dale boyd of agriculture victoria in particular for his generous sharing of detailed data from two victorian sites authors acknowledge useful comments made by 3 anonymous reviewers appendix a supplementary data the following is the supplementary data related to this article supplementary materials supplementary materials appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 010 
26394,rainfall is low and unreliable in much of australia s dryland cropping areas requiring well informed crop management for optimising yield and profit growing season rainfall is usually supplemented by soil water during fallow periods preceding a crop while rainfall is conveniently measured the difficulty of measuring a soil s plant available water paw mm has led to using simulation models for estimating paw here we developed a smartphone application app that simulates soil water balance by accessing weather soil and crop data from databases and on farm records predictions of paw using the howleaky modelling engine were compared with field measurements validation of the simulation engine across sites in australian cropping areas showed good agreement between simulated and measured paw errors in model estimates are compared with variability found within small fields we conclude that estimating paw for paddocks using a simulation model built in a smartphone app is a reliable and adaptable technology keywords decision support soil water water balance monitoring modelling app 1 introduction crop production in australian agriculture is limited by the water supply and water use efficiency wue kg ha mm french and schultz 1984 of farming systems many dryland farmers are familiar with concepts of yield targets based on wue which relates crop yield directly to water supply water stored in the soil at planting plus in crop rainfall wue is simple transparent and well suited to communication with farmers in a study of 334 commercial wheat crops hochman et al 2009 found a wue value of 15 kg ha mm and a threshold value of 67 mm nutritional disorders pests and disease reduce yield below these guideline values of wue and provide evidence of crop disorders cornish and murray 1989 nevertheless the importance of water supply to dryland crops is overarching as summarised by routley 2010 water supply is clearly the factor most limiting the productivity and profitability primary aim of dryland cropping systems maximise the efficient capture storage and use of this limited water in the northern and drier areas of southern australia there is insufficient rainfall during crop growth to achieve economically viable yields so fallows are used to accumulate soil water to supplement in crop rain this dependency on water stored in a fallow varies from 5 in western australia to 60 in central queensland thomas et al 2007 the need for improved soil water management may increase in the future under a changing climate as climate adaptations are likely to have a greater reliance on stored soil water kirkegaard et al 2014 ghahramani et al 2015 major investments in crop production occur at planting time and shortly after when an uncertain water supply makes prediction of yield and financial return difficult financial losses from both under investing and over investing in crop inputs are common but having a robust estimate of soil water at sowing time can reduce uncertainty thomas et al 2007 management options and farm financial risk profiles can be decided by soil moisture status of a paddock a high potential yield attracts greater investment in crop establishment nutrition moeller et al 2009 crop protection and informs marketing decisions on the other hand low yield potential informs a variety of agronomic and business decisions with inputs often being reduced although predicting grain yield before or early in the growing season is challenging applying the wue framework to predict yield is well established french and schultz 1984 moore et al 2011 and is improved by a reliable estimate of plant available water paw near planting paw is water that is available to plants during the crop phase is regarded as safe water as it is mostly immune to evaporation loses due to its depth of storage and sustains crops between rainfall events paw is calculated for each soil layer from the difference between gravimetric water content g g 1 and the soils lower limit ll or wilting point and considering the thickness and bulk density of each soil lawrence et al 2005 plant available water capacity pawc refers to a soil s capacity to store water and is often taken as a soil property although it can be dependent on crop type pawc is calculated from a soil s ll and drained upper limit dul or field capacity dalgliesh and cawthray 2005 estimating paw and pawc is expensive and labour intensive in this paper we explore errors in predicting soil water using a water balance model along with an analysis of spatial variability in relatively small fields it is recognised that there are errors associated with instrument calibration and estimating basic soil properties such as bulk density and ll required in calculating paw dalgliesh et al 2009 because of these errors and high spatial variability in field conditions paw is not a variable to be measured directly in a simple manner by farmers and consultants early simulation models of crop growth and yield were focused on predicting the supply of soil water with a view to managing crop water use and increasing wue e g fitzpatrick and nix 1969 nix and fitzpatrick 1969 the capability to estimate paw within soil and cropping systems models such as howleaky mcclymont et al 2016 and agricultural production systems simulator apsim holzworth et al 2014 is largely inaccessible to practical agronomists and farmers as those models were designed as research tools not as information products decision support tools that do incorporate soil and crop dynamics such as yield prophet https www yieldprophet com au require considerable system specification whereas the app being introduced here aims to provide a robust and rapid estimate of soil water aimed at farmers and consultants as users in developing a smartphone app to provide estimates of paw to farmers and their consultants it was considered prudent to understand the accuracy and reliability of a model based estimate of paw confidence in the performance of models is usually obtained by comparison with field observations for the key variables of interest to the scientist such as runoff erosion and water quality knisel 1980 williams 1983 littleboy et al 1992 or crop biomass and yield carberry et al 2009 while it has largely been assumed that models accurately predict paw such a narrow focus is expected as most components of the water balance are difficult to measure for example runoff is infrequent and unpredictable making it difficult to maitain equipment freebairn et al 1986 while deep drainage is technically difficult to measure and subject to high spatial variability humphreys et al 2003 while evapotranspiration is more spatially homogeneous and accurately measured variable in a water balance analysis calculations of the bowen ratio fritschen 1965 and related methods require advanced instrumentation complex mathematics and are labour intensive and expensive these methods are almost exclusively applied where crops are growing and water flux to the atmosphere is unable to be apportioned to soil evaporation and transpiration the analysis presented here was part of the design of a virtual soil water monitoring system soilwaterapp which is aimed to meet farmer and adviser needs we evaluate the water balance model in howleaky mcclymont et al 2016 used in soilwaterapp to estimate changes in paw also we investigate the ability of a smartphone app to estimate the components of water balance from meteorological soil and crop information providing estimates of paw for improved crop management through system design soilwaterapp is available from the apple store in australia and documented at http www soilwaterapp net au soilwaterapp has some special features fast simulation of the water balance on a smartphone or tablet connection to climate soil and crop databases accept on farm data and sufficiently user friendly to accommodate a wide range of users including farmers and consultants 2 water balance model the water balance model used in the app has evolved from creams knisel 1980 which predicted paw runoff and soil erosion from a combination of rainfall and evaporation data with i the runoff model of williams and laseur 1976 ii the soil evaporation model of ritchie 1972 and iii the usle for soil erosion williams 1983 creams was influential in the development of perfect littleboy et al 1992 and later howleaky mcclymont et al 2016 the latter model uses the williams ritchie water balance model williams and laseur 1976 ritchie 1972 which is a one dimensional mechanistic model with parameterisation strongly based on a wide range of empirical studies littleboy et al 1992 http howleaky net index php library simulation is performed on a daily time step surface runoff is estimated as a function of daily rainfall soil water deficit surface residue and crop cover the model has a cascading bucket structure where infiltration is partitioned into soil layers from the surface filling subsequent layers to total porosity in the model vertical water movement occurs if the layer is wetter than its field capacity and the layer below is drier than its field capacity water flux is limited by the saturated hydraulic conductivity of each layer soil water can be removed from the profile by transpiration soil evaporation and downwards movement from the lowest layer as deep drainage transpiration is a function of pan evaporation a climate input leaf area or percentage green cover and soil moisture soil evaporation removes soil water from the upper two layers the sum of transpiration and soil evaporation evapotranspiration cannot exceed pan evaporation on any day a summary of the soil water balance model in howleaky is presented in the supplementary material s1 3 architecture software and data soilwaterapp has been developed for ios devices using apple s native objective c framework and communicates with a central cloud based server for synchronising both app and user data operating the app involves setting up and monitoring a range of sites with different agro climatic variables selecting a site in the user interface will present an analysis page which automatically updates the soil water results for the latest climate conditions using the howleaky model during this process it will update any outdated climate data and provides the user with a range of input and output infographics that progressively disclose more detail as the user scrolls down fig 1 inputs are presented at the top of the analysis page and are grouped into soil starting conditions fallow crop conditions irrigation local rainfall and soil water sensor options outputs include a summary of predicted paw a time series of recent historical past years and predicted plumes of soil water recent stubble and crop cover a soil moisture profile graph and a water balance summary table the app has been developed with a multithreaded design for parallel processing of data input output and analysis streams it is composed of a range of independent functional modules for data input storage and synchronisation and for running soil water analyses using the howleaky engine fig 2 shows these modules and how they interact with each other and external data sources database operations are handled by a coredata manager module and multiple synchronised database instances known as managed object contexts separate contexts ensure that data integrity is maintained during asynchronous operations of the data synchronisation manager bluetooth manager and running of the howleaky engine temporary view models are used to safely transfer data between the coredata manager and howleaky engine and to provide an undo and redo functionality for user settings changes a climate data manager module facilitates updating local climate data from the silo data server https www longpaddock qld gov au silo while an export manager module allows analysis data and results to be shared with other users the app communicates with several external application programming interfaces apis including a cloud based database a climate data provider and bluetooth rain gauge and soil water sensors key to the software s operation is a data synchronisation manager which synchronises both application and user data between multiple mobile devices and a server data includes soil and vegetation descriptions climate locations project site and simulation data and analytics to track the app s use this allows data to be collected on one device with or without an internet connection and synchronised with the soilwaterapp server and other mobile devices once an internet connection exists it operates in different modes including one way from the server to device two way between server and device and one way between device and server depending on the nature of the data soilwaterapp relies on a range of short and long term time series data that are retrieved and stored using different methodologies long term records of daily historical climate data from the silo patched point database jeffrey et al 2001 www longpaddock qld gov au silo are stored locally as ascii files on the user s device and automatically appended to each time the app is active these files are replaced after 2 weeks as the silo data files are prone to data corrections over time users local short term data such as rain gauge soil water sensor and irrigation data are stored in the database in records spanning three monthly data chunks to facilitate efficient retrieval and synchronisation between device and server 4 model evaluation performance of the water balance model in soilwaterapp was evaluated for three data types fallows with detailed long term observation of gravimetric soil water collected from hydraulically driven soil cores three sets of small catchments daily data for a sequence of fallow and crops at two sites using capacitance probes and a set of bluetooth enabled decagon soil water sensors and a rain gauge integrated with the soilwaterapp comparisons between soil water observations and model estimates were carried out using the howleaky model mcclymont et al 2016 as this was the most efficient process and the app is not suited to model testing the model in the app and howleaky were verified to produce identical outputs when given the same inputs 4 1 fallow in australian dryland cropping systems fallows are instigated to store soil water accumulate soil nitrate and control weeds in preparation for the next crop to reduce the risk of failure and increase yield especially where soils have a high water holding capacity field observations of fallow effect on soil water were available from three long term experimental sites in the state of queensland greenmount 27 44 27 s 151 51 33 e greenwood 27 19 42 s 151 43 47 e wallumbilla 26 34 28 s 149 11 17 e where the focus of each study was to better understand the role of tillage and stubble management on moisture and soil conservation freebairn and wockner 1986 freebairn et al 2009 rainfall runoff and soil water were monitored over periods of 7 17 years equivalent to 170 plot years of data with each plot being sampled at least three times each year start mid and end of each fallow period each soil sampling consisted of nine soil cores taken in each bounded catchment referred to hereafter as a bay at 0 10 10 30 30 60 60 90 90 120 and 120 150 cm depths see fig 3 a for sampling patterns plant available water paw mm was estimated from a field measured lower limit ll cm3 g dalgliesh and foale 1998 and measured soil bulk density g cm3 and is presented in this paper as an average of total profile paw for each sample date field experiments were conducted under two common but contrasting soil management conditions stubble burnt after harvest with little soil cover and zero tillage with 30 80 cover from crop residue measured and predicted change in paw between sample dates is used rather than absolute values of paw to reduce artificial skill in the statistics all related weather agronomy practice and soil descriptions and soil water data are accessible from a database of experimental sites focusing on water balance and water quality across australia http howleaky net index php library 4 2 fallow and crops soilwaterapp s modelling engine was evaluated for its ability to simulate paw in crop fallow sequences at two sites in the state of the victoria youanmite 36 1639os 145 6640ow and hamilton 37 7277os 141 9242ow each site had one sample location with 8 capacitance sensors at 30 40 50 60 70 80 90 and 100 cm depths enviropro ep100g 08 entelechy pty ltd adelaide australia the shallow sample was located to avoid damage by tillage equipment and was sited to avoid wheel tracks the surface 25 cm was not sample while these data are more limited in terms of duration and fallow management practices the daily time series provides more detail on soil water dynamics using measuring equipment similar to that increasingly being installed by farmers and consultants in essence this level of detail is a useful benchmark for evaluating soilwaterapp s ability to track soil water dynamics at a daily time step 4 3 surface water dynamics soilwaterapp accepts data directly from a bluetooth enabled data logger connected to a tipping bucket rain gauge and three decagon devices inc 10hs soil water sensors to test a prototype system we installed a sensor logger system into a bare soil plot at the university of southern queensland toowoomba 27 36 52 0 s 151 56 14 e daily rainfall and soil water content were recorded from june to october 2016 20 weeks in this experiment the surface 10 cm of soil was monitored at 4 cm 6 cm and 8 cm representing a measurement zone of 0 5 cm 5 7 cm and 7 10 cm respectively with a focus on testing the evaporation algorithm within soilwaterapp as well as the robustness of deploying the system into a field setting the soil description reflected the sensed layers for a shallow ferrosol 5 spatial variability in field measurements plant available water capacity pawc is often taken as a soil property and is an important descriptor used in biophysical models we explored the spatial variability in measurements from three long term datasets also used in section 4 1 that represent cropping in southern queensland available at http howleaky net index php library a soil survey with detailed pedology and chemical assessment did not reveal any marked differences in soil type across a 12ha site with soil depth 2 m across the three sites i e soil depth should not be limiting to plant growth at greenmount and greenwood distance between sample areas is relatively short 50 60 m greater variability would be expected at wallumbilla as it was a larger site with sample sites 150 m distance apart on a diverging slope samples displayed greater variation in colour when sampled all three study sites represent areas 10 of a normal paddock in their respective regions 6 operating the app daily rainfall and evaporation data from silo are accessed by soilwaterapp via mobile telephone or wi fi networks once a site is established climate soil type cover and crop dates taking a new user 5 min soil water estimates are immediately updated using weather data up to the day before yesterday each time the app is opened multiple sites can be established and shared between users given the high spatial variability of rainfall and sparse network of gauges in some parts of grain growing regions users can enter local rainfall data manually or use an automated rain gauge that connects to the mobile device via bluetooth low energy ble in addition to the ble rain gauge a small number of ble data loggers have been deployed with soil moisture sensors to monitor paw daily whenever the user is within ble range 20 m a seamless communication and transfer of the soil water and rainfall data from the logger to the app occurs the app and its associated api includes databases of soil parameters for estimating daily runoff deep drainage soil evaporation and pawc similarly a database of crop parameters describing green cover and root depth distributions is required for estimating transpiration and soil water extraction crop residue cover used to modify infiltration and evaporation is specified by the user databases are updated with app use while a system administrator can easily manage the reference databases of soil and vegetation descriptions 7 results and discussions 7 1 modelling capability of the app the performance of the app s modelling engine in simulating changes in paw over fallows and crops for a range of rain fed agricultural systems is evaluated fig 3a presents a comparison of simulated and observed gains in paw for the three long term field studies in the state of queensland greenmount greenwood and wallumbilla there was agreement between simulated and observed values for each practice and soil type the scatter of measured and observed data in fig 3a has a range of r2 values from 0 72 to 0 76 across sites and treatments with an overall r2 of 0 69 indicating reasonable confidence in model estimates across multiple seasons soil types and fallow management conditions regression analyses were carried out using ms excel statistics the pattern of error shown in fig 3a frequently occurs in models of this type due to inadequate representation of biological and physical processes that give rise to low and high values for example regression of observed and apsim predictions in 15 studies of crop yields resulted in low slope 1 in 12 studies and a high intercept 0 in 13 of those studies carberry et al 2009 at youanmite state of victoria daily values of observed and predicted paw are in step during the crop of 2015 that was planted with low paw and subsequently suffered severe water stress fig 3b the disagreement between simulated and measured replenishment of paw over the summer fallow where howleaky overestimated paw accumulation may be attributed to errors in parameters describing surface soil water characteristics or missing rainfall data the flat sections in model estimates are an artefact of soil water data being available for 30 100 cm layers as sensors were buried beneath the tillage depth plotted model estimates ignored layer 1 0 20 cm therefore not showing soil surface water dynamics paw predictions in june and july are in agreement with measurements an important outcome given the app is used to guide inputs during early crop growth results from the hamilton site show good agreement between observations and estimates of paw with both accumulation and depletion of paw predicted well fig 3b these comparisons lead to confidence in predicting gains in paw during fallows with small and generally explainable errors during periods of crop water use an analysis of errors found that unreliable rainfall data and poor specification of soil constraints accounted for most poor predictions of paw unreliable rainfall data came from poorly maintained digital rain gauges it is recognised that field data collection is challenging given environmental extremes while soil specification remains a challenge for site specific applications of models fig 3c shows paw estimated by soilwaterapp and measured values derived from capacitance sensors demonstrating linking local rainfall and soil water data with the ability to customise a soil type for a site fig 3c shows that when soilwaterapp is suitably configured its water balance simulation and field sensors provided comparable estimates of soil water in a simple experiment focusing on evaporation losses this combination of sensors a bluetooth logger and soilwaterapp is a valuable development in bringing a simple and reliable soil water sensing system and modelling to soil scientists agronomists and eventually farmers while this system is not commercially available fig 3c displays the system s simplicity low cost accuracy without calibration and direct link between a monitoring device and a water balance simulation 7 2 why a virtual soil water monitoring tool to compare field variability of soil water observations with errors associated with modelling soil water individual soil core data from the wallumbilla study were analysed in detail for several sample dates at the studies initiation the site was chosen to be representative of a major soil type of the region and to be relatively uniform across the site yet fig 4 a shows variability in pawc 180 280 mm for four adjacent catchments over a 15 ha site see fig 3a pawc is regarded an important soil descriptor in water balance models and is important in calibrating soil sensors fig 4b shows the variability in measured paw for nine sites within a 3 ha bay for a sample date at the end of a summer fallow while nine samples were collected with the aim of providing a robust estimate of paw and changes in paw for the 3 ha catchment it is clear that reliance on a different number of sample locations would produce a different result the standard deviation of gravimetric soil water for any depth is 1 2 which translates into a standard deviation of 50 mm of paw an outcome from these observations might be that users of models might have a more realistic view regarding specification of a model given the natural variability within a small area these results from an intensively sampled experiment over 17 years demonstrate that even in sites selected for their apparent uniformity it is difficult to assume reliable values of soil lower limit ll or wilting point drained upper limit dul or field capacity and consequent pawc even if errors in estimating soil bulk density are ignored it follows that when soil water monitoring equipment is installed that site selection will be challenging as the single sample site might be any one of the points on fig 4a or 4b it is worth putting all these estimates of soil water into a context with where they may be used by decision makers for example farmers estimate paw in numerous ways intuition from rainfall records and simple rules e g a percentage of rainfall stored steel push probes to detect the depth of moist soil observation of crops weather and soil water monitoring systems and models and apps such as presented here table 1 lists some strengths and weaknesses of six methods of estimating paw each approach has a fit in the real world of decision making for farmers putting a financial value estimates of soil water regardless of method is a challenge dependent on enterprise decision makers attitude to risk and application of information to the decision making process and won t be attempted here nevertheless a simple cost comparison of two contrasting approaches to tracking soil water application of soilwaterapp and the installation of a commercial weather and soil monitoring station is presented while the cost of developing soilwaterapp was considerable aud 500 000 this cost can be spread over many users with minimal ongoing costs a weather station with soil water sensors may cost aud 2 10 000 per installation based on this simple cost model when the app is used for 250 sites the cost per site will be less than a physical installation soilwaterapp has 2000 active sites being monitored 18 months after its release more detail is presented in supplementary materials s2 7 3 adoption of soilwaterapp to date while no formal evaluation of soilwaterapp has been completed fig 5 shows locations and accesses in the first 18 months since soilwaterapp s release soilwaterapp is being applied across australia s grain growing regions where the importance of soil water status is well recognised this initial adoption rate sets the scene for more in depth assessment of the value of paw plant available water estimates for grain growers it is noteworthy that the top 200 users by a number of sites comprise 80 of the total sites to date many of these users are consultants who are monitoring multiple sites 8 conclusions we conclude that soilwaterapp s modelling engine can reliably estimate patterns of paw through fallows and crops and that an app is a practical approach to bringing climate weather soil and crop information together for farmers and consultants to estimate paw in their decision making simulation based estimates of paw are shown to be reliable and less expensive than physical measurements an app based estimate of paw is well suited to application across multiple paddocks and soil types model based estimates of soil water are reliable easy to access in a mobile device app acknowledgement soilwaterapp was developed for the grain research and development corporation project new tools to measure and monitor soil water usq 00014 development has benefited from significant contributions of grain growers who acted as beta testers and scientists who shared data for testing the water balance model we thank dale boyd of agriculture victoria in particular for his generous sharing of detailed data from two victorian sites authors acknowledge useful comments made by 3 anonymous reviewers appendix a supplementary data the following is the supplementary data related to this article supplementary materials supplementary materials appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 010 
