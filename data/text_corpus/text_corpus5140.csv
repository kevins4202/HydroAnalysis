index,text
25700,automatic calibration is widely used to estimate parameters in hydrological models the main idea is to use optimization algorithms to minimize the discrepancy between field data and simulation prediction this process involves iterative exchanges of a parameter set chosen by the optimization and simulation however for computationally expensive models such as groundwater flow and transport models the calibration process may be extremely computationally demanding in this study we introduce and demonstrate a new asynchronous parallel surrogate assisted optimization algorithm with an early truncation feature and different knowledge extraction strategies so aet k results show that our asynchronous algorithm performs significantly better than the synchronous analogue so sp requiring only 40 70 of the computation time to achieve the same averaged results in addition various knowledge extraction strategies of the asynchronous so aet k are tested in comparisons with two other algorithms sce ua and appspack asynchronous so aet k shows significantly better performance in terms of both efficiency and robustness keywords asynchronous parallel knowledge based early truncation surrogate based optimization automatic calibration computationally expensive groundwater model software availability software name pysot year first official release 2015 hardware requirements pc system requirements windows linux mac program language python program size 600 kb availability https github com dme65 pysot documentation https pysot readthedocs io en latest note the default setting of knowledge extraction strategy in pysot is so aet 3 1 introduction optimization methods are widely used to automatically estimate model parameters for a variety of environmental models maier et al 2019 gong and duan 2017 such as large scale hydrological models koppa et al 2019 razavi and tolson 2013 hydrogeological models meyer et al 2018 zhao et al 2019 integrated surface subsurface models danapour et al 2019 the main idea to solve calibration problems is to apply optimization algorithms which find the parameter vector x to minimize a loss function f x that measures how well the model fits observed data many previous studies have been dedicated to automatic calibration much of this work involves derivative based approaches such as quasi newton cheng and yeh 1992 and levenberg marquardt methods dai 2004 and these methods have been integrated into software suites such as pest doherty and hunt 2010 but not all third party codes provide relevant derivatives and even when derivatives are available most derivative based methods are designed to converge to local optima various heuristic optimizers have also been adopted for calibration problems including genetic algorithms giacobbo et al 2002 solomatine et al 1999 adaptive cluster covering solomatine et al 1999 particle swarm optimization and pattern search haddad et al 2013 and differential evolutionary algorithms haddad et al 2013 chiu 2014 related approaches based on artificial neural networks karahan and ayvaz 2008 rather than posing the calibration problem directly as an optimization instead fit a meta model that maps observations to model parameters based on training examples however all these methods may require many expensive computational simulations to reach an acceptable solution for many realistic models the evaluations are expensive so only a few can be made the underlying simulations are black box codes so no analytical derivatives are available and the loss function is a complex non convex function of the parameters which may have many local minima especially expensive are simulations of models based on partial derivative equations pde for water flow and contaminant transport in surface or ground water for example surrogate based optimization methods jones et al 1998 gutmann 2001 björkman and holmström 2000 mugunthan et al 2005 regis and shoemaker 2013 are explicitly designed for complicated and computationally expensive simulation these algorithms approximate the true expensive function evaluations f x by a less computationally expensive approximation s x called a surrogate response surface or meta model and use this approximation to guide further sampling in high dimensional spaces radial basis function and gaussian processes are often used as surrogates and a variety of strategies have been proposed to balance exploitation of the surrogate to sample near predicted minima and exploration of parts of the space where there may be insufficient data for the surrogate to provide accurate predictions this surrogate approximation approach to optimization has been shown to reduce the number of costly objective function evaluations necessary to find a good answer regis and shoemaker 2005 2007 müller and shoemaker 2014 krityakierne et al 2016 timani and peralta 2017 wu et al 2020 many optimization algorithms have been parallelized so that they can make effective use of high performance computing systems in many groundwater related problems such as contaminant source identification water management pollution remediation and model parameterization problems parallel optimization has been proposed and shown to perform well ketabchi and ataie ashtiani 2015 mirghani et al 2009 sayeed and mahinthakumar 2005 garrett et al 1999 vrugt et al 2006 but the strategies currently favored in groundwater applications focus on synchronous parallelism i e methods in which several function evaluations are started simultaneously and all must finish before proceeding to the next step when simulations may have dramatically different run times depending on their inputs synchronous parallel methods can be very inefficient as many processors may stay idle waiting for one long running simulation to end in this situation asynchronous parallelism may make more efficient use of hpc systems several asynchronous parallel optimization algorithms have been studied such as asynchronous tabu search on location allocation problems crainic et al 1996 asynchronous evolutionary algorithms lücken et al 2004 in pump scheduling asynchronous particle swarm koh et al 2006 on biomechanical problem and the development of asynchronous parallel pattern search hough et al 2001 in machine learning asynchronous parallelism has been used for calibrating complex neural networks bottou et al 2018 many approaches are derivative based such as stochastic gradient descent niu et al 2011 feyzmahdavian et al 2016 but to our knowledge very few previous studies have been dedicated to parameter calibration problems for physical models in this paper we further reduce the time required for calibration of computationally expensive water resource models by selective early truncation of evaluations together with an asynchronous parallel strategy for example in calibrating a transient water resource model the loss function might be the sum of squared errors over n successive time steps suppose at a proposed parameter value x the sum of squared errors over the first k n time steps is much worse than the sum of squared errors over all time steps at the current best solution x then we need not compute the errors at x from time steps k 1 n and can save time by terminating the simulation early razavi et al 2010 has introduced a deterministic model preemption technique in parameter calibration for hydrological models showing that an early termination strategy can save large amount of the computational budget around 50 for optimization based automatic calibration problem however their approach does not provide set up for dynamical model termination threshold and no parallelism strategy especially asynchronous parallelism was yet used in their applications since we allow early truncation different function evaluations may take different amounts of time as a result bulk synchronous algorithms that perform batches of simulations together may leave processors idle in each step as the length of the step is determined by the slowest simulation in order to avoid this we employ the asynchronous parallel strategy that can start new simulations as soon as processors become available to make efficient use of the early truncation technique the algorithm we apply in this paper is an asynchronous version of the parallel stochastic radial basis function srbf method regis and shoemaker 2009 we use a surrogate optimization toolbox pysot eriksson et al 2015 which contains options for different surrogate types and parallelism paradigms we apply the method to a modified real world groundwater calibration model from the umatilla super fund site the contribution of this paper is that it is the first use of a surrogate based asynchronous parallel optimization algorithm with an early truncation strategy for calibration truncation threshold is defined dynamically in a way to combine both information we obtained in optimization phase and simulation phase and rules of knowledge extraction are defined to extract the information from the truncated solution and they are incorporated into the asynchronous optimization algorithm we illustrate the efficiency of our early truncation algorithm in an asynchronous parallel optimization framework through experiments using the pysot toolbox for surrogate optimization 1 1 https github com dme65 pysot our results suggest the computational benefits of the approach for other calibration problems with computationally expensive models 2 calibration problems of groundwater model calibration of parameters for a pde model of groundwater flow and transport can be done by combining optimization with relevant data which includes spatial distribution of hydraulic heads and concentration observations as well as input data such as pumping rates other studies have shown that coupled estimation of flow and transport parameters provides more reasonable and more certain parameter estimates than alternate procedures that use only subsets of observations e g only heads or only concentrations hill and tiedeman 2007 we work on calibration problems that are based on integrated groundwater flow and transport models the parameters to be estimated are the hydraulic conductivities distributed on the whole study area scenario 1 the easy site case is based on the true distribution of parameters in the model with nine hydraulic conductivity values each representing a zone with the same hydraulic conductivity scenario 2 the difficult site case has an additional cross shaped area representing two relatively low permeability zones each with an individual hydraulic conductivity value in both cases we want to see if we can obtain the actual distribution of hydraulic conductivities through efficient optimization 2 1 case study umatilla superfund site our case study is the calibration problem based on one u s environmental protection agency epa groundwater superfund site umatilla chemical depot ucd oregon which was contaminated with untreated wash water containing chemical components management of this pump and treatment system is one part of a demonstration project application of flow and transport optimization code to groundwater pump and treat systems funded by the u s department of defense dod and the environmental security technology certification program estcp the detailed dod estcp study report model and data can be found on the project website http www frtr gov estcp in our application a steady pumping strategy is assumed during a four year management period and the contaminant tnt is chosen as the only indication chemical to be removed on site which was a setting based on study from utah state university contained in the dod estcp report the simulation of the groundwater flow and transport is based on usgs modflow2005 harbaugh 2005 and mt3dms 5 3 zheng and wang 1999 model the model contains 132 columns 125 rows and 5 layers where layer 1 i e the top layer represents silt and weathered basalt in convertible state between confined and unconfined aquifer and the other layers are confined alluvial aquifers the hydraulic conductivity in the aquifer is highly heterogeneous especially in layer 1 which varies from 1 ft day to 5000 ft day zones of different hydraulic conductivity in the modflow model are shown in fig 1 there are ten different hydraulic conductivity zones in the current dod estcp model among them one zone has comparatively far lower conductivity than the others as it represents the boundary of an impermeable area based on the this configuration we choose the 9 other hydraulic conductivity zones which vary from 100 ft day to 5000 ft day as our parameters to be calibrated as in scenario 1 to add more complexity and calibration difficulty in the problem two less permeable zones are added in scenario 2 as shown in fig 1 b our goal is to use spatially distributed time series data on hydraulic head and contamination concentration to estimate spatially distributed hydraulic conductivities which are parameters in a transient groundwater flow and transport pde model 2 2 observation data the observation data is obtained from modflow and mt3d simulations using the true values of parameters the goal of the optimization is then to pick the values of the decision vector e g the parameter values that best fit the synthetic observation data for scenarios 1 and 2 the observation data is obtained from the values of configurations given in fig 1 a and b respectively fig 2 illustrates the distribution of all observation wells as well as the hydraulic conductivity zones these wells are not actual in situ wells we set up the locations of heads monitoring wells randomly in the whole region while contamination monitoring wells are based on prior knowledge of the migration of the plume that is to say wells for contaminations are only designed in the region through which the plume travels in order to better capture the value change there are in total 107 observation wells for hydraulic heads and 27 wells for contaminant concentrations the observation data for the calibration problem contains one set of hydraulic heads in all 107 wells at the end of the four year management period since pumping rates and heads are constant in time there is a weekly contamination concentration monitoring data in 27 observation wells so overall there are 107 observations of heads and 52 4 27 observations of contamination concentrations in the four year time period 2 3 flow and transport model the modflow flow models for the two scenarios are assumed to be in steady state with constant hydraulic heads provided for the four year management period as in the study by utah state university in the dod estcp report in contrast the mt3d contamination transport model involves an implicit time stepper and the nonlinear solve required at each transport step may vary in cost depending on the model inputs thus the simulation time required to obtain contaminant concentrations at a given intermediate time point may vary depending on the model parameters though the variation is not great 3 model formulation 3 1 objective function the goal of calibration is to minimize the deviation between the observation and the simulation in our problem the error consists of differences of values in n h 107 head monitoring wells at one time since pumping rates are time invariant and in n c 29 contamination concentration wells on a weekly basis as contamination is changing in space and time the decision variables are hydraulic conductivity values for different spatial zones which are represented in the decision vector x r d where d 9 for scenario 1 and d 11 for scenario 2 the optimization problem is 1 min x r d f x f x g x t where t is the total number of transport steps a constant and 2 g x τ w 1 h x 1 w 1 c x τ for 1 τ t where 3 h x n 1 n h h n s i m x h n o b s 2 4 c x τ m 1 n c t 1 τ c m t s i m x c m t o b s 2 for 1 τ t subject to the constraints 5 x i m i n x i x i m a x in our model x i min 100 ft day and x i max 5000 ft day respectively h n s i m and h n o b s are the simulated and observed hydraulic head at head observation well n c m t s i m and c m t o b s are simulated and observed contamination concentration at concentration observation well m in week t index n in eq 3 stands for hydraulic head wells t stands for time and m in eq 4 is for contamination well t is the total number of transport steps one each week for four years i e 208 the weight w 1 tn c n h tn c compensates for the difference in the number of terms in the sums in eq 3 and eq 4 g x τ increases monotonically with τ because each term in the sum is non negative we consider early truncation of the simulation at a time τ t to obtain a lower bound g x τ f x where τ is chosen by a truncation strategy described in the next section 3 2 early truncation strategy in conventional optimization based calibration we would evaluate the objective function g x t in eq 2 for all time steps so τ t in eq 2 hence the objective function is fully evaluated which can be very expensive we can potentially reduce overall computation by prematurely terminating computation of g x τ for a specific x and τ t if the value of g x τ in eq 2 is poor e g large compared to value of g x t we have for other x this is because the full value g x t g x τ as explained in section 3 1 and once an unacceptably large sum of errors has been seen in the middle of the simulation when τ t there is little benefit to completing the simulation what we develop is a method to determine under which condition to stop the calculation of eq 4 for τ t thus while we always let the modflow flow model run to completion we develop a method to terminate the more time consuming transport simulation with mt3d if its error exceeds some threshold we will call this early truncation to clarify how it works we use fig 3 to demonstrate a possible time pattern of function evaluations we can obtain in the asynchronous parallelism with early truncation strategy assuming we have 12 simulations that are conducted in three cores simultaneously the total wall clock time required for this task is t 3 which ends at 11th started simulation as shown in fig 3 the numbers on the box of simulation represent the ith started simulation two simulations that are truncated early are shown in red boxes the problem lies in how to determine the threshold for which early truncation makes sense 3 3 early truncation threshold early truncation involves a trade off between the cost of computation and the benefit of having a computed value of f x that can be incorporated into the surrogate if the computation is terminated after only a few steps we save computational effort but learn little about the function choosing to terminate later has less benefit but may be worthwhile if the cost function is much larger than the cost of other points that the algorithm has already explored at the ith function evaluation we balance the time savings of early termination of the ith evaluation against the benefits of completing the evaluation through the criterion 6 truncate if g x τ θ τ l s τ l i p and τ l τ τ u where l i p represents the pth percentile of previously computed values or partial evaluations of the objective function l i p is given to the simulation before it starts so there is no communication burden within the simulation as shownshow in fig 3 the 9th started simulation is truncated and the component of its truncation threshold is l 6 p as we have 6 simulations finished before the 9th simulation starts for the 12th simulation whether to use l 9 p as part of truncation threshold depends on which rule of knowledge extraction we use as described in table 1 7 l s τ 1 τ τ u τ u γ l s increases monotonically for τ τ l τ u truncation does not go into effect until τ l steps have been taken nor after τ u steps have been taken in our study we set τ l 1 and τ u 0 9t the functions l i p and l s τ are used to adapt to the stage of the optimization algorithm and the work spent on each function evaluation respectively the value of l i p decreases as the optimization proceeds and more values are computed so the algorithm can be more aggressive about early truncation later in the optimization process it is sensible that the algorithm would become more aggressive as it obtains more information to facilitate searching the optimum we choose p 75 in this study the idea with l s τ is that since g x τ is a sum of positive terms we should terminate earlier for smaller values of τ in our study we choose a cubic polynomial γ 3 in eq 7 for the reason that 1 τ τ u τ u 3 has a large gradient when τ is small so we can allow more trials to be fully executed at the beginning of the simulation and as τ grows the gradient of 1 τ τ u τ u 3 goes smaller therefore the criteria to allow full execution becomes tougher and tougher for large τ various linear or nonlinear equations for l s may also be used depending on prior knowledge of the optimization problems fig 4 illustrates the role of early truncation in the figure the top curve θ 1000 shows the shape of threshold function θ τ at iteration 1000 if we assume a simulation has objective function g x τ based on the vector of parameters as x and value of g x τ is given by the green curve in fig 4 then the evaluation of g x τ would stop at black cross with τ at which g x τ exceeds the threshold θ τ similarly at iteration 2000 the threshold function θ 2000 would be used resulting in truncation at τ 4 optimization algorithm 4 1 so sp regis and shoemaker 2007 describe a strategy for finding the global optimum of a computationally expensive function using a metric response surface mrs the idea is that in each iteration of the algorithm an approximation of the objective function f x also called a response surface or surrogate is constructed based on all the values x f x computed in previous iterations in our study we use an mrs strategy with synchronous parallelism called so sp surrogate optimization with synchronous parallelism so sp combines two algorithms parallel lmsrbf regis and shoemaker 2007 and dycors regis and shoemaker 2013 and it is implemented in our open source software pysot https github com dme65 pysot the detailed steps in so sp are described in algorithm 1 in steps 1 to 3 of algorithm 1 we use an experimental design i e a latin hyper cube to create the initial surrogate approximation using f x in the algorithm iterations rather than fitting the objective function f x we fit the capped function i e f capped x which replaces objective function values from simulation by the median of all solutions obtained so far 8 f c a p p e d x min f x median n a i f x n where a i is the set of indices of points evaluated at round i here capping helps bound the value range so the surrogate function can focus its approximation on the more valuable area for optimal solution therefore we seek to avoid oscillations in the surrogate associated with very large function values and gradients in regions far from the minimum as the iteration goes on the search is either focused on the part with small values or exploring new area so capping is applied during the whole optimization process to help better approximate the areas with small values we then update parameters and check whether we need to restart the whole algorithm or not by using function restart criterion check in restart criterion check we keep track of the number of consecutive successes i e c s u c c e s s and number of consecutive failures i e c f a i l in order to determine the search radius σ i at iteration i the criterion of success iteration is that the best point we found in iteration i i e x i is better compared to all the points we evaluated so far i e f x i f b e s t otherwise this iteration is a failure thresholds t s u c c e s s and t f a i l are pre defined values and once the number of reduction of search radius reaches to the pre defined r max we will restart the algorithm from scratch the restart technique helps the search to escape from a local optimum xx image 1 from step 6 we start to determine the points for function evaluation by first generating candidate points set ω i which is generated from using a normally distributed perturbation amplitude around best points evaluated so far i e x best with a declining probability of perturbation occurring for each iteration as described in eq 9 9 p i p 0 1 log i i i n i t 1 log i m a x i i n i t where i is the current number of iterations i init is the number of iterations for initial design i max is the total number of iterations and p 0 is the initial probability therefore p i is a decreasing function of i the procedure for candidate points was suggested in regis and shoemaker 2013 and eq 9 was suggested by tolson and shoemaker 2007 for the dynamic dimensional search dds algorithm selection evaluation points function described in the function box is used to select points to be included in set of evaluation points i e d i there are two metrics considered in selecting the point for d i 1 the estimated values evaluated on the current surrogate surface 2 the distance information from previously evaluated points to the new one the basic concept is to choose the point which has a good i e low estimated value and also has large distance to the closest previously evaluated points in order to explore more in the area that has not been searched that is to say as we need to generate p number of evaluation points i e s i z e d i p selection evaluation points function need to be processed p times one for each point generation in step 7 so sp starts all cores i e p cores at the same time to evaluate p points in d i for expensive functions results following a synchronous parallelism paradigm after all computational resources i e p cores finish evaluations we can then update both the set of evaluated points a i as well as the response surface based on set a i this iteration loop forms the basic structure of the so sp algorithm image 2 algorithm 1 so sp synchronous parallel image 3 we implement so sp by using surrogate optimization toolbox pysot in https github com dme65 pysot the realization of the parallel paradigm in pysot is based on poap which is an event driven framework for building and combining optimization strategies there are two parts in poap controller and strategy the controller communicates between worker cores and master core in order to distribute evaluation jobs the master core which runs the strategy is responsible for building the surrogate surface and selecting candidate points for evaluation in this study we use the mpi message passing interface version of poap for the parallel communication in high performance computing system and as to strategy we use rbf surrogate surface and candidatedycors method for generating the evaluation points as discussed in eq 9 4 2 so aet so aet is the new algorithm we introduce which combines features from so sp and asynchronous parallelism and it incorporates a new early truncation function as discussed in section 3 2 and table 1 the diagram of this so aet can be found in fig 5 algorithm 2 describes the major steps in so aet as a start we evaluate points in the initial experimental design by objective function f x and initialize set of distance relevant points as a 0 e i and set of surface relevant points a 0 s i in step 2 definitions of both distance relevant and surface relevant are described in table 1 distant relevant points are used in the distance metric in function selection evaluation points to select evaluation points surface relevant points are used in construction of an update surrogate approximation s i x in each iteration then initial points are dispatched to workers and evaluated asynchronously without any truncation and surrogate surface s 0 x is built once all the initial evaluations have been completed or pending using objective function f x as in step 8 once a function evaluation is done we use restart criterion check to determine whether to restart or not similar to so sp if no restart is needed we check the point x i evaluated by simulation model with the early truncation strategy described in eq 6 as in asynchronous paradigm different cores can carry out at different time each time a core finishes the evaluation of x i there are two possible outcomes due to eq 6 1 x i is an early truncated point 2 x i is a fully evaluated point for a fully evaluated point we should follow the same updating technique as in so sp that is to say x i is categorized as both a distance relevant point i e x i e and surface relevant point i e x i s in step 5 in algorithm 2 if x i is a truncated point we record the value of x i and predict its full objective value l p as in eq 10 10 g x w 1 h x 1 w 1 c x if not truncated l p if truncated and then we can determine the category of x i i e distance relevant point or surface relevant point or both based on three different rules of knowledge extraction so aet k for k 1 2 3 which are proposed to help us determine how to incorporate the information of x i the definitions of the three different rules of knowledge extraction are also shown in table 1 if no other worker is updating the surrogate rbf the idle worker will update the rbf surface the value of eq 10 is used to build the response surface for the points either truncated or not based on different so aet k i e if they are considered surface relevant truncated points are assigned the p th percentile value of existing points in step 9 and 10 we generate candidate points ω i by perturbing selected decision variables around the best solution found so far x best the evaluation point x i is selected based on metrics function select evaluation point among candidate points one at a time and then it is added into evaluation queue and popped out to dispatch to a worker for evaluating expensive simulation with possible early truncation the evaluation queue q helps to ensure that after all the points in the initial design are dispatched to worker we can start building the response surface it is dynamically changing as we pop out x i to be evaluated and also add new x i 1 in algorithm 2 so aet asynchronous parallel with early truncation image 4 the explanations of the difference in synchronous and asynchronous parallelism are as follows in synchronous parallel version if any core finishes its job early it needs to wait for all the other cores to finish in order to proceed in each iteration with p cores used we update the rbf surface based on results of p evaluations in each iteration then the set of informative evaluated points and the internal parameters in so sp are updated by contrast for so aet with asynchronous parallelism each core can start its evaluations once it finishes its job without considering the working status of other cores consequently surrogate surface informative evaluated points as well as internal parameters in the algorithm are updated soon after each function evaluation instead of waiting for other cores to finish therefore asynchronous parallelism prevents cores from being idle in the optimization processes if there is no variability in objective function computation time the synchronous parallel algorithm so sp is possibly better as with synchronous optimization each new response surface is updated based on all p evaluations of the objective function with p cores therefore the next evaluation point will be found based on a more informative response surface compared against the case in the asynchronous version for computationally expensive functions that take various times given different inputs the overall computational budget for finding the optimum inputs can be reduced with asynchronous parallelism we expect the advantages of asynchronous to increase as the variability of the objective function evaluation time increases 4 3 other algorithms we will compare so aet and so sp to other algorithms the comparison are one asynchronous local optimizer and one synchronous global optimizer 4 3 1 appspack asynchronous parallel pattern search pack appspack toolbox solves nonlinear optimization problems it is a local optimizer but does not require derivative information the toolbox is based on the asynchronous parallel pattern search apps algorithm hough et al 2001 in apps trial points are iteratively generated according to the parameters defined as search direction and step size based on whether the best evaluation of trial points is found in search or not the search step is identified as successful or unsuccessful step and in consequence the direction and step size change in different fashion successful step changes direction of the search whereas unsuccessful step narrows the selection of trials points around the current best solution as iteration goes on the search domain would be narrowed to the optimal point but since it is a local optimization method the search may be trapped in a local optimum another characteristic of apps is that it is designed for asynchronous parallelism so that there is no idle time the implementation of this asynchronous parallelism is based on mpi in c version the parameters are set as follows bounds tolerance is 10 8 step tolerance is 10 8 sufficient decrease factor is 0 and maximum evaluations is 2000 and we used 16 cores in our experiment in addition we couple the early truncation technique with appspack to introduce as a new strategy appspack aet and test its performance on the umatilla calibration problems for algorithm comparison 4 3 2 sce ua shuffled complex evolutionary algorithm sce ua duan et al 1993 1994 is a general purpose global optimization method although it has been used primarily for calibrating water resource models it also does not require any derivative information the basic concept of this algorithm is to systematically evolve complexes of points spanning in the decision domain in search of the global optimum during the evolution competitive selection and complex shuffling are used to ensure search direction for better objective value and the combination of deterministic and probabilistic ways of searching makes sce ua flexible and robust sce ua has been widely used in water resources research in this study we use a python version of sce ua with 16 cores using mpi parallelism in spotpy houska et al 2015 which is a statistical parameter optimization framework for python in sce ua the number of complexes is set to 16 same as number of cores the other parameters are set to their default values 5 results and discussion 5 1 truncation strategy analysis the truncation policy is implemented to reduce computation time in objective function evaluation the truncation pattern examined numerically varies for different trials and different truncation policies in order to understand the performance of the truncation setting we plot truncation patterns of three different rules of knowledge extraction so aet k table 1 and evaluation results for one trial in scenario 1 as an example of the impact of the truncation policy as in fig 6 in addition we compute the proportion of truncation points φ at the end of 10000 s as in eq 11 for different rules i e k 1 2 3 for two scenarios 11 φ i k number of truncated points from iteration 0 to i based on rule k number of all evaluated points from iteration 0 to i for a good truncation strategy on the one hand we need to truncate many points to allow certain variations in the computation time of simulations in order to benefit from the asynchronous paradigm this then creates a variability in the time required to compute each objective function what we can see from fig 6 results in the first trial in scenario 1 show that simulation time varies significantly and spans over the range of 300 s with the truncation strategy on the other hand we cannot truncate too many points because we need an adequate number of points to provide enough information to build accurate response surface which may leads the search a poor result would be that we can have few non truncated point in optimization because truncation level is too harsh as an example fig 6 shows that we do not have this type of problem with the truncation threshold we defined in eq 6 and eq 7 since there are many non truncated points table 2 describes the mean value i e μ and standard deviation i e σ for the best result obtained i e best eval among 30 trials and the proportion of truncation i e φ as described in eq 11 it shows that for both scenarios the best average and lowest variance are obtained by so aet 3 strategy the knowledge extraction strategy in so aet 3 uses the truncated points both in the distance calculation for candidate points selection algorithm 2 step 7 and selection evaluation points and the value of the objective function for early truncated point is used in building the surrogate s i 5 2 progress graph analysis to analyze the solution of synchronous and asynchronous paradigm we ran each setting so sp and so aet k for k 1 2 3 for 30 trials using 16 cores and measured their results against the wall clock time used all settings start with the same sets of 30 initial designs one initial design for each trial the progress graphs we use in this section describe the solutions of the averaged best results obtained over 30 trials against the wall clock spent fig 7 shows the progress graphs for two scenarios in subfigure a and b respectively in both figures we can see that so aet k with different rules of knowledge extraction show dominantly superior performance compared to so sp as their related curves all stay lower than their synchronous version at any time that means the averaged performance of so aet k for all k can obtain solutions with good objective values sooner than synchronous paradigm in comparison among different rules of knowledge extraction k results from scenario 1 show that so aet 2 obtains averaged results slightly worse than the other two rules at the end of 10000 s whereas in scenario 2 averaged curve of so aet 1 shows a slight worse performance than the other two rules overall so aet 3 performs the best in both scenarios this indicates that even though the values reserved from truncated points are estimations of full evaluation they can be valuable in building the response surface as shown in our test experiments however theoretically cases exist when early truncated points are falsely estimated as unpromising solution while in reality they can yield good outputs in this calibration problem the error sum along simulation transport time might not have such problems as the discrepancies in parameters should result in relatively similar amount of errors in observations at different time steps in addition we compare the performance of so sp and so aet k with two other algorithms i e sce ua appspack and with appspack et which is appspack plus our early truncation strategy and knowledge extraction rule k 3 the truncation threshold of appspack et is the same as the one we use for so aet k as described in section 3 3 to ensure a fair comparison we use the best point found in initial design of so sp and so aet k as the starting point in appspack and appspack et and also for each trial of sce ua in the progress graphs of averaged performance fig 7 a and b for 2 scenarios the averaged results of sce ua are largest comparing against all the other algorithms and followed by the results from appspack et and appspack the early truncation strategy does not improve the performance of appspack as averaged results from appspack et are worse than appspack along the optimization in both scenarios we can see the statistical comparison of appspack and appspack et in box plot in section 5 3 to quantify the efficiency of different algorithms we check the wall clock time required to obtain a level relatively hard to achieve same for all different algorithms in most cases after many iterations in optimization even a small amount of improvement in result would require a long time of optimization therefore if an algorithm variant can reach a better objective value much earlier than the other is of great benefit as a result we compared each of the asynchronous settings with the synchronous version by evaluating the wall clock time required for the so aet k to reach the final results obtained by so sp at the end of optimization the comparison is shown in the first row in table 3 for each scenario which represents the percentage of the wall clock time required by averaged results of so aet k denoted as t so aet k that reach the averaged best result achieved by the synchronous setting so sp at 10000 s out of the total 10000 s it is denoted as η so aet k i e η so aet k t so aet k 10000 the less percentage η so aet k is the more efficient asynchronous setting so aet k is the asynchronous version so aet k only requires in general around 50 wall clock time to achieve the same solutions obtained by synchronous version so sp at 10000 s for both scenarios table 3 as to the alternative algorithms so aet k needs around 30 of the time required by appspack 20 of the time required by appspack et and less than 20 of what sce ua takes to obtain their results at 10000 s and we can find that so aet 3 is the most time efficient setting compared among all rules of knowledge extraction for saving computation budget required by so sp as it obtains the smallest η so aet k among so aet k 5 3 stochastic performance analysis due to the fact that both so sp and so aet k are stochastic optimization algorithms various trials may have different performances therefore we need to analyze the consistency of the performance among different trials we conducted a statistical test of the results achieved by synchronous version so sp to variants of asynchronous version so aet k in this study mann whitney rank sum test is used as it is a non parameter test which does not have any pre assumption for the underlying distribution in the dataset we use the simulation results after 10000 s of 30 trials for each setting for both so sp and so aet k in table 4 results of so sp show that it has no significant differences compared to results from appspack and its variant because appspack has high variance fig 8 but it is significantly better than sce ua at 1 level as to so aet k their results are statistically better than so sp at 10000 s in both scenarios especially so aet 2 and so aet 3 for which they are both significantly better than all the algorithms compared at 1 level except for one case that so aet 2 is significantly better than appspack in scenario 2 at 5 level and all so aet k have a p value smaller than 10 compared against appspack appspack et and sce ua indicating that the asynchronous settings i e so aet k are significantly better than so sp appspack and especially sce ua at 90 confidence level we plot the box plots of the simulation results of 30 trials for each of the settings for both so sp so aet k sce ua appspack and appspack et after 10000 s in fig 8 a and b results show that all so aet k with different rules of knowledge extraction have smallest median as well as smallest range of values among all algorithms after 10000 s in both scenarios in scenario 1 fig 8 a shows that both so sp and so aet k have outliers and so aet 3 has the fewest outliers in so aet k and in scenario 2 8 b shows that so aet 3 has no outlier the algorithms which have the largest value range are appspack and its variant appspack et in both scenarios and appspack and appspack et have the similar value range compared against each other as appspack is a local optimizer it is reasonable to see this behavior since certain trials in appspack get trapped in some local optimum the range of the results can be large in addition as a synchronous parallel algorithm and an evolutionary algorithm which requires large number of simulations for an optimal solution sce ua has the highest median of results among all algorithms which shows that sce ua has trouble in finding the optimal solution with limited time allocated in both scenarios 6 conclusion in this study we introduce early truncation strategy along with different rules of knowledge extraction in asynchronous version of surrogate based optimization algorithm so aet k and find that it provides advantages in searching for the optimum the coupled asynchronous parallel and the early truncation strategy has great advantage for calibration problem with objective as cumulative sum of squared errors the empirical truncation strategy designed for this study introduces an unbalanced load which means that an asynchronous paradigm is preferred over its synchronous version as shown in this study the solutions obtained by so aet k are significantly better than so sp in statistical comparison and moreover computational time can be saved when using asynchronous paradigm compared to the synchronous setting for the problem in two scenarios results show that 40 70 of the computational time can be saved in umatilla calibration problems in order to obtain the same level of averaged results from the synchronous version we also found that incorporating all the information from truncated points so aet 3 is beneficial even though the truncation is based on estimation and their evaluated objective function values cannot be directly used when coupled with the algorithm however for problems with varying trends of function values or less certainty in forecasting the function evaluations discarding evaluation information from truncation points maybe a favorable option so aet 2 or we can also regard the truncated points as points we have never evaluated so aet 1 all in all so aet show a consistently better performance compared to synchronous version so sp as well as two other algorithms compared i e sce ua and appspack along with appspack et in terms of computation saving results quality and robustness in finding good optimal solution for calibration of groundwater flow and transport model with expensive computation budget significant optimization time can be saved beyond that this system is not only limited in implementation in the calibration process but also suitable for problems which have a somewhat predictable objective values such as monotonically increasing or decreasing objective functions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by national natural science foundation of china grant no 52000100 and grants to professor christine ann shoemaker nsf cise 1116298 grants with pi mahowald nsf 1049033 through cornell university and also professor christine ann shoemaker s start up grant at national university of singapore contributed to min pang s support and travel we would like to acknowledge high performance computing support from cheyenne doi 10 5065 d6rx99hx provided by ncar s computational and information systems laboratory sponsored by the national science foundation additionally the named authors have no conflict of interest financial or otherwise 
25700,automatic calibration is widely used to estimate parameters in hydrological models the main idea is to use optimization algorithms to minimize the discrepancy between field data and simulation prediction this process involves iterative exchanges of a parameter set chosen by the optimization and simulation however for computationally expensive models such as groundwater flow and transport models the calibration process may be extremely computationally demanding in this study we introduce and demonstrate a new asynchronous parallel surrogate assisted optimization algorithm with an early truncation feature and different knowledge extraction strategies so aet k results show that our asynchronous algorithm performs significantly better than the synchronous analogue so sp requiring only 40 70 of the computation time to achieve the same averaged results in addition various knowledge extraction strategies of the asynchronous so aet k are tested in comparisons with two other algorithms sce ua and appspack asynchronous so aet k shows significantly better performance in terms of both efficiency and robustness keywords asynchronous parallel knowledge based early truncation surrogate based optimization automatic calibration computationally expensive groundwater model software availability software name pysot year first official release 2015 hardware requirements pc system requirements windows linux mac program language python program size 600 kb availability https github com dme65 pysot documentation https pysot readthedocs io en latest note the default setting of knowledge extraction strategy in pysot is so aet 3 1 introduction optimization methods are widely used to automatically estimate model parameters for a variety of environmental models maier et al 2019 gong and duan 2017 such as large scale hydrological models koppa et al 2019 razavi and tolson 2013 hydrogeological models meyer et al 2018 zhao et al 2019 integrated surface subsurface models danapour et al 2019 the main idea to solve calibration problems is to apply optimization algorithms which find the parameter vector x to minimize a loss function f x that measures how well the model fits observed data many previous studies have been dedicated to automatic calibration much of this work involves derivative based approaches such as quasi newton cheng and yeh 1992 and levenberg marquardt methods dai 2004 and these methods have been integrated into software suites such as pest doherty and hunt 2010 but not all third party codes provide relevant derivatives and even when derivatives are available most derivative based methods are designed to converge to local optima various heuristic optimizers have also been adopted for calibration problems including genetic algorithms giacobbo et al 2002 solomatine et al 1999 adaptive cluster covering solomatine et al 1999 particle swarm optimization and pattern search haddad et al 2013 and differential evolutionary algorithms haddad et al 2013 chiu 2014 related approaches based on artificial neural networks karahan and ayvaz 2008 rather than posing the calibration problem directly as an optimization instead fit a meta model that maps observations to model parameters based on training examples however all these methods may require many expensive computational simulations to reach an acceptable solution for many realistic models the evaluations are expensive so only a few can be made the underlying simulations are black box codes so no analytical derivatives are available and the loss function is a complex non convex function of the parameters which may have many local minima especially expensive are simulations of models based on partial derivative equations pde for water flow and contaminant transport in surface or ground water for example surrogate based optimization methods jones et al 1998 gutmann 2001 björkman and holmström 2000 mugunthan et al 2005 regis and shoemaker 2013 are explicitly designed for complicated and computationally expensive simulation these algorithms approximate the true expensive function evaluations f x by a less computationally expensive approximation s x called a surrogate response surface or meta model and use this approximation to guide further sampling in high dimensional spaces radial basis function and gaussian processes are often used as surrogates and a variety of strategies have been proposed to balance exploitation of the surrogate to sample near predicted minima and exploration of parts of the space where there may be insufficient data for the surrogate to provide accurate predictions this surrogate approximation approach to optimization has been shown to reduce the number of costly objective function evaluations necessary to find a good answer regis and shoemaker 2005 2007 müller and shoemaker 2014 krityakierne et al 2016 timani and peralta 2017 wu et al 2020 many optimization algorithms have been parallelized so that they can make effective use of high performance computing systems in many groundwater related problems such as contaminant source identification water management pollution remediation and model parameterization problems parallel optimization has been proposed and shown to perform well ketabchi and ataie ashtiani 2015 mirghani et al 2009 sayeed and mahinthakumar 2005 garrett et al 1999 vrugt et al 2006 but the strategies currently favored in groundwater applications focus on synchronous parallelism i e methods in which several function evaluations are started simultaneously and all must finish before proceeding to the next step when simulations may have dramatically different run times depending on their inputs synchronous parallel methods can be very inefficient as many processors may stay idle waiting for one long running simulation to end in this situation asynchronous parallelism may make more efficient use of hpc systems several asynchronous parallel optimization algorithms have been studied such as asynchronous tabu search on location allocation problems crainic et al 1996 asynchronous evolutionary algorithms lücken et al 2004 in pump scheduling asynchronous particle swarm koh et al 2006 on biomechanical problem and the development of asynchronous parallel pattern search hough et al 2001 in machine learning asynchronous parallelism has been used for calibrating complex neural networks bottou et al 2018 many approaches are derivative based such as stochastic gradient descent niu et al 2011 feyzmahdavian et al 2016 but to our knowledge very few previous studies have been dedicated to parameter calibration problems for physical models in this paper we further reduce the time required for calibration of computationally expensive water resource models by selective early truncation of evaluations together with an asynchronous parallel strategy for example in calibrating a transient water resource model the loss function might be the sum of squared errors over n successive time steps suppose at a proposed parameter value x the sum of squared errors over the first k n time steps is much worse than the sum of squared errors over all time steps at the current best solution x then we need not compute the errors at x from time steps k 1 n and can save time by terminating the simulation early razavi et al 2010 has introduced a deterministic model preemption technique in parameter calibration for hydrological models showing that an early termination strategy can save large amount of the computational budget around 50 for optimization based automatic calibration problem however their approach does not provide set up for dynamical model termination threshold and no parallelism strategy especially asynchronous parallelism was yet used in their applications since we allow early truncation different function evaluations may take different amounts of time as a result bulk synchronous algorithms that perform batches of simulations together may leave processors idle in each step as the length of the step is determined by the slowest simulation in order to avoid this we employ the asynchronous parallel strategy that can start new simulations as soon as processors become available to make efficient use of the early truncation technique the algorithm we apply in this paper is an asynchronous version of the parallel stochastic radial basis function srbf method regis and shoemaker 2009 we use a surrogate optimization toolbox pysot eriksson et al 2015 which contains options for different surrogate types and parallelism paradigms we apply the method to a modified real world groundwater calibration model from the umatilla super fund site the contribution of this paper is that it is the first use of a surrogate based asynchronous parallel optimization algorithm with an early truncation strategy for calibration truncation threshold is defined dynamically in a way to combine both information we obtained in optimization phase and simulation phase and rules of knowledge extraction are defined to extract the information from the truncated solution and they are incorporated into the asynchronous optimization algorithm we illustrate the efficiency of our early truncation algorithm in an asynchronous parallel optimization framework through experiments using the pysot toolbox for surrogate optimization 1 1 https github com dme65 pysot our results suggest the computational benefits of the approach for other calibration problems with computationally expensive models 2 calibration problems of groundwater model calibration of parameters for a pde model of groundwater flow and transport can be done by combining optimization with relevant data which includes spatial distribution of hydraulic heads and concentration observations as well as input data such as pumping rates other studies have shown that coupled estimation of flow and transport parameters provides more reasonable and more certain parameter estimates than alternate procedures that use only subsets of observations e g only heads or only concentrations hill and tiedeman 2007 we work on calibration problems that are based on integrated groundwater flow and transport models the parameters to be estimated are the hydraulic conductivities distributed on the whole study area scenario 1 the easy site case is based on the true distribution of parameters in the model with nine hydraulic conductivity values each representing a zone with the same hydraulic conductivity scenario 2 the difficult site case has an additional cross shaped area representing two relatively low permeability zones each with an individual hydraulic conductivity value in both cases we want to see if we can obtain the actual distribution of hydraulic conductivities through efficient optimization 2 1 case study umatilla superfund site our case study is the calibration problem based on one u s environmental protection agency epa groundwater superfund site umatilla chemical depot ucd oregon which was contaminated with untreated wash water containing chemical components management of this pump and treatment system is one part of a demonstration project application of flow and transport optimization code to groundwater pump and treat systems funded by the u s department of defense dod and the environmental security technology certification program estcp the detailed dod estcp study report model and data can be found on the project website http www frtr gov estcp in our application a steady pumping strategy is assumed during a four year management period and the contaminant tnt is chosen as the only indication chemical to be removed on site which was a setting based on study from utah state university contained in the dod estcp report the simulation of the groundwater flow and transport is based on usgs modflow2005 harbaugh 2005 and mt3dms 5 3 zheng and wang 1999 model the model contains 132 columns 125 rows and 5 layers where layer 1 i e the top layer represents silt and weathered basalt in convertible state between confined and unconfined aquifer and the other layers are confined alluvial aquifers the hydraulic conductivity in the aquifer is highly heterogeneous especially in layer 1 which varies from 1 ft day to 5000 ft day zones of different hydraulic conductivity in the modflow model are shown in fig 1 there are ten different hydraulic conductivity zones in the current dod estcp model among them one zone has comparatively far lower conductivity than the others as it represents the boundary of an impermeable area based on the this configuration we choose the 9 other hydraulic conductivity zones which vary from 100 ft day to 5000 ft day as our parameters to be calibrated as in scenario 1 to add more complexity and calibration difficulty in the problem two less permeable zones are added in scenario 2 as shown in fig 1 b our goal is to use spatially distributed time series data on hydraulic head and contamination concentration to estimate spatially distributed hydraulic conductivities which are parameters in a transient groundwater flow and transport pde model 2 2 observation data the observation data is obtained from modflow and mt3d simulations using the true values of parameters the goal of the optimization is then to pick the values of the decision vector e g the parameter values that best fit the synthetic observation data for scenarios 1 and 2 the observation data is obtained from the values of configurations given in fig 1 a and b respectively fig 2 illustrates the distribution of all observation wells as well as the hydraulic conductivity zones these wells are not actual in situ wells we set up the locations of heads monitoring wells randomly in the whole region while contamination monitoring wells are based on prior knowledge of the migration of the plume that is to say wells for contaminations are only designed in the region through which the plume travels in order to better capture the value change there are in total 107 observation wells for hydraulic heads and 27 wells for contaminant concentrations the observation data for the calibration problem contains one set of hydraulic heads in all 107 wells at the end of the four year management period since pumping rates and heads are constant in time there is a weekly contamination concentration monitoring data in 27 observation wells so overall there are 107 observations of heads and 52 4 27 observations of contamination concentrations in the four year time period 2 3 flow and transport model the modflow flow models for the two scenarios are assumed to be in steady state with constant hydraulic heads provided for the four year management period as in the study by utah state university in the dod estcp report in contrast the mt3d contamination transport model involves an implicit time stepper and the nonlinear solve required at each transport step may vary in cost depending on the model inputs thus the simulation time required to obtain contaminant concentrations at a given intermediate time point may vary depending on the model parameters though the variation is not great 3 model formulation 3 1 objective function the goal of calibration is to minimize the deviation between the observation and the simulation in our problem the error consists of differences of values in n h 107 head monitoring wells at one time since pumping rates are time invariant and in n c 29 contamination concentration wells on a weekly basis as contamination is changing in space and time the decision variables are hydraulic conductivity values for different spatial zones which are represented in the decision vector x r d where d 9 for scenario 1 and d 11 for scenario 2 the optimization problem is 1 min x r d f x f x g x t where t is the total number of transport steps a constant and 2 g x τ w 1 h x 1 w 1 c x τ for 1 τ t where 3 h x n 1 n h h n s i m x h n o b s 2 4 c x τ m 1 n c t 1 τ c m t s i m x c m t o b s 2 for 1 τ t subject to the constraints 5 x i m i n x i x i m a x in our model x i min 100 ft day and x i max 5000 ft day respectively h n s i m and h n o b s are the simulated and observed hydraulic head at head observation well n c m t s i m and c m t o b s are simulated and observed contamination concentration at concentration observation well m in week t index n in eq 3 stands for hydraulic head wells t stands for time and m in eq 4 is for contamination well t is the total number of transport steps one each week for four years i e 208 the weight w 1 tn c n h tn c compensates for the difference in the number of terms in the sums in eq 3 and eq 4 g x τ increases monotonically with τ because each term in the sum is non negative we consider early truncation of the simulation at a time τ t to obtain a lower bound g x τ f x where τ is chosen by a truncation strategy described in the next section 3 2 early truncation strategy in conventional optimization based calibration we would evaluate the objective function g x t in eq 2 for all time steps so τ t in eq 2 hence the objective function is fully evaluated which can be very expensive we can potentially reduce overall computation by prematurely terminating computation of g x τ for a specific x and τ t if the value of g x τ in eq 2 is poor e g large compared to value of g x t we have for other x this is because the full value g x t g x τ as explained in section 3 1 and once an unacceptably large sum of errors has been seen in the middle of the simulation when τ t there is little benefit to completing the simulation what we develop is a method to determine under which condition to stop the calculation of eq 4 for τ t thus while we always let the modflow flow model run to completion we develop a method to terminate the more time consuming transport simulation with mt3d if its error exceeds some threshold we will call this early truncation to clarify how it works we use fig 3 to demonstrate a possible time pattern of function evaluations we can obtain in the asynchronous parallelism with early truncation strategy assuming we have 12 simulations that are conducted in three cores simultaneously the total wall clock time required for this task is t 3 which ends at 11th started simulation as shown in fig 3 the numbers on the box of simulation represent the ith started simulation two simulations that are truncated early are shown in red boxes the problem lies in how to determine the threshold for which early truncation makes sense 3 3 early truncation threshold early truncation involves a trade off between the cost of computation and the benefit of having a computed value of f x that can be incorporated into the surrogate if the computation is terminated after only a few steps we save computational effort but learn little about the function choosing to terminate later has less benefit but may be worthwhile if the cost function is much larger than the cost of other points that the algorithm has already explored at the ith function evaluation we balance the time savings of early termination of the ith evaluation against the benefits of completing the evaluation through the criterion 6 truncate if g x τ θ τ l s τ l i p and τ l τ τ u where l i p represents the pth percentile of previously computed values or partial evaluations of the objective function l i p is given to the simulation before it starts so there is no communication burden within the simulation as shownshow in fig 3 the 9th started simulation is truncated and the component of its truncation threshold is l 6 p as we have 6 simulations finished before the 9th simulation starts for the 12th simulation whether to use l 9 p as part of truncation threshold depends on which rule of knowledge extraction we use as described in table 1 7 l s τ 1 τ τ u τ u γ l s increases monotonically for τ τ l τ u truncation does not go into effect until τ l steps have been taken nor after τ u steps have been taken in our study we set τ l 1 and τ u 0 9t the functions l i p and l s τ are used to adapt to the stage of the optimization algorithm and the work spent on each function evaluation respectively the value of l i p decreases as the optimization proceeds and more values are computed so the algorithm can be more aggressive about early truncation later in the optimization process it is sensible that the algorithm would become more aggressive as it obtains more information to facilitate searching the optimum we choose p 75 in this study the idea with l s τ is that since g x τ is a sum of positive terms we should terminate earlier for smaller values of τ in our study we choose a cubic polynomial γ 3 in eq 7 for the reason that 1 τ τ u τ u 3 has a large gradient when τ is small so we can allow more trials to be fully executed at the beginning of the simulation and as τ grows the gradient of 1 τ τ u τ u 3 goes smaller therefore the criteria to allow full execution becomes tougher and tougher for large τ various linear or nonlinear equations for l s may also be used depending on prior knowledge of the optimization problems fig 4 illustrates the role of early truncation in the figure the top curve θ 1000 shows the shape of threshold function θ τ at iteration 1000 if we assume a simulation has objective function g x τ based on the vector of parameters as x and value of g x τ is given by the green curve in fig 4 then the evaluation of g x τ would stop at black cross with τ at which g x τ exceeds the threshold θ τ similarly at iteration 2000 the threshold function θ 2000 would be used resulting in truncation at τ 4 optimization algorithm 4 1 so sp regis and shoemaker 2007 describe a strategy for finding the global optimum of a computationally expensive function using a metric response surface mrs the idea is that in each iteration of the algorithm an approximation of the objective function f x also called a response surface or surrogate is constructed based on all the values x f x computed in previous iterations in our study we use an mrs strategy with synchronous parallelism called so sp surrogate optimization with synchronous parallelism so sp combines two algorithms parallel lmsrbf regis and shoemaker 2007 and dycors regis and shoemaker 2013 and it is implemented in our open source software pysot https github com dme65 pysot the detailed steps in so sp are described in algorithm 1 in steps 1 to 3 of algorithm 1 we use an experimental design i e a latin hyper cube to create the initial surrogate approximation using f x in the algorithm iterations rather than fitting the objective function f x we fit the capped function i e f capped x which replaces objective function values from simulation by the median of all solutions obtained so far 8 f c a p p e d x min f x median n a i f x n where a i is the set of indices of points evaluated at round i here capping helps bound the value range so the surrogate function can focus its approximation on the more valuable area for optimal solution therefore we seek to avoid oscillations in the surrogate associated with very large function values and gradients in regions far from the minimum as the iteration goes on the search is either focused on the part with small values or exploring new area so capping is applied during the whole optimization process to help better approximate the areas with small values we then update parameters and check whether we need to restart the whole algorithm or not by using function restart criterion check in restart criterion check we keep track of the number of consecutive successes i e c s u c c e s s and number of consecutive failures i e c f a i l in order to determine the search radius σ i at iteration i the criterion of success iteration is that the best point we found in iteration i i e x i is better compared to all the points we evaluated so far i e f x i f b e s t otherwise this iteration is a failure thresholds t s u c c e s s and t f a i l are pre defined values and once the number of reduction of search radius reaches to the pre defined r max we will restart the algorithm from scratch the restart technique helps the search to escape from a local optimum xx image 1 from step 6 we start to determine the points for function evaluation by first generating candidate points set ω i which is generated from using a normally distributed perturbation amplitude around best points evaluated so far i e x best with a declining probability of perturbation occurring for each iteration as described in eq 9 9 p i p 0 1 log i i i n i t 1 log i m a x i i n i t where i is the current number of iterations i init is the number of iterations for initial design i max is the total number of iterations and p 0 is the initial probability therefore p i is a decreasing function of i the procedure for candidate points was suggested in regis and shoemaker 2013 and eq 9 was suggested by tolson and shoemaker 2007 for the dynamic dimensional search dds algorithm selection evaluation points function described in the function box is used to select points to be included in set of evaluation points i e d i there are two metrics considered in selecting the point for d i 1 the estimated values evaluated on the current surrogate surface 2 the distance information from previously evaluated points to the new one the basic concept is to choose the point which has a good i e low estimated value and also has large distance to the closest previously evaluated points in order to explore more in the area that has not been searched that is to say as we need to generate p number of evaluation points i e s i z e d i p selection evaluation points function need to be processed p times one for each point generation in step 7 so sp starts all cores i e p cores at the same time to evaluate p points in d i for expensive functions results following a synchronous parallelism paradigm after all computational resources i e p cores finish evaluations we can then update both the set of evaluated points a i as well as the response surface based on set a i this iteration loop forms the basic structure of the so sp algorithm image 2 algorithm 1 so sp synchronous parallel image 3 we implement so sp by using surrogate optimization toolbox pysot in https github com dme65 pysot the realization of the parallel paradigm in pysot is based on poap which is an event driven framework for building and combining optimization strategies there are two parts in poap controller and strategy the controller communicates between worker cores and master core in order to distribute evaluation jobs the master core which runs the strategy is responsible for building the surrogate surface and selecting candidate points for evaluation in this study we use the mpi message passing interface version of poap for the parallel communication in high performance computing system and as to strategy we use rbf surrogate surface and candidatedycors method for generating the evaluation points as discussed in eq 9 4 2 so aet so aet is the new algorithm we introduce which combines features from so sp and asynchronous parallelism and it incorporates a new early truncation function as discussed in section 3 2 and table 1 the diagram of this so aet can be found in fig 5 algorithm 2 describes the major steps in so aet as a start we evaluate points in the initial experimental design by objective function f x and initialize set of distance relevant points as a 0 e i and set of surface relevant points a 0 s i in step 2 definitions of both distance relevant and surface relevant are described in table 1 distant relevant points are used in the distance metric in function selection evaluation points to select evaluation points surface relevant points are used in construction of an update surrogate approximation s i x in each iteration then initial points are dispatched to workers and evaluated asynchronously without any truncation and surrogate surface s 0 x is built once all the initial evaluations have been completed or pending using objective function f x as in step 8 once a function evaluation is done we use restart criterion check to determine whether to restart or not similar to so sp if no restart is needed we check the point x i evaluated by simulation model with the early truncation strategy described in eq 6 as in asynchronous paradigm different cores can carry out at different time each time a core finishes the evaluation of x i there are two possible outcomes due to eq 6 1 x i is an early truncated point 2 x i is a fully evaluated point for a fully evaluated point we should follow the same updating technique as in so sp that is to say x i is categorized as both a distance relevant point i e x i e and surface relevant point i e x i s in step 5 in algorithm 2 if x i is a truncated point we record the value of x i and predict its full objective value l p as in eq 10 10 g x w 1 h x 1 w 1 c x if not truncated l p if truncated and then we can determine the category of x i i e distance relevant point or surface relevant point or both based on three different rules of knowledge extraction so aet k for k 1 2 3 which are proposed to help us determine how to incorporate the information of x i the definitions of the three different rules of knowledge extraction are also shown in table 1 if no other worker is updating the surrogate rbf the idle worker will update the rbf surface the value of eq 10 is used to build the response surface for the points either truncated or not based on different so aet k i e if they are considered surface relevant truncated points are assigned the p th percentile value of existing points in step 9 and 10 we generate candidate points ω i by perturbing selected decision variables around the best solution found so far x best the evaluation point x i is selected based on metrics function select evaluation point among candidate points one at a time and then it is added into evaluation queue and popped out to dispatch to a worker for evaluating expensive simulation with possible early truncation the evaluation queue q helps to ensure that after all the points in the initial design are dispatched to worker we can start building the response surface it is dynamically changing as we pop out x i to be evaluated and also add new x i 1 in algorithm 2 so aet asynchronous parallel with early truncation image 4 the explanations of the difference in synchronous and asynchronous parallelism are as follows in synchronous parallel version if any core finishes its job early it needs to wait for all the other cores to finish in order to proceed in each iteration with p cores used we update the rbf surface based on results of p evaluations in each iteration then the set of informative evaluated points and the internal parameters in so sp are updated by contrast for so aet with asynchronous parallelism each core can start its evaluations once it finishes its job without considering the working status of other cores consequently surrogate surface informative evaluated points as well as internal parameters in the algorithm are updated soon after each function evaluation instead of waiting for other cores to finish therefore asynchronous parallelism prevents cores from being idle in the optimization processes if there is no variability in objective function computation time the synchronous parallel algorithm so sp is possibly better as with synchronous optimization each new response surface is updated based on all p evaluations of the objective function with p cores therefore the next evaluation point will be found based on a more informative response surface compared against the case in the asynchronous version for computationally expensive functions that take various times given different inputs the overall computational budget for finding the optimum inputs can be reduced with asynchronous parallelism we expect the advantages of asynchronous to increase as the variability of the objective function evaluation time increases 4 3 other algorithms we will compare so aet and so sp to other algorithms the comparison are one asynchronous local optimizer and one synchronous global optimizer 4 3 1 appspack asynchronous parallel pattern search pack appspack toolbox solves nonlinear optimization problems it is a local optimizer but does not require derivative information the toolbox is based on the asynchronous parallel pattern search apps algorithm hough et al 2001 in apps trial points are iteratively generated according to the parameters defined as search direction and step size based on whether the best evaluation of trial points is found in search or not the search step is identified as successful or unsuccessful step and in consequence the direction and step size change in different fashion successful step changes direction of the search whereas unsuccessful step narrows the selection of trials points around the current best solution as iteration goes on the search domain would be narrowed to the optimal point but since it is a local optimization method the search may be trapped in a local optimum another characteristic of apps is that it is designed for asynchronous parallelism so that there is no idle time the implementation of this asynchronous parallelism is based on mpi in c version the parameters are set as follows bounds tolerance is 10 8 step tolerance is 10 8 sufficient decrease factor is 0 and maximum evaluations is 2000 and we used 16 cores in our experiment in addition we couple the early truncation technique with appspack to introduce as a new strategy appspack aet and test its performance on the umatilla calibration problems for algorithm comparison 4 3 2 sce ua shuffled complex evolutionary algorithm sce ua duan et al 1993 1994 is a general purpose global optimization method although it has been used primarily for calibrating water resource models it also does not require any derivative information the basic concept of this algorithm is to systematically evolve complexes of points spanning in the decision domain in search of the global optimum during the evolution competitive selection and complex shuffling are used to ensure search direction for better objective value and the combination of deterministic and probabilistic ways of searching makes sce ua flexible and robust sce ua has been widely used in water resources research in this study we use a python version of sce ua with 16 cores using mpi parallelism in spotpy houska et al 2015 which is a statistical parameter optimization framework for python in sce ua the number of complexes is set to 16 same as number of cores the other parameters are set to their default values 5 results and discussion 5 1 truncation strategy analysis the truncation policy is implemented to reduce computation time in objective function evaluation the truncation pattern examined numerically varies for different trials and different truncation policies in order to understand the performance of the truncation setting we plot truncation patterns of three different rules of knowledge extraction so aet k table 1 and evaluation results for one trial in scenario 1 as an example of the impact of the truncation policy as in fig 6 in addition we compute the proportion of truncation points φ at the end of 10000 s as in eq 11 for different rules i e k 1 2 3 for two scenarios 11 φ i k number of truncated points from iteration 0 to i based on rule k number of all evaluated points from iteration 0 to i for a good truncation strategy on the one hand we need to truncate many points to allow certain variations in the computation time of simulations in order to benefit from the asynchronous paradigm this then creates a variability in the time required to compute each objective function what we can see from fig 6 results in the first trial in scenario 1 show that simulation time varies significantly and spans over the range of 300 s with the truncation strategy on the other hand we cannot truncate too many points because we need an adequate number of points to provide enough information to build accurate response surface which may leads the search a poor result would be that we can have few non truncated point in optimization because truncation level is too harsh as an example fig 6 shows that we do not have this type of problem with the truncation threshold we defined in eq 6 and eq 7 since there are many non truncated points table 2 describes the mean value i e μ and standard deviation i e σ for the best result obtained i e best eval among 30 trials and the proportion of truncation i e φ as described in eq 11 it shows that for both scenarios the best average and lowest variance are obtained by so aet 3 strategy the knowledge extraction strategy in so aet 3 uses the truncated points both in the distance calculation for candidate points selection algorithm 2 step 7 and selection evaluation points and the value of the objective function for early truncated point is used in building the surrogate s i 5 2 progress graph analysis to analyze the solution of synchronous and asynchronous paradigm we ran each setting so sp and so aet k for k 1 2 3 for 30 trials using 16 cores and measured their results against the wall clock time used all settings start with the same sets of 30 initial designs one initial design for each trial the progress graphs we use in this section describe the solutions of the averaged best results obtained over 30 trials against the wall clock spent fig 7 shows the progress graphs for two scenarios in subfigure a and b respectively in both figures we can see that so aet k with different rules of knowledge extraction show dominantly superior performance compared to so sp as their related curves all stay lower than their synchronous version at any time that means the averaged performance of so aet k for all k can obtain solutions with good objective values sooner than synchronous paradigm in comparison among different rules of knowledge extraction k results from scenario 1 show that so aet 2 obtains averaged results slightly worse than the other two rules at the end of 10000 s whereas in scenario 2 averaged curve of so aet 1 shows a slight worse performance than the other two rules overall so aet 3 performs the best in both scenarios this indicates that even though the values reserved from truncated points are estimations of full evaluation they can be valuable in building the response surface as shown in our test experiments however theoretically cases exist when early truncated points are falsely estimated as unpromising solution while in reality they can yield good outputs in this calibration problem the error sum along simulation transport time might not have such problems as the discrepancies in parameters should result in relatively similar amount of errors in observations at different time steps in addition we compare the performance of so sp and so aet k with two other algorithms i e sce ua appspack and with appspack et which is appspack plus our early truncation strategy and knowledge extraction rule k 3 the truncation threshold of appspack et is the same as the one we use for so aet k as described in section 3 3 to ensure a fair comparison we use the best point found in initial design of so sp and so aet k as the starting point in appspack and appspack et and also for each trial of sce ua in the progress graphs of averaged performance fig 7 a and b for 2 scenarios the averaged results of sce ua are largest comparing against all the other algorithms and followed by the results from appspack et and appspack the early truncation strategy does not improve the performance of appspack as averaged results from appspack et are worse than appspack along the optimization in both scenarios we can see the statistical comparison of appspack and appspack et in box plot in section 5 3 to quantify the efficiency of different algorithms we check the wall clock time required to obtain a level relatively hard to achieve same for all different algorithms in most cases after many iterations in optimization even a small amount of improvement in result would require a long time of optimization therefore if an algorithm variant can reach a better objective value much earlier than the other is of great benefit as a result we compared each of the asynchronous settings with the synchronous version by evaluating the wall clock time required for the so aet k to reach the final results obtained by so sp at the end of optimization the comparison is shown in the first row in table 3 for each scenario which represents the percentage of the wall clock time required by averaged results of so aet k denoted as t so aet k that reach the averaged best result achieved by the synchronous setting so sp at 10000 s out of the total 10000 s it is denoted as η so aet k i e η so aet k t so aet k 10000 the less percentage η so aet k is the more efficient asynchronous setting so aet k is the asynchronous version so aet k only requires in general around 50 wall clock time to achieve the same solutions obtained by synchronous version so sp at 10000 s for both scenarios table 3 as to the alternative algorithms so aet k needs around 30 of the time required by appspack 20 of the time required by appspack et and less than 20 of what sce ua takes to obtain their results at 10000 s and we can find that so aet 3 is the most time efficient setting compared among all rules of knowledge extraction for saving computation budget required by so sp as it obtains the smallest η so aet k among so aet k 5 3 stochastic performance analysis due to the fact that both so sp and so aet k are stochastic optimization algorithms various trials may have different performances therefore we need to analyze the consistency of the performance among different trials we conducted a statistical test of the results achieved by synchronous version so sp to variants of asynchronous version so aet k in this study mann whitney rank sum test is used as it is a non parameter test which does not have any pre assumption for the underlying distribution in the dataset we use the simulation results after 10000 s of 30 trials for each setting for both so sp and so aet k in table 4 results of so sp show that it has no significant differences compared to results from appspack and its variant because appspack has high variance fig 8 but it is significantly better than sce ua at 1 level as to so aet k their results are statistically better than so sp at 10000 s in both scenarios especially so aet 2 and so aet 3 for which they are both significantly better than all the algorithms compared at 1 level except for one case that so aet 2 is significantly better than appspack in scenario 2 at 5 level and all so aet k have a p value smaller than 10 compared against appspack appspack et and sce ua indicating that the asynchronous settings i e so aet k are significantly better than so sp appspack and especially sce ua at 90 confidence level we plot the box plots of the simulation results of 30 trials for each of the settings for both so sp so aet k sce ua appspack and appspack et after 10000 s in fig 8 a and b results show that all so aet k with different rules of knowledge extraction have smallest median as well as smallest range of values among all algorithms after 10000 s in both scenarios in scenario 1 fig 8 a shows that both so sp and so aet k have outliers and so aet 3 has the fewest outliers in so aet k and in scenario 2 8 b shows that so aet 3 has no outlier the algorithms which have the largest value range are appspack and its variant appspack et in both scenarios and appspack and appspack et have the similar value range compared against each other as appspack is a local optimizer it is reasonable to see this behavior since certain trials in appspack get trapped in some local optimum the range of the results can be large in addition as a synchronous parallel algorithm and an evolutionary algorithm which requires large number of simulations for an optimal solution sce ua has the highest median of results among all algorithms which shows that sce ua has trouble in finding the optimal solution with limited time allocated in both scenarios 6 conclusion in this study we introduce early truncation strategy along with different rules of knowledge extraction in asynchronous version of surrogate based optimization algorithm so aet k and find that it provides advantages in searching for the optimum the coupled asynchronous parallel and the early truncation strategy has great advantage for calibration problem with objective as cumulative sum of squared errors the empirical truncation strategy designed for this study introduces an unbalanced load which means that an asynchronous paradigm is preferred over its synchronous version as shown in this study the solutions obtained by so aet k are significantly better than so sp in statistical comparison and moreover computational time can be saved when using asynchronous paradigm compared to the synchronous setting for the problem in two scenarios results show that 40 70 of the computational time can be saved in umatilla calibration problems in order to obtain the same level of averaged results from the synchronous version we also found that incorporating all the information from truncated points so aet 3 is beneficial even though the truncation is based on estimation and their evaluated objective function values cannot be directly used when coupled with the algorithm however for problems with varying trends of function values or less certainty in forecasting the function evaluations discarding evaluation information from truncation points maybe a favorable option so aet 2 or we can also regard the truncated points as points we have never evaluated so aet 1 all in all so aet show a consistently better performance compared to synchronous version so sp as well as two other algorithms compared i e sce ua and appspack along with appspack et in terms of computation saving results quality and robustness in finding good optimal solution for calibration of groundwater flow and transport model with expensive computation budget significant optimization time can be saved beyond that this system is not only limited in implementation in the calibration process but also suitable for problems which have a somewhat predictable objective values such as monotonically increasing or decreasing objective functions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by national natural science foundation of china grant no 52000100 and grants to professor christine ann shoemaker nsf cise 1116298 grants with pi mahowald nsf 1049033 through cornell university and also professor christine ann shoemaker s start up grant at national university of singapore contributed to min pang s support and travel we would like to acknowledge high performance computing support from cheyenne doi 10 5065 d6rx99hx provided by ncar s computational and information systems laboratory sponsored by the national science foundation additionally the named authors have no conflict of interest financial or otherwise 
25701,by mapping land use under projections of socio economic change ecological changes can be predicted to inform conservation decision making we present a land use model that enables the fine scale mapping of land use change under future scenarios its predictions can be used as input to virtually all existing spatially explicit ecological models our model maps the fractional cover of land use within each grid cell providing higher information content than discrete classes at the same spatial resolution the method accurately reproduced land use patterns observed in the amazon both in terms of the allocated fractional amounts and also the direction of predicted land use changes a small case study showcases the application of our model to reproduce patterns of agricultural expansion and natural habitat declines the model source code is provided as an open source r package making this new method available to bridge the gap between socio economic land use and biodiversity modelling keywords land use forecasting fractional land cover continuous fields agricultural expansion socio economic change biodiversity conservation 1 introduction 1 1 accounting for land use change in biodiversity assessments land use change is a key driver of global environmental change causing global declines in biodiversity species extinctions and resulting in the deterioration of ecosystem services foley 2005 e resolved h ipbes et al 2019 there is mounting evidence of adverse impacts of land use change on biodiversity the need for global assessments of future biodiversity change in response to land use change has been increasingly acknowledged urban et al 2016 kim et al 2018 powers and jetz 2019 however work concerned with understanding future biodiversity change tends to focus on climate change titeux et al 2016 struebig et al 2015 urban et al 2016 or other aggregated effects of socio economic change such as forest loss pérez vega 2012 margono et al 2014 and urban expansion seto et al 2012 this is despite the fact that land use change is highly driven by dynamic bio physical and socio economic processes lambin et al 2011 climate change will likely result in global shifts and declines of land suitable for agricultural production with projected depletion of land reserves in the first half of the 21st century lambin et al 2011 food production and international trade of goods will significantly increase o neill et al 2014 2017 and even under lowest impact scenarios crop and livestock production are still likely to be higher and occupy a larger land area than they do today o neill et al 2014 consequently future predictions of biodiversity change will benefit from explicit accounting of the drivers and effects of land use change at the level of individual types of use detailed large scale maps of future land use under competing future scenarios provide useful insights for researchers and policy makers particularly in terms of informing conservation planning and preventing future biodiversity loss 1 2 overview of land use modelling approaches different modelling approaches have been developed to determine the overall amount of future land use and allocate changes across the landscape artificial neural networks and markov chain models learn and infer total amounts and spatial patterns of land use change from historic time series tayyebi and pijanowski 2014 pijanowski et al 2002 markov chain models have been frequently combined with cellular automata ca markov models see hyandye and martz 2017 aburas et al 2017 van schrojenstein lantman et al 2011 in cellular automata the transition probability of a cell to another land use depends on its current state and the state of neighbouring cells both of which are the result of historic changes van schrojenstein lantman et al 2011 cellular automata have been used successfully to simulate strongly auto correlated changes such urban sprawl verburg et al 2004b fang et al 2005 shafizadeh moghadam and helbich 2013 sun et al 2007 many modelling approaches apply regression analysis and other techniques to identify associations between various environmental conditions and observed land use patterns van schrojenstein lantman et al 2011 lambin et al 2000 verburg et al 2004b available models applying this approach include sleuth slope land use exclusion urban transportation hillshade dietzel and clarke 2007 dinamica ego environment for geoprocessing objects soares filho et al 2009 lcm in terrset land change modeler eastman and toledano 2018 and perhaps most prominently the clue model series conversion of land use and its effects verburg and overmars 2009 table 1 clue models have found application in the prediction of spatially explicit patterns of land use at national and continental scales veldkamp and fresco 1996 verburg and overmars 2009 verburg et al 1999 2002 kapitza et al 2021 exogenously determined future changes in area demands for different land uses often predicted by an economic model aguiar et al 2016 may be downscaled by establishing statistical relationships between observed land use and a set of socio economic and bio physical drivers of land use and land use change predicted land use suitability surfaces inform local competition for different land uses verburg et al 2002 meiyappan et al 2014 models can be further parametrized by including transition rules at local cell and landscape levels and constraints on overall turn over through time more simplistic models based on statistical analysis use an ordered allocation algorithm in which competition between land uses is handled by ordering allocations in terms of perceived socio economic value fuchs et al 2013 land use change allocation algorithms are agnostic to the type of statistical analysis conducted to estimate land use suitability surfaces nevertheless most models apply binary logistic regression to model the cell wise probabilities of occurrence for each land use category independent of the probabilities of other land uses the resulting probability of land use occurrence at a site produced by separate models is an incomplete representation of the underlying structure of land use probability because it omits that occurrence probabilities are dependent between land use types and that the probabilities of all discrete classes must sum to one for example when a site has very high probability for urban land use this implies relatively low probabilities for primary natural habitat which separate independent logistic regressions do not fully capture one step toward explicitly modelling competition between land uses is to apply multinomial regression thus allowing for the prediction of conditional binary probabilities of multiple classes noszczyk 2019 1 3 continuous land use fractions categorical land use data sets are increasingly available at spatial resolutions of finer than 1 km three prominent examples include the corine coordination of information on the environment land cover inventory bossard et al 2000 which contains several time steps between 1990 and 2018 at 100 m resolution for the european continent global land cover maps produced for the year 2010 through copernicus land monitoring service european union 2019 at the same resolution as well as global maps of land cover in annual time steps between 1992 and 2018 produced under the european space agency s esa climate change initiative land cover cci lc project european space agency 2019 available at 300 m resolution however the spatial variables that represent drivers of land use and biodiversity change are often not available over large spatial extents at fine resolutions better than 1 km dendoncker et al 2006 therefore it is necessary to resample finely resolved land use data to match the coarser resolution of driver covariates lowering the resolution has the additional advantage of improving computational efficiency due to the smaller number of pixels in the coarser map resampling fine resolution maps by assigning a single category of land use on each coarser pixel effectively eliminates sub pixel information on land use seo et al 2016 so this approach is not desirable fig 1 in order to maximise the retained information contained in the coarser map it is preferable to calculate the fractions of land use covering each new pixel producing continuous fields of information and keeping information at sub pixel level seo et al 2016 fig 1 the higher information content retained in fractional land use representations has high utility in ecological modelling for example many species may be able to persist in an area if only a small proportion of the area is made up of a suitable land class such as remnant vegetation wintle et al 2019 many wide ranging species may persist in landscapes if a certain proportion of the landscape is comprised of old forest it has been shown that continuous fields of land use allow better estimation of biomass and biomass change xian et al 2015 and are better able to explain variation in home range sizes bevanda et al 2014 than categorical land use data continental scale biodiversity assessments have shown that patterns are associated with high spatial resolution fractional land use measures such as the regional aggregation of land use types land cover diversity and land use covariates including land use intensity mouchet et al 2015 and actual evapo transpiration mouchet et al 2015 whittaker et al 2006 creating maps of some of these covariates requires fine scale maps of fractional land use as principal input plutzar et al 2016 the intensification of agriculture and forest harvesting are crucial factors shaping biodiversity levers et al 2014 2016 that require inputs of crop type and vegetation composition within each spatial unit these ecological considerations of the utility of fractional land cover and land use representations are underpinned by recent advancements in algorithms to produce high resolution maps of fractional land cover from satellite data allred et al 2021 hill and guerschman 2020 however only few land use modelling approaches are capable of predicting continuous fractions of land use directly see hasegawa et al 2017 meiyappan et al 2014 by providing fine resolution categorical representations that can be resampled to coarser resolution continuous representations see future land use simulation model flus liu et al 2017 or as part of integrated assessment frameworks see climsave integrated assessment platform climsave ia harrison et al 2013 however some of these documented approaches are not available in a useable package suited to regional continental scale hasegawa et al 2017 meiyappan et al 2014 or provide user interfaces that do not allow seamless reproducible integration into programmatic workflows harrison et al 2013 liu et al 2017 table 1 1 4 objectives of this paper our new land use model flutes fractional land use transitions in ecological systems provides a readily available means to incorporate fractional land use change into ecological modelling an advantage of flutes compared to existing fractional land use modelling approaches is its implementation in r r development core team 2008 making use of a development environment for which high expertise already exists among ecological modellers flutes can be fitted at different scales with minimal parametrization requirements and performs efficiently at various resolutions and extents the source code for our method is freely available as a small open source r package hosted on github kapitzas flutes as such our approach contributes a new open method toward bridging the gap between socio economic land use and biodiversity modelling we provide a mathematical description of the developed fractional land use model and evaluate flutes according to its ability to correctly estimate the direction and intensity of observed land use changes using a case study in the brazilian amazon 2 materials and methods 2 1 model description the model consists of two main components fig 2 first statistical analysis is used to determine how the suitability of the landscape for different land uses relates to a set of environmental drivers of land use change producing a suitability surface for each land use class fig 2a see fig 3 for an example of a suitability surface second fractional changes in additional land use demands are allocated iteratively in the landscape scaling with the land use suitability surfaces fig 2b see fig 3 for an example of a final allocated map of fractional land use we utilize a cellular automaton to introduce cell level allocation decisions that constrain the location and direction of land use changes according to three rules first future land use supply must meet additional demand projections of land use demands may be provided through external models such as computational general equilibrium cge models i e gtap aguiar et al 2016 or through the analysis and extrapolation of historic patterns moulds et al 2015 the model allocates additional demand by adding cell level supply of that time step d i k t 1 in cell i land use k and time step t 1 to the fractions of the current time step q i k t fig 2b the first model objective can be formulated i 1 n q i k t 1 i 1 n q i k t d i k t 1 i 1 n d i k t 1 d k t 1 d k t 1 is the additional landscape wide supply and is at equilibrium with additional demand after the algorithm converges second supply of all land use types in a cell d i k t 1 is allocated across cells so it adds up to one k 1 k q i k t 1 1 fig 2b third cell level supply d i k t 1 has to be distributed in such a way that the allocated amounts in each cell scale with a predicted probability surface s by modelling q i k t 0 s i k f k x i where x i is a set of demographic and bio physical drivers related to land use f k is a multinomial multi response model fig 2a the parameter estimation of this model is based on the first time step and predicted to the conditions of subsequent time steps accordingly while the model assumes stationarity of the modelled statistical relationships it implements temporal dynamics based on changing demand and changing environmental conditions changing environmental conditions are represented as changes to independent model variables the land use status in a cell s neighbourhood has been shown to play an important role in determining a cell s land use dendoncker et al 2007 mustafa et al 2018 van vliet et al 2013 verburg et al 2004a our suitability model applies neighbourhood interactions by calculating autocovariates verburg et al 2004a and including these in the multinomial regression of the land use suitability model following verburg et al 2004a our autocovariates measure the amount of clustering of land uses in a user defined cell neighbourhood when compared to the entire landscape we calculate autocovariates as enrichment factors f d i k t i d q i k t n d i 1 n q i k t n the numerator is the average fraction of land use k in the neighbourhood d of each central cell i and the denominator is the average fraction of land use k in the entire landscape n here we only included neighbourhood characteristics in the 3x3 neighbourhood around each central cell but other neighbourhoods are possible verburg et al 2004a when predicting suitability at each time step the autocovariates are recalculated based on the assigned fractions from the previous timestep our response variable is a fractional land use value not discrete classes normally required in multinomial regression therefore we assume that underlying the land use fractions for each cell is a vector of counts c i k t that sums to a total number of counts c in each cell e g c 1e6 we derive these counts through c i k t q i k t c in integer representation the data are approximately proportional to the original fractions when fitting the suitability model parameter uncertainty depends on the assumption of c c should be chosen to be small enough for fast model convergence and large enough to represent the degree of numerical precision in the observed fractions for example if there are only 2 decimal places setting c 100 results in counts that represent all of the information contained in the original fractions accordingly the multinomial logit model takes the form s i k t p y i k e β k x i t γ d k f d i k t k 1 k e β k x i t γ d k f d i k t where k is the reference land use class β k the estimated parameters in each class for covariates x i t and γ d k the estimated parameters for autocovariates f d i k t we estimated parameters using r s nnet package u venables and ripley 2002 u predicted fractions satisfy k 1 k s i k t 1 all software development and model validation was conducted in r version 4 0 1 r development core team 2008 2 2 data we developed and tested flutes using land use and environmental data from the amazon basin we downloaded 7 time steps 1992 1997 2003 2008 2013 2015 and 2018 of the global land cover map provided through the european space agency s climate change initiative land cover cci lc project european space agency 2019 these data are available at a grid resolution of 300 m we combined the recorded 31 land cover classes to 9 new classes of land use we deemed crucial to identify processes leading to agricultural expansion and declines in habitat table 2 we aggregated the resolution 10 km2 squares calculating fractions of land use from the cell counts of each land use class on the original map present in each new cell fractional land use in k classes is mapped over n raster cells with fractions q i k t in cell i in each land use class k always satisfying 0 q i k t 1 and k 1 k q i k t 1 we downloaded a set of spatially explicit climate topographic soil and human covariates table 3 for a full list of covariates derived neighbourhood covariates from observed land use in the first time step and estimated observed demand change by calculating the landscape wide mean fraction for each land use class in each observed time step all explanatory covariates were standardized to have mean 0 and standard deviation 1 we removed covariates from correlated pairs spearman s rank correlation coefficient 0 7 always retaining the covariate with the smaller average correlation with all other covariates in order to maximise the amount of independent information in the final data set used for fitting 2 3 model constraints analysing time series data we determined that only very small percentages of cells change from being devoid of a particular land use to containing that land use within one time step table 4 to control unrealistic dispersal of land uses into areas where they have not previously existed we added a user defined constraint that land use increases are more likely to be applied to cells where the land use is already present the constraint parameter was the percentage of cells in which a non existent land use was newly established between time steps for example setting the constraint to 100 would allow increases of a land use in all cells that did not contain that land use in the previous time step we parametrized the constraint by determining on how many cells expressed as a percentage we could observe the new establishment of a land use from one time step to the next table 4 to account for annual variation we calculated the mean of these percentages for each land use throughout the entire observed time series for example throughout the simulation we allowed cro increases in 1 35 of the cells in which cro was not present in the preceding time step table 4 we selected those cells for new establishment of a land use that had the highest predicted suitability for that land use see appendix b for more information on this constraint we masked category i and ii protected areas established up until 1992 from land use changes as has been shown previously see fig 4 for a map of protected areas verburg et al 2002 iucn and unep wcmc 2014 kapitza et al 2021 to reflect the high initial investment of urban infrastructure we did not allow reductions in urban land verburg and overmars 2009 2 4 validating the intensity and direction of predicted changes first we examined the accuracy of the multinomial suitability model and how it is affected by spatial resolution and the included covariates to account for spatial autocorrelation in the environmental covariates and land use time series we conducted spatial blocks cross validation valavi et al 2019 by separating the landscape into 9 equal sized spatial blocks we fitted models using data from 8 of the 9 blocks and predicted the model to the withheld block until predictions were made for the entire study area we cross validated suitability models at 1 km and 10 km including 1 only environmental covariates 2 only neighbourhood covariates and 3 both covariate types combined for each of the three models we measured predictive performance by estimating cell level suitability root mean squared error rmsesuit between the predicted suitability surfaces s m i k t and the observed fractions o i k t following r m s e s u i t m i t 1 k k 1 k o i k t s m i k t 2 for each suitability model m second to validate the intensity of changes predicted by the allocation algorithm we assessed the accuracy of predictions of cell level fractions under competing models predicted throughout the observed time series 1 under the null model we assumed no change of land use through time the null model served as reference to measure the improvements provided by each additional model component 2 under the naive model we only allocated additional demands but scaled cell level allocations with the average supply observed across the entire landscape this model assumes that suitability is not informative about where a change will happen and that allocations are equally likely to be anywhere in the landscape 3 under the semi naive model cell level allocations were additionally scaled with the predicted suitability surfaces s i k t as illustrated in fig 2 4 under the full model allocations were scaled with suitability surfaces s i k t and all constraints constraining most increases to cells where land use type already exists and masking protected areas from changes were applied we calculated rmsealloc under each allocation model w to estimate how well the different model components simulated each cell level vector of land use fractions q m i k t compared to the respective observed vectors o i k t following r m s e a l l o c w i t 1 k k 1 k o i k t q w i k t 2 due to the squared term rmse cannot inform on whether the models correctly identified the direction of change therefore we estimated and validated the direction of cell level changes decreases no change increases separately we mapped these transitions for each class between the time steps of the observed time series and the time steps of the time series simulated under each model we calculated overall difference of each pair of corresponding maps to obtain an interpretable measure of similarity of predicted and observed direction of changes pontius and millones 2011 pontius and santacruz 2014 achieving high accuracy in these first two model goals would suggest that simulated patterns of land use change closely resemble observed patterns 2 5 case study agricultural expansion in the amazon basin the amazon catchment is largest river basin in the world and occupies over one third of the south american land mass fig 4a as the world s most diverse tropical forest area the basin hosts at least 10 of the world s known species da silva et al 2005 the amazon biome is threatened by a multitude of interacting factors ecosystem services such as water supply carbon storage and provision of species habitat are directly threatened by the effects of climate change and the increasing pressure on land with projected severe reductions in water yields carbon content and species habitat which is particularly affected by changes in natural vegetation cover prüssmann et al 2016 the primary uses for cleared forest land are pasture for cattle farming and industrial soy cropping nepstad et al 2014 fao 2015 between 1992 and 2018 the biome has seen significant increases in land required for cropping and pasture as well as significant decreases in forest cover fig 4b using a broad reclassification of the predicted and observed land use classes into cropland pasture and habitat we were able to specifically validate flutes s ability to predict agricultural expansion and habitat declines as aggregated threats to ecosystems and biodiversity first we determined areas of agricultural pasture or cropland expansion with simultaneous declines in classes containing natural habitats for wet and oth we categorized the observed and predicted maps into 1 areas with no cropland increase 2 areas where cropland increase led to mostly forest declines net replacement of forest and 3 areas where cropland increase led to mostly declines in other natural habitat classes net replacement of other habitat similarly we categorized the landscape into 1 areas with no pasture increase 2 areas where pasture increase led to mostly forest declines and 3 areas where pasture increase led to mostly declines in other natural habitat classes from the resulting reclassified time series we assessed the difference between the respective observed and predicted maps by overlaying them and identifying where no agricultural increase was observed and predicted persistence predicted as persistence where agricultural increase was correctly predicted and led to decreases in the correct habitat class where agricultural increase was correctly predicted but resulted in decreases in the incorrect habitat class where no agricultural increase was observed but agricultural increase was predicted and where agricultural increase was observed but not predicted pontius et al 2011 3 results 3 1 predicting land use change intensity results of the cross validation of the suitability model component show that including neighbourhood covariates resulted in substantial predictive performance improvements across spatial blocks at both resolutions fig 5 c fig s1 for predicted suitability maps of all 9 land use classes models using neighbourhood covariates alone were approximately as good as the model using the full covariate set including only environmental variables resulted in less accurate predictions at both resolutions with predictions under the fine resolution comparatively worse than under the coarse resolution under all tested models naive semi naive full the accuracy of cell level allocations improved with the intensity of observed changes fig 5a this implies that flutes makes good predictions under scenarios with high expected overall changes where observed changes were large fig 5a bottom two panels including land use suitability and constraints full model resulted in substantial increases of predictive performance in these areas the null model s assumption of no spatial variation in reallocation of land use introduced very high bias which our constraints were able to reduce when observed changes were small fig 5a top two panels the null model made near perfect predictions given how close the null model already was to the truth improvements by allocating demand naive model and accounting for land use suitability semi naive model were difficult to achieve in the smallest change category fig 5a top left panel the naive and semi naive predictions were in fact slightly worse than the null in these areas the largest observed changes were below 0 5 making the assumption of no change under the null model highly plausible under the full model the applied constraint limited the areas that could be flagged for increases accordingly where observed changes were small this model made better predictions than the semi naive and naive models in which this constraint was not applied 3 2 predicting the direction of land use changes the worst predictions of cell level direction of change were made by the naive and semi naive models and the best predictions under the full model fig 5b with overall difference consistently less than 25 predictions became more accurate the more model components were applied under the full model we achieved the highest prediction accuracy overall the semi naive model performed slightly better than the naive model demonstrating the utility of scaling allocations with land use suitability surfaces 3 3 predicting agricultural expansion and habitat declines flutes achieved high accuracy when predicting agricultural cropland and pasture expansion on forest and other land use types containing natural habitats fig 6 in more than 80 of cells flutes predicted correctly whether agricultural land cropland or pasture increased or persisted at current levels or decreased and which habitat type decreased due to increases in agricultural classes fig 6b d the percentage of the landscape in which we correctly predicted cropland increase at the expense of the correct habitat class increased through the time series suggesting that flutes was good at identifying not only where cropland did not change or decreased persistence but also where it increased and on which habitat type that increase took place fig 6b flutes made some incorrect predictions of cropland increase in areas where no increase was observed in the southern tip and the central north of the study area although these areas were very small compared to surrounding areas in which increases were correctly predicted and occurred on the appropriate habitat classes fig 6a perhaps the most severe type of error in terms of ecological considerations was the prediction of no cropland increase persistence in areas where increase was observed however these areas were small increasing from 2 3 of the landscape at the beginning to 9 6 at the end of the time series pasture expansion fig 6c and d was much smaller than cropland expansion overall with much larger areas of the basin correctly predicted as not increasing in pasture land persistence through time fig 6d some small areas in the southern tip the central north and along the western boundary of the basin were correctly predicted to increase in pasture land with decreases in the appropriate natural habitat class pasture expansion was underpredicted in very small areas in the south north and along the eastern boundary of the basin fig 6c 4 discussion and conclusions we have presented a new fractional land use change allocation model to predict land use fractions thus retaining information at sub pixel level the model is able to accurately allocate fractions of land use through time especially under scenarios of more extreme land use change we explicitly accounted for competition between land use types and land use suitability in response to environmental drivers by means of a multinomial logistic model and could show that this aspect brings substantial improvements to predictions when compared to the assumption that land use does not change at all null model flutes made accurate predictions in areas in which only small land use changes were observable but also in areas where land use changes were observed to be high this suggests that flutes provides a suitable method to produce future land use maps under contrasting scenario settings in scenarios where demand changes are expected to be high flutes allocates supply to match aggregated demand changing the total area allocated to different land uses and also allowing land uses to be established in new areas in scenarios with small expected demand changes land use changes including the establishment of land uses in new areas remain small we assumed that the initial land use distribution we used to calibrate flutes resulted from long time periods of optimizing behaviour and we have not yet implemented a parameter allowing to specify land use elasticity the propensity of land uses to shift across the landscape without net changes to their total areas at the study area level as is implemented for example in dyna clue verburg and overmars 2009 for this reason in flutes land use cannot change to match predicted land use suitability alone for example if the modelled cropland suitability in an area is 0 8 but the observed cropland fraction is 0 2 flutes would only allow a local increase in cropland if the aggregated demand for cropland at the study area level increased while the discrepancy between an area s potential for a certain land use measured by the predicted suitability and the realized fraction of that land use implicitly captures processes that cannot be captured by the suitability model in this first version of flutes this only occurs when triggered by changes in external demand for that land use similar to clue our constraint on turn over allowed us to account for conversion effort here data from the observed validation time series allowed us to extract a raw estimate of the constraint parameter to tune flutes we estimated the parameter using long term observed means we assume this to be similarly informative as informal expert knowledge which has been suggested as a primary means to parametrize land use conversion effort in previous land use models van asselen and verburg 2013 overmars et al 2007 we could show that flutes is very easily adaptable to specific ecological study contexts when validating our model s performance in the context of agricultural expansion on natural habitat we mapped the model s ability to reproduce where agricultural expansion occurs in both pasture and cropland and which natural habitat classes decreased in their place consistently more than 80 of the landscape where correctly classified as no change or a decrease persistence or increase of agricultural land with decreases in the correct habitat types crucially underprediction of agricultural expansion with possible negative implications for conservation management remained very small throughout this demonstrates that flutes is a useful tool to predict the spatial configuration of land use change impacts that are driven by agricultural expansion into different habitat types validating the suitability model component of our model approach we found that neighbourhood covariates explained much of the suitability patterns across the landscape this is a common effect of including flexible spatial correlation terms in models with other spatially varying covariates spatial confounding hodges and reich 2010 the models describe the spatial pattern with the spatial correlation term but this effect does not imply causation and other drivers included in the model may still drive changes in the response particularly over long time periods here similar to what was shown by dendoncker et al 2007 including neighbourhood covariates lead to the most highly fitted models allowing spatial autocorrelation to drive patterns seems a sensible choice for predictions in this case study because the model only predicts three decades however for longer time spans spatial autocorrelation probably becomes less important and continental scale environmental driving factors acting homogeneously across the whole landscape may dominate patterns in reality when making such longer term predictions this could be captured by fitting the suitability model with several time steps of data thus ensuring that land use suitability is less reliant on the present land use state but more weight is given to long term and large scale environmental processes the results of our validation also strongly indicate that in the case of flutes adding constraints decision rules in terms of where and how land use changes are allowed to occur are responsible for the majority of increases in predictive performance while we provide initial steps in parametrising these constraints more specific knowledge of bottom up processes that drive land use stasis and change across the landscape could further consolidate the accuracy of flutes for example this could be achieved by including data on the expected behaviour of economic agents who seek to maximise returns on their productive land one example includes the land use trade offs luto model bryan et al 2014 connor et al 2015 which includes pixel wise optimisation of cost and return of alternative land uses however such models are difficult to parametrize in data scarce regions and require significant computational power bottom up processes such as price feedbacks also tend to act at very fine spatial resolutions but have little effect when seen at a continental scale where scenario uncertainty and global processes dominate predictions connor et al 2015 depending on scale including very fine scale dynamics of agent behaviour may simply not pay off or it might be more appropriate to merely downscale them to the study area extent van asselen and verburg 2013 connor et al 2015 in order to allow scaling flutes to global applications we only used drivers that were available at global scales however improvements to the land use suitability model can be achieved by including more proximate drivers of land use change such as market accessibility meiyappan et al 2014 verburg et al 2011 by fitting the land use suitability model for individual subsets of the study area to improve local fit or by creating more land use classes for which particular biophysical constraints are known including location dependent drivers and models and raising the resolution may substantially improve the accuracy of land use suitability maps increasing the contribution of this model component to overall prediction accuracy developments of flutes and expanding application could include the estimation of use intensity of different land use types which has been shown to be an important driver of biodiversity change newbold et al 2015 2016 such developments could enhance efforts to tailor macroeconomic and land use modelling to assess the fate of future biodiversity kapitza et al 2021 authorship contribution statement sk bw conceived the idea of providing a fractional land use model ng and sk and bw designed the model and validation steps sk coded the model and analyzed the data sk led the manuscript with edits from ng and bw data accessibility all input data required to repeat this work will be made available and receive a permanent doi through figshare upon publication an r package containing the model source code is available on github kapitzas flutes all code for data preprocessing and analysis is also provided through a github repository kapitzas frac lumodel both repositories will be receive permanent doi through zenodo upon publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful for contributions made throughout the research phase by j elith and d zurell and helpful comments by robert g pontius jr on a preprint of this manuscript funding this work received funding under the australian research council discovery grant dp170104795 ng was supported by an arc decra fellowship de180100635 sk was supported by the melbourne international research scholarship mirs appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105258 
25701,by mapping land use under projections of socio economic change ecological changes can be predicted to inform conservation decision making we present a land use model that enables the fine scale mapping of land use change under future scenarios its predictions can be used as input to virtually all existing spatially explicit ecological models our model maps the fractional cover of land use within each grid cell providing higher information content than discrete classes at the same spatial resolution the method accurately reproduced land use patterns observed in the amazon both in terms of the allocated fractional amounts and also the direction of predicted land use changes a small case study showcases the application of our model to reproduce patterns of agricultural expansion and natural habitat declines the model source code is provided as an open source r package making this new method available to bridge the gap between socio economic land use and biodiversity modelling keywords land use forecasting fractional land cover continuous fields agricultural expansion socio economic change biodiversity conservation 1 introduction 1 1 accounting for land use change in biodiversity assessments land use change is a key driver of global environmental change causing global declines in biodiversity species extinctions and resulting in the deterioration of ecosystem services foley 2005 e resolved h ipbes et al 2019 there is mounting evidence of adverse impacts of land use change on biodiversity the need for global assessments of future biodiversity change in response to land use change has been increasingly acknowledged urban et al 2016 kim et al 2018 powers and jetz 2019 however work concerned with understanding future biodiversity change tends to focus on climate change titeux et al 2016 struebig et al 2015 urban et al 2016 or other aggregated effects of socio economic change such as forest loss pérez vega 2012 margono et al 2014 and urban expansion seto et al 2012 this is despite the fact that land use change is highly driven by dynamic bio physical and socio economic processes lambin et al 2011 climate change will likely result in global shifts and declines of land suitable for agricultural production with projected depletion of land reserves in the first half of the 21st century lambin et al 2011 food production and international trade of goods will significantly increase o neill et al 2014 2017 and even under lowest impact scenarios crop and livestock production are still likely to be higher and occupy a larger land area than they do today o neill et al 2014 consequently future predictions of biodiversity change will benefit from explicit accounting of the drivers and effects of land use change at the level of individual types of use detailed large scale maps of future land use under competing future scenarios provide useful insights for researchers and policy makers particularly in terms of informing conservation planning and preventing future biodiversity loss 1 2 overview of land use modelling approaches different modelling approaches have been developed to determine the overall amount of future land use and allocate changes across the landscape artificial neural networks and markov chain models learn and infer total amounts and spatial patterns of land use change from historic time series tayyebi and pijanowski 2014 pijanowski et al 2002 markov chain models have been frequently combined with cellular automata ca markov models see hyandye and martz 2017 aburas et al 2017 van schrojenstein lantman et al 2011 in cellular automata the transition probability of a cell to another land use depends on its current state and the state of neighbouring cells both of which are the result of historic changes van schrojenstein lantman et al 2011 cellular automata have been used successfully to simulate strongly auto correlated changes such urban sprawl verburg et al 2004b fang et al 2005 shafizadeh moghadam and helbich 2013 sun et al 2007 many modelling approaches apply regression analysis and other techniques to identify associations between various environmental conditions and observed land use patterns van schrojenstein lantman et al 2011 lambin et al 2000 verburg et al 2004b available models applying this approach include sleuth slope land use exclusion urban transportation hillshade dietzel and clarke 2007 dinamica ego environment for geoprocessing objects soares filho et al 2009 lcm in terrset land change modeler eastman and toledano 2018 and perhaps most prominently the clue model series conversion of land use and its effects verburg and overmars 2009 table 1 clue models have found application in the prediction of spatially explicit patterns of land use at national and continental scales veldkamp and fresco 1996 verburg and overmars 2009 verburg et al 1999 2002 kapitza et al 2021 exogenously determined future changes in area demands for different land uses often predicted by an economic model aguiar et al 2016 may be downscaled by establishing statistical relationships between observed land use and a set of socio economic and bio physical drivers of land use and land use change predicted land use suitability surfaces inform local competition for different land uses verburg et al 2002 meiyappan et al 2014 models can be further parametrized by including transition rules at local cell and landscape levels and constraints on overall turn over through time more simplistic models based on statistical analysis use an ordered allocation algorithm in which competition between land uses is handled by ordering allocations in terms of perceived socio economic value fuchs et al 2013 land use change allocation algorithms are agnostic to the type of statistical analysis conducted to estimate land use suitability surfaces nevertheless most models apply binary logistic regression to model the cell wise probabilities of occurrence for each land use category independent of the probabilities of other land uses the resulting probability of land use occurrence at a site produced by separate models is an incomplete representation of the underlying structure of land use probability because it omits that occurrence probabilities are dependent between land use types and that the probabilities of all discrete classes must sum to one for example when a site has very high probability for urban land use this implies relatively low probabilities for primary natural habitat which separate independent logistic regressions do not fully capture one step toward explicitly modelling competition between land uses is to apply multinomial regression thus allowing for the prediction of conditional binary probabilities of multiple classes noszczyk 2019 1 3 continuous land use fractions categorical land use data sets are increasingly available at spatial resolutions of finer than 1 km three prominent examples include the corine coordination of information on the environment land cover inventory bossard et al 2000 which contains several time steps between 1990 and 2018 at 100 m resolution for the european continent global land cover maps produced for the year 2010 through copernicus land monitoring service european union 2019 at the same resolution as well as global maps of land cover in annual time steps between 1992 and 2018 produced under the european space agency s esa climate change initiative land cover cci lc project european space agency 2019 available at 300 m resolution however the spatial variables that represent drivers of land use and biodiversity change are often not available over large spatial extents at fine resolutions better than 1 km dendoncker et al 2006 therefore it is necessary to resample finely resolved land use data to match the coarser resolution of driver covariates lowering the resolution has the additional advantage of improving computational efficiency due to the smaller number of pixels in the coarser map resampling fine resolution maps by assigning a single category of land use on each coarser pixel effectively eliminates sub pixel information on land use seo et al 2016 so this approach is not desirable fig 1 in order to maximise the retained information contained in the coarser map it is preferable to calculate the fractions of land use covering each new pixel producing continuous fields of information and keeping information at sub pixel level seo et al 2016 fig 1 the higher information content retained in fractional land use representations has high utility in ecological modelling for example many species may be able to persist in an area if only a small proportion of the area is made up of a suitable land class such as remnant vegetation wintle et al 2019 many wide ranging species may persist in landscapes if a certain proportion of the landscape is comprised of old forest it has been shown that continuous fields of land use allow better estimation of biomass and biomass change xian et al 2015 and are better able to explain variation in home range sizes bevanda et al 2014 than categorical land use data continental scale biodiversity assessments have shown that patterns are associated with high spatial resolution fractional land use measures such as the regional aggregation of land use types land cover diversity and land use covariates including land use intensity mouchet et al 2015 and actual evapo transpiration mouchet et al 2015 whittaker et al 2006 creating maps of some of these covariates requires fine scale maps of fractional land use as principal input plutzar et al 2016 the intensification of agriculture and forest harvesting are crucial factors shaping biodiversity levers et al 2014 2016 that require inputs of crop type and vegetation composition within each spatial unit these ecological considerations of the utility of fractional land cover and land use representations are underpinned by recent advancements in algorithms to produce high resolution maps of fractional land cover from satellite data allred et al 2021 hill and guerschman 2020 however only few land use modelling approaches are capable of predicting continuous fractions of land use directly see hasegawa et al 2017 meiyappan et al 2014 by providing fine resolution categorical representations that can be resampled to coarser resolution continuous representations see future land use simulation model flus liu et al 2017 or as part of integrated assessment frameworks see climsave integrated assessment platform climsave ia harrison et al 2013 however some of these documented approaches are not available in a useable package suited to regional continental scale hasegawa et al 2017 meiyappan et al 2014 or provide user interfaces that do not allow seamless reproducible integration into programmatic workflows harrison et al 2013 liu et al 2017 table 1 1 4 objectives of this paper our new land use model flutes fractional land use transitions in ecological systems provides a readily available means to incorporate fractional land use change into ecological modelling an advantage of flutes compared to existing fractional land use modelling approaches is its implementation in r r development core team 2008 making use of a development environment for which high expertise already exists among ecological modellers flutes can be fitted at different scales with minimal parametrization requirements and performs efficiently at various resolutions and extents the source code for our method is freely available as a small open source r package hosted on github kapitzas flutes as such our approach contributes a new open method toward bridging the gap between socio economic land use and biodiversity modelling we provide a mathematical description of the developed fractional land use model and evaluate flutes according to its ability to correctly estimate the direction and intensity of observed land use changes using a case study in the brazilian amazon 2 materials and methods 2 1 model description the model consists of two main components fig 2 first statistical analysis is used to determine how the suitability of the landscape for different land uses relates to a set of environmental drivers of land use change producing a suitability surface for each land use class fig 2a see fig 3 for an example of a suitability surface second fractional changes in additional land use demands are allocated iteratively in the landscape scaling with the land use suitability surfaces fig 2b see fig 3 for an example of a final allocated map of fractional land use we utilize a cellular automaton to introduce cell level allocation decisions that constrain the location and direction of land use changes according to three rules first future land use supply must meet additional demand projections of land use demands may be provided through external models such as computational general equilibrium cge models i e gtap aguiar et al 2016 or through the analysis and extrapolation of historic patterns moulds et al 2015 the model allocates additional demand by adding cell level supply of that time step d i k t 1 in cell i land use k and time step t 1 to the fractions of the current time step q i k t fig 2b the first model objective can be formulated i 1 n q i k t 1 i 1 n q i k t d i k t 1 i 1 n d i k t 1 d k t 1 d k t 1 is the additional landscape wide supply and is at equilibrium with additional demand after the algorithm converges second supply of all land use types in a cell d i k t 1 is allocated across cells so it adds up to one k 1 k q i k t 1 1 fig 2b third cell level supply d i k t 1 has to be distributed in such a way that the allocated amounts in each cell scale with a predicted probability surface s by modelling q i k t 0 s i k f k x i where x i is a set of demographic and bio physical drivers related to land use f k is a multinomial multi response model fig 2a the parameter estimation of this model is based on the first time step and predicted to the conditions of subsequent time steps accordingly while the model assumes stationarity of the modelled statistical relationships it implements temporal dynamics based on changing demand and changing environmental conditions changing environmental conditions are represented as changes to independent model variables the land use status in a cell s neighbourhood has been shown to play an important role in determining a cell s land use dendoncker et al 2007 mustafa et al 2018 van vliet et al 2013 verburg et al 2004a our suitability model applies neighbourhood interactions by calculating autocovariates verburg et al 2004a and including these in the multinomial regression of the land use suitability model following verburg et al 2004a our autocovariates measure the amount of clustering of land uses in a user defined cell neighbourhood when compared to the entire landscape we calculate autocovariates as enrichment factors f d i k t i d q i k t n d i 1 n q i k t n the numerator is the average fraction of land use k in the neighbourhood d of each central cell i and the denominator is the average fraction of land use k in the entire landscape n here we only included neighbourhood characteristics in the 3x3 neighbourhood around each central cell but other neighbourhoods are possible verburg et al 2004a when predicting suitability at each time step the autocovariates are recalculated based on the assigned fractions from the previous timestep our response variable is a fractional land use value not discrete classes normally required in multinomial regression therefore we assume that underlying the land use fractions for each cell is a vector of counts c i k t that sums to a total number of counts c in each cell e g c 1e6 we derive these counts through c i k t q i k t c in integer representation the data are approximately proportional to the original fractions when fitting the suitability model parameter uncertainty depends on the assumption of c c should be chosen to be small enough for fast model convergence and large enough to represent the degree of numerical precision in the observed fractions for example if there are only 2 decimal places setting c 100 results in counts that represent all of the information contained in the original fractions accordingly the multinomial logit model takes the form s i k t p y i k e β k x i t γ d k f d i k t k 1 k e β k x i t γ d k f d i k t where k is the reference land use class β k the estimated parameters in each class for covariates x i t and γ d k the estimated parameters for autocovariates f d i k t we estimated parameters using r s nnet package u venables and ripley 2002 u predicted fractions satisfy k 1 k s i k t 1 all software development and model validation was conducted in r version 4 0 1 r development core team 2008 2 2 data we developed and tested flutes using land use and environmental data from the amazon basin we downloaded 7 time steps 1992 1997 2003 2008 2013 2015 and 2018 of the global land cover map provided through the european space agency s climate change initiative land cover cci lc project european space agency 2019 these data are available at a grid resolution of 300 m we combined the recorded 31 land cover classes to 9 new classes of land use we deemed crucial to identify processes leading to agricultural expansion and declines in habitat table 2 we aggregated the resolution 10 km2 squares calculating fractions of land use from the cell counts of each land use class on the original map present in each new cell fractional land use in k classes is mapped over n raster cells with fractions q i k t in cell i in each land use class k always satisfying 0 q i k t 1 and k 1 k q i k t 1 we downloaded a set of spatially explicit climate topographic soil and human covariates table 3 for a full list of covariates derived neighbourhood covariates from observed land use in the first time step and estimated observed demand change by calculating the landscape wide mean fraction for each land use class in each observed time step all explanatory covariates were standardized to have mean 0 and standard deviation 1 we removed covariates from correlated pairs spearman s rank correlation coefficient 0 7 always retaining the covariate with the smaller average correlation with all other covariates in order to maximise the amount of independent information in the final data set used for fitting 2 3 model constraints analysing time series data we determined that only very small percentages of cells change from being devoid of a particular land use to containing that land use within one time step table 4 to control unrealistic dispersal of land uses into areas where they have not previously existed we added a user defined constraint that land use increases are more likely to be applied to cells where the land use is already present the constraint parameter was the percentage of cells in which a non existent land use was newly established between time steps for example setting the constraint to 100 would allow increases of a land use in all cells that did not contain that land use in the previous time step we parametrized the constraint by determining on how many cells expressed as a percentage we could observe the new establishment of a land use from one time step to the next table 4 to account for annual variation we calculated the mean of these percentages for each land use throughout the entire observed time series for example throughout the simulation we allowed cro increases in 1 35 of the cells in which cro was not present in the preceding time step table 4 we selected those cells for new establishment of a land use that had the highest predicted suitability for that land use see appendix b for more information on this constraint we masked category i and ii protected areas established up until 1992 from land use changes as has been shown previously see fig 4 for a map of protected areas verburg et al 2002 iucn and unep wcmc 2014 kapitza et al 2021 to reflect the high initial investment of urban infrastructure we did not allow reductions in urban land verburg and overmars 2009 2 4 validating the intensity and direction of predicted changes first we examined the accuracy of the multinomial suitability model and how it is affected by spatial resolution and the included covariates to account for spatial autocorrelation in the environmental covariates and land use time series we conducted spatial blocks cross validation valavi et al 2019 by separating the landscape into 9 equal sized spatial blocks we fitted models using data from 8 of the 9 blocks and predicted the model to the withheld block until predictions were made for the entire study area we cross validated suitability models at 1 km and 10 km including 1 only environmental covariates 2 only neighbourhood covariates and 3 both covariate types combined for each of the three models we measured predictive performance by estimating cell level suitability root mean squared error rmsesuit between the predicted suitability surfaces s m i k t and the observed fractions o i k t following r m s e s u i t m i t 1 k k 1 k o i k t s m i k t 2 for each suitability model m second to validate the intensity of changes predicted by the allocation algorithm we assessed the accuracy of predictions of cell level fractions under competing models predicted throughout the observed time series 1 under the null model we assumed no change of land use through time the null model served as reference to measure the improvements provided by each additional model component 2 under the naive model we only allocated additional demands but scaled cell level allocations with the average supply observed across the entire landscape this model assumes that suitability is not informative about where a change will happen and that allocations are equally likely to be anywhere in the landscape 3 under the semi naive model cell level allocations were additionally scaled with the predicted suitability surfaces s i k t as illustrated in fig 2 4 under the full model allocations were scaled with suitability surfaces s i k t and all constraints constraining most increases to cells where land use type already exists and masking protected areas from changes were applied we calculated rmsealloc under each allocation model w to estimate how well the different model components simulated each cell level vector of land use fractions q m i k t compared to the respective observed vectors o i k t following r m s e a l l o c w i t 1 k k 1 k o i k t q w i k t 2 due to the squared term rmse cannot inform on whether the models correctly identified the direction of change therefore we estimated and validated the direction of cell level changes decreases no change increases separately we mapped these transitions for each class between the time steps of the observed time series and the time steps of the time series simulated under each model we calculated overall difference of each pair of corresponding maps to obtain an interpretable measure of similarity of predicted and observed direction of changes pontius and millones 2011 pontius and santacruz 2014 achieving high accuracy in these first two model goals would suggest that simulated patterns of land use change closely resemble observed patterns 2 5 case study agricultural expansion in the amazon basin the amazon catchment is largest river basin in the world and occupies over one third of the south american land mass fig 4a as the world s most diverse tropical forest area the basin hosts at least 10 of the world s known species da silva et al 2005 the amazon biome is threatened by a multitude of interacting factors ecosystem services such as water supply carbon storage and provision of species habitat are directly threatened by the effects of climate change and the increasing pressure on land with projected severe reductions in water yields carbon content and species habitat which is particularly affected by changes in natural vegetation cover prüssmann et al 2016 the primary uses for cleared forest land are pasture for cattle farming and industrial soy cropping nepstad et al 2014 fao 2015 between 1992 and 2018 the biome has seen significant increases in land required for cropping and pasture as well as significant decreases in forest cover fig 4b using a broad reclassification of the predicted and observed land use classes into cropland pasture and habitat we were able to specifically validate flutes s ability to predict agricultural expansion and habitat declines as aggregated threats to ecosystems and biodiversity first we determined areas of agricultural pasture or cropland expansion with simultaneous declines in classes containing natural habitats for wet and oth we categorized the observed and predicted maps into 1 areas with no cropland increase 2 areas where cropland increase led to mostly forest declines net replacement of forest and 3 areas where cropland increase led to mostly declines in other natural habitat classes net replacement of other habitat similarly we categorized the landscape into 1 areas with no pasture increase 2 areas where pasture increase led to mostly forest declines and 3 areas where pasture increase led to mostly declines in other natural habitat classes from the resulting reclassified time series we assessed the difference between the respective observed and predicted maps by overlaying them and identifying where no agricultural increase was observed and predicted persistence predicted as persistence where agricultural increase was correctly predicted and led to decreases in the correct habitat class where agricultural increase was correctly predicted but resulted in decreases in the incorrect habitat class where no agricultural increase was observed but agricultural increase was predicted and where agricultural increase was observed but not predicted pontius et al 2011 3 results 3 1 predicting land use change intensity results of the cross validation of the suitability model component show that including neighbourhood covariates resulted in substantial predictive performance improvements across spatial blocks at both resolutions fig 5 c fig s1 for predicted suitability maps of all 9 land use classes models using neighbourhood covariates alone were approximately as good as the model using the full covariate set including only environmental variables resulted in less accurate predictions at both resolutions with predictions under the fine resolution comparatively worse than under the coarse resolution under all tested models naive semi naive full the accuracy of cell level allocations improved with the intensity of observed changes fig 5a this implies that flutes makes good predictions under scenarios with high expected overall changes where observed changes were large fig 5a bottom two panels including land use suitability and constraints full model resulted in substantial increases of predictive performance in these areas the null model s assumption of no spatial variation in reallocation of land use introduced very high bias which our constraints were able to reduce when observed changes were small fig 5a top two panels the null model made near perfect predictions given how close the null model already was to the truth improvements by allocating demand naive model and accounting for land use suitability semi naive model were difficult to achieve in the smallest change category fig 5a top left panel the naive and semi naive predictions were in fact slightly worse than the null in these areas the largest observed changes were below 0 5 making the assumption of no change under the null model highly plausible under the full model the applied constraint limited the areas that could be flagged for increases accordingly where observed changes were small this model made better predictions than the semi naive and naive models in which this constraint was not applied 3 2 predicting the direction of land use changes the worst predictions of cell level direction of change were made by the naive and semi naive models and the best predictions under the full model fig 5b with overall difference consistently less than 25 predictions became more accurate the more model components were applied under the full model we achieved the highest prediction accuracy overall the semi naive model performed slightly better than the naive model demonstrating the utility of scaling allocations with land use suitability surfaces 3 3 predicting agricultural expansion and habitat declines flutes achieved high accuracy when predicting agricultural cropland and pasture expansion on forest and other land use types containing natural habitats fig 6 in more than 80 of cells flutes predicted correctly whether agricultural land cropland or pasture increased or persisted at current levels or decreased and which habitat type decreased due to increases in agricultural classes fig 6b d the percentage of the landscape in which we correctly predicted cropland increase at the expense of the correct habitat class increased through the time series suggesting that flutes was good at identifying not only where cropland did not change or decreased persistence but also where it increased and on which habitat type that increase took place fig 6b flutes made some incorrect predictions of cropland increase in areas where no increase was observed in the southern tip and the central north of the study area although these areas were very small compared to surrounding areas in which increases were correctly predicted and occurred on the appropriate habitat classes fig 6a perhaps the most severe type of error in terms of ecological considerations was the prediction of no cropland increase persistence in areas where increase was observed however these areas were small increasing from 2 3 of the landscape at the beginning to 9 6 at the end of the time series pasture expansion fig 6c and d was much smaller than cropland expansion overall with much larger areas of the basin correctly predicted as not increasing in pasture land persistence through time fig 6d some small areas in the southern tip the central north and along the western boundary of the basin were correctly predicted to increase in pasture land with decreases in the appropriate natural habitat class pasture expansion was underpredicted in very small areas in the south north and along the eastern boundary of the basin fig 6c 4 discussion and conclusions we have presented a new fractional land use change allocation model to predict land use fractions thus retaining information at sub pixel level the model is able to accurately allocate fractions of land use through time especially under scenarios of more extreme land use change we explicitly accounted for competition between land use types and land use suitability in response to environmental drivers by means of a multinomial logistic model and could show that this aspect brings substantial improvements to predictions when compared to the assumption that land use does not change at all null model flutes made accurate predictions in areas in which only small land use changes were observable but also in areas where land use changes were observed to be high this suggests that flutes provides a suitable method to produce future land use maps under contrasting scenario settings in scenarios where demand changes are expected to be high flutes allocates supply to match aggregated demand changing the total area allocated to different land uses and also allowing land uses to be established in new areas in scenarios with small expected demand changes land use changes including the establishment of land uses in new areas remain small we assumed that the initial land use distribution we used to calibrate flutes resulted from long time periods of optimizing behaviour and we have not yet implemented a parameter allowing to specify land use elasticity the propensity of land uses to shift across the landscape without net changes to their total areas at the study area level as is implemented for example in dyna clue verburg and overmars 2009 for this reason in flutes land use cannot change to match predicted land use suitability alone for example if the modelled cropland suitability in an area is 0 8 but the observed cropland fraction is 0 2 flutes would only allow a local increase in cropland if the aggregated demand for cropland at the study area level increased while the discrepancy between an area s potential for a certain land use measured by the predicted suitability and the realized fraction of that land use implicitly captures processes that cannot be captured by the suitability model in this first version of flutes this only occurs when triggered by changes in external demand for that land use similar to clue our constraint on turn over allowed us to account for conversion effort here data from the observed validation time series allowed us to extract a raw estimate of the constraint parameter to tune flutes we estimated the parameter using long term observed means we assume this to be similarly informative as informal expert knowledge which has been suggested as a primary means to parametrize land use conversion effort in previous land use models van asselen and verburg 2013 overmars et al 2007 we could show that flutes is very easily adaptable to specific ecological study contexts when validating our model s performance in the context of agricultural expansion on natural habitat we mapped the model s ability to reproduce where agricultural expansion occurs in both pasture and cropland and which natural habitat classes decreased in their place consistently more than 80 of the landscape where correctly classified as no change or a decrease persistence or increase of agricultural land with decreases in the correct habitat types crucially underprediction of agricultural expansion with possible negative implications for conservation management remained very small throughout this demonstrates that flutes is a useful tool to predict the spatial configuration of land use change impacts that are driven by agricultural expansion into different habitat types validating the suitability model component of our model approach we found that neighbourhood covariates explained much of the suitability patterns across the landscape this is a common effect of including flexible spatial correlation terms in models with other spatially varying covariates spatial confounding hodges and reich 2010 the models describe the spatial pattern with the spatial correlation term but this effect does not imply causation and other drivers included in the model may still drive changes in the response particularly over long time periods here similar to what was shown by dendoncker et al 2007 including neighbourhood covariates lead to the most highly fitted models allowing spatial autocorrelation to drive patterns seems a sensible choice for predictions in this case study because the model only predicts three decades however for longer time spans spatial autocorrelation probably becomes less important and continental scale environmental driving factors acting homogeneously across the whole landscape may dominate patterns in reality when making such longer term predictions this could be captured by fitting the suitability model with several time steps of data thus ensuring that land use suitability is less reliant on the present land use state but more weight is given to long term and large scale environmental processes the results of our validation also strongly indicate that in the case of flutes adding constraints decision rules in terms of where and how land use changes are allowed to occur are responsible for the majority of increases in predictive performance while we provide initial steps in parametrising these constraints more specific knowledge of bottom up processes that drive land use stasis and change across the landscape could further consolidate the accuracy of flutes for example this could be achieved by including data on the expected behaviour of economic agents who seek to maximise returns on their productive land one example includes the land use trade offs luto model bryan et al 2014 connor et al 2015 which includes pixel wise optimisation of cost and return of alternative land uses however such models are difficult to parametrize in data scarce regions and require significant computational power bottom up processes such as price feedbacks also tend to act at very fine spatial resolutions but have little effect when seen at a continental scale where scenario uncertainty and global processes dominate predictions connor et al 2015 depending on scale including very fine scale dynamics of agent behaviour may simply not pay off or it might be more appropriate to merely downscale them to the study area extent van asselen and verburg 2013 connor et al 2015 in order to allow scaling flutes to global applications we only used drivers that were available at global scales however improvements to the land use suitability model can be achieved by including more proximate drivers of land use change such as market accessibility meiyappan et al 2014 verburg et al 2011 by fitting the land use suitability model for individual subsets of the study area to improve local fit or by creating more land use classes for which particular biophysical constraints are known including location dependent drivers and models and raising the resolution may substantially improve the accuracy of land use suitability maps increasing the contribution of this model component to overall prediction accuracy developments of flutes and expanding application could include the estimation of use intensity of different land use types which has been shown to be an important driver of biodiversity change newbold et al 2015 2016 such developments could enhance efforts to tailor macroeconomic and land use modelling to assess the fate of future biodiversity kapitza et al 2021 authorship contribution statement sk bw conceived the idea of providing a fractional land use model ng and sk and bw designed the model and validation steps sk coded the model and analyzed the data sk led the manuscript with edits from ng and bw data accessibility all input data required to repeat this work will be made available and receive a permanent doi through figshare upon publication an r package containing the model source code is available on github kapitzas flutes all code for data preprocessing and analysis is also provided through a github repository kapitzas frac lumodel both repositories will be receive permanent doi through zenodo upon publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful for contributions made throughout the research phase by j elith and d zurell and helpful comments by robert g pontius jr on a preprint of this manuscript funding this work received funding under the australian research council discovery grant dp170104795 ng was supported by an arc decra fellowship de180100635 sk was supported by the melbourne international research scholarship mirs appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105258 
25702,a comprehensive spatial and temporal understanding of the manganese cycle in drinking water reservoirs has beneficial implications for the management of water treatment related issues in this study a three dimensional manganese cycle model was developed and coupled with a hydrodynamic model for the simulation of the distribution of manganese in a monomictic water reservoir the model was successfully validated and used to model and predict the spatial temporal manganese variation in the reservoir results showed that the variations of the thermal structure of the reservoir affect the seasonal change of manganese levels and in turn the peak concentrations of manganese in the epilimnion it was also evident that the high level of dissolved oxygen in the epilimnion promotes the oxidation of soluble manganese and the oxidised particulate mn settles rapidly the advection processes mainly caused by wind driven currents play a vital role in rising or decreasing the level of soluble manganese near the dam wall the developed numerical tool allows the user to gain a better understanding of spatial manganese distribution in different locations and depths of the reservoir under different wind rain conditions among others such knowledge empowers the local water utility and enables more targeted optimized water treatment strategies keywords hydrodynamic modelling manganese drinking water treatment environmental modelling 1 introduction the world health organisation guidelines for manganese mn in drinking water indicated that mn concentrations of over 0 4 mg l can be toxic and over 0 1 mg l causes dirty water who 2004 the australian health based guideline value for mn is 0 5 mg l and the australian drinking water guidelines recommend an aesthetic guideline of 0 1 mg l to avoid taste and staining issues with soluble mn 0 05 mg l desirable nhmrc 2021 although mn concentrations in australian drinking water supply systems have usually been below the health based value the aesthetic thresholds can be exceeded in addition even lower mn concentrations can cause issues for piped water supply systems such as insoluble mn deposition and growth of bacterial deposits which causes a reduction in flow rate the mn issues in drinking water are generally caused by the increasing mn concentration in the water supply reservoir and the ineffectiveness of the treatment of mn is largely attributed to drastic changes of the mn level in the source of water bertone et al 2016a 2016b therefore understanding the mn cycle in the aquatic environment to predict critical events is the primary task to ensure effective treatment of mn mn originates from the sediments within the catchment areas of reservoirs and divalent mn mn2 can be solubilised at the interface between sediment and water ljung and vahter 2007 the character of the water sediment interface determines the diffusion of reduced mn from the sediments which is the primary source of mn in lakes and reservoirs rodríguez martín et al 2003 do is not disturbed by the mn cycle but influences whether the reduced mn is diffused from the sediments advection and diffusion are governed by the hydrodynamics of aquatic systems and affect the transport capacity of mn from the hypolimnion to the epilimnion the biological and abiotic oxidation of reduced soluble mn and the settling rate of mn oxides together determine the loss of mn in bodies of water therefore the modelling of mn transport in a dynamic system is feasible if hydrodynamic conditions and the chemical and biological reactions can be accurately simulated and integrated the mn cycle occurring in reservoirs as the source of drinking water is highly susceptible to the hydrodynamic process and it is regulated by physical chemical and microbiological processes bertone et al 2014 davison and woof 1984 hem 1963 zaw and chiswell 1999 undertook a correlation study between many water quality parameters temperature true colour turbidity do ph conductivity and alkalinity and the concentrations of the various forms of mn in the hinze dam queensland australia using a regression analysis their statistical analysis indicates that no independent factor dominates the mn variation and the relationship between mn and the water quality parameters is susceptible to the change of depth and season a temperate monomictic reservoir experiences an annual cycle of stratification and mixing conditions the stable stratification limits oxygen exchange between the epilimnion and hypolimnion which causes anoxia in the hypolimnion bertone et al 2015b elçi 2008 wang et al 2019 mn tends to remain in an insoluble reduction state in an anoxic aquatic environment gantzer et al 2009 the concentration of dissolved oxygen do also determines whether mn2 is diffused from the sediment at the bottom of the reservoir johnson et al 1991 a large amount of soluble mn exists as mn2 which gathers in the hypolimnion during stratification and anoxia oldham et al 2017 the inlet of water collection is usually installed in the epilimnion of reservoirs and the mn level in the epilimnion is minimal during stratification without suffering extreme external effects hence the water treatment plants wtps do not require thoughtful arrangements of mn removal during these periods however when destratification occurs hypolimnetic water with a high level of soluble mn is transported to the epilimnion since the oxidation of soluble mn in natural water with a ph of 6 0 8 0 is slow high levels of soluble mn persist in the epilimnion landing and bruland 1980 causing challenges for wtps to the author s knowledge only a few studies have modelled the variation and distribution of mn in aquatic environments in the last half century bertone et al 2015a egeberg et al 1988 johnson et al 1991 wang and van cappellen 1996 zaw and chiswell 1999 one of the few studies about lakes was a one dimensional 1d mathematical model of the mn cycle conducted by johnson et al 1991 this model included mn2 flux from the sediment mn2 oxidation particulate mn sediment and turbulent mixing rates calculated by the temperature method imboden et al 1979 the results emphasised the roles of do in the mn cycle and the seasonal change of mn2 flux from the bottom sediments the study indicated that it is feasible to represent the mn transport of a lacustrine system with a time dependent mathematical model the study by johnson et al 1991 indicated the feasibility of representing the mn cycle in a seasonally anoxic lake with a time dependent model however some limitations were evident in the modelling study the turbulent mixing rates under the assumption of ignoring advective heat transport and geothermal heat flow can often be unrealistic and the performance of the 1d vertical model on shallow areas would be unsatisfying more recently the number of new studies that focused on mn prediction is still restricted with either a narrower scope such as the water sediment continuum giles et al 2016 or to specific processes such as seasonal vertical zonation of redox chemistry hartland et al 2015 the most comprehensive modelling of mn in a drinking water reservoir is provided by bertone et al 2015a who developed a data driven model to predict mn concentrations in the advancetown lake one week in advance a long term historical database collected by a vertical profiling system vps provided an opportunity to apply machine learning to model mn in such sub tropical monomictic reservoir the model accurately simulated the probability of soluble mn exceeding critical thresholds which had a significant practical meaning for managing mn removal the general limitation of the data driven models such as this one is that their reliability depends on the quantity and quality of monitoring data wang et al 2020 in addition like 1d vertical mathematical models data driven models are so far unable to simulate the 3d spatial distribution of mn due to the lack of enough data for most water supply reservoirs characterised by significant morphological differences the role of the spatial distribution of mn is particularly important for the mn cycle the previous studies hardly discussed the effect of horizontal current movement on the mn cycle and neither 1d vertical process based nor data driven models can accurately simulate the 3d mn cycle in most reservoirs despite such limitation of 1d models a modelling study of the mn cycle in three dimensions 3d so far is missing coupling hydrodynamic and chemical models have been applied to a wide range of situations from groundwater to surface water garneau et al 2017 hipsey et al 2014 wang et al 2021 and offers opportunities to be applied within the scope of this work too the aim of this research is therefore to integrate a 3d hydrodynamic and a mn cycle model to enable an accurate simulation of the 3d mn cycle in an australian temperate drinking water reservoir 2 study site and manganese issues 2 1 study site the tarago reservoir 38 1 s 145 56 e fig 1 a is located approximately 85 km east of melbourne victoria australia and the maximum capacity of the reservoir is 3 758 107 m3 the dam was built in 1969 and enlarged in 1971 the surface area of the reservoir is approximately 3 6 106 m2 and its catchment area is 1 14 108 m2 cinque and jayasuriya 2010 the maximum depth of the reservoir is about 22 0 m near the dam wall continuous water quality monitoring and sample testing were conducted at the observation location near the dam see fig 1 b weekly mn sample testing for water at the surface depth of 0 m middle depth of 10 m and bottom depth of 20 m layers have been conducted since 2014 a network of remote water quality monitoring instruments and vps was deployed in the observation location in 2017 the vps recorded the water temperature and do with a 3 h frequency and 1 m vertical resolution for the entire water column the source and frequency of meteorological data boundary conditions mn records and do data have been summarised in table 1 the reservoir supplies drinking water to the westernport and the mornington peninsula which include approximately 165 000 residents via the tarago westernport pipeline the distance of this water distribution is more than 80 km which compels the tarago water treatment plant twtp to maintain a strict criterion for mn removal to avoid the corrosion of the pipeline and subsequent water pollution griffin 1960 the treated water must meets the water quality requirements of the bulk water service agreements bwsas which require that 100 of the treated water contain less than 0 05 mg l total mn and that 95 of the treated water contain less than 0 02 mg l total mn young 2009 through the statistical analysis of all water sampling tests of the mn level in treated water the percentages of the testing sample with more than 0 01 mg l 0 02 mg l and 0 05 mg l mn and the sample sizes are summarised in table 2 the table indicates that only 2018 2019 satisfied the requirement of the bwsas 2 2 data pre processing and analysis melbourne water has recorded soluble and total mn levels in three layers namely surface middle depth of 10 m and bottom depth of 20 m of the tarago reservoir through roughly weekly sampling tests for 10 years preceding this research the total mn concentration was considered as the sum of the soluble and particulate mn concentrations fig 2 presents the variation of the soluble and particulate mn in the surface middle and bottom layers of the tarago reservoir from july 2014 to october 2018 the figure displaying mn in the bottom layer adopted different limits of the y axis because the concentrations of mn in the bottom layer significantly exceed those in the surface layer there was a seasonal fluctuation of both particulate and soluble mn varying with time both concentrations rose dramatically from december and reached their peaks around march sharply decreasing to a minor level before may during the rest of the year the mn concentrations were constantly low the variations in soluble and total mn in the surface and middle layers were almost uniform and the mn concentrations of the bottom layer were approximately five times larger than in the surface and middle layers the soluble and particulate mn concentrations in the whole water column increased to a considerably high level between january and mid march without any exceptions in addition the mn peaks of the bottom layers significantly varied in different years and the peaks of soluble and particulate mn during 2016 were visibly greater than in other years although the mn of the bottom layer increased to a high level during the summer of 2016 the mn in the middle and surface layers did not exhibit such an increase compared to other years there was no clear linear correlation between mn concentrations in the bottom and upper layers zhang et al 2020 investigated the thermal structure of the tarago reservoir and revealed that the destratification occurred in the period from early january to the end of march when significant levels of soluble and particulate mn were observed the mixing process during this period dramatically facilitated the rise of both soluble and particulate mn concentrations in the entire water column the do concentration at the sediment water interface plays a prominent role in the mn cycle rodríguez martín et al 2003 the variations in the do at the bottom of the observation location displayed in fig 1 b and the measured soluble mn concentration were interpolated and displayed in fig 3 the impact of the do on the soluble mn in the bottom of the water column is evident the soluble mn revealed a trend of dramatic increase when the do level decreased to less than approximately 5 0 mg l the peak of the soluble mn in the bottom occurred before the do reached 5 0 mg l and then it sharply decreased after the do recovered to higher than 5 0 mg l as a result 5 0 mg l can be regarded as a threshold value of do that controls the release of soluble mn from the sediment johnson et al 1991 adopted the threshold value of 1 6 mg l to determine the diffusion of soluble mn from the bottom sediment in their modelling study and oldham et al 2017 indicated that the concentration of soluble mn2 rapidly increased when the do became less than 1 28 mg l gantzer et al 2009 indicated that elevating do at the bottom to 6 0 mg l was not effective in preventing the diffusion of soluble mn from the sediments which implicated that the thresholds of do were significantly influenced by individual differences in water bodies the threshold value of the do used in this model is significantly greater than the values in the previous two studies one reason is that the bottom layer of the field measurement is not the actual interface between the water and sediment and the do concentration continued decreasing from the bottom layer to the interface another reason is the individual differences in the study domains for which the differences in ph and orp i e oxidation reduction potential altered the influence of the do on the soluble mn diffusion 3 methods 3 1 development of the manganese cycle model the model of the mn cycle was developed based on the study by johnson et al 1991 in which they constructed a mathematical model of the mn cycle in a seasonally anoxic lake they considered the processes including mn oxide reduction mn2 oxidation particulate mn settling mnco3 precipitation and mn2 sorption on colloidal particles the influence of the latter three processes was determined to be negligible or unpredictable in accordance with the relevant researches müller and sigg 1990 weilenmann et al 1989 and emerson 1980 johnson et al 1991 simplified the cycle of mn as being affected by only the oxidation and reduction reactions between soluble and particulate states and the transport under the effect of hydrodynamic conditions a 3d hydrodynamic model of the tarago reservoir was developed by zhang et al 2020 using the mike 3 flow model the bathymetry map with georeferencing of the research domain was displayed in fig 4 and the key parameters used in the 3d hydrodynamic model were listed in table 3 the hydrodynamic model was validated according to measured water level and temperature and the model achieved good accuracy in simulating the thermal structure and current dynamics of the reservoir zhang et al 2020 the simulated results of the 3d hydrodynamic model provide 3d flow patterns of currents and eddy diffusion coefficients in x y and z directions which is fundamental to simulate the 3d mn cycle the development of the 3d mn cycle model integrated the mn cycle mathematical model published by johnson et al 1991 and the 3d hydrodynamic model built by zhang et al 2020 the mike 3 flow model used in the research is based on 3d incompressible reynolds averaged navier stokes equations invoking the assumption of boussinesq and hydrostatic pressure the mike 3 flow model developed by the danish hydraulic institute dhi was used in the simulation of the hydrodynamic and thermal structure in the tarago reservoir and exhibited good performance eco lab as a module in the mike 3 flow model is a generic and open tool for aquatic ecosystem models to combine the hydrodynamic model and several variables such as bacteria oxygen depletion and excess concentration of nutrients in this research we have developed a model template for the 3d mn cycle considering the chemical reaction between soluble mn and particulate mn advection dispersion transport and source and sink the developed mn module was then used in conjunction with the mike 3 flow model to produce the 3d simulation of do and mn in the study area coupling with the eddy diffusion coefficient and velocity field simulated by the developed 3d hydrodynamic model the eddy diffusion term for soluble mn and particulate mn was updated and the advection terms of both mn were introduced the cycle of soluble mn and particulate mn are represented through equations 3 1 and 3 2 respectively equations 3 3 3 4 and 3 5 represent the soluble mn flux from sediments outflow rate of mn from the waterbody and oxidation rate of soluble mn to particulate mn respectively a 3d mn cycle model was established according to equation 3 1 to 3 5 3 1 c 1 t k x 2 c 1 x 2 k y 2 c 1 y 2 k z 2 c 1 z 2 u c 1 x v c 1 y w c 1 z f q c 1 k o x c 1 3 2 c 2 t k x 2 c 2 x 2 k y 2 c 2 y 2 k z 2 c 2 z 2 u c 2 x v c 2 y w c 2 z k o x c 1 q c 2 v s k c o c 2 z 3 3 f f o 2 o 2 f a n d z 0 0 o 2 o 2 f 3 4 q q o u t f l o w b o u d a r y 0 o t h e r z o n e 3 5 k o x k o x o 2 o 2 o x 0 o 2 o 2 o x where u v w are velocity components c 1 is the concentration of soluble mn2 mg l c 2 is the concentration of particulate mn oxides mg l k x k y and k z are the eddy diffusion coefficients in x y and z directions cm2s 1 z is the depth of lakes m q is the hydraulic rate d 1 o 2 is the concentration of do mg l v s is the settling velocity of coagulated mn oxides m s o 2 f is the threshold oxygen concentration for mn2 flux from sediments mg l k o x is the oxidation rate of mn2 to mn oxide d 1 o 2 o x is the threshold oxygen concentration for the oxidation of mn2 mg l k c o is the coagulation rate of particulate mn oxide d 1 f is the flux of mn2 from sediments mg m 2d 1 the discretization techniques used for solving equations 3 1 and 3 2 are finite difference techniques the horizontal grids are 40 m 40 m vertical grids are 0 5 m and time steps are 20 s for both the hydrodynamic model and the mn cycle model the development of the 3d mn transport model cannot avoid assumptions which may causes some deviation from the measured results first the dynamic variables of this model followed the assumption by johnson et al 1991 that soluble mn is made of divalent mn ions while particulate mn consists of trivalent and tetravalent mn oxides second the reduction and diffusion processes of mn oxides attaching to the bottom sediments were assumed to be the only source of mn in the reservoir system and the outflow of the dam was defined as the only sink of mn the equations ignored the variation in mn mass due to inflow discharge precipitation and evaporation in the simulated system in addition the oxidation of soluble mn is a complex process that includes both abiotic and microbial reactions under the influence of catalysis to enable the simplified calculations a constant k o x was assumed to represent the rate at which soluble mn was oxidised into particulate mn oxides regarding the coagulation and settling of particulate mn oxides johnson et al 1991 assumed that the rates of coagulation and particle settling are uniform and can be represented by a constant because the settling process was regarded as transient neretin et al 2003 estimated the settling velocity of the particulate mn following three assumptions the particles were approximated as spheres the density of birnessite 3 0 g cm 3 as the assumption of the density of mn particles was used in the calculation and the spherical diameter of settling mn particles was 4 μm the result of their calculation indicated that the settling velocity is 0 98 m day the settling period in the epilimnion of the lacustrine zones deeper than 20 m is generally more than two weeks therefore the settling process of the aggregates of particulate mn is vital for the simulation of the mn cycle the introduction of the settling process positively affects the 3d mn transport model equations 3 3 and 3 5 indicate that the variation and distribution of do concentrations in the tarago reservoir are vital factors in the simulation of mn transport the field data of do was defined as a static variable that controls the quantity of mn flux from sediments and the oxidation rate of reduced mn a mathematical formulation for the oxygen balance is given by equation 3 6 dhi 2017 3 6 c d o t v r e a v b o d d v p h v r e s v s o d where c d o is the concentration of do mg l v r e a is the rate of oxygen production by reaeration mg l day v b o d d is the rate of decay of biological oxygen demand bod mg l day v p h is the rate of producing oxygen by photosynthesis mg l day v r e s is the rate of respiration oxygen demand mg l day and v s o d is the rate of sediment oxygen demand mg l day the boundary conditions of mn flux from inflow were ignored in this study due to the absence of relevant data while mn flux diffusing from sediments was considered the major source the determination of sediment mn flux was a challenge due to the absence of relevant measurements however the literature about the flux of mn across the sediment water interface has revealed that the general range of the flux is from 3 85 to 244 75 mg m2 day pakhomova et al 2007 trefry and presley 1982 warnken et al 2001 the investigation by johnson et al 1991 indicated that the sediment mn flux of a seasonally anoxic lake is between 11 and 55 mg m2 day the measured soluble mn in the bottom layer was used to calculate the rate of soluble mn flux from sediments and the relationships are given by 3 7 q s m n c s m n a g r i d t where q s m n is the rate of soluble mn flux from sediments mg l m2 c s m n is the field measured soluble mn concentration mg l and a g r i d is the area of the grid m2 3 2 statistical analysis the error analysis of the measured and simulated data relies on the mean absolute error mae and root mean square error rmse in addition through linear regression analysis between the observed and simulated values the coefficients of determination r 2 were determined to examine the performance of the 3d mn cycle model the index of agreement ia was also used to evaluate the performance of the model since it performs better than r 2 in overcoming the inflation of errors due to their squared values willmott 1984 both values of r 2 and ia vary from 0 to 1 with higher values indicating a better performance of the model the introduction of lake specific indices facilitates the investigation of the contribution of hydrodynamic drivers such as wind read et al 2011 the probability of upwelling events under stratified conditions was defined as wedderburn number wn by thompson 1980 the index has been used to describe the entrainment of hypolimnetic water into the surface layer under the influence of wind induced upwelling monismith 1986 when wn is much less than 1 the wind stress is much more than the buoyancy force and upwelling is probably occurring at the upwind end of the waterbody otherwise the buoyancy force and wind stress are nearly equal and horizontal mixing is considered important when wn is near or more than 1 0 the wedderburn number is given by 3 8 w n g z e 2 u 2 l s where g g ρ h ρ s ρ h is the reduced gravity due to the difference in density between the hypolimnion and epilimnion z e is the thickness of the mixed layer l s is the lake fetch length and u is the water friction velocity due to wind stress 3 3 model calibration the development of the 3d mn transport model relied on the hydrodynamic model that was calibrated and validated by zhang et al 2020 the setup of the mike 3 flow model for mn transport simulation was based on the field data measured from august 31 2017 to august 31 2018 the simulation period includes a complete mn cycle some conditions must be assumed based on the literature review due to the absence of the data the boundary conditions of bod do soluble mn and particulate mn were imposed at the inlet and outlet of the reservoir the initial conditions of bod do smn soluble mn and pmn particulate mn of the entire reservoir were set according to the measured data at the observation point due to the absence of the measurement of bod the empirical value 1 0 mg l of the bod of eutrophic lakes was adopted as the boundary condition and initial condition of bod stefan and fang 1994 since there is no relevant measurement at the inlet the average measured do concentration at the surface water 9 14 mg l was used as the boundary condition of the inlet the analysis of historical mn records in section 2 2 indicated that both particulate and soluble mn maintained a low and uniform level before the do level at the bottom layer decreased to less than 5 0 mg l this indicated that the release of soluble mn from sediments plays a dominant role and the impact of the mn flux of inflow is limited during the mn peak without the mn source from sediments the measured particulate and soluble mn in the epilimnion were consistently below 0 02 mg l and 0 003 mg l respectively which were assumed as the inflow boundary conditions of mn concentration in addition the optimized values for the relevant parameters of the mn model were calibrated through the field measurement of do and mn and all the parameters are summarised in table 4 4 results and discussion 4 1 performance of the 3d mn transport model table 5 displays the results of the error analysis and regression analysis of the simulated do soluble mn and particulate mn at the surface middle and bottom layers the seasonal change of the do exhibits the characteristics of monomictic lakes with the hypolimnion experiencing annual do depletion during stratification helfer et al 2011 townsend 1999 fig 5 presents the measured and simulated do concentrations at the surface middle and bottom layers of the observation location fig 1 b from september 2017 to june 2018 the model accurately simulated the time when the do concentration decreased to less than 5 0 mg l the threshold value for soluble mn diffusion the measured record indicated that the bottom layer do experienced an abnormal fluctuation around march 2018 but the trend of simulated results exhibits a smooth increase nevertheless such a specific error in the do simulation has a limited effect on the mn modelling outputs because most concentrations during this period were below the threshold and the simulation is accurate when the bottom layer do has increased to more than 5 0 mg l the figure and statistical analysis highlight that the simulation matches the trend and dynamics of the measured data while the simulated results present a stronger fluctuation and are slightly higher than the measured data although the simulation of do is not perfect the simulated results are reliable in modelling the release of mn from the sediments figs 6 and 7 present the simulated and measured soluble and particulate mn at the observation location from september 2017 to august 2018 and table 5 displays the error analysis and regression analysis the statistical results indicate that the magnitudes of the simulated and measured mn are close and that the variation of mn in the entire water column was accurately simulated the sediments of the tarago reservoir started to release soluble mn in early december with the mn levels in the water column exhibiting considerable increases both soluble and particulate mn reached their peaks in the first week of march after the peak the mn in the column smoothly decreased due to sedimentation and outflow the soluble and particulate mn decreased to 0 02 mg l on april 5 2018 and may 23 2018 the oscillations of the simulated soluble and particulate mn in january and february 2018 cannot be validated due to the absence of high frequency field measurements the oscillations of the mn in the middle layer 10 m depth are due to their roughly daily variation the simulated isotherms adjacent to the middle layer from 24th to january 31 2018 were displayed in fig 8 the isotherms indicate that the simulated mn results from the internal wave that acts to produce a motion with a daily or smaller temporal scale the simulated soluble and particulate mn of the so called bottom layer shown in figs 6 and 7 does in fact apply to a depth 0 5 m higher than the actual bottom layer due to the fact that the simulation adopted a 0 5 m vertical resolution therefore the validation of the bottom layer mn is unachievable and the comparison indicated that both soluble and particulate mn decreased significantly from the water sediment interface to the depth of 19 5 m although the simulated results did not perfectly match the measured data the graphical comparisons indicate that the model reproduced the variation trend of both soluble and particulate mn and the statistical analysis of the performance validates these claims 4 2 three dimensional distribution of soluble mn the development and validation of the 3d mn transport model provide a feasible way to visualise the distribution and variation of mn in the tarago reservoir the principal factors determining the distribution of mn are hydrodynamic transport and chemical or biological reactions based on the assumption that the unique source of soluble mn is the soluble mn flux diffusing from the sediment which varies with time but is invariant with space and has a constant reaction rate there are some limitations of the simulation of 3d mn transport based on these assumptions the rate of mn diffusing into the water column in the shallow zones would be the same in the deep zones notably because pmn is easily removed by routine treatment the level of soluble mn is the key factor affecting the quality of treated water therefore the following analyses concentrate on the distribution of the soluble mn the epilimnion of the lacustrine zone eol zone near the dam serves as a water supply for the water treatment plant the period with the potential risk of an mn issue i e smn 0 02 mg l was from january 24 2018 to march 24 2018 fig 9 represents at different times the full process of the accumulation upward transportation and decrease of smn in each sub figure the four slice plots represent the smn levels at depths of 0 m 5 5 m 11 m and 16 5 m noticeable smn diffusion from the bottom sediments occurred on january 24 2018 fig 9a and the smn in the bottom of the transition lacustrine zones rose to above 0 1 mg l however the smn between the surface and 5 5 m remained negligible less than 0 02 mg l proving that the upward transport of smn was limited during this period from january 24 2018 to february 27 2018 the upward transport of smn was greatly enhanced and the smn level at the epilimnion layer reached its peak approximately 0 1 mg l for the lacustrine zone smn at the surface layer was more than at the 5 5 m and 11 m layers during the smn peak fig 9b which is an important observation in relation to water treatment and specifically raw water intake depth the cross section of the smn and oxycline at the y axis 4 depicted in fig 9 a next to the dam wall were displayed in fig 10 and it indicated that lower smn concentrations in upper layers were common during stratification while do at the hypolimnion is low however the common vertical distribution of smn changed on february 27 2018 with the homogeneous vertical distribution of do for the top 30 m of the water column while surface layer smn concentrations exceeded the middle layer smn levels such different distributions indicated that the transport of the smn at this time was not dominated by upward vertical transport but was affected by horizontal transport then as shown in fig 9 d the smn concentration of the entire reservoir gradually decreased to a low risk level during the subsequent 4 weeks in addition the simulated results can output the vertical temporal evolution of soluble mn at the location of water intake dam location in fig 1 b as fig 11 shows for the 2017 event the vertical distribution of smn was relatively similar offering limited management opportunities in terms of intake depth selection fig 12 on the other hand shows the vertical distribution of soluble mn from february 18 2018 to march 11 2018 in this case the vertical profile of smn was often not constant with times of higher concentrations in the deeper layers but with times of higher levels at the surface too this shows the relevance of our proposed 3d model for water utilities a better understanding of 3d mn variations under different conditions can provide more accurate information and predictions on expected smn levels at different crucial locations in the water column allowing managers to apply for the most appropriate risk reduction measures 4 3 the role of hydrodynamic processes in manganese cycle 4 3 1 wind driven currents as one of the most important atmospheric forces the wind is a main source of energy in reservoirs the role of the wind is to exert a drag on the surface current and transport floating pollutants if the wind fetch and wind duration are sufficient wind stress will dominate the movement of surface currents and influence the circulation pattern ji 2017 juntunen et al 2019 wu et al 2018 this study investigated the movement of the surface current under the influence of the wind on 25th and february 27 2018 when the mn concentrations were in a significant and increasing level as the wind rose plots in figs 13 and 14 show the dominant wind is south wind and northeast wind respectively on february 25 2018 and february 17 2018 the simulated flow patterns figs 13 and 14 of the reservoir on 25th and 27th february were inverse under the influence of the different wind forces hence the comparisons of flow pattern between these two days are vital to reveal the roles of wind driven current in the transport of soluble mn in addition the analysis at the mn peak is significant because this period is the most challenging for the treatment of mn in the twtp figs 13 and 14 detail the simulated surface current velocity and smn concentration at the surface of the reservoir and the wind rose plots in the 12 h the tarago reservoir has a maximum fetch when the wind blows from north to south because of its morphology and both figures indicate that the wind plays a dominant role in driving the north south movement of the surface current under the influence of a strong south wind the surface current flowed from the dam to the inflow river upstream the smn presents a decreasing trend from the inflow boundary to the dam when northeast was the dominant wind direction on 27th february the movements of the surface current and mn distribution were in the opposite direction i e downstream as mentioned the smn flux diffusing from the sediment was assumed to vary with time but be invariant with space the smn concentration at the surface of the shallow area was greater than at the surface of the deep area therefore the upstream surface current from the lacustrine zone brought the water with a low mn level to the surface of the transition zone which caused the mn distribution pattern depicted in fig 13 and the opposite in fig 14 the significant movement of the surface current impacted the circulation which affects the mn transport in the reservoir meanwhile the thermal structure of the water column affects the vertical magnitude of circulation due to the importance of the eol zone in water management the analysis of the influence of wind stress on mn transport focused on the eol zone and the mn peaks that occurred on 25th and february 27 2018 were chosen as the research periods figs 16 and 17 present the current velocity smn distribution and isotherms of the cross section x1 x2 see figs 13 and 14 fig 15 displays the time series of the vertical temperature distribution in the vps location see fig 1 the wind speed and direction and the wedderburn number of the cross section x1 x2 from 24th to february 28 2018 and the values of wn at 00 00 and 12 00 on 25th and 27th february are marked in the figure the wn at 00 00 25th february is the only one higher than 1 0 which is much higher than the indexes at other three moments as fig 15 shows the thermal structure is stable and the wind is weak during 00 00 25th february hence the generation of upwelling was suppressed comparing the soluble mn patterns in figs 16 and 17 the horizontal distribution of soluble mn was uniform only at 00 00 on 25th february during the other three moments the wn less than 1 0 indicates that the wind stress was greater than the buoyancy force due to the change of density and upwelling likely occurring at the upwind end of the reservoir while the surface current presents a uniform flow direction from the dam wall to the transition zone in fig 16 the current from the bottom of the lacustrine zone flowed to the eol zone near the dam and significantly increased its mn level meanwhile a discernible circulation occurred in the hypolimnion of the lacustrine zone the circulation enhanced transport from the bottom water with a high level of smn to the eol zone which produced potential mn issues in the water treatment as fig 17 depicts a large surface current uniformly flowed from the transition zone to the dam wall driven by the wind on february 27 2018 and was blocked by the dam circulation occurred in the water near the dam and created a downward current that significantly restricted the upward transport of smn in this case the mn levels in the eol zone are more susceptible to the surface water from the transition zone rather than the bottom of the lacustrine zone the bottom current with a high level of mn flows to the side of the lacustrine zone away from the dam because the water in the eol zone was replaced by the surface layer of transition and the upward transport of mn was suppressed the mn level of the eol zone remained relatively low overall the influences of the surface current driven by north and south wind are opposite for the mn transport in the eol zone wind from the longitudinal direction causes a proportionally higher mn concentration in the surface the surface current flowing to the dam diluted the concentration of mn and the reverse surface current greatly increased the mn level of the water near the dam wall because the twtp collects raw water from the eol zone near the dam the current dynamic under the influence of the predominant south wind significantly elevated the risk of high mn in the raw water otherwise the surface current flow with a low level of smn moves to the dam wall which decreases the smn level of the raw water transported to the wtp the 1d vertical transport model is insufficient to reproduce the potential risk of the mn issue due to the inability to simulate such horizontal transport of mn therefore the 3d movement of the current driven by wind in the lacustrine zone plays an important role in the reservoir with a significant longitudinal morphological difference 4 3 2 thermal structure as a temperate monomictic reservoir the tarago reservoir presented seasonal changes in the thermal structure a clear thermal stratification was observed in the reservoir during the summer from december to february of 2018 the thermal stratification occurring in the reservoir caused a depletion of the do in the hypolimnion and the subsequent release of soluble mn from the sediment however the turnover event can effectively replenish the do in the bottom layers and vertically mix the smn antonopoulos and gianniou 2003 gantzer et al 2009 townsend 1999 the formation of a thermocline during stratification can reduce the vertical mixing and turbulence and restrict mn transport from the hypolimnion to the epilimnion the turnover replenishes the do for the hypolimnion while it also boosts the upward transportation of mn boehrer and schultze 2008 elçi 2008 kirillin and shatwell 2016 fig 18 presents the lacustrine zones with different smn distributions when the reservoir was in variable thermal structures the smn diffusion was limited on january 7 2018 when significant thermal gradients occurred fig 18 a and c the summer stratification ensured that the smn concentration above 6 0 m depth was less than 0 02 mg l the vertical distribution of smn changed dramatically in the lacustrine zone after the stratification ended as fig 18 d depicts the body of water tended towards a uniform water temperature on march 3 2018 although the smn from the surface layer to 15 m presented a relatively high uniform concentration the majority of the smn resided in the bottom layers and the amount of smn diffusing into the epilimnion remained minor in the next 4 weeks the replenishment of the do due to the mixing process cut the smn source and the overall smn decreased to a minimal level from the point of view of the thermal structure the rising of smn in the eol zone was contained by significant stratification and promoted during the weakening of the stratification and the beginning of the mixing process the levels of the smn of the epilimnion were kept below the critical threshold of 0 02 mg l during summer stratification the development of destratification is the discernible signal for the wtp to raise the alert level for high levels of mn in raw water in addition although the destratification significantly increased the rate of smn s upward transport fig 18 b and d indicate that the vertical gradient of mn persisted even when the lake appeared to be fully mixed 5 conclusions a combined 3d hydrodynamic model and mn cycle model was successfully applied to the tarago reservoir this has the capability of simulating the hydrodynamic components and the 3d distribution of mn in such a temperate monomictic reservoir the development of the 3d model was based on the 1d mn cycle model developed by johnson et al 1991 while there is room to improve the constants and assumptions used in the development of the model the validation indicates that the model accurately reproduced the mn distribution in the reservoir to the authors knowledge a 3d modelling and simulation of the mn cycle in lacustrine systems was not attempted before and this study filled the gap using the simulated results of the model it was revealed that the horizontal transport of mn significantly affects the mn distribution in the lacustrine zone that supplies the raw water for the water treatment plant in addition the 3d mn model enabled the analysis of the impact of wind driven currents and thermal structure on the mn cycle results showed that wind driven currents toward the dam wall reduce the mn concentration near the dam wall while reverse currents greatly increase the mn level summer stratification ensures that minimal soluble mn is transported into the wtp from the reservoir s epilimnion while destratification significantly raises the levels of mn in the epilimnion due to upwards movements of mn rich hypolimnetic waters the model was also able to explain unexpected vertical mn variations by considering horizontal transport which would have been unachievable with existing 1d mn models the understanding of the 3d spatial temporal relationships between mn cycle and wind speed direction and thermal structure provides new and useful information for more targeted management strategies for water treatment future work will focus on integrating the 3d mn cycle model with data knowledge of mn removal processes at the water treatment plant downstream in order to predict and reduce the potential mn related water quality risks based not only on the meteorological reservoirs conditions but also on the treatment plant operations conditions zhang et al 2021 such source to treated water optimisation approach will provide a fully comprehensive integrated mn related risk assessment and reduction modelling tool for the water treatment operators credit authorship contribution statement fuxin zhang conceptualization methodology software validation formal analysis writing original draft hong zhang conceptualization methodology writing review editing edoardo bertone conceptualization validation writing review editing rodney stewart conceptualization writing review editing xia shen methodology kathy cinque data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge dhi water and environment denmark for their assistance in providing mike modelling system for this study this research work was conducted with the technical support of melbourne water and the griffith university publication assistance scholarship we acknowledge the use of meteorological data and water temperature from the vertical profile system of melbourne water and australian bureau of meteorology appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105213 
25702,a comprehensive spatial and temporal understanding of the manganese cycle in drinking water reservoirs has beneficial implications for the management of water treatment related issues in this study a three dimensional manganese cycle model was developed and coupled with a hydrodynamic model for the simulation of the distribution of manganese in a monomictic water reservoir the model was successfully validated and used to model and predict the spatial temporal manganese variation in the reservoir results showed that the variations of the thermal structure of the reservoir affect the seasonal change of manganese levels and in turn the peak concentrations of manganese in the epilimnion it was also evident that the high level of dissolved oxygen in the epilimnion promotes the oxidation of soluble manganese and the oxidised particulate mn settles rapidly the advection processes mainly caused by wind driven currents play a vital role in rising or decreasing the level of soluble manganese near the dam wall the developed numerical tool allows the user to gain a better understanding of spatial manganese distribution in different locations and depths of the reservoir under different wind rain conditions among others such knowledge empowers the local water utility and enables more targeted optimized water treatment strategies keywords hydrodynamic modelling manganese drinking water treatment environmental modelling 1 introduction the world health organisation guidelines for manganese mn in drinking water indicated that mn concentrations of over 0 4 mg l can be toxic and over 0 1 mg l causes dirty water who 2004 the australian health based guideline value for mn is 0 5 mg l and the australian drinking water guidelines recommend an aesthetic guideline of 0 1 mg l to avoid taste and staining issues with soluble mn 0 05 mg l desirable nhmrc 2021 although mn concentrations in australian drinking water supply systems have usually been below the health based value the aesthetic thresholds can be exceeded in addition even lower mn concentrations can cause issues for piped water supply systems such as insoluble mn deposition and growth of bacterial deposits which causes a reduction in flow rate the mn issues in drinking water are generally caused by the increasing mn concentration in the water supply reservoir and the ineffectiveness of the treatment of mn is largely attributed to drastic changes of the mn level in the source of water bertone et al 2016a 2016b therefore understanding the mn cycle in the aquatic environment to predict critical events is the primary task to ensure effective treatment of mn mn originates from the sediments within the catchment areas of reservoirs and divalent mn mn2 can be solubilised at the interface between sediment and water ljung and vahter 2007 the character of the water sediment interface determines the diffusion of reduced mn from the sediments which is the primary source of mn in lakes and reservoirs rodríguez martín et al 2003 do is not disturbed by the mn cycle but influences whether the reduced mn is diffused from the sediments advection and diffusion are governed by the hydrodynamics of aquatic systems and affect the transport capacity of mn from the hypolimnion to the epilimnion the biological and abiotic oxidation of reduced soluble mn and the settling rate of mn oxides together determine the loss of mn in bodies of water therefore the modelling of mn transport in a dynamic system is feasible if hydrodynamic conditions and the chemical and biological reactions can be accurately simulated and integrated the mn cycle occurring in reservoirs as the source of drinking water is highly susceptible to the hydrodynamic process and it is regulated by physical chemical and microbiological processes bertone et al 2014 davison and woof 1984 hem 1963 zaw and chiswell 1999 undertook a correlation study between many water quality parameters temperature true colour turbidity do ph conductivity and alkalinity and the concentrations of the various forms of mn in the hinze dam queensland australia using a regression analysis their statistical analysis indicates that no independent factor dominates the mn variation and the relationship between mn and the water quality parameters is susceptible to the change of depth and season a temperate monomictic reservoir experiences an annual cycle of stratification and mixing conditions the stable stratification limits oxygen exchange between the epilimnion and hypolimnion which causes anoxia in the hypolimnion bertone et al 2015b elçi 2008 wang et al 2019 mn tends to remain in an insoluble reduction state in an anoxic aquatic environment gantzer et al 2009 the concentration of dissolved oxygen do also determines whether mn2 is diffused from the sediment at the bottom of the reservoir johnson et al 1991 a large amount of soluble mn exists as mn2 which gathers in the hypolimnion during stratification and anoxia oldham et al 2017 the inlet of water collection is usually installed in the epilimnion of reservoirs and the mn level in the epilimnion is minimal during stratification without suffering extreme external effects hence the water treatment plants wtps do not require thoughtful arrangements of mn removal during these periods however when destratification occurs hypolimnetic water with a high level of soluble mn is transported to the epilimnion since the oxidation of soluble mn in natural water with a ph of 6 0 8 0 is slow high levels of soluble mn persist in the epilimnion landing and bruland 1980 causing challenges for wtps to the author s knowledge only a few studies have modelled the variation and distribution of mn in aquatic environments in the last half century bertone et al 2015a egeberg et al 1988 johnson et al 1991 wang and van cappellen 1996 zaw and chiswell 1999 one of the few studies about lakes was a one dimensional 1d mathematical model of the mn cycle conducted by johnson et al 1991 this model included mn2 flux from the sediment mn2 oxidation particulate mn sediment and turbulent mixing rates calculated by the temperature method imboden et al 1979 the results emphasised the roles of do in the mn cycle and the seasonal change of mn2 flux from the bottom sediments the study indicated that it is feasible to represent the mn transport of a lacustrine system with a time dependent mathematical model the study by johnson et al 1991 indicated the feasibility of representing the mn cycle in a seasonally anoxic lake with a time dependent model however some limitations were evident in the modelling study the turbulent mixing rates under the assumption of ignoring advective heat transport and geothermal heat flow can often be unrealistic and the performance of the 1d vertical model on shallow areas would be unsatisfying more recently the number of new studies that focused on mn prediction is still restricted with either a narrower scope such as the water sediment continuum giles et al 2016 or to specific processes such as seasonal vertical zonation of redox chemistry hartland et al 2015 the most comprehensive modelling of mn in a drinking water reservoir is provided by bertone et al 2015a who developed a data driven model to predict mn concentrations in the advancetown lake one week in advance a long term historical database collected by a vertical profiling system vps provided an opportunity to apply machine learning to model mn in such sub tropical monomictic reservoir the model accurately simulated the probability of soluble mn exceeding critical thresholds which had a significant practical meaning for managing mn removal the general limitation of the data driven models such as this one is that their reliability depends on the quantity and quality of monitoring data wang et al 2020 in addition like 1d vertical mathematical models data driven models are so far unable to simulate the 3d spatial distribution of mn due to the lack of enough data for most water supply reservoirs characterised by significant morphological differences the role of the spatial distribution of mn is particularly important for the mn cycle the previous studies hardly discussed the effect of horizontal current movement on the mn cycle and neither 1d vertical process based nor data driven models can accurately simulate the 3d mn cycle in most reservoirs despite such limitation of 1d models a modelling study of the mn cycle in three dimensions 3d so far is missing coupling hydrodynamic and chemical models have been applied to a wide range of situations from groundwater to surface water garneau et al 2017 hipsey et al 2014 wang et al 2021 and offers opportunities to be applied within the scope of this work too the aim of this research is therefore to integrate a 3d hydrodynamic and a mn cycle model to enable an accurate simulation of the 3d mn cycle in an australian temperate drinking water reservoir 2 study site and manganese issues 2 1 study site the tarago reservoir 38 1 s 145 56 e fig 1 a is located approximately 85 km east of melbourne victoria australia and the maximum capacity of the reservoir is 3 758 107 m3 the dam was built in 1969 and enlarged in 1971 the surface area of the reservoir is approximately 3 6 106 m2 and its catchment area is 1 14 108 m2 cinque and jayasuriya 2010 the maximum depth of the reservoir is about 22 0 m near the dam wall continuous water quality monitoring and sample testing were conducted at the observation location near the dam see fig 1 b weekly mn sample testing for water at the surface depth of 0 m middle depth of 10 m and bottom depth of 20 m layers have been conducted since 2014 a network of remote water quality monitoring instruments and vps was deployed in the observation location in 2017 the vps recorded the water temperature and do with a 3 h frequency and 1 m vertical resolution for the entire water column the source and frequency of meteorological data boundary conditions mn records and do data have been summarised in table 1 the reservoir supplies drinking water to the westernport and the mornington peninsula which include approximately 165 000 residents via the tarago westernport pipeline the distance of this water distribution is more than 80 km which compels the tarago water treatment plant twtp to maintain a strict criterion for mn removal to avoid the corrosion of the pipeline and subsequent water pollution griffin 1960 the treated water must meets the water quality requirements of the bulk water service agreements bwsas which require that 100 of the treated water contain less than 0 05 mg l total mn and that 95 of the treated water contain less than 0 02 mg l total mn young 2009 through the statistical analysis of all water sampling tests of the mn level in treated water the percentages of the testing sample with more than 0 01 mg l 0 02 mg l and 0 05 mg l mn and the sample sizes are summarised in table 2 the table indicates that only 2018 2019 satisfied the requirement of the bwsas 2 2 data pre processing and analysis melbourne water has recorded soluble and total mn levels in three layers namely surface middle depth of 10 m and bottom depth of 20 m of the tarago reservoir through roughly weekly sampling tests for 10 years preceding this research the total mn concentration was considered as the sum of the soluble and particulate mn concentrations fig 2 presents the variation of the soluble and particulate mn in the surface middle and bottom layers of the tarago reservoir from july 2014 to october 2018 the figure displaying mn in the bottom layer adopted different limits of the y axis because the concentrations of mn in the bottom layer significantly exceed those in the surface layer there was a seasonal fluctuation of both particulate and soluble mn varying with time both concentrations rose dramatically from december and reached their peaks around march sharply decreasing to a minor level before may during the rest of the year the mn concentrations were constantly low the variations in soluble and total mn in the surface and middle layers were almost uniform and the mn concentrations of the bottom layer were approximately five times larger than in the surface and middle layers the soluble and particulate mn concentrations in the whole water column increased to a considerably high level between january and mid march without any exceptions in addition the mn peaks of the bottom layers significantly varied in different years and the peaks of soluble and particulate mn during 2016 were visibly greater than in other years although the mn of the bottom layer increased to a high level during the summer of 2016 the mn in the middle and surface layers did not exhibit such an increase compared to other years there was no clear linear correlation between mn concentrations in the bottom and upper layers zhang et al 2020 investigated the thermal structure of the tarago reservoir and revealed that the destratification occurred in the period from early january to the end of march when significant levels of soluble and particulate mn were observed the mixing process during this period dramatically facilitated the rise of both soluble and particulate mn concentrations in the entire water column the do concentration at the sediment water interface plays a prominent role in the mn cycle rodríguez martín et al 2003 the variations in the do at the bottom of the observation location displayed in fig 1 b and the measured soluble mn concentration were interpolated and displayed in fig 3 the impact of the do on the soluble mn in the bottom of the water column is evident the soluble mn revealed a trend of dramatic increase when the do level decreased to less than approximately 5 0 mg l the peak of the soluble mn in the bottom occurred before the do reached 5 0 mg l and then it sharply decreased after the do recovered to higher than 5 0 mg l as a result 5 0 mg l can be regarded as a threshold value of do that controls the release of soluble mn from the sediment johnson et al 1991 adopted the threshold value of 1 6 mg l to determine the diffusion of soluble mn from the bottom sediment in their modelling study and oldham et al 2017 indicated that the concentration of soluble mn2 rapidly increased when the do became less than 1 28 mg l gantzer et al 2009 indicated that elevating do at the bottom to 6 0 mg l was not effective in preventing the diffusion of soluble mn from the sediments which implicated that the thresholds of do were significantly influenced by individual differences in water bodies the threshold value of the do used in this model is significantly greater than the values in the previous two studies one reason is that the bottom layer of the field measurement is not the actual interface between the water and sediment and the do concentration continued decreasing from the bottom layer to the interface another reason is the individual differences in the study domains for which the differences in ph and orp i e oxidation reduction potential altered the influence of the do on the soluble mn diffusion 3 methods 3 1 development of the manganese cycle model the model of the mn cycle was developed based on the study by johnson et al 1991 in which they constructed a mathematical model of the mn cycle in a seasonally anoxic lake they considered the processes including mn oxide reduction mn2 oxidation particulate mn settling mnco3 precipitation and mn2 sorption on colloidal particles the influence of the latter three processes was determined to be negligible or unpredictable in accordance with the relevant researches müller and sigg 1990 weilenmann et al 1989 and emerson 1980 johnson et al 1991 simplified the cycle of mn as being affected by only the oxidation and reduction reactions between soluble and particulate states and the transport under the effect of hydrodynamic conditions a 3d hydrodynamic model of the tarago reservoir was developed by zhang et al 2020 using the mike 3 flow model the bathymetry map with georeferencing of the research domain was displayed in fig 4 and the key parameters used in the 3d hydrodynamic model were listed in table 3 the hydrodynamic model was validated according to measured water level and temperature and the model achieved good accuracy in simulating the thermal structure and current dynamics of the reservoir zhang et al 2020 the simulated results of the 3d hydrodynamic model provide 3d flow patterns of currents and eddy diffusion coefficients in x y and z directions which is fundamental to simulate the 3d mn cycle the development of the 3d mn cycle model integrated the mn cycle mathematical model published by johnson et al 1991 and the 3d hydrodynamic model built by zhang et al 2020 the mike 3 flow model used in the research is based on 3d incompressible reynolds averaged navier stokes equations invoking the assumption of boussinesq and hydrostatic pressure the mike 3 flow model developed by the danish hydraulic institute dhi was used in the simulation of the hydrodynamic and thermal structure in the tarago reservoir and exhibited good performance eco lab as a module in the mike 3 flow model is a generic and open tool for aquatic ecosystem models to combine the hydrodynamic model and several variables such as bacteria oxygen depletion and excess concentration of nutrients in this research we have developed a model template for the 3d mn cycle considering the chemical reaction between soluble mn and particulate mn advection dispersion transport and source and sink the developed mn module was then used in conjunction with the mike 3 flow model to produce the 3d simulation of do and mn in the study area coupling with the eddy diffusion coefficient and velocity field simulated by the developed 3d hydrodynamic model the eddy diffusion term for soluble mn and particulate mn was updated and the advection terms of both mn were introduced the cycle of soluble mn and particulate mn are represented through equations 3 1 and 3 2 respectively equations 3 3 3 4 and 3 5 represent the soluble mn flux from sediments outflow rate of mn from the waterbody and oxidation rate of soluble mn to particulate mn respectively a 3d mn cycle model was established according to equation 3 1 to 3 5 3 1 c 1 t k x 2 c 1 x 2 k y 2 c 1 y 2 k z 2 c 1 z 2 u c 1 x v c 1 y w c 1 z f q c 1 k o x c 1 3 2 c 2 t k x 2 c 2 x 2 k y 2 c 2 y 2 k z 2 c 2 z 2 u c 2 x v c 2 y w c 2 z k o x c 1 q c 2 v s k c o c 2 z 3 3 f f o 2 o 2 f a n d z 0 0 o 2 o 2 f 3 4 q q o u t f l o w b o u d a r y 0 o t h e r z o n e 3 5 k o x k o x o 2 o 2 o x 0 o 2 o 2 o x where u v w are velocity components c 1 is the concentration of soluble mn2 mg l c 2 is the concentration of particulate mn oxides mg l k x k y and k z are the eddy diffusion coefficients in x y and z directions cm2s 1 z is the depth of lakes m q is the hydraulic rate d 1 o 2 is the concentration of do mg l v s is the settling velocity of coagulated mn oxides m s o 2 f is the threshold oxygen concentration for mn2 flux from sediments mg l k o x is the oxidation rate of mn2 to mn oxide d 1 o 2 o x is the threshold oxygen concentration for the oxidation of mn2 mg l k c o is the coagulation rate of particulate mn oxide d 1 f is the flux of mn2 from sediments mg m 2d 1 the discretization techniques used for solving equations 3 1 and 3 2 are finite difference techniques the horizontal grids are 40 m 40 m vertical grids are 0 5 m and time steps are 20 s for both the hydrodynamic model and the mn cycle model the development of the 3d mn transport model cannot avoid assumptions which may causes some deviation from the measured results first the dynamic variables of this model followed the assumption by johnson et al 1991 that soluble mn is made of divalent mn ions while particulate mn consists of trivalent and tetravalent mn oxides second the reduction and diffusion processes of mn oxides attaching to the bottom sediments were assumed to be the only source of mn in the reservoir system and the outflow of the dam was defined as the only sink of mn the equations ignored the variation in mn mass due to inflow discharge precipitation and evaporation in the simulated system in addition the oxidation of soluble mn is a complex process that includes both abiotic and microbial reactions under the influence of catalysis to enable the simplified calculations a constant k o x was assumed to represent the rate at which soluble mn was oxidised into particulate mn oxides regarding the coagulation and settling of particulate mn oxides johnson et al 1991 assumed that the rates of coagulation and particle settling are uniform and can be represented by a constant because the settling process was regarded as transient neretin et al 2003 estimated the settling velocity of the particulate mn following three assumptions the particles were approximated as spheres the density of birnessite 3 0 g cm 3 as the assumption of the density of mn particles was used in the calculation and the spherical diameter of settling mn particles was 4 μm the result of their calculation indicated that the settling velocity is 0 98 m day the settling period in the epilimnion of the lacustrine zones deeper than 20 m is generally more than two weeks therefore the settling process of the aggregates of particulate mn is vital for the simulation of the mn cycle the introduction of the settling process positively affects the 3d mn transport model equations 3 3 and 3 5 indicate that the variation and distribution of do concentrations in the tarago reservoir are vital factors in the simulation of mn transport the field data of do was defined as a static variable that controls the quantity of mn flux from sediments and the oxidation rate of reduced mn a mathematical formulation for the oxygen balance is given by equation 3 6 dhi 2017 3 6 c d o t v r e a v b o d d v p h v r e s v s o d where c d o is the concentration of do mg l v r e a is the rate of oxygen production by reaeration mg l day v b o d d is the rate of decay of biological oxygen demand bod mg l day v p h is the rate of producing oxygen by photosynthesis mg l day v r e s is the rate of respiration oxygen demand mg l day and v s o d is the rate of sediment oxygen demand mg l day the boundary conditions of mn flux from inflow were ignored in this study due to the absence of relevant data while mn flux diffusing from sediments was considered the major source the determination of sediment mn flux was a challenge due to the absence of relevant measurements however the literature about the flux of mn across the sediment water interface has revealed that the general range of the flux is from 3 85 to 244 75 mg m2 day pakhomova et al 2007 trefry and presley 1982 warnken et al 2001 the investigation by johnson et al 1991 indicated that the sediment mn flux of a seasonally anoxic lake is between 11 and 55 mg m2 day the measured soluble mn in the bottom layer was used to calculate the rate of soluble mn flux from sediments and the relationships are given by 3 7 q s m n c s m n a g r i d t where q s m n is the rate of soluble mn flux from sediments mg l m2 c s m n is the field measured soluble mn concentration mg l and a g r i d is the area of the grid m2 3 2 statistical analysis the error analysis of the measured and simulated data relies on the mean absolute error mae and root mean square error rmse in addition through linear regression analysis between the observed and simulated values the coefficients of determination r 2 were determined to examine the performance of the 3d mn cycle model the index of agreement ia was also used to evaluate the performance of the model since it performs better than r 2 in overcoming the inflation of errors due to their squared values willmott 1984 both values of r 2 and ia vary from 0 to 1 with higher values indicating a better performance of the model the introduction of lake specific indices facilitates the investigation of the contribution of hydrodynamic drivers such as wind read et al 2011 the probability of upwelling events under stratified conditions was defined as wedderburn number wn by thompson 1980 the index has been used to describe the entrainment of hypolimnetic water into the surface layer under the influence of wind induced upwelling monismith 1986 when wn is much less than 1 the wind stress is much more than the buoyancy force and upwelling is probably occurring at the upwind end of the waterbody otherwise the buoyancy force and wind stress are nearly equal and horizontal mixing is considered important when wn is near or more than 1 0 the wedderburn number is given by 3 8 w n g z e 2 u 2 l s where g g ρ h ρ s ρ h is the reduced gravity due to the difference in density between the hypolimnion and epilimnion z e is the thickness of the mixed layer l s is the lake fetch length and u is the water friction velocity due to wind stress 3 3 model calibration the development of the 3d mn transport model relied on the hydrodynamic model that was calibrated and validated by zhang et al 2020 the setup of the mike 3 flow model for mn transport simulation was based on the field data measured from august 31 2017 to august 31 2018 the simulation period includes a complete mn cycle some conditions must be assumed based on the literature review due to the absence of the data the boundary conditions of bod do soluble mn and particulate mn were imposed at the inlet and outlet of the reservoir the initial conditions of bod do smn soluble mn and pmn particulate mn of the entire reservoir were set according to the measured data at the observation point due to the absence of the measurement of bod the empirical value 1 0 mg l of the bod of eutrophic lakes was adopted as the boundary condition and initial condition of bod stefan and fang 1994 since there is no relevant measurement at the inlet the average measured do concentration at the surface water 9 14 mg l was used as the boundary condition of the inlet the analysis of historical mn records in section 2 2 indicated that both particulate and soluble mn maintained a low and uniform level before the do level at the bottom layer decreased to less than 5 0 mg l this indicated that the release of soluble mn from sediments plays a dominant role and the impact of the mn flux of inflow is limited during the mn peak without the mn source from sediments the measured particulate and soluble mn in the epilimnion were consistently below 0 02 mg l and 0 003 mg l respectively which were assumed as the inflow boundary conditions of mn concentration in addition the optimized values for the relevant parameters of the mn model were calibrated through the field measurement of do and mn and all the parameters are summarised in table 4 4 results and discussion 4 1 performance of the 3d mn transport model table 5 displays the results of the error analysis and regression analysis of the simulated do soluble mn and particulate mn at the surface middle and bottom layers the seasonal change of the do exhibits the characteristics of monomictic lakes with the hypolimnion experiencing annual do depletion during stratification helfer et al 2011 townsend 1999 fig 5 presents the measured and simulated do concentrations at the surface middle and bottom layers of the observation location fig 1 b from september 2017 to june 2018 the model accurately simulated the time when the do concentration decreased to less than 5 0 mg l the threshold value for soluble mn diffusion the measured record indicated that the bottom layer do experienced an abnormal fluctuation around march 2018 but the trend of simulated results exhibits a smooth increase nevertheless such a specific error in the do simulation has a limited effect on the mn modelling outputs because most concentrations during this period were below the threshold and the simulation is accurate when the bottom layer do has increased to more than 5 0 mg l the figure and statistical analysis highlight that the simulation matches the trend and dynamics of the measured data while the simulated results present a stronger fluctuation and are slightly higher than the measured data although the simulation of do is not perfect the simulated results are reliable in modelling the release of mn from the sediments figs 6 and 7 present the simulated and measured soluble and particulate mn at the observation location from september 2017 to august 2018 and table 5 displays the error analysis and regression analysis the statistical results indicate that the magnitudes of the simulated and measured mn are close and that the variation of mn in the entire water column was accurately simulated the sediments of the tarago reservoir started to release soluble mn in early december with the mn levels in the water column exhibiting considerable increases both soluble and particulate mn reached their peaks in the first week of march after the peak the mn in the column smoothly decreased due to sedimentation and outflow the soluble and particulate mn decreased to 0 02 mg l on april 5 2018 and may 23 2018 the oscillations of the simulated soluble and particulate mn in january and february 2018 cannot be validated due to the absence of high frequency field measurements the oscillations of the mn in the middle layer 10 m depth are due to their roughly daily variation the simulated isotherms adjacent to the middle layer from 24th to january 31 2018 were displayed in fig 8 the isotherms indicate that the simulated mn results from the internal wave that acts to produce a motion with a daily or smaller temporal scale the simulated soluble and particulate mn of the so called bottom layer shown in figs 6 and 7 does in fact apply to a depth 0 5 m higher than the actual bottom layer due to the fact that the simulation adopted a 0 5 m vertical resolution therefore the validation of the bottom layer mn is unachievable and the comparison indicated that both soluble and particulate mn decreased significantly from the water sediment interface to the depth of 19 5 m although the simulated results did not perfectly match the measured data the graphical comparisons indicate that the model reproduced the variation trend of both soluble and particulate mn and the statistical analysis of the performance validates these claims 4 2 three dimensional distribution of soluble mn the development and validation of the 3d mn transport model provide a feasible way to visualise the distribution and variation of mn in the tarago reservoir the principal factors determining the distribution of mn are hydrodynamic transport and chemical or biological reactions based on the assumption that the unique source of soluble mn is the soluble mn flux diffusing from the sediment which varies with time but is invariant with space and has a constant reaction rate there are some limitations of the simulation of 3d mn transport based on these assumptions the rate of mn diffusing into the water column in the shallow zones would be the same in the deep zones notably because pmn is easily removed by routine treatment the level of soluble mn is the key factor affecting the quality of treated water therefore the following analyses concentrate on the distribution of the soluble mn the epilimnion of the lacustrine zone eol zone near the dam serves as a water supply for the water treatment plant the period with the potential risk of an mn issue i e smn 0 02 mg l was from january 24 2018 to march 24 2018 fig 9 represents at different times the full process of the accumulation upward transportation and decrease of smn in each sub figure the four slice plots represent the smn levels at depths of 0 m 5 5 m 11 m and 16 5 m noticeable smn diffusion from the bottom sediments occurred on january 24 2018 fig 9a and the smn in the bottom of the transition lacustrine zones rose to above 0 1 mg l however the smn between the surface and 5 5 m remained negligible less than 0 02 mg l proving that the upward transport of smn was limited during this period from january 24 2018 to february 27 2018 the upward transport of smn was greatly enhanced and the smn level at the epilimnion layer reached its peak approximately 0 1 mg l for the lacustrine zone smn at the surface layer was more than at the 5 5 m and 11 m layers during the smn peak fig 9b which is an important observation in relation to water treatment and specifically raw water intake depth the cross section of the smn and oxycline at the y axis 4 depicted in fig 9 a next to the dam wall were displayed in fig 10 and it indicated that lower smn concentrations in upper layers were common during stratification while do at the hypolimnion is low however the common vertical distribution of smn changed on february 27 2018 with the homogeneous vertical distribution of do for the top 30 m of the water column while surface layer smn concentrations exceeded the middle layer smn levels such different distributions indicated that the transport of the smn at this time was not dominated by upward vertical transport but was affected by horizontal transport then as shown in fig 9 d the smn concentration of the entire reservoir gradually decreased to a low risk level during the subsequent 4 weeks in addition the simulated results can output the vertical temporal evolution of soluble mn at the location of water intake dam location in fig 1 b as fig 11 shows for the 2017 event the vertical distribution of smn was relatively similar offering limited management opportunities in terms of intake depth selection fig 12 on the other hand shows the vertical distribution of soluble mn from february 18 2018 to march 11 2018 in this case the vertical profile of smn was often not constant with times of higher concentrations in the deeper layers but with times of higher levels at the surface too this shows the relevance of our proposed 3d model for water utilities a better understanding of 3d mn variations under different conditions can provide more accurate information and predictions on expected smn levels at different crucial locations in the water column allowing managers to apply for the most appropriate risk reduction measures 4 3 the role of hydrodynamic processes in manganese cycle 4 3 1 wind driven currents as one of the most important atmospheric forces the wind is a main source of energy in reservoirs the role of the wind is to exert a drag on the surface current and transport floating pollutants if the wind fetch and wind duration are sufficient wind stress will dominate the movement of surface currents and influence the circulation pattern ji 2017 juntunen et al 2019 wu et al 2018 this study investigated the movement of the surface current under the influence of the wind on 25th and february 27 2018 when the mn concentrations were in a significant and increasing level as the wind rose plots in figs 13 and 14 show the dominant wind is south wind and northeast wind respectively on february 25 2018 and february 17 2018 the simulated flow patterns figs 13 and 14 of the reservoir on 25th and 27th february were inverse under the influence of the different wind forces hence the comparisons of flow pattern between these two days are vital to reveal the roles of wind driven current in the transport of soluble mn in addition the analysis at the mn peak is significant because this period is the most challenging for the treatment of mn in the twtp figs 13 and 14 detail the simulated surface current velocity and smn concentration at the surface of the reservoir and the wind rose plots in the 12 h the tarago reservoir has a maximum fetch when the wind blows from north to south because of its morphology and both figures indicate that the wind plays a dominant role in driving the north south movement of the surface current under the influence of a strong south wind the surface current flowed from the dam to the inflow river upstream the smn presents a decreasing trend from the inflow boundary to the dam when northeast was the dominant wind direction on 27th february the movements of the surface current and mn distribution were in the opposite direction i e downstream as mentioned the smn flux diffusing from the sediment was assumed to vary with time but be invariant with space the smn concentration at the surface of the shallow area was greater than at the surface of the deep area therefore the upstream surface current from the lacustrine zone brought the water with a low mn level to the surface of the transition zone which caused the mn distribution pattern depicted in fig 13 and the opposite in fig 14 the significant movement of the surface current impacted the circulation which affects the mn transport in the reservoir meanwhile the thermal structure of the water column affects the vertical magnitude of circulation due to the importance of the eol zone in water management the analysis of the influence of wind stress on mn transport focused on the eol zone and the mn peaks that occurred on 25th and february 27 2018 were chosen as the research periods figs 16 and 17 present the current velocity smn distribution and isotherms of the cross section x1 x2 see figs 13 and 14 fig 15 displays the time series of the vertical temperature distribution in the vps location see fig 1 the wind speed and direction and the wedderburn number of the cross section x1 x2 from 24th to february 28 2018 and the values of wn at 00 00 and 12 00 on 25th and 27th february are marked in the figure the wn at 00 00 25th february is the only one higher than 1 0 which is much higher than the indexes at other three moments as fig 15 shows the thermal structure is stable and the wind is weak during 00 00 25th february hence the generation of upwelling was suppressed comparing the soluble mn patterns in figs 16 and 17 the horizontal distribution of soluble mn was uniform only at 00 00 on 25th february during the other three moments the wn less than 1 0 indicates that the wind stress was greater than the buoyancy force due to the change of density and upwelling likely occurring at the upwind end of the reservoir while the surface current presents a uniform flow direction from the dam wall to the transition zone in fig 16 the current from the bottom of the lacustrine zone flowed to the eol zone near the dam and significantly increased its mn level meanwhile a discernible circulation occurred in the hypolimnion of the lacustrine zone the circulation enhanced transport from the bottom water with a high level of smn to the eol zone which produced potential mn issues in the water treatment as fig 17 depicts a large surface current uniformly flowed from the transition zone to the dam wall driven by the wind on february 27 2018 and was blocked by the dam circulation occurred in the water near the dam and created a downward current that significantly restricted the upward transport of smn in this case the mn levels in the eol zone are more susceptible to the surface water from the transition zone rather than the bottom of the lacustrine zone the bottom current with a high level of mn flows to the side of the lacustrine zone away from the dam because the water in the eol zone was replaced by the surface layer of transition and the upward transport of mn was suppressed the mn level of the eol zone remained relatively low overall the influences of the surface current driven by north and south wind are opposite for the mn transport in the eol zone wind from the longitudinal direction causes a proportionally higher mn concentration in the surface the surface current flowing to the dam diluted the concentration of mn and the reverse surface current greatly increased the mn level of the water near the dam wall because the twtp collects raw water from the eol zone near the dam the current dynamic under the influence of the predominant south wind significantly elevated the risk of high mn in the raw water otherwise the surface current flow with a low level of smn moves to the dam wall which decreases the smn level of the raw water transported to the wtp the 1d vertical transport model is insufficient to reproduce the potential risk of the mn issue due to the inability to simulate such horizontal transport of mn therefore the 3d movement of the current driven by wind in the lacustrine zone plays an important role in the reservoir with a significant longitudinal morphological difference 4 3 2 thermal structure as a temperate monomictic reservoir the tarago reservoir presented seasonal changes in the thermal structure a clear thermal stratification was observed in the reservoir during the summer from december to february of 2018 the thermal stratification occurring in the reservoir caused a depletion of the do in the hypolimnion and the subsequent release of soluble mn from the sediment however the turnover event can effectively replenish the do in the bottom layers and vertically mix the smn antonopoulos and gianniou 2003 gantzer et al 2009 townsend 1999 the formation of a thermocline during stratification can reduce the vertical mixing and turbulence and restrict mn transport from the hypolimnion to the epilimnion the turnover replenishes the do for the hypolimnion while it also boosts the upward transportation of mn boehrer and schultze 2008 elçi 2008 kirillin and shatwell 2016 fig 18 presents the lacustrine zones with different smn distributions when the reservoir was in variable thermal structures the smn diffusion was limited on january 7 2018 when significant thermal gradients occurred fig 18 a and c the summer stratification ensured that the smn concentration above 6 0 m depth was less than 0 02 mg l the vertical distribution of smn changed dramatically in the lacustrine zone after the stratification ended as fig 18 d depicts the body of water tended towards a uniform water temperature on march 3 2018 although the smn from the surface layer to 15 m presented a relatively high uniform concentration the majority of the smn resided in the bottom layers and the amount of smn diffusing into the epilimnion remained minor in the next 4 weeks the replenishment of the do due to the mixing process cut the smn source and the overall smn decreased to a minimal level from the point of view of the thermal structure the rising of smn in the eol zone was contained by significant stratification and promoted during the weakening of the stratification and the beginning of the mixing process the levels of the smn of the epilimnion were kept below the critical threshold of 0 02 mg l during summer stratification the development of destratification is the discernible signal for the wtp to raise the alert level for high levels of mn in raw water in addition although the destratification significantly increased the rate of smn s upward transport fig 18 b and d indicate that the vertical gradient of mn persisted even when the lake appeared to be fully mixed 5 conclusions a combined 3d hydrodynamic model and mn cycle model was successfully applied to the tarago reservoir this has the capability of simulating the hydrodynamic components and the 3d distribution of mn in such a temperate monomictic reservoir the development of the 3d model was based on the 1d mn cycle model developed by johnson et al 1991 while there is room to improve the constants and assumptions used in the development of the model the validation indicates that the model accurately reproduced the mn distribution in the reservoir to the authors knowledge a 3d modelling and simulation of the mn cycle in lacustrine systems was not attempted before and this study filled the gap using the simulated results of the model it was revealed that the horizontal transport of mn significantly affects the mn distribution in the lacustrine zone that supplies the raw water for the water treatment plant in addition the 3d mn model enabled the analysis of the impact of wind driven currents and thermal structure on the mn cycle results showed that wind driven currents toward the dam wall reduce the mn concentration near the dam wall while reverse currents greatly increase the mn level summer stratification ensures that minimal soluble mn is transported into the wtp from the reservoir s epilimnion while destratification significantly raises the levels of mn in the epilimnion due to upwards movements of mn rich hypolimnetic waters the model was also able to explain unexpected vertical mn variations by considering horizontal transport which would have been unachievable with existing 1d mn models the understanding of the 3d spatial temporal relationships between mn cycle and wind speed direction and thermal structure provides new and useful information for more targeted management strategies for water treatment future work will focus on integrating the 3d mn cycle model with data knowledge of mn removal processes at the water treatment plant downstream in order to predict and reduce the potential mn related water quality risks based not only on the meteorological reservoirs conditions but also on the treatment plant operations conditions zhang et al 2021 such source to treated water optimisation approach will provide a fully comprehensive integrated mn related risk assessment and reduction modelling tool for the water treatment operators credit authorship contribution statement fuxin zhang conceptualization methodology software validation formal analysis writing original draft hong zhang conceptualization methodology writing review editing edoardo bertone conceptualization validation writing review editing rodney stewart conceptualization writing review editing xia shen methodology kathy cinque data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge dhi water and environment denmark for their assistance in providing mike modelling system for this study this research work was conducted with the technical support of melbourne water and the griffith university publication assistance scholarship we acknowledge the use of meteorological data and water temperature from the vertical profile system of melbourne water and australian bureau of meteorology appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105213 
25703,the bioretention basin is a type of green infrastructure that treats stormwater runoff however there is a limited understanding of its treatment potential with different design configurations and limited capability to model bioretention systems current modeling practice fails to represent bacteria mediated biochemical reactions that remove nitrogen this study provides a reaction based model of water quality treatment processes using multiple bacterial monod kinetics under variably saturated flow conditions to predict the dynamics of nitrogen and organics in a bioretention system the model was calibrated and validated against concentrations measured during an eight month field monitoring program because the model simulates both basin hydrologic performance and biological treatment performance it can be used to verify design guidelines by evaluating the response of the system to different design parameters the model shows that nitrogen removal is enhanced by increasing bioretention basin volume and by setting an optimal value of hydraulic conductivity for subsurface infiltration graphical abstract image 1 keywords rain garden bioretention best management practices stormwater water quality models calibration 1 introduction current bioretention basin modeling efforts both hydrological and water quality models have been limited in terms of hydrological models recarga atchison and severson 2004 dussaillant et al 2003 and recharge dussaillant et al 2004 wang et al 2019 the more complex version of recarga are the only existing models that focus specifically on bioretention basins in terms of water quality models there has been no biochemical process based system scale mechanistic model specific to bioretention basins although gifmod massoudieh et al 2017 models a variety of green infrastructure including bioretention basins widely used models such as swmm storm water management model huber et al 1988 rossman 2004 sustain system for urban stormwater treatment and analysis integration model lai et al 2006 2009 shoemaker et al 2009 winslamm source loading and management model for windows myllyoja et al 2001 pitt and voorhees 1995 p 8 program for predicting polluting particle passage through pits puddles ponds walker 2007 and music model for urban stormwater improvement conceptualization wong et al 2002 2006 rely on approximate representations of biochemical processes e g percentage removal rather than on biokinetic models of microbiological reactions these models lack process based water quality formulations for bioretention basins and are thus not able to predict effluent quality accurately based on process understanding instead pollutant removal by each type of bmp is modeled as a removal or first order decay process in which the bmp is a black box with all reaction processes lumped within a single percentage removal coefficient or first order decay constant there is a growing body of literature devoted to understanding the main processes leading to pollutant removal in bioretention basins these studies seek to find process based alternatives to modeling the bioretention basin as a black box process based modeling is challenging because it must capture not only physical processes such as sedimentation and filtration that are commonly included in current models but also chemical and biological mechanisms that mutually influence each other and are simultaneously active with physical processes in fact chemical and biological mechanisms play a key role in nutrient and pathogen removal for instance organic nitrogen removal is a bacteria mediated step wise biochemical process first through ammonification that converts organic nitrogen to ammonia next via nitrification that converts ammonia to nitrate and finally through denitrification that converts nitrate and nitrite to nitrogen a detailed mechanistic breakdown is necessary to track transformation and transport within each zone of the soil filter media models that take all of these biochemical processes into account will give a more complete picture of the complex processes that occur in bioretention basins the advantage of a process based model is that the relative contributions of specific processes and design elements to overall pollutant removal can be quantified through sensitivity analysis which is helpful in basin design for optimal pollutant load reduction current water quality models used for bioretention basins focus mainly on pollutants that are removed primarily by physical processes for instance li and davis 2008a developed a one dimensional model that represents filtration and capture of total suspended solids other analogous models look at filtration and adsorption of particulate and dissolved heavy metals as well as the fate of polycyclic aromatic hydrocarbons in bioretention basins he and davis 2009 li and davis 2008b recent studies have also focused on understanding nitrogen speciation and fate in bioretention basins li and davis 2014 and quantification of the nitrifying and denitrifying bacteria that are responsible for nitrogen removal chen et al 2013 a good starting point in developing a process based model for bioretention basins is to look at systems such as constructed wetlands cws in which similar biochemical transformations take place the microbial community including nitrifiers and denitrifiers that are responsible for nitrogen removal in cws play an equally key role in bioretention basins extensive research on process based numerical models of varying complexity has been conducted for cws one group of models uses the richards equation for variably saturated flow langergraber and šimůnek 2005 ojeda et al 2008 wanko et al 2006 for example cw2d constructed wetlands 2d couples the flow model hydrus with multiple monod type kinetic equations to create a multi component reactive transport model of subsurface flow constructed wetlands but does not simulate associated surface water bodies langergraber and šimůnek 2005 langergraber and šimůnek 2006 šimůnek et al 1998 šimůnek and van genuchten 2006 saeed and sun 2011 recommend that models for wetland systems be developed based on multiple monod kinetics instead of first order or single monod kinetics to predict the removal of nitrogen and organics in order to better evaluate how bioretention basin design affects treatment performance and hence explore management options for solving pollution problems it is imperative to develop a process based model that can be used to make reliable predictions and assist in formulating appropriate controls therefore the objectives of this study were to 1 develop a model for bioretention basins that adds modules for subsurface bacteria mediated nitrogen conversion and removal to an existing flow model for bioretention basins recharge dussaillant et al 2004 wang et al 2019 2 use the verified model to test sensitivity to model parameters including bacterial kinetic parameters and bacteria concentrations 3 develop bioretention design guidelines by applying the model to various hypothetical basin configurations detention depth catchment to basin area ratio and saturated hydraulic conductivity to evaluate basin performance in terms of nitrogen removal 2 methods 2 1 study site and field data for calibration and validation fig 1 shows a cross section of the balam estate bioretention basin in singapore that was the location of this study with an effective treatment area of 240 m2 the basin receives runoff from a 16 800 m2 residential catchment about 88 surface imperviousness with a time of concentration of about 10 min wang et al 2017 wang et al 2009 basin inflow infiltrates through four soil layers consisting of from surface to bottom a 40 cm sandy loam filter layer a 10 cm fine sand transition layer a 30 cm saturated anoxic zone of hard rocks average size of 50 mm and 65 by volume and wood chips average size of 5 mm and 35 by volume and a 10 cm drainage layer of fine gravel average size of 2 5 mm which also remains saturated and anoxic ong et al 2012 wang et al 2009 the entire soil filter media is underlain by a high density polyethylene hdpe liner to prevent exfiltration into the surrounding soil treated runoff is collected by 10 cm underdrains in the drainage layer that discharge to an outlet box that in turn discharges to a 30 cm outlet pipe to an adjacent surface drain the underdrains are at a lower elevation than the outlet pipe thereby creating a perennially saturated anoxic zone favoring denitrification if the basin water level exceeds a depth of 13 7 cm untreated water from the basin overtops a weir and discharges to a 1 1 m high by 1 2 m wide surface overflow culvert if basin water ponds to still higher levels it can also discharge via a 1 m square opening at the top of the outlet box and four 10 cm wide slots on the sides of the outlet box which in turn discharges to the 30 cm outlet pipe the outlet box was included in the design of the basin for flood control but the openings were sealed for our study in order to isolate the flow due to infiltration hydrologic measurements and water quality samples used in this study were obtained from a real time monitoring system that consisted of flow and water level measurement equipment operated continuously and automatic water quality samplers to sample six selected storm events installed from april to november 2013 at balam estate rain garden wang et al 2016 2017 provide a full description of the sample collection scheme and data collection and analysis influent water quality varied with the event mean concentration emc of total nitrogen ranging from 0 58 to 5 1 mg l the latter occurring during a period of unusually poor air quality wang et al 2017 2 2 hydrological model wang et al 2019 describe in detail modeling of the hydrological behavior of the bioretention basin using the recharge hydrologic model in the recharge model the variably saturated subsurface flow is simulated with a one dimensional mixed form of the richards equation which models both saturated and unsaturated conditions in the basin during wet and dry periods dussaillant et al 2004 under the mixed formulation of the richards equation θ is a function of both h and z the governing equation is 1 θ h z t θ h h t z k h z h z 1 s h z where θ soil moisture content cm3 cm3 t time hr z vertical depth cm h pressure head cm k unsaturated hydraulic conductivity cm hr and s evapotranspiration rate hr 1 the van genuchten mualem equations are used in recharge to define the relationships among h θ k and θ h and estimate soil properties assuming that there is no hysteresis van genuchten 1980 in addition to modeling subsurface flow recharge models the filling and emptying of the surface basin through a water balance which predicts the time dependent depth of water ponded in the surface basin 2 a d h s d t q r a i n q i n f l o w q o q c where a bioretention basin area m2 q rain m3 hr direct rain onto the basin q inflow m3 hr catchment runoff entering the basin q o m3 hr water infiltrated into the soil media and out via the pond outlet and q c m3 hr basin surface overflow via the pond culvert to compute q o the darcy buckingham law for unsaturated conditions is used equation 3 3 q o k h z 0 h z 1 in the recharge model equation 1 is discretized using a crank nicholson finite difference numerical scheme and solved using the thomas algorithm with a modified picard iteration for mass balance dussaillant et al 2004 to improve the computation speed and convergence of the model by minimizing finite difference discretization truncation error a variable time stepping scheme dussaillant et al 2004 is adopted with an initial δt of 30 s the minimum and maximum δt are 10 6 hr and 1 min respectively the outputs of the recharge model include vertical water flow and soil moisture content as a function of time which are coupled with biochemical concentrations as described in section 2 3 3 below 2 3 biokinetic model 2 3 1 conceptual model the formulations in constructed wetlands 2d cw2d langergraber and šimůnek 2006 have been adopted to model bacteria mediated biochemical transformation and degradation processes for nitrogen and other constituents in this paper fig 2 summarizes the nine biochemical reactions rc that are responsible for nitrogen removal hydrolysis of chemical oxygen demand cod rc1 aerobic growth of heterotrophs rc2 growth of heterotrophic denitrifiers on no3 rc3 growth of heterotrophic denitrifiers on no2 rc4 lysis of heterotrophs rc5 aerobic growth of nitrosomonas on nh4 rc6 lysis of nitrosomonas rc7 aerobic growth of nitrobacter on no2 rc8 and lysis of nitrobacter rc9 fig 2 shows that organic nitrogen on the starting material that enters the bioretention basin is modeled as part of cod which in turn consists of readily biodegradable cod cr slowly biodegradable cod cs and inert cod ci either through the hydrolysis of cod or cell lysis of heterotrophs xh and autotrophs xans and xanb on is mineralized and converted to nh4 during ammonification nh4 could be incorporated back into bacteria cells as part of aerobic growth for heterotrophs together with inorganic phosphorus ip or po4 3 in the presence of o2 alternatively nh4 undergoes nitrification which is modeled as a two step aerobic process from nh4 to no2 as mediated by autotrophic bacteria nitrosomonas xans and then from no2 to no3 as mediated by autotrophic bacteria nitrobacter xanb nitrification occurs in the presence of o2 which is the preferred electron donor in the absence of free o2 but the presence of bound oxygen no2 and no3 anoxic condition as defined by weißbach et al 2017 no2 and no3 replace o2 as the next thermodynamically preferred electron acceptors that can be utilized by heterotrophic denitrifiers such as paracoccus denitrificans and various pseudomonads organic matter cod in this case acts as the electron donor and the denitrification product is molecular n2 which is lost from the bioretention system and enters into the atmosphere table 1 shows the anticipated zones of primary residence for the different bacteria species that utilize different sets of electron donors and electron acceptors to generate different final products as the presence of o2 determines the type of growth that bacteria undergo it would be expected to directly affect the abundance of bacteria species in the different zones of soil filter media in bioretention basins autotrophic bacteria nitrosomonas and nitrobacter are expected to reside mainly in the upper unsaturated zone of sandy loam deeper into the soil media and approaching the saturated anoxic zone of internal water storage iws the number of autotrophs is likely to decrease heterotrophic denitrifiers in contrast are facultative anaerobes likely to reside throughout the entire soil media this zonation has been confirmed in the field by castellano hinojosa et al 2018 and zheng et al 2017 2 3 2 mathematical formulation of reaction rates the cw2d model langergraber and šimůnek 2006 considers the interactions among twelve components o2 cr cs ci xh xans xanb nh4 no2 no3 n2 ip in nine processes rc1 rc9 its mathematical structure is based on that of the activated sludge models asms in which these nine bacteria facilitated transformation processes are modeled using monod type bacterial growth and decay equations henze et al 2000 the overall reaction rate term d c i d t mgi l hr for component i follows the gujer matrix structure as shown in equation 4 4 d c i d t θ j 1 r v j i r c j where c i is the aqueous concentration θ m3 w m3 s is the volumetric water content v ji mgi mgj is the stoichiometric factor for component i 1 n and process j 1 r and rc j mgj l hr is the monod type kinetic reaction rate for process j in the aqueous phase the reaction rates d c i d t are the resultant equations from element by element multiplications of v ji and rc j for each of the twelve components modeled as listed in table s1 the stoichiometric matrix v ji consists of composition parameters which describe the nitrogen and phosphorus content in cod and bacteria biomass and stoichiometric parameters which describe the generation of cod from hydrolysis and bacteria lysis as well as the bacteria biomass yield coefficients from the consumption of cod therefore v ji represents the extent to which a given microbial mediated process rc j affects the concentration of component i ci the multiple monod type kinetic reaction rate rc j for modeling bacterial growth and decay follows this general structure 5 r c j g r o w t h μ x m a x c s 1 k s 1 c s 1 k i n h s 2 k i n h s 2 c s 2 c n h 4 k x n h 4 c n h 4 c i p k x i p c i p ρ θ s x r c j d e c a y b x c x where x xh xdn xans xanb represent respectively heterotrophic bacteria in general heterotrophic denitrifiers autotrophic nitrosomonas and autotrophic nitrobacter μ x max is the maximum specific growth rate of bacterium x c s1 mg l is the concentration of the limiting substrate s1 for growth k s1 mg l is the half saturation coefficient for substrate s1 c s2 mg l is the concentration of the limiting substrate s2 that inhibits growth k inh s2 mg l is the half inhibition coefficient for substrate s2 c nh4 mg l is the concentration of the limiting substrate ammonium c ip mg l is the concentration of the limiting substrate inorganic phosphate k x nh4 mg l is the half saturation coefficient for ammonium k x ip mg l is the half saturation coefficient for inorganic phosphate ρ is the soil bulk density kgs dms 3 θ is the volumetric water content dmw 3 dms 3 s x is the concentration of bacteria in the solid phase mg kgs c x mg l is the concentration of bacteria in the aqueous phase and b x 1 hr is the bacteria decay rate the term μ m a x c s 1 k s 1 c s 1 represents the specific growth rate of bacteria utilizing substrate s1 for cellular respiration the term k i n h s 2 k i n h s 2 c s 2 represents the inhibitory effect on the growth of bacteria caused by substrate s2 the term c n h 4 k x n h 4 c n h 4 c i p k x i p c i p represents the potential for an insufficiency of either of the limiting nutrients nh4 and ip to reduce the rate of growth these terms are multiplied together as more than one nutrient or growth factor has the potential to limit the growth rate of bacteria the cw2d model relies on a number of assumptions langergraber and šimůnek 2006 all reactions are assumed to occur in the aqueous phase and all components except bacteria are soluble and mobile bacteria associate exclusively with the solid phase and their lysis i e decay rate is set as a constant and does not depend on environmental conditions moreover following the asm models it is assumed that ph rate equation coefficients and stoichiometric factors are constant table s2 cw2d defines kinetic parameters in rc j μ x max k s k inh s and b y using literature values taken from studies of wastewater treatment based on the presumption that municipal wastewater is being treated in the cw however since this study addresses treatment of stormwater the kinetic parameters are set by calibration instead 2 3 3 biokinetic modeling since the twelve biokinetic components are interdependent in a complex manner e g the product of one reaction becomes the reactant of another reaction their time varying concentrations must be evaluated simultaneously at each spatial and temporal step this was done by coding in matlab a model of multiple monod type biological degradation kinetics that employs the outputs from the hydrodynamic model in recharge as shown in fig 3 together the two codes constitute a biokinetic model of 1d reactive transport the advection of influent through the 1d column of soil filter media is computed on a 1 cm spatial grid in recharge two outputs from recharge that become inputs to the biokinetic model are the time evolution of flow rate q z cm hr between two neighboring spatial cells and the soil moisture θ z m3 w m3 s within each spatial cell reaction within each spatial cell is assumed to be uniform that is each cell is modeled as a continuously stirred tank reactor cstr thus the 1d column of soil media can be considered as a vertical series of cstrs at any time step the mass balance for each component i in a single cell cstr reactor at each time step can be expressed as 6 d c i z d t 1 τ c i p r e z 1 τ c i p o s t z where τ is the hydraulic retention time within a single cell cstr reactor c i pre z is the concentration of component i before undergoing simultaneous biokinetic reactions and is calculated according to equation 4 d c i z d t is the rate of change of the concentration of component i in the current spatial cell as governed by the set of monod type kinetics equations in table s1 c i post is the concentration of component i of the single cell cstr reactor z after undergoing the simultaneous biokinetic reactions and also the influent concentration to the next spatial cell z 1 within the same time step and c i pre z is the concentration of component i before undergoing simultaneous biokinetic reactions and is calculated for each time step according to equation 7 7 c i p r e z c i i n i t z θ z d z c i i n f z q z d t θ z d z q z d t the concentration of component i before undergoing biokinetic reactions c i pre is estimated by summing the influent mass c i i n f z q z d t and initial mass c i i n i t z θ z d z within the single cell cstr reactor z as shown in equation 7 the concentration c i p r e is a virtual concentration used for computational convenience in the actual system mass inflow and reactions occur simultaneously at the end of the spatial stepping from the first spatial cell at the surface to the last cell at the bottom of the bioretention basin c i p o s t z is the initial concentration for the next time step as shown in fig 3 the process is iterated until the end of the simulation time the time series for effluent concentration is that of the bottom cstr 2 3 4 model evaluation indices to evaluate the model s predictive capability six indices are used coefficient of determination r 2 nash sutcliffe efficiency nse nash and sutcliffe 1970 mean absolute error mae relative root mean square error rrmse ratio of root mean square error rmse to standard deviation of observations rsr and percent bias pbias for the mean outlet concentration according to equations 8 to 13 khan et al 2013 moriasi et al 2007 woznicki and nejadhashemi 2012 8 r 2 i 1 n o i o m i m 2 i 1 n o i o 2 i 1 n m i m 2 9 n s e 1 i 1 n o i m i 2 i 1 n o i o 2 10 m a e i 1 n o i m i n 11 r r m s e r m s e μ m i i 1 n o i m i 2 n m 12 r s r r m s e σ o i i 1 n o i m i 2 i 1 n o i o 2 13 p b i a s i 1 n o i m i i 1 n o i 100 where o i m i o and m are the observed modeled mean of observed and mean of modeled values respectively n is the number of comparisons made between observed and modeled values these indices were chosen for model evaluation as they represent three major categories of model evaluation techniques standard regression dimensionless and error index moriasi et al 2007 2015 standard regression such as r 2 determines the strength of the linear relationship between modeled and measured data r 2 describes the extent to which the model explains the variance of the observed data it ranges from 0 to 1 the higher the value the smaller the error variance and the stronger is the collinearity between modeled and observed values a value of r 2 0 50 is considered acceptable santhi et al 2001 the dimensionless nash sutcliffe efficiency nse shows how well a plot of observed against modeled values fits the 1 1 line by comparing the relative magnitude of the residual variance noise to the measured data variance information nash and sutcliffe 1970 the nash sutcliffe efficiency ranges from to 1 with 1 being the optimal value and values 0 0 indicating that the observed mean value is a better predictor than the modeled value values above 0 0 are generally taken as an acceptable level of performance while moriasi et al 2015 recommend for watershed scale models to predict nitrogen load the following criteria nse 0 35 0 5 and 0 65 for satisfactory good and very good performance respectively the criteria developed by moriasi et al for watershed models are an imperfect fit with the model evaluated here but nonetheless provide a useful touchstone error indices such as mae rrmse rsr and pbias quantify the error in the units of the observed values the indices rrmse and rsr modify rmse based on scaling factors that are the modeled mean and the observed standard deviation respectively these indices range from 0 to with the optimal value being zero moriasi et al 2007 recommend rsr 0 70 0 60 and 0 50 for satisfactory good and very good watershed model performance respectively the pbias error index shows the tendency of the modeled data to be higher or lower than the observed values the index has an optimal value of 0 with positive values indicating that the model tends to underestimate and negative values indicating that the model tends to overestimate green and stephenson 1986 gupta et al 1999 moriasi et al 2007 recommend pbias 30 20 and 15 for satisfactory good and very good model performance respectively for watershed models 3 results and discussion 3 1 model calibration and validation the six storm events sampled during the monitoring program described by wang et al 2017 were used for model calibration and validation these six events consist of two events with high event mean concentration emc events 1 and 3 tn 5 09 and 4 02 mg l and four with lower emc events 6 5 4 2 tn 1 43 1 24 0 81 0 58 mg l influent emc is given more emphasis than rainfall depth in model calibration and verification since emc has a direct effect on biokinetic reaction rates both the high and low emc events were separated into two datasets one for model calibration events 1 4 6 and one for model validation events 2 3 5 fig 4 a and b show results of simulations of the calibration events using the calibrated parameter set table s2 through table s4 the observed values are shown against the modeled values on a 1 1 line for the event mean outlet concentration and removal rate of six water quality parameters nh3 n no3 n no2 n on tn and ip the model evaluation statistics r 2 nse mae rrmse rsr pbias for each of these water quality parameters are shown in table 2 model performance with the calibrated parameter set in the validation events is similarly shown in fig 4c and d fig 4a and c show generally good agreement between the observed and modeled mean effluent concentration values in the calibration and validation events for all water quality parameters except on for the calibration events shown in fig 4a with the exception of on the model evaluation statistics achieved r 2 0 90 nse 0 81 mae 0 12 mg l rrmse 0 54 rsr 0 44 and pbias 34 table 2 the values of nse and rsr exceed the thresholds from moriasi et al 2007 for very good watershed model evaluation performance nse 0 65 and rsr 0 50 the correlation found in the calibration events is better than those found by saeed and sun 2011 for a multiple monod cstr model for a vertical flow subsurface wetland r 2 0 7 rrmse 1 0 for the validation events shown in fig 4c the correlation between modeled and observed values is strong with r 2 0 93 and error deviation is small with nse 0 76 mae 0 03 mg l rrmse 0 38 rsr 0 49 and pbias 18 table 2 negative values of pbias in table 2 indicate that the modeled values are greater than the observed values this is evident in fig 4a and c as more data points lie above the bisector line than below fig 4b and d show the removal rates of the six water quality parameters nh3 n no3 n no2 n on tn and ip as an additional indicator of model predictive performance for both the calibration and validation sets a strong correlation is seen between the modeled and observed removal rates as the data points lie close to the bisector line poorer correspondence is seen in the calibration for on although the r 2 value of 0 80 for on in the calibration set indicates acceptable agreement between modeled and observed values the other indices show poorer correspondence nse 4 92 rsr 2 30 pbias 76 similarly for the validation set model performance is slightly weaker for no3 n and more so for on and tn the poor performance for tn and on which is a large fraction of tn could be due to the model s indirect computation of on unlike nh3 n no3 n no2 n and ip which are directly computed by biokinetic equations table s1 on is estimated as a fixed fraction of cod in the biokinetic model on is assumed to consist of 0 03 mgn mgcod cr i n c r 0 04 mgn mgcod cs i n c s and 0 01 mgn mgcod ci i n c i as represented by the composition parameters in table s2 3 2 simulation results simulation results using the calibrated and validated model are shown in figs 5 and 6 events 1 and 2 and more completely in figs s1 s6 events 1 2 3 4 5 and 6 the initial conditions within the soil media and the influent concentrations that are the inputs to the biokinetic model are shown in table s3 and table s4 respectively the field measured inflow pollutographs from the six events described by wang et al 2017 were taken to be the time dependent inputs to the model the time depth profiles in figs 5 and 6 and s1 to s6 show the predicted variations in soil moisture and biokinetic model components oxygen nitrogen species nh3 no2 no3 n2 and heterotrophic and autotrophic bacteria species xh xans xanb as the storm event progresses from the start of rainfall t 0 hr table 3 shows the measured tn mass inflow at the inlet and compares the measured and predicted tn mass outflows via the overflow culvert and subsurface outlet table s5 shows similar results for other biochemical components stormwater exiting via the overflow culvert receives some physical treatment in the surface basin but only stormwater exiting via the outlet passes through the subsurface basin and receives biological treatment 3 2 1 water quality at the start of an event initial condition 3 2 1 1 oxygen before a storm event the aqueous phase in the upper 40 cm unsaturated zone is assumed to be at equilibrium with the atmosphere and saturated with oxygen at 29 c o2 7 67 mg l oxygen profile in figs 5 and 6 and s1 to s6 in the bottom 35 cm saturated iws internal water storage zone there is almost no free oxygen while bound oxygen in the form of no3 and no2 is present according to ritter s 2013 dry day measurement at the same study site the concentration of dissolved oxygen in the iws was found to be low 1 75 1 72 and 1 55 mg l at depths of 65 cm 75 cm and 85 cm respectively these measured values were imposed as the initial condition for dissolved oxygen in the anoxic iws zone in the model table s3 in the transition zone 40 50 cm depth dissolved oxygen in the medium range of 4 mg l was assumed 3 2 1 2 bacteria since this water quality model focuses on modeling the growth decay dynamics of bacteria the availability of the electron acceptors o2 no2 and no3 for cellular respiration becomes the determining factor for the distribution of bacterial species within the different zones of the soil media nitrifying bacteria xans and xanb are obligate chemolithoautotrophs as well as obligate aerobes which means that these nitrifiers fix co2 and are not able to oxidize nh3 or no2 for energy needs in the absence of oxygen because nitrification ceases to occur at low oxygen levels 2 mg l the bottom 35 cm saturated iws zone becomes uninhabitable for nitrifiers since in this model bacteria are assumed to be immobile and to not advect with the infiltrated flow populations of xans and xanb are limited to the unsaturated aerobic zone throughout the duration of storm events xans and xanb profiles in figs 5 and 6 and s1 to s6 on the other hand denitrifiers a special group of heterotrophs xh are facultative anaerobes autotrophic denitrifiers such as thiobacillus denitrificans are not included in this model this means that denitrifiers are able to respire no2 and no3 whenever o2 becomes unavailable denitrifiers are part of the general population of heterotrophs that is found throughout soil media at all depths the initial heterotroph concentration s xh init mg kg is assumed to be higher in the anoxic iws zone to account for denitrifiers competitive advantage in utilizing no3 and no2 the bacteria concentrations s xh init s xans init and s xanb init mg kg are computed based on literature reports of nitrifying and denitrifying gene copies per gram of soil table s6 and gene copy numbers per genome of bacteria species kandeler et al 2006 3 2 1 3 nitrogen phosphorus cod the initial concentration distributions of nh3 no2 no3 n2 ip cr cs and ci vary from event to event because they are dependent on the extent of biochemical reactions that occurred in the residual water during the dry period since the previous storm event the streamlines in figs 5 and 6 and s1 to s6 show that pollutant mass from the incoming stormflow reaches a variable depth depending on the amount of rainfall rainfall quantities are given in table s7 the pollutants then undergo biochemical transformations during the intervening dry period and stay within the system before being flushed out by infiltrated water during the subsequent event since we are simulating single events only the initial condition could not be assumed to be applicable to all events thus a different initial condition was calibrated for each event using field measurements by ritter 2013 as a guide the anoxic iws zone can also be expected to be enriched with organics and carbon leached from wood chips within the layer degradation and hydrolysis of wood chips would supply the zone with higher concentrations of organics in the form of cod cr cs ci in the model thus a scenario of higher concentration of cod in the anoxic iws zone was imposed in the model to improve the specification of initial conditions in future studies subsurface water quality should be monitored continuously over consecutive events and intervening dry periods 3 2 2 water quality evolution during an event during a storm event influent water infiltrates into the unsaturated zone with a distinct wetting front as can be seen in the time depth profiles for soil moisture θ cm3 cm3 and flow rate q cm hr figs 5 and 6 and s1 to s6 the progress of the wetting front is further illustrated in these figures by red streamlines indicating the flow paths of selected water particles through depth and time the vertical advance of the wetting front in the 1d soil column is the fastest in the first hour after the start of a storm event as shown by the orientation of the velocity vectors the advancing wetting front carries higher concentrations of oxygen and nutrients nitrogen and phosphorus and changes the microenvironment in which bacteria live and act as an event progresses one can see this effect clearly in fig 5b and d by the dramatic contrast between stormwater with high nh3 n and no3 n in yellow above the wetting front and older treated water with low nh3 n and no3 n in blue below the wetting front 3 2 2 1 nitrification the nitrification process is evident in event 2 fig 6 during event 2 from t 6 7 hr and z 1 0 cm to t 20 7 hr and z 37 4 cm i e following the marked streamline in fig 6a the oxygen concentration drops by 23 from 7 45 mg l to 5 72 mg l there are two factors forcing the oxygen level the first factor is an increase due to re aeration from atmospheric oxygen after surface ponding ends at t 15 hr fig s7 the soil moisture saturation decreases from full moisture saturation θ 0 3 in the first soil layer fig 6f and allows for re aeration to occur deeper into the soil column and not just at the surface the second factor is a decrease in the oxygen level due to bacterial utilization of oxygen in cellular respiration during the time between t 6 7 and 20 7 hr in event 2 the rate of oxygen consumption by bacteria for nitrification is higher than the rate of re aeration which leads to a reduction in oxygen level as part of the two step nitrification process nh3 is converted to no2 by nitrosomonas xan s which is then converted to no3 by nitrobacter xan b fig 2 following the pathways marked in fig 6 nh3 fig 6b is reduced by 93 from 0 18 mg n l to 0 012 mg n l which equates to a nh3 reduction rate of 0 012 mg n l hr no2 fig 6c increases by 62 from 0 058 mg n l to 0 094 mg n l which results in a no2 net production rate of 0 0026 mg n l hr no3 fig 6d increases less by 12 from 0 38 mg n l to 0 42 mg n l which is equivalent to a no3 production rate of 0 0031 mg n l hr on average for all six sampled events the average reductions in nh3 and o2 are 33 and 22 respectively while the average increases in no2 and no3 are 280 and 5 respectively the high build up in no2 and the slight increase in no3 might indicate that the first step of the nitrification process which is the conversion of nh3 to no2 as mediated by nitrosomonas xan s is proceeding at a faster rate than the second step which is the conversion of no2 to no3 as mediated by nitrobacter xan b another possible explanation for only a slight accumulation of no3 is that denitrification to nitrogen gas was occurring at a higher rate than the generation of no3 mediated by nitrobacter 3 2 2 2 denitrification in the anoxic iws zone instead of oxygen no2 and no3 are consumed as the final electron acceptors during the anaerobic cell growth of denitrifiers nitrogen gas is produced as the end product two instances of denitrification occurring in the anoxic iws zone are evident in event 1 fig 5 two regions show extensive nitrogen generation mediated by denitrifying bacteria at very low oxygen levels between t 0 and t 12 hr at a depth of about 50 85 cm as well as between t 14 to t 20 hr at a depth of 50 60 cm in the first region i e following the left marked streamline in fig 5c no2 decreased by 87 from 0 59 to 0 078 mg l no3 decreased by 55 from 0 40 to 0 18 mg l fig 5d while n2 increased by 64 from 1 0 to 1 7 mg l fig 5e and o2 remained low at 1 48 mg l fig 5a in the second region i e following the right marked streamline in fig 5c no2 decreased by 98 from 0 15 to 0 0024 mg l no3 decreased by 64 from 1 3 to 0 46 mg l fig 5d n2 increased by 88 from 0 98 to 1 8 mg l fig 5e and o2 remained low at 0 49 mg l fig 5a in the zone following the marked streamline in fig 5a oxygen saturated water in the unsaturated zone between 0 and 40 cm is advected downwards below the infiltrated inflow and reaches the iws zone along this oxygenated pathway denitrification is prevented from occurring this phenomenon implies that even within the anoxic iws zone where denitrification is expected to occur under normal condition oxygen first needs to be depleted before denitrification can proceed it is thus important for basins to be designed to have enough storage volume to provide adequate residence time for oxygen depletion and denitrification 3 3 model sensitivity effect of parameter values on predictions 3 3 1 sensitivity of the model to bacterial kinetic parameters μ b as this biokinetic model was developed using parameters from the activated sludge model from the field of wastewater treatment henze et al 2000 some parameters need to be adjusted for stormwater applications composition and stoichiometric parameters in table s2 are left unaltered from the original asm values as they describe universal fractions of nitrogen and phosphorus in biomass cr cs and ci however bacterial growth μ and decay b rates are less likely to be universal and thus were evaluated through a sensitivity analysis literature values for growth rates of various bacteria species including nitrifiers and denitrifiers vary considerably table s8 and their range was used to set high medium and low levels of growth rate μ h μ dn μ ans μ anb and decay rate b h b ans b anb for sensitivity analysis table s9 the literature values for growth rates are often based on controlled laboratory studies of a specific bacteria strain growing on specific culture media with abundant nutrients for growth however in stormwater treatment nutrient concentrations are lower presuming literature values reflect ideal conditions they were deemed high rates for bacterial growth μ h 19 2 day 1 μ dn 3 84 day 1 μ ans μ anb 3 24 day 1 and cell lysis b h b ans b anb 1 day 1 in the sensitivity analysis medium rates are the calibrated and validated values in this study μ h 1 5 day 1 μ dn 1 day 1 μ ans μ anb 0 75 day 1 b h b ans b anb 0 1 day 1 while the low rates are set to about an order of magnitude less μ h μ dn 0 1 day 1 μ ans μ anb 0 01 day 1 b h b ans b anb 0 01 day 1 time depth profiles of modeled concentrations of o2 nh3 no3 no2 and n2 are compared between high medium and low rates in fig 7 the comparison between modeled and observed mean outlet concentration is quantified statistically in table s10 a comparison of the left hand column high rates with the middle column medium rates in fig 7 shows that the onset of denitrification is earlier when biokinetic rates μ h μ dn μ ans μ anb b h b ans b anb are high with both high and medium rates denitrification occurs from t 4 hr at z 10 cm to t 20 hr at z 50 cm during this time no3 and no2 are reduced and n2 is generated spatially the upper unsaturated zone is where oxygen dependent processes such as nitrification occur under normal conditions i e at medium biokinetic rates however in a scenario with high biokinetic rates this zone becomes so deficient in oxygen that denitrifiers facultative anaerobes instead utilize no3 and no2 for cellular respiration this occurs because the dissolved oxygen in the infiltrated water is rapidly used up by intensified biological reactions o2 ip and nh3 are consumed by aerobic cell growth reactions rc2 rc6 and rc8 in fig 2 as are no3 and no2 by denitrification rc3 and rc4 ip and nh3 increase at later hours t 14 20 hr at z 50 cm due to another high rate biological activity cell lysis b h b ans b anb the combined effects of the different processes are an increase in nh3 and ip and a decrease in no3 and no2 in statistical terms as shown in table s10 pbias values are more negative for the high rate case for nh3 and ip 192 and 671 indicating higher predicted concentrations than for the medium rate case for no3 and no2 pbias values are positive 65 and 77 indicating lower concentrations for the high rate case than for the medium rate case fig 7 shows that the converse is true when biokinetic rates are low lower biological activity results in insignificant nitrification and denitrification and thus insignificant changes in pollutant concentrations and removal rates 3 3 2 sensitivity of the model with respect to initial bacteria concentration sx the effects of different levels of bacteria concentrations are similar to those caused by increasing bacterial kinetic parameter values as described in the previous section in particular earlier onset of denitrification occurs with high initial bacterial count due to increased no3 and no2 removal low initial bacterial counts imply little biological activity and thus insignificant changes in pollutant concentrations details can be found in the supplementary materials 3 4 model application predicting tn removal at various basin configurations performance of a bioretention basin is often assessed in terms of its ability to remove pollutants such as total nitrogen tn bioretention basins are typically designed with removal targets of 30 65 for tn brisbane city council 2006 department of irrigation and drainage 2011 dierkes et al 2015 mdep 2008 mpca 2008 nhdes 2008 pub 2014a currently design guidelines in singapore are modeled after those of temperate areas primarily australia fawb 2009 under the regulations of singapore s public utilities board pub 2014b basins in singapore must be designed to accommodate a critical design flow rate for a 3 month average recurrence interval ari event and water quality volumes wqvs determined using the music model model for urban stormwater improvement conceptualization crcch 2003 the removal rates in the music generated design curves are yet to be field validated in the tropical context due to a lack of field scale studies nonetheless singapore s design guidelines assume that target removal rates will be achieved as long as the basin configurations and wqvs are determined from the music generated design curves this study provides alternative performance curves for the tropics by using the water quality model developed here to make predictions of pollutant tn removal potential with various basin configurations and wqvs however the model developed in this study has only been validated against individual events and single storms do not necessarily show how well a basin performs in terms of pollutant removal over longer time periods therefore to predict the pollutant removal potential across events of varying sizes and intensities over a half year period an extension from single event simulation to long term simulations was made to evaluate performance over multiple events we completed a hydrological numerical simulation of 80 storms observed during the six month period monitored by wang et al 2016 we lack a complete set of water quality data for these storms and therefore conservatively assumed that the pollutograph for each storm was the same as that measured during event 1 in the study by wang et al 2017 the runoff during event 1 carried the highest emc event mean concentration among the six events sampled in our study accordingly the performance curves generated with the event 1 pollutograph can be used to generate presumably conservative designs the performance curves resulting from the half year water quality simulation are shown in fig 8 a and b the bioretention basin performance in terms of half year tn removal tn r is plotted as a function of three engineering parameters detention depth h d cm ratio of drainage area to bioretention area r and saturated hydraulic conductivity k s cm hr pei performance efficiency index fig 8c and d was developed to illustrate the incremental gain in tn r given a unit increase in either h d or k s 14 p e i d t n r d h d or d t n r d k s 3 4 1 detention depth hd the detention depth h d is the maximum depth of storage in the surface portion of the bioretention basin and controls the portion of the influent stormwater that is captured a shallower pond cannot hold as much stormwater as a deeper pond and thus passes more stormwater out the overflow culvert without biological treatment unsurprisingly fig 8a shows that an increase in h d and thus an increase in the portion of stormwater captured leads to more complete nitrogen removal fig 8a shows that at the current bioretention basin area r 70 tn r increases from 26 to 35 as h d increases from 10 to 70 cm this increase in tn r is non linear with a greater rate of increase seen at lower h d and gradually slower rate of increase at higher h d values this is simply a reflection of the fact that a 10 cm increase in depth represents a larger increase in storage relative to the lesser volume of a shallow pond than to the greater volume of a deeper pond the non linear slope is also seen in the pei plot fig 8c in which a higher pei 0 39 is achieved at h d 10 cm than at h d 70 cm pei 0 05 the plots show that the rate of improvement in tn r drops significantly with increases in detention depth h d beyond a certain point r is the ratio of the contributing drainage area to the surface area of the bioretention pond ponds with greater surface area and thus storage capacity have smaller r values fig 8a shows that as the surface basin is enlarged and r is decreased nitrogen removal improves just as deeper ponds capture and biologically treat a greater portion of the influent stormwater and thus improve overall nitrogen removal so do ponds of greater area while either deepening the pond or enlarging the pond can improve performance there are practical limits to both deeper ponds are less safe for the public but there may not be space available for a larger area pond thus both h d and r must be treated as independent design parameters fig 8a also shows that the incremental gain in tn r resulting from an increase in detention depth h d is greater for larger basin area smaller r reflecting the greater increase in overall storage from deepening a large pond versus deepening a small pond for instance a similar increase in h d from 10 to 80 cm results in an increase of 10 16 19 and 19 in tn r for basin area ratios of r 70 35 23 3 and 17 5 respectively a smaller basin area needs disproportionately higher detention depth h d to achieve the tn removal target of 45 required in singapore pub 2014a design curves in fig 8a show that for decreasing basin area ratios r 17 5 23 3 and 35 0 the detention depth required to achieve tn r 45 are h d 12 18 and 45 cm respectively the corresponding values of wqv range from 240 to 360 m3 and those of wqd water quality depth from 14 to 22 mm at the current basin area r 70 the balam basin is projected to be unable to meet the tn removal target of 45 with h d 80 cm fig 8a although different basin surface storage volumes h d x a could equally meet a tn removal target rate of 45 the extra subsurface volume created when the basin surface area is increased leads to more tn removal than if the extra surface volume is created only by greater detention depth since denitrification in the soil filter media is the main driver of nitrogen removal over the inter event period the larger subsurface storage volume beneath a larger basin area induces a disproportionate increase in tn removal compared to a smaller basin area 3 4 2 saturated hydraulic conductivity ks the rate at which the subsurface portion of the bioretention basin receives stormwater depends upon k s the saturated hydraulic conductivity of the surficial sandy loam layer this is the layer with the lowest hydraulic conductivity and which thus controls the flow through the subsurface basin fig 8b shows the effect of varying saturated hydraulic conductivity k s on tn removal tn r as k s increases from 0 1 to 5 cm hr tn r increases sharply indicating a rise in tn removal as more stormwater can pass through the subsurface treatment layers however tn r decreases rapidly from k s 5 10 cm hr and gradually from k s 10 60 cm hr the best basin performance in terms of tn removal is seen when k s 5 cm hr poorer performance at low k s is consistent with a reduction in the volume of infiltrated influent and hence less subsurface treatment poor removal at high k s is consistent with too rapid drainage of infiltrated influent and insufficient hydraulic residence time for treatment within the system the superior performance at k s 5 cm hr is also reflected in the pei plot fig 8d pei is most negative at k s 5 cm hr indicating peak performance at that value therefore increasing k s beyond 5 cm hr even though it would allow more water to be infiltrated would not improve overall treatment the optimal value of k s represents a trade off between more capture a hydrologic phenomenon and more treatment a biokinetic phenomenon only a model of both hydrology and biokinetics can represent this trade off 3 4 3 recommended basin design values the performance curves presented in fig 8a and b are appropriate for use by bioretention basin designers based on the analysis presented above we make the following recommendations for bioretention basin design 1 increasing retention depth h d improves treatment but only to a point the incremental benefit of tn r drops as h d increases beyond h d 30 cm such that further changes in this engineering parameter provide little improvement 2 hydraulic conductivity of k s 5 cm hr achieves the best balance between maximizing the volume of stormwater infiltrated and ensuring adequate treatment it is important that k s not be so high as to cause inadequate hydraulic residence time for the infiltrated stormwater to receive biochemical treatment for nitrogen removal before it is flushed out of the system 4 conclusion a numerical water quality model has been built to simulate subsurface bacteria mediated biochemical processes occurring in bioretention basins we integrated this biokinetic model which follows the formulation of the cw2d model with the recharge model that we previously adapted to model the hydrology of our bioretention field site wang et al 2019 1 this study has demonstrated that the mathematical formulations originating from the activated sludge model asm used in wastewater treatment are also applicable for modeling the role that bacterial growth and decay play in stormwater treatment in bioretention basins however some biokinetic parameters need to be calibrated for application to bioretention basins the model was successfully calibrated and independently validated against field data collected during an eight month monitoring period wang et al 2017 2019 2 simulation results based on calibrated and validated parameter values show that the build up of no2 and only slight increase in no3 during the roughly 20 hr of a storm event indicate that the first step of the nitrification process the conversion of nh3 to no2 mediated by nitrosomonas is proceeding at a faster rate than the second step the conversion of no2 to no3 mediated by nitrobacter 3 a sensitivity analysis of bacterial kinetic parameters μ h μ dn μ ans μ anb b h b ans b anb and initial bacteria concentrations s xh s xans s xanb indicates that the model is sensitive to both for instance the onset of denitrification will be earlier when either bacterial kinetic rates or initial bacteria concentrations are higher these parameter settings need to be better defined to ensure accurate simulations of real scenarios for that future studies need to focus on quantifying depth dependent microbiological activity occurring over time in the subsurface of bioretention basins 4 well defined initial conditions are important for an accurate short term event based simulation to improve the calibration of initial pollutant concentrations across different soil depths there is a need to continuously monitor water quality over consecutive events a more intensive pre and post event sampling program over a longer time duration would give a better representation of the initial conditions at the start of each event 5 the developed water quality model was used to evaluate total nitrogen removal tn r with various bioretention basin configurations detention depth h d catchment area to basin area ratio r and saturated hydraulic conductivity k s over a six month simulation period because the model simulates both basin hydrologic performance and biological treatment performance it successfully captures how basin design affects overall performance the surface volume characteristics h d and r affect the amount of stormwater captured by the surface basin while the subsurface hydraulic conductivity k s affects both the amount captured in the surface basin and the degree of biological treatment received in subsurface part of the basin the simulation results show that the rate of improvement of tn r decreases as h d increases beyond h d 30 cm such that further increase in this engineering parameter becomes less cost effective for k s a maximum value of tn r is found for k s 5 cm hr with the current basin configuration the estimated tn r is 28 which falls short of a target rate of 45 the performance curves developed using the model are useful as a reference for future bioretention basin design in the tropics name of software nitrobiorem description nitrobiorem is a matlab based model that simulates monod type biological degradation kinetics of nitrogen species in bioretention basins flow outputs from the hydrodynamic model recharge are coupled with nitrobiorem and together the two codes constitute a biokinetic model of 1d reactive transport it is intended to predict temporal accumulation and removal of nitrogen species spatially in a bioretention basin this model can be used to verify the appropriateness of design guidelines for bioretention basins by evaluating the response of the system to different design parameters developer jia wang jiawang alum mit edu peter shanahan peteshan mit edu lloyd h c chua lloyd chua deakin edu au funding sources singapore mit alliance for research and technology smart s center for environmental sensing and modeling censam national research foundation prime minister s office singapore software required matlab supported systems windows year first available 2017 availability the model code and data used in this study are located at https doi org 10 5281 zenodo 4665741 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the national research foundation prime minister s office singapore through the singapore mit alliance for research and technology s center for environmental sensing and modeling research program the authors thank alejandro r dussaillant jones for providing the original recharge codes appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105212 
25703,the bioretention basin is a type of green infrastructure that treats stormwater runoff however there is a limited understanding of its treatment potential with different design configurations and limited capability to model bioretention systems current modeling practice fails to represent bacteria mediated biochemical reactions that remove nitrogen this study provides a reaction based model of water quality treatment processes using multiple bacterial monod kinetics under variably saturated flow conditions to predict the dynamics of nitrogen and organics in a bioretention system the model was calibrated and validated against concentrations measured during an eight month field monitoring program because the model simulates both basin hydrologic performance and biological treatment performance it can be used to verify design guidelines by evaluating the response of the system to different design parameters the model shows that nitrogen removal is enhanced by increasing bioretention basin volume and by setting an optimal value of hydraulic conductivity for subsurface infiltration graphical abstract image 1 keywords rain garden bioretention best management practices stormwater water quality models calibration 1 introduction current bioretention basin modeling efforts both hydrological and water quality models have been limited in terms of hydrological models recarga atchison and severson 2004 dussaillant et al 2003 and recharge dussaillant et al 2004 wang et al 2019 the more complex version of recarga are the only existing models that focus specifically on bioretention basins in terms of water quality models there has been no biochemical process based system scale mechanistic model specific to bioretention basins although gifmod massoudieh et al 2017 models a variety of green infrastructure including bioretention basins widely used models such as swmm storm water management model huber et al 1988 rossman 2004 sustain system for urban stormwater treatment and analysis integration model lai et al 2006 2009 shoemaker et al 2009 winslamm source loading and management model for windows myllyoja et al 2001 pitt and voorhees 1995 p 8 program for predicting polluting particle passage through pits puddles ponds walker 2007 and music model for urban stormwater improvement conceptualization wong et al 2002 2006 rely on approximate representations of biochemical processes e g percentage removal rather than on biokinetic models of microbiological reactions these models lack process based water quality formulations for bioretention basins and are thus not able to predict effluent quality accurately based on process understanding instead pollutant removal by each type of bmp is modeled as a removal or first order decay process in which the bmp is a black box with all reaction processes lumped within a single percentage removal coefficient or first order decay constant there is a growing body of literature devoted to understanding the main processes leading to pollutant removal in bioretention basins these studies seek to find process based alternatives to modeling the bioretention basin as a black box process based modeling is challenging because it must capture not only physical processes such as sedimentation and filtration that are commonly included in current models but also chemical and biological mechanisms that mutually influence each other and are simultaneously active with physical processes in fact chemical and biological mechanisms play a key role in nutrient and pathogen removal for instance organic nitrogen removal is a bacteria mediated step wise biochemical process first through ammonification that converts organic nitrogen to ammonia next via nitrification that converts ammonia to nitrate and finally through denitrification that converts nitrate and nitrite to nitrogen a detailed mechanistic breakdown is necessary to track transformation and transport within each zone of the soil filter media models that take all of these biochemical processes into account will give a more complete picture of the complex processes that occur in bioretention basins the advantage of a process based model is that the relative contributions of specific processes and design elements to overall pollutant removal can be quantified through sensitivity analysis which is helpful in basin design for optimal pollutant load reduction current water quality models used for bioretention basins focus mainly on pollutants that are removed primarily by physical processes for instance li and davis 2008a developed a one dimensional model that represents filtration and capture of total suspended solids other analogous models look at filtration and adsorption of particulate and dissolved heavy metals as well as the fate of polycyclic aromatic hydrocarbons in bioretention basins he and davis 2009 li and davis 2008b recent studies have also focused on understanding nitrogen speciation and fate in bioretention basins li and davis 2014 and quantification of the nitrifying and denitrifying bacteria that are responsible for nitrogen removal chen et al 2013 a good starting point in developing a process based model for bioretention basins is to look at systems such as constructed wetlands cws in which similar biochemical transformations take place the microbial community including nitrifiers and denitrifiers that are responsible for nitrogen removal in cws play an equally key role in bioretention basins extensive research on process based numerical models of varying complexity has been conducted for cws one group of models uses the richards equation for variably saturated flow langergraber and šimůnek 2005 ojeda et al 2008 wanko et al 2006 for example cw2d constructed wetlands 2d couples the flow model hydrus with multiple monod type kinetic equations to create a multi component reactive transport model of subsurface flow constructed wetlands but does not simulate associated surface water bodies langergraber and šimůnek 2005 langergraber and šimůnek 2006 šimůnek et al 1998 šimůnek and van genuchten 2006 saeed and sun 2011 recommend that models for wetland systems be developed based on multiple monod kinetics instead of first order or single monod kinetics to predict the removal of nitrogen and organics in order to better evaluate how bioretention basin design affects treatment performance and hence explore management options for solving pollution problems it is imperative to develop a process based model that can be used to make reliable predictions and assist in formulating appropriate controls therefore the objectives of this study were to 1 develop a model for bioretention basins that adds modules for subsurface bacteria mediated nitrogen conversion and removal to an existing flow model for bioretention basins recharge dussaillant et al 2004 wang et al 2019 2 use the verified model to test sensitivity to model parameters including bacterial kinetic parameters and bacteria concentrations 3 develop bioretention design guidelines by applying the model to various hypothetical basin configurations detention depth catchment to basin area ratio and saturated hydraulic conductivity to evaluate basin performance in terms of nitrogen removal 2 methods 2 1 study site and field data for calibration and validation fig 1 shows a cross section of the balam estate bioretention basin in singapore that was the location of this study with an effective treatment area of 240 m2 the basin receives runoff from a 16 800 m2 residential catchment about 88 surface imperviousness with a time of concentration of about 10 min wang et al 2017 wang et al 2009 basin inflow infiltrates through four soil layers consisting of from surface to bottom a 40 cm sandy loam filter layer a 10 cm fine sand transition layer a 30 cm saturated anoxic zone of hard rocks average size of 50 mm and 65 by volume and wood chips average size of 5 mm and 35 by volume and a 10 cm drainage layer of fine gravel average size of 2 5 mm which also remains saturated and anoxic ong et al 2012 wang et al 2009 the entire soil filter media is underlain by a high density polyethylene hdpe liner to prevent exfiltration into the surrounding soil treated runoff is collected by 10 cm underdrains in the drainage layer that discharge to an outlet box that in turn discharges to a 30 cm outlet pipe to an adjacent surface drain the underdrains are at a lower elevation than the outlet pipe thereby creating a perennially saturated anoxic zone favoring denitrification if the basin water level exceeds a depth of 13 7 cm untreated water from the basin overtops a weir and discharges to a 1 1 m high by 1 2 m wide surface overflow culvert if basin water ponds to still higher levels it can also discharge via a 1 m square opening at the top of the outlet box and four 10 cm wide slots on the sides of the outlet box which in turn discharges to the 30 cm outlet pipe the outlet box was included in the design of the basin for flood control but the openings were sealed for our study in order to isolate the flow due to infiltration hydrologic measurements and water quality samples used in this study were obtained from a real time monitoring system that consisted of flow and water level measurement equipment operated continuously and automatic water quality samplers to sample six selected storm events installed from april to november 2013 at balam estate rain garden wang et al 2016 2017 provide a full description of the sample collection scheme and data collection and analysis influent water quality varied with the event mean concentration emc of total nitrogen ranging from 0 58 to 5 1 mg l the latter occurring during a period of unusually poor air quality wang et al 2017 2 2 hydrological model wang et al 2019 describe in detail modeling of the hydrological behavior of the bioretention basin using the recharge hydrologic model in the recharge model the variably saturated subsurface flow is simulated with a one dimensional mixed form of the richards equation which models both saturated and unsaturated conditions in the basin during wet and dry periods dussaillant et al 2004 under the mixed formulation of the richards equation θ is a function of both h and z the governing equation is 1 θ h z t θ h h t z k h z h z 1 s h z where θ soil moisture content cm3 cm3 t time hr z vertical depth cm h pressure head cm k unsaturated hydraulic conductivity cm hr and s evapotranspiration rate hr 1 the van genuchten mualem equations are used in recharge to define the relationships among h θ k and θ h and estimate soil properties assuming that there is no hysteresis van genuchten 1980 in addition to modeling subsurface flow recharge models the filling and emptying of the surface basin through a water balance which predicts the time dependent depth of water ponded in the surface basin 2 a d h s d t q r a i n q i n f l o w q o q c where a bioretention basin area m2 q rain m3 hr direct rain onto the basin q inflow m3 hr catchment runoff entering the basin q o m3 hr water infiltrated into the soil media and out via the pond outlet and q c m3 hr basin surface overflow via the pond culvert to compute q o the darcy buckingham law for unsaturated conditions is used equation 3 3 q o k h z 0 h z 1 in the recharge model equation 1 is discretized using a crank nicholson finite difference numerical scheme and solved using the thomas algorithm with a modified picard iteration for mass balance dussaillant et al 2004 to improve the computation speed and convergence of the model by minimizing finite difference discretization truncation error a variable time stepping scheme dussaillant et al 2004 is adopted with an initial δt of 30 s the minimum and maximum δt are 10 6 hr and 1 min respectively the outputs of the recharge model include vertical water flow and soil moisture content as a function of time which are coupled with biochemical concentrations as described in section 2 3 3 below 2 3 biokinetic model 2 3 1 conceptual model the formulations in constructed wetlands 2d cw2d langergraber and šimůnek 2006 have been adopted to model bacteria mediated biochemical transformation and degradation processes for nitrogen and other constituents in this paper fig 2 summarizes the nine biochemical reactions rc that are responsible for nitrogen removal hydrolysis of chemical oxygen demand cod rc1 aerobic growth of heterotrophs rc2 growth of heterotrophic denitrifiers on no3 rc3 growth of heterotrophic denitrifiers on no2 rc4 lysis of heterotrophs rc5 aerobic growth of nitrosomonas on nh4 rc6 lysis of nitrosomonas rc7 aerobic growth of nitrobacter on no2 rc8 and lysis of nitrobacter rc9 fig 2 shows that organic nitrogen on the starting material that enters the bioretention basin is modeled as part of cod which in turn consists of readily biodegradable cod cr slowly biodegradable cod cs and inert cod ci either through the hydrolysis of cod or cell lysis of heterotrophs xh and autotrophs xans and xanb on is mineralized and converted to nh4 during ammonification nh4 could be incorporated back into bacteria cells as part of aerobic growth for heterotrophs together with inorganic phosphorus ip or po4 3 in the presence of o2 alternatively nh4 undergoes nitrification which is modeled as a two step aerobic process from nh4 to no2 as mediated by autotrophic bacteria nitrosomonas xans and then from no2 to no3 as mediated by autotrophic bacteria nitrobacter xanb nitrification occurs in the presence of o2 which is the preferred electron donor in the absence of free o2 but the presence of bound oxygen no2 and no3 anoxic condition as defined by weißbach et al 2017 no2 and no3 replace o2 as the next thermodynamically preferred electron acceptors that can be utilized by heterotrophic denitrifiers such as paracoccus denitrificans and various pseudomonads organic matter cod in this case acts as the electron donor and the denitrification product is molecular n2 which is lost from the bioretention system and enters into the atmosphere table 1 shows the anticipated zones of primary residence for the different bacteria species that utilize different sets of electron donors and electron acceptors to generate different final products as the presence of o2 determines the type of growth that bacteria undergo it would be expected to directly affect the abundance of bacteria species in the different zones of soil filter media in bioretention basins autotrophic bacteria nitrosomonas and nitrobacter are expected to reside mainly in the upper unsaturated zone of sandy loam deeper into the soil media and approaching the saturated anoxic zone of internal water storage iws the number of autotrophs is likely to decrease heterotrophic denitrifiers in contrast are facultative anaerobes likely to reside throughout the entire soil media this zonation has been confirmed in the field by castellano hinojosa et al 2018 and zheng et al 2017 2 3 2 mathematical formulation of reaction rates the cw2d model langergraber and šimůnek 2006 considers the interactions among twelve components o2 cr cs ci xh xans xanb nh4 no2 no3 n2 ip in nine processes rc1 rc9 its mathematical structure is based on that of the activated sludge models asms in which these nine bacteria facilitated transformation processes are modeled using monod type bacterial growth and decay equations henze et al 2000 the overall reaction rate term d c i d t mgi l hr for component i follows the gujer matrix structure as shown in equation 4 4 d c i d t θ j 1 r v j i r c j where c i is the aqueous concentration θ m3 w m3 s is the volumetric water content v ji mgi mgj is the stoichiometric factor for component i 1 n and process j 1 r and rc j mgj l hr is the monod type kinetic reaction rate for process j in the aqueous phase the reaction rates d c i d t are the resultant equations from element by element multiplications of v ji and rc j for each of the twelve components modeled as listed in table s1 the stoichiometric matrix v ji consists of composition parameters which describe the nitrogen and phosphorus content in cod and bacteria biomass and stoichiometric parameters which describe the generation of cod from hydrolysis and bacteria lysis as well as the bacteria biomass yield coefficients from the consumption of cod therefore v ji represents the extent to which a given microbial mediated process rc j affects the concentration of component i ci the multiple monod type kinetic reaction rate rc j for modeling bacterial growth and decay follows this general structure 5 r c j g r o w t h μ x m a x c s 1 k s 1 c s 1 k i n h s 2 k i n h s 2 c s 2 c n h 4 k x n h 4 c n h 4 c i p k x i p c i p ρ θ s x r c j d e c a y b x c x where x xh xdn xans xanb represent respectively heterotrophic bacteria in general heterotrophic denitrifiers autotrophic nitrosomonas and autotrophic nitrobacter μ x max is the maximum specific growth rate of bacterium x c s1 mg l is the concentration of the limiting substrate s1 for growth k s1 mg l is the half saturation coefficient for substrate s1 c s2 mg l is the concentration of the limiting substrate s2 that inhibits growth k inh s2 mg l is the half inhibition coefficient for substrate s2 c nh4 mg l is the concentration of the limiting substrate ammonium c ip mg l is the concentration of the limiting substrate inorganic phosphate k x nh4 mg l is the half saturation coefficient for ammonium k x ip mg l is the half saturation coefficient for inorganic phosphate ρ is the soil bulk density kgs dms 3 θ is the volumetric water content dmw 3 dms 3 s x is the concentration of bacteria in the solid phase mg kgs c x mg l is the concentration of bacteria in the aqueous phase and b x 1 hr is the bacteria decay rate the term μ m a x c s 1 k s 1 c s 1 represents the specific growth rate of bacteria utilizing substrate s1 for cellular respiration the term k i n h s 2 k i n h s 2 c s 2 represents the inhibitory effect on the growth of bacteria caused by substrate s2 the term c n h 4 k x n h 4 c n h 4 c i p k x i p c i p represents the potential for an insufficiency of either of the limiting nutrients nh4 and ip to reduce the rate of growth these terms are multiplied together as more than one nutrient or growth factor has the potential to limit the growth rate of bacteria the cw2d model relies on a number of assumptions langergraber and šimůnek 2006 all reactions are assumed to occur in the aqueous phase and all components except bacteria are soluble and mobile bacteria associate exclusively with the solid phase and their lysis i e decay rate is set as a constant and does not depend on environmental conditions moreover following the asm models it is assumed that ph rate equation coefficients and stoichiometric factors are constant table s2 cw2d defines kinetic parameters in rc j μ x max k s k inh s and b y using literature values taken from studies of wastewater treatment based on the presumption that municipal wastewater is being treated in the cw however since this study addresses treatment of stormwater the kinetic parameters are set by calibration instead 2 3 3 biokinetic modeling since the twelve biokinetic components are interdependent in a complex manner e g the product of one reaction becomes the reactant of another reaction their time varying concentrations must be evaluated simultaneously at each spatial and temporal step this was done by coding in matlab a model of multiple monod type biological degradation kinetics that employs the outputs from the hydrodynamic model in recharge as shown in fig 3 together the two codes constitute a biokinetic model of 1d reactive transport the advection of influent through the 1d column of soil filter media is computed on a 1 cm spatial grid in recharge two outputs from recharge that become inputs to the biokinetic model are the time evolution of flow rate q z cm hr between two neighboring spatial cells and the soil moisture θ z m3 w m3 s within each spatial cell reaction within each spatial cell is assumed to be uniform that is each cell is modeled as a continuously stirred tank reactor cstr thus the 1d column of soil media can be considered as a vertical series of cstrs at any time step the mass balance for each component i in a single cell cstr reactor at each time step can be expressed as 6 d c i z d t 1 τ c i p r e z 1 τ c i p o s t z where τ is the hydraulic retention time within a single cell cstr reactor c i pre z is the concentration of component i before undergoing simultaneous biokinetic reactions and is calculated according to equation 4 d c i z d t is the rate of change of the concentration of component i in the current spatial cell as governed by the set of monod type kinetics equations in table s1 c i post is the concentration of component i of the single cell cstr reactor z after undergoing the simultaneous biokinetic reactions and also the influent concentration to the next spatial cell z 1 within the same time step and c i pre z is the concentration of component i before undergoing simultaneous biokinetic reactions and is calculated for each time step according to equation 7 7 c i p r e z c i i n i t z θ z d z c i i n f z q z d t θ z d z q z d t the concentration of component i before undergoing biokinetic reactions c i pre is estimated by summing the influent mass c i i n f z q z d t and initial mass c i i n i t z θ z d z within the single cell cstr reactor z as shown in equation 7 the concentration c i p r e is a virtual concentration used for computational convenience in the actual system mass inflow and reactions occur simultaneously at the end of the spatial stepping from the first spatial cell at the surface to the last cell at the bottom of the bioretention basin c i p o s t z is the initial concentration for the next time step as shown in fig 3 the process is iterated until the end of the simulation time the time series for effluent concentration is that of the bottom cstr 2 3 4 model evaluation indices to evaluate the model s predictive capability six indices are used coefficient of determination r 2 nash sutcliffe efficiency nse nash and sutcliffe 1970 mean absolute error mae relative root mean square error rrmse ratio of root mean square error rmse to standard deviation of observations rsr and percent bias pbias for the mean outlet concentration according to equations 8 to 13 khan et al 2013 moriasi et al 2007 woznicki and nejadhashemi 2012 8 r 2 i 1 n o i o m i m 2 i 1 n o i o 2 i 1 n m i m 2 9 n s e 1 i 1 n o i m i 2 i 1 n o i o 2 10 m a e i 1 n o i m i n 11 r r m s e r m s e μ m i i 1 n o i m i 2 n m 12 r s r r m s e σ o i i 1 n o i m i 2 i 1 n o i o 2 13 p b i a s i 1 n o i m i i 1 n o i 100 where o i m i o and m are the observed modeled mean of observed and mean of modeled values respectively n is the number of comparisons made between observed and modeled values these indices were chosen for model evaluation as they represent three major categories of model evaluation techniques standard regression dimensionless and error index moriasi et al 2007 2015 standard regression such as r 2 determines the strength of the linear relationship between modeled and measured data r 2 describes the extent to which the model explains the variance of the observed data it ranges from 0 to 1 the higher the value the smaller the error variance and the stronger is the collinearity between modeled and observed values a value of r 2 0 50 is considered acceptable santhi et al 2001 the dimensionless nash sutcliffe efficiency nse shows how well a plot of observed against modeled values fits the 1 1 line by comparing the relative magnitude of the residual variance noise to the measured data variance information nash and sutcliffe 1970 the nash sutcliffe efficiency ranges from to 1 with 1 being the optimal value and values 0 0 indicating that the observed mean value is a better predictor than the modeled value values above 0 0 are generally taken as an acceptable level of performance while moriasi et al 2015 recommend for watershed scale models to predict nitrogen load the following criteria nse 0 35 0 5 and 0 65 for satisfactory good and very good performance respectively the criteria developed by moriasi et al for watershed models are an imperfect fit with the model evaluated here but nonetheless provide a useful touchstone error indices such as mae rrmse rsr and pbias quantify the error in the units of the observed values the indices rrmse and rsr modify rmse based on scaling factors that are the modeled mean and the observed standard deviation respectively these indices range from 0 to with the optimal value being zero moriasi et al 2007 recommend rsr 0 70 0 60 and 0 50 for satisfactory good and very good watershed model performance respectively the pbias error index shows the tendency of the modeled data to be higher or lower than the observed values the index has an optimal value of 0 with positive values indicating that the model tends to underestimate and negative values indicating that the model tends to overestimate green and stephenson 1986 gupta et al 1999 moriasi et al 2007 recommend pbias 30 20 and 15 for satisfactory good and very good model performance respectively for watershed models 3 results and discussion 3 1 model calibration and validation the six storm events sampled during the monitoring program described by wang et al 2017 were used for model calibration and validation these six events consist of two events with high event mean concentration emc events 1 and 3 tn 5 09 and 4 02 mg l and four with lower emc events 6 5 4 2 tn 1 43 1 24 0 81 0 58 mg l influent emc is given more emphasis than rainfall depth in model calibration and verification since emc has a direct effect on biokinetic reaction rates both the high and low emc events were separated into two datasets one for model calibration events 1 4 6 and one for model validation events 2 3 5 fig 4 a and b show results of simulations of the calibration events using the calibrated parameter set table s2 through table s4 the observed values are shown against the modeled values on a 1 1 line for the event mean outlet concentration and removal rate of six water quality parameters nh3 n no3 n no2 n on tn and ip the model evaluation statistics r 2 nse mae rrmse rsr pbias for each of these water quality parameters are shown in table 2 model performance with the calibrated parameter set in the validation events is similarly shown in fig 4c and d fig 4a and c show generally good agreement between the observed and modeled mean effluent concentration values in the calibration and validation events for all water quality parameters except on for the calibration events shown in fig 4a with the exception of on the model evaluation statistics achieved r 2 0 90 nse 0 81 mae 0 12 mg l rrmse 0 54 rsr 0 44 and pbias 34 table 2 the values of nse and rsr exceed the thresholds from moriasi et al 2007 for very good watershed model evaluation performance nse 0 65 and rsr 0 50 the correlation found in the calibration events is better than those found by saeed and sun 2011 for a multiple monod cstr model for a vertical flow subsurface wetland r 2 0 7 rrmse 1 0 for the validation events shown in fig 4c the correlation between modeled and observed values is strong with r 2 0 93 and error deviation is small with nse 0 76 mae 0 03 mg l rrmse 0 38 rsr 0 49 and pbias 18 table 2 negative values of pbias in table 2 indicate that the modeled values are greater than the observed values this is evident in fig 4a and c as more data points lie above the bisector line than below fig 4b and d show the removal rates of the six water quality parameters nh3 n no3 n no2 n on tn and ip as an additional indicator of model predictive performance for both the calibration and validation sets a strong correlation is seen between the modeled and observed removal rates as the data points lie close to the bisector line poorer correspondence is seen in the calibration for on although the r 2 value of 0 80 for on in the calibration set indicates acceptable agreement between modeled and observed values the other indices show poorer correspondence nse 4 92 rsr 2 30 pbias 76 similarly for the validation set model performance is slightly weaker for no3 n and more so for on and tn the poor performance for tn and on which is a large fraction of tn could be due to the model s indirect computation of on unlike nh3 n no3 n no2 n and ip which are directly computed by biokinetic equations table s1 on is estimated as a fixed fraction of cod in the biokinetic model on is assumed to consist of 0 03 mgn mgcod cr i n c r 0 04 mgn mgcod cs i n c s and 0 01 mgn mgcod ci i n c i as represented by the composition parameters in table s2 3 2 simulation results simulation results using the calibrated and validated model are shown in figs 5 and 6 events 1 and 2 and more completely in figs s1 s6 events 1 2 3 4 5 and 6 the initial conditions within the soil media and the influent concentrations that are the inputs to the biokinetic model are shown in table s3 and table s4 respectively the field measured inflow pollutographs from the six events described by wang et al 2017 were taken to be the time dependent inputs to the model the time depth profiles in figs 5 and 6 and s1 to s6 show the predicted variations in soil moisture and biokinetic model components oxygen nitrogen species nh3 no2 no3 n2 and heterotrophic and autotrophic bacteria species xh xans xanb as the storm event progresses from the start of rainfall t 0 hr table 3 shows the measured tn mass inflow at the inlet and compares the measured and predicted tn mass outflows via the overflow culvert and subsurface outlet table s5 shows similar results for other biochemical components stormwater exiting via the overflow culvert receives some physical treatment in the surface basin but only stormwater exiting via the outlet passes through the subsurface basin and receives biological treatment 3 2 1 water quality at the start of an event initial condition 3 2 1 1 oxygen before a storm event the aqueous phase in the upper 40 cm unsaturated zone is assumed to be at equilibrium with the atmosphere and saturated with oxygen at 29 c o2 7 67 mg l oxygen profile in figs 5 and 6 and s1 to s6 in the bottom 35 cm saturated iws internal water storage zone there is almost no free oxygen while bound oxygen in the form of no3 and no2 is present according to ritter s 2013 dry day measurement at the same study site the concentration of dissolved oxygen in the iws was found to be low 1 75 1 72 and 1 55 mg l at depths of 65 cm 75 cm and 85 cm respectively these measured values were imposed as the initial condition for dissolved oxygen in the anoxic iws zone in the model table s3 in the transition zone 40 50 cm depth dissolved oxygen in the medium range of 4 mg l was assumed 3 2 1 2 bacteria since this water quality model focuses on modeling the growth decay dynamics of bacteria the availability of the electron acceptors o2 no2 and no3 for cellular respiration becomes the determining factor for the distribution of bacterial species within the different zones of the soil media nitrifying bacteria xans and xanb are obligate chemolithoautotrophs as well as obligate aerobes which means that these nitrifiers fix co2 and are not able to oxidize nh3 or no2 for energy needs in the absence of oxygen because nitrification ceases to occur at low oxygen levels 2 mg l the bottom 35 cm saturated iws zone becomes uninhabitable for nitrifiers since in this model bacteria are assumed to be immobile and to not advect with the infiltrated flow populations of xans and xanb are limited to the unsaturated aerobic zone throughout the duration of storm events xans and xanb profiles in figs 5 and 6 and s1 to s6 on the other hand denitrifiers a special group of heterotrophs xh are facultative anaerobes autotrophic denitrifiers such as thiobacillus denitrificans are not included in this model this means that denitrifiers are able to respire no2 and no3 whenever o2 becomes unavailable denitrifiers are part of the general population of heterotrophs that is found throughout soil media at all depths the initial heterotroph concentration s xh init mg kg is assumed to be higher in the anoxic iws zone to account for denitrifiers competitive advantage in utilizing no3 and no2 the bacteria concentrations s xh init s xans init and s xanb init mg kg are computed based on literature reports of nitrifying and denitrifying gene copies per gram of soil table s6 and gene copy numbers per genome of bacteria species kandeler et al 2006 3 2 1 3 nitrogen phosphorus cod the initial concentration distributions of nh3 no2 no3 n2 ip cr cs and ci vary from event to event because they are dependent on the extent of biochemical reactions that occurred in the residual water during the dry period since the previous storm event the streamlines in figs 5 and 6 and s1 to s6 show that pollutant mass from the incoming stormflow reaches a variable depth depending on the amount of rainfall rainfall quantities are given in table s7 the pollutants then undergo biochemical transformations during the intervening dry period and stay within the system before being flushed out by infiltrated water during the subsequent event since we are simulating single events only the initial condition could not be assumed to be applicable to all events thus a different initial condition was calibrated for each event using field measurements by ritter 2013 as a guide the anoxic iws zone can also be expected to be enriched with organics and carbon leached from wood chips within the layer degradation and hydrolysis of wood chips would supply the zone with higher concentrations of organics in the form of cod cr cs ci in the model thus a scenario of higher concentration of cod in the anoxic iws zone was imposed in the model to improve the specification of initial conditions in future studies subsurface water quality should be monitored continuously over consecutive events and intervening dry periods 3 2 2 water quality evolution during an event during a storm event influent water infiltrates into the unsaturated zone with a distinct wetting front as can be seen in the time depth profiles for soil moisture θ cm3 cm3 and flow rate q cm hr figs 5 and 6 and s1 to s6 the progress of the wetting front is further illustrated in these figures by red streamlines indicating the flow paths of selected water particles through depth and time the vertical advance of the wetting front in the 1d soil column is the fastest in the first hour after the start of a storm event as shown by the orientation of the velocity vectors the advancing wetting front carries higher concentrations of oxygen and nutrients nitrogen and phosphorus and changes the microenvironment in which bacteria live and act as an event progresses one can see this effect clearly in fig 5b and d by the dramatic contrast between stormwater with high nh3 n and no3 n in yellow above the wetting front and older treated water with low nh3 n and no3 n in blue below the wetting front 3 2 2 1 nitrification the nitrification process is evident in event 2 fig 6 during event 2 from t 6 7 hr and z 1 0 cm to t 20 7 hr and z 37 4 cm i e following the marked streamline in fig 6a the oxygen concentration drops by 23 from 7 45 mg l to 5 72 mg l there are two factors forcing the oxygen level the first factor is an increase due to re aeration from atmospheric oxygen after surface ponding ends at t 15 hr fig s7 the soil moisture saturation decreases from full moisture saturation θ 0 3 in the first soil layer fig 6f and allows for re aeration to occur deeper into the soil column and not just at the surface the second factor is a decrease in the oxygen level due to bacterial utilization of oxygen in cellular respiration during the time between t 6 7 and 20 7 hr in event 2 the rate of oxygen consumption by bacteria for nitrification is higher than the rate of re aeration which leads to a reduction in oxygen level as part of the two step nitrification process nh3 is converted to no2 by nitrosomonas xan s which is then converted to no3 by nitrobacter xan b fig 2 following the pathways marked in fig 6 nh3 fig 6b is reduced by 93 from 0 18 mg n l to 0 012 mg n l which equates to a nh3 reduction rate of 0 012 mg n l hr no2 fig 6c increases by 62 from 0 058 mg n l to 0 094 mg n l which results in a no2 net production rate of 0 0026 mg n l hr no3 fig 6d increases less by 12 from 0 38 mg n l to 0 42 mg n l which is equivalent to a no3 production rate of 0 0031 mg n l hr on average for all six sampled events the average reductions in nh3 and o2 are 33 and 22 respectively while the average increases in no2 and no3 are 280 and 5 respectively the high build up in no2 and the slight increase in no3 might indicate that the first step of the nitrification process which is the conversion of nh3 to no2 as mediated by nitrosomonas xan s is proceeding at a faster rate than the second step which is the conversion of no2 to no3 as mediated by nitrobacter xan b another possible explanation for only a slight accumulation of no3 is that denitrification to nitrogen gas was occurring at a higher rate than the generation of no3 mediated by nitrobacter 3 2 2 2 denitrification in the anoxic iws zone instead of oxygen no2 and no3 are consumed as the final electron acceptors during the anaerobic cell growth of denitrifiers nitrogen gas is produced as the end product two instances of denitrification occurring in the anoxic iws zone are evident in event 1 fig 5 two regions show extensive nitrogen generation mediated by denitrifying bacteria at very low oxygen levels between t 0 and t 12 hr at a depth of about 50 85 cm as well as between t 14 to t 20 hr at a depth of 50 60 cm in the first region i e following the left marked streamline in fig 5c no2 decreased by 87 from 0 59 to 0 078 mg l no3 decreased by 55 from 0 40 to 0 18 mg l fig 5d while n2 increased by 64 from 1 0 to 1 7 mg l fig 5e and o2 remained low at 1 48 mg l fig 5a in the second region i e following the right marked streamline in fig 5c no2 decreased by 98 from 0 15 to 0 0024 mg l no3 decreased by 64 from 1 3 to 0 46 mg l fig 5d n2 increased by 88 from 0 98 to 1 8 mg l fig 5e and o2 remained low at 0 49 mg l fig 5a in the zone following the marked streamline in fig 5a oxygen saturated water in the unsaturated zone between 0 and 40 cm is advected downwards below the infiltrated inflow and reaches the iws zone along this oxygenated pathway denitrification is prevented from occurring this phenomenon implies that even within the anoxic iws zone where denitrification is expected to occur under normal condition oxygen first needs to be depleted before denitrification can proceed it is thus important for basins to be designed to have enough storage volume to provide adequate residence time for oxygen depletion and denitrification 3 3 model sensitivity effect of parameter values on predictions 3 3 1 sensitivity of the model to bacterial kinetic parameters μ b as this biokinetic model was developed using parameters from the activated sludge model from the field of wastewater treatment henze et al 2000 some parameters need to be adjusted for stormwater applications composition and stoichiometric parameters in table s2 are left unaltered from the original asm values as they describe universal fractions of nitrogen and phosphorus in biomass cr cs and ci however bacterial growth μ and decay b rates are less likely to be universal and thus were evaluated through a sensitivity analysis literature values for growth rates of various bacteria species including nitrifiers and denitrifiers vary considerably table s8 and their range was used to set high medium and low levels of growth rate μ h μ dn μ ans μ anb and decay rate b h b ans b anb for sensitivity analysis table s9 the literature values for growth rates are often based on controlled laboratory studies of a specific bacteria strain growing on specific culture media with abundant nutrients for growth however in stormwater treatment nutrient concentrations are lower presuming literature values reflect ideal conditions they were deemed high rates for bacterial growth μ h 19 2 day 1 μ dn 3 84 day 1 μ ans μ anb 3 24 day 1 and cell lysis b h b ans b anb 1 day 1 in the sensitivity analysis medium rates are the calibrated and validated values in this study μ h 1 5 day 1 μ dn 1 day 1 μ ans μ anb 0 75 day 1 b h b ans b anb 0 1 day 1 while the low rates are set to about an order of magnitude less μ h μ dn 0 1 day 1 μ ans μ anb 0 01 day 1 b h b ans b anb 0 01 day 1 time depth profiles of modeled concentrations of o2 nh3 no3 no2 and n2 are compared between high medium and low rates in fig 7 the comparison between modeled and observed mean outlet concentration is quantified statistically in table s10 a comparison of the left hand column high rates with the middle column medium rates in fig 7 shows that the onset of denitrification is earlier when biokinetic rates μ h μ dn μ ans μ anb b h b ans b anb are high with both high and medium rates denitrification occurs from t 4 hr at z 10 cm to t 20 hr at z 50 cm during this time no3 and no2 are reduced and n2 is generated spatially the upper unsaturated zone is where oxygen dependent processes such as nitrification occur under normal conditions i e at medium biokinetic rates however in a scenario with high biokinetic rates this zone becomes so deficient in oxygen that denitrifiers facultative anaerobes instead utilize no3 and no2 for cellular respiration this occurs because the dissolved oxygen in the infiltrated water is rapidly used up by intensified biological reactions o2 ip and nh3 are consumed by aerobic cell growth reactions rc2 rc6 and rc8 in fig 2 as are no3 and no2 by denitrification rc3 and rc4 ip and nh3 increase at later hours t 14 20 hr at z 50 cm due to another high rate biological activity cell lysis b h b ans b anb the combined effects of the different processes are an increase in nh3 and ip and a decrease in no3 and no2 in statistical terms as shown in table s10 pbias values are more negative for the high rate case for nh3 and ip 192 and 671 indicating higher predicted concentrations than for the medium rate case for no3 and no2 pbias values are positive 65 and 77 indicating lower concentrations for the high rate case than for the medium rate case fig 7 shows that the converse is true when biokinetic rates are low lower biological activity results in insignificant nitrification and denitrification and thus insignificant changes in pollutant concentrations and removal rates 3 3 2 sensitivity of the model with respect to initial bacteria concentration sx the effects of different levels of bacteria concentrations are similar to those caused by increasing bacterial kinetic parameter values as described in the previous section in particular earlier onset of denitrification occurs with high initial bacterial count due to increased no3 and no2 removal low initial bacterial counts imply little biological activity and thus insignificant changes in pollutant concentrations details can be found in the supplementary materials 3 4 model application predicting tn removal at various basin configurations performance of a bioretention basin is often assessed in terms of its ability to remove pollutants such as total nitrogen tn bioretention basins are typically designed with removal targets of 30 65 for tn brisbane city council 2006 department of irrigation and drainage 2011 dierkes et al 2015 mdep 2008 mpca 2008 nhdes 2008 pub 2014a currently design guidelines in singapore are modeled after those of temperate areas primarily australia fawb 2009 under the regulations of singapore s public utilities board pub 2014b basins in singapore must be designed to accommodate a critical design flow rate for a 3 month average recurrence interval ari event and water quality volumes wqvs determined using the music model model for urban stormwater improvement conceptualization crcch 2003 the removal rates in the music generated design curves are yet to be field validated in the tropical context due to a lack of field scale studies nonetheless singapore s design guidelines assume that target removal rates will be achieved as long as the basin configurations and wqvs are determined from the music generated design curves this study provides alternative performance curves for the tropics by using the water quality model developed here to make predictions of pollutant tn removal potential with various basin configurations and wqvs however the model developed in this study has only been validated against individual events and single storms do not necessarily show how well a basin performs in terms of pollutant removal over longer time periods therefore to predict the pollutant removal potential across events of varying sizes and intensities over a half year period an extension from single event simulation to long term simulations was made to evaluate performance over multiple events we completed a hydrological numerical simulation of 80 storms observed during the six month period monitored by wang et al 2016 we lack a complete set of water quality data for these storms and therefore conservatively assumed that the pollutograph for each storm was the same as that measured during event 1 in the study by wang et al 2017 the runoff during event 1 carried the highest emc event mean concentration among the six events sampled in our study accordingly the performance curves generated with the event 1 pollutograph can be used to generate presumably conservative designs the performance curves resulting from the half year water quality simulation are shown in fig 8 a and b the bioretention basin performance in terms of half year tn removal tn r is plotted as a function of three engineering parameters detention depth h d cm ratio of drainage area to bioretention area r and saturated hydraulic conductivity k s cm hr pei performance efficiency index fig 8c and d was developed to illustrate the incremental gain in tn r given a unit increase in either h d or k s 14 p e i d t n r d h d or d t n r d k s 3 4 1 detention depth hd the detention depth h d is the maximum depth of storage in the surface portion of the bioretention basin and controls the portion of the influent stormwater that is captured a shallower pond cannot hold as much stormwater as a deeper pond and thus passes more stormwater out the overflow culvert without biological treatment unsurprisingly fig 8a shows that an increase in h d and thus an increase in the portion of stormwater captured leads to more complete nitrogen removal fig 8a shows that at the current bioretention basin area r 70 tn r increases from 26 to 35 as h d increases from 10 to 70 cm this increase in tn r is non linear with a greater rate of increase seen at lower h d and gradually slower rate of increase at higher h d values this is simply a reflection of the fact that a 10 cm increase in depth represents a larger increase in storage relative to the lesser volume of a shallow pond than to the greater volume of a deeper pond the non linear slope is also seen in the pei plot fig 8c in which a higher pei 0 39 is achieved at h d 10 cm than at h d 70 cm pei 0 05 the plots show that the rate of improvement in tn r drops significantly with increases in detention depth h d beyond a certain point r is the ratio of the contributing drainage area to the surface area of the bioretention pond ponds with greater surface area and thus storage capacity have smaller r values fig 8a shows that as the surface basin is enlarged and r is decreased nitrogen removal improves just as deeper ponds capture and biologically treat a greater portion of the influent stormwater and thus improve overall nitrogen removal so do ponds of greater area while either deepening the pond or enlarging the pond can improve performance there are practical limits to both deeper ponds are less safe for the public but there may not be space available for a larger area pond thus both h d and r must be treated as independent design parameters fig 8a also shows that the incremental gain in tn r resulting from an increase in detention depth h d is greater for larger basin area smaller r reflecting the greater increase in overall storage from deepening a large pond versus deepening a small pond for instance a similar increase in h d from 10 to 80 cm results in an increase of 10 16 19 and 19 in tn r for basin area ratios of r 70 35 23 3 and 17 5 respectively a smaller basin area needs disproportionately higher detention depth h d to achieve the tn removal target of 45 required in singapore pub 2014a design curves in fig 8a show that for decreasing basin area ratios r 17 5 23 3 and 35 0 the detention depth required to achieve tn r 45 are h d 12 18 and 45 cm respectively the corresponding values of wqv range from 240 to 360 m3 and those of wqd water quality depth from 14 to 22 mm at the current basin area r 70 the balam basin is projected to be unable to meet the tn removal target of 45 with h d 80 cm fig 8a although different basin surface storage volumes h d x a could equally meet a tn removal target rate of 45 the extra subsurface volume created when the basin surface area is increased leads to more tn removal than if the extra surface volume is created only by greater detention depth since denitrification in the soil filter media is the main driver of nitrogen removal over the inter event period the larger subsurface storage volume beneath a larger basin area induces a disproportionate increase in tn removal compared to a smaller basin area 3 4 2 saturated hydraulic conductivity ks the rate at which the subsurface portion of the bioretention basin receives stormwater depends upon k s the saturated hydraulic conductivity of the surficial sandy loam layer this is the layer with the lowest hydraulic conductivity and which thus controls the flow through the subsurface basin fig 8b shows the effect of varying saturated hydraulic conductivity k s on tn removal tn r as k s increases from 0 1 to 5 cm hr tn r increases sharply indicating a rise in tn removal as more stormwater can pass through the subsurface treatment layers however tn r decreases rapidly from k s 5 10 cm hr and gradually from k s 10 60 cm hr the best basin performance in terms of tn removal is seen when k s 5 cm hr poorer performance at low k s is consistent with a reduction in the volume of infiltrated influent and hence less subsurface treatment poor removal at high k s is consistent with too rapid drainage of infiltrated influent and insufficient hydraulic residence time for treatment within the system the superior performance at k s 5 cm hr is also reflected in the pei plot fig 8d pei is most negative at k s 5 cm hr indicating peak performance at that value therefore increasing k s beyond 5 cm hr even though it would allow more water to be infiltrated would not improve overall treatment the optimal value of k s represents a trade off between more capture a hydrologic phenomenon and more treatment a biokinetic phenomenon only a model of both hydrology and biokinetics can represent this trade off 3 4 3 recommended basin design values the performance curves presented in fig 8a and b are appropriate for use by bioretention basin designers based on the analysis presented above we make the following recommendations for bioretention basin design 1 increasing retention depth h d improves treatment but only to a point the incremental benefit of tn r drops as h d increases beyond h d 30 cm such that further changes in this engineering parameter provide little improvement 2 hydraulic conductivity of k s 5 cm hr achieves the best balance between maximizing the volume of stormwater infiltrated and ensuring adequate treatment it is important that k s not be so high as to cause inadequate hydraulic residence time for the infiltrated stormwater to receive biochemical treatment for nitrogen removal before it is flushed out of the system 4 conclusion a numerical water quality model has been built to simulate subsurface bacteria mediated biochemical processes occurring in bioretention basins we integrated this biokinetic model which follows the formulation of the cw2d model with the recharge model that we previously adapted to model the hydrology of our bioretention field site wang et al 2019 1 this study has demonstrated that the mathematical formulations originating from the activated sludge model asm used in wastewater treatment are also applicable for modeling the role that bacterial growth and decay play in stormwater treatment in bioretention basins however some biokinetic parameters need to be calibrated for application to bioretention basins the model was successfully calibrated and independently validated against field data collected during an eight month monitoring period wang et al 2017 2019 2 simulation results based on calibrated and validated parameter values show that the build up of no2 and only slight increase in no3 during the roughly 20 hr of a storm event indicate that the first step of the nitrification process the conversion of nh3 to no2 mediated by nitrosomonas is proceeding at a faster rate than the second step the conversion of no2 to no3 mediated by nitrobacter 3 a sensitivity analysis of bacterial kinetic parameters μ h μ dn μ ans μ anb b h b ans b anb and initial bacteria concentrations s xh s xans s xanb indicates that the model is sensitive to both for instance the onset of denitrification will be earlier when either bacterial kinetic rates or initial bacteria concentrations are higher these parameter settings need to be better defined to ensure accurate simulations of real scenarios for that future studies need to focus on quantifying depth dependent microbiological activity occurring over time in the subsurface of bioretention basins 4 well defined initial conditions are important for an accurate short term event based simulation to improve the calibration of initial pollutant concentrations across different soil depths there is a need to continuously monitor water quality over consecutive events a more intensive pre and post event sampling program over a longer time duration would give a better representation of the initial conditions at the start of each event 5 the developed water quality model was used to evaluate total nitrogen removal tn r with various bioretention basin configurations detention depth h d catchment area to basin area ratio r and saturated hydraulic conductivity k s over a six month simulation period because the model simulates both basin hydrologic performance and biological treatment performance it successfully captures how basin design affects overall performance the surface volume characteristics h d and r affect the amount of stormwater captured by the surface basin while the subsurface hydraulic conductivity k s affects both the amount captured in the surface basin and the degree of biological treatment received in subsurface part of the basin the simulation results show that the rate of improvement of tn r decreases as h d increases beyond h d 30 cm such that further increase in this engineering parameter becomes less cost effective for k s a maximum value of tn r is found for k s 5 cm hr with the current basin configuration the estimated tn r is 28 which falls short of a target rate of 45 the performance curves developed using the model are useful as a reference for future bioretention basin design in the tropics name of software nitrobiorem description nitrobiorem is a matlab based model that simulates monod type biological degradation kinetics of nitrogen species in bioretention basins flow outputs from the hydrodynamic model recharge are coupled with nitrobiorem and together the two codes constitute a biokinetic model of 1d reactive transport it is intended to predict temporal accumulation and removal of nitrogen species spatially in a bioretention basin this model can be used to verify the appropriateness of design guidelines for bioretention basins by evaluating the response of the system to different design parameters developer jia wang jiawang alum mit edu peter shanahan peteshan mit edu lloyd h c chua lloyd chua deakin edu au funding sources singapore mit alliance for research and technology smart s center for environmental sensing and modeling censam national research foundation prime minister s office singapore software required matlab supported systems windows year first available 2017 availability the model code and data used in this study are located at https doi org 10 5281 zenodo 4665741 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the national research foundation prime minister s office singapore through the singapore mit alliance for research and technology s center for environmental sensing and modeling research program the authors thank alejandro r dussaillant jones for providing the original recharge codes appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105212 
25704,data portals and services have increased coastal water quality data availability and accessibility however tools to process this data are limited geospatial frameworks at the land sea interface are either adapted from open water frameworks or extended from watershed frameworks this study explores use of a geospatial framework based on hexagons from a discrete global grid system dggs in a coastal area two dggs implementations are explored dggridr and h3 the geospatial frameworks are compared based on their ability to aggregate data to scales from existing frameworks integrate data across frameworks and connect flows across the land sea interface dggridr was simpler with more flexibility to match scales and use smaller units h3 was more performant identifying neighbors and moving between scales more efficiently point line and grid data were aggregated to h3 units to test the implementation s ability to model and visualize coastal data h3 performed these additional tasks well keywords discrete global grid system coastal water quality geospatial framework big data 1 introduction people live work and recreate in coastal waters making the quality of those waters pivotal to our health society and economy a number of organizations including state and federal regulatory agencies monitor a variety of coastal water quality characteristics e g temperature dissolved oxygen total suspended solids fecal coliform chlorophyll content nutrients etc water quality data are used to characterize waters identify trends over time identify emerging problems determine effectiveness of restoration or pollution controls help direct restoration and pollution control efforts to where they are most needed and respond to emergencies such as floods or spills usepa 2013 these uses often require water quality data at different scales for example siting a restoration project requires data that can differentiate two neighboring sites along the same shoreline whereas a state impairments assessment requires data that can differentiate bays within an estuary whereas the epa s national coastal condition assessment ncca differentiates condition in one estuary compared to another usepa 2001 such use cases all have differences in spatial and temporal scales quality assurance thresholds geographic locality etc but one commonality is that most users need to depict or relate water quality spatially using geospatial frameworks a geospatial framework using a new grid system to aggregate integrate and scale water quality data will add benefit particularly for ecological applications in coastal areas made more complicated by the land water interface the volume of available water quality data is skyrocketing with the development of ocean observing arrays and in situ buoys glasgow et al 2004 legler et al 2015 interconnected low cost sensors adamo et al 2015 park et al 2020 vijayakumar and ramya 2015 and inference methods for remote sensing gholizadeh et al 2016 iogc 2018 ross et al 2019 portals and web services help to standardize and share this water quality data blodgett et al 2016 for example the water quality exchange wqx allows sharing of data using a standardized data model and the water quality portal wqp provides access to that data read et al 2017 however each such data collection is best suited for data derived a certain way for example the consortium of universities for the advancement of hydrologic sciences cuahsi geosling et al 2015 hydrologic information system may be more appropriate than wqp for real time continuous data streams to analyze this wealth of water quality data the data must be gathered from various portals and services harmonized and integrated into one spatial framework most end users need to depict or relate water quality spatially and often across the land water interface geospatial frameworks help to structure these data for use in analysis historically geospatial data for onshore and offshore areas are not well integrated due to fundamental differences in mapping conventions and reference datums national research council 2004 noaa has developed methods to consistently define the tidal datum for most of the nation allowing water level based shoreline definitions to be related to other vertical datums parker et al 2003 however once the shoreline is defined by water level keeping it up to date is complicated by the dynamic nature of shorelines which move over time due to erosion and accretion boak and turner 2005 vos et al 2019 several geospatial frameworks exist for coastal areas with both consistencies and differences in how units are defined how they deal with shorelines and how far to extend offshore the national hydrography dataset plus nhdplus medium resolution uses catchments to define the area of the landscape that drains to a specific stream segment moore and dewald 2016 linkages and consistencies between nhdplus catchments and other frameworks or datasets have made it easier to standardize and integrate information state regulatory waterbody units used state specific boundaries that may or may not include coastline definitions but epa s assessment total maximum daily load tmdl tracking and implementation system attains geospatial framework standardizes assessment units using nhdplus usepa 2016a the wqp indexes sample sites to the nhdplus catchments utilizing the upstream downstream relational network to define hydrologic relationships and hierarchies read et al 2017 when nhdplus catchments were delineated they were related to a snapshot of the watershed boundary dataset wbd berelson et al 2004 at hydrologic unit code huc 8 and 12 mckay et al 2012 however relating the datasets does not mean the boundary definitions were completely aligned and wbd units have been updated since making it difficult to relate current units wbd units typically extend out farther into coastal areas than land catchments and larger huc units use the noaa three nautical mile line as a default offshore boundary usgs 2013 the nhdplus framework uses national elevation dataset ned elevations to define catchment boundaries meaning these align with a consistent 30 30 m square grid however catchment boundaries only extended out into coastal areas as far as elevation models can direct flows i e stopping where elevation is zero resulting in catchments that excluded shorelines and estuary boundaries and inhibited networking coastal sample sites to upstream conditions nhdplus recently added ocean catchments a 1500 1500 m grid that extends seamlessly from land catchments out to the noaa 14 mile maritime limit usgs 2019 the ned system has now been superseded by the 3d elevation program 3dep part of the coastal national elevation database coned danielson et al 2018 and national map arundel et al 2015 the nhdplus high resolution dataset in development uses 3dep elevations and differentiates estuaries from the ocean moore et al 2019 suggesting coastline definitions in the medium resolution land and ocean catchments may evolve although square grids are widely adopted as the basis for geospatial frameworks one viable alternative is hexagonal grids hexagonal grids have several advantages over square grids for ecological applications birch et al 2007 a hexagon unit has a smaller perimeter to diameter ratio than a square unit meaning it more closely resembles a circle and has less bias from edge effects this is an important attribute when a point data sample is used to characterize the larger unit as a whole hexagons are better for connectivity and path analysis because the centroids center points of all six neighboring hexagons are equidistant birch et al 2007 in contrast square units have eight neighbors the four neighbors sharing a side are at one distance whereas the other four diagonal neighbors sharing a vertex are at a greater distance hexagonal grids have been used as the basis for coastal frameworks both for smaller scale habitat assessments smith et al 2010 and for broader scale characterizations shamaskin et al 2020 the 2019 noaa alaska department of natural resources coastal priorities assessment used a hexagon based framework to gather stakeholder mapping needs for coned alaska dnr 2020 epa s ncca usepa 2016b samples conditions at points selected through generalized random tessellation stratified survey design these sampling plans are an extension of the former environmental monitoring and assessment program emap diaz ramos et al 1996 where spatially weighted sample points were chosen from a tessellation of triangles to place hexagon unit centroids for statistical randomness the sample site selection methods start the hexagonal grid from a random point pebesma et al 2021 but for consistency some applications have instead used the origin point i e min x and min y of the sample frame or area of interest e g myers et al 2021 tools have generated hexagons to fill the area based on the desired number of sample units desired hexagon area e g evans et al 2021 or a consistent distance between hexagon centroids these differences in hexagon generation and alignment make it difficult to integrate results from disparate efforts or to promote one as the basis for a coastal spatial framework a reasonable alternative to using an arbitrary starting point for the hexagonal grids is to use a discrete global grid system dggs a dggs is a hierarchical data structure for covering the surface of the earth with a consistent grid of regular shaped equal area units in this case the earth is treated as an isosahedron divided into hexagons units spatially reference locations bypassing transformation issues related to planar representations of coordinates dggss are not a new concept in fact they were considered in the early emap surveys carr et al 1997 a dggs provides advantages for big data and digital earth research the past decade has seen several dggs advancements new implementations and example applications birk 2018 sahr 2011 sirdeshmukh et al 2019 jendryke and mcclure 2019 several implementations of dggs have used hexagons as their equal area units including h3 brodsky 2018 dggrid sahr 2019a openeager bush 2017 and rhealpix gibb 2016 dggss have become pervasive enough that the open geospatial consortium ogc has started developing standards for them purss et al 2017 although current implementations may not yet adhere to all these standards bondaruk et al 2020 there is a clear advantage to begin integrating these data structures coastal areas can benefit from a new dggs geospatial framework but it must function at the necessary scales and integrate with existing geospatial frameworks given their spatial overlap and upstream relationship to coastal areas this manuscript focuses on two open source implementations of hexagonal frameworks h3 uber 2020 and dggridr barnes and sahr 2017 the methodology behind how the two hexagonal implementations each scale data is described hexagon scales are then compared to average units from nhdplus the wbd huc 08 and the regulatory units used in attains next we show how h3 hexagonal framework units network together and how to integrate that network with nhdplus land catchments for comparison nhdplus ocean catchments are also networked together and as downstream of land catchments lastly the h3 implementation is used to convert aggregate scale and interpolate actual coastal temperature data for data rich tampa bay florida converted data types included point line and remotely sensed grid data 2 methods 2 1 hexagonal frameworks and scaling two open source implementations of hexagon based dggss were used the h3 python library v3 6 4 uber 2020 and the dggridr r package barnes and sahr 2017 both implementations are available in multiple programing languages this manuscript s supplemental materials include raw data and code examples the code is presented within jupyter notebooks to allow users to repeat our methods on the provided raw data kluyver et al 2016 although maps in this manuscript highlight example areas the jupyter notebook allows users to explore results by panning or zooming to other places on the map hexagon projection and orientation are essential to consistent units h3 hexagons utilize an icosahedral projection with spherical orientation fuller 1975 with a pentagon at each of twelve icosahedron vertices dggridr is an r package for implementing the dggrid software v6 2b sahr 2019a and supports the same typologies i e hexagons triangles or diamonds and projections e g icosahedral snyder equal area to be consistent with h3 all dggridr examples used hexagons the icosahedral projection and spherical fuller orientation hexagonal dggs scaling involves moving from a finer resolution child hexagon to a coarser resolution parent hexagon or vice versa one of the disadvantages of hexagons as compared to square units is that hexagons are non congruent between scales a square unit divides into four equal area subunits e g four 30 m 30 m squares completely fit into one 60 m 60 m square h3 and dggridr divide hexagons into seven sub unit hexagons but those units either do not completely cover the original hexagon or they exceed the original hexagon bounds depending on the aperture used i e the ratio of child units to a parent unit area common apertures used in hexagonal grids are 3 4 and 7 the smaller the aperture the smaller the change in unit size from one scale to the next fig 1 in all three apertures there are seven child units for each parent unit with the central unit sharing the centroid of the parent unit but with different ratios of parent to child area and with different orientations table 1 for aperture 3 the centroid of each of the six child units align with the six parent vertices rotation of units 90 from one scale to the next results in a change in unit orientation pointy topped vs flat topped a single child unit may overlap as many as three different parent units using aperture 3 of the three apertures this aperture has the smallest change in area from one resolution to the next aperture 3 is available in dggridr but was not demonstrated here as it is like dggridr aperture 4 in functionality for aperture 4 the centerlines of each of six child units align with the six parent edges the units have the same orientation across the different scales aperture 4 has the advantage of splitting each of the six child units evenly between parent units dggridr can use aperture 3 aperture 4 or mix the two either to alternate or to approximate a desired unit area at a specific resolution all dggridr applications in this manuscript used aperture 4 scaling aperture 7 has the advantage of fitting the child units most closely to the area covered by the parent unit using this aperture allows h3 to scale data without re indexing the original coordinates however unlike aperture 3 and 4 child units aperture 7 child units do not completely cover the area of the parent unit to resample point data at a smaller scale would require consideration of 12 child units instead of six section 3 4 compares point data scaled using the index method to re aggregated data to assess scaling distortion all h3 applications in this manuscript used aperture 7 scaling 2 2 scale matching with existing geospatial frameworks hexagon scales must be close to decision scales to be useful for representing information in support of those decisions to match hexagonal unit scales to decision scales the hexagon scale with the average hexagon unit area closest to the average unit of three existing frameworks was identified table 2 the frameworks considered were the nhdplus catchments for local decisions e g for restoration the regulatory units used attains for sub estuary decisions e g state impairments and the wbd huc 08 for estuary decisions e g ncca comparisons matching hexagon unit areas to the average unit area of these frameworks helps to identify comparable scales for visualizations but does not capture the information the irregularly shaped units of those frameworks represent for example nhdplus land catchments represent hydrologically connected parts of the landscape because their boundaries are defined by the area of the landscape that drains to a specific nhd stream segment to appropriately represent hydrologic relationships using the hexagonal frameworks hexagon units would need to be the same size as the units used to originally delineate the catchments and be related to nhd stream segments a fourth scale of hexagon units an elevation data scale was approximated to match the 30 m grid used to delineate catchments nhdplus land catchments have an average area of 2 9 km2 nhdplus ocean catchments are 1500 1500 m square units but at the land ocean catchment boundary these squares are only partial to avoid overlapping land catchments as a result the average area for ocean catchments is 2 01 km2 usgs 2019 attains waterbodies vary but the average area is 22 km2 usepa 2020 the average wbd 8 digit huc is 1800 km2 with a range of 10 6 19 200 km2 the elevation data scale is the smallest square cell considered 900 m2 these square units are regularly shaped and unlike the other scales their accuracy is as important as their area grid cell accuracy is the maximum distance between any two points in that cell measured as the distance from vertex to opposite vertex approximately 42 m these average areas represent decision scales to approximate with hexagonal units from h3 and dggridr 2 3 networking units within and across frameworks coastal waterbodies are not closed systems water flows in from upstream watersheds and mixes within the embayment and with ocean waters outside the embayment modeling this mixing is beyond the scope of this manuscript however a coastal geospatial framework must have the capacity to network units within the framework to support mixing flows and be able to network as downstream of watershed units all the frameworks examined in this manuscript use a unique index to identify individual units in nhdplus each land and ocean catchment geometry has a unique id for land catchments this same id is stored in a field comid in a separate plusflow table that captures the network of upstream downstream relationships flows between units in the estuary are not as easy to define as those in watersheds but methods using unit indexes are demonstrated for networking ocean catchment and h3 hexagon units to both neighboring units within those frameworks and to neighboring land catchments ocean catchments and h3 hexagons downstream of land catchments were identified and integrated into the flow network downstream ocean catchments were those adjacent to a land catchment while downstream hexagons were those overlapping a land catchment the ocean catchment comids or hexagon indices from the corresponding downstream unit were added to the plusflow table ocean catchments already have a 9 digit index starting with 7 unique from any land catchment 9 digit land catchment indexes start with either 1 or 9 the h3 index is an unsigned 64 bit integer uint64 e g 8b441a8f028cfff that had to be converted to store as integer based indexes in the plusflow table e g 627198441950859263 converted h3 indexes all have more digits than the 9 digit nhdplus indexes making it easy to avoid duplicates open water ocean catchments and h3 spatial units non adjacent and non overlapping with land catchments interconnect with one another as well each open water ocean catchment had four neighboring catchments added to the plusflow table as downstream this resulted in all ocean catchments being both upstream and downstream of their neighbors h3 uses a hierarchical index where each identifier is specific to a specific spatial unit and resolution brodsky 2018 the index system allows the coarser resolution parent hexagon to be identified from the finer resolution child hexagon by simply truncating the index the assignment of digits 0 6 at each resolution uses a central place indexing arrangement sahr 2019b allowing h3 to identify neighboring edge cells or cells n given cell widths away from a unit by its index h3 has built in index based methods e g hexrange and kring methods that were used to network units within the framework since the center of each hexagon is a consistent distance from its neighbor these same methods were also used for distance based interpolation as described in section 2 5 compared to h3 dggridr indexing is simpler each spatial unit in a grid has a global sequence parameter seqnum numbered 1 to n where n is the number of cells generated however ddgridr does not have the same built in index based neighbor identification as h3 and therefore was not used to network cells within the framework as with h3 hexagons any dggridr hexagons overlapping a land catchment could be networked as downstream of that catchment by adding its index to the plusflow table as with ocean catchments dggridr hexagon neighbor to neighbor relationships could be stored by index in the plusflow table in both cases the hexagon index must not duplicate any existing catchment featureid 2 4 demonstration site and data aggregation raw data aggregation data scaling and integration with nhdplus using dggridr hex aperture 4 and h3 hex aperture 7 hexagons are demonstrated in the tampa bay estuary as defined by the ncca survey program fig 2 a tampa bay was chosen because the area is data rich three datasets point line and grid were converted to h3 hexagon units the point data came from sampling points in the wqp fig 2b results at these sites represented water temperatures uploaded to the wqp read et al 2017 usepa et al 2021 line data came from the noaa continually updated shoreline product cusp fig 2c noaa s cusp dataset was downloaded for gulf of mexico noaa 2020 and lines intersecting hexagons were characterized as having shoreline present absent grid data came from the multi scale ultra high resolution mur fig 2d sea surface temperature dataset a daily 1 km resolution dataset that merges multiple level 2 satellite datasets chin et al 2017 sea surface temperatures over land were represented with no data temperature point and grid data were used because it influences species composition varies more in coastal environments and correlates with circulation azmi et al 2015 brando et al 2015 physical characteristics such as temperature are also the most common result in wqp read et al 2017 point data from the wqp were pre processed before aggregation to ensure consistent data the query restricted results to water temperature sample points within the ncca tampa bay polygon extent and from january 1 1995 through december 31 2020 python scripts automated data gathering and pre processing see supplemental materials pre processing involved a few key assumptions about coordinates time zones depth and duplicates coordinates retrieved for sample sites were transformed to nad 1983 if in a different datum e g wgs 1984 if the coordinate reference system was not specified the script assumed site coordinates were nad 1983 because wqp uses it in spatial queries time zone handling assumed time zone entries were accurate where any time zone listed as standard time or daylight time uses that coordinated universal time utc adjustment regardless of time of year for example eastern standard time est is interpreted as utc 5 h and eastern daylight time edt as utc 4 h even if an est time is during daylight savings this is consistent with usgs dataretrieval packages for r or python de cicco et al 2018 hodson and black 2020 only surface temperatures were examined to be more consistent with sea surface temperatures from remote sensing results with a depth greater than 5 m were rejected and those with no depth measurement were assumed to be surface measurements only the first of any duplicate temperature for the same location and time was considered 2 5 data interpolation a simplified inverse distance weighting approach to interpolation was taken for units with missing 2011 average temperature data across h3 scale 11 and scale 7 hexagons this simplistic approach used built in krings weighting each by the distance band and giving less weight to neighbors based on how many cells away they were to a maximum distance of 20 cells due to the number of empty cells it was important to rescale each value based on the number of values at all distance bands rather than the number of cells at each distance band the equation can be expressed as hex value i 1 n 20 x i k i 1 k where x the value from a given cell is divided by k its distance band from the hexagon of interest the result for all neighboring cells with a value within distance bands of 1 to n 20 is summed for the numerator the denominator is the sum of 1 divided by distance bands used since all points within a distance band are treated as equidistant and each band is a multiple of that distance the actual distance is never measured directly 3 results 3 1 comparison of scales four scales were considered from existing spatial frameworks table 2 nhdplus land and ocean catchments local attains waterbodies sub estuary wbd huc 08 estuary and the elevation data resolution table 2 fig 3 the average area of h3 and dggrid hexagon units at different scales were compared to units from existing spatial frameworks at the four scales table 3 h3 hexagon units were mapped alongside units from the spatial frameworks at the four local scales to visualize this comparison in tampa bay fl fig 3 at the local scale comparable to 2 0 2 9 km2 ocean land catchment units h3 hexagons scale 8 to 7 have 0 7370 5 4610 km2 areas respectively dggridr hexagons scale 13 to 12 have 0 7601 3 0402 km2 areas respectively at the sub estuary scale comparable to 22 km2 attains waterbodies h3 scale 7 to 6 hexagons have 5 1613 36 1291 km2 areas dggridr scale 11 to 10 hexagons have 12 1609 48 6436 km2 areas at the estuary scale comparable to a 1800 km2 8 digit huc h3 scale 4 to 3 hexagon areas are 1770 3240 12392 2649 km2 and dggridr scale 8 to 7 hexagon areas are 778 2984 3113 1935 km2 comparing the elevation data grid size to units at different hexagon scales should be based on the accuracy of those units rather than coverage area the accuracy of a unit is based on the maximum distance between any two points in that unit the maximum distance from any point in a hexagon to any other point in the hexagon is the width along the long diagonal measured vertex to opposite vertex and is double edge length for a square it is measured from vertex to opposite vertex and is 2 times the length of one side elevation data on a 30 m grid therefore has a 42 m accuracy and is comparable to a hexagon unit with a width of 42 m this is within h3 hexagons scale 12 to 11 with an 18 8320 49 8220 m accuracy for dggridr it is within hexagons scale 18 to 17 with a 33 8047 67 6094 m accuracy in other words h3 scale 12 hexagons with a width of 18 832 m are comparable to a square unit 13 316 m wide whereas h3 scale 11 hexagons are comparable to a square unit 35 235 m wide the highest scale h3 hexagon is 15 with a 0 9 m2 area and an accuracy comparable to a square unit 0 7212 m wide due to its less restricted indexing dggridr has a maximum scale of 30 with the projection and aperture explored here at scale 30 hexagons have a 0 4424 cm2 area comparable to a square unit 0 5835 cm wide the intent of these scale comparisons is to represent a similar level of aggregation section 3 4 explores point data converted to h3 hexagons for potential data scale distortion when moving between scales without re aggregating 3 2 networking frameworks ocean catchments and h3 hexagons were added to the flow network as downstream of land catchments within ocean catchments and h3 hexagons adjacent neighbors were also networked for ocean catchments these were non diagonal neighbors to visualize this network structure a field was added to land catchments and one coastal catchment was populated with a value of 10 using the network upstream downstream spatial units were given a value of 10 n where n is the number of units away from the source to the unit this helps to quickly show how neighboring units relate fig 4 the relationship is simple but the structure allows for more sophisticated relationships between sources in upstream land catchments and downstream coastal hexagons 3 3 data aggregation using the extent for the tampa bay area wqp returned 12 356 unique sample sites of those sites 9034 were within the ncca defined tampa bay area aggregating these site points to the h3 scale 12 hexagons resulted in 6554 hexagons the maximum number of sites in one hexagon was 12 aggregating to h3 scale 11 reduced this number of hexagons to 5550 with a maximum site count of 22 at two hexagons fig 5 over the 26 year period of data retrieved 1 4 1995 to 12 21 2020 there were 182 418 water temperature results at the sample sites this number was reduced to 170 161 water temperature results after pre processing with 1 5698 results at each of 5550 sites aggregating to daily averages reduced the total to 54 756 results across 3987 unique days 42 of the 9480 days retrieved aggregating to annual averages reduced results to 14 012 most sites 92 only had results in one year the year 2004 had the most sites with temperature results with 883 the 2004 average temperatures aggregated to 794 hexagons with a maximum of 6 sites in a single scale 11 hexagon fig 5 the results from converting noaa s cusp data to hexagons were more convoluted than anticipated each vertex is converted to a hexagon index meaning sub hexagon scale intricacies such as dips or double backs are lost at course scales the set of hexagon indices that make up each cusp line can be treated either individually to allow for identification of multiple parts of the shoreline within single hexagons or combined to identify the unique set of hexagons intersecting cusp lines in aggregate the latter approach was employed as it is much easier to visualize fig 6 the spatial resolution of the gridded data was 1 km between h3 scales 7 and 8 the geotiff was converted to a comma separated values csv file with coordinates and values and the points were aggregated to scale 7 hexagons this has the same effect as a fishnet with points at raster cell centroids scale 7 hexagons were larger than the gridded data resolution and each hexagon value is the average of the point values within it this may distort aggregate values slightly compared to an area weighted average but should be comparable to nearest neighbor resampling scale 8 hexagons are smaller than the grid resolution and the same method would create gaps anywhere a hexagon fell between raster centroids to avoid this the grid data was instead sampled using scale 8 hexagon centroids fig 7 3 4 data scaling distortion a global spatial system faces several spatial distortion sources one source is the non congruence of the aperture 7 hexagon units between scales when data is scaled by truncation as it allows data to be assigned to a parent hexagon that does not overlap the original coordinate point fig 8 the tampa bay demonstration identified examples of this scaling distortion between scale 11 with 2293 hexagons and scale 10 with 2904 hexagons only 97 hexagon assignments changed when using index based scaling h3 to parent compared to reconverting sample points to scale 10 this represents a small portion 5 of the 1904 hexagons at scale 10 this comparison did not account for sites that may have been incorrectly assigned to a parent cell already containing a different sample site based on the geometry 14 375 of the area of a parent scale unit is non congruent with units of a child with 7 of the child unit area outside the parent unit and 7 of parent unit area outside the child units it is important to note assigning coordinates a new index at a new scale avoids this distortion index based scaling is not an option for the aperture 4 hexagons generated by dggridr as scaling distortion would be greater their more regular geometry with half of each exterior child unit outside of the parent unit leads to 43 of child cell areas being outside of the parent unit but no points within the parent unit that are not in one of the child units identifying points in aperture 4 units with multiple parent units for selective re indexing is computationally feasible however re indexing is computationally efficient making it more reasonable to just re index all aperture 4 parent units 3 5 data interpolation the available water temperature point data were aggregated to hexagons but there are many hexagons within tampa bay that do not have temperatures sampled the equidistant network of hexagons and built in hex ring hexrange and k ring kring methods make simple distance weighted linear interpolation of these hexagon values efficient fig 9 though this interpolation is simple the same built in methods can be used with more advanced machine learning methods 4 discussion the two hexagonal implementations h3 and dggridr were both able to represent data at similar scales to the existing frameworks explored here nhdplus land and ocean catchments attains waterbodies and wbd huc 08 for this application the level of approximation achieved by both hexagonal frameworks was adequate given that it was well within the range of areas for irregular units such as catchments both a larger and smaller hexagon resolution were identified for each decision scale leaving options for end users to tailor to their specific context if matching unit area more closely is important dggridr hexagons of mixed 3 and 4 aperture can be used though not available in dggridr ddgrid does this with preset grids e g approximates a 500 m grid with the superfund preset sahr 2019a both implementations have scales able to represent the original data used to define the other frameworks i e the nhdplus raw data resolution these scales were compared based on the accuracy of raw data rather than the unit area i e 42 m for the 30 m grid the same scales also exceed the sample point accuracy 0 2 nautical miles 37 m required of ncca methods usepa 2010 although hexagon scales compared favorably to existing framework units hexagons at these scales would not represent the same spatial relationships between units to use hexagons this way the raw elevation data would need to be aggregated to hexagons and then hexagon based catchments could be delineated liao et al 2020 used a hexagon grid to delineate watersheds and found improvements over conventional square grids suggesting the same could be done using h3 or dggridr the most resolute scale used in h3 scale 15 0 9 m2 unit area is more resolute than similar frameworks in development such as nhdplus hr buto and anderson 2020 and even higher resolution land use land classifications such as noaa s 1 m2 coastal change analysis program mccombs et al 2016 however this is not enough to compare to the accuracy of some data collection efforts such as individual lidar point clouds and with advances in data resolution over the past decade it is conceivable that h3 scale 15 may become inadequate for some applications in the future dggridr is less limited by the index system used and therefore can achieve smaller scales than h3 down to a scale 30 with 0 8253 cm unit area by tweaking the aperture and projection even smaller units could likely be achieved this application benefitted from h3 index methods but there may be other use cases where dggridr is superior h3 aperture 7 indexing was more computationally efficient compared to dggridr seqnum assignment h3 built in index based neighbor functions were used for networking flows between h3 units whereas networking dggridr units required storing neighbors in a plusflow like table since coastal flows were assumed to be bi directional instead of one way like upstream to downstream this flow table gets larger faster and could be prohibitive at large scales some of the built in index based functionality in h3 could be added to dggridr but would require customization for each aperture projection and typology some of this functionality may be available in newer versions of dggrid not yet used by dggridr without the full suite of dggrid options such as the superfund preset grid dggridr lacks h3 s simplicity of navigation between grid resolutions and neighbors compared to h3 dggridr has more options for how hexagon units are defined i e aperture projection typology can achieve smaller scales 30 vs h3 s 15 and can use mixed 3 and 4 apertures to better approximate unit areas this makes dggridr a better implementation for applications requiring hexagon units that are very small in a specific area or otherwise need to align in a specific way h3 had a clear advantage over dggridr or ocean catchments when networking neighboring units determining neighbors from the unit index directly rather than needing to store it in a separate table this allows omnidirectional networking but more sophisticated connectivity would have to be stored in a different way one option not explored here is to use alternative h3 index modes modes are 4 bits of the 64 bit integer where mode 1 is the cell index mode 2 is a unidirectional edge index e g unit 1 unit 2 and mode 3 will be a bi directional edge index e g unit 1 unit 2 uber 2020 although mode 3 is still in development this could be an area for future exploration in addition to networking hexagonal units together another objective was to network the units to other frameworks networking to nhdplus catchments was of particular interest given these could act as upstream sources of downstream water quality impairments to connect any of the units to land catchments the unique id was added as downstream to neighboring land catchments in the plusflow table this worked well for both ocean catchments since they already have a 9 digit index unique from any land catchments and for h3 indexes since they all have more than 9 digits given the sequential dggridr seqnum assignment the risk of duplicating an nhdplus index is much higher depending on the region being considered in tampa bay the number of dggridr units would have to be considerable 25 000 for this to be an issue one way to avoid the issue would be to alter the stored index for example making it a 9 digit index not already occupied by land catchments e g add 500000000 making index 1 into 500000001 once hexagons or ocean catchments were networked to land catchments the functionality of the network changed slightly land catchments are upstream downstream of one another and one catchment may have multiple catchments upstream or downstream stepwise navigation incrementally querying the next upstream or downstream catchment s can be chained for example to exhaustively identify all catchments upstream or downstream in that watershed stepwise navigation still works with ocean catchments or hexagon units but these units are interconnected with one another and all watersheds causing an exhaustive search to yield all catchments instead of one watershed land catchments are also associated with stream segments that have a distance allowing the network to perform distance based navigation for example the network could determine what catchments are within 5 km upstream although distance based navigation using the expanded network still works for land catchments ocean catchments and hexagon units do not have a similar flow path to use this functionality the distance from one unit to the next could instead be measured as the distance from centroid to centroid although the result is zig zag routing from unit to unit that measures manhattan distance rather than the shortest navigable path this exploratory work did not investigate spatial overlap issues with integration coastal hexagons were defined by ncca estuary boundaries rather than areas missing from land catchments and they were left overlapping with land catchments rather than clipping them to be seamlessly adjacent as ocean catchments are changing this would not alter how h3 hexagon indexes are assigned when filling the area of interest when assigning raw point data to hexagons that data could be reduced to the area not covered by land catchments to avoid double assignment the geometry of the hexagons could be clipped to the outline of land catchments like partial ocean catchments when making area based calculations and for visualizations a similar clipping approach could be taken to identifying hexagons that are split into sub units by land catchments although these hexagon sub units share an index flow between them may be inhibited by land this is not always true since land catchments often extend into submerged areas ideally hexagon based elevation models could be used to develop hexagon based catchments and then those would be seamlessly adjacent in such a framework units at the land sea interface could be identified as such using cusp as was done here or polygon based shoreline definitions such as open street map s osmcoastline 2021 the main advantage of polygon based definitions is that aquatic and terrestrial areas can be discerned explored scales were adequate for aggregation of point line and grid data the number of sample points in overlapping h3 scale 11 hexagons suggests this is an adequate scale for differentiating most sites in estuaries when multiple sites fell within the same hexagon the sites tended to have different sampling times which suggests that the sampling sites were either moving over time or are intensive limited duration studies temperature data in tampa bay was investigated because it is a commonly measured characteristic and tampa bay is data rich suggesting hexagon scale 11 should be adequate in other estuaries with less data as well as coastal water quality monitoring devices become more affordable and report readings with greater frequency h3 scale 11 may become too large but smaller alternatives are readily available down to scale 15 although scale 11 was adequate for most cusp lines these lines presented some interesting issues cusp lines are discontinuous meaning they are not suited for determining what percent of the hexagon area was land or water based polygons such as those available from openstreetmap coastline database are better suited for that and merit future investigation when cusp lines were compared to land catchments it showed these catchments do typically extend into the water but it was difficult to consistently measure how far because cusp lines are often close to each other to represent small channels although similar raster data equivalents now exist for hexagons de sousa and leitão 2018 national datasets in the gridded raster format are far more prevalent raster images representing remote sensed temperatures were aggregated to h3 hexagons scale 7 and 8 following the existing best practices and workflows ion et al 2014 robertson et al 2020 reinforced that these scales were most appropriate at scale 8 some hexagons covered multiple grid cells and aggregating by hexagon centroid would cause under sampling grid cells not being included so they were sampled by grid cell centroid and averaged per hexagon at scale 7 the space between some grid cell centroids exceeded the width of hexagon units to deal with this the grids were aggregated by hexagon centroid temperatures over land were excluded from analysis but land also interferes with some of the remote sensing methods included in the dataset which may impact near coastal data accuracy chin et al 2017 future efforts to fill gaps at unsampled sites will benefit from considering remotely sensed data but should consider alignment of resolutions and the level of interpolation already present scale distortion was present in tampa bay when using built in index based methods to go from scale 11 to scale 10 hexagons this distortion occurred in a limited number of situations and at least at scale 11 reconverting sample points to hexagon indices at the new scale was not time or processing intensive future work with a larger scope e g across the gulf of mexico or all u s coasts could benefit from using the built in index based scaling especially for visualizations or other use cases where the decreased accuracy is a worthwhile tradeoff for reduced processing or storage size even as water quality data increases it will remain difficult to completely sample an estuary making it important to be able to estimate water characteristics between sites here the intention was to demonstrate how to interpolate using the h3 framework h3 built in methods for equal distance bands were helpful when conducting a simple inverse distance weighted interpolation at the smaller h3 scale 11 fig 9d the interpolated surface is not very smooth this is likely due to a combination of data spatial and temporal sparsity visible in fig 9c where most units had no data and the interpolation method used the same built in methods could also be used for more complex interpolation and more advanced machine learning methods e g pytorch paszke et al 2019 more sophisticated hydrodynamic models could make use of h3 mode 2 and mode 3 to store directionality of flows across time periods to further calibrate such models to more realistic mixing a 20 neighbor limit was placed on how far away to interpolate but for actual use cases there are additional considerations for example spatial autocorrelation places limits on how fine a scale data should be discretized and or interpolated at from an efficiency perspective smaller grid units allow for greater granularity and precision but tradeoff against the storage and computational cost in large areas with no data one alternative to interpolation may also be hexagon compaction where hexagons of coarser scales are intermixed between smaller hexagons to reduce the storage and computation of that data compaction relies on truncation to move between scales so it introduces shape distortion but the reverse un compaction is exact this means data could be interpolated at higher scales then compacted for most use cases 5 conclusion discrete global grid system implementations will likely continue to evolve to meet new ogc standards yet this work showed that even existing hexagon implementations are already an effective alternative to existing spatial frameworks two dggs implementations were tested in a coastal area for their ability to match existing decision scales integrate with existing spatial frameworks and represent existing coastal water quality data dggridr was at least as effective as existing ocean catchments and h3 had additional index structuring that facilitates data scaling and interpolation as coastal data increases in velocity and volume a dggs is an appropriate way to continue to process this data while also still being able to integrate with other spatial frameworks disclaimer the views expressed in this article are those of the authors and do not necessarily represent the views or policies of the u s environmental protection agency any mention of trade names products or services does not imply endorsement by the u s government or the u s environmental protection agency the epa does not endorse any commercial products services or enterprises this contribution is identified by tracking number ord 042125 of the u s environmental protection agency office of research and development center for environmental measurement and modeling gulf ecosystem measurement and modeling division declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank ashley chappell michelle thawley kevin summers lisa m smith and elizabeth george for their thoughtful reviews and insightful comments appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105210 
25704,data portals and services have increased coastal water quality data availability and accessibility however tools to process this data are limited geospatial frameworks at the land sea interface are either adapted from open water frameworks or extended from watershed frameworks this study explores use of a geospatial framework based on hexagons from a discrete global grid system dggs in a coastal area two dggs implementations are explored dggridr and h3 the geospatial frameworks are compared based on their ability to aggregate data to scales from existing frameworks integrate data across frameworks and connect flows across the land sea interface dggridr was simpler with more flexibility to match scales and use smaller units h3 was more performant identifying neighbors and moving between scales more efficiently point line and grid data were aggregated to h3 units to test the implementation s ability to model and visualize coastal data h3 performed these additional tasks well keywords discrete global grid system coastal water quality geospatial framework big data 1 introduction people live work and recreate in coastal waters making the quality of those waters pivotal to our health society and economy a number of organizations including state and federal regulatory agencies monitor a variety of coastal water quality characteristics e g temperature dissolved oxygen total suspended solids fecal coliform chlorophyll content nutrients etc water quality data are used to characterize waters identify trends over time identify emerging problems determine effectiveness of restoration or pollution controls help direct restoration and pollution control efforts to where they are most needed and respond to emergencies such as floods or spills usepa 2013 these uses often require water quality data at different scales for example siting a restoration project requires data that can differentiate two neighboring sites along the same shoreline whereas a state impairments assessment requires data that can differentiate bays within an estuary whereas the epa s national coastal condition assessment ncca differentiates condition in one estuary compared to another usepa 2001 such use cases all have differences in spatial and temporal scales quality assurance thresholds geographic locality etc but one commonality is that most users need to depict or relate water quality spatially using geospatial frameworks a geospatial framework using a new grid system to aggregate integrate and scale water quality data will add benefit particularly for ecological applications in coastal areas made more complicated by the land water interface the volume of available water quality data is skyrocketing with the development of ocean observing arrays and in situ buoys glasgow et al 2004 legler et al 2015 interconnected low cost sensors adamo et al 2015 park et al 2020 vijayakumar and ramya 2015 and inference methods for remote sensing gholizadeh et al 2016 iogc 2018 ross et al 2019 portals and web services help to standardize and share this water quality data blodgett et al 2016 for example the water quality exchange wqx allows sharing of data using a standardized data model and the water quality portal wqp provides access to that data read et al 2017 however each such data collection is best suited for data derived a certain way for example the consortium of universities for the advancement of hydrologic sciences cuahsi geosling et al 2015 hydrologic information system may be more appropriate than wqp for real time continuous data streams to analyze this wealth of water quality data the data must be gathered from various portals and services harmonized and integrated into one spatial framework most end users need to depict or relate water quality spatially and often across the land water interface geospatial frameworks help to structure these data for use in analysis historically geospatial data for onshore and offshore areas are not well integrated due to fundamental differences in mapping conventions and reference datums national research council 2004 noaa has developed methods to consistently define the tidal datum for most of the nation allowing water level based shoreline definitions to be related to other vertical datums parker et al 2003 however once the shoreline is defined by water level keeping it up to date is complicated by the dynamic nature of shorelines which move over time due to erosion and accretion boak and turner 2005 vos et al 2019 several geospatial frameworks exist for coastal areas with both consistencies and differences in how units are defined how they deal with shorelines and how far to extend offshore the national hydrography dataset plus nhdplus medium resolution uses catchments to define the area of the landscape that drains to a specific stream segment moore and dewald 2016 linkages and consistencies between nhdplus catchments and other frameworks or datasets have made it easier to standardize and integrate information state regulatory waterbody units used state specific boundaries that may or may not include coastline definitions but epa s assessment total maximum daily load tmdl tracking and implementation system attains geospatial framework standardizes assessment units using nhdplus usepa 2016a the wqp indexes sample sites to the nhdplus catchments utilizing the upstream downstream relational network to define hydrologic relationships and hierarchies read et al 2017 when nhdplus catchments were delineated they were related to a snapshot of the watershed boundary dataset wbd berelson et al 2004 at hydrologic unit code huc 8 and 12 mckay et al 2012 however relating the datasets does not mean the boundary definitions were completely aligned and wbd units have been updated since making it difficult to relate current units wbd units typically extend out farther into coastal areas than land catchments and larger huc units use the noaa three nautical mile line as a default offshore boundary usgs 2013 the nhdplus framework uses national elevation dataset ned elevations to define catchment boundaries meaning these align with a consistent 30 30 m square grid however catchment boundaries only extended out into coastal areas as far as elevation models can direct flows i e stopping where elevation is zero resulting in catchments that excluded shorelines and estuary boundaries and inhibited networking coastal sample sites to upstream conditions nhdplus recently added ocean catchments a 1500 1500 m grid that extends seamlessly from land catchments out to the noaa 14 mile maritime limit usgs 2019 the ned system has now been superseded by the 3d elevation program 3dep part of the coastal national elevation database coned danielson et al 2018 and national map arundel et al 2015 the nhdplus high resolution dataset in development uses 3dep elevations and differentiates estuaries from the ocean moore et al 2019 suggesting coastline definitions in the medium resolution land and ocean catchments may evolve although square grids are widely adopted as the basis for geospatial frameworks one viable alternative is hexagonal grids hexagonal grids have several advantages over square grids for ecological applications birch et al 2007 a hexagon unit has a smaller perimeter to diameter ratio than a square unit meaning it more closely resembles a circle and has less bias from edge effects this is an important attribute when a point data sample is used to characterize the larger unit as a whole hexagons are better for connectivity and path analysis because the centroids center points of all six neighboring hexagons are equidistant birch et al 2007 in contrast square units have eight neighbors the four neighbors sharing a side are at one distance whereas the other four diagonal neighbors sharing a vertex are at a greater distance hexagonal grids have been used as the basis for coastal frameworks both for smaller scale habitat assessments smith et al 2010 and for broader scale characterizations shamaskin et al 2020 the 2019 noaa alaska department of natural resources coastal priorities assessment used a hexagon based framework to gather stakeholder mapping needs for coned alaska dnr 2020 epa s ncca usepa 2016b samples conditions at points selected through generalized random tessellation stratified survey design these sampling plans are an extension of the former environmental monitoring and assessment program emap diaz ramos et al 1996 where spatially weighted sample points were chosen from a tessellation of triangles to place hexagon unit centroids for statistical randomness the sample site selection methods start the hexagonal grid from a random point pebesma et al 2021 but for consistency some applications have instead used the origin point i e min x and min y of the sample frame or area of interest e g myers et al 2021 tools have generated hexagons to fill the area based on the desired number of sample units desired hexagon area e g evans et al 2021 or a consistent distance between hexagon centroids these differences in hexagon generation and alignment make it difficult to integrate results from disparate efforts or to promote one as the basis for a coastal spatial framework a reasonable alternative to using an arbitrary starting point for the hexagonal grids is to use a discrete global grid system dggs a dggs is a hierarchical data structure for covering the surface of the earth with a consistent grid of regular shaped equal area units in this case the earth is treated as an isosahedron divided into hexagons units spatially reference locations bypassing transformation issues related to planar representations of coordinates dggss are not a new concept in fact they were considered in the early emap surveys carr et al 1997 a dggs provides advantages for big data and digital earth research the past decade has seen several dggs advancements new implementations and example applications birk 2018 sahr 2011 sirdeshmukh et al 2019 jendryke and mcclure 2019 several implementations of dggs have used hexagons as their equal area units including h3 brodsky 2018 dggrid sahr 2019a openeager bush 2017 and rhealpix gibb 2016 dggss have become pervasive enough that the open geospatial consortium ogc has started developing standards for them purss et al 2017 although current implementations may not yet adhere to all these standards bondaruk et al 2020 there is a clear advantage to begin integrating these data structures coastal areas can benefit from a new dggs geospatial framework but it must function at the necessary scales and integrate with existing geospatial frameworks given their spatial overlap and upstream relationship to coastal areas this manuscript focuses on two open source implementations of hexagonal frameworks h3 uber 2020 and dggridr barnes and sahr 2017 the methodology behind how the two hexagonal implementations each scale data is described hexagon scales are then compared to average units from nhdplus the wbd huc 08 and the regulatory units used in attains next we show how h3 hexagonal framework units network together and how to integrate that network with nhdplus land catchments for comparison nhdplus ocean catchments are also networked together and as downstream of land catchments lastly the h3 implementation is used to convert aggregate scale and interpolate actual coastal temperature data for data rich tampa bay florida converted data types included point line and remotely sensed grid data 2 methods 2 1 hexagonal frameworks and scaling two open source implementations of hexagon based dggss were used the h3 python library v3 6 4 uber 2020 and the dggridr r package barnes and sahr 2017 both implementations are available in multiple programing languages this manuscript s supplemental materials include raw data and code examples the code is presented within jupyter notebooks to allow users to repeat our methods on the provided raw data kluyver et al 2016 although maps in this manuscript highlight example areas the jupyter notebook allows users to explore results by panning or zooming to other places on the map hexagon projection and orientation are essential to consistent units h3 hexagons utilize an icosahedral projection with spherical orientation fuller 1975 with a pentagon at each of twelve icosahedron vertices dggridr is an r package for implementing the dggrid software v6 2b sahr 2019a and supports the same typologies i e hexagons triangles or diamonds and projections e g icosahedral snyder equal area to be consistent with h3 all dggridr examples used hexagons the icosahedral projection and spherical fuller orientation hexagonal dggs scaling involves moving from a finer resolution child hexagon to a coarser resolution parent hexagon or vice versa one of the disadvantages of hexagons as compared to square units is that hexagons are non congruent between scales a square unit divides into four equal area subunits e g four 30 m 30 m squares completely fit into one 60 m 60 m square h3 and dggridr divide hexagons into seven sub unit hexagons but those units either do not completely cover the original hexagon or they exceed the original hexagon bounds depending on the aperture used i e the ratio of child units to a parent unit area common apertures used in hexagonal grids are 3 4 and 7 the smaller the aperture the smaller the change in unit size from one scale to the next fig 1 in all three apertures there are seven child units for each parent unit with the central unit sharing the centroid of the parent unit but with different ratios of parent to child area and with different orientations table 1 for aperture 3 the centroid of each of the six child units align with the six parent vertices rotation of units 90 from one scale to the next results in a change in unit orientation pointy topped vs flat topped a single child unit may overlap as many as three different parent units using aperture 3 of the three apertures this aperture has the smallest change in area from one resolution to the next aperture 3 is available in dggridr but was not demonstrated here as it is like dggridr aperture 4 in functionality for aperture 4 the centerlines of each of six child units align with the six parent edges the units have the same orientation across the different scales aperture 4 has the advantage of splitting each of the six child units evenly between parent units dggridr can use aperture 3 aperture 4 or mix the two either to alternate or to approximate a desired unit area at a specific resolution all dggridr applications in this manuscript used aperture 4 scaling aperture 7 has the advantage of fitting the child units most closely to the area covered by the parent unit using this aperture allows h3 to scale data without re indexing the original coordinates however unlike aperture 3 and 4 child units aperture 7 child units do not completely cover the area of the parent unit to resample point data at a smaller scale would require consideration of 12 child units instead of six section 3 4 compares point data scaled using the index method to re aggregated data to assess scaling distortion all h3 applications in this manuscript used aperture 7 scaling 2 2 scale matching with existing geospatial frameworks hexagon scales must be close to decision scales to be useful for representing information in support of those decisions to match hexagonal unit scales to decision scales the hexagon scale with the average hexagon unit area closest to the average unit of three existing frameworks was identified table 2 the frameworks considered were the nhdplus catchments for local decisions e g for restoration the regulatory units used attains for sub estuary decisions e g state impairments and the wbd huc 08 for estuary decisions e g ncca comparisons matching hexagon unit areas to the average unit area of these frameworks helps to identify comparable scales for visualizations but does not capture the information the irregularly shaped units of those frameworks represent for example nhdplus land catchments represent hydrologically connected parts of the landscape because their boundaries are defined by the area of the landscape that drains to a specific nhd stream segment to appropriately represent hydrologic relationships using the hexagonal frameworks hexagon units would need to be the same size as the units used to originally delineate the catchments and be related to nhd stream segments a fourth scale of hexagon units an elevation data scale was approximated to match the 30 m grid used to delineate catchments nhdplus land catchments have an average area of 2 9 km2 nhdplus ocean catchments are 1500 1500 m square units but at the land ocean catchment boundary these squares are only partial to avoid overlapping land catchments as a result the average area for ocean catchments is 2 01 km2 usgs 2019 attains waterbodies vary but the average area is 22 km2 usepa 2020 the average wbd 8 digit huc is 1800 km2 with a range of 10 6 19 200 km2 the elevation data scale is the smallest square cell considered 900 m2 these square units are regularly shaped and unlike the other scales their accuracy is as important as their area grid cell accuracy is the maximum distance between any two points in that cell measured as the distance from vertex to opposite vertex approximately 42 m these average areas represent decision scales to approximate with hexagonal units from h3 and dggridr 2 3 networking units within and across frameworks coastal waterbodies are not closed systems water flows in from upstream watersheds and mixes within the embayment and with ocean waters outside the embayment modeling this mixing is beyond the scope of this manuscript however a coastal geospatial framework must have the capacity to network units within the framework to support mixing flows and be able to network as downstream of watershed units all the frameworks examined in this manuscript use a unique index to identify individual units in nhdplus each land and ocean catchment geometry has a unique id for land catchments this same id is stored in a field comid in a separate plusflow table that captures the network of upstream downstream relationships flows between units in the estuary are not as easy to define as those in watersheds but methods using unit indexes are demonstrated for networking ocean catchment and h3 hexagon units to both neighboring units within those frameworks and to neighboring land catchments ocean catchments and h3 hexagons downstream of land catchments were identified and integrated into the flow network downstream ocean catchments were those adjacent to a land catchment while downstream hexagons were those overlapping a land catchment the ocean catchment comids or hexagon indices from the corresponding downstream unit were added to the plusflow table ocean catchments already have a 9 digit index starting with 7 unique from any land catchment 9 digit land catchment indexes start with either 1 or 9 the h3 index is an unsigned 64 bit integer uint64 e g 8b441a8f028cfff that had to be converted to store as integer based indexes in the plusflow table e g 627198441950859263 converted h3 indexes all have more digits than the 9 digit nhdplus indexes making it easy to avoid duplicates open water ocean catchments and h3 spatial units non adjacent and non overlapping with land catchments interconnect with one another as well each open water ocean catchment had four neighboring catchments added to the plusflow table as downstream this resulted in all ocean catchments being both upstream and downstream of their neighbors h3 uses a hierarchical index where each identifier is specific to a specific spatial unit and resolution brodsky 2018 the index system allows the coarser resolution parent hexagon to be identified from the finer resolution child hexagon by simply truncating the index the assignment of digits 0 6 at each resolution uses a central place indexing arrangement sahr 2019b allowing h3 to identify neighboring edge cells or cells n given cell widths away from a unit by its index h3 has built in index based methods e g hexrange and kring methods that were used to network units within the framework since the center of each hexagon is a consistent distance from its neighbor these same methods were also used for distance based interpolation as described in section 2 5 compared to h3 dggridr indexing is simpler each spatial unit in a grid has a global sequence parameter seqnum numbered 1 to n where n is the number of cells generated however ddgridr does not have the same built in index based neighbor identification as h3 and therefore was not used to network cells within the framework as with h3 hexagons any dggridr hexagons overlapping a land catchment could be networked as downstream of that catchment by adding its index to the plusflow table as with ocean catchments dggridr hexagon neighbor to neighbor relationships could be stored by index in the plusflow table in both cases the hexagon index must not duplicate any existing catchment featureid 2 4 demonstration site and data aggregation raw data aggregation data scaling and integration with nhdplus using dggridr hex aperture 4 and h3 hex aperture 7 hexagons are demonstrated in the tampa bay estuary as defined by the ncca survey program fig 2 a tampa bay was chosen because the area is data rich three datasets point line and grid were converted to h3 hexagon units the point data came from sampling points in the wqp fig 2b results at these sites represented water temperatures uploaded to the wqp read et al 2017 usepa et al 2021 line data came from the noaa continually updated shoreline product cusp fig 2c noaa s cusp dataset was downloaded for gulf of mexico noaa 2020 and lines intersecting hexagons were characterized as having shoreline present absent grid data came from the multi scale ultra high resolution mur fig 2d sea surface temperature dataset a daily 1 km resolution dataset that merges multiple level 2 satellite datasets chin et al 2017 sea surface temperatures over land were represented with no data temperature point and grid data were used because it influences species composition varies more in coastal environments and correlates with circulation azmi et al 2015 brando et al 2015 physical characteristics such as temperature are also the most common result in wqp read et al 2017 point data from the wqp were pre processed before aggregation to ensure consistent data the query restricted results to water temperature sample points within the ncca tampa bay polygon extent and from january 1 1995 through december 31 2020 python scripts automated data gathering and pre processing see supplemental materials pre processing involved a few key assumptions about coordinates time zones depth and duplicates coordinates retrieved for sample sites were transformed to nad 1983 if in a different datum e g wgs 1984 if the coordinate reference system was not specified the script assumed site coordinates were nad 1983 because wqp uses it in spatial queries time zone handling assumed time zone entries were accurate where any time zone listed as standard time or daylight time uses that coordinated universal time utc adjustment regardless of time of year for example eastern standard time est is interpreted as utc 5 h and eastern daylight time edt as utc 4 h even if an est time is during daylight savings this is consistent with usgs dataretrieval packages for r or python de cicco et al 2018 hodson and black 2020 only surface temperatures were examined to be more consistent with sea surface temperatures from remote sensing results with a depth greater than 5 m were rejected and those with no depth measurement were assumed to be surface measurements only the first of any duplicate temperature for the same location and time was considered 2 5 data interpolation a simplified inverse distance weighting approach to interpolation was taken for units with missing 2011 average temperature data across h3 scale 11 and scale 7 hexagons this simplistic approach used built in krings weighting each by the distance band and giving less weight to neighbors based on how many cells away they were to a maximum distance of 20 cells due to the number of empty cells it was important to rescale each value based on the number of values at all distance bands rather than the number of cells at each distance band the equation can be expressed as hex value i 1 n 20 x i k i 1 k where x the value from a given cell is divided by k its distance band from the hexagon of interest the result for all neighboring cells with a value within distance bands of 1 to n 20 is summed for the numerator the denominator is the sum of 1 divided by distance bands used since all points within a distance band are treated as equidistant and each band is a multiple of that distance the actual distance is never measured directly 3 results 3 1 comparison of scales four scales were considered from existing spatial frameworks table 2 nhdplus land and ocean catchments local attains waterbodies sub estuary wbd huc 08 estuary and the elevation data resolution table 2 fig 3 the average area of h3 and dggrid hexagon units at different scales were compared to units from existing spatial frameworks at the four scales table 3 h3 hexagon units were mapped alongside units from the spatial frameworks at the four local scales to visualize this comparison in tampa bay fl fig 3 at the local scale comparable to 2 0 2 9 km2 ocean land catchment units h3 hexagons scale 8 to 7 have 0 7370 5 4610 km2 areas respectively dggridr hexagons scale 13 to 12 have 0 7601 3 0402 km2 areas respectively at the sub estuary scale comparable to 22 km2 attains waterbodies h3 scale 7 to 6 hexagons have 5 1613 36 1291 km2 areas dggridr scale 11 to 10 hexagons have 12 1609 48 6436 km2 areas at the estuary scale comparable to a 1800 km2 8 digit huc h3 scale 4 to 3 hexagon areas are 1770 3240 12392 2649 km2 and dggridr scale 8 to 7 hexagon areas are 778 2984 3113 1935 km2 comparing the elevation data grid size to units at different hexagon scales should be based on the accuracy of those units rather than coverage area the accuracy of a unit is based on the maximum distance between any two points in that unit the maximum distance from any point in a hexagon to any other point in the hexagon is the width along the long diagonal measured vertex to opposite vertex and is double edge length for a square it is measured from vertex to opposite vertex and is 2 times the length of one side elevation data on a 30 m grid therefore has a 42 m accuracy and is comparable to a hexagon unit with a width of 42 m this is within h3 hexagons scale 12 to 11 with an 18 8320 49 8220 m accuracy for dggridr it is within hexagons scale 18 to 17 with a 33 8047 67 6094 m accuracy in other words h3 scale 12 hexagons with a width of 18 832 m are comparable to a square unit 13 316 m wide whereas h3 scale 11 hexagons are comparable to a square unit 35 235 m wide the highest scale h3 hexagon is 15 with a 0 9 m2 area and an accuracy comparable to a square unit 0 7212 m wide due to its less restricted indexing dggridr has a maximum scale of 30 with the projection and aperture explored here at scale 30 hexagons have a 0 4424 cm2 area comparable to a square unit 0 5835 cm wide the intent of these scale comparisons is to represent a similar level of aggregation section 3 4 explores point data converted to h3 hexagons for potential data scale distortion when moving between scales without re aggregating 3 2 networking frameworks ocean catchments and h3 hexagons were added to the flow network as downstream of land catchments within ocean catchments and h3 hexagons adjacent neighbors were also networked for ocean catchments these were non diagonal neighbors to visualize this network structure a field was added to land catchments and one coastal catchment was populated with a value of 10 using the network upstream downstream spatial units were given a value of 10 n where n is the number of units away from the source to the unit this helps to quickly show how neighboring units relate fig 4 the relationship is simple but the structure allows for more sophisticated relationships between sources in upstream land catchments and downstream coastal hexagons 3 3 data aggregation using the extent for the tampa bay area wqp returned 12 356 unique sample sites of those sites 9034 were within the ncca defined tampa bay area aggregating these site points to the h3 scale 12 hexagons resulted in 6554 hexagons the maximum number of sites in one hexagon was 12 aggregating to h3 scale 11 reduced this number of hexagons to 5550 with a maximum site count of 22 at two hexagons fig 5 over the 26 year period of data retrieved 1 4 1995 to 12 21 2020 there were 182 418 water temperature results at the sample sites this number was reduced to 170 161 water temperature results after pre processing with 1 5698 results at each of 5550 sites aggregating to daily averages reduced the total to 54 756 results across 3987 unique days 42 of the 9480 days retrieved aggregating to annual averages reduced results to 14 012 most sites 92 only had results in one year the year 2004 had the most sites with temperature results with 883 the 2004 average temperatures aggregated to 794 hexagons with a maximum of 6 sites in a single scale 11 hexagon fig 5 the results from converting noaa s cusp data to hexagons were more convoluted than anticipated each vertex is converted to a hexagon index meaning sub hexagon scale intricacies such as dips or double backs are lost at course scales the set of hexagon indices that make up each cusp line can be treated either individually to allow for identification of multiple parts of the shoreline within single hexagons or combined to identify the unique set of hexagons intersecting cusp lines in aggregate the latter approach was employed as it is much easier to visualize fig 6 the spatial resolution of the gridded data was 1 km between h3 scales 7 and 8 the geotiff was converted to a comma separated values csv file with coordinates and values and the points were aggregated to scale 7 hexagons this has the same effect as a fishnet with points at raster cell centroids scale 7 hexagons were larger than the gridded data resolution and each hexagon value is the average of the point values within it this may distort aggregate values slightly compared to an area weighted average but should be comparable to nearest neighbor resampling scale 8 hexagons are smaller than the grid resolution and the same method would create gaps anywhere a hexagon fell between raster centroids to avoid this the grid data was instead sampled using scale 8 hexagon centroids fig 7 3 4 data scaling distortion a global spatial system faces several spatial distortion sources one source is the non congruence of the aperture 7 hexagon units between scales when data is scaled by truncation as it allows data to be assigned to a parent hexagon that does not overlap the original coordinate point fig 8 the tampa bay demonstration identified examples of this scaling distortion between scale 11 with 2293 hexagons and scale 10 with 2904 hexagons only 97 hexagon assignments changed when using index based scaling h3 to parent compared to reconverting sample points to scale 10 this represents a small portion 5 of the 1904 hexagons at scale 10 this comparison did not account for sites that may have been incorrectly assigned to a parent cell already containing a different sample site based on the geometry 14 375 of the area of a parent scale unit is non congruent with units of a child with 7 of the child unit area outside the parent unit and 7 of parent unit area outside the child units it is important to note assigning coordinates a new index at a new scale avoids this distortion index based scaling is not an option for the aperture 4 hexagons generated by dggridr as scaling distortion would be greater their more regular geometry with half of each exterior child unit outside of the parent unit leads to 43 of child cell areas being outside of the parent unit but no points within the parent unit that are not in one of the child units identifying points in aperture 4 units with multiple parent units for selective re indexing is computationally feasible however re indexing is computationally efficient making it more reasonable to just re index all aperture 4 parent units 3 5 data interpolation the available water temperature point data were aggregated to hexagons but there are many hexagons within tampa bay that do not have temperatures sampled the equidistant network of hexagons and built in hex ring hexrange and k ring kring methods make simple distance weighted linear interpolation of these hexagon values efficient fig 9 though this interpolation is simple the same built in methods can be used with more advanced machine learning methods 4 discussion the two hexagonal implementations h3 and dggridr were both able to represent data at similar scales to the existing frameworks explored here nhdplus land and ocean catchments attains waterbodies and wbd huc 08 for this application the level of approximation achieved by both hexagonal frameworks was adequate given that it was well within the range of areas for irregular units such as catchments both a larger and smaller hexagon resolution were identified for each decision scale leaving options for end users to tailor to their specific context if matching unit area more closely is important dggridr hexagons of mixed 3 and 4 aperture can be used though not available in dggridr ddgrid does this with preset grids e g approximates a 500 m grid with the superfund preset sahr 2019a both implementations have scales able to represent the original data used to define the other frameworks i e the nhdplus raw data resolution these scales were compared based on the accuracy of raw data rather than the unit area i e 42 m for the 30 m grid the same scales also exceed the sample point accuracy 0 2 nautical miles 37 m required of ncca methods usepa 2010 although hexagon scales compared favorably to existing framework units hexagons at these scales would not represent the same spatial relationships between units to use hexagons this way the raw elevation data would need to be aggregated to hexagons and then hexagon based catchments could be delineated liao et al 2020 used a hexagon grid to delineate watersheds and found improvements over conventional square grids suggesting the same could be done using h3 or dggridr the most resolute scale used in h3 scale 15 0 9 m2 unit area is more resolute than similar frameworks in development such as nhdplus hr buto and anderson 2020 and even higher resolution land use land classifications such as noaa s 1 m2 coastal change analysis program mccombs et al 2016 however this is not enough to compare to the accuracy of some data collection efforts such as individual lidar point clouds and with advances in data resolution over the past decade it is conceivable that h3 scale 15 may become inadequate for some applications in the future dggridr is less limited by the index system used and therefore can achieve smaller scales than h3 down to a scale 30 with 0 8253 cm unit area by tweaking the aperture and projection even smaller units could likely be achieved this application benefitted from h3 index methods but there may be other use cases where dggridr is superior h3 aperture 7 indexing was more computationally efficient compared to dggridr seqnum assignment h3 built in index based neighbor functions were used for networking flows between h3 units whereas networking dggridr units required storing neighbors in a plusflow like table since coastal flows were assumed to be bi directional instead of one way like upstream to downstream this flow table gets larger faster and could be prohibitive at large scales some of the built in index based functionality in h3 could be added to dggridr but would require customization for each aperture projection and typology some of this functionality may be available in newer versions of dggrid not yet used by dggridr without the full suite of dggrid options such as the superfund preset grid dggridr lacks h3 s simplicity of navigation between grid resolutions and neighbors compared to h3 dggridr has more options for how hexagon units are defined i e aperture projection typology can achieve smaller scales 30 vs h3 s 15 and can use mixed 3 and 4 apertures to better approximate unit areas this makes dggridr a better implementation for applications requiring hexagon units that are very small in a specific area or otherwise need to align in a specific way h3 had a clear advantage over dggridr or ocean catchments when networking neighboring units determining neighbors from the unit index directly rather than needing to store it in a separate table this allows omnidirectional networking but more sophisticated connectivity would have to be stored in a different way one option not explored here is to use alternative h3 index modes modes are 4 bits of the 64 bit integer where mode 1 is the cell index mode 2 is a unidirectional edge index e g unit 1 unit 2 and mode 3 will be a bi directional edge index e g unit 1 unit 2 uber 2020 although mode 3 is still in development this could be an area for future exploration in addition to networking hexagonal units together another objective was to network the units to other frameworks networking to nhdplus catchments was of particular interest given these could act as upstream sources of downstream water quality impairments to connect any of the units to land catchments the unique id was added as downstream to neighboring land catchments in the plusflow table this worked well for both ocean catchments since they already have a 9 digit index unique from any land catchments and for h3 indexes since they all have more than 9 digits given the sequential dggridr seqnum assignment the risk of duplicating an nhdplus index is much higher depending on the region being considered in tampa bay the number of dggridr units would have to be considerable 25 000 for this to be an issue one way to avoid the issue would be to alter the stored index for example making it a 9 digit index not already occupied by land catchments e g add 500000000 making index 1 into 500000001 once hexagons or ocean catchments were networked to land catchments the functionality of the network changed slightly land catchments are upstream downstream of one another and one catchment may have multiple catchments upstream or downstream stepwise navigation incrementally querying the next upstream or downstream catchment s can be chained for example to exhaustively identify all catchments upstream or downstream in that watershed stepwise navigation still works with ocean catchments or hexagon units but these units are interconnected with one another and all watersheds causing an exhaustive search to yield all catchments instead of one watershed land catchments are also associated with stream segments that have a distance allowing the network to perform distance based navigation for example the network could determine what catchments are within 5 km upstream although distance based navigation using the expanded network still works for land catchments ocean catchments and hexagon units do not have a similar flow path to use this functionality the distance from one unit to the next could instead be measured as the distance from centroid to centroid although the result is zig zag routing from unit to unit that measures manhattan distance rather than the shortest navigable path this exploratory work did not investigate spatial overlap issues with integration coastal hexagons were defined by ncca estuary boundaries rather than areas missing from land catchments and they were left overlapping with land catchments rather than clipping them to be seamlessly adjacent as ocean catchments are changing this would not alter how h3 hexagon indexes are assigned when filling the area of interest when assigning raw point data to hexagons that data could be reduced to the area not covered by land catchments to avoid double assignment the geometry of the hexagons could be clipped to the outline of land catchments like partial ocean catchments when making area based calculations and for visualizations a similar clipping approach could be taken to identifying hexagons that are split into sub units by land catchments although these hexagon sub units share an index flow between them may be inhibited by land this is not always true since land catchments often extend into submerged areas ideally hexagon based elevation models could be used to develop hexagon based catchments and then those would be seamlessly adjacent in such a framework units at the land sea interface could be identified as such using cusp as was done here or polygon based shoreline definitions such as open street map s osmcoastline 2021 the main advantage of polygon based definitions is that aquatic and terrestrial areas can be discerned explored scales were adequate for aggregation of point line and grid data the number of sample points in overlapping h3 scale 11 hexagons suggests this is an adequate scale for differentiating most sites in estuaries when multiple sites fell within the same hexagon the sites tended to have different sampling times which suggests that the sampling sites were either moving over time or are intensive limited duration studies temperature data in tampa bay was investigated because it is a commonly measured characteristic and tampa bay is data rich suggesting hexagon scale 11 should be adequate in other estuaries with less data as well as coastal water quality monitoring devices become more affordable and report readings with greater frequency h3 scale 11 may become too large but smaller alternatives are readily available down to scale 15 although scale 11 was adequate for most cusp lines these lines presented some interesting issues cusp lines are discontinuous meaning they are not suited for determining what percent of the hexagon area was land or water based polygons such as those available from openstreetmap coastline database are better suited for that and merit future investigation when cusp lines were compared to land catchments it showed these catchments do typically extend into the water but it was difficult to consistently measure how far because cusp lines are often close to each other to represent small channels although similar raster data equivalents now exist for hexagons de sousa and leitão 2018 national datasets in the gridded raster format are far more prevalent raster images representing remote sensed temperatures were aggregated to h3 hexagons scale 7 and 8 following the existing best practices and workflows ion et al 2014 robertson et al 2020 reinforced that these scales were most appropriate at scale 8 some hexagons covered multiple grid cells and aggregating by hexagon centroid would cause under sampling grid cells not being included so they were sampled by grid cell centroid and averaged per hexagon at scale 7 the space between some grid cell centroids exceeded the width of hexagon units to deal with this the grids were aggregated by hexagon centroid temperatures over land were excluded from analysis but land also interferes with some of the remote sensing methods included in the dataset which may impact near coastal data accuracy chin et al 2017 future efforts to fill gaps at unsampled sites will benefit from considering remotely sensed data but should consider alignment of resolutions and the level of interpolation already present scale distortion was present in tampa bay when using built in index based methods to go from scale 11 to scale 10 hexagons this distortion occurred in a limited number of situations and at least at scale 11 reconverting sample points to hexagon indices at the new scale was not time or processing intensive future work with a larger scope e g across the gulf of mexico or all u s coasts could benefit from using the built in index based scaling especially for visualizations or other use cases where the decreased accuracy is a worthwhile tradeoff for reduced processing or storage size even as water quality data increases it will remain difficult to completely sample an estuary making it important to be able to estimate water characteristics between sites here the intention was to demonstrate how to interpolate using the h3 framework h3 built in methods for equal distance bands were helpful when conducting a simple inverse distance weighted interpolation at the smaller h3 scale 11 fig 9d the interpolated surface is not very smooth this is likely due to a combination of data spatial and temporal sparsity visible in fig 9c where most units had no data and the interpolation method used the same built in methods could also be used for more complex interpolation and more advanced machine learning methods e g pytorch paszke et al 2019 more sophisticated hydrodynamic models could make use of h3 mode 2 and mode 3 to store directionality of flows across time periods to further calibrate such models to more realistic mixing a 20 neighbor limit was placed on how far away to interpolate but for actual use cases there are additional considerations for example spatial autocorrelation places limits on how fine a scale data should be discretized and or interpolated at from an efficiency perspective smaller grid units allow for greater granularity and precision but tradeoff against the storage and computational cost in large areas with no data one alternative to interpolation may also be hexagon compaction where hexagons of coarser scales are intermixed between smaller hexagons to reduce the storage and computation of that data compaction relies on truncation to move between scales so it introduces shape distortion but the reverse un compaction is exact this means data could be interpolated at higher scales then compacted for most use cases 5 conclusion discrete global grid system implementations will likely continue to evolve to meet new ogc standards yet this work showed that even existing hexagon implementations are already an effective alternative to existing spatial frameworks two dggs implementations were tested in a coastal area for their ability to match existing decision scales integrate with existing spatial frameworks and represent existing coastal water quality data dggridr was at least as effective as existing ocean catchments and h3 had additional index structuring that facilitates data scaling and interpolation as coastal data increases in velocity and volume a dggs is an appropriate way to continue to process this data while also still being able to integrate with other spatial frameworks disclaimer the views expressed in this article are those of the authors and do not necessarily represent the views or policies of the u s environmental protection agency any mention of trade names products or services does not imply endorsement by the u s government or the u s environmental protection agency the epa does not endorse any commercial products services or enterprises this contribution is identified by tracking number ord 042125 of the u s environmental protection agency office of research and development center for environmental measurement and modeling gulf ecosystem measurement and modeling division declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank ashley chappell michelle thawley kevin summers lisa m smith and elizabeth george for their thoughtful reviews and insightful comments appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105210 
