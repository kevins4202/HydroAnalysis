index,text
25395,this article presents apexmod an open source qgis plugin that helps add a modflow grid to the apex watershed model the apex model integrates modflow a physically based groundwater model and rt3d salt a groundwater nutrient and salt transport model for basin scale application as a coupled surface subsurface hydrologic modeling system apexmod is developed as a tool that enables the dynamic linkage between apex modflow and rt3d salt apexmod is written in python and integrates qgis geoprocessors and spatial packages the apexmod interface is constructed with three modules i a pre processor that prepares inputs for modflow and rt3d salt and performs spatial linkage between apex subareas and modflow grid cells ii a model configurator that controls simulation options and scenario setting and iii a post processor for visualizing and analyzing model outputs a case study was conducted in the animas river watershed to demonstrate the core capabilities of apexmod keywords apex modflow rt3d salt qgis plugin groundwater surface water interactions data availability i have shared the link to my data in the manuscript 1 introduction the apex agricultural policy environmental extender model is a distributed continuous daily time step farm or watershed scale hydrologic water quality model williams et al 2008 the apex model has been applied in numerous modeling studies in the united states because of its robust crop growth routines for approximately one hundred crop types gassman et al 2009 x wang et al 2012 similar to other watershed models arnold et al 1998 subsurface hydrologic processes of apex such as groundwater flow groundwater storage and groundwater surface water interaction are calculated by simple water balance and lagging equations the oversight in groundwater hydrologic simulation in apex makes it difficult to evaluate watershed hydrology in watershed where groundwater drainage is a dominant component of surface water flow worqlul et al 2021 to provide physically based groundwater flow modeling into the apex code bailey et al 2021 coupled apex and modflow niswonger et al 2011 for dynamic integration subsequently the apex modflow modeling code was extended to nutrient and salt ion transport simulation bailey et al 2022 the subroutines simulating the transport of salt ion transport in groundwater are based on rt3d clement 1997 tavakoli kivi et al 2019 the apex modflow rt3d code was applied and tested in the price river watershed utah usa bailey et al 2022 these individual modeling components of the enhanced apex model have their graphical user interfaces guis for applications as individual software programs for example winapex gassman et al 2009 and arcapex tuppad et al 2009 are windows programs that help build apex input files winapex is a windows interface program designed to assist users in developing input files and executing apex for a field or small watersheds arcapex is an arcgis based user interface that integrates gis capabilities and algorithms in the arcgis development framework redlands c e s r i 2011 arcgis desktop release 10 with apex databases input and output management for watersheds and river basins guis for modflow and rt3d include groundwater modeling system gms owen et al 1996 goundwater vista rumbaugh and rumbaugh 2004 and public domain tools e g freewat foglia et al 2018 modelmuse winston 2009 no gui has been developed for creating salt inputs for rt3d salt integrating physical models help explore the hydrological dynamics in a complex watershed and evaluate constituents transport processes however the integrated apex modflow rt3d salt amrs model requires a substantial amount of effort in input preparation and spatial linking of apex subareas and modflow grid cells which may hamper its broader adaptation and utilization meyer and riechert 2019 nielsen et al 2021 besides standard input files required by individual models spatial linkages between apex subareas and modflow grid cells and between apex streams and modflow river cells must be established and corresponding details must be stored in additional input files the spatial information stored in the linkage files is used to pass hydrologic volumetric fluxes m3 day and solute mass between apex subareas apex stream channels and modflow rt3d grid cells preparing these linkage files is generally a manual task that is a slow tedious and error prone process the process of spatial linking objects between two models has been implemented in similar integrated hydrological models gsflow markstrom et al 2008 mike she butts and graham 2008 parflow kollet and maxwell 2006 hydrogeosphere brunner and simmons 2012 swat modflow bailey et al 2016 kim et al 2008 and swat modflow bailey et al 2020 swatmod prep bailey et al 2017 and qswatmod park et al 2019 were developed to provide users with a standardized and easy to use workflow for model application and evaluation qswatmod is a plugin tool for the open source qgis environment http www qgis org a similar gui for the amrs model is needed for input file preparation running simulations and viewing the output of the models that comprise the amrs this paper presents apexmod a new gui developed as a qgis plugin for the amrs qgis was chosen as an ideal platform for both the development and implementation of the apexmod workflow qgis processing functionalities and the python engine have been shown to outperform other software such as arcgis and surfer surfer from golden software llc www goldensoftware com in several qgis plugin tools dile et al 2016 foglia et al 2018 nielsen et al 2017 2021 that have demonstrated adequate functionalities for most general applications in water resource management apexmod automates the spatial linkage between apex and modflow and prepares inputs for rt3d to simulate the reactive transport of nitrate phosphate and eight major salt ions so4 2 cl co3 2 hco3 ca2 na mg2 and k required inputs for the rt3d subroutines include the initial concentration of nitrate and phosphate and parameters for sorption and kinetic reactions e g denitrification required inputs for the salinity module include initial salt ion concentrations in groundwater and soil water and initial fractions of salt minerals in the aquifer and the soil profile here we demonstrate the features and capabilities of apexmod in a case study conducted in the animas river watershed which spans portions of colorado and new mexico usa 2 material and methods 2 1 overview of apexmod structure apexmod is developed as a qgis plugin program written in python 3 van rossum and drake 2009 the purpose of the software is to provide the amrs model users with utility in preparing input data configuring simulation settings and viewing analyzing and post processing results from the amrs model through a user interface in the qgis platform the current version of apexmod described in the manuscript is the version 1 4 3 which is the latest version available in the github repository https github com spark brc apexmod releases the amrs model https github com spark brc amrs releases tag v0 2 0 requires inputs from a specific version of apex v1501 and modflow 2005 or nwt while it is possible to generate rt3d inputs using other software such as gms it should be noted that these inputs may not be compatible with the amrs model due to the absence of salinity information for further details regarding the required model inputs please refer to section 2 3 although the current version of apexmod doesn t provide a calibration option we have developed a python library called apexmf https github com spark brc apexmf specifically for calibration with a jupyter notebook workflow kluyver et al 2016 which is an integral part of our apexmod development this workflow and library are actively being tested with the parameters of all four models to provide reasonable ranges as initial values in the apexmod tool and and will be made available in the next version of apexmod to make apexmod functional qgis3 https download qgis org is required on the system our recommendation is to use the long term release version of qgis3 firenze as we have only tested apexmod with the ltr versions 3 10 4 3 28 5 of qgis3 after qgis installation apexmod can be installed from an installer file apexmod exe available for downloading at a github repo https github com spark brc apexmod releases download v1 4 3 apexmod exe and currently compatible with the windows operation system the apexmod plugin can be launched from the qgis toolbar followed by one time manual activation via the manage and install plugins window under the plugins menu in qgis fig 1 a unlike the qswatmod qgis plugin park et al 2019 users are not required to install dependent python libraries flopy bakker et al 2016 and pyshp which are not available in qgis by default manual installation of third party packages on qgis can be problematic for novice model users because it requires installation through prompt syntaxes and administrator privileges for the windows system instead the current release version ver 1 0 or higher of apexmod offers these external dependency packages directly embedded within apexmod thus additional python system updates or installations via python s standard package manager pip are unnecessary apexmod comprises three main modules pre processing configuration and post processing hydrology and solute transport each main module is implemented in different tabs fig 2 a user can create a new project or open an existing one when the user creates a new project apexmod generates a qgis qgz project file and a neighboring project directory with the same name the main structure of an apexmod project consists of apexmf exes holding model executables apex modflow model inputs and outputs db database file containing project and file paths and project status exported files post processing files gis rasters and vectors and scenarios management inputs and outputs folders under the main project directory fig 3 2 2 pre processing module the pre processing tab loads existing model input files apex and modflow create models modflow rt3d and salinity and perform a spatial linking process in step 1 users need to provide apex model inputs subbasin and river vector files fig 4 a in the modflow option 2 users can load an existing modflow model if the modflow model has a grid shapefile that defines a boundary the grid shapefile can be loaded for the linking process if not users must provide coordinates of the north west corner of the modflow model boundary to create a modflow grid the modflow grid can also be created based on the extent of the apex subarea boundary shapefile with reading cell information width length top elevation from the modflow discretization file dis instructions on creating a new modflow model are provided in section 2 3 spatial linkage is required between modflow river cells and the apex river network to simulate groundwater river flow interactions step 3 in fig 4a the locations and quantities of their spatial linkages can affect the model performance streamflow sediment yield water table and water quality thus apexmod provides three options 1 use only the modflow river package 2 use only the apex river network and 3 use both modflow river packages and the apex river network for the first option apexmod extracts river cell values river stage riverbed conductance river bottom elevation from the modflow river package file riv the second option is to create a new modflow river package by intersecting the modflow grid with the apex river network shapefile and then calculating the river cell parameters for each river cell based on information provided by the apex river network stream stage stream length and width and user inputs thickness and hydraulic conductivity of riverbed material for example riverbed conductance for each river cell is calculated by equation 1 r i v c d l w r i v k r i v t where l is the length of the river w is the width riv k is riverbed hydraulic conductivity and riv t is riverbed thickness for the third option apexmod reads information from the existing modflow river cells and river cells created by the apex river network and combines them to generate new combined river cells users can choose or remove grid cells from the combined river cells based on users decisions then create a new modflow river package users can start the linking process without completing steps 4 5 and 6 the description of step 4 is provided in the supplementary material the description of steps 5 and 6 is provided in section 2 3 the check modflow file option step 7 creates the modflow mfn file by modifying unit numbers in the modflow input nam and output oc control files and checking the dis file simulation period and model state all the modifications made are stored in the modflow editlog txt file the unit numbers are file index identifiers used by the modflow and apex codes whose values are often below 100 for stand alone modflow and apex simulations each input output file in the apex modflow simulation must have a unique unit number the check modflow file option modifies the given unit numbers to be greater than 5000 in the modflow mfn file to not coincide with numbers already assigned to multiple apex input files in addition step 7 converts the state of a modflow simulation to transient tr in the modflow discretization file dis if the already existing modflow is set to steady state ss because the apex model is continuous which is the transient state the linking process step 7 intersects the subarea vector polygons shapefile with modflow grid cells creates modflow grid and subarea indexes and calculates grid and subarea sizes and the size of subarea occupied by each modflow grid apexmod uses these intersected sub grid and riv grid objects to create the three required apex modflow linkage table files link sa grid link grid sa and link river grid which are stored in the project folder modflow 2 3 utilizing modflow rt3d and salinity models if a modflow model were not provided only the 3rd option could be checked from step 2 of the pre processing tab fig 4a and apexmod allows users to create a simple modflow model with single layer powered by the flopy3 bakker et al 2016 package because a modflow model created by apexmod can be used as a stand alone model users need to specify a path to a folder where to store modflow inputs and its model name step a 1 in fig 5 a raster file of the digital elevation model dem and a polygon shapefile for the watershed boundary are required to determine modflow s top layer elevation and modflow model boundary respectively step a 2 apexmod offers the option to create the modflow model boundary based on the extent of the apex model boundaries if no boundary information is available step a 4 is the same as the second option of identifying river cells using the apex river network stream stage length and width and user inputs thickness and hydraulic conductivity of riverbed material step a 5 sets aquifer properties including aquifer thickness hydraulic conductivity specific storage specific yield initial hydraulic head and evapotranspiration a single value for a homogenous input or a raster file for heterogeneous inputs can be loaded for these aquifer properties users can adjust the horizontal anisotropy step a 5 in fig 5 the ratio of hydraulic conductivity along columns to hydraulic conductivity along rows where kx is the hydraulic conductivity along rows the aquifer thickness and initial hydraulic head are specified with two options single value subtracted from groundwater surface elevation at each cell to provide a value of bottom layer elevation and initial head for each cell or uniform value which is the constant elevation for the whole watershed step a 6 reads all aquifer properties and writes input text files in the format required by modflow bailey et al 2022 integrated the rt3d modeling code with the apex modflow code of bailey et al 2021 and the rt3d code was modified to simulate the transport of the eight salt ions as well as nitrate and phosphorous called rt3d salt additional inputs are required to simulate the reactive transport of these solutes according to advection dispersion and chemical reaction terms apexmod incorporates the rt3d salt component of the enhanced apex model the rt3d ui can be popped up separately fig 5b by clicking the rt3d activation button step 5 in fig 4a once the rt3d window is opened apexmod reads grid information from the modflow model creates an rt3d grid object and stores active and inactive cell properties porosity and initial concentrations for nitrate and phosphorus can be provided as a single value or raster value linear sorption coefficients for nitrate and phosphorous the first order rate constant of denitrification 1 time and the monod half saturation term denitrification are provided regarding sorption and reaction parameters for the aquifer dispersivity apexmod sets up the bulk density of the aquifer porous medium m l3 longitudinal dispersivity l horizontal and vertical transverse to longitudinal dispersivity ratios and effective molecular diffusion coefficient like setting groundwater level observation wells step 4 in fig 4a locations of solute observation wells can be established by using a point shapefile of the observation well or through the user selecting cells from the modflow grids on the qgis canvas due to the potentially sizable daily grid outputs 1 gb apexmod provides an option to set the frequency of writing daily grid outputs step 4 in fig 5b reads all rt3d settings and writes input text files such as basic transport package btn advection adv dispersion dsp reaction rct source sink mixing ssm and super file rt3d filenames after all necessary rt3d inputs are generated the salinity model option is activated users can prepare salinity inputs to simulate eight salt ions in the surface and subsurface and their interactions using the salinity ui fig 5c steps 1 and 2 in fig 5c guide users to provide eight initial salt ion concentrations and five salt mineral fractions for soil and aquifer with a single value raster of values or an already created vector layer after the salinity module is activated two vector layers salt input sub for soil salt ions and salt input grid for aquifer salt ions will be generated the salt input sub and salt input grid vector layers will store eight initial salt ion concentrations and five salt mineral fractions for each subarea and each modflow grid respectively users can also load an already created vector layer that includes field names of salt ions or fractions then apexmod reads and finds a matching field name extracts values from the field and stores them in the salt input sub or salt input grid layer for the initial salt mineral fraction of soil apexmod can use gridded soil survey geographic gssurgo geodatabase type from usda nrcs https nrcs app box com v soils folder 148414960239 once users provide a gssurgo database apexmod will start loading all necessary polygon and tables mupolygon component chorizon from the geodatabase and linking caco3 and caso4 mineral fraction values to the salt input sub vector layer using qgis default functions such as join intersection and identification this option in the current apexmod version is available only for the united states including alaska and hawaii the last dataset step 4 in fig 5c for the salinity module is irrigation salt concentration mg l referring to eight salt ion concentrations in irrigation water for each subarea dataset can be provided in the same way as step 1 or 2 in fig 5c if no dataset is given apexmod will consider zero salt concentrations in irrigation water and store zero values in the irri salt sub vector layer step 4 in fig 5c reads all salinity datasets writes input text files salt input and salt and modifies the btn file by adding gridded base salt data the required inputs for modflow rt3d and salinity models are summarized in table 1 2 4 configuring modules and scenarios the enhanced apex model bailey et al 2022 now utilizes modflow rt3d and salinity modules the simulation tab of apexmod makes the integrated model application more flexible with the capability of using additional modules and applying dynamic scenarios with these configuration features in the simulation tab users can effortlessly access various management practices and analyze spatio temporal variations of water fluxes and solute transportation between groundwater and surface water module activations and management configurations can be conducted with simple user selections once modflow rt3d or salinity module is activated apexmod examines necessary inputs for each module and notifies users whether there is missing information this step also synchronizes simulation periods for each module based on the apex model set in the input file apexcont dat users can control to print optional model outputs or change the frequency of writing daily grid based outputs fig 6 2 5 visualizing simulation outputs the post processing tab offers instantaneous visualization of outputs in time series charts or spatial color coded maps which can be used for publication or inspection of simulation results apexmod has three processing modules in the post processing tab plotting mapping and exporting results the plot function powered by the matplotlib python library hunter 2007 is designed to compare simulated values with observed data in time series and display them in a line or scatter plot for example users can evaluate stream discharge sediment yield nitrate phosphorus and eight salt ions for each reach and groundwater levels and solute concentrations for user specified grids fig 7 a and b along with the water quantity and quality hydrographs or scatter plots apexmod offers various performance statistics e g nash sutcliffe efficiency nse percent bias pbias root mean square error rmse r squared r2 to assist users in evaluating the model performance the plot function is also used for visualizing interactions of water fluxes and solute transports between groundwater and surface water fig 7d apexmod reads the coordinates of an object that intersects apex river networks with modflow river cells and draws bars as their water fluxes or solute loadings along with their geo locations apexmod also provides an option to assess temporal variations of the water balance for the model area reading simulated values of precipitation surface runoff lateral flow groundwater flow to streams deep percolation to groundwater soil water seepage from streams to the aquifer and groundwater volume fig 7e the mapping function is designed to visualize spatial variations of groundwater recharge groundwater head and solute percolations from surface and concentrations in the aquifer displaying them on the qgis canvas via shapefiles apexmod extracts the spatially simulated data according to the user specified duration and stores the data in a shapefile fig 7c an apex modflow simulation also produces various spatial temporal datasets apexmod stores those spatial temporal datasets in a vector object e g grid or subarea scale by containing timestamps and linking spatial indexes to geological locations from the object features this step allows users to perform a dynamic visualization the simulation outputs are displayed for each time step over a period through animation on the qgis canvas the time step and speed of the animation can be adjusted and users can manually control the slider and view the outputs at a certain time or export slices of data between specific time intervals that can be combined into an animated gif or rendered into a video file except for simulated gridded base datasets users can export all results to text formatted files according to user specified location and time 3 case study the animas river watershed the use of apexmod is demonstrated in the animas river watershed arw located at the border of colorado and new mexico the arw is situated in the semi arid western region of the united states bailey et al 2021 describe more details of the study area the apexmod interface was used to load the existing apex and modflow models create an rt3d model and salinity inputs based on modflow properties and datasets perform the spatial linking process run the apex modflow rt3d salt simulation and visualize simulation results fig 8 3 1 input data and model setup 3 1 1 apex and modflow we used a calibrated apex modflow model dataset bailey et al 2021 to demonstrate the utility of apexmod the features of the animas river watershed and model construction are presented in table 2 the apex model was constructed with a 30 m national elevation dataset http ned usgs gov 30 m u s geological survey national land cover data 2016 https www mrlc gov data 30 m usda nrcs soil survey geographic ssurgo https datagateway nrcs usda gov in the arcapex environment a plugin tool to create apex input files within arcmap based on the unique combination of those datasets the animas watershed was delineated into 75 subareas the daily rainfall and maximum minimum air temperatures are obtained from noaa national climate data center https www ncdc noaa gov cdo web search the modflow model has a uniform cell size of 1000 m2 and includes 3778 active cells out of 9956 grid cells the 250 m resolution aquifer thickness map shangguan et al 2017 http globalchange bnu edu cn research dtb jsp was applied to the bottom elevation of each modflow grid cell initial hydraulic conductivity and specific yield were used based on the permeability maps developed by huscroft et al 2018 https dataverse scholarsportal info dataset xhtml persistentid doi 10 5683 sp2 ttjniu and then optimized during a calibration process by matching the apex river network with active modflow grids 706 river cells were generated riverbed conductance values for each river cell were initially calculated using river width and depth from the apex river network a constant riverbed thickness of 0 1 m and the riverbed hydraulic conductivity of 0 1 m day then adjusted during model calibration 3 1 2 rt3d and salinity inputs the salt model requires four primary inputs the initial fraction of salt minerals in the soil layers of each apex subarea the initial salt ion concentrations in the soil layers of each apex subarea the initial fraction of salt minerals in each rt3d grid cell and the initial groundwater salt ion concentrations in each rt3d grid cell initial soil fractions of caco3 and caso4 in the soil profile were determined using the predicted soil raster maps from nauman et al 2019 which were prepared from ssurgo data mean values were used for each subarea initial groundwater salt ion concentrations in each rt3d grid cell were obtained by spatially interpolating point groundwater measurements from the usgs national water information system nwis for monitoring wells available in the study area usgs 2020 interpolation was performed using inverse distance weighting idw groundwater salt ion concentrations were then aggregated by subarea to establish the initial soil salt ion concentrations assuming that the groundwater and soil concentrations are similar in magnitude lastly the salt mineral fractions in the aquifer were estimated using a geologic map of the watershed as all initial conditions have a high level of uncertainty they can be included in parameter estimation methods 3 1 3 calibration and validation in this study we increased the calibration period of the model from 1 1 1992 12 31 1996 5 years bailey et al 2021 to 1 1 1992 12 31 2011 20 years to implement the optimization of salinity simulation the calibration and validation periods of monthly streamflow at 3 locations are 1 1 1992 12 31 2011 20 years and 1 1 2012 12 31 2019 8 years with the first five years of simulation being a warm up period 1987 1991 for groundwater estimation we selected two monitoring wells usgs 371422107473301 usgs 371127107484801 that have relatively continuous measurements and are available from 1995 to 2003 the parameter estimation approach has also been changed from zonation to combined zonal and pilot point using the pest parameter estimation software doherty 2018 we enhanced the automated parameter estimation framework liu et al 2019 park 2018 by replacing a batch mode windows bat files with executables with a jupyter notebook workflow and developing the apexmf python library https doi org 10 5281 zenodo 7051205 to support the automatic apex modflow optimization process in the jupyter notebook kluyver et al 2016 we used 16 apex parameters e g soil evaporation coefficient surface runoff curve number soil evaporation plant cover factor potential et coefficients plant canopy rainfall interception limit etc and 84 modflow parameters 37 pilot points were set for hydraulic conductivity and specific yield and five zones for riverbed conductance and bottom elevation resulting in 100 parameters 3 2 results 3 2 1 spatial linking after loading existing datasets apex and modflow creating rt3d and salinity inputs and examining all inputs the spatial linking process was executed the time required by apexmod to perform the linking processes intersecting the apex river network with modflow river cells and subbasins with modflow grids was 4 and 40 s respectively on an intel core i 7 1160g7 2 1 ghz cpu laptop computer with 16 gb of ram the spatial linking process generated 5098 intersected objects between apex subareas and modflow grids and 648 between the apex river network and modflow river cells the rt3d and salinity were activated through the configuration option of the simulation tab in apexmod then the model ran 3 2 2 model calibration and validation the automated parameter estimation process was completed after eight iterations with 1380 simulations fig 9a shows the hydrographs for comparing simulated results with observations along the animas river at the stream gages of below silverton co cedar hill nm and farmington nm the calibration performance for all stream discharge shows reasonable degrees of nse 0 43 0 66 and pbias 18 12 0 72 and captures the seasonal flow patterns well the model performance during the validation period shows that the general temporal patterns match their measurements for all gages however the magnitude of peak flow from spring snowmelt runoff may july gets underestimated and the intermediate flow from the monsoon rain august october is overestimated more during the validation period we assumed that the uncertainty in the runoff from the snowmelt and the monsoon occurred because the model could not capture the realistic pattern of spring snowmelt runoff with the current model calibration settings worqlul et al 2021 fig 9b shows the 1 to 1 scatter plot left and time series right of simulated and observed daily groundwater heads at two observation wells the 1 to 1 plot of simulated vs measured groundwater head for two sites shows a reasonable agreement with an r2 of 0 9 the time series plots right demonstrate that the model captures the long term trends of groundwater fluctuations as measured by the root mean square errors rmse of 0 71 and 0 94 m respectively for each location simulated groundwater heads are generally within the estimated error range of the measured groundwater levels of 1m grey error bars there are discrepancies in the timing of fluctuation patterns this uncertainty can be caused by large grid size 1000 m 1000 m and discontinued groundwater level measurements 3 2 3 visualization of results from the post processing tab apexmod ran the hydrologically calibrated model with rt3d and salinity inputs and visualized results using the post processing tab figs 10a 1 shows a hydrograph and scatterplot comparing simulated to observed stream discharges with statistics users can draw these plots for channel outputs on sediment and nutrient yields and groundwater levels and solute concentrations at user specified grids figs 10a 2 shows the temporal variations of the water balance for the study area users can assess seasonal water availability or balance for each water content or the total amount of water apexmod also visualizes interactions of water fluxes and solute transports between surface and subsurface figs 10a 3 selected data for the interactions can be exported to a shapefile and users can color code objects on the qgis canvas maps of groundwater head groundwater recharge nitrate and sulfate salt concentrations in the aquifer were generated and displayed on the qgis canvas using the mapping function in apexmod users can also visualize all the other solute percolations and concentrations by selecting each variable apexmod can create and display animations of the apex modflow outputs regarding gw sw interactions and all mapping visualizations over a period these animations can be exported to a video or animated image file gif the results of annual average sulfate salt ion concentration in the aquifer from 2000 to 2019 were created as a gif file and provided as supplementary material except for simulated gridded base datasets all processed data to generate the plots from apexmod are stored in the user s computer memory temporarily and apexmod provides the option of exporting the processed data to a text formatted file allowing users to draw their plots easily for publication purposes 4 limitations and future work currently not all of the modflow packages and processes are available in apexmod as per the modflow suite for instance apexmod only provides the newton solver nwt niswonger et al 2011 as a solver option and upstream weighting upw for a flow package regarding boundary conditions apexmod allows users to create the recharge rch evapotranspiration evt and river riv packages moreover apexmod can only create a single layer modflow model due to the decision to prioritize ease and speed in the linking process between apex and modflow models even though this limits its full capabilities nonetheless advanced users can directly modify modflow model files or import inputs including multi layer model inputs created from other modflow software our plan for the future development of apexmod involves incorporating the automated parameter estimation framework sensitivity analysis and uncertainty quantification for the amrs model as a separate tab within the apexmod tool 5 conclusions the novelty of apexmod lies in its ability to provide a user friendly interface that allows for easy application and evaluation of the enhanced apex model this tool offers a unique feature of creating rt3d and salinity input files which is not commonly available in other similar tools with its user friendly interface users can quickly prepare input data configure simulation and scenario settings and analyze results in a visual manner while this paper only presented the core features of apexmod the plugin is capable of generating input files for dynamic scenarios such as drain and irrigation as well as facilitating the comparison of results between a baseline and scenario model the tool has been successfully applied in a case study watershed where it was able to create the required inputs perform the spatial linking and visualize the model results with ease with its flexibility and versatility apexmod has the potential to enhance modeling studies in the field of hydrology and water resource management further development and refinement of the tool are ongoing and we look forward to future releases and applications of apexmod in a range of different contexts software availability name of software apexmod 15 5 mb developer and contact information seonggyu park brc tamus edu year first available 2022 software required qgis programming language python availability and cost source files and installer are freely available for download at https github com spark brc apexmod releases license freely available under gnu general public license gpl version 2 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project was supported by the bureau of land management award l17ac00125 
25395,this article presents apexmod an open source qgis plugin that helps add a modflow grid to the apex watershed model the apex model integrates modflow a physically based groundwater model and rt3d salt a groundwater nutrient and salt transport model for basin scale application as a coupled surface subsurface hydrologic modeling system apexmod is developed as a tool that enables the dynamic linkage between apex modflow and rt3d salt apexmod is written in python and integrates qgis geoprocessors and spatial packages the apexmod interface is constructed with three modules i a pre processor that prepares inputs for modflow and rt3d salt and performs spatial linkage between apex subareas and modflow grid cells ii a model configurator that controls simulation options and scenario setting and iii a post processor for visualizing and analyzing model outputs a case study was conducted in the animas river watershed to demonstrate the core capabilities of apexmod keywords apex modflow rt3d salt qgis plugin groundwater surface water interactions data availability i have shared the link to my data in the manuscript 1 introduction the apex agricultural policy environmental extender model is a distributed continuous daily time step farm or watershed scale hydrologic water quality model williams et al 2008 the apex model has been applied in numerous modeling studies in the united states because of its robust crop growth routines for approximately one hundred crop types gassman et al 2009 x wang et al 2012 similar to other watershed models arnold et al 1998 subsurface hydrologic processes of apex such as groundwater flow groundwater storage and groundwater surface water interaction are calculated by simple water balance and lagging equations the oversight in groundwater hydrologic simulation in apex makes it difficult to evaluate watershed hydrology in watershed where groundwater drainage is a dominant component of surface water flow worqlul et al 2021 to provide physically based groundwater flow modeling into the apex code bailey et al 2021 coupled apex and modflow niswonger et al 2011 for dynamic integration subsequently the apex modflow modeling code was extended to nutrient and salt ion transport simulation bailey et al 2022 the subroutines simulating the transport of salt ion transport in groundwater are based on rt3d clement 1997 tavakoli kivi et al 2019 the apex modflow rt3d code was applied and tested in the price river watershed utah usa bailey et al 2022 these individual modeling components of the enhanced apex model have their graphical user interfaces guis for applications as individual software programs for example winapex gassman et al 2009 and arcapex tuppad et al 2009 are windows programs that help build apex input files winapex is a windows interface program designed to assist users in developing input files and executing apex for a field or small watersheds arcapex is an arcgis based user interface that integrates gis capabilities and algorithms in the arcgis development framework redlands c e s r i 2011 arcgis desktop release 10 with apex databases input and output management for watersheds and river basins guis for modflow and rt3d include groundwater modeling system gms owen et al 1996 goundwater vista rumbaugh and rumbaugh 2004 and public domain tools e g freewat foglia et al 2018 modelmuse winston 2009 no gui has been developed for creating salt inputs for rt3d salt integrating physical models help explore the hydrological dynamics in a complex watershed and evaluate constituents transport processes however the integrated apex modflow rt3d salt amrs model requires a substantial amount of effort in input preparation and spatial linking of apex subareas and modflow grid cells which may hamper its broader adaptation and utilization meyer and riechert 2019 nielsen et al 2021 besides standard input files required by individual models spatial linkages between apex subareas and modflow grid cells and between apex streams and modflow river cells must be established and corresponding details must be stored in additional input files the spatial information stored in the linkage files is used to pass hydrologic volumetric fluxes m3 day and solute mass between apex subareas apex stream channels and modflow rt3d grid cells preparing these linkage files is generally a manual task that is a slow tedious and error prone process the process of spatial linking objects between two models has been implemented in similar integrated hydrological models gsflow markstrom et al 2008 mike she butts and graham 2008 parflow kollet and maxwell 2006 hydrogeosphere brunner and simmons 2012 swat modflow bailey et al 2016 kim et al 2008 and swat modflow bailey et al 2020 swatmod prep bailey et al 2017 and qswatmod park et al 2019 were developed to provide users with a standardized and easy to use workflow for model application and evaluation qswatmod is a plugin tool for the open source qgis environment http www qgis org a similar gui for the amrs model is needed for input file preparation running simulations and viewing the output of the models that comprise the amrs this paper presents apexmod a new gui developed as a qgis plugin for the amrs qgis was chosen as an ideal platform for both the development and implementation of the apexmod workflow qgis processing functionalities and the python engine have been shown to outperform other software such as arcgis and surfer surfer from golden software llc www goldensoftware com in several qgis plugin tools dile et al 2016 foglia et al 2018 nielsen et al 2017 2021 that have demonstrated adequate functionalities for most general applications in water resource management apexmod automates the spatial linkage between apex and modflow and prepares inputs for rt3d to simulate the reactive transport of nitrate phosphate and eight major salt ions so4 2 cl co3 2 hco3 ca2 na mg2 and k required inputs for the rt3d subroutines include the initial concentration of nitrate and phosphate and parameters for sorption and kinetic reactions e g denitrification required inputs for the salinity module include initial salt ion concentrations in groundwater and soil water and initial fractions of salt minerals in the aquifer and the soil profile here we demonstrate the features and capabilities of apexmod in a case study conducted in the animas river watershed which spans portions of colorado and new mexico usa 2 material and methods 2 1 overview of apexmod structure apexmod is developed as a qgis plugin program written in python 3 van rossum and drake 2009 the purpose of the software is to provide the amrs model users with utility in preparing input data configuring simulation settings and viewing analyzing and post processing results from the amrs model through a user interface in the qgis platform the current version of apexmod described in the manuscript is the version 1 4 3 which is the latest version available in the github repository https github com spark brc apexmod releases the amrs model https github com spark brc amrs releases tag v0 2 0 requires inputs from a specific version of apex v1501 and modflow 2005 or nwt while it is possible to generate rt3d inputs using other software such as gms it should be noted that these inputs may not be compatible with the amrs model due to the absence of salinity information for further details regarding the required model inputs please refer to section 2 3 although the current version of apexmod doesn t provide a calibration option we have developed a python library called apexmf https github com spark brc apexmf specifically for calibration with a jupyter notebook workflow kluyver et al 2016 which is an integral part of our apexmod development this workflow and library are actively being tested with the parameters of all four models to provide reasonable ranges as initial values in the apexmod tool and and will be made available in the next version of apexmod to make apexmod functional qgis3 https download qgis org is required on the system our recommendation is to use the long term release version of qgis3 firenze as we have only tested apexmod with the ltr versions 3 10 4 3 28 5 of qgis3 after qgis installation apexmod can be installed from an installer file apexmod exe available for downloading at a github repo https github com spark brc apexmod releases download v1 4 3 apexmod exe and currently compatible with the windows operation system the apexmod plugin can be launched from the qgis toolbar followed by one time manual activation via the manage and install plugins window under the plugins menu in qgis fig 1 a unlike the qswatmod qgis plugin park et al 2019 users are not required to install dependent python libraries flopy bakker et al 2016 and pyshp which are not available in qgis by default manual installation of third party packages on qgis can be problematic for novice model users because it requires installation through prompt syntaxes and administrator privileges for the windows system instead the current release version ver 1 0 or higher of apexmod offers these external dependency packages directly embedded within apexmod thus additional python system updates or installations via python s standard package manager pip are unnecessary apexmod comprises three main modules pre processing configuration and post processing hydrology and solute transport each main module is implemented in different tabs fig 2 a user can create a new project or open an existing one when the user creates a new project apexmod generates a qgis qgz project file and a neighboring project directory with the same name the main structure of an apexmod project consists of apexmf exes holding model executables apex modflow model inputs and outputs db database file containing project and file paths and project status exported files post processing files gis rasters and vectors and scenarios management inputs and outputs folders under the main project directory fig 3 2 2 pre processing module the pre processing tab loads existing model input files apex and modflow create models modflow rt3d and salinity and perform a spatial linking process in step 1 users need to provide apex model inputs subbasin and river vector files fig 4 a in the modflow option 2 users can load an existing modflow model if the modflow model has a grid shapefile that defines a boundary the grid shapefile can be loaded for the linking process if not users must provide coordinates of the north west corner of the modflow model boundary to create a modflow grid the modflow grid can also be created based on the extent of the apex subarea boundary shapefile with reading cell information width length top elevation from the modflow discretization file dis instructions on creating a new modflow model are provided in section 2 3 spatial linkage is required between modflow river cells and the apex river network to simulate groundwater river flow interactions step 3 in fig 4a the locations and quantities of their spatial linkages can affect the model performance streamflow sediment yield water table and water quality thus apexmod provides three options 1 use only the modflow river package 2 use only the apex river network and 3 use both modflow river packages and the apex river network for the first option apexmod extracts river cell values river stage riverbed conductance river bottom elevation from the modflow river package file riv the second option is to create a new modflow river package by intersecting the modflow grid with the apex river network shapefile and then calculating the river cell parameters for each river cell based on information provided by the apex river network stream stage stream length and width and user inputs thickness and hydraulic conductivity of riverbed material for example riverbed conductance for each river cell is calculated by equation 1 r i v c d l w r i v k r i v t where l is the length of the river w is the width riv k is riverbed hydraulic conductivity and riv t is riverbed thickness for the third option apexmod reads information from the existing modflow river cells and river cells created by the apex river network and combines them to generate new combined river cells users can choose or remove grid cells from the combined river cells based on users decisions then create a new modflow river package users can start the linking process without completing steps 4 5 and 6 the description of step 4 is provided in the supplementary material the description of steps 5 and 6 is provided in section 2 3 the check modflow file option step 7 creates the modflow mfn file by modifying unit numbers in the modflow input nam and output oc control files and checking the dis file simulation period and model state all the modifications made are stored in the modflow editlog txt file the unit numbers are file index identifiers used by the modflow and apex codes whose values are often below 100 for stand alone modflow and apex simulations each input output file in the apex modflow simulation must have a unique unit number the check modflow file option modifies the given unit numbers to be greater than 5000 in the modflow mfn file to not coincide with numbers already assigned to multiple apex input files in addition step 7 converts the state of a modflow simulation to transient tr in the modflow discretization file dis if the already existing modflow is set to steady state ss because the apex model is continuous which is the transient state the linking process step 7 intersects the subarea vector polygons shapefile with modflow grid cells creates modflow grid and subarea indexes and calculates grid and subarea sizes and the size of subarea occupied by each modflow grid apexmod uses these intersected sub grid and riv grid objects to create the three required apex modflow linkage table files link sa grid link grid sa and link river grid which are stored in the project folder modflow 2 3 utilizing modflow rt3d and salinity models if a modflow model were not provided only the 3rd option could be checked from step 2 of the pre processing tab fig 4a and apexmod allows users to create a simple modflow model with single layer powered by the flopy3 bakker et al 2016 package because a modflow model created by apexmod can be used as a stand alone model users need to specify a path to a folder where to store modflow inputs and its model name step a 1 in fig 5 a raster file of the digital elevation model dem and a polygon shapefile for the watershed boundary are required to determine modflow s top layer elevation and modflow model boundary respectively step a 2 apexmod offers the option to create the modflow model boundary based on the extent of the apex model boundaries if no boundary information is available step a 4 is the same as the second option of identifying river cells using the apex river network stream stage length and width and user inputs thickness and hydraulic conductivity of riverbed material step a 5 sets aquifer properties including aquifer thickness hydraulic conductivity specific storage specific yield initial hydraulic head and evapotranspiration a single value for a homogenous input or a raster file for heterogeneous inputs can be loaded for these aquifer properties users can adjust the horizontal anisotropy step a 5 in fig 5 the ratio of hydraulic conductivity along columns to hydraulic conductivity along rows where kx is the hydraulic conductivity along rows the aquifer thickness and initial hydraulic head are specified with two options single value subtracted from groundwater surface elevation at each cell to provide a value of bottom layer elevation and initial head for each cell or uniform value which is the constant elevation for the whole watershed step a 6 reads all aquifer properties and writes input text files in the format required by modflow bailey et al 2022 integrated the rt3d modeling code with the apex modflow code of bailey et al 2021 and the rt3d code was modified to simulate the transport of the eight salt ions as well as nitrate and phosphorous called rt3d salt additional inputs are required to simulate the reactive transport of these solutes according to advection dispersion and chemical reaction terms apexmod incorporates the rt3d salt component of the enhanced apex model the rt3d ui can be popped up separately fig 5b by clicking the rt3d activation button step 5 in fig 4a once the rt3d window is opened apexmod reads grid information from the modflow model creates an rt3d grid object and stores active and inactive cell properties porosity and initial concentrations for nitrate and phosphorus can be provided as a single value or raster value linear sorption coefficients for nitrate and phosphorous the first order rate constant of denitrification 1 time and the monod half saturation term denitrification are provided regarding sorption and reaction parameters for the aquifer dispersivity apexmod sets up the bulk density of the aquifer porous medium m l3 longitudinal dispersivity l horizontal and vertical transverse to longitudinal dispersivity ratios and effective molecular diffusion coefficient like setting groundwater level observation wells step 4 in fig 4a locations of solute observation wells can be established by using a point shapefile of the observation well or through the user selecting cells from the modflow grids on the qgis canvas due to the potentially sizable daily grid outputs 1 gb apexmod provides an option to set the frequency of writing daily grid outputs step 4 in fig 5b reads all rt3d settings and writes input text files such as basic transport package btn advection adv dispersion dsp reaction rct source sink mixing ssm and super file rt3d filenames after all necessary rt3d inputs are generated the salinity model option is activated users can prepare salinity inputs to simulate eight salt ions in the surface and subsurface and their interactions using the salinity ui fig 5c steps 1 and 2 in fig 5c guide users to provide eight initial salt ion concentrations and five salt mineral fractions for soil and aquifer with a single value raster of values or an already created vector layer after the salinity module is activated two vector layers salt input sub for soil salt ions and salt input grid for aquifer salt ions will be generated the salt input sub and salt input grid vector layers will store eight initial salt ion concentrations and five salt mineral fractions for each subarea and each modflow grid respectively users can also load an already created vector layer that includes field names of salt ions or fractions then apexmod reads and finds a matching field name extracts values from the field and stores them in the salt input sub or salt input grid layer for the initial salt mineral fraction of soil apexmod can use gridded soil survey geographic gssurgo geodatabase type from usda nrcs https nrcs app box com v soils folder 148414960239 once users provide a gssurgo database apexmod will start loading all necessary polygon and tables mupolygon component chorizon from the geodatabase and linking caco3 and caso4 mineral fraction values to the salt input sub vector layer using qgis default functions such as join intersection and identification this option in the current apexmod version is available only for the united states including alaska and hawaii the last dataset step 4 in fig 5c for the salinity module is irrigation salt concentration mg l referring to eight salt ion concentrations in irrigation water for each subarea dataset can be provided in the same way as step 1 or 2 in fig 5c if no dataset is given apexmod will consider zero salt concentrations in irrigation water and store zero values in the irri salt sub vector layer step 4 in fig 5c reads all salinity datasets writes input text files salt input and salt and modifies the btn file by adding gridded base salt data the required inputs for modflow rt3d and salinity models are summarized in table 1 2 4 configuring modules and scenarios the enhanced apex model bailey et al 2022 now utilizes modflow rt3d and salinity modules the simulation tab of apexmod makes the integrated model application more flexible with the capability of using additional modules and applying dynamic scenarios with these configuration features in the simulation tab users can effortlessly access various management practices and analyze spatio temporal variations of water fluxes and solute transportation between groundwater and surface water module activations and management configurations can be conducted with simple user selections once modflow rt3d or salinity module is activated apexmod examines necessary inputs for each module and notifies users whether there is missing information this step also synchronizes simulation periods for each module based on the apex model set in the input file apexcont dat users can control to print optional model outputs or change the frequency of writing daily grid based outputs fig 6 2 5 visualizing simulation outputs the post processing tab offers instantaneous visualization of outputs in time series charts or spatial color coded maps which can be used for publication or inspection of simulation results apexmod has three processing modules in the post processing tab plotting mapping and exporting results the plot function powered by the matplotlib python library hunter 2007 is designed to compare simulated values with observed data in time series and display them in a line or scatter plot for example users can evaluate stream discharge sediment yield nitrate phosphorus and eight salt ions for each reach and groundwater levels and solute concentrations for user specified grids fig 7 a and b along with the water quantity and quality hydrographs or scatter plots apexmod offers various performance statistics e g nash sutcliffe efficiency nse percent bias pbias root mean square error rmse r squared r2 to assist users in evaluating the model performance the plot function is also used for visualizing interactions of water fluxes and solute transports between groundwater and surface water fig 7d apexmod reads the coordinates of an object that intersects apex river networks with modflow river cells and draws bars as their water fluxes or solute loadings along with their geo locations apexmod also provides an option to assess temporal variations of the water balance for the model area reading simulated values of precipitation surface runoff lateral flow groundwater flow to streams deep percolation to groundwater soil water seepage from streams to the aquifer and groundwater volume fig 7e the mapping function is designed to visualize spatial variations of groundwater recharge groundwater head and solute percolations from surface and concentrations in the aquifer displaying them on the qgis canvas via shapefiles apexmod extracts the spatially simulated data according to the user specified duration and stores the data in a shapefile fig 7c an apex modflow simulation also produces various spatial temporal datasets apexmod stores those spatial temporal datasets in a vector object e g grid or subarea scale by containing timestamps and linking spatial indexes to geological locations from the object features this step allows users to perform a dynamic visualization the simulation outputs are displayed for each time step over a period through animation on the qgis canvas the time step and speed of the animation can be adjusted and users can manually control the slider and view the outputs at a certain time or export slices of data between specific time intervals that can be combined into an animated gif or rendered into a video file except for simulated gridded base datasets users can export all results to text formatted files according to user specified location and time 3 case study the animas river watershed the use of apexmod is demonstrated in the animas river watershed arw located at the border of colorado and new mexico the arw is situated in the semi arid western region of the united states bailey et al 2021 describe more details of the study area the apexmod interface was used to load the existing apex and modflow models create an rt3d model and salinity inputs based on modflow properties and datasets perform the spatial linking process run the apex modflow rt3d salt simulation and visualize simulation results fig 8 3 1 input data and model setup 3 1 1 apex and modflow we used a calibrated apex modflow model dataset bailey et al 2021 to demonstrate the utility of apexmod the features of the animas river watershed and model construction are presented in table 2 the apex model was constructed with a 30 m national elevation dataset http ned usgs gov 30 m u s geological survey national land cover data 2016 https www mrlc gov data 30 m usda nrcs soil survey geographic ssurgo https datagateway nrcs usda gov in the arcapex environment a plugin tool to create apex input files within arcmap based on the unique combination of those datasets the animas watershed was delineated into 75 subareas the daily rainfall and maximum minimum air temperatures are obtained from noaa national climate data center https www ncdc noaa gov cdo web search the modflow model has a uniform cell size of 1000 m2 and includes 3778 active cells out of 9956 grid cells the 250 m resolution aquifer thickness map shangguan et al 2017 http globalchange bnu edu cn research dtb jsp was applied to the bottom elevation of each modflow grid cell initial hydraulic conductivity and specific yield were used based on the permeability maps developed by huscroft et al 2018 https dataverse scholarsportal info dataset xhtml persistentid doi 10 5683 sp2 ttjniu and then optimized during a calibration process by matching the apex river network with active modflow grids 706 river cells were generated riverbed conductance values for each river cell were initially calculated using river width and depth from the apex river network a constant riverbed thickness of 0 1 m and the riverbed hydraulic conductivity of 0 1 m day then adjusted during model calibration 3 1 2 rt3d and salinity inputs the salt model requires four primary inputs the initial fraction of salt minerals in the soil layers of each apex subarea the initial salt ion concentrations in the soil layers of each apex subarea the initial fraction of salt minerals in each rt3d grid cell and the initial groundwater salt ion concentrations in each rt3d grid cell initial soil fractions of caco3 and caso4 in the soil profile were determined using the predicted soil raster maps from nauman et al 2019 which were prepared from ssurgo data mean values were used for each subarea initial groundwater salt ion concentrations in each rt3d grid cell were obtained by spatially interpolating point groundwater measurements from the usgs national water information system nwis for monitoring wells available in the study area usgs 2020 interpolation was performed using inverse distance weighting idw groundwater salt ion concentrations were then aggregated by subarea to establish the initial soil salt ion concentrations assuming that the groundwater and soil concentrations are similar in magnitude lastly the salt mineral fractions in the aquifer were estimated using a geologic map of the watershed as all initial conditions have a high level of uncertainty they can be included in parameter estimation methods 3 1 3 calibration and validation in this study we increased the calibration period of the model from 1 1 1992 12 31 1996 5 years bailey et al 2021 to 1 1 1992 12 31 2011 20 years to implement the optimization of salinity simulation the calibration and validation periods of monthly streamflow at 3 locations are 1 1 1992 12 31 2011 20 years and 1 1 2012 12 31 2019 8 years with the first five years of simulation being a warm up period 1987 1991 for groundwater estimation we selected two monitoring wells usgs 371422107473301 usgs 371127107484801 that have relatively continuous measurements and are available from 1995 to 2003 the parameter estimation approach has also been changed from zonation to combined zonal and pilot point using the pest parameter estimation software doherty 2018 we enhanced the automated parameter estimation framework liu et al 2019 park 2018 by replacing a batch mode windows bat files with executables with a jupyter notebook workflow and developing the apexmf python library https doi org 10 5281 zenodo 7051205 to support the automatic apex modflow optimization process in the jupyter notebook kluyver et al 2016 we used 16 apex parameters e g soil evaporation coefficient surface runoff curve number soil evaporation plant cover factor potential et coefficients plant canopy rainfall interception limit etc and 84 modflow parameters 37 pilot points were set for hydraulic conductivity and specific yield and five zones for riverbed conductance and bottom elevation resulting in 100 parameters 3 2 results 3 2 1 spatial linking after loading existing datasets apex and modflow creating rt3d and salinity inputs and examining all inputs the spatial linking process was executed the time required by apexmod to perform the linking processes intersecting the apex river network with modflow river cells and subbasins with modflow grids was 4 and 40 s respectively on an intel core i 7 1160g7 2 1 ghz cpu laptop computer with 16 gb of ram the spatial linking process generated 5098 intersected objects between apex subareas and modflow grids and 648 between the apex river network and modflow river cells the rt3d and salinity were activated through the configuration option of the simulation tab in apexmod then the model ran 3 2 2 model calibration and validation the automated parameter estimation process was completed after eight iterations with 1380 simulations fig 9a shows the hydrographs for comparing simulated results with observations along the animas river at the stream gages of below silverton co cedar hill nm and farmington nm the calibration performance for all stream discharge shows reasonable degrees of nse 0 43 0 66 and pbias 18 12 0 72 and captures the seasonal flow patterns well the model performance during the validation period shows that the general temporal patterns match their measurements for all gages however the magnitude of peak flow from spring snowmelt runoff may july gets underestimated and the intermediate flow from the monsoon rain august october is overestimated more during the validation period we assumed that the uncertainty in the runoff from the snowmelt and the monsoon occurred because the model could not capture the realistic pattern of spring snowmelt runoff with the current model calibration settings worqlul et al 2021 fig 9b shows the 1 to 1 scatter plot left and time series right of simulated and observed daily groundwater heads at two observation wells the 1 to 1 plot of simulated vs measured groundwater head for two sites shows a reasonable agreement with an r2 of 0 9 the time series plots right demonstrate that the model captures the long term trends of groundwater fluctuations as measured by the root mean square errors rmse of 0 71 and 0 94 m respectively for each location simulated groundwater heads are generally within the estimated error range of the measured groundwater levels of 1m grey error bars there are discrepancies in the timing of fluctuation patterns this uncertainty can be caused by large grid size 1000 m 1000 m and discontinued groundwater level measurements 3 2 3 visualization of results from the post processing tab apexmod ran the hydrologically calibrated model with rt3d and salinity inputs and visualized results using the post processing tab figs 10a 1 shows a hydrograph and scatterplot comparing simulated to observed stream discharges with statistics users can draw these plots for channel outputs on sediment and nutrient yields and groundwater levels and solute concentrations at user specified grids figs 10a 2 shows the temporal variations of the water balance for the study area users can assess seasonal water availability or balance for each water content or the total amount of water apexmod also visualizes interactions of water fluxes and solute transports between surface and subsurface figs 10a 3 selected data for the interactions can be exported to a shapefile and users can color code objects on the qgis canvas maps of groundwater head groundwater recharge nitrate and sulfate salt concentrations in the aquifer were generated and displayed on the qgis canvas using the mapping function in apexmod users can also visualize all the other solute percolations and concentrations by selecting each variable apexmod can create and display animations of the apex modflow outputs regarding gw sw interactions and all mapping visualizations over a period these animations can be exported to a video or animated image file gif the results of annual average sulfate salt ion concentration in the aquifer from 2000 to 2019 were created as a gif file and provided as supplementary material except for simulated gridded base datasets all processed data to generate the plots from apexmod are stored in the user s computer memory temporarily and apexmod provides the option of exporting the processed data to a text formatted file allowing users to draw their plots easily for publication purposes 4 limitations and future work currently not all of the modflow packages and processes are available in apexmod as per the modflow suite for instance apexmod only provides the newton solver nwt niswonger et al 2011 as a solver option and upstream weighting upw for a flow package regarding boundary conditions apexmod allows users to create the recharge rch evapotranspiration evt and river riv packages moreover apexmod can only create a single layer modflow model due to the decision to prioritize ease and speed in the linking process between apex and modflow models even though this limits its full capabilities nonetheless advanced users can directly modify modflow model files or import inputs including multi layer model inputs created from other modflow software our plan for the future development of apexmod involves incorporating the automated parameter estimation framework sensitivity analysis and uncertainty quantification for the amrs model as a separate tab within the apexmod tool 5 conclusions the novelty of apexmod lies in its ability to provide a user friendly interface that allows for easy application and evaluation of the enhanced apex model this tool offers a unique feature of creating rt3d and salinity input files which is not commonly available in other similar tools with its user friendly interface users can quickly prepare input data configure simulation and scenario settings and analyze results in a visual manner while this paper only presented the core features of apexmod the plugin is capable of generating input files for dynamic scenarios such as drain and irrigation as well as facilitating the comparison of results between a baseline and scenario model the tool has been successfully applied in a case study watershed where it was able to create the required inputs perform the spatial linking and visualize the model results with ease with its flexibility and versatility apexmod has the potential to enhance modeling studies in the field of hydrology and water resource management further development and refinement of the tool are ongoing and we look forward to future releases and applications of apexmod in a range of different contexts software availability name of software apexmod 15 5 mb developer and contact information seonggyu park brc tamus edu year first available 2022 software required qgis programming language python availability and cost source files and installer are freely available for download at https github com spark brc apexmod releases license freely available under gnu general public license gpl version 2 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project was supported by the bureau of land management award l17ac00125 
25396,the use of numerical models to anticipate the effects of floods and storms in coastal regions is essential to mitigate the damages of these natural disasters however local studies require high spatial and temporal resolution numerical models limiting their use due to the involved high computational costs this constraint becomes even more critical when these models are used for real time monitoring and warning systems therefore the objective of this paper was to reduce the computational time of coastal morphodynamic models simulations by implementing a deep learning emulator the emulator performance was evaluated using different scenarios run with the xbeach software which considered different grid resolutions and the effects of a storm event in the morphodynamic patterns around a breakwater and a groin the morphodynamic simulation time was reduced by 23 and it was identified that the major restriction to reducing the computational cost was the hydrodynamic numerical model simulation graphical abstract image 1 keywords numerical model emulator xbeach deep learning tensorflow morphodynamics hydrodynamics data availability data will be made available on request deep learning model for xbeach morphodynamic emulation original data hydroshare 1 introduction forecasting the impacts of extreme events such as droughts floods and storms is fundamental to identify threatened areas and to allow early actions to increase resilience and reduce the eventual related damages anticipating extreme events becomes even more relevant in low lying coastal areas which present high risks due to the combination of meteo oceanic events climate trends and human activities in open ocean coastal areas cities and other settlements are more vulnerable to coastal flooding whilst pluvial floods are one of the major threats in deltas and estuaries prtner et al 2022 moreover these areas shelter approximately 11 of the global population which lives within 10 m above mean sea level haasnoot et al 2021 prtner et al 2022 vousdoukas et al 2020 to anticipate and mitigate extreme events impacts the implementation of forecasting systems must be addressed these systems are usually implemented based on numerical models such as xbeach and delft3d d3d deltares 2018 roelvink et al 2009 which had demonstrated to be accurate in predicting hydro morphodynamic hmd variables under extreme events scenarios ferreira et al 2019 iglesias et al 2022 simmons and splinter 2022 vousdoukas et al 2012 however these numerical models demand considerable simulation computational time mainly when applied to high spatial and or temporal resolution domains and when different modules are used in a forecast simulation e g hydrodynamics waves winds sediments and morphology however the use of high resolution models is crucial to correctly simulate hydrodynamic and morphodynamic patterns at coastal stretches moreover real time monitoring and warning systems require computationally efficient numerical models to optimize the prediction of future trends and to allow a faster response by competent authorities to improve the computing efficiency of numerical models like xbeach rautenbach et al 2022 compared the performance of the software in hydrodynamics and wave dynamics simulations using a cpu and a gpu finding that even a desktop grade gpu can compete with the computational efficiency of high performance computing cpu facilities however there are still few studies about the computational efficiency of hmd numerical models and ways to improve it an alternative to reduce the computational costs of hmd numerical models is the use of emulators which are based on the application of statistical techniques that surrogate the numerical model these are artificial intelligence derived methods namely machine learning ml and deep learning dl which implementation for surrogating morphodynamic numerical models had been approached by other authors poelhekke et al 2016 developed a xbeach emulator to predict hazards at faro beach anderson et al 2021 used several numerical models and a gaussian process regression gpr emulator to predict wave runup and overwash depths and gharagozlou et al 2022 developed a gpr based emulator for post storm subaerial beach and dune profile shapes however such works developed emulators for specific punctual locations within the domain hence this work intends to fill this gap solving the emulators constraint in morphodynamic variables forecasting at full 2dh domains in a previous work of the authors a dl model with convolutional layers was implemented using simulation results obtained with d3d software weber de melo et al 2022 the developed technique used images of a d3d numerical model hydrodynamic variables as input and was able to forecast the long term morphodynamic evolution of an intertidal shoal estuary demonstrating that it was possible to reduce the total simulation time by surrogating the morphodynamic module of the numerical model for an emulator however short term morphodynamics in beaches are more complex than long term estuarine morphodynamics estuarine morphodynamics main drivers are the water currents which varies according to tides and freshwater flows on the contrary beach morphodynamics results from the complex interactions between winds non linear waves processes tides water currents and return flows and the processes involved in coastal morphodynamics are dominated by complex onshore and offshore transport thus the numerical model selected in weber de melo et al 2022 is not well suited to model sand beaches bar behaviour because the processes of dune erosion and offshore sediment transport by the return flow are not included in the numerical code on the other hand xbeach software has all necessary processes to model bar behaviour trouw et al 2012 these authors also note that the default values for wave related bed load and suspended load factors in d3d are too high and this way the physics of the model are likely flawed in this context this paper has the objective to improve the dl model architecture by weber de melo et al 2022 and test if this improved architecture could reproduce the morphodynamic results of an experiment run with the xbeach model considering an extreme event scenario the objective is to demonstrate if the new emulator that successfully surrogated the d3d morphodynamic module for an estuarine environment could also be applied to xbeach simulations of non linear wave dominated coastal stretches two simulation sets were used to evaluate the xbeach emulator the first set used a simplified linear bathymetry to assess if a change in the up sampling operation in the neural network could reduce the error in the validation dataset and to determine the total simulation time reduction the second simulation set tested if the emulator could reproduce the morphological evolution of a more complex coastal stretch involving a sandy beach with different coastal defence structures including a submerged breakwater and a groin 2 methodology the methodology applied in this study was adapted and improved from the previous work of weber de melo et al 2022 which designed and implemented an hmd numerical model emulator to forecast the long term morphological evolution of an estuarine area however as an already mentioned in the introduction the processes that affect the morphodynamics in estuaries are different from those that occur in coastal areas in the first case the main morphological evolution driver is the water currents generated by river flows and tides in the second case the waves tides and winds are the main drivers that affect the coastal morphological evolution due to the complexity of the morphological processes in coastal regions and the need to introduce other non linear variables besides the water current this work developed a new methodology to reduce the computational costs of a xbeach model to emulate the short term morphological evolution of coastal stretches 2 1 xbeach model xbeach is an open source numerical model that simulates hmd processes in sandy coastal areas by solving 2dh equations for wave propagation flow sediment transport and bottom changes roelvink et al 2009 this software solves the shallow water equation for low frequency waves and average flows considering the depth averaged generalized lagrangian mean formulation roelvink et al 2009 the sediment transport equation was solved accordingly to the van thiel van rijn formulation van rijn 2007 van thiel de vries 2009 xbeach based methodologies had already been demonstrated to be capable of achieving reliable results having been applied to study the uncertainties in run up predictions on natural beaches e g rutten et al 2021 to predict extreme offshore directed sediment transport suzuki and cox 2021 and to model the morphological response of a sandy barrier island during hurricane conditions smallegan et al 2016 however xbeach simulations require intensive computational resources mainly when the domain has a high spatial resolution restraining its application in real time early warning forecasting and decision support systems ferreira et al 2019 gharagozlou et al 2022 poelhekke et al 2016 in this study two different simulation sets were defined to evaluate and optimize the performance of the emulator the spatial domain of the first set was a simplified beach with a linear bathymetry fig 1 a the focus of this first set was to assess the up sampling operation in the architecture of the emulator to reduce the error in the forecasts in the second set the capacity of the emulator in reproducing the morphodynamics was evaluated in a more complex geometrical domain involving two different coastal defence structures a submerged breakwater fig 1b and a groin fig 1c 2 1 1 coastal domains characteristics three different coastal domains were defined to analyse the performance of the emulator fig 1 the first domain consists of a beach with a 2 km cross shore width and 4 km alongshore length discretized with a 100 200 grid with 20 m cells size resolution the bathymetry varied from 0 m to 20 m and the topography from 0 m to 10 m the topo bathymetry is homogeneous in the north south direction in the west east direction it varied linearly fig 1a the second and third domains used a 335 375 grid with 5 m cells size resolution resulting in a coastal domain of 1 68 km cross shore width and 1 88 km alongshore length the bathymetry varied between 0 m and 12 m at the offshore boundary and the topography from 0 m to 2 m at the inland boundary fig 1b and c 2 1 2 numerical models initial and boundary conditions the xbeach simulations run in the surfbeat mode in which the short wave motion is solved using the wave action equation roelvink et al 2009 for the first set of simulations that used the linear bathymetry the initial water level was assumed equal to 1 6 m at the offshore boundary and 1 3 m at the landward boundary while in the second set of simulations which included domains with coastal defence structures the initial water level was assumed equal to the mean sea level 0 m msl the joint north sea wave project jonswap spectral wave model was selected to force the models to represent the occurrence of an extreme event the significant wave height used in simulation set 1 was 6 m intending to simulate a highly energetic storm to force intense morphological changes during the simulation the significant wave height hs was 3 m for the simulation set 2 which is a representative value based on the historical wave measurements of a typical storm in portugal c a oliveira et al 2020 for this geographical location the average peak periods tp vary between 5 s and 12 s and the mean hs usually varies between 1 5 m and 2 m but values between 3 and 6 m are recurrent vieira et al 2020 viitak et al 2021 the hydrodynamic simulation period was 6 h for both simulation sets although the sediment transport and morphology modules started with half hour lag in the simulation set 1 and with one day lag in simulation set 2 to avoid numerical inconsistencies additionally a morphological acceleration factor morfac of 20 was adopted in simulation set 2 to represent the morphological evolution along 5 days furthermore the adopted critical avalanching slope above water was 0 15 and underwater was 1 the summary of the boundary conditions used for each simulation set are presented in table 1 where the mainang parameter is the main wave angle in the nautical convention s is the directional spreading coefficient gammajsp is the peak enhancement factor and fnyq is the highest frequency of the jonswap spectrum 2 1 3 simulation scenarios as already briefly explained in the introduction each simulation set was elaborated to accomplish different objectives the first set had the goal to study up sampling operation to improve the performance of the emulator the second set assessed the capacity of the dl model to emulate more complex coastal environments under the influence of coastal defence structures and assessed different combinations of inputs and outputs during the training of the network scenarios 1 to 4 in simulation set 2 assessed the capacity of the emulator in reproducing different morphodynamic variables the current timestep bed level change ctblc or the cumulative erosion and sedimentation ces scenarios 5 to 8 were defined to evaluate the possibility of implementing a single emulator for different domains the models were trained using the data of both domains but its performance was assessed considering the results of each domain individually the characteristics of the scenarios are synthesized in table 2 the simulation set 1 presents the up sampling functions that were tested convt refers to the transpose convolution layer followed by the activation function that was used in that layer at the simulation set 2 the acronyms refer to the training and testing conditions the first acronym refers to the domains of the data used to train the network bw is for breakwater g is for groin and bwg for both domains the second acronym refers to the xbeach variable used as output by the emulator ces or ctblc the third acronym presented in scenarios 5 to 8 refers to the domain wherein results were used to assess the performance of the emulator this last acronym was only necessary to differentiate the validating conditions of the models that were trained with the data of both domains hence it was not necessary in scenarios 1 to 4 additionally the sensitivity of the emulator to the numerical model grid resolution was assessed for this test the set 1 was selected using the same model domain and the same topo bathymetry but with two different grid resolutions 20 m 20 m grid resolution original domain and 40 m 40 m grid resolution a quarter of the original resolution these models which will receive the acronyms or for the highest resolution domain and lr for the lowest resolution domain were used to assess if the emulator could learn using the coarser resolution hydrodynamic input to predict the morphodynamic output of the finer domain without losing accuracy 2 2 implementation of the morphodynamic emulator the dl model was implemented in the tensor flow framework abadi et al 2016 in python programming language version 3 9 7 using a spyder environment version 5 2 2 details about the implementation of the emulator are presented in the following sub sections 2 2 1 input and output images processing the dl network used the generalized lagrangian mean velocity glmv magnitude and the bottom shear stress bss as inputs the output in simulation set 1 was ctblc in simulation set 2 the ces result was additionally used to generate the image datasets with these variables a python script was programmed to ensure that all the images presented the same resolution and colour scale characteristics the axes of the images were turned off and the margins were set to 0 to ensure that all the pixels of the images coincided with the numerical model domain the netcdf4 library whitaker et al 2020 was used to read the xbeach results and the matplotlib library was used to plot the results hunter 2007 the images were created with the imshow function and were exported with 300 dpi resulting in a resolution of 452 900 pixels for simulation set 1 and 540 600 pixels for simulation set 2 the images were plotted in grayscale and the limit values of the colour scales were determined using a programmed routine that iterated through all the variables datasets to find the maximum and minimum values for each variable at all run time steps brighter tones indicate positive values sedimentation while darker tones indicate negative values erosion or zero in the case of glmv and bss examples of these images are presented in fig 2 the exporting loop was divided into 3 parts firstly it read the results of the ith time step and created an image with the pre defined configurations secondly it updated the image name according to the respective time step to avoid overwriting the results thirdly the image was exported to a folder in which all the images were stored according to each variable it resulted in 54 and 150 images per variable for the simulation set 1 and 2 respectively the scenario that evaluated the emulator with different input and output grid resolutions used 109 images per variable maintaining the same training test ratio this increase in the number of images was necessary to reduce the emulator mean error for the simulation set 1 only the results of the ctblc were used to evaluate the performance of the model for the set 2 the ces results were also used to train the network and to evaluate if the emulator could properly forecast the morphological evolution of the domains the inputs were the same in this second case the unique difference was the output variable used for training the network 2 2 2 network architecture a u net and a recursively deconvolutional branched network are the basis for the emulator architecture in which each branch mapped the image features of one specific resolution ronneberger et al 2015 santhanam et al 2016 this configuration which was also used by weber de melo et al 2022 is presented in fig 3 the hyperparameters namely the activation functions number of filters kernel size strides and dilation rate of the convolutional layers remained the same the dl model was implemented using the functional api of the tensorflow framework due to the complexity of the chosen architecture the network initially mapped the features of the input data and reduced the footprint of the data using convolutional operations with strides set to 2 to half the resolution of the layer inputs after each stride convolution a concatenation layer merged the layers outputs after that up sampling layers restored the original resolution of the input data and 3 other convolutions were performed the activation function was the relu in all convolutional layers except for the last which used a hyperbolic tangent function tanh besides a deconvolution layer also known as transposed convolution dumoulin and visin 2016 replaced the up sampling operation green arrows in fig 3 this change increases the number of trainable parameters in the architecture of the network allowing the improvement of the emulator performance the relu and tanh activation functions were assessed during this step the main difference between these two operations is how they increase the resolution of the images the up sampling layer increases the number and or columns of the input data the values of the new elements will depend on the interpolation method of the layer the deconvolutions on the other hand consists of a convolution with a filter derived from the transposition and inversion of the tensor resulting from the original convolution filter aggarwal 2018 this type of layer presents the same properties parameters and hyperparameters of a convolution layer however the output has more dimensions than the input in the last two layers a relu operation with a maximum value equal to 1 limited the values of the output image between 0 and 1 and a rescaling operation set the output values in a 0 255 scale which is the same scale as the output images 2 2 3 training and validation the networks were trained using the rmsprop optimizer with a learning rate of 0 005 and a limit of 300 epochs early stopping was configured to interrupt the training if the performance of the emulator stabilized after 15 consecutive epochs the loss function was the mean squared error mse and the root mean squared error rmse was used to monitor the training progress 80 of the images were used for training and 20 for validating the model each training epoch lasted approximately 20 s for simulation set 1 and 35 s for set 2 the training was performed in a cpu intel core i7 8750h 2 20 ghz 16 gb ram and a graphics card nvidia geforce gtx 1060 with 6 gb using parallel computing aided by the cuda toolkit v 11 2 harish and narayanan 2007 the model architecture with the best performance obtained with the simulation set 1 was used in the simulation set 2 scenarios for scenarios 1 to 4 the models were trained for each domain and for each output variable ces and ctblc resulting in 4 emulators in scenarios 5 to 8 the models were trained using the data of both domains at once resulting in two emulators these last scenarios verified the possibility of using only one model to predict the morphological evolution of different domains additionally in this second case the models would be capable of identifying the domain according to the input data 2 2 4 performance analysis to compare the results of the numerical models and the results of the emulator the root mean squared error rmse of the model in the validation dataset was computed for each time step 1 r m s e 1 n i 1 n e m n m 2 where n is the number of grid points em is the result of the emulator and nm is the result of the numerical model also the mean error was determined to identify the location of the highest errors in the domain 2 m e 1 m i 1 n e m n m where m is the number of time steps of the validation dataset besides the kernel density estimation kde of the errors in the domains was computed considering all the time steps the kde method estimates the probability density function of a non parametric dataset chen 2017 this last metric is relevant to successfully analyse the error distribution in all the domains to identify areas in which the emulator performance is better 3 results and discussion 3 1 simulation set 1 simulation set 1 had the objective of evaluating the influence of the up sampling operation in the architecture of the emulator the rmse in scenario 1 was 0 0241 mm 0 0247 mm and 0 0183 mm for scenarios 1 2 and 3 respectively all the models could achieve reasonable results however the use of the tanh activation function in the transposed convolution layer scenario 3 resulted in the emulator with the best performance it probably occurred due to the increase in the number of trainable parameters in the network granting a better approximation to the numerical model results additionally it did not meaningfully affect the computational time required by the emulator to process the validation dataset therefore the architecture used in scenario 3 was used to evaluate the computational performance in simulation set 2 scenarios 3 2 computational performance the dl models needed 5 s to generate the morphodynamics results referred to the whole dataset training and validation however it is also necessary to run the hydrodynamic module of the numerical model to generate the input data required by the emulator therefore to correctly estimate the computational time required for the dl model it was necessary to run the numerical model with the sediment transport and morphology modules deactivated this comparison was conducted using the simulation set 1 model due to its simplified bathymetry and consequently hydrodynamics patterns this model run a 6 h simulation with morfac set to 0 and with the sediment transport and morphology modules deactivated with both morphodynamics modules activated 28 min were necessary to run the simulation with only the hydrodynamics module activated the simulation took 23 min additionally 15 s were necessary to generate 108 input images 54 per variable and 5 s to run the dl model with this data representing a reduction of 23 in the total time necessary to forecast the morphodynamic evolution of the domain considering the predictions for the whole simulation period therefore the main limiter to reduce the morphodynamics forecast computational time is the hydrodynamic simulation the emulator that used different input and output resolutions had the best performance gain the hydrodynamic simulation lasted 3 3 min resulting in a time reduction of 87 to generate the morphodynamics results as the major restriction to reduce the computational time was the hydrodynamic simulation reducing the grid resolution to train the emulator could represent a significant performance gain additionally it must be highlighted that it would be necessary to update the numerical model bathymetry before generating the hydrodynamics results of the next forecast window the bathymetry variation directly affects the hydrodynamics variables consequently update it during this type of simulation is of utmost importance to correctly forecast the hmd variables and reduce the uncertainties of the results looking forward to optimizing the generation of the hydrodynamics results three approaches could be considered firstly the xbeach simulation time presents a high dependence on the wave velocities that must satisfy the cfl condition courant et al 1928 roelvink et al 2009 schneider et al 2013 the simplest method to ensure this condition and maintain the grid resolution is to set the smallest time step for the entire simulation which is determined according to the highest wave speed gnedin et al 2018 although this approach could increase the total simulation time especially if the objective of the model is to simulate extreme events which would result in higher wave velocities and demand a reduction in the simulation time step to satisfy the cfl condition alternatively the reduction of the grid resolution would allow higher time steps reducing the total simulation time and satisfying the cfl condition however the focus of this methodology was to reduce the computational costs of high resolution numerical models during short term events hence this hypothesis was not considered in this work the last option would be to optimize the input image generation loop in this study a loop was used to generate the images of each variable in each step of the loop the image referring to the ith time step was created and exported to reduce this time a single loop for all variables was performed which partially reduced the redundancy in the code instead of using one loop for each variable optimizing this process in despite of that the total time required to generate the hydrodynamics images was 15 s hence an optimization in this process would not significantly reduce the total time required additionally an important result achieved if compared with previous works is the reduction in the number of images necessary to train the emulator in weber de melo et al weber de melo et al 2022 1095 images were generated to train and test the emulator whereas herein only 162 images were needed in the simulation set 1 to achieve good results this demonstrate that using ai models do not necessarily require a huge database for training and testing which agrees with data centric ai concepts that affirms that the quality of the data is far more important than its quantity sambasivan et al 2021 the xbeach database required 20 61 mb of disk space while the d3d needed 137 1 mb representing a reduction of 85 in the disk space usage the emulator with different input output resolutions needed 40 8 mb representing 70 less disk space 3 3 influence of the grid resolution the difference between the emulators trained with inputs of different resolution and the numerical model output for the test dataset are presented in fig 4 the mean error of both emulators was negligible but the mean error of the lr emulator was one order higher than the or emulator the maximum error occurred in the or emulator results was 21 cm and in the lr emulator was 26 cm these values are considerably higher than the mean error however they are restricted to specific areas of the domain particularly near the domain boundaries which normally fall out of the region of interest when using numerical modelling approaches fig 5 shows the rmse of the or and lr emulators in the test dataset it can be observed that the performance of both models is remarkably similar in all points and the maximum mean error was below 5 cm in the or model and below 8 cm in the lr besides both error curves follow a similar pattern having the same behaviour at timesteps 14 and 17 for example 3 4 simulation set 2 fig 6 presents a comparison between the output in the last time step of the xbeach model with the results obtained with the emulator using the ces images scenarios 1 and 3 the emulator could reproduce the main patterns of the morphological evolution of the domains with a remarkable resemblance the erosion and accretion bars can be observed in the southern area of the beach in both images as well as a larger accretion area around the coordinate 300 300 this pattern results from the 315 wave angle of the wave spectrum applied at the boundaries of the xbeach model which created this accretion spot upstream of the structures however the emulator underestimated the accretion area near this coordinate which is evident due to the thinner blue bar in the breakwater domain and the reduced quantity of purple pixels in the groin domain this result demonstrates that the use of ces as output instead of ctblc did not significantly affect the performance of the emulator the emulator accurately simulated the main morphodynamic patterns of both study areas independently of the selected domain comparing the results of both selected coastal structures the output of the breakwater emulator is more similar to the numerical model result than the output of the groin emulator this could be related with the uniformity of the breakwater influence along the beach when compared to the groin the groin breaks the linearity of the erosion accretion bars creating an area with a more complex morphodynamic pattern hence more training images would be necessary to improve the precision of the groin emulator additionally the emulator underestimated the erosion that occurred near the groin in the centre of the domain it can be observed that the numerical model simulated a larger erosion area in the north south direction whilst this pattern was not accurately forecasted by the emulator mainly at the north of the breakwater in the case of the breakwater emulator it underestimated the accretion that occurred in the east face the performance of the emulator could be improved by increasing the size and variability of the training dataset variables the use of more images would give more information about the output pattern expected in that area of the domain increasing its performance moreover the error in the areas wherein the morphodynamic variation is more uniform was lower than the error in areas with higher variability namely near the breakwater and the groin however it must be stressed that the emulator could still identify that the morphological change in the groin was zero probably because this pattern was more common in all training time steps to assess the error variance of the emulators in all the validation timesteps the rmse was computed between the dl model output and the numerical model result for each pixel of the image the mean value of the rmse was then determined by dividing the rmse for the validation time steps fig 7 presents these results separated by the output variable used to train the dl models the results demonstrated that the models trained in only one domain had a better performance than the models trained in both domains it probably befell because the input variables values were affected by the groin and the breakwater only in the area around these interventions in the other parts of the domain the input values were more resemblant due to the boundary conditions being the same hence the network could learn the ces patterns for both domains having similar errors as demonstrated in fig 7 a in fig 7 b the error of scenarios 6 and 8 was quite different when compared to the models trained with ces data it probably happened because the ctblc patterns are more similar between the domains given that these results show only the areas in which the bed level was altered in the last time step this fact could explain why the performance of scenario 8 was closer to scenarios 2 and 4 a different pattern when analyzing the ces results despite that the performance of the models was better when the training conditions were from only one domain it probably occurred owing to a possible ambiguity in the input and output data since most of the pixel values in the input images were not affected by the domain besides the bwg emulator had to decide which domain to prioritize during the training resulting in a better adjustment to the groin domain instead of the breakwater this could explain why the error in scenario 8 was lower than the error in scenario 6 however the performance in scenario 8 overcame scenario 4 between time steps 15 and 23 indicating that the dl model parameters were more suitable for those inputs this would allow the use of ensemble techniques biolchi et al 2022 iglesias et al 2022 to create a unique and more robust model that could merge individual models trained in specific conditions allowing the model to select a determined input output path according to the available data fig 8 presents the mean error obtained in the simulation set 2 scenarios this result allows to identify the average location of the major errors in the numerical model domains the whiter colors represent areas where the error was near 0 red colors represent the emulator underestimation and blue colors represent overestimation scenarios 2 3 and 5 had an average overestimation of the numerical model results whilst the other scenarios presented an average underestimation fig 8 a shows that in all scenarios the dl models underestimated the values of an erosion area that occurred near the coordinates 280 350 this probably occurred owing to the proximity to the boundary of the numerical model resulting in a higher uncertainty for that area moreover the model also underestimated the accretion area at coordinates 300 300 despite that all the models had a reasonable agreement between the numerical model results and the data model outputs it can be observed in fig 8 b that the highest errors in scenarios 2 and 6 occurred near the beach where the sediment transport is more intense although is in this same area where the lowest errors in scenarios 4 and 8 are found this demonstrates that the dl model trained in both domains scenarios 6 and 8 adjusted its parameters to better represent only one domain which was the one with lower errors this same behaviour can be observed in scenarios 5 and 7 in which the error in scenario 7 was much lower than the error in scenario 5 lastly the error distribution was plotted fig 9 which allows for assessing the variability of the errors considering all the time steps of the validation dataset the peak values in the distributions are approximately the mean errors presented in fig 8 the models trained in single domain data had a lower variability in the errors regardless of the output variable additionally the emulators presented acceptable results for all the domain areas which is indicated by the error low variability therefore dl emulators could successfully surrogate morphological modules of numerical modelling tools as the xbeach without significant changes in the result the errors achieved by the dl models represented 1 1 of the maximum ces values and 0 6 of the maximum ctblc values reinforcing the similarity between the results of the emulator and the xbeach models 3 5 future of hmd modelling the capacity of the emulator in reproducing the main morphodynamic patterns of coastal beaches and the reduction of computational resources requirements demonstrate the potential of data driven models staring to modify the logic behind environmental modelling numerical models can achieve reliable results by solving deterministic equations although these equations simplify some aspects of the reality for instance the bottom friction coefficient is usually simplified to a uniform value for all the computational domain although it is known that this coefficient varies according to the characteristics of the sediments bottom structure depth and vegetation these details can be inserted into numerical models but the efforts in their implementation would sharply rose data driven models conversely intrinsically consider all the domain features as it will be reflected in the training data it is also important to stress that the data characteristics determines the validation interval in which data models can be applied for example for the implementation of a model to predict extreme events effects it is not expected that a data model trained in average conditions will have satisfactory performance as well as a numerical model calibrated in average conditions will not either in this context a change of paradigm may be interesting for future modelling methodologies instead of creating more complex models that can consider all sources of uncertainty efforts could be directed to improve the quality variability and frequency of data thereby reducing the uncertainties of forecasts to the characteristics of the training data and to the errors in the measurements regarding the computational resources required for modelling the use of parallel computing and high performance computers would reduce the simulation computational time however these types of machines and their necessary infrastructure have high associated costs limiting the application of real time forecasting platforms to those who can afford them therefore the development of methodologies that does not depend on super computers is a promising path to guarantee a wider application of early warning systems 4 conclusions this study applied an emulator to a hydrodynamic coastal numerical model using a deep learning approach implemented in python and the tensor flow framework images of the hydrodynamic variables bed shear stress and glm velocity simulated with a xbeach model were used as input of the dl models that forecasted the morphological evolution for three different domains in simulation set 1 a dl architecture based on the previous work of weber de melo et al 2022 had a satisfactory performance for the xbeach models demonstrating the suitability of the purposed methodology to reduce the computational time of morphodynamic numerical models there is no reason to think that the developed emulator cannot be used with other hydro morphodynamic software appropriate to simulate coastal stretches additionally the performance of the emulator was improved by the replacement of the up sampling operation in the neural network architecture by transposed convolutions this procedure increased the number of trainable parameters in the network allowing the reduction of the error between the numerical model results and the dl model outputs without significantly affecting the computational time required by the emulator to forecast the validation dataset furthermore the tanh activation function resulted in a lower error when compared to the relu activation function regarding simulation set 2 it was demonstrated that the emulator could reproduce the numerical model morphodynamic results in coastal domains under the influence of a submerged breakwater or a groin the errors obtained were two orders lower than the maximum and minimum values of the output variables furthermore the emulator accurately reproduced both ces and ctblc results indicating that the dl model can be easily adapted to forecast any variable of interest if there is enough data to train the network to determine the size of the training dataset it is necessary to assess the results in the validation dataset the use of few data can result in models that can barely forecast the main patterns of the output variable whilst the use of excessive data can drastically increase the time required for training the network it is recommended that the training data includes enough variability to correctly represent the application range of the emulator moreover the image database used in this paper requires 85 less disk space than in weber de melo et al 2022 demonstrating that the methodology can achieve reliable results with fewer data it can be also concluded that it is possible to train one emulator with data from different domains however some precautions are necessary namely identifying the possibility of a domain prioritization by the dl model the errors increased in three of the scenarios in which the emulators were trained with both datasets scenarios 5 6 and 7 when compared to the emulators trained with a single one scenario 8 model results overcame scenario 4 in several time steps although its error was slightly worse in the others time steps regarding the computational costs of the emulator it was achieved a time reduction of 23 in simulating the morphodynamic evolution using numerical models with the same input and output grid resolution whilst a time reduction of 87 was obtained using the emulator with low grid resolution inputs the main time consuming task in this methodology was the generation of hydrodynamics results by the numerical model which limited the improvement of the proposed methodology hence the performance gain with the emulator application would be higher for long term hydrodynamic simulations due to the lower wave velocities magnitudes which allows simulations with higher time steps although the error in the lr emulator had slightly increased its results were still similar to the numerical model additionally the use of images as input and output allows the potential application of this methodology to any other numerical model as mentioned before reducing the total computational time required for simulating the short term morphological evolution of a coastal area can be important to facilitate the application of xbeach models to real time early warning and forecasting systems this would allow to optimize the response of the authorities during extreme events and to increase the resilience of coastal communities finally this study demonstrated that the use of deep learning techniques can reduce the computational costs of numerical models allowing their implementation in real time forecasting and warning systems the emulators accurately reproduced the ces and ctblc results obtained with the xbeach software which is one of the most advanced coastal morphodynamics simulation model available achieving an error two orders lower than the morphodynamics variables range values software and data availability name of the software xbeach version 1 23 developers deltares xbeach open source community first year available 2009 cost free software availability https download deltares nl en download xbeach open source program size 330 97 mb the deep learning based emulator used for surrogating the xbeach morphodynamic module was implemented in python language version 3 9 based on tensorflow library the authors used a windows 11 home os environment cpu intel r core tm i7 8750h 2 20 ghz ram 16 gb gpu nvidia geforce gtx 1060 the architecture of the model is available at http www hydroshare org resource b4ae97df748842a1800816b32a3d640 b author contributions w m j p and i i designed the research w m performed the research w m j p and i i analyzed the data w m and j p implemented the models j p and i i proofread the article declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the doctoral grant sfrh bd 151383 2021 financed by the portuguese foundation for science and technology fct and with funds from the ministry of science technology and higher education under the mit portugal program i iglesias also acknowledge the fct financing through the ceec program 2022 07420 ceecind 
25396,the use of numerical models to anticipate the effects of floods and storms in coastal regions is essential to mitigate the damages of these natural disasters however local studies require high spatial and temporal resolution numerical models limiting their use due to the involved high computational costs this constraint becomes even more critical when these models are used for real time monitoring and warning systems therefore the objective of this paper was to reduce the computational time of coastal morphodynamic models simulations by implementing a deep learning emulator the emulator performance was evaluated using different scenarios run with the xbeach software which considered different grid resolutions and the effects of a storm event in the morphodynamic patterns around a breakwater and a groin the morphodynamic simulation time was reduced by 23 and it was identified that the major restriction to reducing the computational cost was the hydrodynamic numerical model simulation graphical abstract image 1 keywords numerical model emulator xbeach deep learning tensorflow morphodynamics hydrodynamics data availability data will be made available on request deep learning model for xbeach morphodynamic emulation original data hydroshare 1 introduction forecasting the impacts of extreme events such as droughts floods and storms is fundamental to identify threatened areas and to allow early actions to increase resilience and reduce the eventual related damages anticipating extreme events becomes even more relevant in low lying coastal areas which present high risks due to the combination of meteo oceanic events climate trends and human activities in open ocean coastal areas cities and other settlements are more vulnerable to coastal flooding whilst pluvial floods are one of the major threats in deltas and estuaries prtner et al 2022 moreover these areas shelter approximately 11 of the global population which lives within 10 m above mean sea level haasnoot et al 2021 prtner et al 2022 vousdoukas et al 2020 to anticipate and mitigate extreme events impacts the implementation of forecasting systems must be addressed these systems are usually implemented based on numerical models such as xbeach and delft3d d3d deltares 2018 roelvink et al 2009 which had demonstrated to be accurate in predicting hydro morphodynamic hmd variables under extreme events scenarios ferreira et al 2019 iglesias et al 2022 simmons and splinter 2022 vousdoukas et al 2012 however these numerical models demand considerable simulation computational time mainly when applied to high spatial and or temporal resolution domains and when different modules are used in a forecast simulation e g hydrodynamics waves winds sediments and morphology however the use of high resolution models is crucial to correctly simulate hydrodynamic and morphodynamic patterns at coastal stretches moreover real time monitoring and warning systems require computationally efficient numerical models to optimize the prediction of future trends and to allow a faster response by competent authorities to improve the computing efficiency of numerical models like xbeach rautenbach et al 2022 compared the performance of the software in hydrodynamics and wave dynamics simulations using a cpu and a gpu finding that even a desktop grade gpu can compete with the computational efficiency of high performance computing cpu facilities however there are still few studies about the computational efficiency of hmd numerical models and ways to improve it an alternative to reduce the computational costs of hmd numerical models is the use of emulators which are based on the application of statistical techniques that surrogate the numerical model these are artificial intelligence derived methods namely machine learning ml and deep learning dl which implementation for surrogating morphodynamic numerical models had been approached by other authors poelhekke et al 2016 developed a xbeach emulator to predict hazards at faro beach anderson et al 2021 used several numerical models and a gaussian process regression gpr emulator to predict wave runup and overwash depths and gharagozlou et al 2022 developed a gpr based emulator for post storm subaerial beach and dune profile shapes however such works developed emulators for specific punctual locations within the domain hence this work intends to fill this gap solving the emulators constraint in morphodynamic variables forecasting at full 2dh domains in a previous work of the authors a dl model with convolutional layers was implemented using simulation results obtained with d3d software weber de melo et al 2022 the developed technique used images of a d3d numerical model hydrodynamic variables as input and was able to forecast the long term morphodynamic evolution of an intertidal shoal estuary demonstrating that it was possible to reduce the total simulation time by surrogating the morphodynamic module of the numerical model for an emulator however short term morphodynamics in beaches are more complex than long term estuarine morphodynamics estuarine morphodynamics main drivers are the water currents which varies according to tides and freshwater flows on the contrary beach morphodynamics results from the complex interactions between winds non linear waves processes tides water currents and return flows and the processes involved in coastal morphodynamics are dominated by complex onshore and offshore transport thus the numerical model selected in weber de melo et al 2022 is not well suited to model sand beaches bar behaviour because the processes of dune erosion and offshore sediment transport by the return flow are not included in the numerical code on the other hand xbeach software has all necessary processes to model bar behaviour trouw et al 2012 these authors also note that the default values for wave related bed load and suspended load factors in d3d are too high and this way the physics of the model are likely flawed in this context this paper has the objective to improve the dl model architecture by weber de melo et al 2022 and test if this improved architecture could reproduce the morphodynamic results of an experiment run with the xbeach model considering an extreme event scenario the objective is to demonstrate if the new emulator that successfully surrogated the d3d morphodynamic module for an estuarine environment could also be applied to xbeach simulations of non linear wave dominated coastal stretches two simulation sets were used to evaluate the xbeach emulator the first set used a simplified linear bathymetry to assess if a change in the up sampling operation in the neural network could reduce the error in the validation dataset and to determine the total simulation time reduction the second simulation set tested if the emulator could reproduce the morphological evolution of a more complex coastal stretch involving a sandy beach with different coastal defence structures including a submerged breakwater and a groin 2 methodology the methodology applied in this study was adapted and improved from the previous work of weber de melo et al 2022 which designed and implemented an hmd numerical model emulator to forecast the long term morphological evolution of an estuarine area however as an already mentioned in the introduction the processes that affect the morphodynamics in estuaries are different from those that occur in coastal areas in the first case the main morphological evolution driver is the water currents generated by river flows and tides in the second case the waves tides and winds are the main drivers that affect the coastal morphological evolution due to the complexity of the morphological processes in coastal regions and the need to introduce other non linear variables besides the water current this work developed a new methodology to reduce the computational costs of a xbeach model to emulate the short term morphological evolution of coastal stretches 2 1 xbeach model xbeach is an open source numerical model that simulates hmd processes in sandy coastal areas by solving 2dh equations for wave propagation flow sediment transport and bottom changes roelvink et al 2009 this software solves the shallow water equation for low frequency waves and average flows considering the depth averaged generalized lagrangian mean formulation roelvink et al 2009 the sediment transport equation was solved accordingly to the van thiel van rijn formulation van rijn 2007 van thiel de vries 2009 xbeach based methodologies had already been demonstrated to be capable of achieving reliable results having been applied to study the uncertainties in run up predictions on natural beaches e g rutten et al 2021 to predict extreme offshore directed sediment transport suzuki and cox 2021 and to model the morphological response of a sandy barrier island during hurricane conditions smallegan et al 2016 however xbeach simulations require intensive computational resources mainly when the domain has a high spatial resolution restraining its application in real time early warning forecasting and decision support systems ferreira et al 2019 gharagozlou et al 2022 poelhekke et al 2016 in this study two different simulation sets were defined to evaluate and optimize the performance of the emulator the spatial domain of the first set was a simplified beach with a linear bathymetry fig 1 a the focus of this first set was to assess the up sampling operation in the architecture of the emulator to reduce the error in the forecasts in the second set the capacity of the emulator in reproducing the morphodynamics was evaluated in a more complex geometrical domain involving two different coastal defence structures a submerged breakwater fig 1b and a groin fig 1c 2 1 1 coastal domains characteristics three different coastal domains were defined to analyse the performance of the emulator fig 1 the first domain consists of a beach with a 2 km cross shore width and 4 km alongshore length discretized with a 100 200 grid with 20 m cells size resolution the bathymetry varied from 0 m to 20 m and the topography from 0 m to 10 m the topo bathymetry is homogeneous in the north south direction in the west east direction it varied linearly fig 1a the second and third domains used a 335 375 grid with 5 m cells size resolution resulting in a coastal domain of 1 68 km cross shore width and 1 88 km alongshore length the bathymetry varied between 0 m and 12 m at the offshore boundary and the topography from 0 m to 2 m at the inland boundary fig 1b and c 2 1 2 numerical models initial and boundary conditions the xbeach simulations run in the surfbeat mode in which the short wave motion is solved using the wave action equation roelvink et al 2009 for the first set of simulations that used the linear bathymetry the initial water level was assumed equal to 1 6 m at the offshore boundary and 1 3 m at the landward boundary while in the second set of simulations which included domains with coastal defence structures the initial water level was assumed equal to the mean sea level 0 m msl the joint north sea wave project jonswap spectral wave model was selected to force the models to represent the occurrence of an extreme event the significant wave height used in simulation set 1 was 6 m intending to simulate a highly energetic storm to force intense morphological changes during the simulation the significant wave height hs was 3 m for the simulation set 2 which is a representative value based on the historical wave measurements of a typical storm in portugal c a oliveira et al 2020 for this geographical location the average peak periods tp vary between 5 s and 12 s and the mean hs usually varies between 1 5 m and 2 m but values between 3 and 6 m are recurrent vieira et al 2020 viitak et al 2021 the hydrodynamic simulation period was 6 h for both simulation sets although the sediment transport and morphology modules started with half hour lag in the simulation set 1 and with one day lag in simulation set 2 to avoid numerical inconsistencies additionally a morphological acceleration factor morfac of 20 was adopted in simulation set 2 to represent the morphological evolution along 5 days furthermore the adopted critical avalanching slope above water was 0 15 and underwater was 1 the summary of the boundary conditions used for each simulation set are presented in table 1 where the mainang parameter is the main wave angle in the nautical convention s is the directional spreading coefficient gammajsp is the peak enhancement factor and fnyq is the highest frequency of the jonswap spectrum 2 1 3 simulation scenarios as already briefly explained in the introduction each simulation set was elaborated to accomplish different objectives the first set had the goal to study up sampling operation to improve the performance of the emulator the second set assessed the capacity of the dl model to emulate more complex coastal environments under the influence of coastal defence structures and assessed different combinations of inputs and outputs during the training of the network scenarios 1 to 4 in simulation set 2 assessed the capacity of the emulator in reproducing different morphodynamic variables the current timestep bed level change ctblc or the cumulative erosion and sedimentation ces scenarios 5 to 8 were defined to evaluate the possibility of implementing a single emulator for different domains the models were trained using the data of both domains but its performance was assessed considering the results of each domain individually the characteristics of the scenarios are synthesized in table 2 the simulation set 1 presents the up sampling functions that were tested convt refers to the transpose convolution layer followed by the activation function that was used in that layer at the simulation set 2 the acronyms refer to the training and testing conditions the first acronym refers to the domains of the data used to train the network bw is for breakwater g is for groin and bwg for both domains the second acronym refers to the xbeach variable used as output by the emulator ces or ctblc the third acronym presented in scenarios 5 to 8 refers to the domain wherein results were used to assess the performance of the emulator this last acronym was only necessary to differentiate the validating conditions of the models that were trained with the data of both domains hence it was not necessary in scenarios 1 to 4 additionally the sensitivity of the emulator to the numerical model grid resolution was assessed for this test the set 1 was selected using the same model domain and the same topo bathymetry but with two different grid resolutions 20 m 20 m grid resolution original domain and 40 m 40 m grid resolution a quarter of the original resolution these models which will receive the acronyms or for the highest resolution domain and lr for the lowest resolution domain were used to assess if the emulator could learn using the coarser resolution hydrodynamic input to predict the morphodynamic output of the finer domain without losing accuracy 2 2 implementation of the morphodynamic emulator the dl model was implemented in the tensor flow framework abadi et al 2016 in python programming language version 3 9 7 using a spyder environment version 5 2 2 details about the implementation of the emulator are presented in the following sub sections 2 2 1 input and output images processing the dl network used the generalized lagrangian mean velocity glmv magnitude and the bottom shear stress bss as inputs the output in simulation set 1 was ctblc in simulation set 2 the ces result was additionally used to generate the image datasets with these variables a python script was programmed to ensure that all the images presented the same resolution and colour scale characteristics the axes of the images were turned off and the margins were set to 0 to ensure that all the pixels of the images coincided with the numerical model domain the netcdf4 library whitaker et al 2020 was used to read the xbeach results and the matplotlib library was used to plot the results hunter 2007 the images were created with the imshow function and were exported with 300 dpi resulting in a resolution of 452 900 pixels for simulation set 1 and 540 600 pixels for simulation set 2 the images were plotted in grayscale and the limit values of the colour scales were determined using a programmed routine that iterated through all the variables datasets to find the maximum and minimum values for each variable at all run time steps brighter tones indicate positive values sedimentation while darker tones indicate negative values erosion or zero in the case of glmv and bss examples of these images are presented in fig 2 the exporting loop was divided into 3 parts firstly it read the results of the ith time step and created an image with the pre defined configurations secondly it updated the image name according to the respective time step to avoid overwriting the results thirdly the image was exported to a folder in which all the images were stored according to each variable it resulted in 54 and 150 images per variable for the simulation set 1 and 2 respectively the scenario that evaluated the emulator with different input and output grid resolutions used 109 images per variable maintaining the same training test ratio this increase in the number of images was necessary to reduce the emulator mean error for the simulation set 1 only the results of the ctblc were used to evaluate the performance of the model for the set 2 the ces results were also used to train the network and to evaluate if the emulator could properly forecast the morphological evolution of the domains the inputs were the same in this second case the unique difference was the output variable used for training the network 2 2 2 network architecture a u net and a recursively deconvolutional branched network are the basis for the emulator architecture in which each branch mapped the image features of one specific resolution ronneberger et al 2015 santhanam et al 2016 this configuration which was also used by weber de melo et al 2022 is presented in fig 3 the hyperparameters namely the activation functions number of filters kernel size strides and dilation rate of the convolutional layers remained the same the dl model was implemented using the functional api of the tensorflow framework due to the complexity of the chosen architecture the network initially mapped the features of the input data and reduced the footprint of the data using convolutional operations with strides set to 2 to half the resolution of the layer inputs after each stride convolution a concatenation layer merged the layers outputs after that up sampling layers restored the original resolution of the input data and 3 other convolutions were performed the activation function was the relu in all convolutional layers except for the last which used a hyperbolic tangent function tanh besides a deconvolution layer also known as transposed convolution dumoulin and visin 2016 replaced the up sampling operation green arrows in fig 3 this change increases the number of trainable parameters in the architecture of the network allowing the improvement of the emulator performance the relu and tanh activation functions were assessed during this step the main difference between these two operations is how they increase the resolution of the images the up sampling layer increases the number and or columns of the input data the values of the new elements will depend on the interpolation method of the layer the deconvolutions on the other hand consists of a convolution with a filter derived from the transposition and inversion of the tensor resulting from the original convolution filter aggarwal 2018 this type of layer presents the same properties parameters and hyperparameters of a convolution layer however the output has more dimensions than the input in the last two layers a relu operation with a maximum value equal to 1 limited the values of the output image between 0 and 1 and a rescaling operation set the output values in a 0 255 scale which is the same scale as the output images 2 2 3 training and validation the networks were trained using the rmsprop optimizer with a learning rate of 0 005 and a limit of 300 epochs early stopping was configured to interrupt the training if the performance of the emulator stabilized after 15 consecutive epochs the loss function was the mean squared error mse and the root mean squared error rmse was used to monitor the training progress 80 of the images were used for training and 20 for validating the model each training epoch lasted approximately 20 s for simulation set 1 and 35 s for set 2 the training was performed in a cpu intel core i7 8750h 2 20 ghz 16 gb ram and a graphics card nvidia geforce gtx 1060 with 6 gb using parallel computing aided by the cuda toolkit v 11 2 harish and narayanan 2007 the model architecture with the best performance obtained with the simulation set 1 was used in the simulation set 2 scenarios for scenarios 1 to 4 the models were trained for each domain and for each output variable ces and ctblc resulting in 4 emulators in scenarios 5 to 8 the models were trained using the data of both domains at once resulting in two emulators these last scenarios verified the possibility of using only one model to predict the morphological evolution of different domains additionally in this second case the models would be capable of identifying the domain according to the input data 2 2 4 performance analysis to compare the results of the numerical models and the results of the emulator the root mean squared error rmse of the model in the validation dataset was computed for each time step 1 r m s e 1 n i 1 n e m n m 2 where n is the number of grid points em is the result of the emulator and nm is the result of the numerical model also the mean error was determined to identify the location of the highest errors in the domain 2 m e 1 m i 1 n e m n m where m is the number of time steps of the validation dataset besides the kernel density estimation kde of the errors in the domains was computed considering all the time steps the kde method estimates the probability density function of a non parametric dataset chen 2017 this last metric is relevant to successfully analyse the error distribution in all the domains to identify areas in which the emulator performance is better 3 results and discussion 3 1 simulation set 1 simulation set 1 had the objective of evaluating the influence of the up sampling operation in the architecture of the emulator the rmse in scenario 1 was 0 0241 mm 0 0247 mm and 0 0183 mm for scenarios 1 2 and 3 respectively all the models could achieve reasonable results however the use of the tanh activation function in the transposed convolution layer scenario 3 resulted in the emulator with the best performance it probably occurred due to the increase in the number of trainable parameters in the network granting a better approximation to the numerical model results additionally it did not meaningfully affect the computational time required by the emulator to process the validation dataset therefore the architecture used in scenario 3 was used to evaluate the computational performance in simulation set 2 scenarios 3 2 computational performance the dl models needed 5 s to generate the morphodynamics results referred to the whole dataset training and validation however it is also necessary to run the hydrodynamic module of the numerical model to generate the input data required by the emulator therefore to correctly estimate the computational time required for the dl model it was necessary to run the numerical model with the sediment transport and morphology modules deactivated this comparison was conducted using the simulation set 1 model due to its simplified bathymetry and consequently hydrodynamics patterns this model run a 6 h simulation with morfac set to 0 and with the sediment transport and morphology modules deactivated with both morphodynamics modules activated 28 min were necessary to run the simulation with only the hydrodynamics module activated the simulation took 23 min additionally 15 s were necessary to generate 108 input images 54 per variable and 5 s to run the dl model with this data representing a reduction of 23 in the total time necessary to forecast the morphodynamic evolution of the domain considering the predictions for the whole simulation period therefore the main limiter to reduce the morphodynamics forecast computational time is the hydrodynamic simulation the emulator that used different input and output resolutions had the best performance gain the hydrodynamic simulation lasted 3 3 min resulting in a time reduction of 87 to generate the morphodynamics results as the major restriction to reduce the computational time was the hydrodynamic simulation reducing the grid resolution to train the emulator could represent a significant performance gain additionally it must be highlighted that it would be necessary to update the numerical model bathymetry before generating the hydrodynamics results of the next forecast window the bathymetry variation directly affects the hydrodynamics variables consequently update it during this type of simulation is of utmost importance to correctly forecast the hmd variables and reduce the uncertainties of the results looking forward to optimizing the generation of the hydrodynamics results three approaches could be considered firstly the xbeach simulation time presents a high dependence on the wave velocities that must satisfy the cfl condition courant et al 1928 roelvink et al 2009 schneider et al 2013 the simplest method to ensure this condition and maintain the grid resolution is to set the smallest time step for the entire simulation which is determined according to the highest wave speed gnedin et al 2018 although this approach could increase the total simulation time especially if the objective of the model is to simulate extreme events which would result in higher wave velocities and demand a reduction in the simulation time step to satisfy the cfl condition alternatively the reduction of the grid resolution would allow higher time steps reducing the total simulation time and satisfying the cfl condition however the focus of this methodology was to reduce the computational costs of high resolution numerical models during short term events hence this hypothesis was not considered in this work the last option would be to optimize the input image generation loop in this study a loop was used to generate the images of each variable in each step of the loop the image referring to the ith time step was created and exported to reduce this time a single loop for all variables was performed which partially reduced the redundancy in the code instead of using one loop for each variable optimizing this process in despite of that the total time required to generate the hydrodynamics images was 15 s hence an optimization in this process would not significantly reduce the total time required additionally an important result achieved if compared with previous works is the reduction in the number of images necessary to train the emulator in weber de melo et al weber de melo et al 2022 1095 images were generated to train and test the emulator whereas herein only 162 images were needed in the simulation set 1 to achieve good results this demonstrate that using ai models do not necessarily require a huge database for training and testing which agrees with data centric ai concepts that affirms that the quality of the data is far more important than its quantity sambasivan et al 2021 the xbeach database required 20 61 mb of disk space while the d3d needed 137 1 mb representing a reduction of 85 in the disk space usage the emulator with different input output resolutions needed 40 8 mb representing 70 less disk space 3 3 influence of the grid resolution the difference between the emulators trained with inputs of different resolution and the numerical model output for the test dataset are presented in fig 4 the mean error of both emulators was negligible but the mean error of the lr emulator was one order higher than the or emulator the maximum error occurred in the or emulator results was 21 cm and in the lr emulator was 26 cm these values are considerably higher than the mean error however they are restricted to specific areas of the domain particularly near the domain boundaries which normally fall out of the region of interest when using numerical modelling approaches fig 5 shows the rmse of the or and lr emulators in the test dataset it can be observed that the performance of both models is remarkably similar in all points and the maximum mean error was below 5 cm in the or model and below 8 cm in the lr besides both error curves follow a similar pattern having the same behaviour at timesteps 14 and 17 for example 3 4 simulation set 2 fig 6 presents a comparison between the output in the last time step of the xbeach model with the results obtained with the emulator using the ces images scenarios 1 and 3 the emulator could reproduce the main patterns of the morphological evolution of the domains with a remarkable resemblance the erosion and accretion bars can be observed in the southern area of the beach in both images as well as a larger accretion area around the coordinate 300 300 this pattern results from the 315 wave angle of the wave spectrum applied at the boundaries of the xbeach model which created this accretion spot upstream of the structures however the emulator underestimated the accretion area near this coordinate which is evident due to the thinner blue bar in the breakwater domain and the reduced quantity of purple pixels in the groin domain this result demonstrates that the use of ces as output instead of ctblc did not significantly affect the performance of the emulator the emulator accurately simulated the main morphodynamic patterns of both study areas independently of the selected domain comparing the results of both selected coastal structures the output of the breakwater emulator is more similar to the numerical model result than the output of the groin emulator this could be related with the uniformity of the breakwater influence along the beach when compared to the groin the groin breaks the linearity of the erosion accretion bars creating an area with a more complex morphodynamic pattern hence more training images would be necessary to improve the precision of the groin emulator additionally the emulator underestimated the erosion that occurred near the groin in the centre of the domain it can be observed that the numerical model simulated a larger erosion area in the north south direction whilst this pattern was not accurately forecasted by the emulator mainly at the north of the breakwater in the case of the breakwater emulator it underestimated the accretion that occurred in the east face the performance of the emulator could be improved by increasing the size and variability of the training dataset variables the use of more images would give more information about the output pattern expected in that area of the domain increasing its performance moreover the error in the areas wherein the morphodynamic variation is more uniform was lower than the error in areas with higher variability namely near the breakwater and the groin however it must be stressed that the emulator could still identify that the morphological change in the groin was zero probably because this pattern was more common in all training time steps to assess the error variance of the emulators in all the validation timesteps the rmse was computed between the dl model output and the numerical model result for each pixel of the image the mean value of the rmse was then determined by dividing the rmse for the validation time steps fig 7 presents these results separated by the output variable used to train the dl models the results demonstrated that the models trained in only one domain had a better performance than the models trained in both domains it probably befell because the input variables values were affected by the groin and the breakwater only in the area around these interventions in the other parts of the domain the input values were more resemblant due to the boundary conditions being the same hence the network could learn the ces patterns for both domains having similar errors as demonstrated in fig 7 a in fig 7 b the error of scenarios 6 and 8 was quite different when compared to the models trained with ces data it probably happened because the ctblc patterns are more similar between the domains given that these results show only the areas in which the bed level was altered in the last time step this fact could explain why the performance of scenario 8 was closer to scenarios 2 and 4 a different pattern when analyzing the ces results despite that the performance of the models was better when the training conditions were from only one domain it probably occurred owing to a possible ambiguity in the input and output data since most of the pixel values in the input images were not affected by the domain besides the bwg emulator had to decide which domain to prioritize during the training resulting in a better adjustment to the groin domain instead of the breakwater this could explain why the error in scenario 8 was lower than the error in scenario 6 however the performance in scenario 8 overcame scenario 4 between time steps 15 and 23 indicating that the dl model parameters were more suitable for those inputs this would allow the use of ensemble techniques biolchi et al 2022 iglesias et al 2022 to create a unique and more robust model that could merge individual models trained in specific conditions allowing the model to select a determined input output path according to the available data fig 8 presents the mean error obtained in the simulation set 2 scenarios this result allows to identify the average location of the major errors in the numerical model domains the whiter colors represent areas where the error was near 0 red colors represent the emulator underestimation and blue colors represent overestimation scenarios 2 3 and 5 had an average overestimation of the numerical model results whilst the other scenarios presented an average underestimation fig 8 a shows that in all scenarios the dl models underestimated the values of an erosion area that occurred near the coordinates 280 350 this probably occurred owing to the proximity to the boundary of the numerical model resulting in a higher uncertainty for that area moreover the model also underestimated the accretion area at coordinates 300 300 despite that all the models had a reasonable agreement between the numerical model results and the data model outputs it can be observed in fig 8 b that the highest errors in scenarios 2 and 6 occurred near the beach where the sediment transport is more intense although is in this same area where the lowest errors in scenarios 4 and 8 are found this demonstrates that the dl model trained in both domains scenarios 6 and 8 adjusted its parameters to better represent only one domain which was the one with lower errors this same behaviour can be observed in scenarios 5 and 7 in which the error in scenario 7 was much lower than the error in scenario 5 lastly the error distribution was plotted fig 9 which allows for assessing the variability of the errors considering all the time steps of the validation dataset the peak values in the distributions are approximately the mean errors presented in fig 8 the models trained in single domain data had a lower variability in the errors regardless of the output variable additionally the emulators presented acceptable results for all the domain areas which is indicated by the error low variability therefore dl emulators could successfully surrogate morphological modules of numerical modelling tools as the xbeach without significant changes in the result the errors achieved by the dl models represented 1 1 of the maximum ces values and 0 6 of the maximum ctblc values reinforcing the similarity between the results of the emulator and the xbeach models 3 5 future of hmd modelling the capacity of the emulator in reproducing the main morphodynamic patterns of coastal beaches and the reduction of computational resources requirements demonstrate the potential of data driven models staring to modify the logic behind environmental modelling numerical models can achieve reliable results by solving deterministic equations although these equations simplify some aspects of the reality for instance the bottom friction coefficient is usually simplified to a uniform value for all the computational domain although it is known that this coefficient varies according to the characteristics of the sediments bottom structure depth and vegetation these details can be inserted into numerical models but the efforts in their implementation would sharply rose data driven models conversely intrinsically consider all the domain features as it will be reflected in the training data it is also important to stress that the data characteristics determines the validation interval in which data models can be applied for example for the implementation of a model to predict extreme events effects it is not expected that a data model trained in average conditions will have satisfactory performance as well as a numerical model calibrated in average conditions will not either in this context a change of paradigm may be interesting for future modelling methodologies instead of creating more complex models that can consider all sources of uncertainty efforts could be directed to improve the quality variability and frequency of data thereby reducing the uncertainties of forecasts to the characteristics of the training data and to the errors in the measurements regarding the computational resources required for modelling the use of parallel computing and high performance computers would reduce the simulation computational time however these types of machines and their necessary infrastructure have high associated costs limiting the application of real time forecasting platforms to those who can afford them therefore the development of methodologies that does not depend on super computers is a promising path to guarantee a wider application of early warning systems 4 conclusions this study applied an emulator to a hydrodynamic coastal numerical model using a deep learning approach implemented in python and the tensor flow framework images of the hydrodynamic variables bed shear stress and glm velocity simulated with a xbeach model were used as input of the dl models that forecasted the morphological evolution for three different domains in simulation set 1 a dl architecture based on the previous work of weber de melo et al 2022 had a satisfactory performance for the xbeach models demonstrating the suitability of the purposed methodology to reduce the computational time of morphodynamic numerical models there is no reason to think that the developed emulator cannot be used with other hydro morphodynamic software appropriate to simulate coastal stretches additionally the performance of the emulator was improved by the replacement of the up sampling operation in the neural network architecture by transposed convolutions this procedure increased the number of trainable parameters in the network allowing the reduction of the error between the numerical model results and the dl model outputs without significantly affecting the computational time required by the emulator to forecast the validation dataset furthermore the tanh activation function resulted in a lower error when compared to the relu activation function regarding simulation set 2 it was demonstrated that the emulator could reproduce the numerical model morphodynamic results in coastal domains under the influence of a submerged breakwater or a groin the errors obtained were two orders lower than the maximum and minimum values of the output variables furthermore the emulator accurately reproduced both ces and ctblc results indicating that the dl model can be easily adapted to forecast any variable of interest if there is enough data to train the network to determine the size of the training dataset it is necessary to assess the results in the validation dataset the use of few data can result in models that can barely forecast the main patterns of the output variable whilst the use of excessive data can drastically increase the time required for training the network it is recommended that the training data includes enough variability to correctly represent the application range of the emulator moreover the image database used in this paper requires 85 less disk space than in weber de melo et al 2022 demonstrating that the methodology can achieve reliable results with fewer data it can be also concluded that it is possible to train one emulator with data from different domains however some precautions are necessary namely identifying the possibility of a domain prioritization by the dl model the errors increased in three of the scenarios in which the emulators were trained with both datasets scenarios 5 6 and 7 when compared to the emulators trained with a single one scenario 8 model results overcame scenario 4 in several time steps although its error was slightly worse in the others time steps regarding the computational costs of the emulator it was achieved a time reduction of 23 in simulating the morphodynamic evolution using numerical models with the same input and output grid resolution whilst a time reduction of 87 was obtained using the emulator with low grid resolution inputs the main time consuming task in this methodology was the generation of hydrodynamics results by the numerical model which limited the improvement of the proposed methodology hence the performance gain with the emulator application would be higher for long term hydrodynamic simulations due to the lower wave velocities magnitudes which allows simulations with higher time steps although the error in the lr emulator had slightly increased its results were still similar to the numerical model additionally the use of images as input and output allows the potential application of this methodology to any other numerical model as mentioned before reducing the total computational time required for simulating the short term morphological evolution of a coastal area can be important to facilitate the application of xbeach models to real time early warning and forecasting systems this would allow to optimize the response of the authorities during extreme events and to increase the resilience of coastal communities finally this study demonstrated that the use of deep learning techniques can reduce the computational costs of numerical models allowing their implementation in real time forecasting and warning systems the emulators accurately reproduced the ces and ctblc results obtained with the xbeach software which is one of the most advanced coastal morphodynamics simulation model available achieving an error two orders lower than the morphodynamics variables range values software and data availability name of the software xbeach version 1 23 developers deltares xbeach open source community first year available 2009 cost free software availability https download deltares nl en download xbeach open source program size 330 97 mb the deep learning based emulator used for surrogating the xbeach morphodynamic module was implemented in python language version 3 9 based on tensorflow library the authors used a windows 11 home os environment cpu intel r core tm i7 8750h 2 20 ghz ram 16 gb gpu nvidia geforce gtx 1060 the architecture of the model is available at http www hydroshare org resource b4ae97df748842a1800816b32a3d640 b author contributions w m j p and i i designed the research w m performed the research w m j p and i i analyzed the data w m and j p implemented the models j p and i i proofread the article declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the doctoral grant sfrh bd 151383 2021 financed by the portuguese foundation for science and technology fct and with funds from the ministry of science technology and higher education under the mit portugal program i iglesias also acknowledge the fct financing through the ceec program 2022 07420 ceecind 
25397,this report presents a reference flow network for the conterminous united states that is built from the best available information from the u s geological survey the national oceanic and atmospheric administration national weather service and the u s environmental protection agency the work is intended to support durable data integration and reproducibility originating from the national hydrography dataset plus nhdplus v2 1 the reference flow network incorporates network connectivity enhancements from federal agency efforts after incorporating these network improvements many original nhdplus attributes were regenerated to enable network navigation and related operations after introducing the motivation and background for this work this report describes the attribute generation workflow and data quality checks that were performed in preparation of the dataset the reference flow network follows the nhdplus data model and is described using terms defined in the mainstem and drainage basin logical model and waterml2 part3 surface hydrology features conceptual model keywords hydrofabric flow network catchment hydrography abbreviations findable available interoperable and reusable fair national hydrography dataset nhd waterml2 part 3 surface hydrology features hy features data availability all data is referenced in the paper 1 introduction the route water follows from its source on the landscape to an inland sink or the ocean is important to many domains of environmental science including hydrology hydrodynamics geomorphology water quality limnology aquatic ecology water availability disaster management and others many water quantity and water quality models adopt network connectivity as a given that is not altered and has strong impacts on model performance for this reason improving network connectivity should help minimize fundamental errors in our models models that simulate flowing water or that use data located on a flow network require the best network representation possible the connectivity of these flow networks can be very complex and ever changing as a result even when authoritative quality controlled networks are used it is common for projects to apply changes based on local knowledge while many modeling groups start with the same source flow network changes to improve network connectivity rarely make it back into an integrated updated network systems to update the geometry of a latest available hydrographic dataset in the united states us the national hydrography dataset u s geological survey 2022 have been used to capture updates and fixes in some cases however fixes to the flow network of source datasets for modeling made by modeling projects have by in large not been incorporated back into a reference flow network for use in future modeling given this common occurrence interoperability and reproducibility become a challenge in the us digital representation of the national flow network has evolved for three decades horn 1994 bondelid et al 2010 mckay et al 2015 brakebill et al 2020 national oceanic and atmospheric administration 2021 since its release in 2015 the u s geological survey the national oceanic and atmospheric administration national weather service and the u s environmental protection agency have all worked to improve the network dewald 2017 of the national hydrography dataset plus v2 1 nhdplusv2 mckay et al 2015 the identifiers used to integrate data with these datasets the comid which is a per line identifier and the reachcode which aggregates one or more comid s do not persist across datasets and have changed through time as data improvements have been made the focus of the work described here is incorporation of this legacy of changes into a central reference flow network italicized throughout this report that can be updated regularly but use persistent identifiers and still support reproducibility and durable data integration see blodgett 2023 blodgett 2023 1 data releases persistent or durable identifiers and appropriate representation of flow networks across scale support continental scale nationally consistent and locally relevant modeling automation of data quality checks facilitates work with national or continental scale datasets that may be too large for review by a domain expert reproducibility and comparability can be difficult with a changing even if improving flow network if the connectivity and representation of flowing water bodies changes for example associations between stream gages or water quality sample locations sources of truth and rivers subjects of reproducible science must remain consistent if we are to expect reproducible outcomes however if identifiers or representations of the flow network change we will get different results the reference flow network aims to resolve the problem of durable identification and network representation it improves our collective ability to build representative modeling frameworks and integrate relevant landscape and observational data by simultaneously addressing the needs of continental hydrologic modeling and findable available interoperable and reusable fair wilkinson et al 2016 environmental data to facilitate these goals the reference flow network is based on the logical data model presented in blodgett et al 2021 and the more general conceptual data model in blodgett 2020 key terms used in this paper that are defined in these reports are introduced when first used and in addition to reference flow network the reference flow network is part of what is referred to here as a reference fabric the concept of reference fabric as introduced here is intended to support collaborative inter agency hydrologic modeling fig 1 a reference fabric is an integrated collection of data that is both a reference system to which information can be addressed and a reference dataset with which to create baseline representations of hydrologic systems a reference fabric includes a non spatial reference flow network line and polygon geometries and community recognized hydrologic locations points of interest that are integral to the flow network e g stream gages dams this report describes the creation of the reference flow network only us initiatives that may benefit from this work include the u s geological survey national water census michelsen et al 2016 miller et al 2020 and the national weather service national water model national oceanic and atmospheric administration 2016 within these the national oceanic and atmospheric administration noaa next generation water resource modeling framework ogden et al 2021 and the u s geological survey national hydrologic model bock et al 2021 regan et al 2019 are the work s initial focus there have been calls in the hydrologic and earth system modeling science communities for general advancement in continental domain hydrologic modeling which this work seeks to advance archfield et al 2015 clark et al 2015 treating public data as a universally available and useable strategic asset is widely understood to be in the public interest oecd 2021 recognizing this the development of the reference fabric is also motivated by the go fair initiative and the principles of the internet of water internet of water organization 2021 the work has specific focus on public domain web identifiers for environmental features blodgett 2020 that allow all data providers to contribute to a well understood system of linked environmental data these fair public data aspects of the motivation for this work are realized through a community internet of water data indexing system known as the network linked data index blodgett 2023 2 and the geoconnex us identifier registry and knowledge graph internet of water organization 2023 objectives of modeling and fair data work together naturally models benefit from additional data brought to bear while fair data can be made more capable by leveraging and incorporating data improvements and outputs of models 1 1 background the hy features conceptual data model standard blodgett and dornblut 2018 provides a set of concepts for formalizing hydrologic data integration and reproducibility in the hydrosciences it introduces a wholistic conceptual definition of catchment italicized when used in the hy features sense that serves two roles conveying water from the land into a flow network and conveying water from an inlet to an outlet these two roles are fulfilled by the flowpath and catchment area conceptual realizations of the catchment concept respectively flowpath and catchment area are italicized to draw attention to their specific definition from the hy features conceptual data model the catchment concept is wholistic in that it includes all aspects of hydrologic function within its divide and it can be applied at any scale a special case but still wholistic and applicable at any scale of the catchment concept is drainage basin a drainage basin is a catchment that is the total upstream area draining to an outlet blodgett johnson 2022 1 drainage basin is italicized to draw attention to its specific definition as such a drainage basin can be used to define nested hierarchies across a wide range of spatial scales given that a drainage basin has a primary flowpath a mainstem nested hierarchies of drainage basins are connected by a directed acyclic graph or tree of mainstems blodgett et al 2021 mainstem is italicized to draw attention to its specific definition the reference flow network depends heavily on the mainstem concept and its implementation in the nhdplus data model level path in nhdplus the level path attribute is a unique identifier for the primary upstream path from anywhere in the network that is described in detail below level paths generally follows a river s name but are defined for features without names as well fig 2 shows the relationship between incremental catchment areas surrounded by their divides flowpaths a scale dependent drainage basin surrounded by its divide and its respective mainstem this framework is intended to support integration of multi scale hydrologic process investigation from zero order to large mainstems using common identifiers and feature linkages previous works on national hydrographic data have been distributed as static snapshots that do not evolve with time the source datasets and software used to create the hydrographic datasets have not been made available for inspection reuse or enhancement as with horn 1994 bondelid et al 2010 and mckay et al 2015 this in addition to diversity in manually created and edited content has posed challenges to data curation maintenance and general community involvement in contrast the reference flow network has been developed as a reproducible and open source workflow based on publicly available input datasets and software while updating such a dataset is a non trivial process these practices aim to make that process more efficient and foster better community understanding and involvement as the network is updated the dataset will evolve in three ways 1 its resolution will improve 2 its representation of the network will improve and 3 the flaws in its source data will be identified and fixed these changes will be incorporated in an open and reproducible process where identifiers are maintained and backward compatibility is established explicitly the mainstem identifier system at the core of the reference flow network facilitates incorporation of these updates and backward compatibility into the future 1 2 network representation and attributes the nhdplusv2 dataset mckay et al 2015 is used as the base data source and base data model for the reference fabric dewald 2017 brakebill et al 2020 this ensures that the work can capitalize on the efforts that came before and that this work is compatible with applications using the nhd data model u s geological survey 2022 and associated features the nhdplus data model includes what it calls value added attributes that are documented in the nhdplusv2 manual mckay et al 2015 and are also implemented in the reference flow network all flow networks can be represented as an edge to edge edge list or edge to node topology node topology fig 3 an edge list only expresses the connectivity between edges flowpaths in the context of rivers requiring nodes confluences in the context of rivers to be inferred both of these schemes are present in the nhdplus data model specifically the hydroseq dnhydroseq hydrosequence down hydrosequence relationship expresses the network as a dendritic edge list and the fromnode tonode relationship expresses it as a node topology these attributes are described in detail in table 2 this initial work on the reference flow network did not require representation of diverted flow given this the current reference flow network only includes the edge list representation of a network and does not include diverted flow by treating the network as a dendritic tree of primary downstream paths headwaters and diverted paths are treated similarly practically this means that the diverted fraction is always 0 at a divergence in practice a flow routing algorithm could change this assumption as source dataset diversion information is not lost for example fig 4 illustrates two paths one that is diverted from the other the path through edges 1 4 and 5 would be considered the main path the diversion at n2 is only represented in the edge node version of the flow network edge 2 is treated as if it has no inflow in the edge to edge version of the flow network and the path through edges 2 and 3 is treated as a tributary path explicit diversion handling could be introduced by reintroducing edge node connections but was not required for the initial needs of the dataset so was not included while the current version of the reference flow network focuses on the nhdplus it is important to note that the methods and software developed for it are applicable to any hydrologic network that contains a set of key base attributes table 1 these attributes are used to generate two key network attributes hydrosequence and level path any algorithm that uses a flow network and requires understanding the upstream to downstream relationship of network elements requires a sorted version or attribute that facilitates upstream downstream sorting of the network the nhdplus data model attribute hydrosequence is functionally a topological sort f the flowpath network cormen and leiserson 2022 an attribute functionally equivalent to hydrosequence has been used since the earliest digital hydrographic datasets horn 1994 it is an integer identifier that is guaranteed to decrease in the downstream direction for flowpaths that are not connected by a single direction navigation e g parallel tributaries the hydrosequence has no significance in other words when two flowpaths have a single direction navigable connection the downstream flowpath will always have the smaller hydrosequence attribute fig 5 d shows the hydrosequence attribute visually level path is derived from stream level which is a constant integer attribute along a mainstem rivers from outlet to headwater stream leveling is the process of establishing level paths through a stream network this is accomplished with a set of rules that determine which tributary should be considered dominant at every confluence and establishes the mainstem for each drainage basin in a network in the stream level algorithm rivers terminating to the ocean are given level 1 this level extends all the way to the headwater rivers terminating into level 1 rivers are given level 2 and so on fig 5 illustrates stream level 5b and level path 5c as a point of reference the nhdplusv2 has about 2 7 million flowlines and includes about 1 million unique level paths the longest level path the missouri river is over 2400 individual flowlines in the nhdplus data model level path identifiers are set to the same value as the hydrosequence of the flowpath at the level path s outlet see blodgett et al 2021 for a more in depth discussion of these concepts a detail worth illustrating here relates to durability of identifiers given that the value of hydrosequence attributes the sort order will change if the number of features in the network changes using the outlet hydrosequence value as a level path s value results in unstable level path identifiers as a result use of the hydrosequence based level path identifier for cross dataset integration is impossible level path and hydrosequence form the basis for a number of additional attributes useful for hydrologic networking table 2 summarizes these attributes definitions and their purposes for hydrologic network operations 2 methods the main improvements made to the nhdplusv2 network have been to the network topology anytime there is a change to the topology there are cascading impacts to the base attributes table 1 the resulting hydrosequence level path and their derived values table 2 a robust open source workflow for regenerating the derived values from the base attributes should help meet the objectives of a reference fabric that represents the legacy of improvements and is able to adapt to future change 2 1 source data processing on the reference flow network started by combining the value added attribute table of the nhdplusv2 mckay et al 2015 with the network improvements contained in the e2nhdplus brakebill et al 2020 and nwm national oceanic and atmospheric administration 2021 applications the first challenge in combining the networks is associating their respective network topologies the nhdplusv2 and e2nhdplus representation of the network are designed to account for divergences and thus include nodes in the network topology representation as noted in table 1 this node topology can be used to derive an edge list a node topology is included in the nhdplusv2 and e2nhdplus but not the nwm the nwm only includes an edge list topology defined as link and to in the nwm routelink file the respective topologies are shown in table 3 in general modifications to network connectivity in the nwm network established connections where the nhdplusv2 indicated disconnected network modifications in the e2nhdplus are generally corrections to the main path where a divergence occurs in the nhdplusv2 figs 6 and 8 show the spatial distribution of changes to the network while these are the initial improvements integrated with the network they are not exhaustive as new network improvements are identified the design of the reference flow network could support incorporation of the updates 2 2 nhdplustools r package the nhdplustools r package blodgett and johnson 2022 houses the majority of the hydrologic network and hydrographic data functionality used in development of the reference flow network workflow repositories associated with the mainstem rivers of the conterminous united states data release blodgett 2023 blodgett 2023 1 and the broader geospatial fabric for national hydrologic modeling data release provisional at the time of writing as described in https waterdata usgs gov blog nldi intro blodgett and johnson 2023 contain additional data manipulation logic the following sections describe hydrologic network attribute generation functionality built into nhdplustools and used in the workflows all nhdplustools functions are implemented using a rigorous test driven development style that is tests were developed as part of the software development process verifying that results match expectations precisely from initial implementation into the future most functions have associated tests that reproduce nhdplusv2 attributes and verify that results are equivalent and tests that verify fine grained details and edge cases additionally as bugs and edge cases were identified tests to reproduce the bug were implemented as part of the fix ensuring the issue does not reappear as the code base is modified in the future this testing approach is critical to ensure accurate results in future use of workflows with new and more regularly updated data nhdplustools is the first open source implementation of the nhdplus data model attributes that uses this test driven development style 2 3 workflow the workflow described here is in two parts 1 the logic for combining nhdplusv2 nwm and e2nhdplus networks 2 the application of nhdplustools to regenerate network attributes 2 3 1 network combination the three input datasets were developed from the nhdplusv2 originally and all use the same common identifier comid as a first step the nhdplusv2 and e2nhdplus tables were joined based on comid in places where there was disagreement in the node topology the e2nhdplus modifications were kept once changed a new tocomid edge list field was created based on the updated node topology out of a total 2 7 million features in the nhdplusv2 2151 tocomid values and 2 769 divergence indicators were changed with this network connections from the nwm could be added the following prefilters were applied to avoid changes that could not be used for the reference flow network 1 where a given feature had more than one tocomid the divergence indicator was used to limit network connectivity to one and only one downstream feature there were about 73 000 diversions avoided here 2 no changes were made to relationships involving features with a coastal feature type code even though some nwm changes were directed to coastline features 3 changes were not applied where more than ten features were upstream of one downstream feature some nwm changes were directed through large lakes and could not be used 4 only changes directing flow to features originally part of the nhdplusv2 were considered for inclusion some nwm changes went to features not included in the nhdplusv2 domain and could not be used 5 after release of the first version of the reference flow network an issue was found such that changes to primary downstream connectivity applied in e2nhdplus were being reverted when applying nwm changes to fix the issue any disagreement in downstream connectivity where the e2nhdplus made a change were not altered when incorporating the nwm network the reference flow network was released as a version 2 blodgett 2023 1 with these caveats applied 201 features locations shown in fig 6 had network connectivity in the nwm that did not exist in the e2nhdplus update of nhdplusv2 for these the divergence and tocomid attributes were updated if what was a divergence was to become a main path or vice versa the divergence indicator was switched the tocomid attribute was then switched to that indicated by the nwm with these applications the resulting network retained the information in table 4 from which nhdplus network attribute calculations table 2 could be made 2 3 3 application of nhdplustools with the completed network defined in table 4 one remaining attribute is needed per table 1 a weight for determination of primary upstream paths for this application the arbolate sum was calculated using the calculate arbolate sum nhdplustools blodgett and johnson 2022 function once added a series of nhdplustools functions shown in fig 7 were executed sequentially to generate the hydrosequence and level path identifiers and the nhdplus data model attributes in table 2 calculate arbolate sum calculates the sum of all upstream flowpath lengths for each flowpath outlet get sorted walks the network of features from outlets to headwaters returning data in the order it was encountered the row number of the returned features is a topological sort for the network that can be used as a hydrosequence get levelpaths uses a name identifier in this application the name is the nhdplus mckay et al 2015 levelpathid which corresponds to the nhd geographic names information system u s geological survey 2021 name and weight to identify dominant level paths through the network the upstream tributary with the same name is considered dominant unless the weight is x times larger x is defined by the user for the unnamed or differently named upstream feature for the work here arbolate sum was used for the weight and the override factor was set to 5 get pathlengths walks the network of features from outlets to headwaters adding the length of a feature to the already visited pathlength of its downstream neighbor calculate total drainage area accumulates incremental drainage area starting at headwaters working downstream summing upstream neighbor s total drainage area run plus attributes function calls get sorted get levelpaths get pathlengths and calculate total drainage area and adds down level path and down hydrosequence with a table join post process as noted early in this process summary section the name identifier used in get levelpaths was actually the level path from nhdplusv2 typically this would be a name from a source of geographic names in the nhd this is derived from the u s geographic names information system u s geological survey 2021 this was a convenient way to ensure the new level path identifiers would correspond to the nhdplusv2 source except where the weight indicated is 5 times greater than the arbolate sum along the name indicated path the original nhdplusv2 level paths followed names without this override so by using that level path here names are generally followed while major outliers could be fixed notable outliers were related to artificial paths that form connections in lakes and rivers that did not have the name of a major river associated with them in this case a named tributary would be followed rather than the major upstream path that would have otherwise been the obvious option to follow the selection of 5 for the override was based on evaluation of instances where following the name would cause issues but not set so high as to change important confluences such as the missouri river and mississippi river arbolate sum 940 400 km vs 302 300 km once processing described just above was complete the nhdplusv2 level path identifier i e nameid was dropped and the weight attribute renamed to arbolate sum a stream order and stream level were then added to the output network by calling the get streamlevel and get streamorder functions while not applicable to modeling and data integration applications stream order and stream level were added for completeness relative to nhdplusv2 value added attributes as a last step useful nhdplusv2 attributes that were not affected by network topology were joined to the output network and saved 3 results the reference flow network blodgett 2023 1 has been checked and reviewed using automated and manual methods automated checks ensured that expected relationships between identifiers such as hydrosequence level path and terminal path were correct and that all attributes were fully populated and in their expected range due to the size of the dataset manual checks were not comprehensive but were used to identify some systematic issues specifically with initializing some coastal outlets that were addressed prior to release of the dataset manual checks also cross validated attributes such as total drainage area between nhdplusv2 and this new network due to the addition of previously disconnected areas and alteration of primary vs diverted path fully automated checks of accumulated network attributes were not possible as no one accurate value is available for all feature attributes however some wholistic validation checks were performed for example the only places with more than 10 difference in total drainage area fig 9 were are associated with nearby alteration of network connections fig 8 note that the absence of yellow overlay in fig 9 indicates agreement in total drainage area in addition to automated and manual checks the reference flow network blodgett 2023 1 has been tested through its use as a primary input to the ongoing national reference fabric development fig 1 blodgett and johnson 2023 one of these derived products is a conus wide hydrofabric created for development of the noaa nextgen national water resource modeling framework johnson 2022 for this initial proof of concept a complete 2 month hourly conus scale hydrologic simulation has been executed on this dataset each of these modeling applications has been a rigorous test of the dataset s continuity and quality in the interest of fair data and the internet of water the reference flow network has also been tested through its use as the primary input to the mainstem rivers of the conterminous united states data release blodgett 2023 through development of that data release the reference flow network has been subjected to in depth verification the workflow that created the mainstem rivers of the conterminous united states data release identified matching level paths between the reference flow network and several other hydrographic networks manual inspection e g not automated conducted during dataset development and the fact that strong correspondence was found between many networks is further evidence of the quality of this network 4 discussion while implemented at a national scale this work is also intended to serve the needs of regional and local applications the national model applications this work supports may provide local scale information and multi purpose local modeling capacity regional and local studies may be able to conduct research and assessment activities reusing the network data data integration it enables and or the tool chain used in its creation these three modes of reuse illustrate the value that could be derived from this work 1 adoption of network features and attributes 2 use of the network features as a data integration aid and 3 use of the tool chain developed in support of 1 and 2 on various hydrographic datasets meeting minimum data requirements us federal water resources research assessment and forecasting applications michelsen et al 2016 miller et al 2020 ogden et al 2021 were the primary motivation for this work and the ability for local and regional stakeholders to utilize it for their own context is important but the work also has global significance the tools developed for this work concerning us national hydrographic datasets blodgett and johnson 2022 have also been used with a global network of river features for the national hydrologic model geospatial fabric the global merit basins dataset lin et al 2019 yamazaki et al 2019 was used in the alaska domain the tools for creation of nhdplus network attributes such as level path were also used for a complete global river network web visualization learner 2023 these data are available in mainstem rivers of the world based on merit hydrography and natural earth names blodgett 2022 and could support a global dataset of flow network features as has been done for the continental us additionally the idea of a reference flow network could be extended to include other kinds of reference features like coastal zones and hydrogeology the specificity of the data models and implementation of this work are intentionally limited in scope with an eye on incremental advancement divergences and divergence groups is a topic that was out of scope for this work but inclusion may be required support some applications referred to as diversion groups in the report accompanying the e2nhdplus dataset brakebill et al 2020 this concept recognizes that some sets of diversions represent a group of diverted paths whose flow recombines some distance downstream these groups of diversions result from many physical phenomena but are most commonly found where rivers combine in a complex system of low slope channels in contrast to hydrologic diversions where a diversion forms the mainstem of its own drainage basin the combination of channels that make up a divergence group can be treated as a single flowpath relative to overall network connectivity representing the distinction between groups of divergent paths that recombine and more significant hydrologic diversions is challenging due to many factors and is an important topic for future efforts of this overall body of work a hydrologic flow network as presented in this report is a dendritic tree represented as a directed acyclic graph in reality each element of the network is at some time a flowing body of water representation of flowing waterbodies as linestrings as has been presented here is common and useful but not complete by any means visually a line representation of a river has some nominal width that depending on map scale represents some real world waterbody width the datasets discussed here do not include a physically based attribute to tie to this nominal line width taking this one step further a given body of water has some depth which could be expressed simply as a representative depth or more precisely with a collection of cross sections or other physical representations this problem of waterbody integration with flow network representation is confounded by large waterbodies that especially when flooded inundate parts of the hydrologic landscape and associated flow network the joint channel as container and waterbody as contained fluid body concepts presented in hy features provide a framework for this problem research into how to separate the topo bathymetric channel and hydrodynamic waterbody representation may allow necessary integrations of waterbody in channel and river corridor models 5 conclusions the reference flow network is built on nhdplusv2 mckay et al 2015 an improved version of nhdplusv1 which itself was based on best available inputs and was the product of significant quality control bondelid et al 2010 additional network improvements from the e2nhdplus dataset brakebill et al 2020 and the nwm hydrofabric national oceanic and atmospheric administration 2021 were then applied in an open source reproducible workflow national scope integration and network manipulation work have successfully used this network with as good or better results than previous efforts johnson 2022 combined these advances justify application of the reference flow network in local to national scale hydrologic modeling applications however representation of hydrologic features is imperfect with numerous sources of ambiguity and inaccuracy it is because of this that a second equally critical part of this work is an automated and rigorously tested workflow that generates the attributes based on changes in topology blodgett and johnson 2022 it should be expected that while rare there will be cases where the network connectivity in the reference flow network does not match reality some will be instances where the scale at which these data resolve features is too coarse others will be cases where on the ground conditions are misrepresented or have changed given this updates should be expected when issues are encountered however the design of this dataset and system surrounding it is intended for this and capable of rapidly integrating changes to serve the needs of the hydroscience community software and data availability all software and data used in preparation of this report can be found at blodgett d l 2022 mainstems workflow hu12 nhdplusv2 nhdplus hires matching https doi org 10 5066 p9h0ptrh blodgett d l johnson j m 2022 nhdplustools tools for accessing and working with the nhdplus https doi org 10 5066 p97as8jd blodgett d l 2023 updated conus river network attributes based on the e2nhdplusv2 and nwmv2 1 networks ver 2 0 february 2023 u s geological survey data release https doi org 10 5066 p976xcvt declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements all u s geological survey work documented in this report was funded by the u s geological survey water mission area water availability and use science program national oceanic and atmospheric administration national weather service contributions are part of the next generation water modeling framework initiative of the office of water prediction any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25397,this report presents a reference flow network for the conterminous united states that is built from the best available information from the u s geological survey the national oceanic and atmospheric administration national weather service and the u s environmental protection agency the work is intended to support durable data integration and reproducibility originating from the national hydrography dataset plus nhdplus v2 1 the reference flow network incorporates network connectivity enhancements from federal agency efforts after incorporating these network improvements many original nhdplus attributes were regenerated to enable network navigation and related operations after introducing the motivation and background for this work this report describes the attribute generation workflow and data quality checks that were performed in preparation of the dataset the reference flow network follows the nhdplus data model and is described using terms defined in the mainstem and drainage basin logical model and waterml2 part3 surface hydrology features conceptual model keywords hydrofabric flow network catchment hydrography abbreviations findable available interoperable and reusable fair national hydrography dataset nhd waterml2 part 3 surface hydrology features hy features data availability all data is referenced in the paper 1 introduction the route water follows from its source on the landscape to an inland sink or the ocean is important to many domains of environmental science including hydrology hydrodynamics geomorphology water quality limnology aquatic ecology water availability disaster management and others many water quantity and water quality models adopt network connectivity as a given that is not altered and has strong impacts on model performance for this reason improving network connectivity should help minimize fundamental errors in our models models that simulate flowing water or that use data located on a flow network require the best network representation possible the connectivity of these flow networks can be very complex and ever changing as a result even when authoritative quality controlled networks are used it is common for projects to apply changes based on local knowledge while many modeling groups start with the same source flow network changes to improve network connectivity rarely make it back into an integrated updated network systems to update the geometry of a latest available hydrographic dataset in the united states us the national hydrography dataset u s geological survey 2022 have been used to capture updates and fixes in some cases however fixes to the flow network of source datasets for modeling made by modeling projects have by in large not been incorporated back into a reference flow network for use in future modeling given this common occurrence interoperability and reproducibility become a challenge in the us digital representation of the national flow network has evolved for three decades horn 1994 bondelid et al 2010 mckay et al 2015 brakebill et al 2020 national oceanic and atmospheric administration 2021 since its release in 2015 the u s geological survey the national oceanic and atmospheric administration national weather service and the u s environmental protection agency have all worked to improve the network dewald 2017 of the national hydrography dataset plus v2 1 nhdplusv2 mckay et al 2015 the identifiers used to integrate data with these datasets the comid which is a per line identifier and the reachcode which aggregates one or more comid s do not persist across datasets and have changed through time as data improvements have been made the focus of the work described here is incorporation of this legacy of changes into a central reference flow network italicized throughout this report that can be updated regularly but use persistent identifiers and still support reproducibility and durable data integration see blodgett 2023 blodgett 2023 1 data releases persistent or durable identifiers and appropriate representation of flow networks across scale support continental scale nationally consistent and locally relevant modeling automation of data quality checks facilitates work with national or continental scale datasets that may be too large for review by a domain expert reproducibility and comparability can be difficult with a changing even if improving flow network if the connectivity and representation of flowing water bodies changes for example associations between stream gages or water quality sample locations sources of truth and rivers subjects of reproducible science must remain consistent if we are to expect reproducible outcomes however if identifiers or representations of the flow network change we will get different results the reference flow network aims to resolve the problem of durable identification and network representation it improves our collective ability to build representative modeling frameworks and integrate relevant landscape and observational data by simultaneously addressing the needs of continental hydrologic modeling and findable available interoperable and reusable fair wilkinson et al 2016 environmental data to facilitate these goals the reference flow network is based on the logical data model presented in blodgett et al 2021 and the more general conceptual data model in blodgett 2020 key terms used in this paper that are defined in these reports are introduced when first used and in addition to reference flow network the reference flow network is part of what is referred to here as a reference fabric the concept of reference fabric as introduced here is intended to support collaborative inter agency hydrologic modeling fig 1 a reference fabric is an integrated collection of data that is both a reference system to which information can be addressed and a reference dataset with which to create baseline representations of hydrologic systems a reference fabric includes a non spatial reference flow network line and polygon geometries and community recognized hydrologic locations points of interest that are integral to the flow network e g stream gages dams this report describes the creation of the reference flow network only us initiatives that may benefit from this work include the u s geological survey national water census michelsen et al 2016 miller et al 2020 and the national weather service national water model national oceanic and atmospheric administration 2016 within these the national oceanic and atmospheric administration noaa next generation water resource modeling framework ogden et al 2021 and the u s geological survey national hydrologic model bock et al 2021 regan et al 2019 are the work s initial focus there have been calls in the hydrologic and earth system modeling science communities for general advancement in continental domain hydrologic modeling which this work seeks to advance archfield et al 2015 clark et al 2015 treating public data as a universally available and useable strategic asset is widely understood to be in the public interest oecd 2021 recognizing this the development of the reference fabric is also motivated by the go fair initiative and the principles of the internet of water internet of water organization 2021 the work has specific focus on public domain web identifiers for environmental features blodgett 2020 that allow all data providers to contribute to a well understood system of linked environmental data these fair public data aspects of the motivation for this work are realized through a community internet of water data indexing system known as the network linked data index blodgett 2023 2 and the geoconnex us identifier registry and knowledge graph internet of water organization 2023 objectives of modeling and fair data work together naturally models benefit from additional data brought to bear while fair data can be made more capable by leveraging and incorporating data improvements and outputs of models 1 1 background the hy features conceptual data model standard blodgett and dornblut 2018 provides a set of concepts for formalizing hydrologic data integration and reproducibility in the hydrosciences it introduces a wholistic conceptual definition of catchment italicized when used in the hy features sense that serves two roles conveying water from the land into a flow network and conveying water from an inlet to an outlet these two roles are fulfilled by the flowpath and catchment area conceptual realizations of the catchment concept respectively flowpath and catchment area are italicized to draw attention to their specific definition from the hy features conceptual data model the catchment concept is wholistic in that it includes all aspects of hydrologic function within its divide and it can be applied at any scale a special case but still wholistic and applicable at any scale of the catchment concept is drainage basin a drainage basin is a catchment that is the total upstream area draining to an outlet blodgett johnson 2022 1 drainage basin is italicized to draw attention to its specific definition as such a drainage basin can be used to define nested hierarchies across a wide range of spatial scales given that a drainage basin has a primary flowpath a mainstem nested hierarchies of drainage basins are connected by a directed acyclic graph or tree of mainstems blodgett et al 2021 mainstem is italicized to draw attention to its specific definition the reference flow network depends heavily on the mainstem concept and its implementation in the nhdplus data model level path in nhdplus the level path attribute is a unique identifier for the primary upstream path from anywhere in the network that is described in detail below level paths generally follows a river s name but are defined for features without names as well fig 2 shows the relationship between incremental catchment areas surrounded by their divides flowpaths a scale dependent drainage basin surrounded by its divide and its respective mainstem this framework is intended to support integration of multi scale hydrologic process investigation from zero order to large mainstems using common identifiers and feature linkages previous works on national hydrographic data have been distributed as static snapshots that do not evolve with time the source datasets and software used to create the hydrographic datasets have not been made available for inspection reuse or enhancement as with horn 1994 bondelid et al 2010 and mckay et al 2015 this in addition to diversity in manually created and edited content has posed challenges to data curation maintenance and general community involvement in contrast the reference flow network has been developed as a reproducible and open source workflow based on publicly available input datasets and software while updating such a dataset is a non trivial process these practices aim to make that process more efficient and foster better community understanding and involvement as the network is updated the dataset will evolve in three ways 1 its resolution will improve 2 its representation of the network will improve and 3 the flaws in its source data will be identified and fixed these changes will be incorporated in an open and reproducible process where identifiers are maintained and backward compatibility is established explicitly the mainstem identifier system at the core of the reference flow network facilitates incorporation of these updates and backward compatibility into the future 1 2 network representation and attributes the nhdplusv2 dataset mckay et al 2015 is used as the base data source and base data model for the reference fabric dewald 2017 brakebill et al 2020 this ensures that the work can capitalize on the efforts that came before and that this work is compatible with applications using the nhd data model u s geological survey 2022 and associated features the nhdplus data model includes what it calls value added attributes that are documented in the nhdplusv2 manual mckay et al 2015 and are also implemented in the reference flow network all flow networks can be represented as an edge to edge edge list or edge to node topology node topology fig 3 an edge list only expresses the connectivity between edges flowpaths in the context of rivers requiring nodes confluences in the context of rivers to be inferred both of these schemes are present in the nhdplus data model specifically the hydroseq dnhydroseq hydrosequence down hydrosequence relationship expresses the network as a dendritic edge list and the fromnode tonode relationship expresses it as a node topology these attributes are described in detail in table 2 this initial work on the reference flow network did not require representation of diverted flow given this the current reference flow network only includes the edge list representation of a network and does not include diverted flow by treating the network as a dendritic tree of primary downstream paths headwaters and diverted paths are treated similarly practically this means that the diverted fraction is always 0 at a divergence in practice a flow routing algorithm could change this assumption as source dataset diversion information is not lost for example fig 4 illustrates two paths one that is diverted from the other the path through edges 1 4 and 5 would be considered the main path the diversion at n2 is only represented in the edge node version of the flow network edge 2 is treated as if it has no inflow in the edge to edge version of the flow network and the path through edges 2 and 3 is treated as a tributary path explicit diversion handling could be introduced by reintroducing edge node connections but was not required for the initial needs of the dataset so was not included while the current version of the reference flow network focuses on the nhdplus it is important to note that the methods and software developed for it are applicable to any hydrologic network that contains a set of key base attributes table 1 these attributes are used to generate two key network attributes hydrosequence and level path any algorithm that uses a flow network and requires understanding the upstream to downstream relationship of network elements requires a sorted version or attribute that facilitates upstream downstream sorting of the network the nhdplus data model attribute hydrosequence is functionally a topological sort f the flowpath network cormen and leiserson 2022 an attribute functionally equivalent to hydrosequence has been used since the earliest digital hydrographic datasets horn 1994 it is an integer identifier that is guaranteed to decrease in the downstream direction for flowpaths that are not connected by a single direction navigation e g parallel tributaries the hydrosequence has no significance in other words when two flowpaths have a single direction navigable connection the downstream flowpath will always have the smaller hydrosequence attribute fig 5 d shows the hydrosequence attribute visually level path is derived from stream level which is a constant integer attribute along a mainstem rivers from outlet to headwater stream leveling is the process of establishing level paths through a stream network this is accomplished with a set of rules that determine which tributary should be considered dominant at every confluence and establishes the mainstem for each drainage basin in a network in the stream level algorithm rivers terminating to the ocean are given level 1 this level extends all the way to the headwater rivers terminating into level 1 rivers are given level 2 and so on fig 5 illustrates stream level 5b and level path 5c as a point of reference the nhdplusv2 has about 2 7 million flowlines and includes about 1 million unique level paths the longest level path the missouri river is over 2400 individual flowlines in the nhdplus data model level path identifiers are set to the same value as the hydrosequence of the flowpath at the level path s outlet see blodgett et al 2021 for a more in depth discussion of these concepts a detail worth illustrating here relates to durability of identifiers given that the value of hydrosequence attributes the sort order will change if the number of features in the network changes using the outlet hydrosequence value as a level path s value results in unstable level path identifiers as a result use of the hydrosequence based level path identifier for cross dataset integration is impossible level path and hydrosequence form the basis for a number of additional attributes useful for hydrologic networking table 2 summarizes these attributes definitions and their purposes for hydrologic network operations 2 methods the main improvements made to the nhdplusv2 network have been to the network topology anytime there is a change to the topology there are cascading impacts to the base attributes table 1 the resulting hydrosequence level path and their derived values table 2 a robust open source workflow for regenerating the derived values from the base attributes should help meet the objectives of a reference fabric that represents the legacy of improvements and is able to adapt to future change 2 1 source data processing on the reference flow network started by combining the value added attribute table of the nhdplusv2 mckay et al 2015 with the network improvements contained in the e2nhdplus brakebill et al 2020 and nwm national oceanic and atmospheric administration 2021 applications the first challenge in combining the networks is associating their respective network topologies the nhdplusv2 and e2nhdplus representation of the network are designed to account for divergences and thus include nodes in the network topology representation as noted in table 1 this node topology can be used to derive an edge list a node topology is included in the nhdplusv2 and e2nhdplus but not the nwm the nwm only includes an edge list topology defined as link and to in the nwm routelink file the respective topologies are shown in table 3 in general modifications to network connectivity in the nwm network established connections where the nhdplusv2 indicated disconnected network modifications in the e2nhdplus are generally corrections to the main path where a divergence occurs in the nhdplusv2 figs 6 and 8 show the spatial distribution of changes to the network while these are the initial improvements integrated with the network they are not exhaustive as new network improvements are identified the design of the reference flow network could support incorporation of the updates 2 2 nhdplustools r package the nhdplustools r package blodgett and johnson 2022 houses the majority of the hydrologic network and hydrographic data functionality used in development of the reference flow network workflow repositories associated with the mainstem rivers of the conterminous united states data release blodgett 2023 blodgett 2023 1 and the broader geospatial fabric for national hydrologic modeling data release provisional at the time of writing as described in https waterdata usgs gov blog nldi intro blodgett and johnson 2023 contain additional data manipulation logic the following sections describe hydrologic network attribute generation functionality built into nhdplustools and used in the workflows all nhdplustools functions are implemented using a rigorous test driven development style that is tests were developed as part of the software development process verifying that results match expectations precisely from initial implementation into the future most functions have associated tests that reproduce nhdplusv2 attributes and verify that results are equivalent and tests that verify fine grained details and edge cases additionally as bugs and edge cases were identified tests to reproduce the bug were implemented as part of the fix ensuring the issue does not reappear as the code base is modified in the future this testing approach is critical to ensure accurate results in future use of workflows with new and more regularly updated data nhdplustools is the first open source implementation of the nhdplus data model attributes that uses this test driven development style 2 3 workflow the workflow described here is in two parts 1 the logic for combining nhdplusv2 nwm and e2nhdplus networks 2 the application of nhdplustools to regenerate network attributes 2 3 1 network combination the three input datasets were developed from the nhdplusv2 originally and all use the same common identifier comid as a first step the nhdplusv2 and e2nhdplus tables were joined based on comid in places where there was disagreement in the node topology the e2nhdplus modifications were kept once changed a new tocomid edge list field was created based on the updated node topology out of a total 2 7 million features in the nhdplusv2 2151 tocomid values and 2 769 divergence indicators were changed with this network connections from the nwm could be added the following prefilters were applied to avoid changes that could not be used for the reference flow network 1 where a given feature had more than one tocomid the divergence indicator was used to limit network connectivity to one and only one downstream feature there were about 73 000 diversions avoided here 2 no changes were made to relationships involving features with a coastal feature type code even though some nwm changes were directed to coastline features 3 changes were not applied where more than ten features were upstream of one downstream feature some nwm changes were directed through large lakes and could not be used 4 only changes directing flow to features originally part of the nhdplusv2 were considered for inclusion some nwm changes went to features not included in the nhdplusv2 domain and could not be used 5 after release of the first version of the reference flow network an issue was found such that changes to primary downstream connectivity applied in e2nhdplus were being reverted when applying nwm changes to fix the issue any disagreement in downstream connectivity where the e2nhdplus made a change were not altered when incorporating the nwm network the reference flow network was released as a version 2 blodgett 2023 1 with these caveats applied 201 features locations shown in fig 6 had network connectivity in the nwm that did not exist in the e2nhdplus update of nhdplusv2 for these the divergence and tocomid attributes were updated if what was a divergence was to become a main path or vice versa the divergence indicator was switched the tocomid attribute was then switched to that indicated by the nwm with these applications the resulting network retained the information in table 4 from which nhdplus network attribute calculations table 2 could be made 2 3 3 application of nhdplustools with the completed network defined in table 4 one remaining attribute is needed per table 1 a weight for determination of primary upstream paths for this application the arbolate sum was calculated using the calculate arbolate sum nhdplustools blodgett and johnson 2022 function once added a series of nhdplustools functions shown in fig 7 were executed sequentially to generate the hydrosequence and level path identifiers and the nhdplus data model attributes in table 2 calculate arbolate sum calculates the sum of all upstream flowpath lengths for each flowpath outlet get sorted walks the network of features from outlets to headwaters returning data in the order it was encountered the row number of the returned features is a topological sort for the network that can be used as a hydrosequence get levelpaths uses a name identifier in this application the name is the nhdplus mckay et al 2015 levelpathid which corresponds to the nhd geographic names information system u s geological survey 2021 name and weight to identify dominant level paths through the network the upstream tributary with the same name is considered dominant unless the weight is x times larger x is defined by the user for the unnamed or differently named upstream feature for the work here arbolate sum was used for the weight and the override factor was set to 5 get pathlengths walks the network of features from outlets to headwaters adding the length of a feature to the already visited pathlength of its downstream neighbor calculate total drainage area accumulates incremental drainage area starting at headwaters working downstream summing upstream neighbor s total drainage area run plus attributes function calls get sorted get levelpaths get pathlengths and calculate total drainage area and adds down level path and down hydrosequence with a table join post process as noted early in this process summary section the name identifier used in get levelpaths was actually the level path from nhdplusv2 typically this would be a name from a source of geographic names in the nhd this is derived from the u s geographic names information system u s geological survey 2021 this was a convenient way to ensure the new level path identifiers would correspond to the nhdplusv2 source except where the weight indicated is 5 times greater than the arbolate sum along the name indicated path the original nhdplusv2 level paths followed names without this override so by using that level path here names are generally followed while major outliers could be fixed notable outliers were related to artificial paths that form connections in lakes and rivers that did not have the name of a major river associated with them in this case a named tributary would be followed rather than the major upstream path that would have otherwise been the obvious option to follow the selection of 5 for the override was based on evaluation of instances where following the name would cause issues but not set so high as to change important confluences such as the missouri river and mississippi river arbolate sum 940 400 km vs 302 300 km once processing described just above was complete the nhdplusv2 level path identifier i e nameid was dropped and the weight attribute renamed to arbolate sum a stream order and stream level were then added to the output network by calling the get streamlevel and get streamorder functions while not applicable to modeling and data integration applications stream order and stream level were added for completeness relative to nhdplusv2 value added attributes as a last step useful nhdplusv2 attributes that were not affected by network topology were joined to the output network and saved 3 results the reference flow network blodgett 2023 1 has been checked and reviewed using automated and manual methods automated checks ensured that expected relationships between identifiers such as hydrosequence level path and terminal path were correct and that all attributes were fully populated and in their expected range due to the size of the dataset manual checks were not comprehensive but were used to identify some systematic issues specifically with initializing some coastal outlets that were addressed prior to release of the dataset manual checks also cross validated attributes such as total drainage area between nhdplusv2 and this new network due to the addition of previously disconnected areas and alteration of primary vs diverted path fully automated checks of accumulated network attributes were not possible as no one accurate value is available for all feature attributes however some wholistic validation checks were performed for example the only places with more than 10 difference in total drainage area fig 9 were are associated with nearby alteration of network connections fig 8 note that the absence of yellow overlay in fig 9 indicates agreement in total drainage area in addition to automated and manual checks the reference flow network blodgett 2023 1 has been tested through its use as a primary input to the ongoing national reference fabric development fig 1 blodgett and johnson 2023 one of these derived products is a conus wide hydrofabric created for development of the noaa nextgen national water resource modeling framework johnson 2022 for this initial proof of concept a complete 2 month hourly conus scale hydrologic simulation has been executed on this dataset each of these modeling applications has been a rigorous test of the dataset s continuity and quality in the interest of fair data and the internet of water the reference flow network has also been tested through its use as the primary input to the mainstem rivers of the conterminous united states data release blodgett 2023 through development of that data release the reference flow network has been subjected to in depth verification the workflow that created the mainstem rivers of the conterminous united states data release identified matching level paths between the reference flow network and several other hydrographic networks manual inspection e g not automated conducted during dataset development and the fact that strong correspondence was found between many networks is further evidence of the quality of this network 4 discussion while implemented at a national scale this work is also intended to serve the needs of regional and local applications the national model applications this work supports may provide local scale information and multi purpose local modeling capacity regional and local studies may be able to conduct research and assessment activities reusing the network data data integration it enables and or the tool chain used in its creation these three modes of reuse illustrate the value that could be derived from this work 1 adoption of network features and attributes 2 use of the network features as a data integration aid and 3 use of the tool chain developed in support of 1 and 2 on various hydrographic datasets meeting minimum data requirements us federal water resources research assessment and forecasting applications michelsen et al 2016 miller et al 2020 ogden et al 2021 were the primary motivation for this work and the ability for local and regional stakeholders to utilize it for their own context is important but the work also has global significance the tools developed for this work concerning us national hydrographic datasets blodgett and johnson 2022 have also been used with a global network of river features for the national hydrologic model geospatial fabric the global merit basins dataset lin et al 2019 yamazaki et al 2019 was used in the alaska domain the tools for creation of nhdplus network attributes such as level path were also used for a complete global river network web visualization learner 2023 these data are available in mainstem rivers of the world based on merit hydrography and natural earth names blodgett 2022 and could support a global dataset of flow network features as has been done for the continental us additionally the idea of a reference flow network could be extended to include other kinds of reference features like coastal zones and hydrogeology the specificity of the data models and implementation of this work are intentionally limited in scope with an eye on incremental advancement divergences and divergence groups is a topic that was out of scope for this work but inclusion may be required support some applications referred to as diversion groups in the report accompanying the e2nhdplus dataset brakebill et al 2020 this concept recognizes that some sets of diversions represent a group of diverted paths whose flow recombines some distance downstream these groups of diversions result from many physical phenomena but are most commonly found where rivers combine in a complex system of low slope channels in contrast to hydrologic diversions where a diversion forms the mainstem of its own drainage basin the combination of channels that make up a divergence group can be treated as a single flowpath relative to overall network connectivity representing the distinction between groups of divergent paths that recombine and more significant hydrologic diversions is challenging due to many factors and is an important topic for future efforts of this overall body of work a hydrologic flow network as presented in this report is a dendritic tree represented as a directed acyclic graph in reality each element of the network is at some time a flowing body of water representation of flowing waterbodies as linestrings as has been presented here is common and useful but not complete by any means visually a line representation of a river has some nominal width that depending on map scale represents some real world waterbody width the datasets discussed here do not include a physically based attribute to tie to this nominal line width taking this one step further a given body of water has some depth which could be expressed simply as a representative depth or more precisely with a collection of cross sections or other physical representations this problem of waterbody integration with flow network representation is confounded by large waterbodies that especially when flooded inundate parts of the hydrologic landscape and associated flow network the joint channel as container and waterbody as contained fluid body concepts presented in hy features provide a framework for this problem research into how to separate the topo bathymetric channel and hydrodynamic waterbody representation may allow necessary integrations of waterbody in channel and river corridor models 5 conclusions the reference flow network is built on nhdplusv2 mckay et al 2015 an improved version of nhdplusv1 which itself was based on best available inputs and was the product of significant quality control bondelid et al 2010 additional network improvements from the e2nhdplus dataset brakebill et al 2020 and the nwm hydrofabric national oceanic and atmospheric administration 2021 were then applied in an open source reproducible workflow national scope integration and network manipulation work have successfully used this network with as good or better results than previous efforts johnson 2022 combined these advances justify application of the reference flow network in local to national scale hydrologic modeling applications however representation of hydrologic features is imperfect with numerous sources of ambiguity and inaccuracy it is because of this that a second equally critical part of this work is an automated and rigorously tested workflow that generates the attributes based on changes in topology blodgett and johnson 2022 it should be expected that while rare there will be cases where the network connectivity in the reference flow network does not match reality some will be instances where the scale at which these data resolve features is too coarse others will be cases where on the ground conditions are misrepresented or have changed given this updates should be expected when issues are encountered however the design of this dataset and system surrounding it is intended for this and capable of rapidly integrating changes to serve the needs of the hydroscience community software and data availability all software and data used in preparation of this report can be found at blodgett d l 2022 mainstems workflow hu12 nhdplusv2 nhdplus hires matching https doi org 10 5066 p9h0ptrh blodgett d l johnson j m 2022 nhdplustools tools for accessing and working with the nhdplus https doi org 10 5066 p97as8jd blodgett d l 2023 updated conus river network attributes based on the e2nhdplusv2 and nwmv2 1 networks ver 2 0 february 2023 u s geological survey data release https doi org 10 5066 p976xcvt declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements all u s geological survey work documented in this report was funded by the u s geological survey water mission area water availability and use science program national oceanic and atmospheric administration national weather service contributions are part of the next generation water modeling framework initiative of the office of water prediction any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25398,many wildfire behavior modeling studies have focused on fires during extreme conditions where the dominant processes are resolved and smaller scale variations have less influence on fire behavior as such wildfire behavior models typically perform well for these cases however they can struggle in marginal conditions e g low intensity fire as small scale variations significantly influence fire physics at scales below grid resolution in an effort to generalize wildfire behavior models and improve their overall performance we have developed a new set of equations for wet and dry fuel to capture the finer scale sub grid variations in temperature and moisture we explore the behavior of these equations in simple scenarios ranging from high to low intensity fire furthermore we evaluate the performance against observations of surface fire in all cases the proposed model performs well after peak temperature is reached however the rise of fuel temperature at the onset of combustion is faster than expected keywords multiphysics modeling wildfire subgrid closure data availability the authors do not have permission to share data 1 introduction wildfire behavior and spread is influenced by many complex processes such as the exchange of mass momentum and energy through non linear convective and radiative heating and cooling combustion and turbulence linn 1997 mell et al 2007 linn et al 2002 mell et al 2009 accary et al 2014 these interactions depend heavily on the dynamic and heterogeneous turbulent flow fields that connect fire to its environment including the surrounding atmosphere fuels dead and live vegetation and topography current physics based fire behavior models leverage computational fluid dynamics cfd techniques to represent the coupled fire atmosphere interaction using partial differential equations linn 1997 mell et al 2007 morvan et al 2018 an overview of fire models can be found in sullivan 2009 since fires both wildfires and prescribed occur at large spatial scales hundreds of meters to hundreds of kilometers and involve complex non linear processes occurring at a wide variety of scales e g atmospheric eddies hundreds of meters wide and reactions occurring in millimeter diameter conifer needles it is not currently feasible to resolve important phenomena over all these scales thus in order to simulate wildland fires at landscape scales 100s of meters to kilometers or larger compromises on resolution must be made to offset computational limitations in large study areas linn et al 2002 linn and cunningham 2005a as cell sizes increase physics based models are unable to resolve fine scale sub grid processes and variations that have non negligible influences thus parameterizations must be developed to capture the net or aggregated effects of sub grid phenomena this need is analogous to the development of turbulence closure models that capture the net effects of variations in a flow field that cannot be resolved recent cfd based wildfire studies have focused on understanding the behavior of high intensity wildfire scenarios e g hoffman et al 2015 2016 marshall et al 2020 banerjee et al 2020 banerjee 2020 frangieh et al 2018 these cases pose significant risks to people communities and infrastructure and are the most challenging to manage and suppress martell 2001 the characteristic length scales of dominant fire phenomena e g flame length and fire geometry e g fireline depth typically increases with intensity of the fire byram 1959 simultaneously the sensitivity of fires to fine scale variations in the ambient environment e g turbulence or fuel heterogeneity and the impact of fine scale variations in fire conditions e g temperature or moisture variations decreases with the increase in the characteristic length scales when environmental conditions are conducive to extreme fire i e hot dry and windy potter and mcevoy 2021 and fires are intense existing physics based wildfire behavior models are typically able to appropriately capture fire spread and fire intensity e g hoffman et al 2016 since the dominating processes can generally be resolved and the impacts of sub grid heterogeneities in temperature wind turbulence fuel and moisture are less significant an example of fire and fuel within a single computational cell for high intensity fire is shown in fig 1a however during lower intensity fires the spatial scales of fire behavior are smaller and the importance of finer scale variations in fire activity and fire environment is greater than during high intensity fire jonko et al 2021 linn et al 2021 parsons et al 2011 zhou et al 2005 as the relevant length scales e g flame length of the fire decrease to below model resolution physical wildfire behavior models struggle as a greater fraction of the critical fire behavior determining processes cannot be explicitly captured and must be captured by subgrid models linn 1997 veynante and vervisch 2002 im et al 1997 for example there can be significant heterogeneity in temperatures of both gases and solids within unresolved volumes under these marginal conditions hotter and cooler regions with respect to the mean resolved temperature as is illustrated in fig 1b capturing the influences of this heterogeneity is important as it directly relates to the drying and combustion processes and thus the spread of fires with increasing desire to consider the ecological effects of fires the role of flanking and backing fires and the use of prescribed fires the importance of representing low intensity fires is increasing physics based coupled fire atmosphere models such as firetec linn 1997 linn et al 2002 use a series of coupled partial differential equations to track the evolution of mass momentum energy turbulence and species of gases moving around a fire and the mass moisture content and temperature of the fuel dead and live vegetation in firetec as well as other similar models such as the wildland fire dynamics simulator wfds mell et al 2007 2009 the solution of these coupled partial differential equations is done numerically on a three dimensional grid typical grid sizes for simulations with kilometer scale domains is on the order of meters for both firetec and wfds the temperature and moisture variations that govern some ignition and drying processes can thus be explicitly resolved on meter scales but even at sub meter scales there can be distributions of temperatures and moisture contents currently firetec employs the notion of a probability distribution function pdf of temperatures within each grid cell to determine the moisture evaporation and combustion rates using this approach firetec avoids having a step function in the rate of evaporation or combustion associated with the mean temperatures reaching critical values such as evaporation and combustion temperatures instead by accounting for the existence of a distribution of temperatures allows for a crude representation of the fact that a small fraction of the fuel might be hot enough to evaporate water or begin to combust this approach has shown promise for some high intensity fire scenarios based on model agreement with observations however the current formulation which assumes a static subgrid temperature distribution shape still presents challenges for low intensity fire due to the significance of the sub grid temperature variations which is greater than in high intensity fire scenarios for example during high intensity fire which is characterized by highly turbulent flow and mixing clements et al 2008 seto et al 2013 clements and seto 2015 clark et al 2020 and flame lengths much larger than a single cell the entire control volume computational cell would be relatively well mixed after the flame front has moved through the cell and combustion will occur in most if not all the fuel in these cases the cell size is small relative to the size of the flames and fireline depth conversely during low intensity fire where flame lengths are much smaller than the cell and mixing is much lower there can be regions of cool unburned fuel regions of hot burning fuels and regions of burned cooling fuel desai et al 2021 as the importance of fine scale variations increases with decreasing physical scales of the fires lower intensity the need for a more dynamic and scenario dependent representation of the distributions of sub grid variations increases the current firetec formulation explicitly tracks a mean solid fuel temperature and allows an assumed temperature variance the combination is used to determine how much of the fuel is hot enough to evaporate water and to begin combusting as the temperature of the mean fuel increases for example by including the notion of the distribution of temperatures within a cell some hot locations and some that might be still at ambient water can start being evaporated in some hot regions while the temperature is still at ambient levels in other parts of the cell what is missing in the current original formulation is scenario dependent methodology for determining the width of the distribution which will be determined by environmental factors such as wind speed gas temperature variation and even initial moisture content levels furthermore the current formulation does not distinguish between wet fuels and dry fuels individually but instead tracks only a single fuel mean fuel with a dynamic average moisture content in reality simultaneous processes could occur within a control volume in which water evaporates from wet fuel to produce dry fuels while at the same time dry fuels may be subject to pyrolysis and combustion in lower intensity fire regimes where the coincidence of drying and combustion occurring in the same cell increases it is challenging to represent the influences of temperature distributions with a single energy equation for the solid fuel in an effort to improve the representation of subgrid processes related to temperature and moisture variation within a computational cell we are working as part of a larger research initiative toward the implementation of a new set of equations describing the evolution of the variation of gas temperatures and both wet and dry solid temperatures particularly in this manuscript however we develop and propose a new set of equations that incorporate the conservation of mass and energy for the dry and wet fuels individually to ensure the model can better represent sub grid combustion and evaporation processes simultaneously this sets the first step towards completing our larger research initiative to this end the current manuscript is structured such that we outline the derivation of these new equations in section 2 followed by a description of the proof of concept simulations in section 3 we then present the results from a set of simple scenarios in section 4 main conclusions are presented in section 5 2 model formulation 2 1 equations for temperature of wet and dry fuel critical processes governing the spread of wildland fire depend on the temperature of the combustible material in most natural scenarios the fuel is initially a combination of combustible biomass and water the amount of water in the fuel is expressed as the fuel moisture fraction which we define in the context of this manuscript as the mass of water divided by the mass of the oven dried combustible biomass or moisture content which is the moisture fraction written as a percent this fuel moisture influences the temperature dynamics of combustible material because it adds significant thermal mass to the fuel and adds an energy sink associated with the evaporation of water the initial water content of live fuels depends largely on plant physiological processes which respond to a variety of environmental factors such as seasonality and available water in the ground jolly et al 2014 the ambient water content of the dead fuel is a dynamic quantity that depends on the history of the local humidity and the size and shape of the biomass fine fuels equilibrate with the atmospheric moisture levels rapidly 1 h whereas thicker fuels take 10s 100s or even thousands of hours to equilibrate van der kamp et al 2017 since wildland fire spread is heavily dependent on the fine fuels such as foliage grasses or twigs our attention is currently focused on the dynamic conditions of this subset of the biomass but future work will be needed to include larger fuels in this formulation as they are important for determining fire emissions fire effects and the sustaining of ignitions an equation for the evolution of the temperature of the solid combustible fuel and associated water content at a single point can be achieved by beginning with the equation for the conservation of internal energy of the solid linn 1997 then we subtract the conservation of mass equation multiplied by the temperature of the solid t s and specific heat c p s which is a mass weighted specific heat capacity of the combined wood c p f 2500 j kg 1 k 1 and water c p w a t e r 4200 j kg 1 k 1 for this purpose the temperature of the biomass and water content are assumed to be the same value since the fuels are thermally and moisture thin equations of this sort are currently used in physics based wildland fire models such as firetec as is shown in eq 1 linn et al 2002 linn and cunningham 2005b in this equation the bulk density of the solid  s is treated as the sum of the bulk density of water  h 2 o and fuel  f the combined internal energy of the fuel and water is the product of  s t s and the mass weighted specific heat c p s 1 c p s  s t s t q h a v t g t s f f h f  c p f t c r i t n f f h 2 o h h 2 o c p h 2 o t v a p in eq 1 t is time q is the net radiation heat transfer rate per unit volume h is the convective heat transfer coefficient a v is the area per unit volume of the fuel in a cell t g is the local temperature of the gas f f is the mixing limited reaction rate as described by linn 1997 h f is the heat of combustion of the fuel  is the fraction of the reaction energy that is deposited directly back to the fuel t c r i t is a critical local temperature for pyrolysis taken to be 600 k in this manuscript which is consistent with the critical temperature currently used in firetec n f is a stoichiometric coefficient for the net burning reaction as described by drysdale 1985 f h 2 o is the rate of water evaporation h h 2 o is the heat of vaporization of liquid water and t v a p is the temperature of water vaporization at standard pressure for a single location f h 2 o only has a non zero value when the temperature of that location is t v a p and above this temperature the moisture content is zero f f begins is non zero at the critical temperature but it can persist at higher temperatures since combustion depends on other factors such as oxygen concentration and mixing there are challenges associated with applying this equation in numerical simulations where computations are not performed at every single point but are instead used to describe the mean temperature in a control volume or computational cell in the case of models designed for landscape scale fires length scales in these control volumes can be on the order of meters in such volume sizes there can be a mixture of states e g distribution of temperatures and a mixture of fuels that are wet and have begun drying that are not explicitly resolved linn 1997 for example it is possible to have fuel that is wet drying dried and heating up and combusting all in the same computational cell especially in the context of low intensity fires in an attempt to account for the fact that there was likely a range of temperatures within the control volume the concept of temperature distribution or pdf was introduced linn 1997 in the reaction rate and later to the moisture evaporation this concept which captures the presence of both warm and cold fuel within a volume allowed moisture to begin to evaporate before the mean temperature reached vaporization temperature and for combustion to start even before all of the moisture was evaporated in the original implementation of firetec linn 1997 linn et al 2002 however there was no mechanism that allowed for the pdf width or shape to dynamically evolve based on the fire environment i e the distribution is fixed and does not evolve and adapt this presents challenges to the generality of the model and its applicability to portions of low intensity fire since it cannot be assumed that the distribution is universal in all fire scenarios in order to increase the generality flexibility or range of scenarios that are applicable of this approach it is necessary to incorporate a dynamic pdf that adapts to the fire environment during the development of the original formulations linn 1997 and subsequent model improvement efforts attempts to derive the governing equations for the dynamic width or standard deviation of the pdf for the solid temperature distribution ran into challenges related to the phase change threshold associated with moisture evaporation the equations for a variance of the temperature distribution were tractable for temperatures above and below the temperature of evaporation in this original formulation the amount of moisture associated with the fuel was tracked and one could back out the fraction of the solid that was wet and dry but it was not possible to track the temperature of the wet and dry components separately and thus it was difficult to estimate the fraction of the fuel that was hot enough to combust and the fraction that still had moisture to evaporate thus in order to better track the distribution of temperatures the evaporation of water and the potentially simultaneous combustion processes that occur within a control volume we propose a new alternative approach to capture the evolution of wet and dry solids for this newly proposed approach we partition the solid fuel into two categories or states wet and dry the wet fuel is the biomass that has the initial moisture content determined by the ambient conditions pre fire and dry fuel is the biomass that has had the moisture driven off as a result of heating by the fire using this construct the bulk density of the fuel moisture mixture  s is the sum of dry fuel  d and wet fuel  w and water  h 2 o this is illustrated in eq 2 where wet fuel and water can be combined to obtain the bulk density of the wet solid that is  s w  w  h 2 o 2  s  d  w  h 2 o  d  s w the conceptual division between the wet and dry fuel is based on the notion that fuel is initially wet and then it is dried to generate dry fuel we assume that all fuel within a control volume has the same initial moisture fraction or ratio r m o i s t r m o i s t 0 which remains constant for the wet fuel while it is possible to have multiple fuels with differing moisture contents co existing within a single cell for simplicity we adopt only one representative fuel with a single moisture fraction additionally the moisture fraction of the wet fuel remains constant since the water evaporation in a fuel particle results in the creation of dry fuel and consistent removal of wet fuel thus we are tracking moisture in the cell through the shifting mass balance of wet fuels which remain at the initial moisture level and dry fuels which always have zero moisture the total moisture fraction of the cell mass weighted average moisture combining wet and dry fuel components will evolve and decrease with evaporation furthermore there is initially no dry fuel since even dead fuels e g dry needles and dried grass have a non zero equilibrium moisture content with the atmospheric humidity even in extremely dry conditions dead fuel moistures are typically above 0 03 or higher however moistures can be as low as 0 01 or lower aguado et al 2007 thus the bulk density of the wet solid is the same as the bulk density of the solid  s w  s at the initialization with this in mind the conservation of mass equations for the water and the wet fuel based on the rate of evaporation of water per unit volume f h 2 o are 3  h 2 o t f h 2 o 4  w t f h 2 o r m o i s t w where the moisture fraction of the wet solid is given by 5 r m o i s t w  h 2 o  w the term r m o i s t w must be greater than zero since the wet solid fuel will never be completely dry but r m o i s t w can be larger than one which simply indicates that the mass of the water is larger than the mass of the biomass for example a live deciduous leaf could have a moisture fraction of 2 meaning the mass of water contained in the leaf is twice that of the dry mass we assume that  h 2 o approaches zero at a rate proportional to the rate that wet mass becomes dry thus r m o i s t w remains a constant even when water is completely evaporated we can then form the conservation of mass of the wet solid and its moisture  s w 6  s w t f h 2 o 1 1 r m o i s t w where mass loss from the wet fuel conservation equation corresponds to a mass source in the conservation of dry fuel equation 7  d t f f n f f h 2 o r m o i s t w the additional sink term in eq 7 f f n f corresponds to the mass loss rate due to the burning of dry fuel using the definition of the two different categories of solids we can then write conservation equations for the internal energy of both wet and dry solids respectively 8 c p s w  s w t w t q w h a v s w t g t w f f h f  w f h 2 o c p h 2 o c p f r m o i s t w t v a p f h 2 o h h 2 o and 9 c p f  d t d t q d h a v d t g t d f f h f  d f h 2 o c p f r m o i s t w t v a p f f n f c p f t c o m b u s t where c p s w  s w  h 2 o c p h 2 o c p f  w c p s w is the mass weighted specific heat capacity of the wet fuel t d is the dry fuel temperature t w is the wet fuel temperature t g is the gas phase temperature  w and  d are the fraction of combustion energy that is directly deposited on the wet and dry solid respectively t c o m b u s t is the temperature at which combustion occurs and solid fuel mass is converted to gaseous products and h is the convective heat transfer coefficient which is assumed to be the same for wet and dry materials given they are the same shape and size a v s w and a v d are the area per unit volume of the wet and dry fuel which are calculated based on their respective bulk density  x the material density  m i c r o and size scale radius of cylindrical fuel particles s s of the fuel particles 10 a v x 2  x s s  m i c r o where the subscript x indicates either wet fuel abbreviated s w or dry fuel abbreviated d bulk density in both eqs 8 and 9 the first terms on the right side of the equation are net radiation the second terms are convective heat transfer and the third terms are deposition of energy deposited via the combustion processes the fourth terms on the right side of these equations represent the transfer of energy associated with the movement of mass from the wet state to the dry state and the loss of the mass of the water from the wet state the final terms in the wet internal energy equation eq 8 represents the endothermic evaporation process it is important to note that the evaporation rate goes to zero as the density of wet fuel goes to zero the final term in the dry internal energy equation eq 9 accounts for energy loss from the dry state due to the mass losses associated with gaseous products in the combustion process since the wet fuel temperatures are expected to change less than 100 k and the moisture fraction of this fuel is held fixed at the initial level we treat the specific heat of the wet fuel as constant throughout the simulation thus we can subtract the product of the specific heat the temperature of wet fuel and the conservation of wet mass equation eq 6 that is 11 c p s w t w  s w t f h 2 o c p h 2 o c p f r m o i s t w t w from eq 8 to arrive at 12 c p s w  s w t w t q w h a v s w t g t w f f h f  w f h 2 o h h 2 o f h 2 o c p h 2 o c p f r m o i s t w t w  w e t t v a p in eq 12  w e t defined in eq 13 has been added to account for the fact that water evaporation only occurs when t w t v a p which means that f h 2 o 0 when the wet solid temperature is t v a p thus the final f h 2 o term will cancel since we assume there is no evaporation when the temperature is below t v a p and when any portion of the wet fuel reaches t v a p the water is immediately evaporated assuming standard pressure the newly dried fuel is now tracked in the dry fuel equation 13  w e t 0 t w t v a p 1 t w t v a p for the dry fuels we similarly subtract the term c p f t d  d t defined as 14 c p f t d  d t c p f t d f f n f c p f t d f h 2 o r m o i s t w from eq 9 thus we arrive at 15 c p f  d t d t q d h a v d t g t d f f h f  d f h 2 o r m o i s t w c p f t v a p t d f f n f c p f t d  d r y t c o m b u s t analogous to  w e t in the wet temperature equation  d r y is added that is 16  d r y 0 t d t c o m b u s t 1 t d t c o m b u s t thus the last two terms in eq 15 now cancel each other it is worth noting that t c o m b u s t is not a fixed value but it is the temperature at which combustion is taking place at any given location or instant the concept of wet and dry fuel for this formulation is associated with wet and dry fuel particles or even wet and dry sections of length of fuel particles in the case of thin fuels which is our focus here in future formulations this can be expanded to handle gradients of moisture within thicker fuels as the outer shell can be dried while the inner core might still be wet for the current case however we only considered the case of thermally thin and moisture thin fuels which constitutes homogeneous temperatures and moisture fraction throughout the fuel particle thickness in development of a new model it is reasonable to start with the more simplified but still realistic conditions such as the one considered here the areas per volume of the wet and dry mass a v s w and a v d can be related to an aggregate area per volume that is if we assume that the difference in wet versus dry fuel is predominantly due to position in the cell i e one spot has been dried out while another spot is still wet 17 a v s w a v  s w  d  s w and 18 a v d a v  d  d  s w this follows a reasonable observation that heating will begin on just one side of the cell as wind and turbulence initially advect and mix hot gases into the cell from one direction the fraction of combustion energy returning directly to the solid  s can be split into a fraction that is deposited directly on the dry and wet fuels in such a spatially segregated paradigm however we are in principle assuming that the energy returning to the solid is predominantly to the dry solid where combustion is occurring thus we approximate these terms as  d  s and  w 0 2 2 mean temperature equations to derive the equation for the mean temperatures of the wet and dry solids we decompose the quantities of eqs 12 and 15 into mean denoted by an overbar and fluctuating parts denoted with then we take the ensemble average of these equations similar to the development of mean velocity equations in turbulence modeling daly and harlow 1970 thus we get the following two expressions 19 c p f  d t d t  d t d t q d h a v d t g t d h a v d t g a v d t d a v d h t g h t d h a v d t g t d h a v d t g h a v d t d f f h f  d f h 2 o c p f r m o i s t w t v a p t d c p f r m o i s t w f h 2 o t d 20 c p s w  s w t w t  s w t w t q w h a v s w t g t w h a v s w t g a v s w t w a v s w h t g h t w h a v s w t g t w h a v s w t g h a v s w t d f f h f  w f h 2 o h h 2 o if we consider the solid density distribution or density variation to be somewhat dominated by the presence of or the lack of a fuel particle at a specific location and thus bimodal then we can neglect t d  d terms this implies that both positive and negative dry fuel temperature fluctuations exist where there is fuel positive density fluctuation and neither are relevant in locations where there is no dry fuel we extend this argument to wet fuels as well additionally if we assume that fuel particles do not change their radius while they burn but rather shrink in their length and local density until they disappear then the correlations between a v d and a v s w can be neglected this approximation is consistent with previous work in which the size scale of particles does not change with combustion for example linn 1997 and linn et al 2002 thus the correlations between a v d and temperature are minimal even if the radius was changing it is also reasonable to assume changes in the heat transfer coefficient are not due to changing solid temperature but rather due to properties of the air i e temperature or velocity since the heat transfer coefficient itself is a function of air properties see eq 23 as a first approximation therefore we assign h t d 0 and h t w 0 finally we neglect third and higher order correlation terms for this initial approach at the dynamic pdf for simplicity with the above simplifications we are left with the following expressions 21 c p f  d t d t q d h a v d t g t d a v d h t g f f h f  d f h 2 o c p f r m o i s t w t v a p t d c p f r m o i s t w f h 2 o t d 22 c p s w  s w t w t q w h a v s w t g t w a v s w h t g f f h f  w f h 2 o h h 2 o these expressions are not in a closed form due to two terms h t g and f h 2 o t d we now try to address this closure problem beginning with the covariance between the heat transfer coefficient and the gas temperature we first consider the equation for the heat transfer coefficient used in firetec that is 23 h 0 25 0 683 r e 0 466  a i r s s as was described by incropera and dewitt 1996 for forced convection over tubes with the local reynolds number defined as 24 r e u s s  in these equations  a i r is the thermal conductivity of air u is the local velocity including contributions from mean and fluctuating components and  is kinematic viscosity we utilize a simple linear relationship with gas temperature for both viscosity and conductivity such that  c 1 t g c 2 and  a i r c 3 t g c 4 where c 1 1 66 1 0 7 c 2 3 37 1 0 5 c 3 5 55 1 0 5 and c 4 9 59 1 0 3 incorporating these new equations into eq 23 we arrive at 25 h 0 25 0 683 u s s c 1 t g c 2 0 466 c 3 t g c 4 s s formulating the convective heat transfer coefficient as a function of t g allows us to determine h t g and estimate the correlation h t g using 26 h t g h t g t g t g based on the approximated equation for h we find 27 h t g 0 25 0 683 u 0 466 s s 0 534 1 c 1 t g c 2 0 466 c 3 0 466 c 1 c 1 t g c 2 1 466 c 3 t g c 4 the correlation between the fluctuations in water evaporation rate and the temperature of the dry fuel that is f h 2 o t d is nonzero in the case where the mean dry temperature is above the temperature for vaporization and there is still additional mass being moved from wet to dry fuel state in this scenario a positive fluctuation in the evaporation as the mean temperature of the wet fuel will always be less than the evaporation temperature results in additional mass source for the dry fuel at a temperature lower than the mean dry temperature a negative temperature fluctuation if the evaporation rate is non zero but the mean dry temperature has fallen below the temperature for vaporization we expect a positive fluctuation in the evaporation rate to coincide with a positive fluctuation in the dry temperature as we are adding dry fuel at a temperature that is higher than the current mean dry temperature the correlation is expected to increase with the width of the distribution and scale with the mean evaporation rate thus we model this correlation as 28 f h 2 o t d f h 2 o t d t d t d t v a p t d 2 3 equations for temperature variance evolution of the width of the probability density functions for the temperatures of the wet and dry solids within a volume is tracked based on a similar approach as the development of turbulence transport equations daly and harlow 1970 similar to velocities and pressure in turbulence modeling the quantities of eq 15 are decomposed into mean and fluctuating parts then both sides of the equation are multiplied by the fluctuation of the temperature followed by ensemble averaging of the entire equation starting with the dry temperature equation this process results in the following expression 29 c p f  d t d t d t t d  d t d t t d  d t d t t d q d h a v d t d t g t d t d h t d a v d t g t d h a v d t g t d a v d t d t d a v d t d h t g t d h t g t d h t d t d t d h t d h a v d t g t d t d h a v d t g t d h a v d t d t d f f h f  d f h 2 o c p f r m o i s t w t d t d c p f r m o i s t w f h 2 o t d t v a p t d c p f r m o i s t w f h 2 o t d t d rearranging eq 29 and applying the assumptions described above for the mean temperature equations we arrive at an equation for the variance t d t d 30 t d t d t 2 c p f  d q d t d h a v d t d t g t d t d t d f f h f  d c p f r m o i s t w f h 2 o t d t v a p t d c p f r m o i s t w f h 2 o t d t d here we have three unknown terms q d t d t d t g and t d f f that we need to address to close eq 30 the correlation between the evaporation rate and the dry temperature f h 2 o t d was discussed in the previous subsection for the correlation between radiation and dry temperature q d t d we assume the net heat transfer will depend only on the energy emitted from the dry fuel this is a simplification but lab studies have shown cohen and finney 2022a b that in thermally thin fuels convective heating plays a larger role in pre heating the fuels thus we omit radiative energy gains at this stage in development leveraging approaches used in variable density turbulence besnard et al 1992 and the stefan boltzmann law for blackbody radiation emission q e m i t t e d a v   t 4 incropera and dewitt 1996 we can approximate the dependence of a perturbation of the emitted radiation on the temperature of the dry fuel with q e m i t t e d 4 a v   t 3 t in these equations  is the stefan boltzmann constant  is emissivity and we include an added view factor correction  for scenarios where fuels are compacted in the bottom of a computational cell as is the case for litter or grass layer the value of  is 1 when the fuel is distributed throughout the cell this view factor can be thought of as a correction on the area per unit volume since emitted radiation is a net negative radiation contribution this term will act as a sink from the t d t d equation furthermore neglecting a terms due to the assumed constant radius of the fuel particle we arrive at 31 q t d 4 a v d    t d 3 t d t d the correlation between the temperature of the dry fuel and the reaction rate t d f f is important in cases where the mean temperature is below the combustion temperature but the combustion rate might be nonzero such as in instants or locations in which there is a positive temperature fluctuation reaching above the mean temperature to this effect positive temperature fluctuations are associated with positive fluctuations in the reaction rate and contribute to the increase in the variance of dry temperature t d t d similarly when the mean temperature is greater than the minimum combustion temperature i e the reaction is occurring in more than half the available fuel negative temperature fluctuations or cooler temperatures in the distribution can represent locations and instances at which the localized temperature is not sufficient to support the reaction i e it is too cold to react thus there is a negative fluctuation in the reaction rate however there are other contributions to the variations in reactions such as localized mixing and oxygen concentrations with increasing fraction of the volume above the critical temperature although the correlation t d f f is expected to be positive it will decrease with decreasing fraction of the temperature distribution above the critical temperature this is conceptually shown in fig 2 here we expect the correlation to be closer to zero when r c o m b u s t which is the fraction of dry fuel that is hot enough to react is large however we anticipate the strongest correlation when less fuel is above the critical temperature r c o m b u s t is small considering this we approximate the correlation as 32 t d f f c t d f f f t d t d 1 r c o m b u s t where c t d f is taken to be 0 7 this constant must be empirically determined and here we estimate it to be 0 7 using the little fire data we have available however we will return to this constant in future studies the final unclosed term in eq 30 t d t g is the correlation between fluctuations in dry fuel temperature and gas temperatures the correlation between these temperatures are largely related to the rate of convective heat transfer which lower the temperature difference between adjacent gases and solids when the convective heat transfer rate is high perturbations in dry solid temperature are strongly related to the gas temperature perturbations since t d t g is symmetric in t d and t g so should be the modeled term this covariance is expected to increase depending on the magnitude of the variance of both the dry fuel and gas temperature due to its symmetry as well as the strength of the convective heat transfer as the convective heat transfer coefficient increases there will be greater energy transfer between the gas and solid phases and thus a stronger correlation between the fluctuations to this end we propose the following expression for the dry fuel and gas temperature covariance 33 h a v d t d t g h a v d 1 e c h h h n o r m t g t g t d t d 2 in this equation h n o r m is the normalization constant for the convective heat transfer coefficient which is the approximate background value before ignition or heating in the current study h n o r m is assigned the value of 25 0 w m 2 k 1 which is representative of the ambient value of h in firetec before fire and during low wind conditions and c h 5 the constant c h was determined by comparing modeled fire behavior over a variety of values ranging from 1 to 5 not shown for simple scenarios this constant must be determined using empirical methods and we estimate it using the little fire data we have available however we will return to this and other constants in future work we now apply the same decomposition and derivation to the instantaneous wet solid temperature equation eq 12 thus taking the ensemble average of the expression yields 34 c p s w  s w t w t w t t w  s w t w t t w  s w t w t t w q w h a v s w t w t g t w t w h t w a v s w t g t w a v s w t g h t w a v s w t w t w a v s w t w a v s w t w h t g t w h t g a v s w t w h t w t w t w h a v s w h t w t g t w t w h a v s w t g t w h a v s w t w f f t w h f  w f h 2 o t w h h 2 o now applying analogous simplifications that were discussed previously for the dry fuel equation returns 35 t w t w t 2 c p s w  s w t w q w h a v w t w t g t w t w f f t w h f  w f h 2 o t w h h 2 o since we assume that the fraction of energy from the combustion process deposited on the wet fuel  w is negligible f f t w h f  w will have no contribution combustion is not occurring in the wet fuels and so the local retention of heat at the site of the reaction is not on the wet fuels thus there should be little to no energy from the combustion process directly contributing to wet fuel temperature changes this assumption does not preclude the reaction from heating the wet fuels in this proposed version the reactions can heat the wet fuels because elevated temperature dry fuels heat the gases and the gases convectively heat the wet fuels ultimately when this model is fully implemented in firetec the wet fuels will also be heated by the dry fuels directly via radiation heat transfer but this mechanism is not in the current formulation thus the only term left to complete the closure is the covariance of wet fuel temperature and evaporation rate f h 2 o t w using the radiation and convection heat transfer rates we can formulate the rate at which energy is added to the wet material when the energy gained by the wet fuels causes the upper limit of the wet fuel distribution to reach or exceed the vaporization temperature without enforcing the phase change on water or removing the energy of vaporization evaporation of water begins the energy available to evaporate water e w is related to the evaporation rate by f h 2 o e w h h 2 o we define a theoretical ratio for the fraction of wet fuel mass r h 2 o which would have risen above the evaporation temperature if evaporation had not commenced assuming the distribution of temperature is symmetric about the mean for simplicity thus we have 36 r h 2 o t w m a x t v a p 2 c p d f t w t w where c p d f is a constant with a value as the ratio of halfwidth of the temperature distribution to the standard deviation and t w m a x is the maximum temperature in the cell moreover this constant depends on the type of distribution that would be assumed the mean evaporation rate is therefore determined to be 37 t w m i n t w m a x f h 2 o t d t t w m i n t w m a x d t f h 2 o here f h 2 o t is the evaporation rate associated with fuel at any specific temperature t and t w m i n is the minimum temperature in the cell now we define a normalized evaporation rate f as 38 f h 2 o f h 2 o t f h 2 o where 39 f h 2 o 0 t w m i n t t v a p 1 r h 2 o t v a p t t w m a x this yields unity for the mean normalized vaporization rate 40 f h 2 o t w m i n t w m a x f d t t w m i n t w m a x d t t w m i n t v a p 0 d t t v a p t w m a x 1 r h 2 o d t 2 c p d f t w t w 1 decomposing f h 2 o into its mean and fluctuating components reveals 41 f h 2 o f h 2 o f h 2 o 1 f h 2 o using these definitions we can compute f h 2 o t w as 42 f h 2 o t w f h 2 o t w m i n t w m a x f h 2 o t w d t t w m i n t w m a x d t f h 2 o t w m i n t w m a x f h 2 o 1 t w d t 2 c p d f t w t w where the temperature range of the wet solid is 43 t w c p d f t w t w t w t w c p d f t w t w given the definition of f h 2 o this simplifies to 44 f h 2 o t w f h 2 o t v a p t w c p d f t w t w 1 r h 2 o 1 t w d t 2 c p d f t w t w using eq 36 in the lower limit of the integral we arrive at 45 f h 2 o t w f h 2 o 1 r h 2 o 1 c p d f t w t w 2 c p d f t w t w 2 r h 2 o c p d f t w t w 2 4 c p d f t w t w manipulating and rearranging these terms we arrive at 46 f h 2 o t w f h 2 o c p d f t w t w 1 r h 2 o 2 2 4 final equations the following subsection presents a summary of the new model equations with the closure completed for all terms 47 t d t 1 c p f  d q d h a v d t g t d h t g t g t g a v d f f h f  d f h 2 o c p f r m o i s t w t v a p t d f h 2 o c p f r m o i s t w t d t d t d t v a p t d 48 t w t 1 c p s w  s w q w h a v s w t g t w h t g t g t g a v s w f h 2 o h h 2 o 49 t d t d t 2 c p f  d 4 a v d    t d 3 t d t d h a v d t g t g t d t d 2 1 e c h h h n o r m t d t d c t d f f f t d t d 1 r c o m b u s t h f  d f h 2 o c p f r m o i s t w t d t d f h 2 o t d t d t d t d t v a p 2 c p f r m o i s t w and 50 t w t w t 2 c p s w  s w 4 a v s w    t w 3 t w t w h a v s w t g t g t w t w 2 1 e c h h h n o r m t w t w f h 2 o c p d f t w t w 1 r h 2 o 2 h h 2 o where h t g is described in eq 27 3 proof of concept simulations the new approach described here is intended to capture sub grid variability in temperatures and thus increase the generality of wildland fire simulation capabilities this enables properly simulating high and low intensity fires this formulation constitutes the first stage in the development of a multi phase coupled fire atmosphere model which has so far been confined to the solid phase since solids are not moving no advection or spatial diffusion terms we can study the performance in a single 1 m 1 m 1 m control volume where the externally driven conditions are prescribed e g wind speed upstream gas temperature upstream oxygen concentration and initial fuel moisture in order to understand the performance of the proposed models eqs 47 50 we developed a series of idealized tests by varying initial fuel moisture conditions wind speeds and upstream gas temperatures the wet fuels fuels at their initial moisture state are assigned an initial bulk density of 0 5 kg m 3 and moisture fractions of a 0 05 b 1 and c 2 these correspond to fuel conditions similar to a dry dead needles and fine brachwood or matted dead grass b live conifer fine branchwood and needles and c live deciduous fine branchwood and leaves respectively wind speeds have constant values of 0 1 m s 1 1 m s 1 and 2 m s 1 as this is a plausible set of wind speeds very near the ground height below 1 m in the vicinity of a surface fire for a special case we apply a wind speed value of 4 m s 1 which constitutes a high intensity fire in these proof of concept simulations we aim to explore a range of plausible scenarios by varying some of the primary driving factors for fire wind moisture and upwind temperatures these factors are important for determining fire intensity and spread for the purpose of this paper we define the range of wildfire intensity as low intensity fires that have depths and flame lengths 10s of centimeters up to intense fires that have burning zones of 10s of meters and flame lengths of similar size even though the focus of these proof of concept simulations is modeling the solid phase temperatures and their variations it is necessary to vary the local gas temperature for the closure of the convective heat transfer terms for this purpose a simplified evolution equation is developed that combines the prescribed velocity and upwind temperature the convective heat transfer coefficient an area per unit volume and a grossly simplified radiation energy sink this expression estimates the gas temperature in the control volume 51 c v a i r  g t g t q g c v a i r  g u t g x h a v d t d t g h a v s w t w t g f f h f  g here c v a i r is 718 j kg 1 k 1 q g o 2 a m b i e n t o 2 o 2 a m b i e n t  t g 4 which is the radiation loss scaled by the oxygen depletion that relates gas emissivity to combustion products in an oxygen poor environment x is the spatial coordinate and  g is 0 75 75 of reaction energy deposited in gas and 25 in the solid which is the fraction of reaction energy absorbed by the gas phase these values are within range with those observed in firetec and we do not allow them to vary for simplicity this equation for the gas temperature is simply intended to capture the fact that there is feedback between the solid and gas temperatures and the influences of energy in surrounding regions on gas temperatures the variance of gas temperatures is prescribed based on the mean gas temperature using the following equations this first equation 52 t g t g 30 t g 243 59 3 2 is the variance assigned to the low intensity gas scenario which ensures that the minimum gas temperature in the assumed wide distribution does not fall below 300 k the second version of the prescribed gas variance is 53 t g t g 60 t g 300 6 2 which we assigned for the other simulations in subsequent phases of this study related to the development of a multi phase sub grid fire atmosphere model these gas temperature equations are replaced with full transport equations similar to the technique applied for the solid phase in the current study initially the gas temperatures in the control volume in all of the idealized simulation were assumed to be 300 k before upwind gases began to advect into the control volume high intensity fire scenarios were simulated by assigning an upwind mean gas temperature that starts at 300 k and ramps up to 1000 k over 15 s and tapers off following this relation 54 t g u w 300 700 2 0 95 0 95 tanh t 15 50 0 05 and the low intensity fires were simulated with the upwind temperatures rising from 300 k to 500 k over 100 s and tapers off as 55 t g u w 300 200 2 0 95 0 95 tanh t 500 120 0 05 additionally we prescribe an oscillating upwind gas temperature with the following equation 56 t g u w 750 450 sin t 15 2  60 all three upwind mean gas temperature prescriptions for the idealized scenarios are plotted in fig 3 the first two temperature scenarios involving near sustained elevated gas temperatures were designed to reduce the complexity of environmental drivers with the elevated temperature that represent gases advected from upwind with the approaching fire the third gas temperature paradigm was chosen specifically to highlight the response of the model in dynamic environments with oscillating conditions high intensity fire scenarios are unlikely to coincide with light winds and thus we used only the case of wind speed at 4 m s 1 and temperature of 1000 k gas temperatures at or exceeding 1000 k are frequently observed in high intensity crown fire e g taylor et al 2004 the time step for all simulations is 0 001 s and we temporally discretize the differential equations using an explicit forward in time euler method the simulations ran for at least 600 s the size scale of the fuel half of the volume to surface area ratio remains constant and is prescribed as 0 0006 m which corresponds to roughly that of thermally thin grass needles or fine branchwood thermally thin fuel refers to fine fuel particles that have no internal temperature gradients as such an individual fuel particle will warm and cool uniformly non thermally thin fuel particles will be addressed in future efforts which would build on some approaches that have been outlined in this document sub grid turbulent kinetic energy tke is fixed at k a 0 125 u 2 k b 0 005 u 2 and k c 0 2 k b m 2 s 2 where u is the prescribed scalar wind speed firetec incorporates three sub grid tke scales and their associated turbulence energy spectra corresponding roughly to the unresolved scales associated with vegetation structure additional information on sub grid tke in firetec is discussed by linn 1997 for the purposes of this concept demonstration the turbulent length scales are equivalent to the distance between larger vegetation structures e g shrubs branches and needles the radiation loss terms are computed using the mean and variance of the wet and dry fuel temperatures to compute t 4 and blackbody radiation the effective area that radiates energy away from the solid is estimated based on the surface area per unit volume times  which is the ratio of the fuel depth height to cell height a compression factor 57 q  a v   t 4 while radiation ahead of the flame front has a role in preheating fuels radiation alone does not typically induce fire spread in fine wildland fuels that are loosely packed due to attenuation in low density discontinuous fuels and the fact that convective cooling between fine fuel elements counteracts radiative heating as was discussed by finney et al 2015 results from their experiments provide strong evidence that convection is largely responsible for wildland fire spread cohen and finney 2022a b thus in the interest of simplicity we omit the influences of radiative heating in this initial testing phase the radiative heating will be addressed in more detail in subsequent efforts the evaporation rate is calculated by first determining the difference between the energy gained by the wet fuel through convective and radiative heat transfer within one time step and the energy that it takes to raise the temperature distribution to the point where the max temperature reaches 373 k this residual energy is the energy available to evaporate water within the time step and determine the water evaporation rate the theoretical maximum temperature t w m a x that would be reached within a time step without accounting for the phase change provides an estimate for the fraction of fuel r h 2 o that would be above 373 k due to the radiative and convective heating and cooling for this initial effort we use a top hat distribution using this simplified distribution c p d f 3 in eq 36 we recognize that if a normal distribution is applied instead r h 2 o is a form of the error function or cumulative distribution function for a gaussian shape more details on the error function can be found in jeffrey 1995 and we will be exploring this topic in the future separately we apply a similar philosophy to determine what fraction of the fuel in a cell is above the critical temperature for combustion  this fraction is used in the mixing limited reaction rate as described by linn 1997 a simplified model for the advection and consumption of oxygen density is used to represent the depletion and replenishment of oxygen with time the available oxygen is initialized as 21 of the air density 0 21  g a m b i e n t and over time it is calculated explicitly using the following equation 58 o 2 t f f n o x y g e n u o 2 x here n o x y g e n is the stoichiometric coefficient for oxygen in the combustion reaction drysdale 1985 normalized by the total mass of reactants the upwind oxygen value is related to the upwind temperature such that as temperature decreases increases oxygen increases decreases proportionally corresponding to the upwind drop in temperature we assign upwind oxygen such that 59 o 2 u w 0 15 0 09 2 0 95 0 95 tanh t 15 50 0 05 in the high intensity simulations and 60 o 2 u w 0 15 0 09 2 0 95 0 95 tanh t 500 120 0 05 for the low intensity cases lastly the upwind oxygen concentration in the oscillating simulations is 61 o 2 u w 0 18 0 06 sin t 15 2  60 all three scenarios are plotted in fig 4 finally we only apply the equations for temperature of the wet dry fuel equations and their variances when their respective density is greater than 1 1 0 6 kg m 3 4 results and discussion 4 1 proof of concept simulation results we begin with scenarios of high intensity fire placed upwind of the control volume we simulate high upwind temperature with high wind speed for all local moisture scenarios shown in fig 5 and table 1 in these simulations the mean upwind gas temperature is raised from 300 k to 1000 k over 15 s with a slow decline back to ambient temperature it is unlikely that a 1000 k gas temperature would coincide with light winds thus for this high intensity simulation we present the results for only the case of 4 m s 1 wind speed applied to all three fuel moisture scenarios 5 100 and 200 in fig 5 mean wet and dry fuel temperatures are only shown in cases with the density above 1 1 0 6 kg m 3 the blue solid line with circles indicates the mean wet fuel temperature the red line with squares is the mean dry fuel temperature the light gray line is mean gas temperature the solid purple line is the upwind prescribed gas temperature and the dashed blue and dotted red lines are the wet fuel and dry fuel density respectively shading corresponds to one standard deviation above and below the mean temperature value in all moisture fraction scenarios a fraction of wet fuel is quickly dried and dry fuel begins to burn within the first 13 s of all simulations it should be noted that there is an assumption that there are burning ashes etc present such that ignition is piloted at the onset of combustion in the fuel with a moisture fraction of 0 05 99 of the fuel is already dried i e 99 moved from the wet to dry fuel category and thus available for consumption this can be contrasted to 33 and 17 of the fuels that have been dried when combustion starts for the fuels with moisture fractions 1 and 2 respectively as shown in table 1 the maximum temperature is slightly higher in the lower moisture fraction scenarios but the time for the peak temperature differs only by 6 s among these three simulations the similarity between these three cases is indicative of the fact that the strong winds and high temperatures are very significant and they overwhelm the effects of moisture on combustion initiation it is important to remember that the prescribed high intensity conditions are associated with fire upwind of the control volume likely indicating an abundance of dry fuel in that region the similarity of the results relates to the fact that when a fire moves from one set of conditions to a new set of conditions the changes to fire behavior do not occur instantaneously time or space thus when our control volume has high moisture i e moisture ratio or fractions of 1 or 2 the high intensity fire from upstream still ignites the fuel and consumes it but the energy release proportional to the dry mass loss rate is lower in the wetter fuel cases we also note that the temperature begins to decrease after the initial reaction in all scenarios as a combined result of reduced oxygen availability leading to lower reaction rates and convective cooling after 600 s only 0 9 or less of the fuel remains in all simulations which indicates near complete fuel consumption regardless of moisture content this is consistent with observations and empirical models in high intensity wildfire during high intensity crown fire where observed in fire mean gas temperatures exceed 1000 k e g taylor et al 2004 the fraction of fine fuels less than 5 mm diameter that are consumed is high and often approaches 100 thompson et al 2020 forestry canada fire danger group 1992 call and albini 1997 stocks et al 2004 de groot et al 2022 reducing the upwind gas temperature and variance after 100 s provides enough time to examine the fire behavior simulated in the model at constant high intensity for all simulations presented here it is important to remember that holding the wind and upwind gas temperature steady and remaining steady for an extended period of time is unrealistic as any given location in a fire has a transient set of conditions as the fire approaches it and moves past thus we have chosen a simplified set of environmental conditions to expose the solid fuel to but we recognize the importance of coupling this to the full set of dynamic conditions which will be done as this solid model is connected to a similar gas model in the cfd context of firetec before the weight is put on the specifics of model results evaluating the results in fig 6 and table 2 we next examine the behavior of our proposed model for a lower intensity upwind fire scenario and a lower mean upwind gas temperature with a higher variance these conditions are selected to illustrate the behavior of the proposed model during a low intensity head fire impinging on a cell or flanking backing fire behavior where only a fraction of the gas temperature within the cell would be hot enough to initiate combustion in the dry fuel in these scenarios the upwind gas temperature begins at ambient ramping up to 500 k over 100 s and slowly decreasing as shown in figs 3 and 6 the results presented in figs 6a 6c correspond to wind speed 0 1 m s 1 figs 6d 6f to wind speed of 1 0 m s 1 and figs 6g 6i to a wind speed of 2 0 m s 1 furthermore figs 6a 6d and 6g correspond to moisture fraction of 0 05 figs 6b 6e and 6h are moisture fraction of 1 and figs 6c 6f and 6i show results for moisture fraction of 2 in simulations where the wind speed is near calm figs 6a 6c convective heat transfer is not significant enough to warm the fuel and overcome radiative losses which increase rapidly with temperature rise due to their dependence on the fourth power of temperature we remind the reader that we are not simulating radiative gains and thus the primary source of energy before combustion is convection therefore no fraction of the fuel is warm enough to begin combustion in any of the scenarios increasing the wind speed to 1 m s 1 provides enough convective heating to induce combustion when the fuel is dry however the energy sink required to evaporate water from the fuel with a higher moisture content prevents combustion from occurring increasing the wind speed to 2 m s 1 results in all moisture scenarios reaching the threshold for combustion interestingly the fuel consumption in all cases where combustion occurs is 89 or greater this result is somewhat surprising since during a low intensity burn we would expect less than 89 consumption for these idealized cases with constant winds and steady gas temperature prescription however we are not capturing the effects of cool air entrainment this can contribute to residual unburned fuel furthermore the elevated upwind temperatures persist for more than 5 min which is likely unrealistic over a 1 m spatial scale in some of these scenarios arguably a near surface wind speed of 2 m s 1 would produce a higher intensity surface fire than we are prescribing upwind especially for the driest fuel for example the canadian forest fire behavior prediction system forestry canada fire danger group 1992 an empirically based system used by fire management agencies predicts a moderately intense fire with 1 3 m s 1 forward rate of spread in dead matted grass that has a moisture content of 5 if we estimate the 10 m open wind speed to be 5 4 m s 1 using a simple logarithmic wind profile with 2 m s 1 wind at 1 m roughness length z 0 05 m and zero plane displacement d 0 65 m considering the depth of the fireline being at most tens of meters deep and the forward rate of spread the residence time for such a fire would be much less than 5 min cheney and sullivan 2008 wotton et al 2012 in this case in simulations where wet fuel is still being converted to dry fuel during combustion figs 6h and 6i the initiation of burning coincides with a slight decrease in mean wet fuel temperature since the mass of wet fuel drops quickly in response to accelerated evaporation caused by the increase in gas temperature the mean temperature of the wet fuel will fall as the warmest of the wet fuel is converted to dry fuel and the cooler portions of the wet fuel remains furthermore the sharp increase in temperature at the onset of combustion is more rapid than expected for a low intensity scenario in a cell 1 m 1 m and is due to the fact that 1 we do not simulate cool air entrainment and temperature fluctuations from surrounding cells when the upwind gas is elevated and 2 we may not be accurately capturing the oxygen concentration in the control volume this also likely contributes to a larger fraction of fuel being consumed than is expected during marginal conditions these limitations will be corrected with the coupling of the gas equations to the fuel equations in the near future where we capture the influence of adjacent cells i e more realistic advection of gas and oxygen and turbulent variations in fig 7 we show a comparison of results using an isolated cell from the current formulation of firetec and the newly proposed method the simulations presented in fig 7 are for the moderate wind speed and low intensity scenarios shown in fig 6d 6f it is important to note that the proposed method utilizes top hat distributions that shift and dilate based on the equations for the mean and the variance the distribution for the original formulation is a fixed curve similar to a gaussian and thus the interpretation of the predicted distributions must be different both sets of simulations were driven by the same initial and boundary conditions as described in section 3 furthermore both sets of equations are coupled to a gas equation and oxygen equation eqs 51 and 58 this comparison highlights differences between the current method in firetec and the need for a scenario dependent dynamic temperature distribution while dry conditions result in combustion occurring in both the original and new formulations when moisture content is high only the original formulation of the equations produces a combustion reaction furthermore when conditions are driest combustion occurs in less than half the time for the original formulation compared to the proposed formulation which is a result of the fixed temperature distribution this highlights the need for improved methods to capture the subgrid distributions of temperature and moisture which are critical for spread and ignition in low intensity scenarios combustion does not occur in either formulation when winds are near calm not shown here for brevity the main motivation behind this work is to improve the sub grid models for fire behavior simulations through the development of new closure schemes thus we proceed with examining the individual terms and closures in the proposed model specifically looking at the processes in fig 6i this simulation was selected since it highlights the most important processes in the lower intensity upwind scenarios i e slower warming and evaporation moderate wind speeds higher moisture and eventual but delayed combustion hereafter we represent all terms in fig 8 related to radiation contribution as  x all terms related to convection as  x all terms related to the heat of combustion or heat of vaporization as  x and all other terms as  x where x denotes d for the mean dry fuel temperature d v is the mean dry fuel temperature variance w is the mean wet fuel temperature and w v is the mean wet fuel temperature variance in the case where  x or  x have two terms these terms are assigned in the order that they are shown in the governing equations that have been outlined in previous sections these terms are more specifically outlined in table 3 beginning with the solid wet fuel mean temperature and variance as shown in figs 8a and 8b the mean temperature rises and the variance increase in response to increasing gas temperature at the onset of the simulation this is via convection before the onset of evaporation at 18 s once evaporation begins convection source terms  w and  w v and the sinks associated with heat of evaporation  w and  w v increase the mean temperature and reduce the variance respectively as warm wet fuel is being converted to dry fuel regarding the dry temperature results in fig 8c the mean dry fuel temperature falls with the radiative and convective cooling sinks  d and  d which exceed the source of warm dry fuel  d until the gas temperature rises above the mean dry temperature this results in positive convective heat transfer which forces a rise in dry temperature at the same time we observe a rise in the variance fig 8d of the dry fuel since the positive contribution from the convective term  d v outweighs the sink from radiation and the dry fuel mass source terms  d v and  d v respectively fig 8 d the dry variance and mean temperature continue to rise until a fraction of mass in the dry fuel temperature distribution reaches 600 k at approximately 136 s and pyrolysis combustion begins the exothermic reaction source term  d quickly raises the mean temperature of the dry fuel this is while the combined sources  d v and  d v are greater than the combined sinks  d v  d v and  d v 2 resulting in a rapid net increase of the variance as previously discussed for figs 6h and 6i at the onset of combustion we observe energy and mass losses in the wet fuel which are due to rapid heating thus the sudden drop in  w results in a momentary decrease of the mean wet fuel temperature and an increase in the wet fuel variance as warm wet fuel is quickly converted to dry fuel the mass loss due to combustion coincides with depleted oxygen and lowering of the reaction rate  d this combined with convective cooling and mass gain at lower temperatures  d and  d leads to a drop in mean temperature shortly after combustion commences similarly  d v in the variance equation falls as the reaction rate drops this combined with the sinks related to radiation  d v and cooling convective evaporative and mass gain  d v  d v and  d v 2 the variance falls as well at this stage the upwind prescribed gas temperature is decreasing and all source and sink terms are reduced in all equations until both the reaction and evaporation ceases and convection and radiation are the only contributing terms the qualitative examination of the individual terms in the reaction in the simulations with mean upwind gas temperatures ranging from 300 k to 500 k and a high variance provides a reasonable explanation for the observed fire behavior in the conceptual model presented here most of the known processes are represented in this simple scenario and the equations are stable and provide results consistent with expectations although we are unable to directly compare the results of this study with observed in fire data to determine the accuracy of magnitude and relative contributions of the individual terms the net fire behavior follows the expected trends we acknowledge the scenarios presented are unrealistic given constant or steady wind mean upwind gas temperatures and gas variance however we are able to examine the behavior of the model by removing the complexities of variations in gas temperature and wind speed due to turbulence this also enables examining the impact of each of the terms and closures presented here the final set of idealized gas scenarios presented is driven by an oscillating upwind gas temperature which alternates between 300 and 1200 k with a 60 s period as shown in fig 9 and table 4 wind and moisture are the same as outlined in fig 6 in nearly all simulations we observe a step like structure in the density as a response to the fluctuations in gas temperature as gas temperature increases the movement of mass from wet to dry accelerates and the decrease in the rate of dry mass gains coincides with the slowing of the mass transfer moreover this step like structure is evident in the mass decrease of dry fuel as well particularly in the two higher wind speed scenarios in which convective heat transfer and mixing is stronger combustion does not occur in the lowest wind speed simulations even with maximum gas temperatures upwind reach 1200 k as there is not enough mixing furthermore upwind gas temperatures only remain above 600 k for 36 s at a time before falling below 600 k in the oscillation all scenarios where combustion occurs have near complete combustion i e 2 or less fuel remains at the end of the simulation this is not surprising as sufficient mixing and very hot gas temperatures contribute to the reaction consuming fine fuels quickly we observe a rapid rise in dry temperature at the onset of combustion in the higher wind speed simulations figs 9d 9i similar to the gas scenarios discussed earlier for figs 6 and 8 however in the moderate wind speed scenario with a slower fuel response time due to reduced convective heat transfer figs 9d 9f we observe a markedly cooler peak temperature with a slower decline when compared to the high intensity scenarios table 1 and fig 5 where gas temperatures remain high because of this slower response time when combustion finally occurs the upwind temperature is already in the decreasing phase of the oscillation and so the control volume and upwind gas temperature is cooler at the onset of combustion this leads to a slowed acceleration and deceleration of the reaction rate and a lower overall maximum temperature this phase could loosely be compared to the effects of cool air advection from surrounding cells and turbulence or vorticity induced cooling as discussed by finney et al 2015 local temperatures fluctuate rapidly as a result of the effects of turbulence and buoyancy induced circulation which can be periodic in both stream wise and transverse directions since we are not incorporating these effects in the control volume boundary conditions we can expect variation in reaction rate and mean dry fuel temperature when coupling the unsteady gas equation and fuel equations in three dimensional scenarios future work that includes more accurate oxygen advection should further improve the reaction and fuel consumption rates in these simulations the lag and offset between the dry fuel temperature the cell level gas temperature and the upwind gas temperature is most pronounced in the lower wind scenario for all three moisture fractions this is due to the reduced convective energy transfer for solid to gas and gas to solid the variations in wet fuel temperature in response to variation in gas temperature similarly increases with wind speed unlike the low intensity scenario shown in fig 6 we observe a very modest decrease in wet fuel temperature during the onset of combustion instead changes in wet fuel temperature are mainly a result of gas temperature variation in the control volume 4 2 effects of gas temperature variance on dry fuel while we only present the results for a single control volume whose fuel evolution is not directly dependent of cell size although the overly simplified gas temperature and oxygen equation assumes a 1 m upwind distance the improvement of sub grid temperature distribution relaxes the current restraint on cell size in three dimensional formulations of firetec the proposed model for the evolution of sub grid temperature distribution is expected to improve the current static shape of sub grid temperature distribution in firetec while the cell size is not explicitly adjusted in the proposed model we can instead achieve a similar result by prescribing a wide gas temperature distribution this is because a fire approaching a larger cell will result in a wide range of sub grid temperatures given a small fraction of fuel and gas in the large cell would initially warm while the remainder of the cell remains closer to the ambient temperature in the next phases of this work the inclusion of advection and turbulent diffusion as well as influences on local gas temperature distributions are incorporated to account for cell size effects these will feed back to the solid phase equations presented here however we can explore this concept by modifying the prescribed gas temperature distribution shown in fig 10 in these simulations we assign a mean gas temperature of 500 k in the control volume and apply a range of variances for the moisture fraction 1 scenario under all 3 wind speed cases the value indicated by x in  g x represents half the width of the top hat distribution i e 500  g x k is the maximum temperature in the top hat distribution with  g x ranging from 100 to 300 k this figure highlights that increasing width of the gas temperature distribution results in an increase in width of the fuel temperature distribution in all wind cases regardless of the state of combustion furthermore increasing the wind speed and thus convective heat transfer increases dry fuel temperature variance in both combustion and non combustion scenarios this is because stronger convection will result in a stronger influence of the gas temperature on solid temperature currently firetec simulations require significant computational resources on high performance computing systems due in part to restrictions on grid spacing a single simulation in a small domain e g 200 cells 200 cells 41 cells requires tracking over 1 6 million cells and within each of those cells computing or storing upwards of 40 variables at each time step with time steps as small as 0 001 s increasing the cell size has the potential to reduce computing requirements without compromising modeled fire behavior and will be examined in future work 4 3 field experiments and model comparison on the afternoon of november 15 2017 a series of experimental fires were conducted between 1700 and 1800 utc on the university of georgia campus providing data for the evaluation of the overall performance of the proposed model desai et al 2021 for these experiments the burn area was 2 4 2 4 m and the fuel bed consisted of pine litter with a 4 moisture content and a fuel load of 0 37 kg m 2 approximately 10 cm deep the weather station recorded wind speeds of 0 635 and 0 474 m s 1 at 1700 and 1800 utc infrared and visual imagery were obtained using a forward looking infrared sc660 thermal imaging system flir systems inc boston ma usa and a gopro hero3 mounted to a 7 m tall aluminum tripod above the burn area facing downward the flir system has a resolution of 0 8 cm and a focal plane array of 640 480 pixels at the 7 m distance the temperature data was collected at 1 hz for the range of 573 to 1773 k and three different ignition patterns were completed a point source ring source and 2 parallel lines four 1 1 m cells were segregated for each experimental fire and temperature distributions within each cell were analyzed at each second additional details can be found in desai et al 2021 strother 2020 precise descriptions of the gas temperature and associated variance are unknown for these experiments given the lower intensity of the fire we approximate the upwind gas temperature as t g u w 300 500 tanh t 30 100 400 tanh 400 t 1000 which allows for a slow gas temperature rise the gas temperature variance like the idealized scenarios previously discussed is assigned as a function of temperature with a variance t g t g 60 t g 300 6 2 fuel bed temperature is initialized at 292 k and we estimate the ground level winds as u 0 6 m s 1 and tke is assigned the same as described in section 3 which used for the mixing limited reaction rate near ground the completed simulation compared to the observations is illustrated in fig 11 where the dashed lines are the mean temperatures of the various burning observations within the 1 1 m cell the red solid line with squares indicates modeled mean dry temperature the blue solid line with circles is mean wet fuel temperature the gray line is gas temperature and shading indicates one standard deviation above and below the mean based on the dry and wet modeled variances the observed data is only presented for values above 773 k since there are multiple fire observations we align the model data with the observations at peak temperatures furthermore it is important to note that flir temperature observations are from the two dimensional surface layer and do not capture variations in temperature below the surface the proposed model however accounts for three dimensional temperature distributions with that said unsurprisingly the observed fuel temperatures are significantly hotter than the modeled mean temperatures however if we truncate the simulation data from the modeled mean temperature and variance to values in the distribution greater than 773 k we observe a significant improvement in the alignment between modeled and observed temperature especially after the peak temperature as shown by the solid black line distinct differences include a more rapid modeled rise in temperature during the initial combustion phase with a slightly premature drop in temperature as fuel is being consumed and a slower reaction rate as previously discussed in the idealized scenarios we hold the upwind gas temperature nearly constant with gradual increases and decreases thus we are not capturing the turbulent gas temperature variations that would influence fire behavior these limitations are not associated with the final model but instead are the result of the overly simplified test cases presented here fig 12 illustrates the modeled variance and the observed variance in this figure the red solid line with squares is the modeled variance the light gray line is the prescribed gas temperature variance and the dashed lines indicate the ensemble of variance observations while the observed variances are approximately half to three quarters of the modeled values in red this is not necessarily a case of overestimation by the modeled variance rather it is the result of restricted range of observed temperatures measured by the flir it is reasonable to assume that temperatures within a single cell solid would be below 500 c during a low intensity fire which is not accounted for in these observations thus the results show fictitiously low variance furthermore the three dimensional distribution of temperatures which are not accounted for in the flir observations would further increase the variance this is because deeper fuel initially has lower temperatures compared to the surface temperature which would be exposed to convective and radiant heating ahead of the flame front as discussed in the previous figure with a mean temperature adjustment we similarly observe a more realistic modeled variance by excluding fuels below 773 k while magnitude of the modeled variance does not reflect the observed variance presented here the overall shape and behavior of the variance aligns with the shape and behavior of the observed variance given the strong influence of the gas temperature variance on the fuel temperature variance improvements to modeled gas temperature will improve the overall model performance 4 4 future work the focus of this manuscript is on describing the governing equations for wet and dry fuels and their temperature evolution our future plan currently ongoing is to focus on the development of equations describing the evolution of the temperature variations for the gas phase based on a similar approach as described here these new equations for the gas phase will then be coupled to the set of equations for the wet and dry fuels which should improve the modeled energy exchange between wet and dry fuel and surrounding gas this has a significant influence on fire behavior as we have previously shown this energy exchange is one of the essential components contributing to the self determining nature of firetec as such considerable effort is being put toward these next developments current assumptions neglecting triple correlation and higher order terms for multi variable correlations are reasonable as a first approximation for this proof of concept discussion similarly the assumed top hat distribution is physically unrealistic but allows us to remove many of the complications associated with an evolving distribution shape however future work will incorporate a more realistic probability distribution and return to the triple correlation terms that may have a non negligible contribution while a normal distribution in some cases is warranted it cannot be assumed universally valid as such we are working towards capturing the sub grid dynamic distribution changes over time next steps will also include triple correlations in the gas equation to account for skewness in the distribution we recognize that the constants assigned initially in this study may be adjusted in future work we anticipate that coupling a variable gas equation model to the solid fuel model described here will improve the convective heat transfer exchange and provide us with the opportunity to better evaluate and assign the constants c h and c t d f thus we will return to these constants in future studies furthermore while we currently apply a spatially segregated approach to the fine fuels i e an individual fuel particle s moisture and temperature are always homogeneous fuel is distributed evenly within the cell and heating cooling evaporation and combustion can occur in a fraction of the cell leading to changes in the sub grid temperature distributions the current approach can be modified to a shell type paradigm in this new approach fuel particles are initially heated on the outer layer or shell while the inner core of the particle may remain cool and wet this new formation will allow gradients in fuel moisture and temperature which is the first step towards the development of burning in thermally thick fuels and smouldering combustion processes which are critically important to smoke and emissions production and many other fire effects 5 conclusion a new model for the sub grid processes related to temperature and moisture variation in wildfire behavior models is presented other wildfire behavior models have succeeded in simulating high intensity fire during extreme conditions where the relevant length scales are large enough to be resolved and the impact of sub grid variations is minimal however the performance of these models suffers when conditions are less extreme and the length scales are small the aim of the work described here is to improve the overall performance of these models particularly during lower intensity fire where the sub grid spatial and temporal variations have significant impacts on fire behavior the first set of simulations presented are idealized scenarios with prescribed upwind mean gas temperature constant wind speed and moisture content in the fuels these simulations highlight the behavior of the model without the complex and non linear effects of variations in wind speed and gas temperature while we couple the fuel equations to an evolving gas temperature equation the simplistic nature of the prescribed upwind gas temperature and the lack of cool air and overly simplified oxygen advection results in limitations on model performance in nearly all simulations with combustion an unnaturally rapid increase and decrease in temperature occurs as a result of this simplified advection aside from this deficiency the model is able to effectively simulate the evolution of unresolved moisture and temperature and the resulting fuel consumption when conditions are both extreme and marginal furthermore we illustrated the increased flexibility with respect to grid size in the proposed model through the influence of the gas equation with the dry fuel equation we illustrated that wider variations in gas temperature directly result in wider variations in fuel temperature while we are unable to evaluate the performance of each term and closure in these new equations with observations the overall behavior of the mean temperature and the variance equations is compared to a set of experimental burns conducted in 2017 at the university of georgia we present the results of a simulation using the known fuel load moisture content and approximate wind speed during these experimental fires for a 1 m 1 m cell the in fire data is limited to temperatures above 773 k on the surface layer only and so adjusting the model results to calculate the mean and variance in the fraction of modeled fuel hotter than 773 k is comparable to the observations the major limitation of this comparison lies in the absence of modeled cool air entrainment grossly simplified oxygen advection and the fact that observations are on a single plane but modeled temperature and fuel is 3 dimensional despite this the model performs adequately this is the first step in ongoing work which aims to couple the equations described here with new equations for the evolving gas temperature this new coupled system of equations will ultimately be incorporated into the three dimensional version of firetec to improve the overall performance of the model particularly during marginal burning conditions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper funding this work was partially supported by the strategic environmental research and development program usa project numbers rc 2643 rc20 c3 1382 and los alamos national laboratory ldrd program usa project 20220024dr fighting fire with fire enabling a proactive approach to wildland fire 
25398,many wildfire behavior modeling studies have focused on fires during extreme conditions where the dominant processes are resolved and smaller scale variations have less influence on fire behavior as such wildfire behavior models typically perform well for these cases however they can struggle in marginal conditions e g low intensity fire as small scale variations significantly influence fire physics at scales below grid resolution in an effort to generalize wildfire behavior models and improve their overall performance we have developed a new set of equations for wet and dry fuel to capture the finer scale sub grid variations in temperature and moisture we explore the behavior of these equations in simple scenarios ranging from high to low intensity fire furthermore we evaluate the performance against observations of surface fire in all cases the proposed model performs well after peak temperature is reached however the rise of fuel temperature at the onset of combustion is faster than expected keywords multiphysics modeling wildfire subgrid closure data availability the authors do not have permission to share data 1 introduction wildfire behavior and spread is influenced by many complex processes such as the exchange of mass momentum and energy through non linear convective and radiative heating and cooling combustion and turbulence linn 1997 mell et al 2007 linn et al 2002 mell et al 2009 accary et al 2014 these interactions depend heavily on the dynamic and heterogeneous turbulent flow fields that connect fire to its environment including the surrounding atmosphere fuels dead and live vegetation and topography current physics based fire behavior models leverage computational fluid dynamics cfd techniques to represent the coupled fire atmosphere interaction using partial differential equations linn 1997 mell et al 2007 morvan et al 2018 an overview of fire models can be found in sullivan 2009 since fires both wildfires and prescribed occur at large spatial scales hundreds of meters to hundreds of kilometers and involve complex non linear processes occurring at a wide variety of scales e g atmospheric eddies hundreds of meters wide and reactions occurring in millimeter diameter conifer needles it is not currently feasible to resolve important phenomena over all these scales thus in order to simulate wildland fires at landscape scales 100s of meters to kilometers or larger compromises on resolution must be made to offset computational limitations in large study areas linn et al 2002 linn and cunningham 2005a as cell sizes increase physics based models are unable to resolve fine scale sub grid processes and variations that have non negligible influences thus parameterizations must be developed to capture the net or aggregated effects of sub grid phenomena this need is analogous to the development of turbulence closure models that capture the net effects of variations in a flow field that cannot be resolved recent cfd based wildfire studies have focused on understanding the behavior of high intensity wildfire scenarios e g hoffman et al 2015 2016 marshall et al 2020 banerjee et al 2020 banerjee 2020 frangieh et al 2018 these cases pose significant risks to people communities and infrastructure and are the most challenging to manage and suppress martell 2001 the characteristic length scales of dominant fire phenomena e g flame length and fire geometry e g fireline depth typically increases with intensity of the fire byram 1959 simultaneously the sensitivity of fires to fine scale variations in the ambient environment e g turbulence or fuel heterogeneity and the impact of fine scale variations in fire conditions e g temperature or moisture variations decreases with the increase in the characteristic length scales when environmental conditions are conducive to extreme fire i e hot dry and windy potter and mcevoy 2021 and fires are intense existing physics based wildfire behavior models are typically able to appropriately capture fire spread and fire intensity e g hoffman et al 2016 since the dominating processes can generally be resolved and the impacts of sub grid heterogeneities in temperature wind turbulence fuel and moisture are less significant an example of fire and fuel within a single computational cell for high intensity fire is shown in fig 1a however during lower intensity fires the spatial scales of fire behavior are smaller and the importance of finer scale variations in fire activity and fire environment is greater than during high intensity fire jonko et al 2021 linn et al 2021 parsons et al 2011 zhou et al 2005 as the relevant length scales e g flame length of the fire decrease to below model resolution physical wildfire behavior models struggle as a greater fraction of the critical fire behavior determining processes cannot be explicitly captured and must be captured by subgrid models linn 1997 veynante and vervisch 2002 im et al 1997 for example there can be significant heterogeneity in temperatures of both gases and solids within unresolved volumes under these marginal conditions hotter and cooler regions with respect to the mean resolved temperature as is illustrated in fig 1b capturing the influences of this heterogeneity is important as it directly relates to the drying and combustion processes and thus the spread of fires with increasing desire to consider the ecological effects of fires the role of flanking and backing fires and the use of prescribed fires the importance of representing low intensity fires is increasing physics based coupled fire atmosphere models such as firetec linn 1997 linn et al 2002 use a series of coupled partial differential equations to track the evolution of mass momentum energy turbulence and species of gases moving around a fire and the mass moisture content and temperature of the fuel dead and live vegetation in firetec as well as other similar models such as the wildland fire dynamics simulator wfds mell et al 2007 2009 the solution of these coupled partial differential equations is done numerically on a three dimensional grid typical grid sizes for simulations with kilometer scale domains is on the order of meters for both firetec and wfds the temperature and moisture variations that govern some ignition and drying processes can thus be explicitly resolved on meter scales but even at sub meter scales there can be distributions of temperatures and moisture contents currently firetec employs the notion of a probability distribution function pdf of temperatures within each grid cell to determine the moisture evaporation and combustion rates using this approach firetec avoids having a step function in the rate of evaporation or combustion associated with the mean temperatures reaching critical values such as evaporation and combustion temperatures instead by accounting for the existence of a distribution of temperatures allows for a crude representation of the fact that a small fraction of the fuel might be hot enough to evaporate water or begin to combust this approach has shown promise for some high intensity fire scenarios based on model agreement with observations however the current formulation which assumes a static subgrid temperature distribution shape still presents challenges for low intensity fire due to the significance of the sub grid temperature variations which is greater than in high intensity fire scenarios for example during high intensity fire which is characterized by highly turbulent flow and mixing clements et al 2008 seto et al 2013 clements and seto 2015 clark et al 2020 and flame lengths much larger than a single cell the entire control volume computational cell would be relatively well mixed after the flame front has moved through the cell and combustion will occur in most if not all the fuel in these cases the cell size is small relative to the size of the flames and fireline depth conversely during low intensity fire where flame lengths are much smaller than the cell and mixing is much lower there can be regions of cool unburned fuel regions of hot burning fuels and regions of burned cooling fuel desai et al 2021 as the importance of fine scale variations increases with decreasing physical scales of the fires lower intensity the need for a more dynamic and scenario dependent representation of the distributions of sub grid variations increases the current firetec formulation explicitly tracks a mean solid fuel temperature and allows an assumed temperature variance the combination is used to determine how much of the fuel is hot enough to evaporate water and to begin combusting as the temperature of the mean fuel increases for example by including the notion of the distribution of temperatures within a cell some hot locations and some that might be still at ambient water can start being evaporated in some hot regions while the temperature is still at ambient levels in other parts of the cell what is missing in the current original formulation is scenario dependent methodology for determining the width of the distribution which will be determined by environmental factors such as wind speed gas temperature variation and even initial moisture content levels furthermore the current formulation does not distinguish between wet fuels and dry fuels individually but instead tracks only a single fuel mean fuel with a dynamic average moisture content in reality simultaneous processes could occur within a control volume in which water evaporates from wet fuel to produce dry fuels while at the same time dry fuels may be subject to pyrolysis and combustion in lower intensity fire regimes where the coincidence of drying and combustion occurring in the same cell increases it is challenging to represent the influences of temperature distributions with a single energy equation for the solid fuel in an effort to improve the representation of subgrid processes related to temperature and moisture variation within a computational cell we are working as part of a larger research initiative toward the implementation of a new set of equations describing the evolution of the variation of gas temperatures and both wet and dry solid temperatures particularly in this manuscript however we develop and propose a new set of equations that incorporate the conservation of mass and energy for the dry and wet fuels individually to ensure the model can better represent sub grid combustion and evaporation processes simultaneously this sets the first step towards completing our larger research initiative to this end the current manuscript is structured such that we outline the derivation of these new equations in section 2 followed by a description of the proof of concept simulations in section 3 we then present the results from a set of simple scenarios in section 4 main conclusions are presented in section 5 2 model formulation 2 1 equations for temperature of wet and dry fuel critical processes governing the spread of wildland fire depend on the temperature of the combustible material in most natural scenarios the fuel is initially a combination of combustible biomass and water the amount of water in the fuel is expressed as the fuel moisture fraction which we define in the context of this manuscript as the mass of water divided by the mass of the oven dried combustible biomass or moisture content which is the moisture fraction written as a percent this fuel moisture influences the temperature dynamics of combustible material because it adds significant thermal mass to the fuel and adds an energy sink associated with the evaporation of water the initial water content of live fuels depends largely on plant physiological processes which respond to a variety of environmental factors such as seasonality and available water in the ground jolly et al 2014 the ambient water content of the dead fuel is a dynamic quantity that depends on the history of the local humidity and the size and shape of the biomass fine fuels equilibrate with the atmospheric moisture levels rapidly 1 h whereas thicker fuels take 10s 100s or even thousands of hours to equilibrate van der kamp et al 2017 since wildland fire spread is heavily dependent on the fine fuels such as foliage grasses or twigs our attention is currently focused on the dynamic conditions of this subset of the biomass but future work will be needed to include larger fuels in this formulation as they are important for determining fire emissions fire effects and the sustaining of ignitions an equation for the evolution of the temperature of the solid combustible fuel and associated water content at a single point can be achieved by beginning with the equation for the conservation of internal energy of the solid linn 1997 then we subtract the conservation of mass equation multiplied by the temperature of the solid t s and specific heat c p s which is a mass weighted specific heat capacity of the combined wood c p f 2500 j kg 1 k 1 and water c p w a t e r 4200 j kg 1 k 1 for this purpose the temperature of the biomass and water content are assumed to be the same value since the fuels are thermally and moisture thin equations of this sort are currently used in physics based wildland fire models such as firetec as is shown in eq 1 linn et al 2002 linn and cunningham 2005b in this equation the bulk density of the solid  s is treated as the sum of the bulk density of water  h 2 o and fuel  f the combined internal energy of the fuel and water is the product of  s t s and the mass weighted specific heat c p s 1 c p s  s t s t q h a v t g t s f f h f  c p f t c r i t n f f h 2 o h h 2 o c p h 2 o t v a p in eq 1 t is time q is the net radiation heat transfer rate per unit volume h is the convective heat transfer coefficient a v is the area per unit volume of the fuel in a cell t g is the local temperature of the gas f f is the mixing limited reaction rate as described by linn 1997 h f is the heat of combustion of the fuel  is the fraction of the reaction energy that is deposited directly back to the fuel t c r i t is a critical local temperature for pyrolysis taken to be 600 k in this manuscript which is consistent with the critical temperature currently used in firetec n f is a stoichiometric coefficient for the net burning reaction as described by drysdale 1985 f h 2 o is the rate of water evaporation h h 2 o is the heat of vaporization of liquid water and t v a p is the temperature of water vaporization at standard pressure for a single location f h 2 o only has a non zero value when the temperature of that location is t v a p and above this temperature the moisture content is zero f f begins is non zero at the critical temperature but it can persist at higher temperatures since combustion depends on other factors such as oxygen concentration and mixing there are challenges associated with applying this equation in numerical simulations where computations are not performed at every single point but are instead used to describe the mean temperature in a control volume or computational cell in the case of models designed for landscape scale fires length scales in these control volumes can be on the order of meters in such volume sizes there can be a mixture of states e g distribution of temperatures and a mixture of fuels that are wet and have begun drying that are not explicitly resolved linn 1997 for example it is possible to have fuel that is wet drying dried and heating up and combusting all in the same computational cell especially in the context of low intensity fires in an attempt to account for the fact that there was likely a range of temperatures within the control volume the concept of temperature distribution or pdf was introduced linn 1997 in the reaction rate and later to the moisture evaporation this concept which captures the presence of both warm and cold fuel within a volume allowed moisture to begin to evaporate before the mean temperature reached vaporization temperature and for combustion to start even before all of the moisture was evaporated in the original implementation of firetec linn 1997 linn et al 2002 however there was no mechanism that allowed for the pdf width or shape to dynamically evolve based on the fire environment i e the distribution is fixed and does not evolve and adapt this presents challenges to the generality of the model and its applicability to portions of low intensity fire since it cannot be assumed that the distribution is universal in all fire scenarios in order to increase the generality flexibility or range of scenarios that are applicable of this approach it is necessary to incorporate a dynamic pdf that adapts to the fire environment during the development of the original formulations linn 1997 and subsequent model improvement efforts attempts to derive the governing equations for the dynamic width or standard deviation of the pdf for the solid temperature distribution ran into challenges related to the phase change threshold associated with moisture evaporation the equations for a variance of the temperature distribution were tractable for temperatures above and below the temperature of evaporation in this original formulation the amount of moisture associated with the fuel was tracked and one could back out the fraction of the solid that was wet and dry but it was not possible to track the temperature of the wet and dry components separately and thus it was difficult to estimate the fraction of the fuel that was hot enough to combust and the fraction that still had moisture to evaporate thus in order to better track the distribution of temperatures the evaporation of water and the potentially simultaneous combustion processes that occur within a control volume we propose a new alternative approach to capture the evolution of wet and dry solids for this newly proposed approach we partition the solid fuel into two categories or states wet and dry the wet fuel is the biomass that has the initial moisture content determined by the ambient conditions pre fire and dry fuel is the biomass that has had the moisture driven off as a result of heating by the fire using this construct the bulk density of the fuel moisture mixture  s is the sum of dry fuel  d and wet fuel  w and water  h 2 o this is illustrated in eq 2 where wet fuel and water can be combined to obtain the bulk density of the wet solid that is  s w  w  h 2 o 2  s  d  w  h 2 o  d  s w the conceptual division between the wet and dry fuel is based on the notion that fuel is initially wet and then it is dried to generate dry fuel we assume that all fuel within a control volume has the same initial moisture fraction or ratio r m o i s t r m o i s t 0 which remains constant for the wet fuel while it is possible to have multiple fuels with differing moisture contents co existing within a single cell for simplicity we adopt only one representative fuel with a single moisture fraction additionally the moisture fraction of the wet fuel remains constant since the water evaporation in a fuel particle results in the creation of dry fuel and consistent removal of wet fuel thus we are tracking moisture in the cell through the shifting mass balance of wet fuels which remain at the initial moisture level and dry fuels which always have zero moisture the total moisture fraction of the cell mass weighted average moisture combining wet and dry fuel components will evolve and decrease with evaporation furthermore there is initially no dry fuel since even dead fuels e g dry needles and dried grass have a non zero equilibrium moisture content with the atmospheric humidity even in extremely dry conditions dead fuel moistures are typically above 0 03 or higher however moistures can be as low as 0 01 or lower aguado et al 2007 thus the bulk density of the wet solid is the same as the bulk density of the solid  s w  s at the initialization with this in mind the conservation of mass equations for the water and the wet fuel based on the rate of evaporation of water per unit volume f h 2 o are 3  h 2 o t f h 2 o 4  w t f h 2 o r m o i s t w where the moisture fraction of the wet solid is given by 5 r m o i s t w  h 2 o  w the term r m o i s t w must be greater than zero since the wet solid fuel will never be completely dry but r m o i s t w can be larger than one which simply indicates that the mass of the water is larger than the mass of the biomass for example a live deciduous leaf could have a moisture fraction of 2 meaning the mass of water contained in the leaf is twice that of the dry mass we assume that  h 2 o approaches zero at a rate proportional to the rate that wet mass becomes dry thus r m o i s t w remains a constant even when water is completely evaporated we can then form the conservation of mass of the wet solid and its moisture  s w 6  s w t f h 2 o 1 1 r m o i s t w where mass loss from the wet fuel conservation equation corresponds to a mass source in the conservation of dry fuel equation 7  d t f f n f f h 2 o r m o i s t w the additional sink term in eq 7 f f n f corresponds to the mass loss rate due to the burning of dry fuel using the definition of the two different categories of solids we can then write conservation equations for the internal energy of both wet and dry solids respectively 8 c p s w  s w t w t q w h a v s w t g t w f f h f  w f h 2 o c p h 2 o c p f r m o i s t w t v a p f h 2 o h h 2 o and 9 c p f  d t d t q d h a v d t g t d f f h f  d f h 2 o c p f r m o i s t w t v a p f f n f c p f t c o m b u s t where c p s w  s w  h 2 o c p h 2 o c p f  w c p s w is the mass weighted specific heat capacity of the wet fuel t d is the dry fuel temperature t w is the wet fuel temperature t g is the gas phase temperature  w and  d are the fraction of combustion energy that is directly deposited on the wet and dry solid respectively t c o m b u s t is the temperature at which combustion occurs and solid fuel mass is converted to gaseous products and h is the convective heat transfer coefficient which is assumed to be the same for wet and dry materials given they are the same shape and size a v s w and a v d are the area per unit volume of the wet and dry fuel which are calculated based on their respective bulk density  x the material density  m i c r o and size scale radius of cylindrical fuel particles s s of the fuel particles 10 a v x 2  x s s  m i c r o where the subscript x indicates either wet fuel abbreviated s w or dry fuel abbreviated d bulk density in both eqs 8 and 9 the first terms on the right side of the equation are net radiation the second terms are convective heat transfer and the third terms are deposition of energy deposited via the combustion processes the fourth terms on the right side of these equations represent the transfer of energy associated with the movement of mass from the wet state to the dry state and the loss of the mass of the water from the wet state the final terms in the wet internal energy equation eq 8 represents the endothermic evaporation process it is important to note that the evaporation rate goes to zero as the density of wet fuel goes to zero the final term in the dry internal energy equation eq 9 accounts for energy loss from the dry state due to the mass losses associated with gaseous products in the combustion process since the wet fuel temperatures are expected to change less than 100 k and the moisture fraction of this fuel is held fixed at the initial level we treat the specific heat of the wet fuel as constant throughout the simulation thus we can subtract the product of the specific heat the temperature of wet fuel and the conservation of wet mass equation eq 6 that is 11 c p s w t w  s w t f h 2 o c p h 2 o c p f r m o i s t w t w from eq 8 to arrive at 12 c p s w  s w t w t q w h a v s w t g t w f f h f  w f h 2 o h h 2 o f h 2 o c p h 2 o c p f r m o i s t w t w  w e t t v a p in eq 12  w e t defined in eq 13 has been added to account for the fact that water evaporation only occurs when t w t v a p which means that f h 2 o 0 when the wet solid temperature is t v a p thus the final f h 2 o term will cancel since we assume there is no evaporation when the temperature is below t v a p and when any portion of the wet fuel reaches t v a p the water is immediately evaporated assuming standard pressure the newly dried fuel is now tracked in the dry fuel equation 13  w e t 0 t w t v a p 1 t w t v a p for the dry fuels we similarly subtract the term c p f t d  d t defined as 14 c p f t d  d t c p f t d f f n f c p f t d f h 2 o r m o i s t w from eq 9 thus we arrive at 15 c p f  d t d t q d h a v d t g t d f f h f  d f h 2 o r m o i s t w c p f t v a p t d f f n f c p f t d  d r y t c o m b u s t analogous to  w e t in the wet temperature equation  d r y is added that is 16  d r y 0 t d t c o m b u s t 1 t d t c o m b u s t thus the last two terms in eq 15 now cancel each other it is worth noting that t c o m b u s t is not a fixed value but it is the temperature at which combustion is taking place at any given location or instant the concept of wet and dry fuel for this formulation is associated with wet and dry fuel particles or even wet and dry sections of length of fuel particles in the case of thin fuels which is our focus here in future formulations this can be expanded to handle gradients of moisture within thicker fuels as the outer shell can be dried while the inner core might still be wet for the current case however we only considered the case of thermally thin and moisture thin fuels which constitutes homogeneous temperatures and moisture fraction throughout the fuel particle thickness in development of a new model it is reasonable to start with the more simplified but still realistic conditions such as the one considered here the areas per volume of the wet and dry mass a v s w and a v d can be related to an aggregate area per volume that is if we assume that the difference in wet versus dry fuel is predominantly due to position in the cell i e one spot has been dried out while another spot is still wet 17 a v s w a v  s w  d  s w and 18 a v d a v  d  d  s w this follows a reasonable observation that heating will begin on just one side of the cell as wind and turbulence initially advect and mix hot gases into the cell from one direction the fraction of combustion energy returning directly to the solid  s can be split into a fraction that is deposited directly on the dry and wet fuels in such a spatially segregated paradigm however we are in principle assuming that the energy returning to the solid is predominantly to the dry solid where combustion is occurring thus we approximate these terms as  d  s and  w 0 2 2 mean temperature equations to derive the equation for the mean temperatures of the wet and dry solids we decompose the quantities of eqs 12 and 15 into mean denoted by an overbar and fluctuating parts denoted with then we take the ensemble average of these equations similar to the development of mean velocity equations in turbulence modeling daly and harlow 1970 thus we get the following two expressions 19 c p f  d t d t  d t d t q d h a v d t g t d h a v d t g a v d t d a v d h t g h t d h a v d t g t d h a v d t g h a v d t d f f h f  d f h 2 o c p f r m o i s t w t v a p t d c p f r m o i s t w f h 2 o t d 20 c p s w  s w t w t  s w t w t q w h a v s w t g t w h a v s w t g a v s w t w a v s w h t g h t w h a v s w t g t w h a v s w t g h a v s w t d f f h f  w f h 2 o h h 2 o if we consider the solid density distribution or density variation to be somewhat dominated by the presence of or the lack of a fuel particle at a specific location and thus bimodal then we can neglect t d  d terms this implies that both positive and negative dry fuel temperature fluctuations exist where there is fuel positive density fluctuation and neither are relevant in locations where there is no dry fuel we extend this argument to wet fuels as well additionally if we assume that fuel particles do not change their radius while they burn but rather shrink in their length and local density until they disappear then the correlations between a v d and a v s w can be neglected this approximation is consistent with previous work in which the size scale of particles does not change with combustion for example linn 1997 and linn et al 2002 thus the correlations between a v d and temperature are minimal even if the radius was changing it is also reasonable to assume changes in the heat transfer coefficient are not due to changing solid temperature but rather due to properties of the air i e temperature or velocity since the heat transfer coefficient itself is a function of air properties see eq 23 as a first approximation therefore we assign h t d 0 and h t w 0 finally we neglect third and higher order correlation terms for this initial approach at the dynamic pdf for simplicity with the above simplifications we are left with the following expressions 21 c p f  d t d t q d h a v d t g t d a v d h t g f f h f  d f h 2 o c p f r m o i s t w t v a p t d c p f r m o i s t w f h 2 o t d 22 c p s w  s w t w t q w h a v s w t g t w a v s w h t g f f h f  w f h 2 o h h 2 o these expressions are not in a closed form due to two terms h t g and f h 2 o t d we now try to address this closure problem beginning with the covariance between the heat transfer coefficient and the gas temperature we first consider the equation for the heat transfer coefficient used in firetec that is 23 h 0 25 0 683 r e 0 466  a i r s s as was described by incropera and dewitt 1996 for forced convection over tubes with the local reynolds number defined as 24 r e u s s  in these equations  a i r is the thermal conductivity of air u is the local velocity including contributions from mean and fluctuating components and  is kinematic viscosity we utilize a simple linear relationship with gas temperature for both viscosity and conductivity such that  c 1 t g c 2 and  a i r c 3 t g c 4 where c 1 1 66 1 0 7 c 2 3 37 1 0 5 c 3 5 55 1 0 5 and c 4 9 59 1 0 3 incorporating these new equations into eq 23 we arrive at 25 h 0 25 0 683 u s s c 1 t g c 2 0 466 c 3 t g c 4 s s formulating the convective heat transfer coefficient as a function of t g allows us to determine h t g and estimate the correlation h t g using 26 h t g h t g t g t g based on the approximated equation for h we find 27 h t g 0 25 0 683 u 0 466 s s 0 534 1 c 1 t g c 2 0 466 c 3 0 466 c 1 c 1 t g c 2 1 466 c 3 t g c 4 the correlation between the fluctuations in water evaporation rate and the temperature of the dry fuel that is f h 2 o t d is nonzero in the case where the mean dry temperature is above the temperature for vaporization and there is still additional mass being moved from wet to dry fuel state in this scenario a positive fluctuation in the evaporation as the mean temperature of the wet fuel will always be less than the evaporation temperature results in additional mass source for the dry fuel at a temperature lower than the mean dry temperature a negative temperature fluctuation if the evaporation rate is non zero but the mean dry temperature has fallen below the temperature for vaporization we expect a positive fluctuation in the evaporation rate to coincide with a positive fluctuation in the dry temperature as we are adding dry fuel at a temperature that is higher than the current mean dry temperature the correlation is expected to increase with the width of the distribution and scale with the mean evaporation rate thus we model this correlation as 28 f h 2 o t d f h 2 o t d t d t d t v a p t d 2 3 equations for temperature variance evolution of the width of the probability density functions for the temperatures of the wet and dry solids within a volume is tracked based on a similar approach as the development of turbulence transport equations daly and harlow 1970 similar to velocities and pressure in turbulence modeling the quantities of eq 15 are decomposed into mean and fluctuating parts then both sides of the equation are multiplied by the fluctuation of the temperature followed by ensemble averaging of the entire equation starting with the dry temperature equation this process results in the following expression 29 c p f  d t d t d t t d  d t d t t d  d t d t t d q d h a v d t d t g t d t d h t d a v d t g t d h a v d t g t d a v d t d t d a v d t d h t g t d h t g t d h t d t d t d h t d h a v d t g t d t d h a v d t g t d h a v d t d t d f f h f  d f h 2 o c p f r m o i s t w t d t d c p f r m o i s t w f h 2 o t d t v a p t d c p f r m o i s t w f h 2 o t d t d rearranging eq 29 and applying the assumptions described above for the mean temperature equations we arrive at an equation for the variance t d t d 30 t d t d t 2 c p f  d q d t d h a v d t d t g t d t d t d f f h f  d c p f r m o i s t w f h 2 o t d t v a p t d c p f r m o i s t w f h 2 o t d t d here we have three unknown terms q d t d t d t g and t d f f that we need to address to close eq 30 the correlation between the evaporation rate and the dry temperature f h 2 o t d was discussed in the previous subsection for the correlation between radiation and dry temperature q d t d we assume the net heat transfer will depend only on the energy emitted from the dry fuel this is a simplification but lab studies have shown cohen and finney 2022a b that in thermally thin fuels convective heating plays a larger role in pre heating the fuels thus we omit radiative energy gains at this stage in development leveraging approaches used in variable density turbulence besnard et al 1992 and the stefan boltzmann law for blackbody radiation emission q e m i t t e d a v   t 4 incropera and dewitt 1996 we can approximate the dependence of a perturbation of the emitted radiation on the temperature of the dry fuel with q e m i t t e d 4 a v   t 3 t in these equations  is the stefan boltzmann constant  is emissivity and we include an added view factor correction  for scenarios where fuels are compacted in the bottom of a computational cell as is the case for litter or grass layer the value of  is 1 when the fuel is distributed throughout the cell this view factor can be thought of as a correction on the area per unit volume since emitted radiation is a net negative radiation contribution this term will act as a sink from the t d t d equation furthermore neglecting a terms due to the assumed constant radius of the fuel particle we arrive at 31 q t d 4 a v d    t d 3 t d t d the correlation between the temperature of the dry fuel and the reaction rate t d f f is important in cases where the mean temperature is below the combustion temperature but the combustion rate might be nonzero such as in instants or locations in which there is a positive temperature fluctuation reaching above the mean temperature to this effect positive temperature fluctuations are associated with positive fluctuations in the reaction rate and contribute to the increase in the variance of dry temperature t d t d similarly when the mean temperature is greater than the minimum combustion temperature i e the reaction is occurring in more than half the available fuel negative temperature fluctuations or cooler temperatures in the distribution can represent locations and instances at which the localized temperature is not sufficient to support the reaction i e it is too cold to react thus there is a negative fluctuation in the reaction rate however there are other contributions to the variations in reactions such as localized mixing and oxygen concentrations with increasing fraction of the volume above the critical temperature although the correlation t d f f is expected to be positive it will decrease with decreasing fraction of the temperature distribution above the critical temperature this is conceptually shown in fig 2 here we expect the correlation to be closer to zero when r c o m b u s t which is the fraction of dry fuel that is hot enough to react is large however we anticipate the strongest correlation when less fuel is above the critical temperature r c o m b u s t is small considering this we approximate the correlation as 32 t d f f c t d f f f t d t d 1 r c o m b u s t where c t d f is taken to be 0 7 this constant must be empirically determined and here we estimate it to be 0 7 using the little fire data we have available however we will return to this constant in future studies the final unclosed term in eq 30 t d t g is the correlation between fluctuations in dry fuel temperature and gas temperatures the correlation between these temperatures are largely related to the rate of convective heat transfer which lower the temperature difference between adjacent gases and solids when the convective heat transfer rate is high perturbations in dry solid temperature are strongly related to the gas temperature perturbations since t d t g is symmetric in t d and t g so should be the modeled term this covariance is expected to increase depending on the magnitude of the variance of both the dry fuel and gas temperature due to its symmetry as well as the strength of the convective heat transfer as the convective heat transfer coefficient increases there will be greater energy transfer between the gas and solid phases and thus a stronger correlation between the fluctuations to this end we propose the following expression for the dry fuel and gas temperature covariance 33 h a v d t d t g h a v d 1 e c h h h n o r m t g t g t d t d 2 in this equation h n o r m is the normalization constant for the convective heat transfer coefficient which is the approximate background value before ignition or heating in the current study h n o r m is assigned the value of 25 0 w m 2 k 1 which is representative of the ambient value of h in firetec before fire and during low wind conditions and c h 5 the constant c h was determined by comparing modeled fire behavior over a variety of values ranging from 1 to 5 not shown for simple scenarios this constant must be determined using empirical methods and we estimate it using the little fire data we have available however we will return to this and other constants in future work we now apply the same decomposition and derivation to the instantaneous wet solid temperature equation eq 12 thus taking the ensemble average of the expression yields 34 c p s w  s w t w t w t t w  s w t w t t w  s w t w t t w q w h a v s w t w t g t w t w h t w a v s w t g t w a v s w t g h t w a v s w t w t w a v s w t w a v s w t w h t g t w h t g a v s w t w h t w t w t w h a v s w h t w t g t w t w h a v s w t g t w h a v s w t w f f t w h f  w f h 2 o t w h h 2 o now applying analogous simplifications that were discussed previously for the dry fuel equation returns 35 t w t w t 2 c p s w  s w t w q w h a v w t w t g t w t w f f t w h f  w f h 2 o t w h h 2 o since we assume that the fraction of energy from the combustion process deposited on the wet fuel  w is negligible f f t w h f  w will have no contribution combustion is not occurring in the wet fuels and so the local retention of heat at the site of the reaction is not on the wet fuels thus there should be little to no energy from the combustion process directly contributing to wet fuel temperature changes this assumption does not preclude the reaction from heating the wet fuels in this proposed version the reactions can heat the wet fuels because elevated temperature dry fuels heat the gases and the gases convectively heat the wet fuels ultimately when this model is fully implemented in firetec the wet fuels will also be heated by the dry fuels directly via radiation heat transfer but this mechanism is not in the current formulation thus the only term left to complete the closure is the covariance of wet fuel temperature and evaporation rate f h 2 o t w using the radiation and convection heat transfer rates we can formulate the rate at which energy is added to the wet material when the energy gained by the wet fuels causes the upper limit of the wet fuel distribution to reach or exceed the vaporization temperature without enforcing the phase change on water or removing the energy of vaporization evaporation of water begins the energy available to evaporate water e w is related to the evaporation rate by f h 2 o e w h h 2 o we define a theoretical ratio for the fraction of wet fuel mass r h 2 o which would have risen above the evaporation temperature if evaporation had not commenced assuming the distribution of temperature is symmetric about the mean for simplicity thus we have 36 r h 2 o t w m a x t v a p 2 c p d f t w t w where c p d f is a constant with a value as the ratio of halfwidth of the temperature distribution to the standard deviation and t w m a x is the maximum temperature in the cell moreover this constant depends on the type of distribution that would be assumed the mean evaporation rate is therefore determined to be 37 t w m i n t w m a x f h 2 o t d t t w m i n t w m a x d t f h 2 o here f h 2 o t is the evaporation rate associated with fuel at any specific temperature t and t w m i n is the minimum temperature in the cell now we define a normalized evaporation rate f as 38 f h 2 o f h 2 o t f h 2 o where 39 f h 2 o 0 t w m i n t t v a p 1 r h 2 o t v a p t t w m a x this yields unity for the mean normalized vaporization rate 40 f h 2 o t w m i n t w m a x f d t t w m i n t w m a x d t t w m i n t v a p 0 d t t v a p t w m a x 1 r h 2 o d t 2 c p d f t w t w 1 decomposing f h 2 o into its mean and fluctuating components reveals 41 f h 2 o f h 2 o f h 2 o 1 f h 2 o using these definitions we can compute f h 2 o t w as 42 f h 2 o t w f h 2 o t w m i n t w m a x f h 2 o t w d t t w m i n t w m a x d t f h 2 o t w m i n t w m a x f h 2 o 1 t w d t 2 c p d f t w t w where the temperature range of the wet solid is 43 t w c p d f t w t w t w t w c p d f t w t w given the definition of f h 2 o this simplifies to 44 f h 2 o t w f h 2 o t v a p t w c p d f t w t w 1 r h 2 o 1 t w d t 2 c p d f t w t w using eq 36 in the lower limit of the integral we arrive at 45 f h 2 o t w f h 2 o 1 r h 2 o 1 c p d f t w t w 2 c p d f t w t w 2 r h 2 o c p d f t w t w 2 4 c p d f t w t w manipulating and rearranging these terms we arrive at 46 f h 2 o t w f h 2 o c p d f t w t w 1 r h 2 o 2 2 4 final equations the following subsection presents a summary of the new model equations with the closure completed for all terms 47 t d t 1 c p f  d q d h a v d t g t d h t g t g t g a v d f f h f  d f h 2 o c p f r m o i s t w t v a p t d f h 2 o c p f r m o i s t w t d t d t d t v a p t d 48 t w t 1 c p s w  s w q w h a v s w t g t w h t g t g t g a v s w f h 2 o h h 2 o 49 t d t d t 2 c p f  d 4 a v d    t d 3 t d t d h a v d t g t g t d t d 2 1 e c h h h n o r m t d t d c t d f f f t d t d 1 r c o m b u s t h f  d f h 2 o c p f r m o i s t w t d t d f h 2 o t d t d t d t d t v a p 2 c p f r m o i s t w and 50 t w t w t 2 c p s w  s w 4 a v s w    t w 3 t w t w h a v s w t g t g t w t w 2 1 e c h h h n o r m t w t w f h 2 o c p d f t w t w 1 r h 2 o 2 h h 2 o where h t g is described in eq 27 3 proof of concept simulations the new approach described here is intended to capture sub grid variability in temperatures and thus increase the generality of wildland fire simulation capabilities this enables properly simulating high and low intensity fires this formulation constitutes the first stage in the development of a multi phase coupled fire atmosphere model which has so far been confined to the solid phase since solids are not moving no advection or spatial diffusion terms we can study the performance in a single 1 m 1 m 1 m control volume where the externally driven conditions are prescribed e g wind speed upstream gas temperature upstream oxygen concentration and initial fuel moisture in order to understand the performance of the proposed models eqs 47 50 we developed a series of idealized tests by varying initial fuel moisture conditions wind speeds and upstream gas temperatures the wet fuels fuels at their initial moisture state are assigned an initial bulk density of 0 5 kg m 3 and moisture fractions of a 0 05 b 1 and c 2 these correspond to fuel conditions similar to a dry dead needles and fine brachwood or matted dead grass b live conifer fine branchwood and needles and c live deciduous fine branchwood and leaves respectively wind speeds have constant values of 0 1 m s 1 1 m s 1 and 2 m s 1 as this is a plausible set of wind speeds very near the ground height below 1 m in the vicinity of a surface fire for a special case we apply a wind speed value of 4 m s 1 which constitutes a high intensity fire in these proof of concept simulations we aim to explore a range of plausible scenarios by varying some of the primary driving factors for fire wind moisture and upwind temperatures these factors are important for determining fire intensity and spread for the purpose of this paper we define the range of wildfire intensity as low intensity fires that have depths and flame lengths 10s of centimeters up to intense fires that have burning zones of 10s of meters and flame lengths of similar size even though the focus of these proof of concept simulations is modeling the solid phase temperatures and their variations it is necessary to vary the local gas temperature for the closure of the convective heat transfer terms for this purpose a simplified evolution equation is developed that combines the prescribed velocity and upwind temperature the convective heat transfer coefficient an area per unit volume and a grossly simplified radiation energy sink this expression estimates the gas temperature in the control volume 51 c v a i r  g t g t q g c v a i r  g u t g x h a v d t d t g h a v s w t w t g f f h f  g here c v a i r is 718 j kg 1 k 1 q g o 2 a m b i e n t o 2 o 2 a m b i e n t  t g 4 which is the radiation loss scaled by the oxygen depletion that relates gas emissivity to combustion products in an oxygen poor environment x is the spatial coordinate and  g is 0 75 75 of reaction energy deposited in gas and 25 in the solid which is the fraction of reaction energy absorbed by the gas phase these values are within range with those observed in firetec and we do not allow them to vary for simplicity this equation for the gas temperature is simply intended to capture the fact that there is feedback between the solid and gas temperatures and the influences of energy in surrounding regions on gas temperatures the variance of gas temperatures is prescribed based on the mean gas temperature using the following equations this first equation 52 t g t g 30 t g 243 59 3 2 is the variance assigned to the low intensity gas scenario which ensures that the minimum gas temperature in the assumed wide distribution does not fall below 300 k the second version of the prescribed gas variance is 53 t g t g 60 t g 300 6 2 which we assigned for the other simulations in subsequent phases of this study related to the development of a multi phase sub grid fire atmosphere model these gas temperature equations are replaced with full transport equations similar to the technique applied for the solid phase in the current study initially the gas temperatures in the control volume in all of the idealized simulation were assumed to be 300 k before upwind gases began to advect into the control volume high intensity fire scenarios were simulated by assigning an upwind mean gas temperature that starts at 300 k and ramps up to 1000 k over 15 s and tapers off following this relation 54 t g u w 300 700 2 0 95 0 95 tanh t 15 50 0 05 and the low intensity fires were simulated with the upwind temperatures rising from 300 k to 500 k over 100 s and tapers off as 55 t g u w 300 200 2 0 95 0 95 tanh t 500 120 0 05 additionally we prescribe an oscillating upwind gas temperature with the following equation 56 t g u w 750 450 sin t 15 2  60 all three upwind mean gas temperature prescriptions for the idealized scenarios are plotted in fig 3 the first two temperature scenarios involving near sustained elevated gas temperatures were designed to reduce the complexity of environmental drivers with the elevated temperature that represent gases advected from upwind with the approaching fire the third gas temperature paradigm was chosen specifically to highlight the response of the model in dynamic environments with oscillating conditions high intensity fire scenarios are unlikely to coincide with light winds and thus we used only the case of wind speed at 4 m s 1 and temperature of 1000 k gas temperatures at or exceeding 1000 k are frequently observed in high intensity crown fire e g taylor et al 2004 the time step for all simulations is 0 001 s and we temporally discretize the differential equations using an explicit forward in time euler method the simulations ran for at least 600 s the size scale of the fuel half of the volume to surface area ratio remains constant and is prescribed as 0 0006 m which corresponds to roughly that of thermally thin grass needles or fine branchwood thermally thin fuel refers to fine fuel particles that have no internal temperature gradients as such an individual fuel particle will warm and cool uniformly non thermally thin fuel particles will be addressed in future efforts which would build on some approaches that have been outlined in this document sub grid turbulent kinetic energy tke is fixed at k a 0 125 u 2 k b 0 005 u 2 and k c 0 2 k b m 2 s 2 where u is the prescribed scalar wind speed firetec incorporates three sub grid tke scales and their associated turbulence energy spectra corresponding roughly to the unresolved scales associated with vegetation structure additional information on sub grid tke in firetec is discussed by linn 1997 for the purposes of this concept demonstration the turbulent length scales are equivalent to the distance between larger vegetation structures e g shrubs branches and needles the radiation loss terms are computed using the mean and variance of the wet and dry fuel temperatures to compute t 4 and blackbody radiation the effective area that radiates energy away from the solid is estimated based on the surface area per unit volume times  which is the ratio of the fuel depth height to cell height a compression factor 57 q  a v   t 4 while radiation ahead of the flame front has a role in preheating fuels radiation alone does not typically induce fire spread in fine wildland fuels that are loosely packed due to attenuation in low density discontinuous fuels and the fact that convective cooling between fine fuel elements counteracts radiative heating as was discussed by finney et al 2015 results from their experiments provide strong evidence that convection is largely responsible for wildland fire spread cohen and finney 2022a b thus in the interest of simplicity we omit the influences of radiative heating in this initial testing phase the radiative heating will be addressed in more detail in subsequent efforts the evaporation rate is calculated by first determining the difference between the energy gained by the wet fuel through convective and radiative heat transfer within one time step and the energy that it takes to raise the temperature distribution to the point where the max temperature reaches 373 k this residual energy is the energy available to evaporate water within the time step and determine the water evaporation rate the theoretical maximum temperature t w m a x that would be reached within a time step without accounting for the phase change provides an estimate for the fraction of fuel r h 2 o that would be above 373 k due to the radiative and convective heating and cooling for this initial effort we use a top hat distribution using this simplified distribution c p d f 3 in eq 36 we recognize that if a normal distribution is applied instead r h 2 o is a form of the error function or cumulative distribution function for a gaussian shape more details on the error function can be found in jeffrey 1995 and we will be exploring this topic in the future separately we apply a similar philosophy to determine what fraction of the fuel in a cell is above the critical temperature for combustion  this fraction is used in the mixing limited reaction rate as described by linn 1997 a simplified model for the advection and consumption of oxygen density is used to represent the depletion and replenishment of oxygen with time the available oxygen is initialized as 21 of the air density 0 21  g a m b i e n t and over time it is calculated explicitly using the following equation 58 o 2 t f f n o x y g e n u o 2 x here n o x y g e n is the stoichiometric coefficient for oxygen in the combustion reaction drysdale 1985 normalized by the total mass of reactants the upwind oxygen value is related to the upwind temperature such that as temperature decreases increases oxygen increases decreases proportionally corresponding to the upwind drop in temperature we assign upwind oxygen such that 59 o 2 u w 0 15 0 09 2 0 95 0 95 tanh t 15 50 0 05 in the high intensity simulations and 60 o 2 u w 0 15 0 09 2 0 95 0 95 tanh t 500 120 0 05 for the low intensity cases lastly the upwind oxygen concentration in the oscillating simulations is 61 o 2 u w 0 18 0 06 sin t 15 2  60 all three scenarios are plotted in fig 4 finally we only apply the equations for temperature of the wet dry fuel equations and their variances when their respective density is greater than 1 1 0 6 kg m 3 4 results and discussion 4 1 proof of concept simulation results we begin with scenarios of high intensity fire placed upwind of the control volume we simulate high upwind temperature with high wind speed for all local moisture scenarios shown in fig 5 and table 1 in these simulations the mean upwind gas temperature is raised from 300 k to 1000 k over 15 s with a slow decline back to ambient temperature it is unlikely that a 1000 k gas temperature would coincide with light winds thus for this high intensity simulation we present the results for only the case of 4 m s 1 wind speed applied to all three fuel moisture scenarios 5 100 and 200 in fig 5 mean wet and dry fuel temperatures are only shown in cases with the density above 1 1 0 6 kg m 3 the blue solid line with circles indicates the mean wet fuel temperature the red line with squares is the mean dry fuel temperature the light gray line is mean gas temperature the solid purple line is the upwind prescribed gas temperature and the dashed blue and dotted red lines are the wet fuel and dry fuel density respectively shading corresponds to one standard deviation above and below the mean temperature value in all moisture fraction scenarios a fraction of wet fuel is quickly dried and dry fuel begins to burn within the first 13 s of all simulations it should be noted that there is an assumption that there are burning ashes etc present such that ignition is piloted at the onset of combustion in the fuel with a moisture fraction of 0 05 99 of the fuel is already dried i e 99 moved from the wet to dry fuel category and thus available for consumption this can be contrasted to 33 and 17 of the fuels that have been dried when combustion starts for the fuels with moisture fractions 1 and 2 respectively as shown in table 1 the maximum temperature is slightly higher in the lower moisture fraction scenarios but the time for the peak temperature differs only by 6 s among these three simulations the similarity between these three cases is indicative of the fact that the strong winds and high temperatures are very significant and they overwhelm the effects of moisture on combustion initiation it is important to remember that the prescribed high intensity conditions are associated with fire upwind of the control volume likely indicating an abundance of dry fuel in that region the similarity of the results relates to the fact that when a fire moves from one set of conditions to a new set of conditions the changes to fire behavior do not occur instantaneously time or space thus when our control volume has high moisture i e moisture ratio or fractions of 1 or 2 the high intensity fire from upstream still ignites the fuel and consumes it but the energy release proportional to the dry mass loss rate is lower in the wetter fuel cases we also note that the temperature begins to decrease after the initial reaction in all scenarios as a combined result of reduced oxygen availability leading to lower reaction rates and convective cooling after 600 s only 0 9 or less of the fuel remains in all simulations which indicates near complete fuel consumption regardless of moisture content this is consistent with observations and empirical models in high intensity wildfire during high intensity crown fire where observed in fire mean gas temperatures exceed 1000 k e g taylor et al 2004 the fraction of fine fuels less than 5 mm diameter that are consumed is high and often approaches 100 thompson et al 2020 forestry canada fire danger group 1992 call and albini 1997 stocks et al 2004 de groot et al 2022 reducing the upwind gas temperature and variance after 100 s provides enough time to examine the fire behavior simulated in the model at constant high intensity for all simulations presented here it is important to remember that holding the wind and upwind gas temperature steady and remaining steady for an extended period of time is unrealistic as any given location in a fire has a transient set of conditions as the fire approaches it and moves past thus we have chosen a simplified set of environmental conditions to expose the solid fuel to but we recognize the importance of coupling this to the full set of dynamic conditions which will be done as this solid model is connected to a similar gas model in the cfd context of firetec before the weight is put on the specifics of model results evaluating the results in fig 6 and table 2 we next examine the behavior of our proposed model for a lower intensity upwind fire scenario and a lower mean upwind gas temperature with a higher variance these conditions are selected to illustrate the behavior of the proposed model during a low intensity head fire impinging on a cell or flanking backing fire behavior where only a fraction of the gas temperature within the cell would be hot enough to initiate combustion in the dry fuel in these scenarios the upwind gas temperature begins at ambient ramping up to 500 k over 100 s and slowly decreasing as shown in figs 3 and 6 the results presented in figs 6a 6c correspond to wind speed 0 1 m s 1 figs 6d 6f to wind speed of 1 0 m s 1 and figs 6g 6i to a wind speed of 2 0 m s 1 furthermore figs 6a 6d and 6g correspond to moisture fraction of 0 05 figs 6b 6e and 6h are moisture fraction of 1 and figs 6c 6f and 6i show results for moisture fraction of 2 in simulations where the wind speed is near calm figs 6a 6c convective heat transfer is not significant enough to warm the fuel and overcome radiative losses which increase rapidly with temperature rise due to their dependence on the fourth power of temperature we remind the reader that we are not simulating radiative gains and thus the primary source of energy before combustion is convection therefore no fraction of the fuel is warm enough to begin combustion in any of the scenarios increasing the wind speed to 1 m s 1 provides enough convective heating to induce combustion when the fuel is dry however the energy sink required to evaporate water from the fuel with a higher moisture content prevents combustion from occurring increasing the wind speed to 2 m s 1 results in all moisture scenarios reaching the threshold for combustion interestingly the fuel consumption in all cases where combustion occurs is 89 or greater this result is somewhat surprising since during a low intensity burn we would expect less than 89 consumption for these idealized cases with constant winds and steady gas temperature prescription however we are not capturing the effects of cool air entrainment this can contribute to residual unburned fuel furthermore the elevated upwind temperatures persist for more than 5 min which is likely unrealistic over a 1 m spatial scale in some of these scenarios arguably a near surface wind speed of 2 m s 1 would produce a higher intensity surface fire than we are prescribing upwind especially for the driest fuel for example the canadian forest fire behavior prediction system forestry canada fire danger group 1992 an empirically based system used by fire management agencies predicts a moderately intense fire with 1 3 m s 1 forward rate of spread in dead matted grass that has a moisture content of 5 if we estimate the 10 m open wind speed to be 5 4 m s 1 using a simple logarithmic wind profile with 2 m s 1 wind at 1 m roughness length z 0 05 m and zero plane displacement d 0 65 m considering the depth of the fireline being at most tens of meters deep and the forward rate of spread the residence time for such a fire would be much less than 5 min cheney and sullivan 2008 wotton et al 2012 in this case in simulations where wet fuel is still being converted to dry fuel during combustion figs 6h and 6i the initiation of burning coincides with a slight decrease in mean wet fuel temperature since the mass of wet fuel drops quickly in response to accelerated evaporation caused by the increase in gas temperature the mean temperature of the wet fuel will fall as the warmest of the wet fuel is converted to dry fuel and the cooler portions of the wet fuel remains furthermore the sharp increase in temperature at the onset of combustion is more rapid than expected for a low intensity scenario in a cell 1 m 1 m and is due to the fact that 1 we do not simulate cool air entrainment and temperature fluctuations from surrounding cells when the upwind gas is elevated and 2 we may not be accurately capturing the oxygen concentration in the control volume this also likely contributes to a larger fraction of fuel being consumed than is expected during marginal conditions these limitations will be corrected with the coupling of the gas equations to the fuel equations in the near future where we capture the influence of adjacent cells i e more realistic advection of gas and oxygen and turbulent variations in fig 7 we show a comparison of results using an isolated cell from the current formulation of firetec and the newly proposed method the simulations presented in fig 7 are for the moderate wind speed and low intensity scenarios shown in fig 6d 6f it is important to note that the proposed method utilizes top hat distributions that shift and dilate based on the equations for the mean and the variance the distribution for the original formulation is a fixed curve similar to a gaussian and thus the interpretation of the predicted distributions must be different both sets of simulations were driven by the same initial and boundary conditions as described in section 3 furthermore both sets of equations are coupled to a gas equation and oxygen equation eqs 51 and 58 this comparison highlights differences between the current method in firetec and the need for a scenario dependent dynamic temperature distribution while dry conditions result in combustion occurring in both the original and new formulations when moisture content is high only the original formulation of the equations produces a combustion reaction furthermore when conditions are driest combustion occurs in less than half the time for the original formulation compared to the proposed formulation which is a result of the fixed temperature distribution this highlights the need for improved methods to capture the subgrid distributions of temperature and moisture which are critical for spread and ignition in low intensity scenarios combustion does not occur in either formulation when winds are near calm not shown here for brevity the main motivation behind this work is to improve the sub grid models for fire behavior simulations through the development of new closure schemes thus we proceed with examining the individual terms and closures in the proposed model specifically looking at the processes in fig 6i this simulation was selected since it highlights the most important processes in the lower intensity upwind scenarios i e slower warming and evaporation moderate wind speeds higher moisture and eventual but delayed combustion hereafter we represent all terms in fig 8 related to radiation contribution as  x all terms related to convection as  x all terms related to the heat of combustion or heat of vaporization as  x and all other terms as  x where x denotes d for the mean dry fuel temperature d v is the mean dry fuel temperature variance w is the mean wet fuel temperature and w v is the mean wet fuel temperature variance in the case where  x or  x have two terms these terms are assigned in the order that they are shown in the governing equations that have been outlined in previous sections these terms are more specifically outlined in table 3 beginning with the solid wet fuel mean temperature and variance as shown in figs 8a and 8b the mean temperature rises and the variance increase in response to increasing gas temperature at the onset of the simulation this is via convection before the onset of evaporation at 18 s once evaporation begins convection source terms  w and  w v and the sinks associated with heat of evaporation  w and  w v increase the mean temperature and reduce the variance respectively as warm wet fuel is being converted to dry fuel regarding the dry temperature results in fig 8c the mean dry fuel temperature falls with the radiative and convective cooling sinks  d and  d which exceed the source of warm dry fuel  d until the gas temperature rises above the mean dry temperature this results in positive convective heat transfer which forces a rise in dry temperature at the same time we observe a rise in the variance fig 8d of the dry fuel since the positive contribution from the convective term  d v outweighs the sink from radiation and the dry fuel mass source terms  d v and  d v respectively fig 8 d the dry variance and mean temperature continue to rise until a fraction of mass in the dry fuel temperature distribution reaches 600 k at approximately 136 s and pyrolysis combustion begins the exothermic reaction source term  d quickly raises the mean temperature of the dry fuel this is while the combined sources  d v and  d v are greater than the combined sinks  d v  d v and  d v 2 resulting in a rapid net increase of the variance as previously discussed for figs 6h and 6i at the onset of combustion we observe energy and mass losses in the wet fuel which are due to rapid heating thus the sudden drop in  w results in a momentary decrease of the mean wet fuel temperature and an increase in the wet fuel variance as warm wet fuel is quickly converted to dry fuel the mass loss due to combustion coincides with depleted oxygen and lowering of the reaction rate  d this combined with convective cooling and mass gain at lower temperatures  d and  d leads to a drop in mean temperature shortly after combustion commences similarly  d v in the variance equation falls as the reaction rate drops this combined with the sinks related to radiation  d v and cooling convective evaporative and mass gain  d v  d v and  d v 2 the variance falls as well at this stage the upwind prescribed gas temperature is decreasing and all source and sink terms are reduced in all equations until both the reaction and evaporation ceases and convection and radiation are the only contributing terms the qualitative examination of the individual terms in the reaction in the simulations with mean upwind gas temperatures ranging from 300 k to 500 k and a high variance provides a reasonable explanation for the observed fire behavior in the conceptual model presented here most of the known processes are represented in this simple scenario and the equations are stable and provide results consistent with expectations although we are unable to directly compare the results of this study with observed in fire data to determine the accuracy of magnitude and relative contributions of the individual terms the net fire behavior follows the expected trends we acknowledge the scenarios presented are unrealistic given constant or steady wind mean upwind gas temperatures and gas variance however we are able to examine the behavior of the model by removing the complexities of variations in gas temperature and wind speed due to turbulence this also enables examining the impact of each of the terms and closures presented here the final set of idealized gas scenarios presented is driven by an oscillating upwind gas temperature which alternates between 300 and 1200 k with a 60 s period as shown in fig 9 and table 4 wind and moisture are the same as outlined in fig 6 in nearly all simulations we observe a step like structure in the density as a response to the fluctuations in gas temperature as gas temperature increases the movement of mass from wet to dry accelerates and the decrease in the rate of dry mass gains coincides with the slowing of the mass transfer moreover this step like structure is evident in the mass decrease of dry fuel as well particularly in the two higher wind speed scenarios in which convective heat transfer and mixing is stronger combustion does not occur in the lowest wind speed simulations even with maximum gas temperatures upwind reach 1200 k as there is not enough mixing furthermore upwind gas temperatures only remain above 600 k for 36 s at a time before falling below 600 k in the oscillation all scenarios where combustion occurs have near complete combustion i e 2 or less fuel remains at the end of the simulation this is not surprising as sufficient mixing and very hot gas temperatures contribute to the reaction consuming fine fuels quickly we observe a rapid rise in dry temperature at the onset of combustion in the higher wind speed simulations figs 9d 9i similar to the gas scenarios discussed earlier for figs 6 and 8 however in the moderate wind speed scenario with a slower fuel response time due to reduced convective heat transfer figs 9d 9f we observe a markedly cooler peak temperature with a slower decline when compared to the high intensity scenarios table 1 and fig 5 where gas temperatures remain high because of this slower response time when combustion finally occurs the upwind temperature is already in the decreasing phase of the oscillation and so the control volume and upwind gas temperature is cooler at the onset of combustion this leads to a slowed acceleration and deceleration of the reaction rate and a lower overall maximum temperature this phase could loosely be compared to the effects of cool air advection from surrounding cells and turbulence or vorticity induced cooling as discussed by finney et al 2015 local temperatures fluctuate rapidly as a result of the effects of turbulence and buoyancy induced circulation which can be periodic in both stream wise and transverse directions since we are not incorporating these effects in the control volume boundary conditions we can expect variation in reaction rate and mean dry fuel temperature when coupling the unsteady gas equation and fuel equations in three dimensional scenarios future work that includes more accurate oxygen advection should further improve the reaction and fuel consumption rates in these simulations the lag and offset between the dry fuel temperature the cell level gas temperature and the upwind gas temperature is most pronounced in the lower wind scenario for all three moisture fractions this is due to the reduced convective energy transfer for solid to gas and gas to solid the variations in wet fuel temperature in response to variation in gas temperature similarly increases with wind speed unlike the low intensity scenario shown in fig 6 we observe a very modest decrease in wet fuel temperature during the onset of combustion instead changes in wet fuel temperature are mainly a result of gas temperature variation in the control volume 4 2 effects of gas temperature variance on dry fuel while we only present the results for a single control volume whose fuel evolution is not directly dependent of cell size although the overly simplified gas temperature and oxygen equation assumes a 1 m upwind distance the improvement of sub grid temperature distribution relaxes the current restraint on cell size in three dimensional formulations of firetec the proposed model for the evolution of sub grid temperature distribution is expected to improve the current static shape of sub grid temperature distribution in firetec while the cell size is not explicitly adjusted in the proposed model we can instead achieve a similar result by prescribing a wide gas temperature distribution this is because a fire approaching a larger cell will result in a wide range of sub grid temperatures given a small fraction of fuel and gas in the large cell would initially warm while the remainder of the cell remains closer to the ambient temperature in the next phases of this work the inclusion of advection and turbulent diffusion as well as influences on local gas temperature distributions are incorporated to account for cell size effects these will feed back to the solid phase equations presented here however we can explore this concept by modifying the prescribed gas temperature distribution shown in fig 10 in these simulations we assign a mean gas temperature of 500 k in the control volume and apply a range of variances for the moisture fraction 1 scenario under all 3 wind speed cases the value indicated by x in  g x represents half the width of the top hat distribution i e 500  g x k is the maximum temperature in the top hat distribution with  g x ranging from 100 to 300 k this figure highlights that increasing width of the gas temperature distribution results in an increase in width of the fuel temperature distribution in all wind cases regardless of the state of combustion furthermore increasing the wind speed and thus convective heat transfer increases dry fuel temperature variance in both combustion and non combustion scenarios this is because stronger convection will result in a stronger influence of the gas temperature on solid temperature currently firetec simulations require significant computational resources on high performance computing systems due in part to restrictions on grid spacing a single simulation in a small domain e g 200 cells 200 cells 41 cells requires tracking over 1 6 million cells and within each of those cells computing or storing upwards of 40 variables at each time step with time steps as small as 0 001 s increasing the cell size has the potential to reduce computing requirements without compromising modeled fire behavior and will be examined in future work 4 3 field experiments and model comparison on the afternoon of november 15 2017 a series of experimental fires were conducted between 1700 and 1800 utc on the university of georgia campus providing data for the evaluation of the overall performance of the proposed model desai et al 2021 for these experiments the burn area was 2 4 2 4 m and the fuel bed consisted of pine litter with a 4 moisture content and a fuel load of 0 37 kg m 2 approximately 10 cm deep the weather station recorded wind speeds of 0 635 and 0 474 m s 1 at 1700 and 1800 utc infrared and visual imagery were obtained using a forward looking infrared sc660 thermal imaging system flir systems inc boston ma usa and a gopro hero3 mounted to a 7 m tall aluminum tripod above the burn area facing downward the flir system has a resolution of 0 8 cm and a focal plane array of 640 480 pixels at the 7 m distance the temperature data was collected at 1 hz for the range of 573 to 1773 k and three different ignition patterns were completed a point source ring source and 2 parallel lines four 1 1 m cells were segregated for each experimental fire and temperature distributions within each cell were analyzed at each second additional details can be found in desai et al 2021 strother 2020 precise descriptions of the gas temperature and associated variance are unknown for these experiments given the lower intensity of the fire we approximate the upwind gas temperature as t g u w 300 500 tanh t 30 100 400 tanh 400 t 1000 which allows for a slow gas temperature rise the gas temperature variance like the idealized scenarios previously discussed is assigned as a function of temperature with a variance t g t g 60 t g 300 6 2 fuel bed temperature is initialized at 292 k and we estimate the ground level winds as u 0 6 m s 1 and tke is assigned the same as described in section 3 which used for the mixing limited reaction rate near ground the completed simulation compared to the observations is illustrated in fig 11 where the dashed lines are the mean temperatures of the various burning observations within the 1 1 m cell the red solid line with squares indicates modeled mean dry temperature the blue solid line with circles is mean wet fuel temperature the gray line is gas temperature and shading indicates one standard deviation above and below the mean based on the dry and wet modeled variances the observed data is only presented for values above 773 k since there are multiple fire observations we align the model data with the observations at peak temperatures furthermore it is important to note that flir temperature observations are from the two dimensional surface layer and do not capture variations in temperature below the surface the proposed model however accounts for three dimensional temperature distributions with that said unsurprisingly the observed fuel temperatures are significantly hotter than the modeled mean temperatures however if we truncate the simulation data from the modeled mean temperature and variance to values in the distribution greater than 773 k we observe a significant improvement in the alignment between modeled and observed temperature especially after the peak temperature as shown by the solid black line distinct differences include a more rapid modeled rise in temperature during the initial combustion phase with a slightly premature drop in temperature as fuel is being consumed and a slower reaction rate as previously discussed in the idealized scenarios we hold the upwind gas temperature nearly constant with gradual increases and decreases thus we are not capturing the turbulent gas temperature variations that would influence fire behavior these limitations are not associated with the final model but instead are the result of the overly simplified test cases presented here fig 12 illustrates the modeled variance and the observed variance in this figure the red solid line with squares is the modeled variance the light gray line is the prescribed gas temperature variance and the dashed lines indicate the ensemble of variance observations while the observed variances are approximately half to three quarters of the modeled values in red this is not necessarily a case of overestimation by the modeled variance rather it is the result of restricted range of observed temperatures measured by the flir it is reasonable to assume that temperatures within a single cell solid would be below 500 c during a low intensity fire which is not accounted for in these observations thus the results show fictitiously low variance furthermore the three dimensional distribution of temperatures which are not accounted for in the flir observations would further increase the variance this is because deeper fuel initially has lower temperatures compared to the surface temperature which would be exposed to convective and radiant heating ahead of the flame front as discussed in the previous figure with a mean temperature adjustment we similarly observe a more realistic modeled variance by excluding fuels below 773 k while magnitude of the modeled variance does not reflect the observed variance presented here the overall shape and behavior of the variance aligns with the shape and behavior of the observed variance given the strong influence of the gas temperature variance on the fuel temperature variance improvements to modeled gas temperature will improve the overall model performance 4 4 future work the focus of this manuscript is on describing the governing equations for wet and dry fuels and their temperature evolution our future plan currently ongoing is to focus on the development of equations describing the evolution of the temperature variations for the gas phase based on a similar approach as described here these new equations for the gas phase will then be coupled to the set of equations for the wet and dry fuels which should improve the modeled energy exchange between wet and dry fuel and surrounding gas this has a significant influence on fire behavior as we have previously shown this energy exchange is one of the essential components contributing to the self determining nature of firetec as such considerable effort is being put toward these next developments current assumptions neglecting triple correlation and higher order terms for multi variable correlations are reasonable as a first approximation for this proof of concept discussion similarly the assumed top hat distribution is physically unrealistic but allows us to remove many of the complications associated with an evolving distribution shape however future work will incorporate a more realistic probability distribution and return to the triple correlation terms that may have a non negligible contribution while a normal distribution in some cases is warranted it cannot be assumed universally valid as such we are working towards capturing the sub grid dynamic distribution changes over time next steps will also include triple correlations in the gas equation to account for skewness in the distribution we recognize that the constants assigned initially in this study may be adjusted in future work we anticipate that coupling a variable gas equation model to the solid fuel model described here will improve the convective heat transfer exchange and provide us with the opportunity to better evaluate and assign the constants c h and c t d f thus we will return to these constants in future studies furthermore while we currently apply a spatially segregated approach to the fine fuels i e an individual fuel particle s moisture and temperature are always homogeneous fuel is distributed evenly within the cell and heating cooling evaporation and combustion can occur in a fraction of the cell leading to changes in the sub grid temperature distributions the current approach can be modified to a shell type paradigm in this new approach fuel particles are initially heated on the outer layer or shell while the inner core of the particle may remain cool and wet this new formation will allow gradients in fuel moisture and temperature which is the first step towards the development of burning in thermally thick fuels and smouldering combustion processes which are critically important to smoke and emissions production and many other fire effects 5 conclusion a new model for the sub grid processes related to temperature and moisture variation in wildfire behavior models is presented other wildfire behavior models have succeeded in simulating high intensity fire during extreme conditions where the relevant length scales are large enough to be resolved and the impact of sub grid variations is minimal however the performance of these models suffers when conditions are less extreme and the length scales are small the aim of the work described here is to improve the overall performance of these models particularly during lower intensity fire where the sub grid spatial and temporal variations have significant impacts on fire behavior the first set of simulations presented are idealized scenarios with prescribed upwind mean gas temperature constant wind speed and moisture content in the fuels these simulations highlight the behavior of the model without the complex and non linear effects of variations in wind speed and gas temperature while we couple the fuel equations to an evolving gas temperature equation the simplistic nature of the prescribed upwind gas temperature and the lack of cool air and overly simplified oxygen advection results in limitations on model performance in nearly all simulations with combustion an unnaturally rapid increase and decrease in temperature occurs as a result of this simplified advection aside from this deficiency the model is able to effectively simulate the evolution of unresolved moisture and temperature and the resulting fuel consumption when conditions are both extreme and marginal furthermore we illustrated the increased flexibility with respect to grid size in the proposed model through the influence of the gas equation with the dry fuel equation we illustrated that wider variations in gas temperature directly result in wider variations in fuel temperature while we are unable to evaluate the performance of each term and closure in these new equations with observations the overall behavior of the mean temperature and the variance equations is compared to a set of experimental burns conducted in 2017 at the university of georgia we present the results of a simulation using the known fuel load moisture content and approximate wind speed during these experimental fires for a 1 m 1 m cell the in fire data is limited to temperatures above 773 k on the surface layer only and so adjusting the model results to calculate the mean and variance in the fraction of modeled fuel hotter than 773 k is comparable to the observations the major limitation of this comparison lies in the absence of modeled cool air entrainment grossly simplified oxygen advection and the fact that observations are on a single plane but modeled temperature and fuel is 3 dimensional despite this the model performs adequately this is the first step in ongoing work which aims to couple the equations described here with new equations for the evolving gas temperature this new coupled system of equations will ultimately be incorporated into the three dimensional version of firetec to improve the overall performance of the model particularly during marginal burning conditions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper funding this work was partially supported by the strategic environmental research and development program usa project numbers rc 2643 rc20 c3 1382 and los alamos national laboratory ldrd program usa project 20220024dr fighting fire with fire enabling a proactive approach to wildland fire 
25399,modelers are proposing sets of better practices to improve modeling processes and outcomes we need to evaluate how they perform in practice i use autoethnography to describe four of my modeling and interdisciplinary training experiences and test how i applied a specific set of modeling best practices proposed in eitzel 2021 exploring whether how they resulted in processes whose outcomes were more relevant trustworthy and just than they would otherwise have been the practices did improve the outcomes of my models especially triangulating between multiple data sources and perspectives improving transparency through better descriptions of methods and data and engaging in community based modeling some practices mutually reinforced each other though balancing transparency with data sovereignty was critical when working with indigenous communities i invite other modelers to follow this example analyzing their own experiences using autoethnography testing my definitions of better modeling and proposed practices or substituting their own graphical abstract keywords moderate autoethnography data science best practices community based modeling correction of published research interdisciplinary training science and technology studies data availability relevant data underlying this article are available in the appendix materials 1 introduction beset by multiple global crises including biodiversity loss climate change social inequity and the covid 19 pandemic where do we turn for information to guide adaptation and action i would argue that in many cases we make individual and collective use of models which i broadly define as a representation of something of interest 1 1 nearly all definitions in the oxford english dictionary make some reference to representation 2021a though my focus is on quantitative models typically rendered in a computer i also encourage modelers working on qualitative or physical representations to consider the content of this paper because how we make models can strongly impact how well we can address pressing concerns sarewitz et al 2000 many modelers have proposed sets of individual modeling practices 2 2 note that by practice i mean the actual application or use of an idea belief or method as opposed to the theory or principles of it oed 2021b sometimes framed as manifestos e g derman and wilmott 2009 that in aggregate could improve both the larger social and technical processes involved in modeling as well as the outcomes of these processes with respect to their desired goals to illustrate the relationship between these concepts consider a goal of ensuring that a model addresses an ongoing inequity in resource access an improved outcome would be greater equity and the associated process could be participatory modeling the practices might involve including many voices in the design of the model the model produced by this process might be a better representation of the system in question which could in turn therefore have greater impact on the problem it was created to solve meeting the goals of the modeling process as a practicing modeler working to improve my modeling process and outcomes i created my own modeler s manifesto 3 3 a summarized version is available in eitzel 2022 for the detailed version see eitzel 2021 i and many modelers are actively engaged in implementing these manifestos or better practices however i would further argue that if we are motivated by pressing societal problems we need to concretely assess the effectiveness of these practices this involves both defining the criteria for assessment given the goals of the modeler what constitutes a better modeling process and or outcome and choosing a method for assessment in attempting this i have found that each modeling situation is quite unique and the context strongly influences how practices can be applied i have therefore come to value autoethnography a form of systematic self assessment as a method for evaluating modeling manifesto implementation in this paper i autoethnographically assess my own manifesto by outlining criteria for assessment and then systematically applying them to four of my modeling and interdisciplinary training experiences investigating whether and how following my proposed manifesto practices actually contributed to improved modeling processes and outcomes the goals of my analysis are 1 to demonstrate a way to evaluate modeler s manifestos in which others could substitute their own criteria modeling practices and or modeling cases fig 1 and 2 to assess both my modeling experiences and my particular collection of better practices in the remainder of the introduction i first provide background on autoethnography as a method for those unfamiliar with it then in order to make this method of assessment modular see fig 1 and in line with good autoethnographical methods i outline my own values that are salient to my approach to modeling then summarize the manifesto practices i will be assessing and establish my criteria for better modeling personal values influence many modeling choices so describing mine allows a reader to note points of agreement or divergence and to take these into account when choosing whether to apply my criteria practices or assessment method to their own work i end with the research questions that guided my analysis 1 1 autoethnography of modeling autoethnography is a method of observing and reflecting on one s own research and learning processes ellis et al 2011 it has a long history in a variety of disciplines with a dedicated journal founded in 2020 adams and herrmann 2020 autoethnography can range from deeply qualitative narrative based strategies le roux 2017 to highly quantified methods of self assessment anderson 2006 with all possibilities in between as well as combinations of multiple strategies stahlke wall 2016 autoethnography as practice is one way to make sense of your own experience and to understand the mechanisms and values behind what you do newman and farren 2018 and how it impacts others simultaneously autoethnography as research product can be a venue for sharing what you have learned and can therefore be a way to make change and collectively improve practice ellis et al 2011 le roux 2017 modelers are already engaging in autoethnography in a variety of forms including evocative strategies reporting and reflecting through a narrative levin 2019 as well as more quantified analytical strategies newman and farren 2018 analytical autoethnography of modeling can range from simple reflections by a research or software development team member mulvenna et al 2018 bodo et al 2018 possibly comparing their journals and notes with internal reports and meeting minutes caesarius and johansson 2013 all the way up to a thorough analysis of one s time tracking and systematic coding of personal journals newman and farren 2018 my adoption of this technique is therefore far from unusual and autoethnography has great potential for modelers to better understand their own modeling paths lahtinen et al 2017 and habits babel et al 2019 revealing what they ve done and why they did it this practice can therefore help to assess the transformative potential of modeler s manifestos 1 2 personal values and modeling choices in addition to allowing the reader to evaluate overlap between my values and theirs describing salient aspects of my standpoint and background walter and andersen 2013 or situating myself haraway 1988 also enables them to understand what biases i may have in my modeling practice and in evaluating my own work this self disclosure is good autoethnographic practice because the author is both the subject of study and the instrument used to conduct the study i give a much more detailed accounting of my background and how it shapes my values in appendix a but here i focus specifically on how my values influence my choices regarding modeling epistemologically i identify with an objectivist sense of an external reality and place a high value on empiricism as a way of knowing that said i define empiricism broadly and recognize the impact that our perspectives have on what we perceive and believe this means that i tend towards both qualitative and quantitative ways of validating models and also feel the importance of relativism and the influence of power dynamics between people on what modeling gets done and how from an equity standpoint i strongly value just distribution of benefit and equitable protection from harm faced with the global crises listed above i put a high priority on modeling that supports justice and equity in the face of these problems and i am therefore most interested in applied modeling projects including this paper s project of evaluating modeling manifestos 1 3 manifesto themes and practices in my search for guidance on how to improve the outcomes of my modeling practices i pursued interdisciplinary science and technology studies sts training and read other modelers thoughts on better modeling practices see appendix a for a brief summary in the end i synthesized what i learned into a manifesto with nine practices grouped into three themes largely grounded in donna haraway s situated knowledges 1988 context collaboration and justice see eitzel 2021 for detailed descriptions of the themes and practices and table 1 for a summary my values on empiricism and relativism are both reflected in the context and collaboration practices in which i emphasize the need for outside validation but encourage it to come from multiple perspectives my values on justice and equity underlie the third theme s focus on power dynamics and consideration of modeling impacts 1 4 criteria for better models in order to assess my manifesto i asked myself what constitutes a better model modeling process or outcome the criteria i arrived at derived from my values and are also grounded in philosophy science and technology studies and modeling literature see below for that reason they are not entirely independent of the manifesto practices i proposed which also derive from my values and studies of better modeling though from an earlier round of analysis see eitzel 2021 one of the purposes of evaluating my modeling experiences through the lens of the manifesto practices and against these criteria is to examine how much overlap there is between the criteria and practices when they are actually applied whether there are any surprises and whether and how different manifesto practices worked together or interfered with each other in meeting those criteria below i outline the three criteria i use in this paper noting when they conceptually connect particularly strongly with the manifesto practices listed above 1 4 1 trustworthiness increasingly we appear to be living in a post truth society higgins 2016 amid a crisis of reproducibility in science spiegelhalter 2017 inspired by my value on empiricism i established one of my criteria on the idea that we need to be able to believe what a model is telling us in order for it to serve our needs philosophically trust in others testimony can be justified by ensuring the reliability of belief forming process pritchard 2004 criteria that could assist a novice in assessing the reliability of an expert s testimony include checking their testimony against other experts experts making their own biases clear and experts demonstrating a positive track record goldman 2001 this criterion echoes manifesto practices around triangulation and detailed descriptions of modeling processes including describing our own biases as well as drawing conclusions consistent with the epistemic restriction of the method e g correlation does not imply causation reproducible and participatory modeling processes may also be perceived as more trustworthy 1 4 2 relevance as box is often quoted all models are wrong but some are useful 1987 meaning that representations are never perfect but they can still generate relevant and actionable understanding of the entities they represent for some purposes models may be simplified e g for teaching or to explore theory while others can be quite complex in many different ways e g for accurate prediction of a specific system or mechanistic understanding of a generalized system holling 1966 breiman 2001 efron 2001 and often no single model is appropriate for all purposes shackley et al 1998 i propose that relevance has two parts first the modeler specifies the goal that the model is intended to achieve and second they assess whether the model is appropriate to and achieves that goal given my values on applied modeling that addresses global crises my specific criteria for relevance relate to actionable results for policy makers environmental managers and community members this definition of relevance also points towards participatory processes for determining modeling questions 1 4 3 justice a number of authors have recently pointed out that many models may exacerbate problems they were ostensibly designed to solve o neil 2016 reinforcing existing biases in data and thereby harming vulnerable populations who lack a political voice to oppose unjust conclusions mah 2017 eubanks 2018 grounded in my values around justice and equity i determined one of my criteria for better modeling would be justice i subscribe to rawls framing of justice as fairness in the distribution of rights and duties towards and rewards from human cooperation rawls 2009 i further incorporate reardon s exhortation to keep our assessment and pursuit of justice from relying on universals and to always stay grounded in the specifics of a given situation including its history reardon 2013 i therefore define justice in line with these ideas ensuring equal benefit and avoiding unequal harm keeping in mind the histories of unequal benefit and harm this criterion aligns well with the three practices in the manifesto framed around justice though it also aligns with the idea of transparency in modeling for example via data biographies practice 1b in table 1 it is no accident that justice became both a set of practices and a criterion though there is an element of critical theory behind many of the other practices it is most clearly reflected in the justice practices the modeler s values are particularly important here for example a different definition of justice could result in very different modeling processes and outcomes the third criterion and the third set of modeling practices are therefore both firmly grounded in critical theory and are decidedly social rather than technical however social and technical aspects of modeling can mutually support each other in some cases eitzel et al 2022 so one point of curiosity is whether and how these practices interact 1 5 research questions the questions i seek to answer are the following 1 does applying the manifesto practices make my modeling better based on the above definition of better 2 how do these practices support each other or interfere with each other especially the technical practices and the critical practices 3 how could the manifesto practices or criteria for better models be refined i briefly outline my methodological approach to autoethnography how i selected modeling experiences to use in testing the manifesto and how i established qualitative validity of the results 2 methods to describe and analyze my experiences i used a moderate autoethnographic approach stahlke wall 2016 mixing evocative narrative via descriptive vignettes of my modeling experiences with systematic analysis of how the modeling manifesto practices affected my modeling processes and outcomes le roux 2017 i selected four of my modeling and interdisciplinary projects from 2010 2020 see details in appendix a on the selection process 1 correcting published models from my phd dissertation on forest tree growth and survival 2 assembling a dataset and planning a model for oak tree population dynamics 3 a combination of various interdisciplinary training experiences 4 my experience as a postdoctoral scholar learning critical theory and qualitative analysis techniques and applying them to community based modeling with zimbabwean farmer researchers from an analytical angle these vignettes cover a range of different types of projects and models which can be fruitfully compared from a narrative angle the vignettes form an arc representing different stages of my application of the manifesto ideas i constructed the vignettes largely from memory with some reference to personal journals notes on modeling projects and email exchanges with collaborators for some vignettes i used the consolidated criteria for reporting qualitative studies coreq checklist to ensure i included enough detail tong et al 2007 2 1 analysis of vignettes i relied on the practice of writing itself as a mode of inquiry in its own right in which writing and research are co produced richardson 2003 gibbs 2007 and not assuming that writing only documents what is already known st pierre 2015 i coded my modeling experiences and answered my research questions by writing and editing drafts and presentations of this paper in each draft i iteratively reviewed each vignette with multiple rounds of deepening questions 1 which of the manifesto practices did i apply and how 2 what lessons did i learn from this experience 3 to what degree did the manifesto practices improve the outcomes of my models via increased trustworthiness relevance and justice 4 how do the practices reinforce each other or interfere with each other 5 which of these practices were most important was the manifesto a useful tool and will i continue to use it in my own modeling practice in the future 6 where do the manifesto practices themselves need more careful definition in order to be used in an assessment i coded the vignettes using the manifesto practices from eitzel 2021 as a concept driven deductive coding tree with three overarching themes and nine individual practices as well as coding each vignette for trustworthiness relevance and justice as defined in the introduction i also allowed for open inductive coding regarding lessons learned saldaa 2021 with each draft of this paper s results and discussion sections i added a layer of questions i first wrote the sections that describe which practices were applied coding each vignette based on presence or absence of each practice and then in the next edit wrote the section that highlighted lessons learned and so on altogether i wrote five major paper drafts with associated rounds of analysis and writing see appendix a to code whether the vignettes describe more trustworthy processes where possible i noted the ways in which others reacted to the models and other research products in addition to my own sense of trust in my results to code for relevance i investigated the applicability of the research to related questions of concern for larger communities than myself e g scientists and practitioners involved in conservation or management of natural resources to code for justice impacts i asked whether the research process served to correct a current or historical inequity and specified whose inequities i then systematically reviewed the way each pair of manifesto practices potentially interacted for each vignette details given in appendix a the last two questions were higher level summative analyses synthesizing the answers to the other four questions 2 2 credibility consistency and transferability concepts of validity replicability and generalizability are framed differently for qualitative analysis than for quantitative analysis validity in a quantitative analysis shifts to become credibility in a qualitative analysis merriam and tisdell 2015 to make my analysis credible i sought peer review of my work from colleagues in both social science and modeling and triangulated between different cases vignettes with differing applicability of the manifesto practices and different project types quantitative replicability is recast qualitatively as consistency merriam and tisdell 2015 because many qualitative interpretations are possible with the same data independent replication is often not practical and an alternative interpretation may not invalidate the conclusions of the study therefore any individual interpretation should be self consistent rather than replicable through peer review i checked that the observations in my results and discussions section were consistent with the data given in the vignettes and i gave details on their selection and analysis in appendix a in addition researcher self reflexiveness which is also a key autoethnographic principle can help evaluate consistency of data and findings i included a description of my own biases and background in the introduction and appendix a to help readers assess the consistency of my conclusions quantitative generalizability is reframed as qualitative transferability or whether a reader feels that the results apply to them merriam and tisdell 2015 i facilitated this by giving a thick detailed description in the vignettes that my peer reviewers used to evaluate the study s applicability to their own experience i also presented a range of different cases vignettes with different types of models and projects and i sought feedback on them from both modelers and social scientists in effect a more diverse sample in autoethnography in particular validity can be assessed by the following criteria subjectivity that the self is primarily visible in the research self reflexivity self awareness self exposure and self conscious introspection are shown in the work resonance there is commonality between researcher and audience credibility the research is permeated by honesty and contribution the work teaches informs and inspires see le roux 2017 for more details i elaborate how this work meets those criteria in detail in appendix a 3 results after each of the four vignettes i analyze what practices were applied and how effective they were in making my modeling more relevant trustworthy and just vignettes are set in italics to distinguish them from their further analysis in the final section i compare and contrast the vignettes 3 1 vignette 1 the art of modeling tree growth and survival in 2008 i could barely contain my excitement as i started a phd program in environmental science policy and management at uc berkeley i had been studying physics and seismology and discovered that i loved doing research but i had found the topics of my research to be too abstract simulating bose einstein condensates hurt et al 2006 and analyzing seismic noise eitzel tanimoto et al 2008 i wanted to do research that addressed the urgent issues of the day at the time what felt most important to me were topics around energy sustainability and ecological conservation i d also felt that simulation and time series analysis were useful skills to have but that statistical modeling methods were essential to work with the kinds of messy real world problems that seemed most pressing so i enthusiastically dove in to studies on ecology statistics remote sensing and in particular forest and landscape modeling but as i progressed in learning new analysis methods over the next several years i found myself increasingly aware of how sensitive they were to my choices as the analyst as it turned out i was learning the art of modeling one particularly tricky technical aspect of the statistical models i was creating for my first two dissertation chapters was the question of adequate markov chain monte carlo mcmc mixing from a technical standpoint mcmc estimation of statistical parameters is considered valid if the algorithm has tried values that spanned a reasonable range for that parameter and did so in a relatively random and evenly distributed way ways to assess this good mixing varied from qualitative it should look like a caterpillar or like noise paraphrased from various peers and advisors to quantitative you should use a gelman rubin diagnostic to compare two different mcmc chains with different starting points summarized advice from a peer reviewer even bayesian modeling textbooks indicated that model checking is a delicate balance of technique and judgment guided by the applied context of the problem gelman et al 2014 in the end i used both qualitative judgments and quantitative metrics to decide when mixing was adequate fig 2 however this experience among others led me to adopt an overall strategy of conducting sensitivity analysis to my modeling choices i was also uncomfortable with the number of judgment calls i had to make leading me to be cautious with my conclusions in my dissertation chapters on repurposing forest inventory data to study tree growth and survival i tried a variety of models with different variables included or excluded and drew conclusions only about the results that were robust across all of them this sensitivity analysis meant that it took me longer to generate results and write papers than it otherwise would have i also tended to focus more on the methods and less on the results i was ecstatic that even in spite of that i was able to get both chapters published in respectable ecological journals during and very soon after completing my phd eitzel et al 2013 2015a imagine my horror then when i discovered in late 2015 soon after the papers publication that i had made a data processing error back at the beginning of my graduate work when i was still a novice at the programming language i was using after discovering the error i wrote in my personal journal trying to encourage myself i know everyone makes mistakes like these and what sets you apart is how you handle it and the world needs to see that this happens to science and we need to be honest about it so i tried to put aside my fears about potentially having to retract my only two first author analysis papers a huge blow for an early career researcher fortunately both my phd and postdoctoral mentors were extremely supportive of me correcting my papers validating my perception of scientific culture as bravely self correcting i fixed the error and reran all my analyses a process eased by my copious notes and programming scripts which allowed me to re do the work in about a tenth the time it had initially taken some of the results did shift but i was deeply relieved to discover that i had been sufficiently broad in my initial conclusions that the basics did not change my cautious instincts had served me well fortunately the journal editors agreed with me and we published errata correcting the two papers ecological applications 2016 ecosphere 2016 no retractions necessary i was also relieved that i had taken a similar strategy for the third chapter of my dissertation focusing on presenting methods doing a sensitivity analysis in this case to the spatial resolution in a remote sensing project with data unrelated to the other two papers eitzel et al 2015b and making conclusions broad enough to be defended by the results across the sensitivity analysis this whole experience made a strong impression on me that i had been right to be appropriately conservative with my conclusions 3 1 1 manifesto practices applied in the art of modeling because this story comes early in my narrative arc before i had begun studying critical theory and social science frameworks most of the manifesto practices that speak to it are in the context category even my attempt at triangulation was a sensitivity analysis given that i did not have access to other datasets to use to check my results i did write many lengthy appendices on each of these projects which served at least some aspects of the data biography largely extended methods which might help someone to re purpose my work or learn how to do the method themselves by far the clearest message i learned from this experience was that it was essential that i had been conservative in my conclusions because that meant limited impact from needing to correct the published work in reviewing the experience to describe it here i realized i had used both qualitative and quantitative assessments but this is not reported explicitly in any of the copious appendix material 3 1 2 effectiveness of manifesto practices in the art of modeling the biggest success of the manifesto s practices for this case is the trustworthiness of the models due to my use of triangulation in the form of sensitivity analysis and especially to the conservative claims i derived from them even in the face of errors and corrections editors were willing to simply publish errata rather than retract the work i also acknowledge that there may have been additional influences on their editorial decisions however in reviewing the experience i realized that i had failed to engage sufficiently with the forest managers i was working with to ensure that these models produced knowledge that they might use in managing mixed conifer forests in california the questions were derived from ecological topics around population dynamics and individual tree growth and survival i also utterly failed to engage with any of the justice issues around the management of these forests the models themselves were very focused on the ecology of the trees at the time i was only just beginning to learn about the importance of non timber forest products and the ways indigenous peoples in california had been specifically disenfranchised with respect to these ecosystems and culturally important species norgaard 2019 by only modeling timber species was i putting more intellectual and financial resources into a single way of looking at the value of the forest of course if i had addressed any justice issues in this work the cost of errors and corrections would have been even greater this underlines the importance that if one is to pay attention to impacts and implications of one s modeling both in terms of relevance and justice the model s trustworthiness is paramount 3 2 vignette 2 ethnography of data driven oak conservation during my graduate work at berkeley i developed a kind of niche skill of re purposing legacy ecological data many such datasets were getting long enough to do analyses that might reveal time trends particularly of interest as global change continued to be a pressing concern i was surrounded by ideals of measuring everything in order to manage ecosystems better for example the founding of the us national earth observation network 4 4 https www neonscience org about overview history and as data science became more and more fashionable i began pitching myself as an analyst who specialized in making use of what we already had high variety big data collected over multiple decades by a large and constantly changing set of people with potentially varying methods a particular passion project at the time was trying to synthesize a wide range of field research on oak tree populations in california there had been widespread concern since the 1970s that there was a recruitment problem in which there were many oak seedlings and adequate numbers of adults but not many saplings griffin 1971 1976 the reasoning went if seedlings could never get to the sapling stage would we ever have new adults but with the accumulating ecological data we had in the early 2010s surely we could actually construct a population viability model this would involve integrating data on growth survival and reproduction for a variety of oak life stages seedlings saplings and adults if i could account for the oddities of different datasets we could combine them to get a sense of whether populations were declining or increasing and which aspects of growth survival or reproduction in which life stages impacted that conclusion the most so in summer 2009 i tried asking researchers at a field research station if they had data on various demographic aspects of california blue oak quercus douglasii my initial inquiries made over email were not very successful another researcher a postdoc working at the research station was later able to inquire much more successfully and the oak researchers were willing to share their data with her and then with me at that point i was able to ask these researchers about other data they knew of until i was working with six different datasets i noticed that this process of asking people to provide information and to refer me to other researchers snowball sampling and especially the importance of physical presence and rapport building bore much more resemblance to ethnography than modeling so now i had the data along with papers and reports describing methods a key part of my re purposing approach was modeling the observation process as well as the growth survival or reproduction of the trees but as i studied the materials shared with me i realized that they didn t contain enough detail for me to model the observation process i pined for the extensive appendices i d published alongside my dissertation chapters if only the researchers had better described their methods i thought but they hadn t so instead i found myself essentially interviewing them via email asking questions about what they d done and how and sometimes why fortunately at this point they were quite willing to share these details and the challenge was simply to ask the right questions and iterate until i had the information i needed i also asked them questions about potential data errors sometimes they remembered what happened or used their knowledge of the species to answer e g saplings don t grow that fast and sometimes they didn t know and i was left with the decision of whether to throw out a potential error or keep it in the analysis i also found myself working to interpret both quantitative data and qualitative descriptions and narratives about the methods mixed methods analysis one particularly vexing issue i discovered was that measurement methods for tree size differed from study to study to construct the population viability model i needed to assess tree growth as a function of diameter but as i worked to bring the datasets together i noticed that the diameter measurement varied systematically between data on seedlings versus adults looking into it further i found that seedling diameter was measured with digital calipers at the soil surface while adult diameter was measured at breast height approximately 1 3 to 1 5 meters above the soil surface with a measuring tape a difference that became clearest to me when i was in the field fig 3 though this is ostensibly a measurement issue because i planned to explicitly represent the measurement processes in my statistical model it became a model construction issue as well and based on the limited description in the reports whose authors assumed readers understood the methods it took direct experience for me to truly understand what i had to model and indeed this difference in measurement processes made much more sense to me after i d been to the field station to gather some additional data for a different part of the model during that visit i also saw how hard it was to locate seedlings and how difficult it was to make judgment calls about where to measure their diameter and how this contrasted with the adult diameter measurements in the end i lamented how much time i d taken in gathering the data and metadata i ran out of funding to finish the analysis and my earlier modeling training had not adequately prepared me for working with this particular set of high variety big data i essentially needed qualitative data gathering skills i had to learn on the fly and at the same time it became clear to me that even if i could eventually do an analysis of population viability i wasn t sure i d feel comfortable making recommendations to decision makers about oak management given all the idiosyncrasies in the analysis i felt woefully unprepared to address these gaps 3 2 1 manifesto practices applied in ethnography of oak conservation i was definitely engaged in creating a data biography practice 1b from table 1 for the oak population dynamics datasets and i was also triangulating practice 2a between the written accounts in the reports and metadata and the accounts i elicited by asking the researchers questions in order to get a clear picture of what their data said about oak demography the model i had intended to make would have incorporated a sensitivity analysis that could have helped to frame uncertainty as openness practice 2b guiding further research i needed to develop qualitative research skills on the fly and to the degree that i succeeded i was able to build the needed data biography this was only possible due to the interdisciplinary fluency practice 2c i had already built if i had been able to create the model the impacts and implications of the research would very much have been in my mind practice 3b 3 2 2 effectiveness of manifesto practices in ethnography of oak conservation though there is no actual model in this vignette because i was never able to create it i had many plans for what it would look like i did think about what the impacts of the model might be because i had a personal interest in oak conservation i was much more connected to the community of researchers and others who cared about managing the trees if i had been able to make the population viability model it would very likely have had use for these communities i would have taken care to ensure its trustworthiness through many of the practices that i had already employed in my earlier forest modeling project sensitivity analysis making conservative conclusions and so on it was also clear that there was a trustworthiness to the data itself that was lacking in the form of missing data biographies and if i had not engaged in what amounted to qualitative research to create the data biographies the trustworthiness of my model would have suffered to the degree that i could following the practices of the manifesto did make the data and possible future model more trustworthy and more relevant however i did not engage with any justice issues that might emerge from oak conservation actions that could be based on the hypothetical model again the story of disenfranchisement of indigenous people of california is relevant here particularly due to the importance of acorn as a key food staple and cultural resource sowerwine et al 2019 this comparison makes it clear that some of these better modeling practices can help one consider impacts and implications and make the model more relevant and trustworthy but not necessarily more just here i was more engaged with the community of oak researchers but not with land managers or indigenous people so i would not call the work community based if i had created the model i might have reached out to these audiences with the results but that would still have been a fairly limited kind of engagement 3 3 vignette 3 challenges and opportunities of interdisciplinarity 5 5 i have been involved with a wide variety of interdisciplinary training experiences during my schooling see appendix for details i began to notice patterns in the ways the groups formed developed and dispersed in this vignette i created a composite of these experiences in order to protect the identity of any one program or collaborator relational ethics in autoethnography ellis 2016 and to distill the commonalities murchison 2010 my colleague e described in the vignette is no one person but is a combination of myself and a variety of colleagues over the course of many years after my experiences in working with legacy ecological data particularly in the context of decision makers using the results i d been particularly keen to seek out interdisciplinary training i needed a better grounding in qualitative social science data gathering and analysis methods as i d learned the hard way with the oak study and it had become increasingly obvious to me that for really pressing concerns understanding the ecology was only half of improving sustainability human management of ecosystems was at least as important to study and understand and in fact the two aspects interacted and influenced each other so i made extra time in my schedule one school term to join an interdisciplinary group whose entire purpose was to tackle these complex issue based kinds of questions initially i noted differences in vocabulary between my training and that of my colleagues sometimes even different definitions of the same words my favorite was comparing the different uses of the word community between ecologists and social scientists but i was ecstatic about the new ideas i was exposed to in the group including fundamentals outside my discipline that i could immediately see application for in my own work i was the kid in an intellectual candy shop i couldn t get enough of the interdisciplinary confections all around me suddenly the issues around environmental management that i d previously felt were intractable seemed more amenable to solutions when we were discussing it in a team with different backgrounds one person was more theoretical while another was more applied one person thought about the problem from a relativist perspective embedded in the cultures and history of the system while another came at it from an quantitative measurement oriented angle it seemed to me that the discussions were most fruitful if only more of academia could function this way i thought i was looking forward to the next event in our schedule when i got into a discussion with my friend e who had also been in the group they thought they wouldn t be joining the next event they told me at first i was confused by this reaction i d heard them participating in the discussions just as enthusiastically as i had surely they d gotten new insights as well sort of they told me but they really didn t have time for the group next term too much of a teaching load and their mentor wanted them to get one of their projects ready to submit to a conference with a close deadline saddened i said wistfully that perhaps just one event every few months might work i d thought e s contributions had been so key to moving our discussions to a generative place e shook their head hesitated for a moment and then told me that they d found the discussions of their discipline to be rudimentary not novel at all and pretty boring actually now i was really surprised i d thought they d made such insightful comments e told me that they felt they spent a lot of the time teaching us the basics of their discipline and they already had to do that with the undergrads three hours a week and besides that e told me with a sigh maybe i was open minded but not everyone in our group was so welcoming of challenging their ideas about how knowledge is made even if people were trying to be open minded some of them were just more confrontational when encountering differences some people had been making generalizations about e s discipline and methodological background that were at best unflattering and simplistic e said that they could really see the stamp of the founding discipline on the group though participants were from many different disciplines you could just tell what the underlying assumptions were about knowledge and what the cultural norms were about how we talked to each other who does what labor in the group and so on i saw their point i reluctantly said i understood but i d miss their thoughts e told me they d be interested in working with me on a project in the future sometime maybe especially if they got the fellowship they d applied for they really didn t have time for things like this when they had to teach and also had qualifying exams coming up i d been surprised about that too because in my discipline you were expected to take a lot longer to prepare for those exams of course i d delayed my exams even further by joining the group maybe i d regret that later but at the moment i thought it was a valuable use of my time e never did rejoin the group though i was able to co write a paper with them later while i d gotten a lot of value out of my interdisciplinary experience i was definitely left with a feeling that there were still missed opportunities for me and especially for a number of colleagues whose timelines constraints and interests hadn t been taken into account in how we d run the group i could see how they d been unintentionally excluded i vowed in my next interdisciplinary collaboration to pay closer attention to these dynamics and help keep everyone supported and involved 3 3 1 manifesto practices applied in interdisciplinarity this vignette intentionally spans a wide range of experiences and times in my life so it includes times when i was initially learning about for example mixed methods analysis and also times when i was already using those methods that said by learning about other epistemic traditions i ended up learning about my own much of my understanding of the underlying epistemology of statistical ways of knowing comes from either sts scholars or historians of science who have studied statisticians i also learned qualitative methods through these interdisciplinary training opportunities with the additional benefit that because there were often other quantitatively trained participants learning alongside me we could help each other translate across the methodological gap i heard of triangulation in this context and of course the entire vignette is about acquiring interdiscipilinary fluency noting that this fluency refers to learning collaborative skills rather than simply studying interdisciplinary topics though the two may occur together the surprise for me in my early interdisciplinary training experiences was how power dynamics between participants of different backgrounds impacted our activities and learning and this directly led to me seeking additional critical theory and social science training 3 3 2 effectiveness of manifesto practices in interdisciplinarity this vignette again does not feature a specific model however the products we generated in the various interdisciplinary projects i have engaged in tended to feel to me to be more trustworthy due to the triangulation of different perspectives the greater attention to epistemic contexts and consistency and the frequent explicit use of mixed methods analysis again due to the varied disciplinary perspectives the work we generated tended to be more relevant applied to salient problems more effective because we addressed the problems in a more holistic way and more just because typically social scientists trained in critical theory were involved and tended to hold the group accountable to justice issues however the processes themselves often tended to exclude people many of these programs were explicitly about graduate training and the programs that involved graduate students in planning and running the group tended to have better retention and more concrete outputs similar to community based modeling interdisciplinary training can contribute to relevance trustworthiness and justice while sometimes itself still needing a careful eye to the inequities in collaboration 3 4 vignette 4 science justice and community based modeling after i finished my phd at the end of 2014 i started a postdoctoral fellowship at the science justice research center sjrc at uc santa cruz having had a taste of the importance of studying how knowledge is made and decisions are formed i was determined to dive into learning more about science and technology studies early on in my postdoc in late 2015 i attended an event where the speaker quoted and raised issue with a now infamous wired article on the end of theory and how machine learning meant that we didn t need to understand the mechanisms behind things anymore because prediction was the ultimate goal anyway anderson 2008 the speaker s commentary on this idea was oriented as a science and technology studies critique which i found very useful but i was also incensed by the article s claim that machine learning was model free i knew from my training in statistics that these kinds of methods absolutely contained model assumptions and often simple ones like linear relationships i raised my hand to comment only to hear to my gratification one of the other attendees a computer science professor make that exact point i was not the only one aware of these issues and willing to raise them not long after that i read cathy o neil s weapons of math destruction 2016 where i had been concerned about the epistemological issues with big data e g how to repurpose high variety big data i now found myself concerned about data justice as well horrified by the examples o neil cited of people being judged by algorithms that produced self fulfilling prophecies reinforcing unfair and cruel systems at the same time family members started sending me popular press articles and commentary on the reproducibility crisis the ethics and trustworthiness of data science and big data were being called into question all around me even the music i was listening to reinforced my concerns vienna teng singer songwriter pianist engineer captured it beautifully in the lyrics of the hymn of acxiom leave your life open you don t have to hide someone is gathering every crumb you drop these mindless decisions and moments you long forgot keep them all let our formulas find your soul we ll divine your artesian source in your mind marshal feed and force our machines will to design you a perfect love or better still a perfect lust o how glorious glorious a brand new need is born now we possess you you ll own that you ll own that now we possess you you ll own that in time vienna teng 2013 though i still felt that technology had a role to play in solving pressing environmental and societal problems i increasingly also felt that we needed to be vigilant about the epistemological and justice issues associated with that role i was fast becoming disillusioned with the rapid pace of innovation in data science and the apparent blindness of innovators to how their inventions were impacting people and the world it was a period of increasing mistrust both for me mistrusting techno utopianism and apparently for larger society mistrusting the scientific process so i began asking myself in this environment of mistrust and injustice what does my repurposing skill set look like particularly if i want my skills to benefit decision makers in uncertain environments and at the same time i was becoming involved in the community and citizen science movement and learning more about collaborative and community engaged research this interest in collaborative research was why part of my research project at ucsc was framed around repurposing a 35 year archive of community held data from rural zimbabwe i had already been conscious of watching the power dynamics between myself and my community collaborators when we had proposed the project but in light of the alarmingly one sided techno utopianism i was experiencing in the san francisco bay area i renewed my vow to learn to do truly authentically engaged data science work with my co researchers in mazvihwa communal area we had designed the project to be community led and i was at least somewhat reassured that i was using their data with their permission and guidance and i was working hard to make sure the models i created were driven by their questions and concerns about their own system eitzel et al 2020a b this was all along the lines of what i had come to understand as deeply and appropriately collaborative community based science and i was lucky that my re purposing was building on 35 years of work that had been intentionally community led all along by non governmental organization the muonde trust outsiders provided training and resources when the community sought them and muonde already had a strong sense of agency in their own research but i still had more to learn of course when i went to zimbabwe to meet and work with the community in person i and other outsiders had developed an agent based model abm to represent the complex feedbacks in mazvihwa s agro pastoral system in the summer of 2015 we had done this using the community s data and with input from muonde s co founders but when i brought the model there in march 2016 i was still nervous that the community might feel that this technical object was going to supersede their own rich place based knowledge and lived experience in their own system the goal was not for the model to predict how much land to devote to agricultural production but for it to generate discussion among community members and local leaders i took great pains to emphasize this but still i worried perhaps i hadn t needed to worry though because my collaborators on the muonde team quickly learned how to run the model on their computer and then how to run small group workshops to share the model with others they would later do this with local leaders giving rise to changes in local land use policy that could reduce deforestation eitzel et al 2021 in those initial workshops very quickly the facilitators defaulted to using shona rather than english and giving all the explanations and instructions to the other participants i found myself sitting idly by fig 4 frankly superfluous and it was a delight i was still glad to have consistently emphasized their own agency in the modeling and discussion process and to have spent much of my model development time checking that we were representing the right things but in the end i just trusted the community researchers to use the model as they saw fit to lead workshops and generate discussion this is not to say that any community based research team is automatically right but in this case at least i was happy to trust them and the results were excellent for them and for me in addition to the on the ground impacts we were proud to represent our modeling process and outcomes in several published academic venues eitzel et al 2020a b 2021 2022 one lingering issue remained however i was able to publish our work in open access journals which was important both because i was funded by public money from the united states and because the community in mazvihwa and other indigenous people throughout the world should at least theoretically have access to the work even if inconsistent internet access could still limit the practicality of access but some of the journals required that i include the data as part of their commitment to open science of course i agreed heartily with this commitment that was part of why i embraced the idea of a data biography however it was not my data to share and muonde at the time did not have a formal data release process in place so i worked with them to discuss what the rules should be and what i would be able to share publicly in the end we were able to include the suitably anonymized subset of their data that was required to reproduce the results in the papers but this felt far from a universal solution to me so i was definitely left with the question how do we balance open science with data sovereignty 3 4 1 manifesto practices applied in science justice this vignette integrates two experiences studying sts at the science justice research center and working with the muonde trust on community based modeling the two experiences mutually reinforced each other with respect to the manifesto recommendations i was testing this experience also takes place later in the narrative so i was able to incorporate all of the practices into my work below i focus mostly on the community based modeling rather than the justice training which was already analyzed in part through vignette 3 on interdisciplinary training in working to emphasize the community based model as a discussion tool and not a predictive or prescriptive one i was both following epistemic good practice for that type of agent based model and also staying conscious of the power dynamics of the community researcher context i also used the uncertainties in the model to reinforce the idea that the model was a supplement to the community s own extensive knowledge of their system in doing highly engaged community based modeling i was constantly checking with my zimbabwean colleagues regarding the model s accuracy and potential uses impacts and implications this work required all the interdisciplinary fluency i had gained over the years as well as my ability to apply the given modeling technique to incorporate a wide variety of community data mixed methods and triangulation between their different datasets finally i will note that this was my first attempt at an entirely separate set of publications on the modeling process these data biographies have been a great deal of work to prepare and though i am proud of the result it may be unrealistic to expect modelers to write a second analysis of their process after working to publish the results of their models finally i noted that often different manifesto practices applied to different stages of the modeling work model construction validation presentation application and so on 3 4 2 effectiveness of manifesto practices in science justice this project is the first time i believe i have met the goal of my models being relevant trustworthy and just the muonde team was able to use the model we created to improve historical inequities in resource access and to help heal colonial damage to traditional practices and authority eitzel et al 2021 the relevance of the process and model derived from selecting a project that i knew was community based and had been from the start the trustworthiness of the model to my muonde collaborators and their local community and leaders derived from the model s basis in muonde s own empirically gathered data and experience and in their involvement in the model s development and muonde itself is considered a source of trusted expert information in their community community based modeling therefore enhanced both relevance and trustworthiness because the model is being used to correct an unjust colonial land use pattern this project achieves at least in small part the goal of improving inequities at least for one community i did make use of all the other manifesto practices for example using mixed methods to pull together a data biography of muonde s data which helped me use the data appropriately in the model but they were all in support of the community based work 3 5 comparing and contrasting vignettes table 2 summarizes the analysis sections above noting the practices associated with each vignette whether the criteria for better modeling were met and highlights some of the important interactions noted below i directly compare the vignettes to elucidate patterns and potential surprises not all of the vignettes explicitly depict a model vignette 2 s model was never completed vignette 3 is more about interdisciplinary education but they are all good tests of whether and how i applied the manifesto practices to my work and comparing them is also fruitful vignettes 1 and 2 occurred long before i began thinking systematically about how to do better modeling while vignettes 3 and 4 overlapped with the period when i developed the manifesto it is therefore not surprising that i was able to apply more of the practices to the latter two vignettes especially vignette 4 and in particular the practices under the justice theme in turn the later vignettes also demonstrate evidence of processes and outcomes that are more trustworthy relevant and just however this analysis has enabled me to take a close look at the reasons why so many more of the practices applied to the later projects most notably due to the choice of collaborators and projects that allowed me to do impactful community engaged work in both vignette 3 and 4 often there was a clear connection between the manifesto practices and these outcomes for example vignette 1 s models were likely to be more trustworthy to journal editors due to triangulation and the conservative conclusions that emerged from that approach or that the improved relevance and justice outcomes of vignette 4 s models derived largely from the community based selection of impactful research questions i also noted that many of these practices caused me to take longer in my analysis doing sensitivity analysis in my tree models getting metadata to do oak modeling doing interdisciplinary training in the midst of all my other responsibilities and doing community based work this implies that planning for projects to have better outcomes means allowing more time for these processes we may need to shift norms for the speed of analysis to give modelers time to engage with better modeling practices 4 discussion below i return to my research questions did the manifesto practices make my modeling better how did the practices interact where do the manifesto and or the criteria need refinement 4 1 manifesto practices improved my modeling outcomes i found the manifesto to be a useful framework for evaluating my past work conversely examining how the practices applied to my experiences helped me to ground the manifesto and make it less abstract each of my proposed practices to the degree that i applied them in my work did improve the outcomes of my modeling in terms of relevance trustworthiness and justice even asking questions about hypotheticals and missing pieces for example what communities i could have worked with in my tree models revealed places to shift practice in the future making silences speak clarke 2005 among the specific manifesto practices triangulation emerged as particularly important as have the practices that improve transparency data biographies epistemic consistency community based modeling improved all my indicators of better modeling particularly when it integrated many of the other practices the contextualizing practices improved the transparency of modeling while the collaborative practices broadened the empirical basis of models and the justice practices held modeling processes accountable trying to apply justice practices to a project which was originally conceived as an ecological methods project was much more difficult than applying them to deliberately community based work however this comparison is what makes it clear to me that project choice is an important step not to neglect when considering the outcomes of one s work 4 2 interactions between practices did support better modeling for some vignettes there were clusters of practices that mutually reinforced each other in producing better modeling outcomes in the terminology of statistical models higher order interactions for example in vignette 3 on interdisciplinary training epistemic consistency interdisciplinary fluency mixed methods triangulation and impacts and implications were all aligned with each other almost describing the same phenomenon the training programs while in other vignettes these practices did not necessarily interact as a group in vignette 1 on correcting my dissertation work epistemic consistency data biographies triangulation and uncertainty as openness all worked together to improve trustworthiness while in vignette 4 on collaborative modeling power dynamics uncertainty as openness and epistemic consistency worked together to improve relevance and justice in vignette 4 too there was more clustering across the broader themes in which the contextual practices e g epistemic consistency worked in synergy with the justice practices e g impacts and implications indeed i observed the potential for technical and social better practices to mutually support each other i also noted that in vignette 2 on oak modeling different aspects of the modeling process benefited from different clusters of practices in interviewing ecologists about their data mixed methods interdisciplinary fluency and power dynamics came into play and in assembling the datasets i used data biographies triangulation and epistemic consistency on the other hand the model i would have created would have worked with uncertainty as openness and impacts and implications finally some combinations of practices were perversely helpful for example in vignette 1 my conclusions were so conservative as to be not terribly helpful for managers reducing the impact of the research but also the impact of the corrections of my dissertation papers i have mostly noted synergies between practices here but there were also potential conflicts between the practices most notably that specific conceptions of what constitutes open science can be in conflict with the rights of communities to control their own data data sovereignty rainie et al 2017 i successfully navigated this challenge in my work with muonde and there may be situations in which sharing data is not in conflict with control over that data but in many community based projects this warrants careful attention asking who chooses to enclose or restrict information and for whose benefit it may be that negotiating this tension will be different in each situation but more discussion of this potential conflict is warranted 4 3 manifesto practices need refinement not all of the practices were easy to test for each vignette by systematically checking whether the practices interacted i noted ambiguities and complexities that point towards areas of improvement in the manifesto the simplest of these was finding that i needed to carefully define what constituted a data biography in vignette 1 this was largely appendices to the papers while in vignette 4 the data biography was multiple papers describing the process of the modeling also though i had originally called it a data biography i meant the term to apply to more than just data i could easily have called it a model biography for example the justice practices were particularly tricky to apply wilmer et al 2021 with respect to power dynamics it was important to note that sometimes these dynamics worked in a positive way to improve trustworthiness relevance or justice e g the work with muonde and in other cases they worked in a way that reduced the potential for those outcomes e g the interdisciplinary graduate training and both observations were important in thinking about how to do better modeling in addition i needed to carefully delineate that i was referring to power dynamics in the process of the modeling rather than in the subject matter being modeled e g interdisciplinary graduate training projects might have been studying power dynamics but i was referring to the power dynamics within the group itself i found that one useful way to frame questions around power dynamics was to ask what was at stake and who controls it for example in vignette 1 the retraction of my papers was at stake and the career implications that could have had for me and the editors held control over that another important definition in need of careful articulation involved community based modeling namely what was the community in question in the first two vignettes was it natural resource managers ecologists indigenous people and others who value forest ecosystems in different ways my engagement with each of these groups was quite limited in those two stories in the third vignette i defined the community as the graduate students themselves while in the fourth vignette the community was very clearly defined the muonde trust and the farmers in mazvihwa communal area because the project was designed to be community based from the start the three justice practices in general were the least well defined of all the manifesto prescriptions i found that they could be fruitfully refined and nuanced based on what i have learned from this evaluation largely that careful definitions should be part of each of those practices e g who is the community power dynamics between which entities impacts and implications for whom at a higher level in writing this paper i have found that it is helpful to distinguish between specific practices and the larger processes that the practices contribute to and terms like collaborative modeling may be too broad to be clear i used the term here to refer to a group of specific practices e g those described in tienne 2013 but it could also refer to a larger process in addition as i applied my relevance criterion it was useful explicitly define relevant for whom and for what purpose 5 conclusions and future work i have demonstrated an autoethnographic method for assessing modeler s manifesto practices using my own modeling experiences and criteria that modeling should be more trustworthy relevant and just i plan to periodically review the manifesto itself to see if my experience with different practices has shifted or if new ones are necessary including re evaluating my own values in terms of what i want my modeling to accomplish in the world with a single autoethnography we can see where the general resides in the particular merriam and tisdell 2015 and from there start to get a picture of the general this kind of comparative qualitative study can form the foundation of later larger scale possibly quantitative assessments therefore the more modelers who autoethnographically test out proposed modeling manifestos the better we can collectively understand how to improve modeling processes and outcomes upon further personal observation modelers may find that they are already employing many of these improved modeling practices in that case articulating how these practices play out when applied could help in assessing their impact and outcomes autoethnography by modelers can complement the interview and ethnographic methods of sts scholars that seek to illuminate the choices and processes underlying models and modeling products babel et al 2019 i welcome alternative framings of improved modeling outcomes as well as alternative sets of practices other modelers could develop and then test their own manifestos against their own values of what constitutes better modeling or test my manifesto practices or those suggested by others see voinov et al 2014 for their 10 commandments of modeling as an example i do encourage modelers of all kinds to consider my criteria and practices but i recognize that not all practices may apply to all kinds of modeling nor will my specific criteria what is most important is for modelers to consider their own values and how these influence their modeling choices we may decide that we have done high quality work but we might not ask ourselves how our values determine what we think of as high quality or how our values determine what questions we ask in the first place pirsig 1984 we may not stop to consider why we take on certain projects and not others with the ubiquity of models in a wide range of contemporary decision making spaces we as modelers hold a lot of power over the outcomes that can affect many our choices matter when i have presented the manifesto i have received earnest thoughtful questions from people wanting to know how to apply the practices i am suggesting i hope that my analysis and those like it could be helpful in developing data science pedagogy bates et al 2020 and in informing larger conversations on improving modeling processes indeed autoethnography is a technique that can serve us in developing larger norms for more transparent modeling saltelli et al 2020 individual actions can add up to systemic changes elevating the individual stories behind those actions can help us see better how to create more relevant trustworthy and just models which do the work in the world that we hoped they would credit authorship contribution statement m v eitzel all aspects of this work acknowledgments this work was supported by the united states national science foundation award number 1415130 and by a university of california davis academic federation innovative developmental award the funders had no role in study design in the collection analysis and interpretation of data in the writing of the report and in the decision to submit the article for publication the statements made and views expressed in this article are solely the responsibilities of the author and do not reflect the views of the national science foundation this work was enabled by the many scholars who shared their time with me in discussions about science and justice sts modeling and more many of whom also offered insightful comments on various drafts of the manifesto and i thank a subset of them here martha kenney jon solera lizzy hare jenny reardon katherine weatherford darling perry de valpine carl boettiger andrew mathews aaron fisher ashley buchanan eric nost jenny goldstein luke bergmann justin kitzes k b wilson jennifer glaubius trevor caughlin david o sullivan sara stoudt rob sokolowski and peer reviewers and editors at engaging science technology and society environmental modeling and software and plos one declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105690 appendix a supplementary data the following is the supplementary material related to this article mmc s1 more details on selection construction and analysis of the vignettes and validation of the study 
25399,modelers are proposing sets of better practices to improve modeling processes and outcomes we need to evaluate how they perform in practice i use autoethnography to describe four of my modeling and interdisciplinary training experiences and test how i applied a specific set of modeling best practices proposed in eitzel 2021 exploring whether how they resulted in processes whose outcomes were more relevant trustworthy and just than they would otherwise have been the practices did improve the outcomes of my models especially triangulating between multiple data sources and perspectives improving transparency through better descriptions of methods and data and engaging in community based modeling some practices mutually reinforced each other though balancing transparency with data sovereignty was critical when working with indigenous communities i invite other modelers to follow this example analyzing their own experiences using autoethnography testing my definitions of better modeling and proposed practices or substituting their own graphical abstract keywords moderate autoethnography data science best practices community based modeling correction of published research interdisciplinary training science and technology studies data availability relevant data underlying this article are available in the appendix materials 1 introduction beset by multiple global crises including biodiversity loss climate change social inequity and the covid 19 pandemic where do we turn for information to guide adaptation and action i would argue that in many cases we make individual and collective use of models which i broadly define as a representation of something of interest 1 1 nearly all definitions in the oxford english dictionary make some reference to representation 2021a though my focus is on quantitative models typically rendered in a computer i also encourage modelers working on qualitative or physical representations to consider the content of this paper because how we make models can strongly impact how well we can address pressing concerns sarewitz et al 2000 many modelers have proposed sets of individual modeling practices 2 2 note that by practice i mean the actual application or use of an idea belief or method as opposed to the theory or principles of it oed 2021b sometimes framed as manifestos e g derman and wilmott 2009 that in aggregate could improve both the larger social and technical processes involved in modeling as well as the outcomes of these processes with respect to their desired goals to illustrate the relationship between these concepts consider a goal of ensuring that a model addresses an ongoing inequity in resource access an improved outcome would be greater equity and the associated process could be participatory modeling the practices might involve including many voices in the design of the model the model produced by this process might be a better representation of the system in question which could in turn therefore have greater impact on the problem it was created to solve meeting the goals of the modeling process as a practicing modeler working to improve my modeling process and outcomes i created my own modeler s manifesto 3 3 a summarized version is available in eitzel 2022 for the detailed version see eitzel 2021 i and many modelers are actively engaged in implementing these manifestos or better practices however i would further argue that if we are motivated by pressing societal problems we need to concretely assess the effectiveness of these practices this involves both defining the criteria for assessment given the goals of the modeler what constitutes a better modeling process and or outcome and choosing a method for assessment in attempting this i have found that each modeling situation is quite unique and the context strongly influences how practices can be applied i have therefore come to value autoethnography a form of systematic self assessment as a method for evaluating modeling manifesto implementation in this paper i autoethnographically assess my own manifesto by outlining criteria for assessment and then systematically applying them to four of my modeling and interdisciplinary training experiences investigating whether and how following my proposed manifesto practices actually contributed to improved modeling processes and outcomes the goals of my analysis are 1 to demonstrate a way to evaluate modeler s manifestos in which others could substitute their own criteria modeling practices and or modeling cases fig 1 and 2 to assess both my modeling experiences and my particular collection of better practices in the remainder of the introduction i first provide background on autoethnography as a method for those unfamiliar with it then in order to make this method of assessment modular see fig 1 and in line with good autoethnographical methods i outline my own values that are salient to my approach to modeling then summarize the manifesto practices i will be assessing and establish my criteria for better modeling personal values influence many modeling choices so describing mine allows a reader to note points of agreement or divergence and to take these into account when choosing whether to apply my criteria practices or assessment method to their own work i end with the research questions that guided my analysis 1 1 autoethnography of modeling autoethnography is a method of observing and reflecting on one s own research and learning processes ellis et al 2011 it has a long history in a variety of disciplines with a dedicated journal founded in 2020 adams and herrmann 2020 autoethnography can range from deeply qualitative narrative based strategies le roux 2017 to highly quantified methods of self assessment anderson 2006 with all possibilities in between as well as combinations of multiple strategies stahlke wall 2016 autoethnography as practice is one way to make sense of your own experience and to understand the mechanisms and values behind what you do newman and farren 2018 and how it impacts others simultaneously autoethnography as research product can be a venue for sharing what you have learned and can therefore be a way to make change and collectively improve practice ellis et al 2011 le roux 2017 modelers are already engaging in autoethnography in a variety of forms including evocative strategies reporting and reflecting through a narrative levin 2019 as well as more quantified analytical strategies newman and farren 2018 analytical autoethnography of modeling can range from simple reflections by a research or software development team member mulvenna et al 2018 bodo et al 2018 possibly comparing their journals and notes with internal reports and meeting minutes caesarius and johansson 2013 all the way up to a thorough analysis of one s time tracking and systematic coding of personal journals newman and farren 2018 my adoption of this technique is therefore far from unusual and autoethnography has great potential for modelers to better understand their own modeling paths lahtinen et al 2017 and habits babel et al 2019 revealing what they ve done and why they did it this practice can therefore help to assess the transformative potential of modeler s manifestos 1 2 personal values and modeling choices in addition to allowing the reader to evaluate overlap between my values and theirs describing salient aspects of my standpoint and background walter and andersen 2013 or situating myself haraway 1988 also enables them to understand what biases i may have in my modeling practice and in evaluating my own work this self disclosure is good autoethnographic practice because the author is both the subject of study and the instrument used to conduct the study i give a much more detailed accounting of my background and how it shapes my values in appendix a but here i focus specifically on how my values influence my choices regarding modeling epistemologically i identify with an objectivist sense of an external reality and place a high value on empiricism as a way of knowing that said i define empiricism broadly and recognize the impact that our perspectives have on what we perceive and believe this means that i tend towards both qualitative and quantitative ways of validating models and also feel the importance of relativism and the influence of power dynamics between people on what modeling gets done and how from an equity standpoint i strongly value just distribution of benefit and equitable protection from harm faced with the global crises listed above i put a high priority on modeling that supports justice and equity in the face of these problems and i am therefore most interested in applied modeling projects including this paper s project of evaluating modeling manifestos 1 3 manifesto themes and practices in my search for guidance on how to improve the outcomes of my modeling practices i pursued interdisciplinary science and technology studies sts training and read other modelers thoughts on better modeling practices see appendix a for a brief summary in the end i synthesized what i learned into a manifesto with nine practices grouped into three themes largely grounded in donna haraway s situated knowledges 1988 context collaboration and justice see eitzel 2021 for detailed descriptions of the themes and practices and table 1 for a summary my values on empiricism and relativism are both reflected in the context and collaboration practices in which i emphasize the need for outside validation but encourage it to come from multiple perspectives my values on justice and equity underlie the third theme s focus on power dynamics and consideration of modeling impacts 1 4 criteria for better models in order to assess my manifesto i asked myself what constitutes a better model modeling process or outcome the criteria i arrived at derived from my values and are also grounded in philosophy science and technology studies and modeling literature see below for that reason they are not entirely independent of the manifesto practices i proposed which also derive from my values and studies of better modeling though from an earlier round of analysis see eitzel 2021 one of the purposes of evaluating my modeling experiences through the lens of the manifesto practices and against these criteria is to examine how much overlap there is between the criteria and practices when they are actually applied whether there are any surprises and whether and how different manifesto practices worked together or interfered with each other in meeting those criteria below i outline the three criteria i use in this paper noting when they conceptually connect particularly strongly with the manifesto practices listed above 1 4 1 trustworthiness increasingly we appear to be living in a post truth society higgins 2016 amid a crisis of reproducibility in science spiegelhalter 2017 inspired by my value on empiricism i established one of my criteria on the idea that we need to be able to believe what a model is telling us in order for it to serve our needs philosophically trust in others testimony can be justified by ensuring the reliability of belief forming process pritchard 2004 criteria that could assist a novice in assessing the reliability of an expert s testimony include checking their testimony against other experts experts making their own biases clear and experts demonstrating a positive track record goldman 2001 this criterion echoes manifesto practices around triangulation and detailed descriptions of modeling processes including describing our own biases as well as drawing conclusions consistent with the epistemic restriction of the method e g correlation does not imply causation reproducible and participatory modeling processes may also be perceived as more trustworthy 1 4 2 relevance as box is often quoted all models are wrong but some are useful 1987 meaning that representations are never perfect but they can still generate relevant and actionable understanding of the entities they represent for some purposes models may be simplified e g for teaching or to explore theory while others can be quite complex in many different ways e g for accurate prediction of a specific system or mechanistic understanding of a generalized system holling 1966 breiman 2001 efron 2001 and often no single model is appropriate for all purposes shackley et al 1998 i propose that relevance has two parts first the modeler specifies the goal that the model is intended to achieve and second they assess whether the model is appropriate to and achieves that goal given my values on applied modeling that addresses global crises my specific criteria for relevance relate to actionable results for policy makers environmental managers and community members this definition of relevance also points towards participatory processes for determining modeling questions 1 4 3 justice a number of authors have recently pointed out that many models may exacerbate problems they were ostensibly designed to solve o neil 2016 reinforcing existing biases in data and thereby harming vulnerable populations who lack a political voice to oppose unjust conclusions mah 2017 eubanks 2018 grounded in my values around justice and equity i determined one of my criteria for better modeling would be justice i subscribe to rawls framing of justice as fairness in the distribution of rights and duties towards and rewards from human cooperation rawls 2009 i further incorporate reardon s exhortation to keep our assessment and pursuit of justice from relying on universals and to always stay grounded in the specifics of a given situation including its history reardon 2013 i therefore define justice in line with these ideas ensuring equal benefit and avoiding unequal harm keeping in mind the histories of unequal benefit and harm this criterion aligns well with the three practices in the manifesto framed around justice though it also aligns with the idea of transparency in modeling for example via data biographies practice 1b in table 1 it is no accident that justice became both a set of practices and a criterion though there is an element of critical theory behind many of the other practices it is most clearly reflected in the justice practices the modeler s values are particularly important here for example a different definition of justice could result in very different modeling processes and outcomes the third criterion and the third set of modeling practices are therefore both firmly grounded in critical theory and are decidedly social rather than technical however social and technical aspects of modeling can mutually support each other in some cases eitzel et al 2022 so one point of curiosity is whether and how these practices interact 1 5 research questions the questions i seek to answer are the following 1 does applying the manifesto practices make my modeling better based on the above definition of better 2 how do these practices support each other or interfere with each other especially the technical practices and the critical practices 3 how could the manifesto practices or criteria for better models be refined i briefly outline my methodological approach to autoethnography how i selected modeling experiences to use in testing the manifesto and how i established qualitative validity of the results 2 methods to describe and analyze my experiences i used a moderate autoethnographic approach stahlke wall 2016 mixing evocative narrative via descriptive vignettes of my modeling experiences with systematic analysis of how the modeling manifesto practices affected my modeling processes and outcomes le roux 2017 i selected four of my modeling and interdisciplinary projects from 2010 2020 see details in appendix a on the selection process 1 correcting published models from my phd dissertation on forest tree growth and survival 2 assembling a dataset and planning a model for oak tree population dynamics 3 a combination of various interdisciplinary training experiences 4 my experience as a postdoctoral scholar learning critical theory and qualitative analysis techniques and applying them to community based modeling with zimbabwean farmer researchers from an analytical angle these vignettes cover a range of different types of projects and models which can be fruitfully compared from a narrative angle the vignettes form an arc representing different stages of my application of the manifesto ideas i constructed the vignettes largely from memory with some reference to personal journals notes on modeling projects and email exchanges with collaborators for some vignettes i used the consolidated criteria for reporting qualitative studies coreq checklist to ensure i included enough detail tong et al 2007 2 1 analysis of vignettes i relied on the practice of writing itself as a mode of inquiry in its own right in which writing and research are co produced richardson 2003 gibbs 2007 and not assuming that writing only documents what is already known st pierre 2015 i coded my modeling experiences and answered my research questions by writing and editing drafts and presentations of this paper in each draft i iteratively reviewed each vignette with multiple rounds of deepening questions 1 which of the manifesto practices did i apply and how 2 what lessons did i learn from this experience 3 to what degree did the manifesto practices improve the outcomes of my models via increased trustworthiness relevance and justice 4 how do the practices reinforce each other or interfere with each other 5 which of these practices were most important was the manifesto a useful tool and will i continue to use it in my own modeling practice in the future 6 where do the manifesto practices themselves need more careful definition in order to be used in an assessment i coded the vignettes using the manifesto practices from eitzel 2021 as a concept driven deductive coding tree with three overarching themes and nine individual practices as well as coding each vignette for trustworthiness relevance and justice as defined in the introduction i also allowed for open inductive coding regarding lessons learned saldaa 2021 with each draft of this paper s results and discussion sections i added a layer of questions i first wrote the sections that describe which practices were applied coding each vignette based on presence or absence of each practice and then in the next edit wrote the section that highlighted lessons learned and so on altogether i wrote five major paper drafts with associated rounds of analysis and writing see appendix a to code whether the vignettes describe more trustworthy processes where possible i noted the ways in which others reacted to the models and other research products in addition to my own sense of trust in my results to code for relevance i investigated the applicability of the research to related questions of concern for larger communities than myself e g scientists and practitioners involved in conservation or management of natural resources to code for justice impacts i asked whether the research process served to correct a current or historical inequity and specified whose inequities i then systematically reviewed the way each pair of manifesto practices potentially interacted for each vignette details given in appendix a the last two questions were higher level summative analyses synthesizing the answers to the other four questions 2 2 credibility consistency and transferability concepts of validity replicability and generalizability are framed differently for qualitative analysis than for quantitative analysis validity in a quantitative analysis shifts to become credibility in a qualitative analysis merriam and tisdell 2015 to make my analysis credible i sought peer review of my work from colleagues in both social science and modeling and triangulated between different cases vignettes with differing applicability of the manifesto practices and different project types quantitative replicability is recast qualitatively as consistency merriam and tisdell 2015 because many qualitative interpretations are possible with the same data independent replication is often not practical and an alternative interpretation may not invalidate the conclusions of the study therefore any individual interpretation should be self consistent rather than replicable through peer review i checked that the observations in my results and discussions section were consistent with the data given in the vignettes and i gave details on their selection and analysis in appendix a in addition researcher self reflexiveness which is also a key autoethnographic principle can help evaluate consistency of data and findings i included a description of my own biases and background in the introduction and appendix a to help readers assess the consistency of my conclusions quantitative generalizability is reframed as qualitative transferability or whether a reader feels that the results apply to them merriam and tisdell 2015 i facilitated this by giving a thick detailed description in the vignettes that my peer reviewers used to evaluate the study s applicability to their own experience i also presented a range of different cases vignettes with different types of models and projects and i sought feedback on them from both modelers and social scientists in effect a more diverse sample in autoethnography in particular validity can be assessed by the following criteria subjectivity that the self is primarily visible in the research self reflexivity self awareness self exposure and self conscious introspection are shown in the work resonance there is commonality between researcher and audience credibility the research is permeated by honesty and contribution the work teaches informs and inspires see le roux 2017 for more details i elaborate how this work meets those criteria in detail in appendix a 3 results after each of the four vignettes i analyze what practices were applied and how effective they were in making my modeling more relevant trustworthy and just vignettes are set in italics to distinguish them from their further analysis in the final section i compare and contrast the vignettes 3 1 vignette 1 the art of modeling tree growth and survival in 2008 i could barely contain my excitement as i started a phd program in environmental science policy and management at uc berkeley i had been studying physics and seismology and discovered that i loved doing research but i had found the topics of my research to be too abstract simulating bose einstein condensates hurt et al 2006 and analyzing seismic noise eitzel tanimoto et al 2008 i wanted to do research that addressed the urgent issues of the day at the time what felt most important to me were topics around energy sustainability and ecological conservation i d also felt that simulation and time series analysis were useful skills to have but that statistical modeling methods were essential to work with the kinds of messy real world problems that seemed most pressing so i enthusiastically dove in to studies on ecology statistics remote sensing and in particular forest and landscape modeling but as i progressed in learning new analysis methods over the next several years i found myself increasingly aware of how sensitive they were to my choices as the analyst as it turned out i was learning the art of modeling one particularly tricky technical aspect of the statistical models i was creating for my first two dissertation chapters was the question of adequate markov chain monte carlo mcmc mixing from a technical standpoint mcmc estimation of statistical parameters is considered valid if the algorithm has tried values that spanned a reasonable range for that parameter and did so in a relatively random and evenly distributed way ways to assess this good mixing varied from qualitative it should look like a caterpillar or like noise paraphrased from various peers and advisors to quantitative you should use a gelman rubin diagnostic to compare two different mcmc chains with different starting points summarized advice from a peer reviewer even bayesian modeling textbooks indicated that model checking is a delicate balance of technique and judgment guided by the applied context of the problem gelman et al 2014 in the end i used both qualitative judgments and quantitative metrics to decide when mixing was adequate fig 2 however this experience among others led me to adopt an overall strategy of conducting sensitivity analysis to my modeling choices i was also uncomfortable with the number of judgment calls i had to make leading me to be cautious with my conclusions in my dissertation chapters on repurposing forest inventory data to study tree growth and survival i tried a variety of models with different variables included or excluded and drew conclusions only about the results that were robust across all of them this sensitivity analysis meant that it took me longer to generate results and write papers than it otherwise would have i also tended to focus more on the methods and less on the results i was ecstatic that even in spite of that i was able to get both chapters published in respectable ecological journals during and very soon after completing my phd eitzel et al 2013 2015a imagine my horror then when i discovered in late 2015 soon after the papers publication that i had made a data processing error back at the beginning of my graduate work when i was still a novice at the programming language i was using after discovering the error i wrote in my personal journal trying to encourage myself i know everyone makes mistakes like these and what sets you apart is how you handle it and the world needs to see that this happens to science and we need to be honest about it so i tried to put aside my fears about potentially having to retract my only two first author analysis papers a huge blow for an early career researcher fortunately both my phd and postdoctoral mentors were extremely supportive of me correcting my papers validating my perception of scientific culture as bravely self correcting i fixed the error and reran all my analyses a process eased by my copious notes and programming scripts which allowed me to re do the work in about a tenth the time it had initially taken some of the results did shift but i was deeply relieved to discover that i had been sufficiently broad in my initial conclusions that the basics did not change my cautious instincts had served me well fortunately the journal editors agreed with me and we published errata correcting the two papers ecological applications 2016 ecosphere 2016 no retractions necessary i was also relieved that i had taken a similar strategy for the third chapter of my dissertation focusing on presenting methods doing a sensitivity analysis in this case to the spatial resolution in a remote sensing project with data unrelated to the other two papers eitzel et al 2015b and making conclusions broad enough to be defended by the results across the sensitivity analysis this whole experience made a strong impression on me that i had been right to be appropriately conservative with my conclusions 3 1 1 manifesto practices applied in the art of modeling because this story comes early in my narrative arc before i had begun studying critical theory and social science frameworks most of the manifesto practices that speak to it are in the context category even my attempt at triangulation was a sensitivity analysis given that i did not have access to other datasets to use to check my results i did write many lengthy appendices on each of these projects which served at least some aspects of the data biography largely extended methods which might help someone to re purpose my work or learn how to do the method themselves by far the clearest message i learned from this experience was that it was essential that i had been conservative in my conclusions because that meant limited impact from needing to correct the published work in reviewing the experience to describe it here i realized i had used both qualitative and quantitative assessments but this is not reported explicitly in any of the copious appendix material 3 1 2 effectiveness of manifesto practices in the art of modeling the biggest success of the manifesto s practices for this case is the trustworthiness of the models due to my use of triangulation in the form of sensitivity analysis and especially to the conservative claims i derived from them even in the face of errors and corrections editors were willing to simply publish errata rather than retract the work i also acknowledge that there may have been additional influences on their editorial decisions however in reviewing the experience i realized that i had failed to engage sufficiently with the forest managers i was working with to ensure that these models produced knowledge that they might use in managing mixed conifer forests in california the questions were derived from ecological topics around population dynamics and individual tree growth and survival i also utterly failed to engage with any of the justice issues around the management of these forests the models themselves were very focused on the ecology of the trees at the time i was only just beginning to learn about the importance of non timber forest products and the ways indigenous peoples in california had been specifically disenfranchised with respect to these ecosystems and culturally important species norgaard 2019 by only modeling timber species was i putting more intellectual and financial resources into a single way of looking at the value of the forest of course if i had addressed any justice issues in this work the cost of errors and corrections would have been even greater this underlines the importance that if one is to pay attention to impacts and implications of one s modeling both in terms of relevance and justice the model s trustworthiness is paramount 3 2 vignette 2 ethnography of data driven oak conservation during my graduate work at berkeley i developed a kind of niche skill of re purposing legacy ecological data many such datasets were getting long enough to do analyses that might reveal time trends particularly of interest as global change continued to be a pressing concern i was surrounded by ideals of measuring everything in order to manage ecosystems better for example the founding of the us national earth observation network 4 4 https www neonscience org about overview history and as data science became more and more fashionable i began pitching myself as an analyst who specialized in making use of what we already had high variety big data collected over multiple decades by a large and constantly changing set of people with potentially varying methods a particular passion project at the time was trying to synthesize a wide range of field research on oak tree populations in california there had been widespread concern since the 1970s that there was a recruitment problem in which there were many oak seedlings and adequate numbers of adults but not many saplings griffin 1971 1976 the reasoning went if seedlings could never get to the sapling stage would we ever have new adults but with the accumulating ecological data we had in the early 2010s surely we could actually construct a population viability model this would involve integrating data on growth survival and reproduction for a variety of oak life stages seedlings saplings and adults if i could account for the oddities of different datasets we could combine them to get a sense of whether populations were declining or increasing and which aspects of growth survival or reproduction in which life stages impacted that conclusion the most so in summer 2009 i tried asking researchers at a field research station if they had data on various demographic aspects of california blue oak quercus douglasii my initial inquiries made over email were not very successful another researcher a postdoc working at the research station was later able to inquire much more successfully and the oak researchers were willing to share their data with her and then with me at that point i was able to ask these researchers about other data they knew of until i was working with six different datasets i noticed that this process of asking people to provide information and to refer me to other researchers snowball sampling and especially the importance of physical presence and rapport building bore much more resemblance to ethnography than modeling so now i had the data along with papers and reports describing methods a key part of my re purposing approach was modeling the observation process as well as the growth survival or reproduction of the trees but as i studied the materials shared with me i realized that they didn t contain enough detail for me to model the observation process i pined for the extensive appendices i d published alongside my dissertation chapters if only the researchers had better described their methods i thought but they hadn t so instead i found myself essentially interviewing them via email asking questions about what they d done and how and sometimes why fortunately at this point they were quite willing to share these details and the challenge was simply to ask the right questions and iterate until i had the information i needed i also asked them questions about potential data errors sometimes they remembered what happened or used their knowledge of the species to answer e g saplings don t grow that fast and sometimes they didn t know and i was left with the decision of whether to throw out a potential error or keep it in the analysis i also found myself working to interpret both quantitative data and qualitative descriptions and narratives about the methods mixed methods analysis one particularly vexing issue i discovered was that measurement methods for tree size differed from study to study to construct the population viability model i needed to assess tree growth as a function of diameter but as i worked to bring the datasets together i noticed that the diameter measurement varied systematically between data on seedlings versus adults looking into it further i found that seedling diameter was measured with digital calipers at the soil surface while adult diameter was measured at breast height approximately 1 3 to 1 5 meters above the soil surface with a measuring tape a difference that became clearest to me when i was in the field fig 3 though this is ostensibly a measurement issue because i planned to explicitly represent the measurement processes in my statistical model it became a model construction issue as well and based on the limited description in the reports whose authors assumed readers understood the methods it took direct experience for me to truly understand what i had to model and indeed this difference in measurement processes made much more sense to me after i d been to the field station to gather some additional data for a different part of the model during that visit i also saw how hard it was to locate seedlings and how difficult it was to make judgment calls about where to measure their diameter and how this contrasted with the adult diameter measurements in the end i lamented how much time i d taken in gathering the data and metadata i ran out of funding to finish the analysis and my earlier modeling training had not adequately prepared me for working with this particular set of high variety big data i essentially needed qualitative data gathering skills i had to learn on the fly and at the same time it became clear to me that even if i could eventually do an analysis of population viability i wasn t sure i d feel comfortable making recommendations to decision makers about oak management given all the idiosyncrasies in the analysis i felt woefully unprepared to address these gaps 3 2 1 manifesto practices applied in ethnography of oak conservation i was definitely engaged in creating a data biography practice 1b from table 1 for the oak population dynamics datasets and i was also triangulating practice 2a between the written accounts in the reports and metadata and the accounts i elicited by asking the researchers questions in order to get a clear picture of what their data said about oak demography the model i had intended to make would have incorporated a sensitivity analysis that could have helped to frame uncertainty as openness practice 2b guiding further research i needed to develop qualitative research skills on the fly and to the degree that i succeeded i was able to build the needed data biography this was only possible due to the interdisciplinary fluency practice 2c i had already built if i had been able to create the model the impacts and implications of the research would very much have been in my mind practice 3b 3 2 2 effectiveness of manifesto practices in ethnography of oak conservation though there is no actual model in this vignette because i was never able to create it i had many plans for what it would look like i did think about what the impacts of the model might be because i had a personal interest in oak conservation i was much more connected to the community of researchers and others who cared about managing the trees if i had been able to make the population viability model it would very likely have had use for these communities i would have taken care to ensure its trustworthiness through many of the practices that i had already employed in my earlier forest modeling project sensitivity analysis making conservative conclusions and so on it was also clear that there was a trustworthiness to the data itself that was lacking in the form of missing data biographies and if i had not engaged in what amounted to qualitative research to create the data biographies the trustworthiness of my model would have suffered to the degree that i could following the practices of the manifesto did make the data and possible future model more trustworthy and more relevant however i did not engage with any justice issues that might emerge from oak conservation actions that could be based on the hypothetical model again the story of disenfranchisement of indigenous people of california is relevant here particularly due to the importance of acorn as a key food staple and cultural resource sowerwine et al 2019 this comparison makes it clear that some of these better modeling practices can help one consider impacts and implications and make the model more relevant and trustworthy but not necessarily more just here i was more engaged with the community of oak researchers but not with land managers or indigenous people so i would not call the work community based if i had created the model i might have reached out to these audiences with the results but that would still have been a fairly limited kind of engagement 3 3 vignette 3 challenges and opportunities of interdisciplinarity 5 5 i have been involved with a wide variety of interdisciplinary training experiences during my schooling see appendix for details i began to notice patterns in the ways the groups formed developed and dispersed in this vignette i created a composite of these experiences in order to protect the identity of any one program or collaborator relational ethics in autoethnography ellis 2016 and to distill the commonalities murchison 2010 my colleague e described in the vignette is no one person but is a combination of myself and a variety of colleagues over the course of many years after my experiences in working with legacy ecological data particularly in the context of decision makers using the results i d been particularly keen to seek out interdisciplinary training i needed a better grounding in qualitative social science data gathering and analysis methods as i d learned the hard way with the oak study and it had become increasingly obvious to me that for really pressing concerns understanding the ecology was only half of improving sustainability human management of ecosystems was at least as important to study and understand and in fact the two aspects interacted and influenced each other so i made extra time in my schedule one school term to join an interdisciplinary group whose entire purpose was to tackle these complex issue based kinds of questions initially i noted differences in vocabulary between my training and that of my colleagues sometimes even different definitions of the same words my favorite was comparing the different uses of the word community between ecologists and social scientists but i was ecstatic about the new ideas i was exposed to in the group including fundamentals outside my discipline that i could immediately see application for in my own work i was the kid in an intellectual candy shop i couldn t get enough of the interdisciplinary confections all around me suddenly the issues around environmental management that i d previously felt were intractable seemed more amenable to solutions when we were discussing it in a team with different backgrounds one person was more theoretical while another was more applied one person thought about the problem from a relativist perspective embedded in the cultures and history of the system while another came at it from an quantitative measurement oriented angle it seemed to me that the discussions were most fruitful if only more of academia could function this way i thought i was looking forward to the next event in our schedule when i got into a discussion with my friend e who had also been in the group they thought they wouldn t be joining the next event they told me at first i was confused by this reaction i d heard them participating in the discussions just as enthusiastically as i had surely they d gotten new insights as well sort of they told me but they really didn t have time for the group next term too much of a teaching load and their mentor wanted them to get one of their projects ready to submit to a conference with a close deadline saddened i said wistfully that perhaps just one event every few months might work i d thought e s contributions had been so key to moving our discussions to a generative place e shook their head hesitated for a moment and then told me that they d found the discussions of their discipline to be rudimentary not novel at all and pretty boring actually now i was really surprised i d thought they d made such insightful comments e told me that they felt they spent a lot of the time teaching us the basics of their discipline and they already had to do that with the undergrads three hours a week and besides that e told me with a sigh maybe i was open minded but not everyone in our group was so welcoming of challenging their ideas about how knowledge is made even if people were trying to be open minded some of them were just more confrontational when encountering differences some people had been making generalizations about e s discipline and methodological background that were at best unflattering and simplistic e said that they could really see the stamp of the founding discipline on the group though participants were from many different disciplines you could just tell what the underlying assumptions were about knowledge and what the cultural norms were about how we talked to each other who does what labor in the group and so on i saw their point i reluctantly said i understood but i d miss their thoughts e told me they d be interested in working with me on a project in the future sometime maybe especially if they got the fellowship they d applied for they really didn t have time for things like this when they had to teach and also had qualifying exams coming up i d been surprised about that too because in my discipline you were expected to take a lot longer to prepare for those exams of course i d delayed my exams even further by joining the group maybe i d regret that later but at the moment i thought it was a valuable use of my time e never did rejoin the group though i was able to co write a paper with them later while i d gotten a lot of value out of my interdisciplinary experience i was definitely left with a feeling that there were still missed opportunities for me and especially for a number of colleagues whose timelines constraints and interests hadn t been taken into account in how we d run the group i could see how they d been unintentionally excluded i vowed in my next interdisciplinary collaboration to pay closer attention to these dynamics and help keep everyone supported and involved 3 3 1 manifesto practices applied in interdisciplinarity this vignette intentionally spans a wide range of experiences and times in my life so it includes times when i was initially learning about for example mixed methods analysis and also times when i was already using those methods that said by learning about other epistemic traditions i ended up learning about my own much of my understanding of the underlying epistemology of statistical ways of knowing comes from either sts scholars or historians of science who have studied statisticians i also learned qualitative methods through these interdisciplinary training opportunities with the additional benefit that because there were often other quantitatively trained participants learning alongside me we could help each other translate across the methodological gap i heard of triangulation in this context and of course the entire vignette is about acquiring interdiscipilinary fluency noting that this fluency refers to learning collaborative skills rather than simply studying interdisciplinary topics though the two may occur together the surprise for me in my early interdisciplinary training experiences was how power dynamics between participants of different backgrounds impacted our activities and learning and this directly led to me seeking additional critical theory and social science training 3 3 2 effectiveness of manifesto practices in interdisciplinarity this vignette again does not feature a specific model however the products we generated in the various interdisciplinary projects i have engaged in tended to feel to me to be more trustworthy due to the triangulation of different perspectives the greater attention to epistemic contexts and consistency and the frequent explicit use of mixed methods analysis again due to the varied disciplinary perspectives the work we generated tended to be more relevant applied to salient problems more effective because we addressed the problems in a more holistic way and more just because typically social scientists trained in critical theory were involved and tended to hold the group accountable to justice issues however the processes themselves often tended to exclude people many of these programs were explicitly about graduate training and the programs that involved graduate students in planning and running the group tended to have better retention and more concrete outputs similar to community based modeling interdisciplinary training can contribute to relevance trustworthiness and justice while sometimes itself still needing a careful eye to the inequities in collaboration 3 4 vignette 4 science justice and community based modeling after i finished my phd at the end of 2014 i started a postdoctoral fellowship at the science justice research center sjrc at uc santa cruz having had a taste of the importance of studying how knowledge is made and decisions are formed i was determined to dive into learning more about science and technology studies early on in my postdoc in late 2015 i attended an event where the speaker quoted and raised issue with a now infamous wired article on the end of theory and how machine learning meant that we didn t need to understand the mechanisms behind things anymore because prediction was the ultimate goal anyway anderson 2008 the speaker s commentary on this idea was oriented as a science and technology studies critique which i found very useful but i was also incensed by the article s claim that machine learning was model free i knew from my training in statistics that these kinds of methods absolutely contained model assumptions and often simple ones like linear relationships i raised my hand to comment only to hear to my gratification one of the other attendees a computer science professor make that exact point i was not the only one aware of these issues and willing to raise them not long after that i read cathy o neil s weapons of math destruction 2016 where i had been concerned about the epistemological issues with big data e g how to repurpose high variety big data i now found myself concerned about data justice as well horrified by the examples o neil cited of people being judged by algorithms that produced self fulfilling prophecies reinforcing unfair and cruel systems at the same time family members started sending me popular press articles and commentary on the reproducibility crisis the ethics and trustworthiness of data science and big data were being called into question all around me even the music i was listening to reinforced my concerns vienna teng singer songwriter pianist engineer captured it beautifully in the lyrics of the hymn of acxiom leave your life open you don t have to hide someone is gathering every crumb you drop these mindless decisions and moments you long forgot keep them all let our formulas find your soul we ll divine your artesian source in your mind marshal feed and force our machines will to design you a perfect love or better still a perfect lust o how glorious glorious a brand new need is born now we possess you you ll own that you ll own that now we possess you you ll own that in time vienna teng 2013 though i still felt that technology had a role to play in solving pressing environmental and societal problems i increasingly also felt that we needed to be vigilant about the epistemological and justice issues associated with that role i was fast becoming disillusioned with the rapid pace of innovation in data science and the apparent blindness of innovators to how their inventions were impacting people and the world it was a period of increasing mistrust both for me mistrusting techno utopianism and apparently for larger society mistrusting the scientific process so i began asking myself in this environment of mistrust and injustice what does my repurposing skill set look like particularly if i want my skills to benefit decision makers in uncertain environments and at the same time i was becoming involved in the community and citizen science movement and learning more about collaborative and community engaged research this interest in collaborative research was why part of my research project at ucsc was framed around repurposing a 35 year archive of community held data from rural zimbabwe i had already been conscious of watching the power dynamics between myself and my community collaborators when we had proposed the project but in light of the alarmingly one sided techno utopianism i was experiencing in the san francisco bay area i renewed my vow to learn to do truly authentically engaged data science work with my co researchers in mazvihwa communal area we had designed the project to be community led and i was at least somewhat reassured that i was using their data with their permission and guidance and i was working hard to make sure the models i created were driven by their questions and concerns about their own system eitzel et al 2020a b this was all along the lines of what i had come to understand as deeply and appropriately collaborative community based science and i was lucky that my re purposing was building on 35 years of work that had been intentionally community led all along by non governmental organization the muonde trust outsiders provided training and resources when the community sought them and muonde already had a strong sense of agency in their own research but i still had more to learn of course when i went to zimbabwe to meet and work with the community in person i and other outsiders had developed an agent based model abm to represent the complex feedbacks in mazvihwa s agro pastoral system in the summer of 2015 we had done this using the community s data and with input from muonde s co founders but when i brought the model there in march 2016 i was still nervous that the community might feel that this technical object was going to supersede their own rich place based knowledge and lived experience in their own system the goal was not for the model to predict how much land to devote to agricultural production but for it to generate discussion among community members and local leaders i took great pains to emphasize this but still i worried perhaps i hadn t needed to worry though because my collaborators on the muonde team quickly learned how to run the model on their computer and then how to run small group workshops to share the model with others they would later do this with local leaders giving rise to changes in local land use policy that could reduce deforestation eitzel et al 2021 in those initial workshops very quickly the facilitators defaulted to using shona rather than english and giving all the explanations and instructions to the other participants i found myself sitting idly by fig 4 frankly superfluous and it was a delight i was still glad to have consistently emphasized their own agency in the modeling and discussion process and to have spent much of my model development time checking that we were representing the right things but in the end i just trusted the community researchers to use the model as they saw fit to lead workshops and generate discussion this is not to say that any community based research team is automatically right but in this case at least i was happy to trust them and the results were excellent for them and for me in addition to the on the ground impacts we were proud to represent our modeling process and outcomes in several published academic venues eitzel et al 2020a b 2021 2022 one lingering issue remained however i was able to publish our work in open access journals which was important both because i was funded by public money from the united states and because the community in mazvihwa and other indigenous people throughout the world should at least theoretically have access to the work even if inconsistent internet access could still limit the practicality of access but some of the journals required that i include the data as part of their commitment to open science of course i agreed heartily with this commitment that was part of why i embraced the idea of a data biography however it was not my data to share and muonde at the time did not have a formal data release process in place so i worked with them to discuss what the rules should be and what i would be able to share publicly in the end we were able to include the suitably anonymized subset of their data that was required to reproduce the results in the papers but this felt far from a universal solution to me so i was definitely left with the question how do we balance open science with data sovereignty 3 4 1 manifesto practices applied in science justice this vignette integrates two experiences studying sts at the science justice research center and working with the muonde trust on community based modeling the two experiences mutually reinforced each other with respect to the manifesto recommendations i was testing this experience also takes place later in the narrative so i was able to incorporate all of the practices into my work below i focus mostly on the community based modeling rather than the justice training which was already analyzed in part through vignette 3 on interdisciplinary training in working to emphasize the community based model as a discussion tool and not a predictive or prescriptive one i was both following epistemic good practice for that type of agent based model and also staying conscious of the power dynamics of the community researcher context i also used the uncertainties in the model to reinforce the idea that the model was a supplement to the community s own extensive knowledge of their system in doing highly engaged community based modeling i was constantly checking with my zimbabwean colleagues regarding the model s accuracy and potential uses impacts and implications this work required all the interdisciplinary fluency i had gained over the years as well as my ability to apply the given modeling technique to incorporate a wide variety of community data mixed methods and triangulation between their different datasets finally i will note that this was my first attempt at an entirely separate set of publications on the modeling process these data biographies have been a great deal of work to prepare and though i am proud of the result it may be unrealistic to expect modelers to write a second analysis of their process after working to publish the results of their models finally i noted that often different manifesto practices applied to different stages of the modeling work model construction validation presentation application and so on 3 4 2 effectiveness of manifesto practices in science justice this project is the first time i believe i have met the goal of my models being relevant trustworthy and just the muonde team was able to use the model we created to improve historical inequities in resource access and to help heal colonial damage to traditional practices and authority eitzel et al 2021 the relevance of the process and model derived from selecting a project that i knew was community based and had been from the start the trustworthiness of the model to my muonde collaborators and their local community and leaders derived from the model s basis in muonde s own empirically gathered data and experience and in their involvement in the model s development and muonde itself is considered a source of trusted expert information in their community community based modeling therefore enhanced both relevance and trustworthiness because the model is being used to correct an unjust colonial land use pattern this project achieves at least in small part the goal of improving inequities at least for one community i did make use of all the other manifesto practices for example using mixed methods to pull together a data biography of muonde s data which helped me use the data appropriately in the model but they were all in support of the community based work 3 5 comparing and contrasting vignettes table 2 summarizes the analysis sections above noting the practices associated with each vignette whether the criteria for better modeling were met and highlights some of the important interactions noted below i directly compare the vignettes to elucidate patterns and potential surprises not all of the vignettes explicitly depict a model vignette 2 s model was never completed vignette 3 is more about interdisciplinary education but they are all good tests of whether and how i applied the manifesto practices to my work and comparing them is also fruitful vignettes 1 and 2 occurred long before i began thinking systematically about how to do better modeling while vignettes 3 and 4 overlapped with the period when i developed the manifesto it is therefore not surprising that i was able to apply more of the practices to the latter two vignettes especially vignette 4 and in particular the practices under the justice theme in turn the later vignettes also demonstrate evidence of processes and outcomes that are more trustworthy relevant and just however this analysis has enabled me to take a close look at the reasons why so many more of the practices applied to the later projects most notably due to the choice of collaborators and projects that allowed me to do impactful community engaged work in both vignette 3 and 4 often there was a clear connection between the manifesto practices and these outcomes for example vignette 1 s models were likely to be more trustworthy to journal editors due to triangulation and the conservative conclusions that emerged from that approach or that the improved relevance and justice outcomes of vignette 4 s models derived largely from the community based selection of impactful research questions i also noted that many of these practices caused me to take longer in my analysis doing sensitivity analysis in my tree models getting metadata to do oak modeling doing interdisciplinary training in the midst of all my other responsibilities and doing community based work this implies that planning for projects to have better outcomes means allowing more time for these processes we may need to shift norms for the speed of analysis to give modelers time to engage with better modeling practices 4 discussion below i return to my research questions did the manifesto practices make my modeling better how did the practices interact where do the manifesto and or the criteria need refinement 4 1 manifesto practices improved my modeling outcomes i found the manifesto to be a useful framework for evaluating my past work conversely examining how the practices applied to my experiences helped me to ground the manifesto and make it less abstract each of my proposed practices to the degree that i applied them in my work did improve the outcomes of my modeling in terms of relevance trustworthiness and justice even asking questions about hypotheticals and missing pieces for example what communities i could have worked with in my tree models revealed places to shift practice in the future making silences speak clarke 2005 among the specific manifesto practices triangulation emerged as particularly important as have the practices that improve transparency data biographies epistemic consistency community based modeling improved all my indicators of better modeling particularly when it integrated many of the other practices the contextualizing practices improved the transparency of modeling while the collaborative practices broadened the empirical basis of models and the justice practices held modeling processes accountable trying to apply justice practices to a project which was originally conceived as an ecological methods project was much more difficult than applying them to deliberately community based work however this comparison is what makes it clear to me that project choice is an important step not to neglect when considering the outcomes of one s work 4 2 interactions between practices did support better modeling for some vignettes there were clusters of practices that mutually reinforced each other in producing better modeling outcomes in the terminology of statistical models higher order interactions for example in vignette 3 on interdisciplinary training epistemic consistency interdisciplinary fluency mixed methods triangulation and impacts and implications were all aligned with each other almost describing the same phenomenon the training programs while in other vignettes these practices did not necessarily interact as a group in vignette 1 on correcting my dissertation work epistemic consistency data biographies triangulation and uncertainty as openness all worked together to improve trustworthiness while in vignette 4 on collaborative modeling power dynamics uncertainty as openness and epistemic consistency worked together to improve relevance and justice in vignette 4 too there was more clustering across the broader themes in which the contextual practices e g epistemic consistency worked in synergy with the justice practices e g impacts and implications indeed i observed the potential for technical and social better practices to mutually support each other i also noted that in vignette 2 on oak modeling different aspects of the modeling process benefited from different clusters of practices in interviewing ecologists about their data mixed methods interdisciplinary fluency and power dynamics came into play and in assembling the datasets i used data biographies triangulation and epistemic consistency on the other hand the model i would have created would have worked with uncertainty as openness and impacts and implications finally some combinations of practices were perversely helpful for example in vignette 1 my conclusions were so conservative as to be not terribly helpful for managers reducing the impact of the research but also the impact of the corrections of my dissertation papers i have mostly noted synergies between practices here but there were also potential conflicts between the practices most notably that specific conceptions of what constitutes open science can be in conflict with the rights of communities to control their own data data sovereignty rainie et al 2017 i successfully navigated this challenge in my work with muonde and there may be situations in which sharing data is not in conflict with control over that data but in many community based projects this warrants careful attention asking who chooses to enclose or restrict information and for whose benefit it may be that negotiating this tension will be different in each situation but more discussion of this potential conflict is warranted 4 3 manifesto practices need refinement not all of the practices were easy to test for each vignette by systematically checking whether the practices interacted i noted ambiguities and complexities that point towards areas of improvement in the manifesto the simplest of these was finding that i needed to carefully define what constituted a data biography in vignette 1 this was largely appendices to the papers while in vignette 4 the data biography was multiple papers describing the process of the modeling also though i had originally called it a data biography i meant the term to apply to more than just data i could easily have called it a model biography for example the justice practices were particularly tricky to apply wilmer et al 2021 with respect to power dynamics it was important to note that sometimes these dynamics worked in a positive way to improve trustworthiness relevance or justice e g the work with muonde and in other cases they worked in a way that reduced the potential for those outcomes e g the interdisciplinary graduate training and both observations were important in thinking about how to do better modeling in addition i needed to carefully delineate that i was referring to power dynamics in the process of the modeling rather than in the subject matter being modeled e g interdisciplinary graduate training projects might have been studying power dynamics but i was referring to the power dynamics within the group itself i found that one useful way to frame questions around power dynamics was to ask what was at stake and who controls it for example in vignette 1 the retraction of my papers was at stake and the career implications that could have had for me and the editors held control over that another important definition in need of careful articulation involved community based modeling namely what was the community in question in the first two vignettes was it natural resource managers ecologists indigenous people and others who value forest ecosystems in different ways my engagement with each of these groups was quite limited in those two stories in the third vignette i defined the community as the graduate students themselves while in the fourth vignette the community was very clearly defined the muonde trust and the farmers in mazvihwa communal area because the project was designed to be community based from the start the three justice practices in general were the least well defined of all the manifesto prescriptions i found that they could be fruitfully refined and nuanced based on what i have learned from this evaluation largely that careful definitions should be part of each of those practices e g who is the community power dynamics between which entities impacts and implications for whom at a higher level in writing this paper i have found that it is helpful to distinguish between specific practices and the larger processes that the practices contribute to and terms like collaborative modeling may be too broad to be clear i used the term here to refer to a group of specific practices e g those described in tienne 2013 but it could also refer to a larger process in addition as i applied my relevance criterion it was useful explicitly define relevant for whom and for what purpose 5 conclusions and future work i have demonstrated an autoethnographic method for assessing modeler s manifesto practices using my own modeling experiences and criteria that modeling should be more trustworthy relevant and just i plan to periodically review the manifesto itself to see if my experience with different practices has shifted or if new ones are necessary including re evaluating my own values in terms of what i want my modeling to accomplish in the world with a single autoethnography we can see where the general resides in the particular merriam and tisdell 2015 and from there start to get a picture of the general this kind of comparative qualitative study can form the foundation of later larger scale possibly quantitative assessments therefore the more modelers who autoethnographically test out proposed modeling manifestos the better we can collectively understand how to improve modeling processes and outcomes upon further personal observation modelers may find that they are already employing many of these improved modeling practices in that case articulating how these practices play out when applied could help in assessing their impact and outcomes autoethnography by modelers can complement the interview and ethnographic methods of sts scholars that seek to illuminate the choices and processes underlying models and modeling products babel et al 2019 i welcome alternative framings of improved modeling outcomes as well as alternative sets of practices other modelers could develop and then test their own manifestos against their own values of what constitutes better modeling or test my manifesto practices or those suggested by others see voinov et al 2014 for their 10 commandments of modeling as an example i do encourage modelers of all kinds to consider my criteria and practices but i recognize that not all practices may apply to all kinds of modeling nor will my specific criteria what is most important is for modelers to consider their own values and how these influence their modeling choices we may decide that we have done high quality work but we might not ask ourselves how our values determine what we think of as high quality or how our values determine what questions we ask in the first place pirsig 1984 we may not stop to consider why we take on certain projects and not others with the ubiquity of models in a wide range of contemporary decision making spaces we as modelers hold a lot of power over the outcomes that can affect many our choices matter when i have presented the manifesto i have received earnest thoughtful questions from people wanting to know how to apply the practices i am suggesting i hope that my analysis and those like it could be helpful in developing data science pedagogy bates et al 2020 and in informing larger conversations on improving modeling processes indeed autoethnography is a technique that can serve us in developing larger norms for more transparent modeling saltelli et al 2020 individual actions can add up to systemic changes elevating the individual stories behind those actions can help us see better how to create more relevant trustworthy and just models which do the work in the world that we hoped they would credit authorship contribution statement m v eitzel all aspects of this work acknowledgments this work was supported by the united states national science foundation award number 1415130 and by a university of california davis academic federation innovative developmental award the funders had no role in study design in the collection analysis and interpretation of data in the writing of the report and in the decision to submit the article for publication the statements made and views expressed in this article are solely the responsibilities of the author and do not reflect the views of the national science foundation this work was enabled by the many scholars who shared their time with me in discussions about science and justice sts modeling and more many of whom also offered insightful comments on various drafts of the manifesto and i thank a subset of them here martha kenney jon solera lizzy hare jenny reardon katherine weatherford darling perry de valpine carl boettiger andrew mathews aaron fisher ashley buchanan eric nost jenny goldstein luke bergmann justin kitzes k b wilson jennifer glaubius trevor caughlin david o sullivan sara stoudt rob sokolowski and peer reviewers and editors at engaging science technology and society environmental modeling and software and plos one declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105690 appendix a supplementary data the following is the supplementary material related to this article mmc s1 more details on selection construction and analysis of the vignettes and validation of the study 
