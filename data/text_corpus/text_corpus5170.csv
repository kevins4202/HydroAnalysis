index,text
25850,large scale hydrological models are demanding both in term of memory allocation and cpu time particularly when assessment of modeling uncertainty is required high performance computing offers the opportunity to reach resolutions not achievable with standard serial coding however the advantages may be offset by poor scalability of the model due to components that have to be executed in series such as to simulate the presence of hydraulic infrastructures driven by this motivation we developed hyperstreamhs a model that adopts a holistic approach to simulate hydrological processes in large river basins with streamflow altered by hydraulic infrastructures the model adopts a dual layer parallelization strategy where the paralleled version of the hydrological kernel is the first layer with the second layer taking care of inverse modeling the results show that the processors should be carefully organized and grouped in order to achieve the best overall performance and suggests that this subdivision is problem specific keywords water resources management large scale hydrological modeling parallel computing high performance computing water infrastructures hydropower hyperstream routing scheme 1 introduction distributed hydrological modeling is widely applied as a tool to inform water resources management and climate change impact assessment studies see e g kundzewicz et al 2007 in assessing water resources and devising management strategies modeling is often extended over long time periods at high spatial resolution particularly when the objective is to evaluate the likely impact of climate change on water resources todd et al 2011 as a consequence distributed hydrological simulations on medium to large watersheds are demanding both in term of memory requirements and computational time needed to simulate the relevant processes over such high resolved domains vivoni et al 2011 liu et al 2013 nonetheless high resolution offers a better representation of spatio temporal dynamics of state variables such as soil water storage rojas et al 2008 and snow accumulation scipión et al 2013 simulation of spatially and temporally variable hydrological fluxes is affected by uncertainty stemming from i an imperfect conceptual model i e epistemic uncertainty ii parameters that should be inferred from observational data because not directly measurable parametric uncertainty and iii input data uncertainty i e measurement errors in meteorological forcing data as well as their spatialization quantification of these uncertainties is a difficult task that attracted a wealth of interest in hydrology as well as in the wider environmental modeling community see e g montanari et al 2009 beven and binley 1992 beven 1993 the curse of dimensionality increases dramatically the number of forward simulations needed to explore the parameters space in inverse modeling procedures as the number of parameters increases thereby rocketing the already high computational cost of high resolution parallelization techniques such as message passing interface mpi mpi forum 1994 and open multi processing openmp dagum and menon 1998 have been applied to distributed hydrological models in order to reduce the computational time and manage efficiently large amount of data for example wu et al 2013 and li et al 2011 implemented the mpi standard into both swat and dwm hydrological models by separating the watershed in units attributed to different processors burstedde et al 2018 employed mpi paradigm in order to enhance computational performances of parflow model through parallel adaptive mesh refinement recently wrf hydro modeling system gochis d j 2020 has been equipped with mpi based libraries in order to manage spatial domain decomposition as well as to improve communication among processors senatore et al 2015 fersch et al 2020 gpu based graphics processing unit parallel computing has also been used to reduce the computational time of hydrological modules dealing with the extraction of the digital drainage network of large river basins ortega and rueda 2010 and parallelization of streamflow accumulation qin and zhan 2012 rueda et al 2016 all these models apply single layer parallel computing with the objective of reducing the computational time of the forward simulations however a limitation in scalability which measures the system s capacity to reduce computational time in proportion to the number of processors emerges when the number of processors grow above a given threshold amdahl 1967 which depends on the model adopted the computational domain and hardware this limitation is due to the following factors i tasks inter dependency ii time spent to transfer information between parallel threads which increases with the number of processors adopted and iii load imbalance i e the uneven computational work distribution between processors although the last limitation can be alleviated through careful coding or employing parallel automatic balancing procedures e g mendicino et al 2006 giordano et al 2020 the first two factors are endemic to parallel computing in addition most of the existing hydrological models contain modules that should be run in series thereby reducing the theoretical speed up regardless the number of processors used amdahl 1967 for example routing schemes based on the solution of the de saint venant equation or on cell by cell mass conservation coupled with lumped streamflow river storage relationships suffer from these limitations yamazaki et al 2011 de paiva et al 2013 when multiple model runs are required in order to explore the parameters space a parallelization scheme dispatching to multiple processors independent forward simulations has been also envisioned pitman 1973 rouholahnejad et al 2012 tristram et al 2014 kan et al 2018 fersch et al 2020 however the efficiency of such single layer parameter domain decomposition scheme is hampered by memory allocation problems arising when managing large amounts of data as typically occurs in large scale hydrological modeling applications the aforementioned limitations can be alleviated by using layered parallelization techniques that use model specific strategies to reduce serial dependencies and improve the gain examples are layering the model s units according to flow direction liu et al 2014 or assigning sub basins to separate cluster nodes with the processors within each node performing the computations at the level of the sub basin liu et al 2016 in another work zhang et al 2016 introduced a dual layer parallelization system for the calibration of dyrim hydrological model in which the higher layer second layer handled multiple parallel forward simulations first layer by means of a standard scheduler approach layered parallelization is therefore appealing for distributed modeling though applications in hydrology are still limited in number an important aspect in hydrological modeling for water resources management is the implementation of human systems hs dealing with water transfer and storage for irrigation energy production and other water uses see e g nazemi and wheater 2015a b for a review on the challenges posed by the incorporation of water resources management modules into earth system models hydraulic infrastructures for water uses introduce changes in the network topology and streamflow timing which have to be taken into account in modeling gregory 2006 their effect is also relevant in developing realistic future scenarios and water management plans including the assessment of the impact of new infrastructures reservoirs for example alter streamflow timing while diversion channels change network topology and require the introduction of additional water budget equations see e g bellin et al 2016 the simulation of these artificial systems therefore poses additional limitations to scalability because of the dependencies in upstream downstream water transfer they introduce which can be alleviated by parallel computing layering we contribute to this ongoing effort by developing a new modeling framework we coined hyperstreamhs which enhances the capabilities of the hyperstream routing scheme recently introduced by piccolroaz et al 2016 hyperstream is a multi scale streamflow routing scheme based on the travel time approach which has been specifically designed for reproducing accurately horizontal hydrological fluxes while avoiding grid refinement beyond what needed for the accurate reproduction of vertical fluxes furthermore it can be easily made parallel due to linearity of the routing scheme the development of improved routing schemes to adequately resolve horizontal fluxes with an acceptable computational effort has been indicated as one of the priorities for the improvement of existing large scale hydrological models see e g the review by clark et al 2015 by adopting a holistic approach we coupled hyperstream with continuous modules for surface and subsurface flow generation and added specific modules dealing with the alterations introduced by water infrastructures such as reservoirs and diversions furthermore hyperstreamhs adopts a dual layer parallelization strategy based on the mpi standard in order to fully exploit both spatial domain and parameters domain decomposition mpi was preferred due to its flexibility and portability i e it can be used with different platforms musiał et al 2008 dual layering is applied by subdividing the processors in groups each one performing forward modeling with a set of parameters and within a group partitioning the computational domain among processors the number of processors assigned to each group is below the threshold over which scalability deteriorates such as to optimize the overall performance scalability of hyperstreamhs is here presented and tested with reference to the adige river basin case study south eastern alps italy in addition the capabilities of the model to reproduce observed streamflows in locations impacted by the presence of human infrastructures are presented with reference to one of the gauging stations available in the case study sect 2 presents the structure of hyperstreamhs the adopted parameterization of physical processes 2 1 and of water transfer and storage infrastructures 2 2 as well as data requirements 2 3 parameters identification procedures are presented in sect 3 followed in sect 4 by the description of the dual layer parallelization strategy and in sect 5 by an example of application to the adige river basin finally concluding remarks are drawn in sect 6 2 hyperstreamhs hydrological model 2 1 natural hydrological system large scale hydrological simulations are performed by coupling the hyperstream routing scheme piccolroaz et al 2016 with a grid based runoff generation model composed by the scs cn runoff model michel et al 2005 combined with a bucket type soil moisture model majone et al 2010 routing is performed by means of the instantaneous unit hydrograph iuh applied to each cell node pair the nodes are selected positions along the river network where the water discharge is computed according to the iuh theory see e g gupta et al 1980 rinaldo and rodriguez iturbe 1996 runoff generated at the hillslope is transferred along the river network by means of a transfer function coinciding with the probability density function pdf of the particle residence time which in turn is given by the pdf of the travel distance from the cell to the node multiplied by the celerity of the flow signal rodríguez iturbe and rinaldo 1997 the pdf of the travel distance is extracted from the digital elevation map dem of the catchment at a finer resolution with respect to the grid used to resolve the vertical fluxes this allows to use a relatively coarse grid of the order of a few kilometers without compromising routing accuracy piccolroaz et al 2016 vertical fluxes are represented as follows firstly precipitation falling on a given cell is classified as rainfall or snow according to a given threshold temperature and the latter is accumulated in the snowpack compartment mass balance is applied to the snowpack with water produced by snowmelting computed by the degree day model rango and martinec 1995 majone et al 2010 the resulting water volume composed by the sum of rainfall and snowmelting is partitioned into infiltration and surface runoff by means of the generalization of the scs cn model proposed by michel et al 2005 in the absence of snowpack and if the precipitation is classified as rainfall the entire precipitation is transferred to the scs cn module soil water dynamics is modelled by a nonlinear reservoir majone et al 2010 with infiltration as input flux and evapotranspiration evaluated by means of the hargreaves and samani 1982 model subsurface flow and deep infiltration as output fluxes deep infiltration enters a linear reservoir representing the groundwater compartment with return flow as output see piccolroaz et al 2015 bellin et al 2016 majone et al 2016 for applications of this modeling framework in alpine catchments a schematic of the vertical water fluxes separation and routing scheme is depicted in fig 1 and additional details are provided in appendix a the modeling framework relies on the following key geometric objects see fig 1 i macrocells i e grid cells where the meteorological forcing is assigned and the vertical water fluxes are evaluated fig 1a and ii nodes located along the stream network where water discharge is computed the drainage directions and the stream network are extracted from the dem according to one of the several criteria available for the identification of the hillslope channel separation e g tarboton et al 1991 lazzaro 2009 the information derived from the dem is used to build the probability density function pdf of the flow path lengths i e width function connecting the hillslope channel transition sites of a macrocell to the first downstream node a width function is evaluated for each macrocell node pair fig 1b and then it is rescaled through a constant stream velocity to obtain the instantaneous unit hydrograph used to transfer the flux to the downstream node see appendix a for details the assumption of constant stream velocity makes the transfer process linear and hence the routing model highly parallelizable it has been shown that using a constant uniform velocity along the river network is not a limitation see e g rinaldo et al 1991 rodríguez iturbe and rinaldo 1997 while it provides great flexibility in parallelization 2 2 human systems module the effects of water uses is simulated by including hydraulic infrastructures represented by nodes edges and reservoirs into the natural network this leads to an augmented network fig 2 shows an example of inclusion of a hydropower system composed by a reservoir a diversion channel a penstock and a power station in addition to the reservoir water is taken from natural channels intersecting the diversion channel intakes and is returned to the natural river network at the powerhouse node the main elements types of hs module and their associated water balance equations are presented below 2 2 1 type reservoir reservoirs represent special nodes where water storage may occur as shown in fig 3 each reservoir is subdivided into three volume pools flood control active and inactive volumes reservoir functioning is described by the curve volume versus elevation and the schedule of water use both entering into the mass balance equation 1 σ q i n t σ q u t q e f t q s p i l l t d v d t where q i n t is the inflow to the reservoir q u is the flow transferred through the inlet gate to the uses q e f is the flow released to the downstream river network according to ecological flow rules q s p i l l is the flow released by the spillways and diversion gates and finally v is the volume of water contained in the reservoir and t is the time the summation in the first two terms of eq 1 indicates that the reservoir may receive water from catchments other than the upstream catchment connected catchments and deliver water to multiple uses mass balance described by eq 1 is applied by considering the following priority of releases a ecological flow q e f local environmental regulations determine the minimum amount of water that must be released to the downstream river network by reservoirs and intake points to preserve freshwater ecosystem functioning b spillways q s p i l l it is the sum of the flux discharged downstream through the spillways and the bottom outlet gates the former operates only when the water level is above the maximum regulation level h m a x r e g while the latter can be operated when the water level is above the minimum regulation level h m i n r e g and can be used to empty the reservoir or to create the volume needed to contain an incoming flood reservoirs may be equipped with one or more spillways with outflow regulated by a gate or not regulated in hyperstreamhs we used an equivalent free non regulated spillway with the following characteristics it starts to operate when the water level in the reservoir is at h m a x r e g and its length l e q is computed by imposing that the water discharge is equal to the maximum outflow q s p i l l m a x when h h m a x therefore the water discharge from the equivalent spillway is given by 2 q s p i l l t l e q c q 2 g h t h m a x r e g 3 2 where c q 0 49 is the coefficient of discharge and l e q is computed according to the following equation 3 l e q q s p i l l m a x c q 2 g δ h f c 3 2 in eq 3 δ h f c h m a x h m a x r e g is the maximum hydraulic head the time step adopted in the solution of eq 1 is reduced during flood events in such a way to reduce the numerical error for example in our simulations the adopted time step is of 1 h for the hydrological kernel of hyperstreamhs which is reduced to 1 min in the solution of eq 1 during flood events c utilization schedule q u for h h m i n r e g a water use schedule i e a time series of the values assigned to q u is provided externally by the user 2 2 2 type intake intakes are nodes where water is diverted to satisfy water demand the water can be sent directly to the final uses or stored in a reservoir for successive utilization see fig 2 intake nodes are also used to model restitution points and confluences between channels or conduits an intake node is located along the river network and is connected by a channel to the man made network or to a reservoir see fig 2a b at the intake the following mass balance equation is applied fig 2b 4 q i n n a t t q d i v m m t q o u t n a t t 0 where q i n n a t and q o u t n a t are the river water discharge immediately upstream and downstream the intake respectively and q d i v m m is the diverted water discharge which is bounded by the maximum water discharge that the diversion channel can convey the diversion channel or conduit may discharge in another channel in a reservoir or directly to the final use in the former case a second mass balance equation should be applied 5 q i n m m t q o u t m m t q d i v m m t 0 the first two terms have a similar meaning as in eq 4 but referred to the man made channel network if the final node of the diversion is connected to a reservoir eq 5 is not included and q i n m m constitutes one of the terms of σ q i n in eq 1 concerning the restitution points fig 2c water flux arriving from the man made network q i n m m is added to the incoming flow q i n n a t to obtain the flux downstream the junction 6 q o u t n a t t q i n n a t t q i n m m t confluences in the man made network see fig 2d are treated in a similar way by adding the flux of the merging channel to the incoming flux in the receiving network 2 2 3 type hydropower plant power plants are treated similarly to restitution points illustrated above the water derived from a node is returned to the river at the restitution point and the generated power is computed as follows 7 h p p t η i 1 n γ q t u r b i δ h i t where q t u r b i and δ h i are respectively the turbined water discharge and the hydraulic head of the i th power unit and n is the number of power units turbine plus alternator of the power plant the hydraulic head δ h i is given by the difference between water elevations of the reservoir and the turbine or of the downstream impoundment depending if an action or a reaction turbine is used in run off the river power plants the head is assumed constant in time finally γ is the specific weight of water and η is the turbine efficiency which is assumed constant in the example of application presented in sect 5 we set η 0 8 corresponding to the mean of the efficiency of 12 out of 22 reservoir hydropower plants belonging to the study area for which this value was known 2 2 4 type diversion channel diversion channels divert water from the natural stream network by means of an intake and route it downstream along the man made channel network these objects are fully characterized by the channel length and water celerity with the latter used to compute the delay with which the water is conveyed downstream and is set by the user prior to the simulation in the example of application presented in sect 5 we assumed a value for the celerity equal to 2 0 m s kumar and singhal 2015 2 3 data requirements hyperstreamhs is conceived as a parsimonious model temperature and rainfall gridded dataset are the meteorological forcing required to run the hydrological kernel dem land use and land cover maps are also needed see appendix a 1 for additional details human systems module requires detailed information of the hydraulic infrastructures of the river basin geometry and position of storage reservoirs head and penstock capacity of hydropower plants location of intakes and restitution points and capacity of diversion channels to show that hyperstreamhs can be succesfully applied in most practical situations in the present work we used only publicly available data reservoir information was retrieved from the italian registry of dams rid available at http dgdighe mit gov it while information on hydropower systems were primarily extracted from public reports such as those provided by enel https www enel it the former public operator that managed italian hydropower production from 1962 to 2004 when the power market was liberalized furthermore information on plants installed power and penstock capacity was retrieved directly from operating companies online databases or from informative leaflets 3 inverse modeling and parameter identification as customary in hydrological simulations the 12 parameters of hyperstreamhs are set such as to provide the best possible representation of observational data typically under the form of water discharge measurements with this objective in mind inverse modeling is therefore applied by searching the parameters hyperspace with the objective of updating the prior parameters distribution bayesian updating or identifying the set of parameters that maximizes a given objective function therefore hyperstreamhs is provided with two searching strategies the particle swarm optimizer pso kennedy and eberhart 1995 robinson and rahmat samii 2004 castagna and bellin 2009 majone et al 2010 piccolroaz et al 2015 and the stratified latin hypercube sampling lhs mckay et al 1979 piccolroaz et al 2016 the former is a genetic searching algorithm which has been shown to optimize the searching path kennedy and eberhart 1995 robinson and rahmat samii 2004 down to the optimal point while the latter is often used to efficiently sample the entire parameters space in both cases we used the nash sutcliffe efficiency nse index nash and sutcliffe 1970 as objective function for streamflow time series evaluated at given nodes of the river network 8 n s 1 σ e 2 σ o 2 where σ o 2 is the variance of the observed streamflow time series and σ e 2 is the variance of the residuals i e the difference between observed and simulated streamflow since parameters inference is a computationally demanding step particularly when the number of parameters is large our objective is to alleviate this burden by means of the dual parallelization strategy described below 4 dual layer parallelization strategy dual layer parallelization is organized in such a way that a given number of forward simulations is run in parallel second layer each with a different set of parameters chosen by the searching algorithm each forward run is performed in parallel as well by partitioning the hydrological kernel among a subset of processors first layer this framework is achieved by exploiting the master slaves paradigm according to the mpi standard for illustration purpose fig 4 shows an example with n p 16 processors named as p 0 p 15 hypothetically available in a hpc system as shown in fig 4a before parallelization all the processors p k k 0 n p 1 belong to the same group named as m p i g r o u p w o r l d and are associated to its corresponding communicator termed according to the mpi terminology as m p i c o m m w o r l d for the sake of clarity here we remind that the concept of m p i g r o u p corresponds to the mathematical concept of s e t i e a well defined collection of distinct objects i e processors with their own identifiers i e processor i d on the other hand a c o m m u n i c a t o r is an object connecting all the processors belonging to the same set and giving them independent identifiers p k k 0 n p 1 according to an ordered topology details about mpi groups communicators and topology can be found in mpi forum 1994 and in gropp et al 1996 afterwards the global group and communicator are split into n g sub sets with each sub set composed by n s n p n g processors each sub set constitutes a univocal set of processors and defines the first layer of the mpi parallelization where independent simulation runs are performed for the sake of clarity it is useful to define the following topological operator p i j p δ k which links global processor topology p k k 0 n p 1 with the local topology where i 0 n g 1 identifies the group i e the sub set to which the processor belongs j uniquely defines the processor i d inside the group k 0 n p 1 is the processor i d according the default global communicator and δ is the topology operator which associates the local communicator m p i l o c a l c o m m to the global communicator m p i c o m m w o r l d in the illustrative example see fig 4b the total number of n p 16 processors is divided into 4 sub sets i e n g 4 with each set containing 4 processors see dashed thick lines in fig 4b the figure also highlights that each sub set has its own local communicator m p i l o c a l c o m m and local identifier p i j for the processor i d in addition in each sub set a m a s t e r processor is defined identified with j 0 i e p i 0 which controls the other processors within the sub set these latter processors are usually termed as s l a v e s and are identified as p i j with j 1 n s 1 notice that each master also acts as slave within its own sub set furthermore the second layer of parallelization is organized creating a new group m p i g r o u p m a s t e r see continuous thick line in fig 4b which gathers the master processor of each sub set together with its own communicator m p i m a s t e r c o m m for the sake of completeness listing 1 shows the mpi fortran code used to split the processors into groups and to set up the local communicator while listing 2 illustrates the instructions applied to set up the communicator between the masters and to exclude the slave processors in the ensuing sub sections the two layers of parallelization are described in details with reference to the implementation within the hyperstreamhs hydrological model listing 1 mpi command to create processor groups listing 1 listing 2 mpi command to create the group of masters listing 2 4 1 first parallelization layer as discussed in sect 2 hyperstreamhs is suitable for parallelization given that i the computational domain is divided into independent macrocells each one with its own meteorological forcing ii vertical water fluxes of a macrocell are independent from those of the other macrocells and iii runoff is transferred from the macrocell to downstream node by means of the linear iuh therefore the runoff produced by the macrocells contributing to a given node can be transferred separately and then summed up to take advantage of these characteristics at the first layer tasks are assigned according to the number of macrocell node pairs identified by the pre processor see sect 2 1 and appendix a 1 fig 5 illustrates the parallelization strategy adopted in the first mpi layer as a first step all processors store the model parameters and the network topology see the horizontal frame reading of input data then the macrocell node pairs are equally subdivided among all the available processors which load the corresponding macrocell node width functions as well as the meteorological forcing pertaining only to the macrocells identified in the pairs selection see the horizontal frame macrocell node splitting memory is allocated to the processors according to the number of associated macro cells and width functions the following step is performed in parallel with each processor that computes the runoffs of the assigned macrocells and transfers them to the connected downstream node see the horizontal frame hydrological kernel in fig 5 streamflow is then aggregated at each network node by summing up all the contributions pertaining to the macrocells node pairs notice that this operation refers only to the streamflow contributions originating from the natural component of the hydrological model once streamflow is aggregated at each node level a single processor is designed to perform the serial computations needed to i perform the water budgets at the nodes of the hydraulic infrastructures see sect 2 2 and ii routing along the natural and man made network of water fluxes simulated at the nodes see horizontal frame hs module and final routing in figure 5 notice that among the possible parallelization strategies for balancing the computational load the equally subdivision of macrocell node pairs over the available processors presents the key advantage that the interaction between processors is limited to streamflow aggregation at the network nodes thereby avoiding the exchange of information on vertical fluxes as a consequence each processor performs independent macrocell node convolution without waiting processed data from macrocells belonging to other processors an overall reduction of computational times is then achieved as well as an improved load balance between processors which perform approximately the same number of convolutions as a side effect a limited number of macrocells may be assigned to two or more processors see second horizontal panel in fig 5 which leads to replicate computations of the same vertical fluxes in these cells however the associated computational burden is less than the gain achieved by using this strategy further improvement of load balancing can be envisioned by implementing a subdivision of macrocell node pairs between processors based on the macrocell node convolution length which is a proxy of the computational time required to transfer runoff from the macrocell to the pertaining node in this respect profiling of the code and implementation of automatic load balancing techniques such as scatter type decomposition e g mendicino et al 2006 or graph partitioning e g karypis and kumar 2009 will be considered as possible further code optimization 4 2 second parallelization layer a second parallelization layer can be introduced when the application includes inverse modeling i e parameter identification in hyperstreamhs we implemented the latin hypercube sampling lhs and the particle swarm optimization pso algorithms but other procedures such as markov chain monte carlo see e g vrugt 2016 can be implemented as well fig 6 a and b illustrates a schematic of the implemented dual layer framework for lhs and pso respectively the n p processors that will be used are subdivided into n m a s t e r s masters each one having n s l a v e s slaves such that n m a s t e r s n s l a v e s n m a s t e r s n p in the examples shown in fig 6 the masters are 4 each one with 3 slaves for a total of n p 16 processors the master of masters processor indicated with p 0 0 in fig 6 equally subdivide the sets of parameters to the n m a s t e r s masters and each master performs the forward simulations with the assigned parameters by using n s l a v e s 1 processors in parallel we recall that a master also acts as slave within its own sub set as described in sect 4 1 this procedure is iterated until the space of parameters is explored as desired with the lhs algorithm the parameters space is explored visiting the locations of a pre defined grid the sequence of the locations that are visited is established by the master of masters processor p 0 0 therefore the masters do not communicate among them until the end of the procedure when the efficiency metric at all the explored grid locations is collected and further elaborated according to the objectives of the simulation see fig 6a for example a simple sorting will be performed if the objective is to identify the set of parameters providing the best efficiency metric while more complex elaborations will be performed in case of bayesian updating pso on the other hand requires that at the end of each forward simulation the efficiency metric of each particle is compared with its personal best and with the overall best the best among all the particles in order to update all the personal bests and the global best as well see fig 6b this is needed because the personal bests and the global best are used in the pso algorithm to move the particles in the space of parameters at each iteration step this procedure thus requires inter master communication since all the forward simulations controlled by the masters need to be completed before updating the position of the particles if more than one particle is assigned to a master this may happen when the number of particles is larger than the number of masters the above control and the movement of the particles can be performed only after all the forward simulations have been completed the cycling over the parameter sets assigned to each master is illustrated with red boxes in fig 6 for both lhs and pso while inter master communication occurring at every pso iteration is represented with a red horizontal arrow see fig 6b 5 example of application computational scalability of hyperstreamhs and its capability to reproduce observed streamflows in locations impacted by the presence of human infrastructures are discussed here with reference to the adige river basin closed at the gauging station of vò destro 45 44 6 5 n 10 57 26 9 w with a contributing area of 10600 k m 2 see fig 7 the adige is a typical alpine river basin with a complex morphology characterized by deep valleys 131 m a s l at vò destro and high mountain crests maximum elevation of 3905 m a s l at cima ortles in the north western part of the basin the annual average precipitation ranges from 500 mm in the north west to 1600 mm in the southern portion of the basin laiti et al 2018 diamantini et al 2018 while streamflow has a typical alpine regime with two seasonal maxima one occurring in spring summer due to snow and glacier melt and the other in autumn triggered by cyclonic storms chiogna et al 2016 di marco et al 2021 lutz et al 2016 mallucci et al 2019 fig 7 shows the natural river network the superimposed man made network red edges the hydropower plants and the reservoirs though fragmented the man made network is distributed over the entire river system and exerts a significant impact on streamflow zolezzi et al 2009 bellin et al 2016 majone et al 2016 pérez ciria et al 2019 5 1 model setup the characteristics of hyperstreamhs are illustrated by means of two applications i streamflow simulation in the noce river which is heavily impacted by hydropower particularly in its lower reaches see the upper right panel of fig 7 and ii analysis of the computational performances with reference to the entire adige all the simulations were conducted at a daily time step in the period 1989 2006 by using a grid size of 5 k m streamflow time series collected at the trento bronzolo malè and mezzolombardo gauging stations see fig 7 were used as observations in the inverse modeling experiments furthermore for all configurations the first two years of simulations were used as a spin up and thus were excluded from the computation of the nse efficiency index precipitation and temperature were extracted from the adige dataset mallucci et al 2019 while land use and land cover considered invariant in time were extracted fromthe corine 2006 product http www eea europa eu publications cor0 landcover the width functions used in the routing scheme were obtained by an offline morphological analysis of the 30 m resolution dem provided by eudem project https www eea europa eu data and maps data eu dem the characteristics of the hydraulic infrastructures were retrieved from the italian registry of dams rid available at http dgdighe mit gov it and from public reports and informative leaflets of the hydropower companies parameters inference for the noce experiment was performed adopting the malè gauging station as calibration node and by including in the simulations the presence of the hydraulic infrastructures see the inset of fig 7 management rules of the reservoirs were inherited by the work of bellin et al 2016 and are at the monthly time scale pso was adopted as calibration algorithm validation of the model was performed at the downstream mezzolombardo gauging station which streamflow is dominated by the release of the mezzocorona hydropower plant which is fed by the water derived from the mollaro reservoir which in turn receives the water turbined by the taio phydropower plant fed by the santa giustina reservoir see the upper right panel of fig 7 results of the simulation are shown in section 5 2 concerning speed up analyses a few different levels of detail in representing the hydraulic infrastructures were implemented in order to investigate the computational load of including them in the simulation in a first setup all the hydraulic infrastructures in the adige were included 30 storage reservoirs 41 intake points and 40 hydropower plants for a total including all available streamflow gauging stations of 138 nodes of the 27 gauging stations included into the river basin the two stations located at trento and bronzolo were used for parameters inference furthermore a simplified version of the full setup was adopted this time only considering 5 out of the 138 nodes 2 storage reservoirs plus three gauging stations trento and bronzolo used for inference of the model parameters and vò destro as closing section the two setups differ in the number of macrocells node pairs that need to be considered in the parellelization of the hyperstreamhs model 569 and 1167 macrocells node pairs for the 5 nodes and 138 nodes cases respectively according to the parallelization strategy described in sect 4 the computational performance of hyperstreamhs and its scalability are tested considering both a single and a dual layer parallelization scheme see sects 5 3 and 5 4 respectively in the forward simulations of the one layer scalability experiment the parameters characterizing the natural hydrological component see appendix a were fixed equal to the values obtained during one of the optimization experiments described in sect 5 4 in the case of the dual layer implementation we considered 500 runs for lhs and 10 000 for pso specifically 100 iterations with 100 particles it is worth to clarify that here we are not interested either in evaluating the convergence to the global optimum with pso scheme or in a full uncertainty assessment of the posterior pdf of the model parameters but rather to evaluate how hyperstreamhs scalability is influenced by the adopted setups as a quantitative dimensionless metric of scalability we adopted the speed up s e g mendicino et al 2006 9 s t s t n p where t s is the computational time of a serial simulation i e when only a processor is used and t n p is the computational time of the same simulation conducted using n p processors under ideal scaling conditions speed up is linear meaning that a reduction in the computational time of n p times with respect to the serial simulation is achieved when n p processor are used we remark that ideal speed up however is a theoretical condition due both to communication times between processors and to portions of the program that can not be parallelized 5 2 hydrological modeling performances in the presence of human systems in the middle course of the noce river streamflow is stored into the large reservoir of santa giustina storage capacity of about 182 m m 3 to be used in the taio hydropower plant maximum penstock capacity of 66 m 3 s 1 the water turbined in the taio powerplant is then collected by the mollaro reservoir which has a small yet appreciable regulation capacity 0 86 m m 3 and then transferred to the mezzocorona hydropower plant maximum penstock capacity of 60 m 3 s 1 mezzocorona plant is located along the noce main course 2 5 k m upstream of mezzolombardo thus exerting a strong control on streamflow alterations observed at the gauging station see the inset of fig 8 calibration of hyperstreamhs to the 1991 2006 observed streamflow at malè gauging station provided the satisfactory nse index of 0 77 the resulting model s parameters have been used to compute streamflow at mezzolombardo either including or not the hydropower systems of taio fed by the santa giustina reservoir and mezzocorona fed by the mollaro reservoir hereafter these two cases are referred to as actual and natural conditions respectively fig 8a compares the time series of both simulations with observations at mezzolombardo while the corresponding comparison in terms of flow duration curves is depicted in fig 8b nse efficiencies were 0 27 and 0 43 in natural and actual conditions respectively though in absolute terms these nse values indicate that streamflow is reproduced partially the gain in adding the available though incomplete information on hydraulic infrastructures is remarkable this considering that operational schedule of the reservoir is known only in average and at monthly time scale which is a limitation in the case at hand given that hydropower companies typically operate reservoirs at finer time scales weekly daily up to the hourly scale in order to maximize the revenue nonetheless the modeling framework is able to generally reproduce the observed streamflow pattern in terms of both timing and magnitude with biases occurring solely as a consequence of the non perfect knowledge of high resolved in time reservoir operating rules see the inset of fig 8a for easiness of visualization on the other hand the inclusion of reservoir regulation by means of the human systems module allows a very good reproduction of the peak flows this is not the case of the natural simulation which completely fails in reproducing the observed streamflow these differences are even more evident when considering the flow duration curves depicted in fig 8b both natural and actual cases are able to reproduce fairly well high flows i e 80 m 3 s 1 and higher which are indeed associated to the hydrological response of the watershed on the contrary the mismatch widens moving from intermediate to low flows i e 40 m 3 s 1 and lower where the natural simulation misses completely the observed streamflow while the inclusion of the hydraulic infrastructures allows to accurately reproduce the observed flow duration curve 5 3 first layer parallel performance in a first set of experiments aimed at assessing computational performance hyperstreamhs was applied to the entire adige catchment closed at vò destro it was run by choosing 5 and 138 nodes where to compute streamflow and with two different setups a natural scenario neglecting the impact of hydraulic infrastructures and a second scenario in which the infrastructures are included as described above hence a total of four configurations were investigated in general both the number of macrocell node width functions and the number of inter processors communications increase with the number of nodes leading to a larger computational burden which is increased further when the hydraulic infrastructures are added fig 9 shows the computational time against the number n p of processors in log log scale while the inset illustrates the corresponding speed up computations have been performed by using 1 2 4 8 16 32 64 processors theoretical computational time and speed up corresponding to 100 efficiency in parallel computing are shown with dashed lines as expected there is a strong dependency on the number of nodes the test case with 5 nodes shows a better speed up than the case with 138 nodes and exhibits small differences between the natural and the actual scenario the low incidence of computations that should be performed in series is the reason of the small difference between the natural and actual scenario for the 5 nodes case the difference widens when 138 nodes are used which loss speed up rather soon see inset of fig 9 for n p 8 the curve of the speed up flattens and there is no longer an advantage in increasing the number of processors this limit is extended to n p 32 for 5 nodes though the gain in term of speed up is significantly lower than the theoretical one when more than 8 processors are used both cases show an increase in computational time for n p 32 this shows also that the fraction of computations with respect to the total that should be performed in series increases with the number of nodes thereby reducing the gain achieved by using a larger number of processors in addition the large number of reservoirs 30 in the case with 138 nodes explains the difference in the computational time between natural and actual scenarios green lines in fig 9 in fact when the number of infrastructures increases a progressively larger share of time should be spent in performing the water budgets needed to evaluate the effect of the hydraulic infrastructures as described in sect 2 2 in the specific case with 138 nodes the difference between the computational time between the natural and actual scenarios increased from 17 15 when only one processor is used up to 35 74 when 64 processors are employed loss of speed up due to serial computations involved in streamflow routing is a well known drawback in parallel computing typical of single layer decomposition schemes see e g wang et al 2012 and zhang et al 2016 nevertheless a first layer of parallelization of the hydrological model reduces the computational time with respect to a single processor run and more importantly allows to handle a larger amounts of data overcoming allocation problems typical of shared memory codes according to lai et al 2018 this is a mandatory feature in order to cope with the large amount of data that should be managed in large scale simulations on the light of dual layer approach we adopted in the present work the analysis of scalability of the first mpi level represents a preliminary step in order to define the optimal number of processors to be used as slaves in the final dual layer mpi implementation 5 4 second layer parallel performance in this section we analyze several combinations of master slave processors in a two layer parallelization with the objective to identify possible optimal conditions these combinations are intermediate between two extreme cases a single processor at one extreme the master acts also as a slave and 32 masters each one with its own slave for a total of 64 processors on the other extreme the former case is assumed as reference because it represents the standard situation of sequential computation all the analyses are performed with both 5 and 138 network nodes and with the presence of hydraulic infrastructures speed up of the investigated cases as a function of the number of processors both masters and slaves is shown in figs 10 and 11 for lhs and pso respectively fig 10a and b shows the results of the simulations conducted with lhs with 5 and 138 network nodes respectively for the 5 node case when a single master is used the speed up curve flattens as the number of processors increases on the other hand speed up decreases for n p 32 in the 138 nodes case when 1 or 2 masters are used this is in accordance with the results presented in fig 9 given that for a single master configuration the second mpi layer is deactivated a similar behavior is observed for 2 and 4 masters in the case of 138 nodes fig 10a also when inverse modeling is performed with pso fig 11 however as the number of masters increases the speed up for pso diverges from the theoretical behavior more than for the lhs case in both the 5 and 138 network nodes configurations fig 11a and b the different behavior characterizing the dual layer scheme with lhs and pso is further investigated in fig 12 a and b respectively computations have been performed with 64 processors subdivided in masters shown in the abscissa and slaves indicated above the bullets in the graph as expected in both cases one master results in the largest computational time and while for the lhs scheme the minimum is obtained with 32 masters each one with a slave for pso the minimum is obtained with 8 masters with 7 slaves each see fig 12a and 12b in both cases the computational time reduces quickly as the number of masters increases from 1 to 8 and the marginal gain achieved by increasing the number of masters beyond 8 is small for lhs while for pso the computational time increases this is the consequence of the communication between the masters only active within the pso algorithm that becomes progressively more relevant as the number of masters n m a s t e r s increases consequently the optimal configuration of the dual layer setup depends not only on the number of available processors but also on the number of macrocell node pairs and the network topology results presented in this section also highlight that for a given total number of processors the dual layer strategy applied to lhs scheme presents the best performances as the number of masters is increased at the expenses of the slaves with the only limit being represented by memory availability on the contrary pso requires a preliminary scalability analysis of the forward simulation to identify the optimal number of masters dealing with the inverse modeling procedure i e the second mpi layer 6 conclusions we presented a new dual layer mpi parallel hydrological model named hyperstreamhs specifically developed to deal with large scale simulations in river basins containing hydraulic infrastructures the model allows inverse modeling with the nash sutcliffe efficiency metric but other metrics could be easily implemented and the space of parameters explored by using either the latin hypercube sampling or particle swarm optimizer hyperstreamhs is designed such as to facilitate parallel computing being composed by i a runoff generation module that solves for the vertical water fluxes on the independent macrocells in which the computational domain is subdivided ii a streamflow routing scheme based on the width function iuh theory with the assumption of constant stream velocity which makes the transfer process linear taking advantage of these characteristics we built a dual layer parallelization strategy and designed the inclusion of hydraulic infrastructures for example reservoirs and diversion channels in such a way to reduce the computational time through an optimal use of the available processors the main characteristics of hyperstreamhs are as follows i an optimal integration of natural hydrological components with water transfer and accumulation due to the management of hydraulic infrastructures such as reservoirs diversion channels intakes and hydropower systems this results in a holistic modeling framework for hydrological simulations at multiple scales in river basins with streamflow altered by water uses ii the adoption of a dual layer parallelization strategy where the parallelized version of the hydrological kernel including the modules dealing with hydraulic infrastructures is the first level with the second level taking care of inverse modeling aimed at the identification of model s parameters including their uncertainty iii the optimal configuration of the dual layer setup masters and slaves partitioning depends not only on the number of available processors but also on the inverse modeling algorithm adopted iv finally the dual layer structure adopts the mpi standard which ensures high portability and optimal exploitation of hpc resources hyperstreamhs has been tested in the adige river basin italy showing that the adopted dual layer parallelization scheme optimizes both the speed up of simulations and the allocation of hpc resources in particular the results showed the clear advantages of considering a dual layer parallelization strategy which compensates for the sub optimal gain of the hydrological kernel when increasing the number of processors especially in more regulated configurations this suggests to use a portion of the processors for the second layer parallelization dealing with the inversion procedure furthermore the capabilities of the model to reproduce observed streamflows in locations heavily impacted by the presence of human infrastructures have been presented and discussed overall the model showed high potential for applications aimed at supporting water resources management and optimization issues in regulated river basins declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research received financial support by the energy oriented centre of excellence eocoe ii ga number 824158 funded within the horizon2020 framework of the european union and by the project seasonal hydrological econometric forecasting for hydropower optimization she funded within the call for projects research südtirol alto adige 2019 autonomous province of bozen bolzano south tyrol this research has been also supported by the italian ministry of education university and research miur under the departments of excellence grant l 232 2016 streamflow data were provided by the hydrographic office of the autonomous province of bolzano www provincia bz it hydro and by the ufficio dighe of the autonomous province of trento www floods it a natural hydrological conceptual model in this appendix the vertical water flux generation module adopted in hyperstremhs model is presented together with the main concepts of hyperstream routing scheme piccolroaz et al 2016 a 1 computational grid the spatial domain is partitioned into m macrocells of equal shape and size fig 1b furthermore n nodes are identified where streamflow is computed fig 1a macrocells can be defined such that the hydrological model shares the same grid of an overlaying climate model or of a gridded dataset providing the input meteorological forcing the nodes are arbitrarily distributed along the river network and typically are located in correspondence of existing gauging stations where streamflow observations are available needed for model calibration and validation or at relevant nodes of hydraulic infrastructures that need to be simulated by the human system module a one time and offline pre processing step is run to prepare the geometrical information needed to implement the streamflow routing scheme the dem is analyzed in order to extract the river network the drainage characteristics of the study area and to derive the corresponding geomorphological width functions for each macrocell node pair see fig 1b other properties useful for the evaluation of the water fluxes e g average elevation soil use and type crop coefficient etc are computed for each macrocell based on the analysis of the available dem and land use land cover spatial maps a detailed example of macrocell discretization and width functions derivations can be found in piccolroaz et al 2016 a 1 2 vertical water flux module the vertical water flux module adopted in hyperstreamhs employs the formulation proposed by laiti et al 2018 it relies on the coupling of the continuous soil moisture accounting scs cn scheme for surface flow generation michel et al 2005 with a non linear bucket model for soil moisture dynamics majone et al 2010 and a linear reservoir model to simulate the base flow component in addition a simple degree day model is used for the simulation of snow dynamics while hargreaves and samani 1982 formulation is employed for the computation of potential evapotranspiration e t p a schematic of the water flux generation module is presented in fig 1a while each component is presented below the degree day model for snow accumulation and melting dynamics is based on the following water balance equation e g rango and martinec 1995 hock 2003 majone et al 2010 a 1 d h s d t p s p m where h s l is the snowpack water equivalent p s l t 1 is the solid precipitation intensity and p m l t 1 is the snowmelt intensity p s and p m are quantified according to the following equations depending on two air temperature thresholds a 2 p s p t a t s 0 t a t s a 3 p m 0 t a t m c m t a t m t a t m where t a c is the air temperature and t s c and t m c are two threshold temperatures the former is the temperature below which precipitation is assumed as solid and the latter the temperature above which the snowpack melts furthermore c m l t 1 c 1 is the melting factor providing the amount of snow melted per unit of time and temperature notice that when t s t a t m precipitation is liquid but the energy input to the snowpack is not sufficient for triggering melting the effective liquid precipitation p e l t 1 entering the soil can be then computed as follows p e 0 t a t s p t s t a t m p p m t a t m the degree day approach used here requires the mean daily air temperature t a as input thus provides mean daily values of p m if the time step integration of the hyperstreamhs model is less than one day the melt water contribution is evenly distributed during the day soil moisture is controlled by the following mass conservation equation a 4 d s m d t p e q r q p e t r where s m l is the soil moisture q r l t 1 is the surface runoff rate q p l t 1 is the leakage flux i e the water flux from the top soil layer towards groundwater and e t r l t 1 is the real evapotranspiration the surface runoff rate q r is evaluated according to the procedure proposed by michel et al 2005 which is an extension of the well known scs cn approach u s soil conservation service 1964 accounting for a time varying soil moisture in the active soil in particular q r is evaluated as follows q r p e s m s a s 2 s m s a s s m s a 0 s m s a where s l is the maximum potential soil infiltration and s a l is the threshold above which runoff is generated s is given by the product of the maximum potential infiltration estimated from the land use and lithological characteristics of the soil s l and a scaling coefficient c s such that a 5 s c s s this correction allows to account for possible uncertainties in the identification of s s a is the soil moisture at the beginning of a precipitation event plus the initial abstraction and is assumed proportional to s through the following relationship a 6 s a c a s where c a is a scaling coefficient notice that p e q r is the infiltration rate into the soil layer and that its saturation level is given by s s a daily reference evapotranspiration e t 0 l t 1 is estimated through the equation proposed by hargreaves and samani 1982 based on mean minimum and maximum daily air temperature t a following allen et al 1998 e t 0 is multiplied by a monthly varying crop coefficient k c in order to estimate potential evapotranspiration e t p thus accounting for the presence of specific crop or natural vegetation and their seasonal vegetative conditions a 7 e t p k c 0 408 0 0023 r a t a 17 8 t a m a x t a m i n where r a is the extraterrestrial radiation m j m 2 d 1 finally real evapotranspiration e t r is computed taking into account that evapotranspiration reaches its potential upper limit only when soil moisture is larger than the field capacity s m f c and that e t r tends to zero as s m approaches its residual limit s m r e t r 0 s m s m r e t p s m s m r s m f c s m r s m r s m s m f c e t p s m s m f c where s m r s s a c r s m f c s s a c f c and c f c and c r are calibration coefficients smaller than one in particular for values of s m between s m r and s m f c e t r is assumed to vary linearly from zero to e t p similarly to the snowmelting e t r is evaluated as daily average and if the computational time step of hyperstreamhs is smaller than one day e t r is evenly distributed during the day the leakage flux from the active soil q p is evaluated through the following exponential law a 8 q p q r e f e s m s m r μ 1 where q r e f l t 1 and μ l are calibration parameters controlling the maximum and rate of variation of the leakage flux respectively we notice that both q p and e t r tends to zero as s m approaches s m r thus avoiding s m to drop below its lower physical bound s m r in absence of infiltration i e when p e q r 0 similar exponential relationships for storage discharge dynamics coupled with the surface runoff model by michel et al 2005 have been successfully applied in previous applications conducted in alpine and mediterranean catchments piccolroaz et al 2015 bellin et al 2016 majone et al 2012 2016 the leakage flux q p is then divided into two components through a partition parameter α the first component contributes to the runoff as interflow q i l t 1 whilst the latter constitutes deep percolation to groundwater q d p l t 1 q i q p α q d p q p 1 α finally q d p feeds a linear reservoir model described by the following continuity equation a 9 k d q b d t q d p q b where q b l t 1 is the baseflow contributing to the streamflow and k is a calibration parameter the vertical water flux generation model described above is applied to all macrocells separately using as input the meteorological forcing pertaining to each of them at each macrocell the total runoff per unit area q s l t 1 is evaluated by summing together the surface runoff rate q r the interflow q i and the baseflow contribution q b a 1 3 routing algorithm following piccolroaz et al 2016 the streamflow q k i t l 3 t 1 generated by macrocell i and contributing to the node k at time t reads as follow a 10 q k i t a k i 0 t q s i t τ f k i τ d τ a k i q s i t f k i t where a k i l 2 is the fraction area of macrocell i contributing to node k q s i l t 1 is the vertical water flux per unit area produced by the macrocell i f k i is the pdf of the travel times of macrocell i relative to node k obtained by rescaling the width function built in the pre processing step by the stream velocity and the asterisk denotes convolution in doing this it is assumed that q s i is constant through the macrocell and is evaluated according to the vertical flux generation module described above the total streamflow at node k q k t is computed as the sum of the contribution of each macrocell to that node plus the streamflow transferred from the nodes upstream of k a 11 q k t i 1 m k c o n q k i t j 1 n k u p q j t τ j k where τ j k d j k v c t is the travel time from node j located upstream of k to node k d j k l is the distance between the two nodes v c l t 1 is the stream velocity m k c o n is the number of macrocells contributing to node k and n k u p is the number of nodes upstream of k a 4 model s parameters the model requires that 12 parameters are set 11 pertaining to the vertical water flux generation module and 1 to the hyperstream routing scheme spatial heterogeneity of evapotranspiration and infiltration is reproduced by setting the parameters k c and s as spatially variable with their deterministic values chosen from infiltration capacity and soil use maps all the remaining parameters are assumed as spatially uniform but uncertain the routing scheme requires the definition of a single parameter the stream velocity v c which is assumed constant thus making the model linear and easily parallelizable the list of the 12 calibration parameters with their units and range of variation is presented in table a1 table a 1 list of the calibration parameters with their range of variation table a 1 parameter description range of variation unit t s temperature threshold for snow precipitation 2 6 c t m temperature threshold for snow melting 2 6 c c m snow melting factor 0 10 m m c 1 d 1 c s parameter of the rainfall excess model 0 1 10 c a parameter of the rainfall excess model 0 01 1 q r e f parameter of the nonlinear bucket 10 7 10 3 m m s 1 μ parameter of the nonlinear bucket 0 5 300 m m c f c coefficient for field capacity 0 1 c r coefficient for residual soil moisture 0 0 25 k mean residence time for baseflow linear reservoir 200 1000 d a y α partition coefficient for leakage flux 0 1 v c stream velocity 0 2 4 m s 1 
25850,large scale hydrological models are demanding both in term of memory allocation and cpu time particularly when assessment of modeling uncertainty is required high performance computing offers the opportunity to reach resolutions not achievable with standard serial coding however the advantages may be offset by poor scalability of the model due to components that have to be executed in series such as to simulate the presence of hydraulic infrastructures driven by this motivation we developed hyperstreamhs a model that adopts a holistic approach to simulate hydrological processes in large river basins with streamflow altered by hydraulic infrastructures the model adopts a dual layer parallelization strategy where the paralleled version of the hydrological kernel is the first layer with the second layer taking care of inverse modeling the results show that the processors should be carefully organized and grouped in order to achieve the best overall performance and suggests that this subdivision is problem specific keywords water resources management large scale hydrological modeling parallel computing high performance computing water infrastructures hydropower hyperstream routing scheme 1 introduction distributed hydrological modeling is widely applied as a tool to inform water resources management and climate change impact assessment studies see e g kundzewicz et al 2007 in assessing water resources and devising management strategies modeling is often extended over long time periods at high spatial resolution particularly when the objective is to evaluate the likely impact of climate change on water resources todd et al 2011 as a consequence distributed hydrological simulations on medium to large watersheds are demanding both in term of memory requirements and computational time needed to simulate the relevant processes over such high resolved domains vivoni et al 2011 liu et al 2013 nonetheless high resolution offers a better representation of spatio temporal dynamics of state variables such as soil water storage rojas et al 2008 and snow accumulation scipión et al 2013 simulation of spatially and temporally variable hydrological fluxes is affected by uncertainty stemming from i an imperfect conceptual model i e epistemic uncertainty ii parameters that should be inferred from observational data because not directly measurable parametric uncertainty and iii input data uncertainty i e measurement errors in meteorological forcing data as well as their spatialization quantification of these uncertainties is a difficult task that attracted a wealth of interest in hydrology as well as in the wider environmental modeling community see e g montanari et al 2009 beven and binley 1992 beven 1993 the curse of dimensionality increases dramatically the number of forward simulations needed to explore the parameters space in inverse modeling procedures as the number of parameters increases thereby rocketing the already high computational cost of high resolution parallelization techniques such as message passing interface mpi mpi forum 1994 and open multi processing openmp dagum and menon 1998 have been applied to distributed hydrological models in order to reduce the computational time and manage efficiently large amount of data for example wu et al 2013 and li et al 2011 implemented the mpi standard into both swat and dwm hydrological models by separating the watershed in units attributed to different processors burstedde et al 2018 employed mpi paradigm in order to enhance computational performances of parflow model through parallel adaptive mesh refinement recently wrf hydro modeling system gochis d j 2020 has been equipped with mpi based libraries in order to manage spatial domain decomposition as well as to improve communication among processors senatore et al 2015 fersch et al 2020 gpu based graphics processing unit parallel computing has also been used to reduce the computational time of hydrological modules dealing with the extraction of the digital drainage network of large river basins ortega and rueda 2010 and parallelization of streamflow accumulation qin and zhan 2012 rueda et al 2016 all these models apply single layer parallel computing with the objective of reducing the computational time of the forward simulations however a limitation in scalability which measures the system s capacity to reduce computational time in proportion to the number of processors emerges when the number of processors grow above a given threshold amdahl 1967 which depends on the model adopted the computational domain and hardware this limitation is due to the following factors i tasks inter dependency ii time spent to transfer information between parallel threads which increases with the number of processors adopted and iii load imbalance i e the uneven computational work distribution between processors although the last limitation can be alleviated through careful coding or employing parallel automatic balancing procedures e g mendicino et al 2006 giordano et al 2020 the first two factors are endemic to parallel computing in addition most of the existing hydrological models contain modules that should be run in series thereby reducing the theoretical speed up regardless the number of processors used amdahl 1967 for example routing schemes based on the solution of the de saint venant equation or on cell by cell mass conservation coupled with lumped streamflow river storage relationships suffer from these limitations yamazaki et al 2011 de paiva et al 2013 when multiple model runs are required in order to explore the parameters space a parallelization scheme dispatching to multiple processors independent forward simulations has been also envisioned pitman 1973 rouholahnejad et al 2012 tristram et al 2014 kan et al 2018 fersch et al 2020 however the efficiency of such single layer parameter domain decomposition scheme is hampered by memory allocation problems arising when managing large amounts of data as typically occurs in large scale hydrological modeling applications the aforementioned limitations can be alleviated by using layered parallelization techniques that use model specific strategies to reduce serial dependencies and improve the gain examples are layering the model s units according to flow direction liu et al 2014 or assigning sub basins to separate cluster nodes with the processors within each node performing the computations at the level of the sub basin liu et al 2016 in another work zhang et al 2016 introduced a dual layer parallelization system for the calibration of dyrim hydrological model in which the higher layer second layer handled multiple parallel forward simulations first layer by means of a standard scheduler approach layered parallelization is therefore appealing for distributed modeling though applications in hydrology are still limited in number an important aspect in hydrological modeling for water resources management is the implementation of human systems hs dealing with water transfer and storage for irrigation energy production and other water uses see e g nazemi and wheater 2015a b for a review on the challenges posed by the incorporation of water resources management modules into earth system models hydraulic infrastructures for water uses introduce changes in the network topology and streamflow timing which have to be taken into account in modeling gregory 2006 their effect is also relevant in developing realistic future scenarios and water management plans including the assessment of the impact of new infrastructures reservoirs for example alter streamflow timing while diversion channels change network topology and require the introduction of additional water budget equations see e g bellin et al 2016 the simulation of these artificial systems therefore poses additional limitations to scalability because of the dependencies in upstream downstream water transfer they introduce which can be alleviated by parallel computing layering we contribute to this ongoing effort by developing a new modeling framework we coined hyperstreamhs which enhances the capabilities of the hyperstream routing scheme recently introduced by piccolroaz et al 2016 hyperstream is a multi scale streamflow routing scheme based on the travel time approach which has been specifically designed for reproducing accurately horizontal hydrological fluxes while avoiding grid refinement beyond what needed for the accurate reproduction of vertical fluxes furthermore it can be easily made parallel due to linearity of the routing scheme the development of improved routing schemes to adequately resolve horizontal fluxes with an acceptable computational effort has been indicated as one of the priorities for the improvement of existing large scale hydrological models see e g the review by clark et al 2015 by adopting a holistic approach we coupled hyperstream with continuous modules for surface and subsurface flow generation and added specific modules dealing with the alterations introduced by water infrastructures such as reservoirs and diversions furthermore hyperstreamhs adopts a dual layer parallelization strategy based on the mpi standard in order to fully exploit both spatial domain and parameters domain decomposition mpi was preferred due to its flexibility and portability i e it can be used with different platforms musiał et al 2008 dual layering is applied by subdividing the processors in groups each one performing forward modeling with a set of parameters and within a group partitioning the computational domain among processors the number of processors assigned to each group is below the threshold over which scalability deteriorates such as to optimize the overall performance scalability of hyperstreamhs is here presented and tested with reference to the adige river basin case study south eastern alps italy in addition the capabilities of the model to reproduce observed streamflows in locations impacted by the presence of human infrastructures are presented with reference to one of the gauging stations available in the case study sect 2 presents the structure of hyperstreamhs the adopted parameterization of physical processes 2 1 and of water transfer and storage infrastructures 2 2 as well as data requirements 2 3 parameters identification procedures are presented in sect 3 followed in sect 4 by the description of the dual layer parallelization strategy and in sect 5 by an example of application to the adige river basin finally concluding remarks are drawn in sect 6 2 hyperstreamhs hydrological model 2 1 natural hydrological system large scale hydrological simulations are performed by coupling the hyperstream routing scheme piccolroaz et al 2016 with a grid based runoff generation model composed by the scs cn runoff model michel et al 2005 combined with a bucket type soil moisture model majone et al 2010 routing is performed by means of the instantaneous unit hydrograph iuh applied to each cell node pair the nodes are selected positions along the river network where the water discharge is computed according to the iuh theory see e g gupta et al 1980 rinaldo and rodriguez iturbe 1996 runoff generated at the hillslope is transferred along the river network by means of a transfer function coinciding with the probability density function pdf of the particle residence time which in turn is given by the pdf of the travel distance from the cell to the node multiplied by the celerity of the flow signal rodríguez iturbe and rinaldo 1997 the pdf of the travel distance is extracted from the digital elevation map dem of the catchment at a finer resolution with respect to the grid used to resolve the vertical fluxes this allows to use a relatively coarse grid of the order of a few kilometers without compromising routing accuracy piccolroaz et al 2016 vertical fluxes are represented as follows firstly precipitation falling on a given cell is classified as rainfall or snow according to a given threshold temperature and the latter is accumulated in the snowpack compartment mass balance is applied to the snowpack with water produced by snowmelting computed by the degree day model rango and martinec 1995 majone et al 2010 the resulting water volume composed by the sum of rainfall and snowmelting is partitioned into infiltration and surface runoff by means of the generalization of the scs cn model proposed by michel et al 2005 in the absence of snowpack and if the precipitation is classified as rainfall the entire precipitation is transferred to the scs cn module soil water dynamics is modelled by a nonlinear reservoir majone et al 2010 with infiltration as input flux and evapotranspiration evaluated by means of the hargreaves and samani 1982 model subsurface flow and deep infiltration as output fluxes deep infiltration enters a linear reservoir representing the groundwater compartment with return flow as output see piccolroaz et al 2015 bellin et al 2016 majone et al 2016 for applications of this modeling framework in alpine catchments a schematic of the vertical water fluxes separation and routing scheme is depicted in fig 1 and additional details are provided in appendix a the modeling framework relies on the following key geometric objects see fig 1 i macrocells i e grid cells where the meteorological forcing is assigned and the vertical water fluxes are evaluated fig 1a and ii nodes located along the stream network where water discharge is computed the drainage directions and the stream network are extracted from the dem according to one of the several criteria available for the identification of the hillslope channel separation e g tarboton et al 1991 lazzaro 2009 the information derived from the dem is used to build the probability density function pdf of the flow path lengths i e width function connecting the hillslope channel transition sites of a macrocell to the first downstream node a width function is evaluated for each macrocell node pair fig 1b and then it is rescaled through a constant stream velocity to obtain the instantaneous unit hydrograph used to transfer the flux to the downstream node see appendix a for details the assumption of constant stream velocity makes the transfer process linear and hence the routing model highly parallelizable it has been shown that using a constant uniform velocity along the river network is not a limitation see e g rinaldo et al 1991 rodríguez iturbe and rinaldo 1997 while it provides great flexibility in parallelization 2 2 human systems module the effects of water uses is simulated by including hydraulic infrastructures represented by nodes edges and reservoirs into the natural network this leads to an augmented network fig 2 shows an example of inclusion of a hydropower system composed by a reservoir a diversion channel a penstock and a power station in addition to the reservoir water is taken from natural channels intersecting the diversion channel intakes and is returned to the natural river network at the powerhouse node the main elements types of hs module and their associated water balance equations are presented below 2 2 1 type reservoir reservoirs represent special nodes where water storage may occur as shown in fig 3 each reservoir is subdivided into three volume pools flood control active and inactive volumes reservoir functioning is described by the curve volume versus elevation and the schedule of water use both entering into the mass balance equation 1 σ q i n t σ q u t q e f t q s p i l l t d v d t where q i n t is the inflow to the reservoir q u is the flow transferred through the inlet gate to the uses q e f is the flow released to the downstream river network according to ecological flow rules q s p i l l is the flow released by the spillways and diversion gates and finally v is the volume of water contained in the reservoir and t is the time the summation in the first two terms of eq 1 indicates that the reservoir may receive water from catchments other than the upstream catchment connected catchments and deliver water to multiple uses mass balance described by eq 1 is applied by considering the following priority of releases a ecological flow q e f local environmental regulations determine the minimum amount of water that must be released to the downstream river network by reservoirs and intake points to preserve freshwater ecosystem functioning b spillways q s p i l l it is the sum of the flux discharged downstream through the spillways and the bottom outlet gates the former operates only when the water level is above the maximum regulation level h m a x r e g while the latter can be operated when the water level is above the minimum regulation level h m i n r e g and can be used to empty the reservoir or to create the volume needed to contain an incoming flood reservoirs may be equipped with one or more spillways with outflow regulated by a gate or not regulated in hyperstreamhs we used an equivalent free non regulated spillway with the following characteristics it starts to operate when the water level in the reservoir is at h m a x r e g and its length l e q is computed by imposing that the water discharge is equal to the maximum outflow q s p i l l m a x when h h m a x therefore the water discharge from the equivalent spillway is given by 2 q s p i l l t l e q c q 2 g h t h m a x r e g 3 2 where c q 0 49 is the coefficient of discharge and l e q is computed according to the following equation 3 l e q q s p i l l m a x c q 2 g δ h f c 3 2 in eq 3 δ h f c h m a x h m a x r e g is the maximum hydraulic head the time step adopted in the solution of eq 1 is reduced during flood events in such a way to reduce the numerical error for example in our simulations the adopted time step is of 1 h for the hydrological kernel of hyperstreamhs which is reduced to 1 min in the solution of eq 1 during flood events c utilization schedule q u for h h m i n r e g a water use schedule i e a time series of the values assigned to q u is provided externally by the user 2 2 2 type intake intakes are nodes where water is diverted to satisfy water demand the water can be sent directly to the final uses or stored in a reservoir for successive utilization see fig 2 intake nodes are also used to model restitution points and confluences between channels or conduits an intake node is located along the river network and is connected by a channel to the man made network or to a reservoir see fig 2a b at the intake the following mass balance equation is applied fig 2b 4 q i n n a t t q d i v m m t q o u t n a t t 0 where q i n n a t and q o u t n a t are the river water discharge immediately upstream and downstream the intake respectively and q d i v m m is the diverted water discharge which is bounded by the maximum water discharge that the diversion channel can convey the diversion channel or conduit may discharge in another channel in a reservoir or directly to the final use in the former case a second mass balance equation should be applied 5 q i n m m t q o u t m m t q d i v m m t 0 the first two terms have a similar meaning as in eq 4 but referred to the man made channel network if the final node of the diversion is connected to a reservoir eq 5 is not included and q i n m m constitutes one of the terms of σ q i n in eq 1 concerning the restitution points fig 2c water flux arriving from the man made network q i n m m is added to the incoming flow q i n n a t to obtain the flux downstream the junction 6 q o u t n a t t q i n n a t t q i n m m t confluences in the man made network see fig 2d are treated in a similar way by adding the flux of the merging channel to the incoming flux in the receiving network 2 2 3 type hydropower plant power plants are treated similarly to restitution points illustrated above the water derived from a node is returned to the river at the restitution point and the generated power is computed as follows 7 h p p t η i 1 n γ q t u r b i δ h i t where q t u r b i and δ h i are respectively the turbined water discharge and the hydraulic head of the i th power unit and n is the number of power units turbine plus alternator of the power plant the hydraulic head δ h i is given by the difference between water elevations of the reservoir and the turbine or of the downstream impoundment depending if an action or a reaction turbine is used in run off the river power plants the head is assumed constant in time finally γ is the specific weight of water and η is the turbine efficiency which is assumed constant in the example of application presented in sect 5 we set η 0 8 corresponding to the mean of the efficiency of 12 out of 22 reservoir hydropower plants belonging to the study area for which this value was known 2 2 4 type diversion channel diversion channels divert water from the natural stream network by means of an intake and route it downstream along the man made channel network these objects are fully characterized by the channel length and water celerity with the latter used to compute the delay with which the water is conveyed downstream and is set by the user prior to the simulation in the example of application presented in sect 5 we assumed a value for the celerity equal to 2 0 m s kumar and singhal 2015 2 3 data requirements hyperstreamhs is conceived as a parsimonious model temperature and rainfall gridded dataset are the meteorological forcing required to run the hydrological kernel dem land use and land cover maps are also needed see appendix a 1 for additional details human systems module requires detailed information of the hydraulic infrastructures of the river basin geometry and position of storage reservoirs head and penstock capacity of hydropower plants location of intakes and restitution points and capacity of diversion channels to show that hyperstreamhs can be succesfully applied in most practical situations in the present work we used only publicly available data reservoir information was retrieved from the italian registry of dams rid available at http dgdighe mit gov it while information on hydropower systems were primarily extracted from public reports such as those provided by enel https www enel it the former public operator that managed italian hydropower production from 1962 to 2004 when the power market was liberalized furthermore information on plants installed power and penstock capacity was retrieved directly from operating companies online databases or from informative leaflets 3 inverse modeling and parameter identification as customary in hydrological simulations the 12 parameters of hyperstreamhs are set such as to provide the best possible representation of observational data typically under the form of water discharge measurements with this objective in mind inverse modeling is therefore applied by searching the parameters hyperspace with the objective of updating the prior parameters distribution bayesian updating or identifying the set of parameters that maximizes a given objective function therefore hyperstreamhs is provided with two searching strategies the particle swarm optimizer pso kennedy and eberhart 1995 robinson and rahmat samii 2004 castagna and bellin 2009 majone et al 2010 piccolroaz et al 2015 and the stratified latin hypercube sampling lhs mckay et al 1979 piccolroaz et al 2016 the former is a genetic searching algorithm which has been shown to optimize the searching path kennedy and eberhart 1995 robinson and rahmat samii 2004 down to the optimal point while the latter is often used to efficiently sample the entire parameters space in both cases we used the nash sutcliffe efficiency nse index nash and sutcliffe 1970 as objective function for streamflow time series evaluated at given nodes of the river network 8 n s 1 σ e 2 σ o 2 where σ o 2 is the variance of the observed streamflow time series and σ e 2 is the variance of the residuals i e the difference between observed and simulated streamflow since parameters inference is a computationally demanding step particularly when the number of parameters is large our objective is to alleviate this burden by means of the dual parallelization strategy described below 4 dual layer parallelization strategy dual layer parallelization is organized in such a way that a given number of forward simulations is run in parallel second layer each with a different set of parameters chosen by the searching algorithm each forward run is performed in parallel as well by partitioning the hydrological kernel among a subset of processors first layer this framework is achieved by exploiting the master slaves paradigm according to the mpi standard for illustration purpose fig 4 shows an example with n p 16 processors named as p 0 p 15 hypothetically available in a hpc system as shown in fig 4a before parallelization all the processors p k k 0 n p 1 belong to the same group named as m p i g r o u p w o r l d and are associated to its corresponding communicator termed according to the mpi terminology as m p i c o m m w o r l d for the sake of clarity here we remind that the concept of m p i g r o u p corresponds to the mathematical concept of s e t i e a well defined collection of distinct objects i e processors with their own identifiers i e processor i d on the other hand a c o m m u n i c a t o r is an object connecting all the processors belonging to the same set and giving them independent identifiers p k k 0 n p 1 according to an ordered topology details about mpi groups communicators and topology can be found in mpi forum 1994 and in gropp et al 1996 afterwards the global group and communicator are split into n g sub sets with each sub set composed by n s n p n g processors each sub set constitutes a univocal set of processors and defines the first layer of the mpi parallelization where independent simulation runs are performed for the sake of clarity it is useful to define the following topological operator p i j p δ k which links global processor topology p k k 0 n p 1 with the local topology where i 0 n g 1 identifies the group i e the sub set to which the processor belongs j uniquely defines the processor i d inside the group k 0 n p 1 is the processor i d according the default global communicator and δ is the topology operator which associates the local communicator m p i l o c a l c o m m to the global communicator m p i c o m m w o r l d in the illustrative example see fig 4b the total number of n p 16 processors is divided into 4 sub sets i e n g 4 with each set containing 4 processors see dashed thick lines in fig 4b the figure also highlights that each sub set has its own local communicator m p i l o c a l c o m m and local identifier p i j for the processor i d in addition in each sub set a m a s t e r processor is defined identified with j 0 i e p i 0 which controls the other processors within the sub set these latter processors are usually termed as s l a v e s and are identified as p i j with j 1 n s 1 notice that each master also acts as slave within its own sub set furthermore the second layer of parallelization is organized creating a new group m p i g r o u p m a s t e r see continuous thick line in fig 4b which gathers the master processor of each sub set together with its own communicator m p i m a s t e r c o m m for the sake of completeness listing 1 shows the mpi fortran code used to split the processors into groups and to set up the local communicator while listing 2 illustrates the instructions applied to set up the communicator between the masters and to exclude the slave processors in the ensuing sub sections the two layers of parallelization are described in details with reference to the implementation within the hyperstreamhs hydrological model listing 1 mpi command to create processor groups listing 1 listing 2 mpi command to create the group of masters listing 2 4 1 first parallelization layer as discussed in sect 2 hyperstreamhs is suitable for parallelization given that i the computational domain is divided into independent macrocells each one with its own meteorological forcing ii vertical water fluxes of a macrocell are independent from those of the other macrocells and iii runoff is transferred from the macrocell to downstream node by means of the linear iuh therefore the runoff produced by the macrocells contributing to a given node can be transferred separately and then summed up to take advantage of these characteristics at the first layer tasks are assigned according to the number of macrocell node pairs identified by the pre processor see sect 2 1 and appendix a 1 fig 5 illustrates the parallelization strategy adopted in the first mpi layer as a first step all processors store the model parameters and the network topology see the horizontal frame reading of input data then the macrocell node pairs are equally subdivided among all the available processors which load the corresponding macrocell node width functions as well as the meteorological forcing pertaining only to the macrocells identified in the pairs selection see the horizontal frame macrocell node splitting memory is allocated to the processors according to the number of associated macro cells and width functions the following step is performed in parallel with each processor that computes the runoffs of the assigned macrocells and transfers them to the connected downstream node see the horizontal frame hydrological kernel in fig 5 streamflow is then aggregated at each network node by summing up all the contributions pertaining to the macrocells node pairs notice that this operation refers only to the streamflow contributions originating from the natural component of the hydrological model once streamflow is aggregated at each node level a single processor is designed to perform the serial computations needed to i perform the water budgets at the nodes of the hydraulic infrastructures see sect 2 2 and ii routing along the natural and man made network of water fluxes simulated at the nodes see horizontal frame hs module and final routing in figure 5 notice that among the possible parallelization strategies for balancing the computational load the equally subdivision of macrocell node pairs over the available processors presents the key advantage that the interaction between processors is limited to streamflow aggregation at the network nodes thereby avoiding the exchange of information on vertical fluxes as a consequence each processor performs independent macrocell node convolution without waiting processed data from macrocells belonging to other processors an overall reduction of computational times is then achieved as well as an improved load balance between processors which perform approximately the same number of convolutions as a side effect a limited number of macrocells may be assigned to two or more processors see second horizontal panel in fig 5 which leads to replicate computations of the same vertical fluxes in these cells however the associated computational burden is less than the gain achieved by using this strategy further improvement of load balancing can be envisioned by implementing a subdivision of macrocell node pairs between processors based on the macrocell node convolution length which is a proxy of the computational time required to transfer runoff from the macrocell to the pertaining node in this respect profiling of the code and implementation of automatic load balancing techniques such as scatter type decomposition e g mendicino et al 2006 or graph partitioning e g karypis and kumar 2009 will be considered as possible further code optimization 4 2 second parallelization layer a second parallelization layer can be introduced when the application includes inverse modeling i e parameter identification in hyperstreamhs we implemented the latin hypercube sampling lhs and the particle swarm optimization pso algorithms but other procedures such as markov chain monte carlo see e g vrugt 2016 can be implemented as well fig 6 a and b illustrates a schematic of the implemented dual layer framework for lhs and pso respectively the n p processors that will be used are subdivided into n m a s t e r s masters each one having n s l a v e s slaves such that n m a s t e r s n s l a v e s n m a s t e r s n p in the examples shown in fig 6 the masters are 4 each one with 3 slaves for a total of n p 16 processors the master of masters processor indicated with p 0 0 in fig 6 equally subdivide the sets of parameters to the n m a s t e r s masters and each master performs the forward simulations with the assigned parameters by using n s l a v e s 1 processors in parallel we recall that a master also acts as slave within its own sub set as described in sect 4 1 this procedure is iterated until the space of parameters is explored as desired with the lhs algorithm the parameters space is explored visiting the locations of a pre defined grid the sequence of the locations that are visited is established by the master of masters processor p 0 0 therefore the masters do not communicate among them until the end of the procedure when the efficiency metric at all the explored grid locations is collected and further elaborated according to the objectives of the simulation see fig 6a for example a simple sorting will be performed if the objective is to identify the set of parameters providing the best efficiency metric while more complex elaborations will be performed in case of bayesian updating pso on the other hand requires that at the end of each forward simulation the efficiency metric of each particle is compared with its personal best and with the overall best the best among all the particles in order to update all the personal bests and the global best as well see fig 6b this is needed because the personal bests and the global best are used in the pso algorithm to move the particles in the space of parameters at each iteration step this procedure thus requires inter master communication since all the forward simulations controlled by the masters need to be completed before updating the position of the particles if more than one particle is assigned to a master this may happen when the number of particles is larger than the number of masters the above control and the movement of the particles can be performed only after all the forward simulations have been completed the cycling over the parameter sets assigned to each master is illustrated with red boxes in fig 6 for both lhs and pso while inter master communication occurring at every pso iteration is represented with a red horizontal arrow see fig 6b 5 example of application computational scalability of hyperstreamhs and its capability to reproduce observed streamflows in locations impacted by the presence of human infrastructures are discussed here with reference to the adige river basin closed at the gauging station of vò destro 45 44 6 5 n 10 57 26 9 w with a contributing area of 10600 k m 2 see fig 7 the adige is a typical alpine river basin with a complex morphology characterized by deep valleys 131 m a s l at vò destro and high mountain crests maximum elevation of 3905 m a s l at cima ortles in the north western part of the basin the annual average precipitation ranges from 500 mm in the north west to 1600 mm in the southern portion of the basin laiti et al 2018 diamantini et al 2018 while streamflow has a typical alpine regime with two seasonal maxima one occurring in spring summer due to snow and glacier melt and the other in autumn triggered by cyclonic storms chiogna et al 2016 di marco et al 2021 lutz et al 2016 mallucci et al 2019 fig 7 shows the natural river network the superimposed man made network red edges the hydropower plants and the reservoirs though fragmented the man made network is distributed over the entire river system and exerts a significant impact on streamflow zolezzi et al 2009 bellin et al 2016 majone et al 2016 pérez ciria et al 2019 5 1 model setup the characteristics of hyperstreamhs are illustrated by means of two applications i streamflow simulation in the noce river which is heavily impacted by hydropower particularly in its lower reaches see the upper right panel of fig 7 and ii analysis of the computational performances with reference to the entire adige all the simulations were conducted at a daily time step in the period 1989 2006 by using a grid size of 5 k m streamflow time series collected at the trento bronzolo malè and mezzolombardo gauging stations see fig 7 were used as observations in the inverse modeling experiments furthermore for all configurations the first two years of simulations were used as a spin up and thus were excluded from the computation of the nse efficiency index precipitation and temperature were extracted from the adige dataset mallucci et al 2019 while land use and land cover considered invariant in time were extracted fromthe corine 2006 product http www eea europa eu publications cor0 landcover the width functions used in the routing scheme were obtained by an offline morphological analysis of the 30 m resolution dem provided by eudem project https www eea europa eu data and maps data eu dem the characteristics of the hydraulic infrastructures were retrieved from the italian registry of dams rid available at http dgdighe mit gov it and from public reports and informative leaflets of the hydropower companies parameters inference for the noce experiment was performed adopting the malè gauging station as calibration node and by including in the simulations the presence of the hydraulic infrastructures see the inset of fig 7 management rules of the reservoirs were inherited by the work of bellin et al 2016 and are at the monthly time scale pso was adopted as calibration algorithm validation of the model was performed at the downstream mezzolombardo gauging station which streamflow is dominated by the release of the mezzocorona hydropower plant which is fed by the water derived from the mollaro reservoir which in turn receives the water turbined by the taio phydropower plant fed by the santa giustina reservoir see the upper right panel of fig 7 results of the simulation are shown in section 5 2 concerning speed up analyses a few different levels of detail in representing the hydraulic infrastructures were implemented in order to investigate the computational load of including them in the simulation in a first setup all the hydraulic infrastructures in the adige were included 30 storage reservoirs 41 intake points and 40 hydropower plants for a total including all available streamflow gauging stations of 138 nodes of the 27 gauging stations included into the river basin the two stations located at trento and bronzolo were used for parameters inference furthermore a simplified version of the full setup was adopted this time only considering 5 out of the 138 nodes 2 storage reservoirs plus three gauging stations trento and bronzolo used for inference of the model parameters and vò destro as closing section the two setups differ in the number of macrocells node pairs that need to be considered in the parellelization of the hyperstreamhs model 569 and 1167 macrocells node pairs for the 5 nodes and 138 nodes cases respectively according to the parallelization strategy described in sect 4 the computational performance of hyperstreamhs and its scalability are tested considering both a single and a dual layer parallelization scheme see sects 5 3 and 5 4 respectively in the forward simulations of the one layer scalability experiment the parameters characterizing the natural hydrological component see appendix a were fixed equal to the values obtained during one of the optimization experiments described in sect 5 4 in the case of the dual layer implementation we considered 500 runs for lhs and 10 000 for pso specifically 100 iterations with 100 particles it is worth to clarify that here we are not interested either in evaluating the convergence to the global optimum with pso scheme or in a full uncertainty assessment of the posterior pdf of the model parameters but rather to evaluate how hyperstreamhs scalability is influenced by the adopted setups as a quantitative dimensionless metric of scalability we adopted the speed up s e g mendicino et al 2006 9 s t s t n p where t s is the computational time of a serial simulation i e when only a processor is used and t n p is the computational time of the same simulation conducted using n p processors under ideal scaling conditions speed up is linear meaning that a reduction in the computational time of n p times with respect to the serial simulation is achieved when n p processor are used we remark that ideal speed up however is a theoretical condition due both to communication times between processors and to portions of the program that can not be parallelized 5 2 hydrological modeling performances in the presence of human systems in the middle course of the noce river streamflow is stored into the large reservoir of santa giustina storage capacity of about 182 m m 3 to be used in the taio hydropower plant maximum penstock capacity of 66 m 3 s 1 the water turbined in the taio powerplant is then collected by the mollaro reservoir which has a small yet appreciable regulation capacity 0 86 m m 3 and then transferred to the mezzocorona hydropower plant maximum penstock capacity of 60 m 3 s 1 mezzocorona plant is located along the noce main course 2 5 k m upstream of mezzolombardo thus exerting a strong control on streamflow alterations observed at the gauging station see the inset of fig 8 calibration of hyperstreamhs to the 1991 2006 observed streamflow at malè gauging station provided the satisfactory nse index of 0 77 the resulting model s parameters have been used to compute streamflow at mezzolombardo either including or not the hydropower systems of taio fed by the santa giustina reservoir and mezzocorona fed by the mollaro reservoir hereafter these two cases are referred to as actual and natural conditions respectively fig 8a compares the time series of both simulations with observations at mezzolombardo while the corresponding comparison in terms of flow duration curves is depicted in fig 8b nse efficiencies were 0 27 and 0 43 in natural and actual conditions respectively though in absolute terms these nse values indicate that streamflow is reproduced partially the gain in adding the available though incomplete information on hydraulic infrastructures is remarkable this considering that operational schedule of the reservoir is known only in average and at monthly time scale which is a limitation in the case at hand given that hydropower companies typically operate reservoirs at finer time scales weekly daily up to the hourly scale in order to maximize the revenue nonetheless the modeling framework is able to generally reproduce the observed streamflow pattern in terms of both timing and magnitude with biases occurring solely as a consequence of the non perfect knowledge of high resolved in time reservoir operating rules see the inset of fig 8a for easiness of visualization on the other hand the inclusion of reservoir regulation by means of the human systems module allows a very good reproduction of the peak flows this is not the case of the natural simulation which completely fails in reproducing the observed streamflow these differences are even more evident when considering the flow duration curves depicted in fig 8b both natural and actual cases are able to reproduce fairly well high flows i e 80 m 3 s 1 and higher which are indeed associated to the hydrological response of the watershed on the contrary the mismatch widens moving from intermediate to low flows i e 40 m 3 s 1 and lower where the natural simulation misses completely the observed streamflow while the inclusion of the hydraulic infrastructures allows to accurately reproduce the observed flow duration curve 5 3 first layer parallel performance in a first set of experiments aimed at assessing computational performance hyperstreamhs was applied to the entire adige catchment closed at vò destro it was run by choosing 5 and 138 nodes where to compute streamflow and with two different setups a natural scenario neglecting the impact of hydraulic infrastructures and a second scenario in which the infrastructures are included as described above hence a total of four configurations were investigated in general both the number of macrocell node width functions and the number of inter processors communications increase with the number of nodes leading to a larger computational burden which is increased further when the hydraulic infrastructures are added fig 9 shows the computational time against the number n p of processors in log log scale while the inset illustrates the corresponding speed up computations have been performed by using 1 2 4 8 16 32 64 processors theoretical computational time and speed up corresponding to 100 efficiency in parallel computing are shown with dashed lines as expected there is a strong dependency on the number of nodes the test case with 5 nodes shows a better speed up than the case with 138 nodes and exhibits small differences between the natural and the actual scenario the low incidence of computations that should be performed in series is the reason of the small difference between the natural and actual scenario for the 5 nodes case the difference widens when 138 nodes are used which loss speed up rather soon see inset of fig 9 for n p 8 the curve of the speed up flattens and there is no longer an advantage in increasing the number of processors this limit is extended to n p 32 for 5 nodes though the gain in term of speed up is significantly lower than the theoretical one when more than 8 processors are used both cases show an increase in computational time for n p 32 this shows also that the fraction of computations with respect to the total that should be performed in series increases with the number of nodes thereby reducing the gain achieved by using a larger number of processors in addition the large number of reservoirs 30 in the case with 138 nodes explains the difference in the computational time between natural and actual scenarios green lines in fig 9 in fact when the number of infrastructures increases a progressively larger share of time should be spent in performing the water budgets needed to evaluate the effect of the hydraulic infrastructures as described in sect 2 2 in the specific case with 138 nodes the difference between the computational time between the natural and actual scenarios increased from 17 15 when only one processor is used up to 35 74 when 64 processors are employed loss of speed up due to serial computations involved in streamflow routing is a well known drawback in parallel computing typical of single layer decomposition schemes see e g wang et al 2012 and zhang et al 2016 nevertheless a first layer of parallelization of the hydrological model reduces the computational time with respect to a single processor run and more importantly allows to handle a larger amounts of data overcoming allocation problems typical of shared memory codes according to lai et al 2018 this is a mandatory feature in order to cope with the large amount of data that should be managed in large scale simulations on the light of dual layer approach we adopted in the present work the analysis of scalability of the first mpi level represents a preliminary step in order to define the optimal number of processors to be used as slaves in the final dual layer mpi implementation 5 4 second layer parallel performance in this section we analyze several combinations of master slave processors in a two layer parallelization with the objective to identify possible optimal conditions these combinations are intermediate between two extreme cases a single processor at one extreme the master acts also as a slave and 32 masters each one with its own slave for a total of 64 processors on the other extreme the former case is assumed as reference because it represents the standard situation of sequential computation all the analyses are performed with both 5 and 138 network nodes and with the presence of hydraulic infrastructures speed up of the investigated cases as a function of the number of processors both masters and slaves is shown in figs 10 and 11 for lhs and pso respectively fig 10a and b shows the results of the simulations conducted with lhs with 5 and 138 network nodes respectively for the 5 node case when a single master is used the speed up curve flattens as the number of processors increases on the other hand speed up decreases for n p 32 in the 138 nodes case when 1 or 2 masters are used this is in accordance with the results presented in fig 9 given that for a single master configuration the second mpi layer is deactivated a similar behavior is observed for 2 and 4 masters in the case of 138 nodes fig 10a also when inverse modeling is performed with pso fig 11 however as the number of masters increases the speed up for pso diverges from the theoretical behavior more than for the lhs case in both the 5 and 138 network nodes configurations fig 11a and b the different behavior characterizing the dual layer scheme with lhs and pso is further investigated in fig 12 a and b respectively computations have been performed with 64 processors subdivided in masters shown in the abscissa and slaves indicated above the bullets in the graph as expected in both cases one master results in the largest computational time and while for the lhs scheme the minimum is obtained with 32 masters each one with a slave for pso the minimum is obtained with 8 masters with 7 slaves each see fig 12a and 12b in both cases the computational time reduces quickly as the number of masters increases from 1 to 8 and the marginal gain achieved by increasing the number of masters beyond 8 is small for lhs while for pso the computational time increases this is the consequence of the communication between the masters only active within the pso algorithm that becomes progressively more relevant as the number of masters n m a s t e r s increases consequently the optimal configuration of the dual layer setup depends not only on the number of available processors but also on the number of macrocell node pairs and the network topology results presented in this section also highlight that for a given total number of processors the dual layer strategy applied to lhs scheme presents the best performances as the number of masters is increased at the expenses of the slaves with the only limit being represented by memory availability on the contrary pso requires a preliminary scalability analysis of the forward simulation to identify the optimal number of masters dealing with the inverse modeling procedure i e the second mpi layer 6 conclusions we presented a new dual layer mpi parallel hydrological model named hyperstreamhs specifically developed to deal with large scale simulations in river basins containing hydraulic infrastructures the model allows inverse modeling with the nash sutcliffe efficiency metric but other metrics could be easily implemented and the space of parameters explored by using either the latin hypercube sampling or particle swarm optimizer hyperstreamhs is designed such as to facilitate parallel computing being composed by i a runoff generation module that solves for the vertical water fluxes on the independent macrocells in which the computational domain is subdivided ii a streamflow routing scheme based on the width function iuh theory with the assumption of constant stream velocity which makes the transfer process linear taking advantage of these characteristics we built a dual layer parallelization strategy and designed the inclusion of hydraulic infrastructures for example reservoirs and diversion channels in such a way to reduce the computational time through an optimal use of the available processors the main characteristics of hyperstreamhs are as follows i an optimal integration of natural hydrological components with water transfer and accumulation due to the management of hydraulic infrastructures such as reservoirs diversion channels intakes and hydropower systems this results in a holistic modeling framework for hydrological simulations at multiple scales in river basins with streamflow altered by water uses ii the adoption of a dual layer parallelization strategy where the parallelized version of the hydrological kernel including the modules dealing with hydraulic infrastructures is the first level with the second level taking care of inverse modeling aimed at the identification of model s parameters including their uncertainty iii the optimal configuration of the dual layer setup masters and slaves partitioning depends not only on the number of available processors but also on the inverse modeling algorithm adopted iv finally the dual layer structure adopts the mpi standard which ensures high portability and optimal exploitation of hpc resources hyperstreamhs has been tested in the adige river basin italy showing that the adopted dual layer parallelization scheme optimizes both the speed up of simulations and the allocation of hpc resources in particular the results showed the clear advantages of considering a dual layer parallelization strategy which compensates for the sub optimal gain of the hydrological kernel when increasing the number of processors especially in more regulated configurations this suggests to use a portion of the processors for the second layer parallelization dealing with the inversion procedure furthermore the capabilities of the model to reproduce observed streamflows in locations heavily impacted by the presence of human infrastructures have been presented and discussed overall the model showed high potential for applications aimed at supporting water resources management and optimization issues in regulated river basins declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research received financial support by the energy oriented centre of excellence eocoe ii ga number 824158 funded within the horizon2020 framework of the european union and by the project seasonal hydrological econometric forecasting for hydropower optimization she funded within the call for projects research südtirol alto adige 2019 autonomous province of bozen bolzano south tyrol this research has been also supported by the italian ministry of education university and research miur under the departments of excellence grant l 232 2016 streamflow data were provided by the hydrographic office of the autonomous province of bolzano www provincia bz it hydro and by the ufficio dighe of the autonomous province of trento www floods it a natural hydrological conceptual model in this appendix the vertical water flux generation module adopted in hyperstremhs model is presented together with the main concepts of hyperstream routing scheme piccolroaz et al 2016 a 1 computational grid the spatial domain is partitioned into m macrocells of equal shape and size fig 1b furthermore n nodes are identified where streamflow is computed fig 1a macrocells can be defined such that the hydrological model shares the same grid of an overlaying climate model or of a gridded dataset providing the input meteorological forcing the nodes are arbitrarily distributed along the river network and typically are located in correspondence of existing gauging stations where streamflow observations are available needed for model calibration and validation or at relevant nodes of hydraulic infrastructures that need to be simulated by the human system module a one time and offline pre processing step is run to prepare the geometrical information needed to implement the streamflow routing scheme the dem is analyzed in order to extract the river network the drainage characteristics of the study area and to derive the corresponding geomorphological width functions for each macrocell node pair see fig 1b other properties useful for the evaluation of the water fluxes e g average elevation soil use and type crop coefficient etc are computed for each macrocell based on the analysis of the available dem and land use land cover spatial maps a detailed example of macrocell discretization and width functions derivations can be found in piccolroaz et al 2016 a 1 2 vertical water flux module the vertical water flux module adopted in hyperstreamhs employs the formulation proposed by laiti et al 2018 it relies on the coupling of the continuous soil moisture accounting scs cn scheme for surface flow generation michel et al 2005 with a non linear bucket model for soil moisture dynamics majone et al 2010 and a linear reservoir model to simulate the base flow component in addition a simple degree day model is used for the simulation of snow dynamics while hargreaves and samani 1982 formulation is employed for the computation of potential evapotranspiration e t p a schematic of the water flux generation module is presented in fig 1a while each component is presented below the degree day model for snow accumulation and melting dynamics is based on the following water balance equation e g rango and martinec 1995 hock 2003 majone et al 2010 a 1 d h s d t p s p m where h s l is the snowpack water equivalent p s l t 1 is the solid precipitation intensity and p m l t 1 is the snowmelt intensity p s and p m are quantified according to the following equations depending on two air temperature thresholds a 2 p s p t a t s 0 t a t s a 3 p m 0 t a t m c m t a t m t a t m where t a c is the air temperature and t s c and t m c are two threshold temperatures the former is the temperature below which precipitation is assumed as solid and the latter the temperature above which the snowpack melts furthermore c m l t 1 c 1 is the melting factor providing the amount of snow melted per unit of time and temperature notice that when t s t a t m precipitation is liquid but the energy input to the snowpack is not sufficient for triggering melting the effective liquid precipitation p e l t 1 entering the soil can be then computed as follows p e 0 t a t s p t s t a t m p p m t a t m the degree day approach used here requires the mean daily air temperature t a as input thus provides mean daily values of p m if the time step integration of the hyperstreamhs model is less than one day the melt water contribution is evenly distributed during the day soil moisture is controlled by the following mass conservation equation a 4 d s m d t p e q r q p e t r where s m l is the soil moisture q r l t 1 is the surface runoff rate q p l t 1 is the leakage flux i e the water flux from the top soil layer towards groundwater and e t r l t 1 is the real evapotranspiration the surface runoff rate q r is evaluated according to the procedure proposed by michel et al 2005 which is an extension of the well known scs cn approach u s soil conservation service 1964 accounting for a time varying soil moisture in the active soil in particular q r is evaluated as follows q r p e s m s a s 2 s m s a s s m s a 0 s m s a where s l is the maximum potential soil infiltration and s a l is the threshold above which runoff is generated s is given by the product of the maximum potential infiltration estimated from the land use and lithological characteristics of the soil s l and a scaling coefficient c s such that a 5 s c s s this correction allows to account for possible uncertainties in the identification of s s a is the soil moisture at the beginning of a precipitation event plus the initial abstraction and is assumed proportional to s through the following relationship a 6 s a c a s where c a is a scaling coefficient notice that p e q r is the infiltration rate into the soil layer and that its saturation level is given by s s a daily reference evapotranspiration e t 0 l t 1 is estimated through the equation proposed by hargreaves and samani 1982 based on mean minimum and maximum daily air temperature t a following allen et al 1998 e t 0 is multiplied by a monthly varying crop coefficient k c in order to estimate potential evapotranspiration e t p thus accounting for the presence of specific crop or natural vegetation and their seasonal vegetative conditions a 7 e t p k c 0 408 0 0023 r a t a 17 8 t a m a x t a m i n where r a is the extraterrestrial radiation m j m 2 d 1 finally real evapotranspiration e t r is computed taking into account that evapotranspiration reaches its potential upper limit only when soil moisture is larger than the field capacity s m f c and that e t r tends to zero as s m approaches its residual limit s m r e t r 0 s m s m r e t p s m s m r s m f c s m r s m r s m s m f c e t p s m s m f c where s m r s s a c r s m f c s s a c f c and c f c and c r are calibration coefficients smaller than one in particular for values of s m between s m r and s m f c e t r is assumed to vary linearly from zero to e t p similarly to the snowmelting e t r is evaluated as daily average and if the computational time step of hyperstreamhs is smaller than one day e t r is evenly distributed during the day the leakage flux from the active soil q p is evaluated through the following exponential law a 8 q p q r e f e s m s m r μ 1 where q r e f l t 1 and μ l are calibration parameters controlling the maximum and rate of variation of the leakage flux respectively we notice that both q p and e t r tends to zero as s m approaches s m r thus avoiding s m to drop below its lower physical bound s m r in absence of infiltration i e when p e q r 0 similar exponential relationships for storage discharge dynamics coupled with the surface runoff model by michel et al 2005 have been successfully applied in previous applications conducted in alpine and mediterranean catchments piccolroaz et al 2015 bellin et al 2016 majone et al 2012 2016 the leakage flux q p is then divided into two components through a partition parameter α the first component contributes to the runoff as interflow q i l t 1 whilst the latter constitutes deep percolation to groundwater q d p l t 1 q i q p α q d p q p 1 α finally q d p feeds a linear reservoir model described by the following continuity equation a 9 k d q b d t q d p q b where q b l t 1 is the baseflow contributing to the streamflow and k is a calibration parameter the vertical water flux generation model described above is applied to all macrocells separately using as input the meteorological forcing pertaining to each of them at each macrocell the total runoff per unit area q s l t 1 is evaluated by summing together the surface runoff rate q r the interflow q i and the baseflow contribution q b a 1 3 routing algorithm following piccolroaz et al 2016 the streamflow q k i t l 3 t 1 generated by macrocell i and contributing to the node k at time t reads as follow a 10 q k i t a k i 0 t q s i t τ f k i τ d τ a k i q s i t f k i t where a k i l 2 is the fraction area of macrocell i contributing to node k q s i l t 1 is the vertical water flux per unit area produced by the macrocell i f k i is the pdf of the travel times of macrocell i relative to node k obtained by rescaling the width function built in the pre processing step by the stream velocity and the asterisk denotes convolution in doing this it is assumed that q s i is constant through the macrocell and is evaluated according to the vertical flux generation module described above the total streamflow at node k q k t is computed as the sum of the contribution of each macrocell to that node plus the streamflow transferred from the nodes upstream of k a 11 q k t i 1 m k c o n q k i t j 1 n k u p q j t τ j k where τ j k d j k v c t is the travel time from node j located upstream of k to node k d j k l is the distance between the two nodes v c l t 1 is the stream velocity m k c o n is the number of macrocells contributing to node k and n k u p is the number of nodes upstream of k a 4 model s parameters the model requires that 12 parameters are set 11 pertaining to the vertical water flux generation module and 1 to the hyperstream routing scheme spatial heterogeneity of evapotranspiration and infiltration is reproduced by setting the parameters k c and s as spatially variable with their deterministic values chosen from infiltration capacity and soil use maps all the remaining parameters are assumed as spatially uniform but uncertain the routing scheme requires the definition of a single parameter the stream velocity v c which is assumed constant thus making the model linear and easily parallelizable the list of the 12 calibration parameters with their units and range of variation is presented in table a1 table a 1 list of the calibration parameters with their range of variation table a 1 parameter description range of variation unit t s temperature threshold for snow precipitation 2 6 c t m temperature threshold for snow melting 2 6 c c m snow melting factor 0 10 m m c 1 d 1 c s parameter of the rainfall excess model 0 1 10 c a parameter of the rainfall excess model 0 01 1 q r e f parameter of the nonlinear bucket 10 7 10 3 m m s 1 μ parameter of the nonlinear bucket 0 5 300 m m c f c coefficient for field capacity 0 1 c r coefficient for residual soil moisture 0 0 25 k mean residence time for baseflow linear reservoir 200 1000 d a y α partition coefficient for leakage flux 0 1 v c stream velocity 0 2 4 m s 1 
25851,environmental modelling frameworks allow domain experts rather than software developers to implement and run numerical simulation models in earth and environmental sciences because of the need to use more detailed process representations or larger datasets as input to models it may become infeasible to perform modelling studies due to the increased amount of time it takes for models to calculate results the objective of this study is to evaluate the asynchronous many task approach in the implementation of a prototype scalable modelling framework we evaluate the scalability of local focal and zonal map algebra operations and an example model in which these operations are combined our results show that the capacity of the operations and the example model to use additional hardware like nodes in a computer cluster is good with our freely available prototype framework models can be executed faster and modelling studies processing considerably more data can be performed keywords environmental modelling framework map algebra high performance computing asynchronous many tasks hpx lue 1 introduction environmental modellers simulate the physical and biological environment using computer models these models can be developed using a multitude of software ranging from relatively low level general purpose programming languages with no built in support for environmental modelling like c c d fortran java and rust to high level modelling frameworks 1 1 we use the term framework loosely to mean software containing at least data types and algorithms used for the development of individual models this includes the case of a software library implementing these but excludes integration frameworks used for coupling models containing pre built model building blocks like google earth engine for earth science data and analysis gorelick et al 2017 matlab holzbecher 2012 the netlogo agent based modelling framework wilensky 1999 and the pcraster field based modelling framework karssenberg et al 2010 an advantage of using modelling frameworks is that they in different degrees hide some of the low level complexities of implementing models this speeds up model development and allows domain experts without a background in software development to develop models fig 1 and karssenberg 2002 some of the model development interfaces are inspired by map algebra tomlin 1990 which is also the approach that will be followed here existing examples using map algebra include the python programming language packages provided by pcraster arcgis and qgis in map algebra fields of spatially varying environmental attributes are represented by rasters which can be combined and translated into new rasters using a procedural programming style a set of such translations simulating environmental processes during a single time step can be used by a modelling framework to do forward iteration through time error propagation and data assimilation karssenberg et al 2010 the framework provides the elementary data structures and modelling algorithms used by modellers in their models ideally models built with such a framework offer good performance whatever the combination of modelling operations used in our study we look at designing and building such a framework for developing environmental models over time models often outgrow their capacity to calculate results in a timely manner this may be because of an increase in dataset sizes used by models an increase in temporal or spatial resolution or extents or an increase in model complexity in order to solve this discrepancy between the size and performance requirements of large models and their capacity to provide results models must increase their ability to use the current generation of hardware in general newer generations of hardware are more powerful but also more complex than earlier generations typically current computers contain more cores more kinds of cores and a deeper memory hierarchy additionally the availability of computer clusters to modellers containing multiple compute nodes connected by low latency network connections has increased the challenge we focus on is that of building a modelling framework that makes better use of the available hardware there are multiple approaches for developing a modelling framework implementing a collection of parallel and distributed map algebra operations an intuitive and popular approach to creating parallel versions of such operations is to use the synchronous fork join paradigm supported by openmp dagum and menon 1998 for example in which individual algorithms implementing these operations are parallelized and called in sequence examples of frameworks using this approach are the parallel raster processing library prpl guan and clarke 2010 the parallel raster based geocomputation operators pargo qin et al 2014 and the parallel cartographic modelling language pcml shook et al 2016 a drawback of this approach is that it introduces implicit synchronization points at least at the end of each operation the flow of control will wait for all tasks to finish before returning to the caller resulting in workers like cpus and gpus being inactive for some time this negative effect of synchronization points increases with the number of workers and the degree of load imbalance between the workers note that load imbalance between workers is common in environmental modelling operations resulting from an uneven spatial distribution of no data values or because of high spatial dependencies between cell values as is the case in some operations that operate on a flow direction network an alternative approach for implementing a collection of parallel and distributed map algebra operations is to use asynchronous many tasks amt one of the advantages of this approach is that it avoids unnecessary synchronization points with amt work to be done is encoded in a set of relatively small tasks with data dependencies among them tasks are spawned asynchronously allowing the main flow of control to continue into multiple modelling operations resulting in more tasks being spawned tasks get scheduled on workers after their inputs have become available this approach results in a larger collection of tasks than is possible when parallelizing algorithms individually as in the case of the synchronous fork join approach the advantage of having a larger collection of runnable tasks is that it decreases the chance of workers being inactive examples of runtime systems that support amt on distributed memory systems are chapel chamberlain et al 2007 x10 charles et al 2005 hpx kaiser et al 2020a and charm kale and krishnan 1993 of which the first two are specific languages and the latter two are software libraries for a taxonomy of task based parallel programming technologies see thoman et al 2018 we will use the amt runtime system implemented by the hpx software library for implementing our environmental modelling framework the objective of this research is to evaluate the use of amt for the development of a modelling framework containing implementations of map algebra operations that can be used on all kinds of commodity hardware in use by the modelling community ranging from laptops desktops to computer clusters the main question we want to answer is whether the use of such a framework results in scalable models for this we perform different kinds of scaling experiments over different kinds of workers scalability of models is determined by both the software implementing the compute part and the i o part in this study we focus on the compute part additionally we review the resulting framework in terms of its usability by model developers and we review the use of amt in the implementation this paper is organized as follows in section 2 we describe the approach of developing environmental models using map algebra in more detail in section 3 we provide more information about amt and the hpx implementation thereof in section 4 we describe how we used amt to implement a map algebra development interface on top of modelling algorithms in section 5 we present results of scaling experiments we performed with individual algorithms and an example model simulating wildfire in which some of the implemented modelling algorithms are combined we end this paper with a discussion of the results in section 6 the amt runtime system implemented in hpx enables us to write an initial set of high level modelling algorithms that can be called from a map algebra like model development interface in a modelling framework implementing algorithms in terms of asynchronous tasks that translate asynchronously produced input data into output data results in a flexible system in which modelling algorithms can be combined in any order according to the model and still offer good scalability the framework developer is responsible for defining tasks and the dependencies between them and is relieved of the responsibility of scheduling tasks and explicitly sending messages in between processes the framework implementation is freely available for inspection and use section 7 2 model development using map algebra originally the map algebra language was designed for creating cartographic models where the models were collections of maps tomlin 1990 the language consisted of a specific set of relatively simple generic operations that translate raster data a combination of such operations could be used to for example determine suitable locations for land development the advantages of using map algebra are that a finite set of generic operations can be used to handle multiple use cases and that it provides a level of abstraction that makes it suitable for users without a background in software development the principles behind cartographic modelling using map algebra have been extended towards forward numerical simulation of environmental processes as well van deursen et al 2019 karssenberg et al 2010 a map algebra like language is then used to define the initial state of the modelled environmental system and to define the state transitions over time in this context the model refers to the code not to the collection of maps the model shown in listing 1 is an example of an environmental model implemented using a map algebra like language simulating wildfire we used it in our experiments section 4 outputs from the model are shown in fig 2 listing 1 pseudocode of model simulating wildfire listing 1 map algebra operations are often classified according to the kind of neighbourhood from which input raster cells are selected that contribute to the calculation of output raster cells in this study we consider three kinds of operations burrough et al 2015 karssenberg et al 2010 tomlin 1990 local operations fig 3 a focal operations fig 3b and zonal operations fig 3c operations not considered in this work include global operations which can be seen as a subset of zonal operations and network operations operating on a flow direction network 3 asynchronous many tasks and hpx the amt programming model supports defining relatively small tasks of work that need to be executed and the dependencies between them the tasks and their dependencies form a directed acyclic graph that is used by the amt runtime system to determine the order in which the tasks must be executed and to determine which tasks can be scheduled to execute concurrently given enough hardware resources the runtime system will execute concurrent tasks in parallel the requirement for the runtime system to always be able to schedule tasks for execution is that there are enough tasks defined and few dependencies between them in order to achieve this tasks are created asynchronously and do not depend on more tasks than necessary an asynchronously created task is spawned off from its operating system os thread which continues doing other work for example spawning off more tasks hpx is an implementation of the amt programming model and runtime it is an open source software library written in portable c 11 14 17 20 code and does not depend on a compiler from a specific vendor or on compiler extensions it has been used to implement parallel software successfully in multiple studies heller et al 2013 heller et al 2017 heller et al 2019 and khatami et al 2016 using hpx the developer of an environmental modelling framework can define tasks and their dependencies in the usual imperative style of programming in c using the hpx api the graph of tasks is built implicitly and does not need to be explicitly managed by the developer the framework developer s main responsibility is to correctly represent the total amount of work to be executed by a collection of tasks and their data dependencies the size of each task is measured in terms of its latency which depends on the amount of data processed by the task the number of computations performed and on latencies involved in accessing the data the ideal task size is large enough for the overheads of parallelization to be amortized over the sum of the latencies of all tasks and small enough to provide the schedulers with enough concurrent tasks to schedule on workers grubel et al 2015 since the latency of tasks is partly dependent on aspects that are only known at runtime like data values and hardware characteristics it is important that the task size can be influenced by the user one way to do this is to support a parameter representing the amount of data processed by individual tasks to illustrate the differences between the amt approach and other approaches to writing a modelling framework we assume a model exists similar to the map algebra model simulating wildfire shown in listing 1 that calls three modelling operations from the framework for simplicity we will ignore the overheads of parallelization a serial framework executes these operations one after the other on a single worker fig 4 a the latency of this program is the sum of the latencies of all the work that needs to be done since only one worker is used by this program adding more workers will not decrease its latency when the three operations are independent from each other they can be executed in parallel fig 4b the program s latency is determined by the operation taking the most time to finish since each operation is executed by one worker adding more workers will not decrease this program s latency in the implementation os threads can be used for example to spawn threads doing work on multiple cpu cores when the three operations contain concurrent tasks that can be executed in parallel fig 5 another approach can be taken in this case the operations are still executed one after the other but they are partly executed in parallel fig 4c this program s latency is determined by the sum of the latencies of the serial regions and the parallel regions adding more workers will not decrease the latency of the serial regions but given enough concurrent tasks may decrease the latency of the parallel regions in this example it will not though since none of the parallel regions has more than three concurrent tasks in the implementation openmp dagum and menon 1998 can be used for example to create parallel regions in which multiple os threads are used to execute tasks on multiple cpu cores when using the amt approach concurrent tasks from all three operations are executed in parallel taking the dependencies between the tasks into account fig 4d the latency of the program is determined by the maximum of the sums of the latencies of the tasks per worker given enough concurrent tasks adding more workers will decrease the program s latency because tasks from multiple operations are considered there are more options to avoid load imbalance between workers in environmental models most rasters processed by the modelling operations depend on each other the output rasters from operations are used as input in other operations it is therefore unlikely to find many modelling operations whose tasks are completely independent from each other as is shown in the idealized example in fig 4d but since tasks are created asynchronously in amt tasks from different modelling operations can be scheduled for execution as long as the input data of each of these individual tasks is ready depending on the modelling operation input data of individual tasks can be relatively small subsets of the full input rasters of the operations for example in a model containing multiple local operations tasks from every operation may be executing at the same time even though output rasters from some of these operations is input of others the amt runtime considers individual tasks not operations in hpx a data structure called f u t u r e exists which represents the output of a task this output may be ready to be used or it may become ready later on dependencies between tasks are defined by attaching tasks to futures output from other tasks once a task is finished its output future is marked ready and dependent tasks are notified the hpx runtime manages task schedulers one per os thread that manage multiple queues of tasks some of which are ready to be executed while others are still waiting for input dependencies to be satisfied when spawning hpx tasks the framework developer has to specify the target each task must execute on common targets are os processes and object instances within processes called components in hpx processes and components can be local to the computer on which a task is spawned or remote this is transparent to the software developer when using the hpx api the developer programs a single abstract machine consisting of one or more processes running on one or more computers because of this hpx can be used transparently for parallel computing on both a single shared memory computer and on multiple distributed memory computers this is an advantage over existing popular approaches that use multiple apis like using mpi mpi forum 2015 for the distribution and openmp for the parallelization of work 4 method 4 1 implementation an implementation of map algebra requires a data structure for representing rasters and operations translating input rasters to output rasters we designed a partitioned multidimensional array data structure with two capabilities that are important for our purposes first the size of the partitions is configurable which is important because it influences the size of tasks translating array partitions second the partitions can be distributed over multiple operating system processes which is important because tasks translating array partitions are sent to the data the distribution of partitions therefore determines the distribution of most of the computational load array partitions are implemented in terms of hpx component clients these are light weight objects providing a convenient api for interacting with possibly remote component server instances containing the actual array partition elements hpx component client objects are semantically equivalent to futures they refer to data that may or may not be ready to use yet but as any hpx future they allow a task to be attached to them which will be scheduled for execution once the data has arrived and the future becomes ready in fig 6 an example of a partitioned array is shown whose partitions are distributed over three nodes in a cluster the partitioned array allows the whole raster to be stored in the memory available to a single process or distributed over multiple processes possibly on multiple nodes the client code using partitioned array instances like modelling algorithms does not need to make a distinction between these two cases given the partitioned array data structure we developed algorithms implementing a set of local focal and zonal operations the goal of the algorithms is to generate and distribute tasks that will perform the necessary calculations in such a way that the computational load is evenly balanced over the available processes they are fully asynchronous none of the algorithmic steps block the flow of control each algorithm may finish executing even before any of the input array partitions are available input partitions may be the result of another asynchronous task like an i o operation reading partition data from a data set or another local focal or zonal operation this means that the array elements in such a partition may not be available yet the idea is to attach the task for translating input elements to output elements to the future representing the input array partition hpx will schedule such a continuation automatically for execution once the input partition data becomes available our algorithms always contain parts that execute in the process where the input partitioned arrays are located and parts that execute in the processes where the input array partitions referenced by the input partitioned arrays are located these latter parts are used by the former part to perform most of the computations the main steps of the algorithms implementing local focal and zonal operations are shown in algorithm 1 algorithm 2 and algorithm 3 in appendix a although some of the steps may suggest that the flow of control blocks this is not the case for example when an input partition of a task is not ready yet or partition data has not arrived yet the flow of control will continue generating more tasks as soon as partitions do become ready or data arrives the state of associated tasks is changed by the hpx runtime from staged waiting for dependencies to be satisfied to pending ready to run from then on these tasks can be scheduled for execution the latencies involved in requesting data from an array partition depend on the location of the partition server relative to the partition client if they are located in separate processes latencies are much higher because memory has to be copied from the server s process to the client s one possibly involving network traffic instead if they are located in the same process no memory is copied only the address of the data is because input array partitions are never changed themselves the partition data can be assumed to always be in a valid state and no synchronization primitives like mutexes and locks are needed to enforce that the result of our approach is that calling multiple local focal and zonal operations after each other creates many tasks for the hpx runtime to schedule for execution once their input data requirements are met as long as there are more tasks that are ready to be executed than there are workers to execute them the hardware will be fully occupied the creation of a model s tasks will generally finish before the tasks themselves at which point the execution of a model will block until the last task has finished executing 4 2 scalability and performance to characterize the scalability of our modelling framework we developed an example model simulating wildfire listing 1 this model is based on concepts from existing fire models e g clarke et al 1994 li et al 2017 freire and dacamara 2019 trucchia et al 2020 and its scalability and performance can thus be representative for this type of environmental models the model is implemented by combining local focal and zonal modelling operations from our framework two processes relevant in fire models are represented by the model the first is surface fire where an area catches fire because it contains burnable material and a neighbouring area is already burning the second process represented is spotting fire where an area catches fire because an area further away is burning the example model serves as a typical use case for an environmental modelling framework containing map algebra operations results of the scaling experiments of the example model provide information about the usefulness of amt in the implementation of a modelling framework we also assessed the scalability of individual local focal and zonal operations results from these experiments are useful to detect scalability issues with a specific kind of modelling operation we performed scaling experiments on a partition of a computer cluster the hardware and software platform of each of the nodes in this partition is listed in table 1 in each of the cluster nodes cpu cores are grouped into numa non uniform memory access nodes main memory is distributed over these numa nodes and cpus can reference values stored in memory of their own numa node faster than values stored in the memory of neighbouring numa nodes this is relevant when designing scaling experiments when scaling over cpu cores it matters in which numa node these cores are located randomly picking cpu cores results in non reproducible scalability measures we performed separate scaling experiments over three kinds of workers 1 over the 6 cpu cores within a single numa node 2 over the 8 numa nodes within a single cluster node and 3 over 12 cluster nodes within a cluster partition for smaller problems scalability over cpu cores is relevant and for increasingly larger problems the scalability over numa nodes and cluster nodes is when scaling over the 6 cpu cores a single process was assigned to a single numa node and cpu cores were assigned in order from within this numa node when scaling over the 8 numa nodes within a cluster node as many processes were used as numa nodes used by each specific run each of them assigned to the cpu cores within a separate numa node when scaling over cluster nodes on each node 8 processes were used one per numa node using a process per numa node is a convenient way to make sure memory allocations and references are resolved by the nearby main memory in the same numa node as the process we calculated both the relative strong and weak scaling efficiencies the relative strong scaling efficiency provides information about how well the modelling framework is able to use additional workers while the total problem size the number of cells in the rasters processed by the model is kept constant it is calculated by dividing the software s latency t s 1 on a single worker by the latency t s p on p workers multiplied by p equation 1 in the case of linear scaling p t s p equals t s 1 in that case doubling the number of workers halves the latency 1 strong scaling efficiency t s 1 p t s p 100 the relative weak scaling efficiency provides information about how well the modelling framework is able to use additional workers while the problem size per worker is kept constant it is calculated by dividing the software s latency t w 1 on a single worker by the latency t w p on p workers equation 2 in the case of linear scaling t w p equals t w 1 in that case doubling the number of workers and the number of cells in the rasters processed does not influence the latency 2 weak scaling efficiency t w 1 t w p 100 before performing the scaling experiments of the operations and the case study model we first determined their optimal task size determined by the array partition size we measured the optimal task size given the maximum array size and number of workers as used in the strong scaling experiments in order to determine the variability in the latencies of model runs these experiments were repeated three times in this study we focussed on the scalability of the computations time spent on i o was not taken into account when measuring latencies in table 2 the size of the arrays used in the scaling experiments are shown the sizes where chosen such that in all experiments all cpu cores would have a relatively large amount of work to perform which would still fit in the memory of the numa node for comparison a raster of 96 000 96 000 cells can cover an area as large as australia with 30 30 m cells operation experiments were simulated for 500 time steps calling the operation once for each time step and the example model experiments were simulated for 250 time steps these counts were chosen such that each model would take between half an hour and 3 h to finish in appendix b the pseudo code can be found of the models used in the scaling experiments for individual kinds of map algebra operations the wildfire model we used is shown in listing 1 because scalability is the main focus in this work we have not optimized our code for performance we did measure throughputs to get an impression of the performance of each experiment here throughputs are a measure of how many raster cells are being calculated per second during each experiment assuming each experiment results in a single raster at the end of each time step additionally to get an impression of the absolute performance of our framework when using a single cpu core we compared the latency of the example model with the same model implemented using the pcraster modelling framework karssenberg et al 2010 4 3 usability besides the scalability aspects of the new modelling framework we evaluate how easy the framework can be developed and how well the resulting software can be used by model developers to characterize this we evaluated the resulting source code with respect to one of the aspects that are generally considered an important characteristic of maintainable code namely whether or not the code is modular and contains clearly separated layers of abstraction iso iec 25010 2011 2011 this is not meant to be a complete software quality analysis but an evaluation of some of the implications of using the amt programming model as implemented in the hpx library in the implementation of a modelling framework to characterize the usability of the framework by model developers we review what the implications are for the modeller to develop a model using our framework ideally there should be no difference between using our framework and comparable alternatives 5 results 5 1 scalability and performance the results of the partition shape experiments show that there is often a range of partition sizes that result in relatively small latencies fig 7 to provide the hpx schedulers within each process with as many tasks as possible we selected the smallest optimal partition size to use for the strong and weak scaling experiments table 3 the experiments also show that the variability in latencies is relatively small at optimal partition sizes fig 7 we therefore did not perform the strong and weak scaling experiments multiple times in general the strong and weak scaling experiments show good scalability fig 8 and table 4 in most cases the efficiencies are around 80 or higher when scaling over cluster nodes the efficiencies are lower especially in the case of the experiments with the focal operation and the example model but even when using 12 cluster nodes it is still useful to use additional nodes to obtain model results faster or to simulate larger problems the strong scaling experiment of the zonal operation over numa nodes shows supra linear scaling this implies that the performance of the zonal operation when using multiple numa nodes is better than can be expected given the performance when using a single numa node one reason for this may be that the partition size used in each scaling experiment is determined using the problem size and maximum number of workers as used in the strong scaling experiments section 4 it is possible that this partition size is less optimal when running the model on a single numa node this would then increase the latency of running the zonal operation model on a single numa node and increase the associated scaling efficiencies the measured throughputs table 5 show that the local operation experiment is able to provide results faster than the focal operation experiment which is faster than the zonal operation experiment since the wildfire model contains more expressions per time step than the other experiments the speed with which it is able to fill the final raster at the end of each time step is lower given the scaling efficiencies of the experiments throughputs increase with the number of workers we compared the latency of the wildfire model with the same model implemented using pcraster 2 2 the pcraster model is available in the source code repository associated with this paper section 7 in an experiment using a single cpu core with rasters of 500 500 cells and 5000 time steps pcraster took 5 min and the new framework 5 45 min these experiments used a single cpu core and performed i o to different file formats the latencies shown in table 6 show how long the experiments took although the amount of work per cpu core was kept more or less constant between experiments of different kinds of workers see table 2 the weak scaling latencies increase when going from cpu cores to numa nodes to cluster nodes this is likely due to the loss in efficiency when changing the kind of worker the latencies of the networks between numa nodes and cluster nodes add to the total latencies of performing an experiment 5 2 usability the source code implementing the modelling framework currently contains two main layers of abstraction the first one implements the partitioned array and related functionality for creating new arrays distributing partitions over processes and transporting partition data between processes this layer uses facilities provided by the hpx library like components for implementing array partition data servers and serialization archives for communicating data between processes hpx provides a relatively high level of abstraction on top of lower level abstractions for example for managing os threads scheduling tasks for execution communication between processes and communication between cluster nodes in the lowest abstraction level of the modelling framework code we did not have to concern ourselves with this in the second layer of abstraction in the framework the algorithms are implemented using the functionality from the first layer no lower level abstractions are needed when implementing algorithms for example in the implementation of the algorithms it is not necessary to be aware of where partitions are located or to manage the sending and receiving of partition data such steps are therefore missing from the descriptions of the algorithms shown in appendix a the result of being able to separate responsibilities in multiple layers of abstraction is that it has become possible to implement the modelling algorithms using a few lines of code especially after refactoring the code common to similar operations for example all code common to the binary local operations is refactored into a single c function all concrete binary local operations use this function in their implementation passing only the part that is unique for the actual operation in the case of a parallel and distributed local operation calculating the sum of two arrays this part consists of a single line of code calculating the result of summing two array elements 6 discussion 6 1 scalability and performance in this paper we presented the design and implementation of a prototype environmental modelling framework using asynchronous many tasks supported by the hpx library the initial set of local focal and zonal operations included in our framework show good weak and strong scaling characteristics over the three kinds of workers considered we have used a maximum number of 576 cpu cores 12 cluster nodes with 48 cpu cores each and our results show that it is still beneficial to add more cluster nodes to be able to process more data or obtain model results faster the scalability of the example model is comparable with the scalability of the individual operations apart from selecting a good partition size the example model did not require any tuning by the model developer the hpx runtime is able to schedule tasks generated by multiple modelling operations at the same time even though the scalability we measured was good there is still room for improvement also when using our framework on larger cluster partitions with more than the 12 nodes we considered in this study scalability will likely continuously decrease to improve the scalability and performance of the models created with our framework in future work we will look into the effect of the parallel generation of tasks currently the main tasks implementing the modelling operations are generated by a single process using a single cpu core using more cpu cores for this will likely increase the speed with which tasks are distributed over the processes for the same reasons we will also research the automatic load balancing between processes the hpx library contains facilities that make it feasible to include this in our framework see heller et al 2019 for an example in which this is already done increasingly with the inclusion of more kinds of modelling operations some processes may have consistently less to do than others because tasks follow the data moving partitions from one process to the other automatically shifts the computational load as well note that such future improvements to how the framework works internally will not influence how the models themselves are developed by the model developer but only their scalability and performance we have focussed here on the computational aspects of a scalable modelling framework and disregarded that the runtime of models includes i o as well in practice the time spent on i o can be dominating the runtime of a model and more so as the part spent on computation scales better scalable i o depends on the use of parallel i o supported for example by mpi io mpi forum 2015 and higher level apis using it like netcdf unidata 2008 2018 hdf5 the hdf group 1997 2019 or the lue physical data model de jong and karssenberg 2019 the latter api is part of the same software library as the framework described in this paper investigating how to incorporate scalable i o in the framework is an important next step comparing our scalability and performance results with the results of related studies is difficult this is because we focus on the scalability of the compute part of models on the use of different kinds of workers including cluster nodes and because other studies use different operations or models in their experiments for example pcml shook et al 2016 does not support distributed computing scalability results are reported for a maximum of 16 cpu cores and these include time spent on i o the comparison of our example model with the same model implemented with pcraster showed that although these two modelling frameworks have a very different implementation the new framework containing modelling operations that are unoptimized for absolute performance approaches pcraster s performance given the scalability characteristics of the new framework the new framework will be faster and be able to process much larger problems when additional workers are used 6 2 usability our framework allows modellers to write their models using simple imperative statements similar to existing map algebra implementations no technical details related to parallel and distributed computing are leaked to the model development interface as illustrated by the pseudocode models in appendix b currently the modelling operations are available as c api functions models are regular executables that can be run from the command line either distributed or non distributed when run distributed multiple copies of a model must be started this is handled by mpi related tools or a batch scheduler which are available on every standard computer cluster when run non distributed there is no need for a dependency of the model on mpi and models can be run on regular desktop or laptop computers when executing models the modeller must pass a partition size to the model which results in the best performance having to determine this partition size is something we would like to relieve the modeller of possibly by an automatic procedure the integration of the apex performance environment for runtime adaptation huck et al 2015 would allow for the automatic selection of best partition sizes for example most of the envisioned target users are not c developers but are familiar with the python language as a proof of concept we developed a python package containing language bindings for two local operations available in the source code repository section 7 eventually a python package will be made available containing all modelling operations note that the syntax for implementing environmental models is almost the same when developing models in c or python we have implemented and performed experiments with an example model simulating wildfire this model was selected because it combines only local focal and zonal operations we are confident that other models in which the same operations are combined will also result in good scaling efficiencies a favourable property of using asynchronous many tasks in the implementation of modelling operations is that it becomes less important which operations are used and in which order they are called compared to approaches using the synchronous fork join paradigm there is an increased chance of the runtime being able to schedule tasks that are ready to run on workers being able to scale models over multiple nodes in a cluster has the advantage of being able to execute models faster but also to execute larger models as mentioned in section 4 the sizes of the rasters we used were dependent on the kind of scaling experiment and the amount of memory available in a single numa node in a real modelling study the memory in all cluster nodes can be summed and used to calculate the maximum raster sizes that can be used for example the 12 cluster nodes used in our experiments have an aggregated amount of memory of 3072 gib assuming only rasters containing double precision floating point values and 10 state variables similar to the wildfire model we used this results in raster sizes of about 200 000 200 000 cells due to other software using memory the hpx runtime using memory and because tasks from multiple time steps can be executing at the same time the real size will be somewhat lower assuming a cell size of 10 m the example model can model wildfire for an area of 2000 2000 km an area the size of a quarter of the earth s surface can be modelled when using a cell size of 100 m adding more nodes to the cluster partition would increase this maximum possible raster size to use for this model further for the model developer using our framework the usability of the framework is important for the framework developer factors related to the usability of the hpx library are important in particular the in convenience of writing modelling operations in terms of asynchronous many tasks although there is a learning curve involved in using asynchronous many tasks writing modelling operations in terms of interdependent asynchronous many tasks is comparable to writing regular serial code the main difference is that the framework developer cannot assume that data is available by the time the model s flow of control reaches the operation in principle all data is referred to by futures an operation s tasks must be defined as something that will execute once the required input data is available the big advantage of course is that resulting operations scale over multiple workers and this is achieved without the need for using explicit message passing using mpi and the use of synchronization primitives as needed when using os threads it is the responsibility of the hpx runtime to schedule tasks on workers this supports a good software development practice of defining stacks of abstraction layers with different responsibilities rather than mixing framework code with code unrelated to modelling 6 3 future work given the promising results of this work we will continue adding more functionality and improving the existing functionality in our future work for example besides the topics already mentioned in this section we will work on the integration of more advanced operations used in environmental modelling to our framework and assess how well they and models using them scale we will add operations with a higher computational load and a less predictable spatial distribution of computational load and less predictable dependencies between tasks than considered in this work examples of such operations are those that operate on a hydrologic flow direction network and operations that operate on a friction distance path surface 7 software availability the scalable modelling framework is implemented as part of a software package called lue 3 3 lue stands for life the universe and everything which is the title of one of the books in douglas adams hitchhiker s guide to the galaxy trilogy here it refers to the fact that in designing lue we try to make it applicable in as many contexts as possible we pronounce lue as the french pronounce louis lu ee which is hosted on github at https github com computationalgeography lue the framework is implemented by kor de jong corresponding author in c and the source code is freely available under the mit open source license a document called readme md is included in the root of the source code repository detailing the instructions for building the software lue is portable software and has been successfully built on various platforms operating systems linux macos compilers clang gcc architecture x86 64 a project containing the version of lue used in this work and containing additional information about the commands used for the described experiments can also be found on github at https github com computationalgeography paper 2020 scalable algorithms a release of lue python packages for various platforms is planned for 2021 credit authorship contribution statement kor de jong conceptualization writing original draft worked on concepts designed and implemented the framework wrote the manuscript debabrata panja writing original draft wrote the manuscript marc van kreveld writing original draft wrote the manuscript derek karssenberg writing original draft wrote the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding this work was supported by the research it innovation programme utrecht university the netherlands appendix a algorithms algorithm 1 local operation image 2 algorithm 2 focal operation image 3 algorithm 3 zonal operation image 4 appendix b experiments in the scripts shown in this section clone refers to an existing distributed partitioned array an operation like uniform needs this information to be able to create an output array the code of the actual experiments is very similar to the pseudocode shown here but is implemented in c it can be found in the repository associated with this paper section 7 when the experiment models are executed by the modelling framework concurrent tasks are generated that execute in parallel on multiple workers potentially on multiple cluster nodes note that none of the model scripts contain technical details related to parallel and distributed computing listing 2 pseudocode of local operation experiment listing 2 listing 3 pseudocode of focal operation experiment listing 3 listing 4 pseudocode of zonal operation experiment listing 4 
25851,environmental modelling frameworks allow domain experts rather than software developers to implement and run numerical simulation models in earth and environmental sciences because of the need to use more detailed process representations or larger datasets as input to models it may become infeasible to perform modelling studies due to the increased amount of time it takes for models to calculate results the objective of this study is to evaluate the asynchronous many task approach in the implementation of a prototype scalable modelling framework we evaluate the scalability of local focal and zonal map algebra operations and an example model in which these operations are combined our results show that the capacity of the operations and the example model to use additional hardware like nodes in a computer cluster is good with our freely available prototype framework models can be executed faster and modelling studies processing considerably more data can be performed keywords environmental modelling framework map algebra high performance computing asynchronous many tasks hpx lue 1 introduction environmental modellers simulate the physical and biological environment using computer models these models can be developed using a multitude of software ranging from relatively low level general purpose programming languages with no built in support for environmental modelling like c c d fortran java and rust to high level modelling frameworks 1 1 we use the term framework loosely to mean software containing at least data types and algorithms used for the development of individual models this includes the case of a software library implementing these but excludes integration frameworks used for coupling models containing pre built model building blocks like google earth engine for earth science data and analysis gorelick et al 2017 matlab holzbecher 2012 the netlogo agent based modelling framework wilensky 1999 and the pcraster field based modelling framework karssenberg et al 2010 an advantage of using modelling frameworks is that they in different degrees hide some of the low level complexities of implementing models this speeds up model development and allows domain experts without a background in software development to develop models fig 1 and karssenberg 2002 some of the model development interfaces are inspired by map algebra tomlin 1990 which is also the approach that will be followed here existing examples using map algebra include the python programming language packages provided by pcraster arcgis and qgis in map algebra fields of spatially varying environmental attributes are represented by rasters which can be combined and translated into new rasters using a procedural programming style a set of such translations simulating environmental processes during a single time step can be used by a modelling framework to do forward iteration through time error propagation and data assimilation karssenberg et al 2010 the framework provides the elementary data structures and modelling algorithms used by modellers in their models ideally models built with such a framework offer good performance whatever the combination of modelling operations used in our study we look at designing and building such a framework for developing environmental models over time models often outgrow their capacity to calculate results in a timely manner this may be because of an increase in dataset sizes used by models an increase in temporal or spatial resolution or extents or an increase in model complexity in order to solve this discrepancy between the size and performance requirements of large models and their capacity to provide results models must increase their ability to use the current generation of hardware in general newer generations of hardware are more powerful but also more complex than earlier generations typically current computers contain more cores more kinds of cores and a deeper memory hierarchy additionally the availability of computer clusters to modellers containing multiple compute nodes connected by low latency network connections has increased the challenge we focus on is that of building a modelling framework that makes better use of the available hardware there are multiple approaches for developing a modelling framework implementing a collection of parallel and distributed map algebra operations an intuitive and popular approach to creating parallel versions of such operations is to use the synchronous fork join paradigm supported by openmp dagum and menon 1998 for example in which individual algorithms implementing these operations are parallelized and called in sequence examples of frameworks using this approach are the parallel raster processing library prpl guan and clarke 2010 the parallel raster based geocomputation operators pargo qin et al 2014 and the parallel cartographic modelling language pcml shook et al 2016 a drawback of this approach is that it introduces implicit synchronization points at least at the end of each operation the flow of control will wait for all tasks to finish before returning to the caller resulting in workers like cpus and gpus being inactive for some time this negative effect of synchronization points increases with the number of workers and the degree of load imbalance between the workers note that load imbalance between workers is common in environmental modelling operations resulting from an uneven spatial distribution of no data values or because of high spatial dependencies between cell values as is the case in some operations that operate on a flow direction network an alternative approach for implementing a collection of parallel and distributed map algebra operations is to use asynchronous many tasks amt one of the advantages of this approach is that it avoids unnecessary synchronization points with amt work to be done is encoded in a set of relatively small tasks with data dependencies among them tasks are spawned asynchronously allowing the main flow of control to continue into multiple modelling operations resulting in more tasks being spawned tasks get scheduled on workers after their inputs have become available this approach results in a larger collection of tasks than is possible when parallelizing algorithms individually as in the case of the synchronous fork join approach the advantage of having a larger collection of runnable tasks is that it decreases the chance of workers being inactive examples of runtime systems that support amt on distributed memory systems are chapel chamberlain et al 2007 x10 charles et al 2005 hpx kaiser et al 2020a and charm kale and krishnan 1993 of which the first two are specific languages and the latter two are software libraries for a taxonomy of task based parallel programming technologies see thoman et al 2018 we will use the amt runtime system implemented by the hpx software library for implementing our environmental modelling framework the objective of this research is to evaluate the use of amt for the development of a modelling framework containing implementations of map algebra operations that can be used on all kinds of commodity hardware in use by the modelling community ranging from laptops desktops to computer clusters the main question we want to answer is whether the use of such a framework results in scalable models for this we perform different kinds of scaling experiments over different kinds of workers scalability of models is determined by both the software implementing the compute part and the i o part in this study we focus on the compute part additionally we review the resulting framework in terms of its usability by model developers and we review the use of amt in the implementation this paper is organized as follows in section 2 we describe the approach of developing environmental models using map algebra in more detail in section 3 we provide more information about amt and the hpx implementation thereof in section 4 we describe how we used amt to implement a map algebra development interface on top of modelling algorithms in section 5 we present results of scaling experiments we performed with individual algorithms and an example model simulating wildfire in which some of the implemented modelling algorithms are combined we end this paper with a discussion of the results in section 6 the amt runtime system implemented in hpx enables us to write an initial set of high level modelling algorithms that can be called from a map algebra like model development interface in a modelling framework implementing algorithms in terms of asynchronous tasks that translate asynchronously produced input data into output data results in a flexible system in which modelling algorithms can be combined in any order according to the model and still offer good scalability the framework developer is responsible for defining tasks and the dependencies between them and is relieved of the responsibility of scheduling tasks and explicitly sending messages in between processes the framework implementation is freely available for inspection and use section 7 2 model development using map algebra originally the map algebra language was designed for creating cartographic models where the models were collections of maps tomlin 1990 the language consisted of a specific set of relatively simple generic operations that translate raster data a combination of such operations could be used to for example determine suitable locations for land development the advantages of using map algebra are that a finite set of generic operations can be used to handle multiple use cases and that it provides a level of abstraction that makes it suitable for users without a background in software development the principles behind cartographic modelling using map algebra have been extended towards forward numerical simulation of environmental processes as well van deursen et al 2019 karssenberg et al 2010 a map algebra like language is then used to define the initial state of the modelled environmental system and to define the state transitions over time in this context the model refers to the code not to the collection of maps the model shown in listing 1 is an example of an environmental model implemented using a map algebra like language simulating wildfire we used it in our experiments section 4 outputs from the model are shown in fig 2 listing 1 pseudocode of model simulating wildfire listing 1 map algebra operations are often classified according to the kind of neighbourhood from which input raster cells are selected that contribute to the calculation of output raster cells in this study we consider three kinds of operations burrough et al 2015 karssenberg et al 2010 tomlin 1990 local operations fig 3 a focal operations fig 3b and zonal operations fig 3c operations not considered in this work include global operations which can be seen as a subset of zonal operations and network operations operating on a flow direction network 3 asynchronous many tasks and hpx the amt programming model supports defining relatively small tasks of work that need to be executed and the dependencies between them the tasks and their dependencies form a directed acyclic graph that is used by the amt runtime system to determine the order in which the tasks must be executed and to determine which tasks can be scheduled to execute concurrently given enough hardware resources the runtime system will execute concurrent tasks in parallel the requirement for the runtime system to always be able to schedule tasks for execution is that there are enough tasks defined and few dependencies between them in order to achieve this tasks are created asynchronously and do not depend on more tasks than necessary an asynchronously created task is spawned off from its operating system os thread which continues doing other work for example spawning off more tasks hpx is an implementation of the amt programming model and runtime it is an open source software library written in portable c 11 14 17 20 code and does not depend on a compiler from a specific vendor or on compiler extensions it has been used to implement parallel software successfully in multiple studies heller et al 2013 heller et al 2017 heller et al 2019 and khatami et al 2016 using hpx the developer of an environmental modelling framework can define tasks and their dependencies in the usual imperative style of programming in c using the hpx api the graph of tasks is built implicitly and does not need to be explicitly managed by the developer the framework developer s main responsibility is to correctly represent the total amount of work to be executed by a collection of tasks and their data dependencies the size of each task is measured in terms of its latency which depends on the amount of data processed by the task the number of computations performed and on latencies involved in accessing the data the ideal task size is large enough for the overheads of parallelization to be amortized over the sum of the latencies of all tasks and small enough to provide the schedulers with enough concurrent tasks to schedule on workers grubel et al 2015 since the latency of tasks is partly dependent on aspects that are only known at runtime like data values and hardware characteristics it is important that the task size can be influenced by the user one way to do this is to support a parameter representing the amount of data processed by individual tasks to illustrate the differences between the amt approach and other approaches to writing a modelling framework we assume a model exists similar to the map algebra model simulating wildfire shown in listing 1 that calls three modelling operations from the framework for simplicity we will ignore the overheads of parallelization a serial framework executes these operations one after the other on a single worker fig 4 a the latency of this program is the sum of the latencies of all the work that needs to be done since only one worker is used by this program adding more workers will not decrease its latency when the three operations are independent from each other they can be executed in parallel fig 4b the program s latency is determined by the operation taking the most time to finish since each operation is executed by one worker adding more workers will not decrease this program s latency in the implementation os threads can be used for example to spawn threads doing work on multiple cpu cores when the three operations contain concurrent tasks that can be executed in parallel fig 5 another approach can be taken in this case the operations are still executed one after the other but they are partly executed in parallel fig 4c this program s latency is determined by the sum of the latencies of the serial regions and the parallel regions adding more workers will not decrease the latency of the serial regions but given enough concurrent tasks may decrease the latency of the parallel regions in this example it will not though since none of the parallel regions has more than three concurrent tasks in the implementation openmp dagum and menon 1998 can be used for example to create parallel regions in which multiple os threads are used to execute tasks on multiple cpu cores when using the amt approach concurrent tasks from all three operations are executed in parallel taking the dependencies between the tasks into account fig 4d the latency of the program is determined by the maximum of the sums of the latencies of the tasks per worker given enough concurrent tasks adding more workers will decrease the program s latency because tasks from multiple operations are considered there are more options to avoid load imbalance between workers in environmental models most rasters processed by the modelling operations depend on each other the output rasters from operations are used as input in other operations it is therefore unlikely to find many modelling operations whose tasks are completely independent from each other as is shown in the idealized example in fig 4d but since tasks are created asynchronously in amt tasks from different modelling operations can be scheduled for execution as long as the input data of each of these individual tasks is ready depending on the modelling operation input data of individual tasks can be relatively small subsets of the full input rasters of the operations for example in a model containing multiple local operations tasks from every operation may be executing at the same time even though output rasters from some of these operations is input of others the amt runtime considers individual tasks not operations in hpx a data structure called f u t u r e exists which represents the output of a task this output may be ready to be used or it may become ready later on dependencies between tasks are defined by attaching tasks to futures output from other tasks once a task is finished its output future is marked ready and dependent tasks are notified the hpx runtime manages task schedulers one per os thread that manage multiple queues of tasks some of which are ready to be executed while others are still waiting for input dependencies to be satisfied when spawning hpx tasks the framework developer has to specify the target each task must execute on common targets are os processes and object instances within processes called components in hpx processes and components can be local to the computer on which a task is spawned or remote this is transparent to the software developer when using the hpx api the developer programs a single abstract machine consisting of one or more processes running on one or more computers because of this hpx can be used transparently for parallel computing on both a single shared memory computer and on multiple distributed memory computers this is an advantage over existing popular approaches that use multiple apis like using mpi mpi forum 2015 for the distribution and openmp for the parallelization of work 4 method 4 1 implementation an implementation of map algebra requires a data structure for representing rasters and operations translating input rasters to output rasters we designed a partitioned multidimensional array data structure with two capabilities that are important for our purposes first the size of the partitions is configurable which is important because it influences the size of tasks translating array partitions second the partitions can be distributed over multiple operating system processes which is important because tasks translating array partitions are sent to the data the distribution of partitions therefore determines the distribution of most of the computational load array partitions are implemented in terms of hpx component clients these are light weight objects providing a convenient api for interacting with possibly remote component server instances containing the actual array partition elements hpx component client objects are semantically equivalent to futures they refer to data that may or may not be ready to use yet but as any hpx future they allow a task to be attached to them which will be scheduled for execution once the data has arrived and the future becomes ready in fig 6 an example of a partitioned array is shown whose partitions are distributed over three nodes in a cluster the partitioned array allows the whole raster to be stored in the memory available to a single process or distributed over multiple processes possibly on multiple nodes the client code using partitioned array instances like modelling algorithms does not need to make a distinction between these two cases given the partitioned array data structure we developed algorithms implementing a set of local focal and zonal operations the goal of the algorithms is to generate and distribute tasks that will perform the necessary calculations in such a way that the computational load is evenly balanced over the available processes they are fully asynchronous none of the algorithmic steps block the flow of control each algorithm may finish executing even before any of the input array partitions are available input partitions may be the result of another asynchronous task like an i o operation reading partition data from a data set or another local focal or zonal operation this means that the array elements in such a partition may not be available yet the idea is to attach the task for translating input elements to output elements to the future representing the input array partition hpx will schedule such a continuation automatically for execution once the input partition data becomes available our algorithms always contain parts that execute in the process where the input partitioned arrays are located and parts that execute in the processes where the input array partitions referenced by the input partitioned arrays are located these latter parts are used by the former part to perform most of the computations the main steps of the algorithms implementing local focal and zonal operations are shown in algorithm 1 algorithm 2 and algorithm 3 in appendix a although some of the steps may suggest that the flow of control blocks this is not the case for example when an input partition of a task is not ready yet or partition data has not arrived yet the flow of control will continue generating more tasks as soon as partitions do become ready or data arrives the state of associated tasks is changed by the hpx runtime from staged waiting for dependencies to be satisfied to pending ready to run from then on these tasks can be scheduled for execution the latencies involved in requesting data from an array partition depend on the location of the partition server relative to the partition client if they are located in separate processes latencies are much higher because memory has to be copied from the server s process to the client s one possibly involving network traffic instead if they are located in the same process no memory is copied only the address of the data is because input array partitions are never changed themselves the partition data can be assumed to always be in a valid state and no synchronization primitives like mutexes and locks are needed to enforce that the result of our approach is that calling multiple local focal and zonal operations after each other creates many tasks for the hpx runtime to schedule for execution once their input data requirements are met as long as there are more tasks that are ready to be executed than there are workers to execute them the hardware will be fully occupied the creation of a model s tasks will generally finish before the tasks themselves at which point the execution of a model will block until the last task has finished executing 4 2 scalability and performance to characterize the scalability of our modelling framework we developed an example model simulating wildfire listing 1 this model is based on concepts from existing fire models e g clarke et al 1994 li et al 2017 freire and dacamara 2019 trucchia et al 2020 and its scalability and performance can thus be representative for this type of environmental models the model is implemented by combining local focal and zonal modelling operations from our framework two processes relevant in fire models are represented by the model the first is surface fire where an area catches fire because it contains burnable material and a neighbouring area is already burning the second process represented is spotting fire where an area catches fire because an area further away is burning the example model serves as a typical use case for an environmental modelling framework containing map algebra operations results of the scaling experiments of the example model provide information about the usefulness of amt in the implementation of a modelling framework we also assessed the scalability of individual local focal and zonal operations results from these experiments are useful to detect scalability issues with a specific kind of modelling operation we performed scaling experiments on a partition of a computer cluster the hardware and software platform of each of the nodes in this partition is listed in table 1 in each of the cluster nodes cpu cores are grouped into numa non uniform memory access nodes main memory is distributed over these numa nodes and cpus can reference values stored in memory of their own numa node faster than values stored in the memory of neighbouring numa nodes this is relevant when designing scaling experiments when scaling over cpu cores it matters in which numa node these cores are located randomly picking cpu cores results in non reproducible scalability measures we performed separate scaling experiments over three kinds of workers 1 over the 6 cpu cores within a single numa node 2 over the 8 numa nodes within a single cluster node and 3 over 12 cluster nodes within a cluster partition for smaller problems scalability over cpu cores is relevant and for increasingly larger problems the scalability over numa nodes and cluster nodes is when scaling over the 6 cpu cores a single process was assigned to a single numa node and cpu cores were assigned in order from within this numa node when scaling over the 8 numa nodes within a cluster node as many processes were used as numa nodes used by each specific run each of them assigned to the cpu cores within a separate numa node when scaling over cluster nodes on each node 8 processes were used one per numa node using a process per numa node is a convenient way to make sure memory allocations and references are resolved by the nearby main memory in the same numa node as the process we calculated both the relative strong and weak scaling efficiencies the relative strong scaling efficiency provides information about how well the modelling framework is able to use additional workers while the total problem size the number of cells in the rasters processed by the model is kept constant it is calculated by dividing the software s latency t s 1 on a single worker by the latency t s p on p workers multiplied by p equation 1 in the case of linear scaling p t s p equals t s 1 in that case doubling the number of workers halves the latency 1 strong scaling efficiency t s 1 p t s p 100 the relative weak scaling efficiency provides information about how well the modelling framework is able to use additional workers while the problem size per worker is kept constant it is calculated by dividing the software s latency t w 1 on a single worker by the latency t w p on p workers equation 2 in the case of linear scaling t w p equals t w 1 in that case doubling the number of workers and the number of cells in the rasters processed does not influence the latency 2 weak scaling efficiency t w 1 t w p 100 before performing the scaling experiments of the operations and the case study model we first determined their optimal task size determined by the array partition size we measured the optimal task size given the maximum array size and number of workers as used in the strong scaling experiments in order to determine the variability in the latencies of model runs these experiments were repeated three times in this study we focussed on the scalability of the computations time spent on i o was not taken into account when measuring latencies in table 2 the size of the arrays used in the scaling experiments are shown the sizes where chosen such that in all experiments all cpu cores would have a relatively large amount of work to perform which would still fit in the memory of the numa node for comparison a raster of 96 000 96 000 cells can cover an area as large as australia with 30 30 m cells operation experiments were simulated for 500 time steps calling the operation once for each time step and the example model experiments were simulated for 250 time steps these counts were chosen such that each model would take between half an hour and 3 h to finish in appendix b the pseudo code can be found of the models used in the scaling experiments for individual kinds of map algebra operations the wildfire model we used is shown in listing 1 because scalability is the main focus in this work we have not optimized our code for performance we did measure throughputs to get an impression of the performance of each experiment here throughputs are a measure of how many raster cells are being calculated per second during each experiment assuming each experiment results in a single raster at the end of each time step additionally to get an impression of the absolute performance of our framework when using a single cpu core we compared the latency of the example model with the same model implemented using the pcraster modelling framework karssenberg et al 2010 4 3 usability besides the scalability aspects of the new modelling framework we evaluate how easy the framework can be developed and how well the resulting software can be used by model developers to characterize this we evaluated the resulting source code with respect to one of the aspects that are generally considered an important characteristic of maintainable code namely whether or not the code is modular and contains clearly separated layers of abstraction iso iec 25010 2011 2011 this is not meant to be a complete software quality analysis but an evaluation of some of the implications of using the amt programming model as implemented in the hpx library in the implementation of a modelling framework to characterize the usability of the framework by model developers we review what the implications are for the modeller to develop a model using our framework ideally there should be no difference between using our framework and comparable alternatives 5 results 5 1 scalability and performance the results of the partition shape experiments show that there is often a range of partition sizes that result in relatively small latencies fig 7 to provide the hpx schedulers within each process with as many tasks as possible we selected the smallest optimal partition size to use for the strong and weak scaling experiments table 3 the experiments also show that the variability in latencies is relatively small at optimal partition sizes fig 7 we therefore did not perform the strong and weak scaling experiments multiple times in general the strong and weak scaling experiments show good scalability fig 8 and table 4 in most cases the efficiencies are around 80 or higher when scaling over cluster nodes the efficiencies are lower especially in the case of the experiments with the focal operation and the example model but even when using 12 cluster nodes it is still useful to use additional nodes to obtain model results faster or to simulate larger problems the strong scaling experiment of the zonal operation over numa nodes shows supra linear scaling this implies that the performance of the zonal operation when using multiple numa nodes is better than can be expected given the performance when using a single numa node one reason for this may be that the partition size used in each scaling experiment is determined using the problem size and maximum number of workers as used in the strong scaling experiments section 4 it is possible that this partition size is less optimal when running the model on a single numa node this would then increase the latency of running the zonal operation model on a single numa node and increase the associated scaling efficiencies the measured throughputs table 5 show that the local operation experiment is able to provide results faster than the focal operation experiment which is faster than the zonal operation experiment since the wildfire model contains more expressions per time step than the other experiments the speed with which it is able to fill the final raster at the end of each time step is lower given the scaling efficiencies of the experiments throughputs increase with the number of workers we compared the latency of the wildfire model with the same model implemented using pcraster 2 2 the pcraster model is available in the source code repository associated with this paper section 7 in an experiment using a single cpu core with rasters of 500 500 cells and 5000 time steps pcraster took 5 min and the new framework 5 45 min these experiments used a single cpu core and performed i o to different file formats the latencies shown in table 6 show how long the experiments took although the amount of work per cpu core was kept more or less constant between experiments of different kinds of workers see table 2 the weak scaling latencies increase when going from cpu cores to numa nodes to cluster nodes this is likely due to the loss in efficiency when changing the kind of worker the latencies of the networks between numa nodes and cluster nodes add to the total latencies of performing an experiment 5 2 usability the source code implementing the modelling framework currently contains two main layers of abstraction the first one implements the partitioned array and related functionality for creating new arrays distributing partitions over processes and transporting partition data between processes this layer uses facilities provided by the hpx library like components for implementing array partition data servers and serialization archives for communicating data between processes hpx provides a relatively high level of abstraction on top of lower level abstractions for example for managing os threads scheduling tasks for execution communication between processes and communication between cluster nodes in the lowest abstraction level of the modelling framework code we did not have to concern ourselves with this in the second layer of abstraction in the framework the algorithms are implemented using the functionality from the first layer no lower level abstractions are needed when implementing algorithms for example in the implementation of the algorithms it is not necessary to be aware of where partitions are located or to manage the sending and receiving of partition data such steps are therefore missing from the descriptions of the algorithms shown in appendix a the result of being able to separate responsibilities in multiple layers of abstraction is that it has become possible to implement the modelling algorithms using a few lines of code especially after refactoring the code common to similar operations for example all code common to the binary local operations is refactored into a single c function all concrete binary local operations use this function in their implementation passing only the part that is unique for the actual operation in the case of a parallel and distributed local operation calculating the sum of two arrays this part consists of a single line of code calculating the result of summing two array elements 6 discussion 6 1 scalability and performance in this paper we presented the design and implementation of a prototype environmental modelling framework using asynchronous many tasks supported by the hpx library the initial set of local focal and zonal operations included in our framework show good weak and strong scaling characteristics over the three kinds of workers considered we have used a maximum number of 576 cpu cores 12 cluster nodes with 48 cpu cores each and our results show that it is still beneficial to add more cluster nodes to be able to process more data or obtain model results faster the scalability of the example model is comparable with the scalability of the individual operations apart from selecting a good partition size the example model did not require any tuning by the model developer the hpx runtime is able to schedule tasks generated by multiple modelling operations at the same time even though the scalability we measured was good there is still room for improvement also when using our framework on larger cluster partitions with more than the 12 nodes we considered in this study scalability will likely continuously decrease to improve the scalability and performance of the models created with our framework in future work we will look into the effect of the parallel generation of tasks currently the main tasks implementing the modelling operations are generated by a single process using a single cpu core using more cpu cores for this will likely increase the speed with which tasks are distributed over the processes for the same reasons we will also research the automatic load balancing between processes the hpx library contains facilities that make it feasible to include this in our framework see heller et al 2019 for an example in which this is already done increasingly with the inclusion of more kinds of modelling operations some processes may have consistently less to do than others because tasks follow the data moving partitions from one process to the other automatically shifts the computational load as well note that such future improvements to how the framework works internally will not influence how the models themselves are developed by the model developer but only their scalability and performance we have focussed here on the computational aspects of a scalable modelling framework and disregarded that the runtime of models includes i o as well in practice the time spent on i o can be dominating the runtime of a model and more so as the part spent on computation scales better scalable i o depends on the use of parallel i o supported for example by mpi io mpi forum 2015 and higher level apis using it like netcdf unidata 2008 2018 hdf5 the hdf group 1997 2019 or the lue physical data model de jong and karssenberg 2019 the latter api is part of the same software library as the framework described in this paper investigating how to incorporate scalable i o in the framework is an important next step comparing our scalability and performance results with the results of related studies is difficult this is because we focus on the scalability of the compute part of models on the use of different kinds of workers including cluster nodes and because other studies use different operations or models in their experiments for example pcml shook et al 2016 does not support distributed computing scalability results are reported for a maximum of 16 cpu cores and these include time spent on i o the comparison of our example model with the same model implemented with pcraster showed that although these two modelling frameworks have a very different implementation the new framework containing modelling operations that are unoptimized for absolute performance approaches pcraster s performance given the scalability characteristics of the new framework the new framework will be faster and be able to process much larger problems when additional workers are used 6 2 usability our framework allows modellers to write their models using simple imperative statements similar to existing map algebra implementations no technical details related to parallel and distributed computing are leaked to the model development interface as illustrated by the pseudocode models in appendix b currently the modelling operations are available as c api functions models are regular executables that can be run from the command line either distributed or non distributed when run distributed multiple copies of a model must be started this is handled by mpi related tools or a batch scheduler which are available on every standard computer cluster when run non distributed there is no need for a dependency of the model on mpi and models can be run on regular desktop or laptop computers when executing models the modeller must pass a partition size to the model which results in the best performance having to determine this partition size is something we would like to relieve the modeller of possibly by an automatic procedure the integration of the apex performance environment for runtime adaptation huck et al 2015 would allow for the automatic selection of best partition sizes for example most of the envisioned target users are not c developers but are familiar with the python language as a proof of concept we developed a python package containing language bindings for two local operations available in the source code repository section 7 eventually a python package will be made available containing all modelling operations note that the syntax for implementing environmental models is almost the same when developing models in c or python we have implemented and performed experiments with an example model simulating wildfire this model was selected because it combines only local focal and zonal operations we are confident that other models in which the same operations are combined will also result in good scaling efficiencies a favourable property of using asynchronous many tasks in the implementation of modelling operations is that it becomes less important which operations are used and in which order they are called compared to approaches using the synchronous fork join paradigm there is an increased chance of the runtime being able to schedule tasks that are ready to run on workers being able to scale models over multiple nodes in a cluster has the advantage of being able to execute models faster but also to execute larger models as mentioned in section 4 the sizes of the rasters we used were dependent on the kind of scaling experiment and the amount of memory available in a single numa node in a real modelling study the memory in all cluster nodes can be summed and used to calculate the maximum raster sizes that can be used for example the 12 cluster nodes used in our experiments have an aggregated amount of memory of 3072 gib assuming only rasters containing double precision floating point values and 10 state variables similar to the wildfire model we used this results in raster sizes of about 200 000 200 000 cells due to other software using memory the hpx runtime using memory and because tasks from multiple time steps can be executing at the same time the real size will be somewhat lower assuming a cell size of 10 m the example model can model wildfire for an area of 2000 2000 km an area the size of a quarter of the earth s surface can be modelled when using a cell size of 100 m adding more nodes to the cluster partition would increase this maximum possible raster size to use for this model further for the model developer using our framework the usability of the framework is important for the framework developer factors related to the usability of the hpx library are important in particular the in convenience of writing modelling operations in terms of asynchronous many tasks although there is a learning curve involved in using asynchronous many tasks writing modelling operations in terms of interdependent asynchronous many tasks is comparable to writing regular serial code the main difference is that the framework developer cannot assume that data is available by the time the model s flow of control reaches the operation in principle all data is referred to by futures an operation s tasks must be defined as something that will execute once the required input data is available the big advantage of course is that resulting operations scale over multiple workers and this is achieved without the need for using explicit message passing using mpi and the use of synchronization primitives as needed when using os threads it is the responsibility of the hpx runtime to schedule tasks on workers this supports a good software development practice of defining stacks of abstraction layers with different responsibilities rather than mixing framework code with code unrelated to modelling 6 3 future work given the promising results of this work we will continue adding more functionality and improving the existing functionality in our future work for example besides the topics already mentioned in this section we will work on the integration of more advanced operations used in environmental modelling to our framework and assess how well they and models using them scale we will add operations with a higher computational load and a less predictable spatial distribution of computational load and less predictable dependencies between tasks than considered in this work examples of such operations are those that operate on a hydrologic flow direction network and operations that operate on a friction distance path surface 7 software availability the scalable modelling framework is implemented as part of a software package called lue 3 3 lue stands for life the universe and everything which is the title of one of the books in douglas adams hitchhiker s guide to the galaxy trilogy here it refers to the fact that in designing lue we try to make it applicable in as many contexts as possible we pronounce lue as the french pronounce louis lu ee which is hosted on github at https github com computationalgeography lue the framework is implemented by kor de jong corresponding author in c and the source code is freely available under the mit open source license a document called readme md is included in the root of the source code repository detailing the instructions for building the software lue is portable software and has been successfully built on various platforms operating systems linux macos compilers clang gcc architecture x86 64 a project containing the version of lue used in this work and containing additional information about the commands used for the described experiments can also be found on github at https github com computationalgeography paper 2020 scalable algorithms a release of lue python packages for various platforms is planned for 2021 credit authorship contribution statement kor de jong conceptualization writing original draft worked on concepts designed and implemented the framework wrote the manuscript debabrata panja writing original draft wrote the manuscript marc van kreveld writing original draft wrote the manuscript derek karssenberg writing original draft wrote the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding this work was supported by the research it innovation programme utrecht university the netherlands appendix a algorithms algorithm 1 local operation image 2 algorithm 2 focal operation image 3 algorithm 3 zonal operation image 4 appendix b experiments in the scripts shown in this section clone refers to an existing distributed partitioned array an operation like uniform needs this information to be able to create an output array the code of the actual experiments is very similar to the pseudocode shown here but is implemented in c it can be found in the repository associated with this paper section 7 when the experiment models are executed by the modelling framework concurrent tasks are generated that execute in parallel on multiple workers potentially on multiple cluster nodes note that none of the model scripts contain technical details related to parallel and distributed computing listing 2 pseudocode of local operation experiment listing 2 listing 3 pseudocode of focal operation experiment listing 3 listing 4 pseudocode of zonal operation experiment listing 4 
25852,maritime activity is known to increase pollutant concentration levels in neighboring cities in major touristic destinations the singular need of cruise liners to keep supplying energy to on board services and amenities while docked has raised concerns about this industry contribution to pollutant emissions to estimate the impact of port activities and that exclusively due to cruises classical approaches would rely on atmospheric dispersion models although these tools retain the underlying physics lack of details on background flow state and emission inventories limits their predictive capabilities using historical data on pollutant concentration meteorology and traffic intensity at specific locations across the city of barcelona it was found that predictions of local pollutant concentration by the present machine learning tool are more accurate than those provided by the caliope urban v1 0 in our test cases estimated air quality impact due to cruise ships is shown to be limited in comparison to overall port effects keywords urban air pollution cruise ships generalized boosted regression models machine learning 1 introduction transportation represents a significant fraction of the total emissions of several chemical compounds that directly impact human health including carbon monoxide nitrogen and sulfur oxides particulate matter tropospheric ozone and volatile organic compounds voc to mention the most relevant seinfeld and pandis 1998 walker et al 2019 maritime traffic contribution to global emissions both freight and passengers has continuously grown over the last decades sorte et al 2019 viana et al 2014 profillidis et al 2019 zhao et al 2013 despite notable efforts to reduce its impact by for instance limiting the sulfur content in marine fuel schembari et al 2012 globally maritime transport is still characterized by consuming low quality fuel in comparison to road and air traffic lack and corbett 2011 at port shipping activities have been found to contribute to increased levels of air pollutants in neighboring urban areas fileni et al 2019 isakson et al 2001 cruise shipping has raised special concerns because of its particular mode of operation characterized by the continuous supply of fossil fuel energy to the various on board services and amenities during docking hotelling contini et al 2011 merico et al contini the average energy consumption used in manoeuvring cruise liners has been estimated to be only 5 of that used at dock to provide light heating ventilation air conditioning cold storage cooking and other services estimated at 2 5 mw per average cruise ship saxe and larsen 2004 on one hand previous efforts to quantify the impact of ports on air quality have been directed to characterize the pollutant emissions of docked vessels number and location of sources duration and rate composition and use this information to feed more or less complex atmospheric dispersion models to predict the air quality in neighboring areas saxe and larsen 2004 poplawski et al 2011 fileni et al 2019 murena et al toscano other studies analyzed pollutant concentration measurements to statistically infer connections between overall port activity and air quality levels eckhardt et al 2013 donateo et al contini et al 2011 isolated impact of cruise ships and its peculiar regime of sustained emissions during hotelling on major touristic destinations and areas of ecological value has also been the subject of research eckhardt et al 2013 maragkogianni and papaefthimiou 2015 celic et al 2014 murena et al toscano poplawski et al 2011 in general the transport of a chemical species i is governed by the convection diffusion reaction equation that can be written as 1 ρ y i t ρ u y i j i r i s i where ρ is the background mixture fluid density t is time u is the mixture velocity field y i is the local mass fraction of chemical species i j i is the mass diffusion flux r i accounts for any production consumption of i due to chemical reactions and s i represents any other source sink of i full solutions of eq 1 are usually unavailable for at least two reasons on one hand local atmospheric velocity field u responsible for advecting the different chemical species in the turbulent planetary boundary layer is typically not well specified secondly rates of emission consumption by chemical reaction and surface deposition in pollutant inventories are in many cases approximations of varying accuracy several orders of magnitude smaller than the convective transport counterpart diffusive contribution is usually neglected classical approaches have sought for y i x t by numerically integrating some approximated form of eq 1 fan et al 1972 leelossy et al 2014 lateb et al 2016 in gaussian plume models one of the most commonly used approximation hood et al 2018 wind velocity and direction are assumed constant and turbulent transport is parametrized using horizontal and vertical standard deviations of the emission distribution seinfeld and pandis 1998 other approaches to urban pollutant dispersion modelling involve the spatial and temporal discretization of some convenient form of the chemical species transport equation eq 1 over a computational domain covering the geographical region and time period of interest instantaneous velocity and concentration are approximated by some form of averaged filtered fields resulting from modelling the turbulent contribution to the transport of momentum mass and heat common methodologies involve embedded computational meshes with a coarse far field level where boundary conditions are imposed and progressively finer grids where detail is needed although this approach retains part of the physics that governs atmospheric dispersion it also has important inherent sources of error due to the impossibility of reconstructing the full turbulent hydrodynamic field and the lack of detailed characterization of each pollutant emission and chemical transformation rates the approach presented here uses supervised machine learning ml techniques to elucidate the relation between the local concentration of a given pollutant the target or response and several relevant variables for pollutant transport including meteorology and traffic intensity the features or predictors use of machine learning algorithms to exploit the vast amounts of data retrieved from city wide monitor networks has grown in recent years relvas and miranda 2018 song and han 2019 zheng et al 2013 kleine deters et al 2017 while classical approaches retain the underlying physics of pollutant dispersion and are well suited for assessing the impact of different potential scenarios on local air quality by for example considering different spatial locations and emission rates for individual pollutant sources when robust and accurate estimations of the functionality between the features and the response exist ml methodologies can accurately predict the isolated effect of each predictor on pollutant concentration levels regardless of the availability of precise descriptions of the background flow state and pollutant inventories the model presented in this work is especially intended to estimate the overall impact of port activity and that exclusively due to cruise ships on the air quality of a major coastal touristic destination the city of barcelona has been chosen as a benchmark because it is one of the major travel destination with a total of 9 million tourists in 2018 observatori del turisme a barcelona 2018 eurostat table mar mg aa pwhd 2020 and its port impact on the metropolitan area has already been the subject of significant research efforts perez et al 2016 perdiguero and sanz 2020 rodríguez et al 2017 this work is organized as follows sec 2 describes the methods and materials involved in obtaining the working dataset used to train and tune the air quality predictive model sec 3 presents the main results and quantifies the contribution of the port of barcelona activity to increased pollutant concentration for several chemical species and locations across the metropolitan area sec 4 discusses the hypothesis assumptions and potential future improvements to the current methodology and sec 5 summarizes the major findings 2 methods 2 1 overview in order to analyze the impact of the port activity and cruise ship traffic we present and discuss an implementation of supervised machine learning techniques for prediction of air quality levels in urban environments the specific goal is to obtain for each pollutant species and selected spatial location accurate estimations of the function f that captures the dependence between the local pollutant concentration or response y and a set of n variables or predictors x j this response depends upon i e 2 y f x j ε y ˆ ε j 1 n where y ˆ is the predicted concentration and ε is the error associated to unexplained variability in y by inspection of eq 1 predictors must include information on both local atmospheric state and source emission rates of each pollutant precise estimations of pollutant concentration will be available as long as data on x j and y allow to elucidate the function f that explains most of the variability in y and minimizes ε this is the difference between observed and predicted values 2 2 dataset description the working dataset used in this work and described in this section contains pollutant concentration measurements weather measurements and road air and maritime traffic data 2 2 1 pollutant concentration the response dataset is comprised of hourly concentrations measurements of nitric oxide no nitrogen dioxide no 2 total nitrogen oxides no x sulfur dioxide so 2 ozone o 3 particulate matter 10 μg or less in diameter pm 10 and carbon monoxide co the data is retrieved from m 8 pollutant stations belonging to the air pollution monitoring and forecasting network xvpca of the catalan government departament de territori i sostenibilitat 2020 that are distributed across the metropolitan area of barcelona as shown in fig 1 dark red markers height and type of station is shown in table 1 all pollutant concentration is in μ g m 3 except co that is in mg m 3 note that the relation between concentration and mass fraction of species i can be expressed as c i y i ρ m i where m i is the molar weight of i 2 2 2 weather the weather dataset includes measurements on wind velocity k m h 1 and direction relative humidity atmospheric pressure hpa precipitation mm temperature c and solar irradiance wm 2 with a 30 min sampling rate from n 11 different meteorological stations across the metropolitan area of barcelona and surroundings the location of the closest stations to the metropolitan area are shown in fig 1 in blue these data have been obtained from the network of automatic weather stations xema of the meteorological service of catalonia servei meteorològic de catalunya 2020 this predictor set is extended with daily measurements of cloud cover from a single station located at the observatori fabra station see fig 1 obtained from the european climate assessment and dataset eca d tank et al 2002 2 2 3 traffic pollutant emissions from combustion of maritime fossil fuels are incorporated into the model by parsing traffic log files of the port of barcelona barcelona port authority 2020 magenta marker in fig 1 the available data includes arrival and departure time stamps for each vessel and ship type used to identify cruise liners and size length and beam this data has been used to generate two predictors for the hourly number and median size of total vessels and cruise liners predictors for air traffic activity consist of hourly number of arrival and departures from the airport of barcelona nicolas bruno 2020 green marker in fig 1 hourly predictors for road traffic intensity expressed as an index ranging from 1 fluid to 6 total congestion have been obtained from the extensive network of traffic measurement points across the city barcelona s city hall open data service 2020 orange markers in fig 1 distributions of the number of total vessels and cruise ships simultaneously docked at the port of barcelona over the time span considered in this work ranging from october 2017 to march 2020 both included as well as the distribution of flight operations at the airport of barcelona are shown in b 2 3 preprocessing weather and pollutant concentration measurements were joined by their respective observation time stamps into a synchronized single dataset daily measurements of cloud cover from a single weather station barcelona observatori fabra were assumed constant over the corresponding 24 h period and over the entire metropolitan area the working dataset was completed by merging the three traffic data collections by their time stamps estimations for road traffic intensity σ and every weather predictor θ on each pollutant station i 1 m were obtained by weighted interpolation of each observation the weighting factor was set to the 4 th power of the distance d i j between the pollutant station i and each of the j 1 n meteorological stations to ensure that closest observations contributes the most to the interpolated weather predictor analogously the distance d i k between the pollutant station i and each of the k 1 q road transit measurement points were used in the spatial interpolation of traffic intensity specifically 3 θ i j 1 n θ ˆ j η i j σ i k 1 q σ ˆ k η i k where η i j is the distance weight from each pollutant station i to each meteorological station j and η i k is the distance weight from each pollutant station i to each to each traffic measurement location k 4 η i j s i j j 1 n s i j η i k s i k k 1 q s i k where 5 s i j 1 d i j 4 s i k 1 d i k 4 the wind direction predictor is expressed in terms of its alignment with respect to the direction connecting each pollutant station i and the two major transportation infrastructures likely to affect the air quality in the metropolitan area the port and the airport of barcelona given each pollutant station bearing angle with respect to the port α i p and the airport α i a and β i being the direction the wind blows to alignment with respect to the port and airport can be computed as 6 a i p α i p β i 180 1 a i a α i a β i 180 1 where α i p α i a and β i are expressed in degrees with the origin pointing to the north and increasing in the clockwise east direction the values of a i p and a i a are constrained in the range 0 a i 1 where 0 indicates wind blowing against the i th station as seen from the corresponding infrastructure and 1 indicates complete alignment 2 4 working dataset the working dataset is only comprised of complete observations this is all predictors for a given hourly time stamps are known the temporal span of the working dataset ranges from october 2017 to march 2020 both included the list of the 25 predictors used in this work including a description and the variable name is shown in table 2 the data completeness for each pollutant and station defined as the ratio between the actual number of observations and the theoretical number of hourly observations over the 30 months period is shown in table 1 note that while nitrogen oxides concentration data exists for all stations other pollutants are not available for all measurement points to illustrate the data contained in the working dataset fig 2 shows the temporal evolution of some variables including the response for no x for the barcelona eixample station over march 2019 2 5 ml procedure the r package mlr 2 17 1 bischl et al 2016 has been used for data pre processing resampling hyperparameter tuning and post processing regarding the pre processing all continuous features have been normalized by subtracting the average value and dividing by the standard deviation for each pollutant species and measurement station categorical variables have been converted to continuous features using a 1 of 1 method in this work six different learning algorithms or learners widely used in ml applications have been tested namely gradient boosting machine or gbm gbm support vector machines or svm ksvm random forest h2o ra n d omforest multivariate adaptive regression splines earth feed forward multilayer artificial neural network h2o deeplearning and feed forward neural networks for multinomial log linear models nnet where the name of the integrated regression model in the r package mlr 2 17 1 bischl et al 2016 appears in parenthesis a 5 fold cross validations resampling strategy has been used to assess the performance of each learner thus the working data set described above has been repeatedly split into a training and a test set after optimizing each learner hyperparameters the training set has been used to derive the corresponding model which performance has then been estimated using the testing set the benchmark results in terms of mean squared error for each pollutant across all stations for each learner are shown in c results clearly indicate that for this specific problem and working dataset the gradient boosting machine gbm outperforms the rest of the algorithms considered in this work optimal gbm hyperparameters were found to be around 5000 trees and an interaction depth of 25 for most of pollutants and measurement stations all results in sec 3 have been obtained with the gbm methodology all computations have been performed on a dell aio 7770 workstation with a intel r core tm i9 9900 cpu 3 10 ghz learner training and testing hyperparameter tuning and resampling typical cpu time is 45 min for each station and pollutant single point prediction takes less than 1 ms 3 results 3 1 validation to assess the predictive performance of the ml based tool presented in sec 2 predicted concentration levels are compared with those reported by benavides et al 2019 who used the caliope urban v1 0 physics based model to predict no 2 concentration at three of the pollutant measurement stations listed in table 1 caliope urban v1 0 is the result of coupling caliope baldasano et al 2011 an operational mesoscale air quality forecast system based on the hermes emissions guevara et al 2013 wrf meteorology skamarock and klemp and cmaq chemistry byun and schere 2006 models with the urban roadway dispersion model r line snyder et al 2013 the results shown in table 3 indicate that the ml based model outperforms the caliope urban v1 0 platform in every metric for all available stations definitions of geometric mean bias geomean correlation coefficient r mean bias mb root mean square error rmse and fraction of predictions within a factor of 2 in observations fac2 can be found in appendix a 3 2 feature importance the relative importance of each predictor has been determined using the entropy based information gain bischl et al 2016 that returns an importance weight for each feature that maximizes the amount of captured variability in the response the four most important features for each station are shown in fig 3 in descending order from top to bottom rows results indicate that the day of the year is the most important feature for almost all background urban in black and the only background suburban in red stations although this predictor also explains most of the variability in traffic urban stations in blue this leading role is shared with the year number in the case of co the traffic intensity for no x and the temperature for pm 10 the hour of the day is the second most important feature for no x and o 3 at most stations with some exceptions for which this position is occupied by traffic intensity wind speed and temperature in the case of co the second most relevant predictor at the traffic urban stations is the traffic intensity while the year number dominates in background urban sites this latter feature is also the second most important variable in the case of so 2 this suggests that despite the relatively small number of years in the working dataset the concentrations for these two species exhibit significant changes over the 2017 2020 period in contrast to the other species the pm 10 concentration at most locations has been found to be controlled in second place by the temperature while the top two most important features were dominated by temporally related predictors namely the hour of the day the day of the year and the year number the third and fourth positions with a much smaller impact on the concentration variability as discussed later exhibit notable variability across stations and pollutants thus carbon monoxide concentration eventually exhibits some dependence on the docked ship size similar behavior is exhibited by the sulfur dioxide no x mostly controlled by the day of the year and the hour of the day has also been found to change with the intensity of the traffic the wind velocity and the vessel size third and fourth features for ozone include the wind velocity and alignment with respect to the port and the temperature finally the particulate matter strongly dependent on the day of the year and the temperature exhibits also some dependence on the traffic intensity the hour of the day the wind alignment with respect to the port and the number of flights at the airport the importance gain method for each station and pollutant are shown in fig 4 this metric has been normalized such that its sum across all predictors is 100 in general the results indicate a consistent distribution of importance between stations and pollutants with most of the variability in concentration explained by the hour of the day the day number of the year the road traffic intensity and the temperature on the other side weekday predictors are found to be the least important features notably though the model reveals a few notable peculiarities first the results in the suburban station observatori fabra located at an altitude more than twice that of the second one in height suggest that ozone concentration is mostly controlled by the day of the year these results are in agreement to the well known seasonal behavior of tropospheric ozone with higher values in the summer months when atmospheric conditions favour o 3 formation as expected traffic intensity scores high in importance in urban traffic stations eixample and gràcia sant gervasi for nitrogen oxides and carbon monoxide contrarily limited effects on these pollutant concentrations for this feature are found in background and suburban locations the impact of traffic intensity on nitrogen oxides concentration at the observatori fabra station can be explained by the proximity of a major road traffic route ronda de dalt wind velocity seems to affect also the nitrogen oxides level in background urban and suburban stations wind velocity impact on concentration levels significantly varies across stations with palau reial and sants exhibiting the largest effect on nitrogen oxide levels wind alignment with respect to the airport and the port are found to have similar importance with the latter exhibiting larger impact on pm 10 concentration at the two only traffic urban stations namely eixample and gràcia sant gervasi as happens for the two predictors for the number of total vessels and cruise liners the ship size of the latter type is found to be the least relevant the number of airport movements the pressure and irradiance predictors exhibit modest importance and large cross station and cross pollutant variability the overall least important features are those related to the day of the week as shown except sunday a holiday the workweek days do not show a great influence on pollutant concentration negligible importance has also been found for hourly precipitation cloud cover relative humidity and notably the hourly number of docked cruise ships 3 3 overall port impact using the hourly number of total vessels as a proxy for the port of barcelona activity the isolated impact of this predictor on the air quality at each pollutant station has been estimated by comparing the actual and the predicted concentration levels for several values of this predictor thus the x axis of fig 5 shows the difference between the total number of vessels used in the prediction and the actual value the y axis shows the average difference over all observations in the working dataset between the observed concentration and average prediction of the gbm learner using a 3 fold resampling for reference mean and median concentration values for each station and pollutant in the working dataset are shown in table 4 as shown in fig 5 the excellent accuracy of the ml model results in negligible averaged difference between the actual and the predicted concentration y 0 when the number of vessels used in the prediction coincides with the actual one i e x 0 in general the model predicts that an increase in the activity in the port of barcelona as measured by the total vessel traffic leads to an increase in the local concentrations of carbon monoxide nitrogen oxides and particulate matter the overall trend indicates that the impact of the maritime traffic weakens as the distance from the port increases thus the impact of the number of vessels on the concentration levels in ciutadella and eixample stations is notably larger than that in observatori fabra and palau reial the largest impacts predicted by the model are found at the closest to the port station eixample where no and no 2 concentrations are expected to rise around 12 and 10 respectively in comparison to the mean value in table 4 in contrast the largest increases in co 9 and pm 10 11 concentrations are found at slightly further stations gràcia sant gervasi and poblenou respectively the signature of the port activity on the ozone and sulfur dioxide concentration is clearly weaker suggesting lack of correlation between the number of vessels and their concentration levels 3 4 cruise ship impact following an analogous approach the hourly number of liners has been used as a proxy for cruise shipping activity in the port of barcelona in fig 6 the x axis corresponds to the average difference between the actual number of cruise ships and that used in the prediction while the y axis shows the corresponding difference between the observed and average predicted concentration over all observations in the working dataset of course since cruise ships represent a relatively small fraction of the total vessels its isolated effect on the air quality of barcelona is expected to be less pronounced predictions suggest that the largest impact of cruise ships on nitrogen oxide concentration levels occurs at the two stations closest to the port ciutadella and eixample with 1 3 μg m 3 of no x for each additional cruise liner in port impact on nitrogen oxides levels is predicted to decay as distance between the station and the port increases in contrast to this marked variability across stations for no x predicted concentrations of pm 10 are found to vary in a consistent way across stations with an average slope of 0 04 μg m 3 for each additional cruise liner co is predicted to be insensitive when changes in the number of docked cruise liners is modest 3 ships for variations above 5 cruise ships co concentration levels start exhibiting significant variations at the closest stations to the port farther locations are predicted to remain constant independently of the number of liners cruise ship activity seems to have a negligible impact on sulfur dioxide with maximum variations in concentration less than 2 5 change with respect to the across station mean value of 1 67 μg m 3 obtained from the four station mean values in table 4 predicted impact on ozone concentrations is found to be negligible in most stations and negatively correlated in others specially observatori fabra the relatively large horizontal and vertical distance between the port and this station suggest that on overall cruise ships do not affect the ozone levels across the metropolitan area the overall modest increase in pollutant concentration of the hourly number of cruise liners and this predictor relatively small importance gain suggest that in comparison to the overall effects of the port the impact of the cruise activity on the air quality of the city is very limited 4 discussion the availability and ease of access to large amounts of data from uncountable sources of scientific and technological relevance has made it possible to use machine learning techniques to improve our understanding of processes and phenomena until now only accessible through classical approaches this paradigm is illustrated in this work where provided enough data on both the atmospheric state and several proxies on pollutant inventories it is shown that ml based predictors are capable of outperforming traditional pollutant dispersion models while traditional physics based approaches suffer from uncertainties on local weather and pollutant inventories ml approaches are generally criticized by their black box nature offering small insight on the inner workings of the physical processes under study besides these general considerations and despite the excellent predictive capabilities of the tool developed here this model has two main limitations in comparison to traditional approaches on one hand the relatively small number of stations reporting concentration levels across the metropolitan area of barcelona hinders the potential of the model to be used as an operational tool capable of predicting air quality at any location across the region of interest in other words the current model lacks of spatial features therefore air quality predictions are restricted to the locations and heights of each pollutant station for which an independent model has been derived pollutant measurement campaigns with fixed or mobile equipment might provide in the future high spatial resolution information that make possible to use measurement location and altitude as predictors this key feature would enable the transition of the current ml model into a fully operational tool capable of providing accurate fast and cheap predictions of air quality second in this work pollutant emissions are accounted via some proxy variables including the number of flights vessels and road traffic congestion although these are prime contributions to urban pollution industrial and agricultural emission may also play a significant role in worsening the air quality of neighboring cities overall model accuracy could be potentially improved provided better characterizations of pollutant emission inventories in addition to transportation of course the performance of the ml tool presented here is expected to increase as the size and quality of the time series datasets used to train the learners increase 5 conclusions weather and pollutant concentration measurements collected by monitoring networks across the metropolitan area of barcelona have been combined with road air and maritime traffic intensity data to train a machine learning model to predict the air quality in the city in terms of predictive capabilities of local no 2 concentration levels at several station locations across barcelona the resulting tool has been found to have better performance than the caliope urban v1 0 platform using mean squared and absolute errors as metrics for predictive accuracy the benchmark of different classical learners suggested that for this specific dataset and across all stations and pollutants the gradient boosting machine gbm has been found to be the best performer importance analysis based on information gain indicates that the three top features explaining pollutant level variability are the time of the year the daytime and the road traffic intensity the least important predictors include the workweek day the precipitation relative humidity and notably the number of cruise ships in port using the difference between observed and predicted pollutant concentrations for several difference values with respect to the actual number of vessels the ml tool has been used to estimate the contribution to worsened air quality of the overall port activity and that exclusively due to cruise liners results suggest that close to the port of barcelona the overall activity of this infrastructure leads to 12 and 10 increased mean concentration of no and no 2 respectively largest increases in co and pm 10 have been estimated to be 9 and 11 in comparison the port impact on so 2 and o 3 concentration levels is predicted to be significantly weaker using the hourly number of docked cruise ships as a proxy to estimate the isolated effect of this industry predictions suggest that the impact of liners on the air quality is very limited in comparison to the overall port contribution predictions suggest that the largest impact of cruise ships is 1 3 μg m 3 of no x for each additional docked cruise ship in port besides the better accuracy in predicting pollutant concentration the numerical simulations carried out with the ml based model presented here required only a personal workstation with typical run times in the order of hours this computational costs are well below those typically required by state of the art dispersion models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financially supported by the spanish ministry of economy industry and competitiveness research national agency under projects dpi2016 75791 c2 1 pand rti2018 100907 a i00 by feder funds and by the generalitat de catalunya agaur under project 2017 sgr 01234 authors would like to thank the government of catalonia for granting access to atmospheric pollutant data from the xvpca network and the servei meteorològic de catalunya for weather measurements appendix a metrics the geometric mean bias geomean the fraction of model results within a factor of 2 of observations fac2 the correlation coefficient r the mean bias mb and the root mean square error rmse are defined as follows a 1 geomean exp ln y ˆ i ln y i a 2 fac 2 i n 0 5 y ˆ i y i 2 0 n a 3 r y ˆ i y ˆ i y i y i σ y i σ y ˆ i a 4 m b y i y ˆ i a 5 r m s e y i y ˆ i 2 1 2 where y i and y ˆ i are predicted and observed concentration data points average and standard deviation are respectively represented by the symbols and σ and are the iverson brackets appendix b air and maritime traffic the distribution of number of simultaneously docked total vessels and cruise ships at the port of barcelona over the october 2017 to march 2020 period both included are shown in fig b7 and b8 respectively fig b 7 distribution of simultaneously docked total vessels at the port of barcelona from october 2017 to march 2020 both included fig b 7 fig b 8 distribution of simultaneously docked cruise ships at the port of barcelona from october 2017 to march 2020 both included fig b 8 analogously the distribution of number of flight opeartions at the airport of barcelona over the october 2017 to march 2020 period both included is shown in fig b9 fig b 9 distribution of simultaneously docked cruise ships at the port of barcelona from october 2017 to march 2020 both included fig b 9 appendix c learner benchmark the comparison of performance for each optimized learner is shown in fig c10 using the mean squared error for each pollutant averaged across stations fig c 10 mean squared error for each pollutant across all stations for each learner using a 5 fold cross validation resampling fig c 10 
25852,maritime activity is known to increase pollutant concentration levels in neighboring cities in major touristic destinations the singular need of cruise liners to keep supplying energy to on board services and amenities while docked has raised concerns about this industry contribution to pollutant emissions to estimate the impact of port activities and that exclusively due to cruises classical approaches would rely on atmospheric dispersion models although these tools retain the underlying physics lack of details on background flow state and emission inventories limits their predictive capabilities using historical data on pollutant concentration meteorology and traffic intensity at specific locations across the city of barcelona it was found that predictions of local pollutant concentration by the present machine learning tool are more accurate than those provided by the caliope urban v1 0 in our test cases estimated air quality impact due to cruise ships is shown to be limited in comparison to overall port effects keywords urban air pollution cruise ships generalized boosted regression models machine learning 1 introduction transportation represents a significant fraction of the total emissions of several chemical compounds that directly impact human health including carbon monoxide nitrogen and sulfur oxides particulate matter tropospheric ozone and volatile organic compounds voc to mention the most relevant seinfeld and pandis 1998 walker et al 2019 maritime traffic contribution to global emissions both freight and passengers has continuously grown over the last decades sorte et al 2019 viana et al 2014 profillidis et al 2019 zhao et al 2013 despite notable efforts to reduce its impact by for instance limiting the sulfur content in marine fuel schembari et al 2012 globally maritime transport is still characterized by consuming low quality fuel in comparison to road and air traffic lack and corbett 2011 at port shipping activities have been found to contribute to increased levels of air pollutants in neighboring urban areas fileni et al 2019 isakson et al 2001 cruise shipping has raised special concerns because of its particular mode of operation characterized by the continuous supply of fossil fuel energy to the various on board services and amenities during docking hotelling contini et al 2011 merico et al contini the average energy consumption used in manoeuvring cruise liners has been estimated to be only 5 of that used at dock to provide light heating ventilation air conditioning cold storage cooking and other services estimated at 2 5 mw per average cruise ship saxe and larsen 2004 on one hand previous efforts to quantify the impact of ports on air quality have been directed to characterize the pollutant emissions of docked vessels number and location of sources duration and rate composition and use this information to feed more or less complex atmospheric dispersion models to predict the air quality in neighboring areas saxe and larsen 2004 poplawski et al 2011 fileni et al 2019 murena et al toscano other studies analyzed pollutant concentration measurements to statistically infer connections between overall port activity and air quality levels eckhardt et al 2013 donateo et al contini et al 2011 isolated impact of cruise ships and its peculiar regime of sustained emissions during hotelling on major touristic destinations and areas of ecological value has also been the subject of research eckhardt et al 2013 maragkogianni and papaefthimiou 2015 celic et al 2014 murena et al toscano poplawski et al 2011 in general the transport of a chemical species i is governed by the convection diffusion reaction equation that can be written as 1 ρ y i t ρ u y i j i r i s i where ρ is the background mixture fluid density t is time u is the mixture velocity field y i is the local mass fraction of chemical species i j i is the mass diffusion flux r i accounts for any production consumption of i due to chemical reactions and s i represents any other source sink of i full solutions of eq 1 are usually unavailable for at least two reasons on one hand local atmospheric velocity field u responsible for advecting the different chemical species in the turbulent planetary boundary layer is typically not well specified secondly rates of emission consumption by chemical reaction and surface deposition in pollutant inventories are in many cases approximations of varying accuracy several orders of magnitude smaller than the convective transport counterpart diffusive contribution is usually neglected classical approaches have sought for y i x t by numerically integrating some approximated form of eq 1 fan et al 1972 leelossy et al 2014 lateb et al 2016 in gaussian plume models one of the most commonly used approximation hood et al 2018 wind velocity and direction are assumed constant and turbulent transport is parametrized using horizontal and vertical standard deviations of the emission distribution seinfeld and pandis 1998 other approaches to urban pollutant dispersion modelling involve the spatial and temporal discretization of some convenient form of the chemical species transport equation eq 1 over a computational domain covering the geographical region and time period of interest instantaneous velocity and concentration are approximated by some form of averaged filtered fields resulting from modelling the turbulent contribution to the transport of momentum mass and heat common methodologies involve embedded computational meshes with a coarse far field level where boundary conditions are imposed and progressively finer grids where detail is needed although this approach retains part of the physics that governs atmospheric dispersion it also has important inherent sources of error due to the impossibility of reconstructing the full turbulent hydrodynamic field and the lack of detailed characterization of each pollutant emission and chemical transformation rates the approach presented here uses supervised machine learning ml techniques to elucidate the relation between the local concentration of a given pollutant the target or response and several relevant variables for pollutant transport including meteorology and traffic intensity the features or predictors use of machine learning algorithms to exploit the vast amounts of data retrieved from city wide monitor networks has grown in recent years relvas and miranda 2018 song and han 2019 zheng et al 2013 kleine deters et al 2017 while classical approaches retain the underlying physics of pollutant dispersion and are well suited for assessing the impact of different potential scenarios on local air quality by for example considering different spatial locations and emission rates for individual pollutant sources when robust and accurate estimations of the functionality between the features and the response exist ml methodologies can accurately predict the isolated effect of each predictor on pollutant concentration levels regardless of the availability of precise descriptions of the background flow state and pollutant inventories the model presented in this work is especially intended to estimate the overall impact of port activity and that exclusively due to cruise ships on the air quality of a major coastal touristic destination the city of barcelona has been chosen as a benchmark because it is one of the major travel destination with a total of 9 million tourists in 2018 observatori del turisme a barcelona 2018 eurostat table mar mg aa pwhd 2020 and its port impact on the metropolitan area has already been the subject of significant research efforts perez et al 2016 perdiguero and sanz 2020 rodríguez et al 2017 this work is organized as follows sec 2 describes the methods and materials involved in obtaining the working dataset used to train and tune the air quality predictive model sec 3 presents the main results and quantifies the contribution of the port of barcelona activity to increased pollutant concentration for several chemical species and locations across the metropolitan area sec 4 discusses the hypothesis assumptions and potential future improvements to the current methodology and sec 5 summarizes the major findings 2 methods 2 1 overview in order to analyze the impact of the port activity and cruise ship traffic we present and discuss an implementation of supervised machine learning techniques for prediction of air quality levels in urban environments the specific goal is to obtain for each pollutant species and selected spatial location accurate estimations of the function f that captures the dependence between the local pollutant concentration or response y and a set of n variables or predictors x j this response depends upon i e 2 y f x j ε y ˆ ε j 1 n where y ˆ is the predicted concentration and ε is the error associated to unexplained variability in y by inspection of eq 1 predictors must include information on both local atmospheric state and source emission rates of each pollutant precise estimations of pollutant concentration will be available as long as data on x j and y allow to elucidate the function f that explains most of the variability in y and minimizes ε this is the difference between observed and predicted values 2 2 dataset description the working dataset used in this work and described in this section contains pollutant concentration measurements weather measurements and road air and maritime traffic data 2 2 1 pollutant concentration the response dataset is comprised of hourly concentrations measurements of nitric oxide no nitrogen dioxide no 2 total nitrogen oxides no x sulfur dioxide so 2 ozone o 3 particulate matter 10 μg or less in diameter pm 10 and carbon monoxide co the data is retrieved from m 8 pollutant stations belonging to the air pollution monitoring and forecasting network xvpca of the catalan government departament de territori i sostenibilitat 2020 that are distributed across the metropolitan area of barcelona as shown in fig 1 dark red markers height and type of station is shown in table 1 all pollutant concentration is in μ g m 3 except co that is in mg m 3 note that the relation between concentration and mass fraction of species i can be expressed as c i y i ρ m i where m i is the molar weight of i 2 2 2 weather the weather dataset includes measurements on wind velocity k m h 1 and direction relative humidity atmospheric pressure hpa precipitation mm temperature c and solar irradiance wm 2 with a 30 min sampling rate from n 11 different meteorological stations across the metropolitan area of barcelona and surroundings the location of the closest stations to the metropolitan area are shown in fig 1 in blue these data have been obtained from the network of automatic weather stations xema of the meteorological service of catalonia servei meteorològic de catalunya 2020 this predictor set is extended with daily measurements of cloud cover from a single station located at the observatori fabra station see fig 1 obtained from the european climate assessment and dataset eca d tank et al 2002 2 2 3 traffic pollutant emissions from combustion of maritime fossil fuels are incorporated into the model by parsing traffic log files of the port of barcelona barcelona port authority 2020 magenta marker in fig 1 the available data includes arrival and departure time stamps for each vessel and ship type used to identify cruise liners and size length and beam this data has been used to generate two predictors for the hourly number and median size of total vessels and cruise liners predictors for air traffic activity consist of hourly number of arrival and departures from the airport of barcelona nicolas bruno 2020 green marker in fig 1 hourly predictors for road traffic intensity expressed as an index ranging from 1 fluid to 6 total congestion have been obtained from the extensive network of traffic measurement points across the city barcelona s city hall open data service 2020 orange markers in fig 1 distributions of the number of total vessels and cruise ships simultaneously docked at the port of barcelona over the time span considered in this work ranging from october 2017 to march 2020 both included as well as the distribution of flight operations at the airport of barcelona are shown in b 2 3 preprocessing weather and pollutant concentration measurements were joined by their respective observation time stamps into a synchronized single dataset daily measurements of cloud cover from a single weather station barcelona observatori fabra were assumed constant over the corresponding 24 h period and over the entire metropolitan area the working dataset was completed by merging the three traffic data collections by their time stamps estimations for road traffic intensity σ and every weather predictor θ on each pollutant station i 1 m were obtained by weighted interpolation of each observation the weighting factor was set to the 4 th power of the distance d i j between the pollutant station i and each of the j 1 n meteorological stations to ensure that closest observations contributes the most to the interpolated weather predictor analogously the distance d i k between the pollutant station i and each of the k 1 q road transit measurement points were used in the spatial interpolation of traffic intensity specifically 3 θ i j 1 n θ ˆ j η i j σ i k 1 q σ ˆ k η i k where η i j is the distance weight from each pollutant station i to each meteorological station j and η i k is the distance weight from each pollutant station i to each to each traffic measurement location k 4 η i j s i j j 1 n s i j η i k s i k k 1 q s i k where 5 s i j 1 d i j 4 s i k 1 d i k 4 the wind direction predictor is expressed in terms of its alignment with respect to the direction connecting each pollutant station i and the two major transportation infrastructures likely to affect the air quality in the metropolitan area the port and the airport of barcelona given each pollutant station bearing angle with respect to the port α i p and the airport α i a and β i being the direction the wind blows to alignment with respect to the port and airport can be computed as 6 a i p α i p β i 180 1 a i a α i a β i 180 1 where α i p α i a and β i are expressed in degrees with the origin pointing to the north and increasing in the clockwise east direction the values of a i p and a i a are constrained in the range 0 a i 1 where 0 indicates wind blowing against the i th station as seen from the corresponding infrastructure and 1 indicates complete alignment 2 4 working dataset the working dataset is only comprised of complete observations this is all predictors for a given hourly time stamps are known the temporal span of the working dataset ranges from october 2017 to march 2020 both included the list of the 25 predictors used in this work including a description and the variable name is shown in table 2 the data completeness for each pollutant and station defined as the ratio between the actual number of observations and the theoretical number of hourly observations over the 30 months period is shown in table 1 note that while nitrogen oxides concentration data exists for all stations other pollutants are not available for all measurement points to illustrate the data contained in the working dataset fig 2 shows the temporal evolution of some variables including the response for no x for the barcelona eixample station over march 2019 2 5 ml procedure the r package mlr 2 17 1 bischl et al 2016 has been used for data pre processing resampling hyperparameter tuning and post processing regarding the pre processing all continuous features have been normalized by subtracting the average value and dividing by the standard deviation for each pollutant species and measurement station categorical variables have been converted to continuous features using a 1 of 1 method in this work six different learning algorithms or learners widely used in ml applications have been tested namely gradient boosting machine or gbm gbm support vector machines or svm ksvm random forest h2o ra n d omforest multivariate adaptive regression splines earth feed forward multilayer artificial neural network h2o deeplearning and feed forward neural networks for multinomial log linear models nnet where the name of the integrated regression model in the r package mlr 2 17 1 bischl et al 2016 appears in parenthesis a 5 fold cross validations resampling strategy has been used to assess the performance of each learner thus the working data set described above has been repeatedly split into a training and a test set after optimizing each learner hyperparameters the training set has been used to derive the corresponding model which performance has then been estimated using the testing set the benchmark results in terms of mean squared error for each pollutant across all stations for each learner are shown in c results clearly indicate that for this specific problem and working dataset the gradient boosting machine gbm outperforms the rest of the algorithms considered in this work optimal gbm hyperparameters were found to be around 5000 trees and an interaction depth of 25 for most of pollutants and measurement stations all results in sec 3 have been obtained with the gbm methodology all computations have been performed on a dell aio 7770 workstation with a intel r core tm i9 9900 cpu 3 10 ghz learner training and testing hyperparameter tuning and resampling typical cpu time is 45 min for each station and pollutant single point prediction takes less than 1 ms 3 results 3 1 validation to assess the predictive performance of the ml based tool presented in sec 2 predicted concentration levels are compared with those reported by benavides et al 2019 who used the caliope urban v1 0 physics based model to predict no 2 concentration at three of the pollutant measurement stations listed in table 1 caliope urban v1 0 is the result of coupling caliope baldasano et al 2011 an operational mesoscale air quality forecast system based on the hermes emissions guevara et al 2013 wrf meteorology skamarock and klemp and cmaq chemistry byun and schere 2006 models with the urban roadway dispersion model r line snyder et al 2013 the results shown in table 3 indicate that the ml based model outperforms the caliope urban v1 0 platform in every metric for all available stations definitions of geometric mean bias geomean correlation coefficient r mean bias mb root mean square error rmse and fraction of predictions within a factor of 2 in observations fac2 can be found in appendix a 3 2 feature importance the relative importance of each predictor has been determined using the entropy based information gain bischl et al 2016 that returns an importance weight for each feature that maximizes the amount of captured variability in the response the four most important features for each station are shown in fig 3 in descending order from top to bottom rows results indicate that the day of the year is the most important feature for almost all background urban in black and the only background suburban in red stations although this predictor also explains most of the variability in traffic urban stations in blue this leading role is shared with the year number in the case of co the traffic intensity for no x and the temperature for pm 10 the hour of the day is the second most important feature for no x and o 3 at most stations with some exceptions for which this position is occupied by traffic intensity wind speed and temperature in the case of co the second most relevant predictor at the traffic urban stations is the traffic intensity while the year number dominates in background urban sites this latter feature is also the second most important variable in the case of so 2 this suggests that despite the relatively small number of years in the working dataset the concentrations for these two species exhibit significant changes over the 2017 2020 period in contrast to the other species the pm 10 concentration at most locations has been found to be controlled in second place by the temperature while the top two most important features were dominated by temporally related predictors namely the hour of the day the day of the year and the year number the third and fourth positions with a much smaller impact on the concentration variability as discussed later exhibit notable variability across stations and pollutants thus carbon monoxide concentration eventually exhibits some dependence on the docked ship size similar behavior is exhibited by the sulfur dioxide no x mostly controlled by the day of the year and the hour of the day has also been found to change with the intensity of the traffic the wind velocity and the vessel size third and fourth features for ozone include the wind velocity and alignment with respect to the port and the temperature finally the particulate matter strongly dependent on the day of the year and the temperature exhibits also some dependence on the traffic intensity the hour of the day the wind alignment with respect to the port and the number of flights at the airport the importance gain method for each station and pollutant are shown in fig 4 this metric has been normalized such that its sum across all predictors is 100 in general the results indicate a consistent distribution of importance between stations and pollutants with most of the variability in concentration explained by the hour of the day the day number of the year the road traffic intensity and the temperature on the other side weekday predictors are found to be the least important features notably though the model reveals a few notable peculiarities first the results in the suburban station observatori fabra located at an altitude more than twice that of the second one in height suggest that ozone concentration is mostly controlled by the day of the year these results are in agreement to the well known seasonal behavior of tropospheric ozone with higher values in the summer months when atmospheric conditions favour o 3 formation as expected traffic intensity scores high in importance in urban traffic stations eixample and gràcia sant gervasi for nitrogen oxides and carbon monoxide contrarily limited effects on these pollutant concentrations for this feature are found in background and suburban locations the impact of traffic intensity on nitrogen oxides concentration at the observatori fabra station can be explained by the proximity of a major road traffic route ronda de dalt wind velocity seems to affect also the nitrogen oxides level in background urban and suburban stations wind velocity impact on concentration levels significantly varies across stations with palau reial and sants exhibiting the largest effect on nitrogen oxide levels wind alignment with respect to the airport and the port are found to have similar importance with the latter exhibiting larger impact on pm 10 concentration at the two only traffic urban stations namely eixample and gràcia sant gervasi as happens for the two predictors for the number of total vessels and cruise liners the ship size of the latter type is found to be the least relevant the number of airport movements the pressure and irradiance predictors exhibit modest importance and large cross station and cross pollutant variability the overall least important features are those related to the day of the week as shown except sunday a holiday the workweek days do not show a great influence on pollutant concentration negligible importance has also been found for hourly precipitation cloud cover relative humidity and notably the hourly number of docked cruise ships 3 3 overall port impact using the hourly number of total vessels as a proxy for the port of barcelona activity the isolated impact of this predictor on the air quality at each pollutant station has been estimated by comparing the actual and the predicted concentration levels for several values of this predictor thus the x axis of fig 5 shows the difference between the total number of vessels used in the prediction and the actual value the y axis shows the average difference over all observations in the working dataset between the observed concentration and average prediction of the gbm learner using a 3 fold resampling for reference mean and median concentration values for each station and pollutant in the working dataset are shown in table 4 as shown in fig 5 the excellent accuracy of the ml model results in negligible averaged difference between the actual and the predicted concentration y 0 when the number of vessels used in the prediction coincides with the actual one i e x 0 in general the model predicts that an increase in the activity in the port of barcelona as measured by the total vessel traffic leads to an increase in the local concentrations of carbon monoxide nitrogen oxides and particulate matter the overall trend indicates that the impact of the maritime traffic weakens as the distance from the port increases thus the impact of the number of vessels on the concentration levels in ciutadella and eixample stations is notably larger than that in observatori fabra and palau reial the largest impacts predicted by the model are found at the closest to the port station eixample where no and no 2 concentrations are expected to rise around 12 and 10 respectively in comparison to the mean value in table 4 in contrast the largest increases in co 9 and pm 10 11 concentrations are found at slightly further stations gràcia sant gervasi and poblenou respectively the signature of the port activity on the ozone and sulfur dioxide concentration is clearly weaker suggesting lack of correlation between the number of vessels and their concentration levels 3 4 cruise ship impact following an analogous approach the hourly number of liners has been used as a proxy for cruise shipping activity in the port of barcelona in fig 6 the x axis corresponds to the average difference between the actual number of cruise ships and that used in the prediction while the y axis shows the corresponding difference between the observed and average predicted concentration over all observations in the working dataset of course since cruise ships represent a relatively small fraction of the total vessels its isolated effect on the air quality of barcelona is expected to be less pronounced predictions suggest that the largest impact of cruise ships on nitrogen oxide concentration levels occurs at the two stations closest to the port ciutadella and eixample with 1 3 μg m 3 of no x for each additional cruise liner in port impact on nitrogen oxides levels is predicted to decay as distance between the station and the port increases in contrast to this marked variability across stations for no x predicted concentrations of pm 10 are found to vary in a consistent way across stations with an average slope of 0 04 μg m 3 for each additional cruise liner co is predicted to be insensitive when changes in the number of docked cruise liners is modest 3 ships for variations above 5 cruise ships co concentration levels start exhibiting significant variations at the closest stations to the port farther locations are predicted to remain constant independently of the number of liners cruise ship activity seems to have a negligible impact on sulfur dioxide with maximum variations in concentration less than 2 5 change with respect to the across station mean value of 1 67 μg m 3 obtained from the four station mean values in table 4 predicted impact on ozone concentrations is found to be negligible in most stations and negatively correlated in others specially observatori fabra the relatively large horizontal and vertical distance between the port and this station suggest that on overall cruise ships do not affect the ozone levels across the metropolitan area the overall modest increase in pollutant concentration of the hourly number of cruise liners and this predictor relatively small importance gain suggest that in comparison to the overall effects of the port the impact of the cruise activity on the air quality of the city is very limited 4 discussion the availability and ease of access to large amounts of data from uncountable sources of scientific and technological relevance has made it possible to use machine learning techniques to improve our understanding of processes and phenomena until now only accessible through classical approaches this paradigm is illustrated in this work where provided enough data on both the atmospheric state and several proxies on pollutant inventories it is shown that ml based predictors are capable of outperforming traditional pollutant dispersion models while traditional physics based approaches suffer from uncertainties on local weather and pollutant inventories ml approaches are generally criticized by their black box nature offering small insight on the inner workings of the physical processes under study besides these general considerations and despite the excellent predictive capabilities of the tool developed here this model has two main limitations in comparison to traditional approaches on one hand the relatively small number of stations reporting concentration levels across the metropolitan area of barcelona hinders the potential of the model to be used as an operational tool capable of predicting air quality at any location across the region of interest in other words the current model lacks of spatial features therefore air quality predictions are restricted to the locations and heights of each pollutant station for which an independent model has been derived pollutant measurement campaigns with fixed or mobile equipment might provide in the future high spatial resolution information that make possible to use measurement location and altitude as predictors this key feature would enable the transition of the current ml model into a fully operational tool capable of providing accurate fast and cheap predictions of air quality second in this work pollutant emissions are accounted via some proxy variables including the number of flights vessels and road traffic congestion although these are prime contributions to urban pollution industrial and agricultural emission may also play a significant role in worsening the air quality of neighboring cities overall model accuracy could be potentially improved provided better characterizations of pollutant emission inventories in addition to transportation of course the performance of the ml tool presented here is expected to increase as the size and quality of the time series datasets used to train the learners increase 5 conclusions weather and pollutant concentration measurements collected by monitoring networks across the metropolitan area of barcelona have been combined with road air and maritime traffic intensity data to train a machine learning model to predict the air quality in the city in terms of predictive capabilities of local no 2 concentration levels at several station locations across barcelona the resulting tool has been found to have better performance than the caliope urban v1 0 platform using mean squared and absolute errors as metrics for predictive accuracy the benchmark of different classical learners suggested that for this specific dataset and across all stations and pollutants the gradient boosting machine gbm has been found to be the best performer importance analysis based on information gain indicates that the three top features explaining pollutant level variability are the time of the year the daytime and the road traffic intensity the least important predictors include the workweek day the precipitation relative humidity and notably the number of cruise ships in port using the difference between observed and predicted pollutant concentrations for several difference values with respect to the actual number of vessels the ml tool has been used to estimate the contribution to worsened air quality of the overall port activity and that exclusively due to cruise liners results suggest that close to the port of barcelona the overall activity of this infrastructure leads to 12 and 10 increased mean concentration of no and no 2 respectively largest increases in co and pm 10 have been estimated to be 9 and 11 in comparison the port impact on so 2 and o 3 concentration levels is predicted to be significantly weaker using the hourly number of docked cruise ships as a proxy to estimate the isolated effect of this industry predictions suggest that the impact of liners on the air quality is very limited in comparison to the overall port contribution predictions suggest that the largest impact of cruise ships is 1 3 μg m 3 of no x for each additional docked cruise ship in port besides the better accuracy in predicting pollutant concentration the numerical simulations carried out with the ml based model presented here required only a personal workstation with typical run times in the order of hours this computational costs are well below those typically required by state of the art dispersion models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financially supported by the spanish ministry of economy industry and competitiveness research national agency under projects dpi2016 75791 c2 1 pand rti2018 100907 a i00 by feder funds and by the generalitat de catalunya agaur under project 2017 sgr 01234 authors would like to thank the government of catalonia for granting access to atmospheric pollutant data from the xvpca network and the servei meteorològic de catalunya for weather measurements appendix a metrics the geometric mean bias geomean the fraction of model results within a factor of 2 of observations fac2 the correlation coefficient r the mean bias mb and the root mean square error rmse are defined as follows a 1 geomean exp ln y ˆ i ln y i a 2 fac 2 i n 0 5 y ˆ i y i 2 0 n a 3 r y ˆ i y ˆ i y i y i σ y i σ y ˆ i a 4 m b y i y ˆ i a 5 r m s e y i y ˆ i 2 1 2 where y i and y ˆ i are predicted and observed concentration data points average and standard deviation are respectively represented by the symbols and σ and are the iverson brackets appendix b air and maritime traffic the distribution of number of simultaneously docked total vessels and cruise ships at the port of barcelona over the october 2017 to march 2020 period both included are shown in fig b7 and b8 respectively fig b 7 distribution of simultaneously docked total vessels at the port of barcelona from october 2017 to march 2020 both included fig b 7 fig b 8 distribution of simultaneously docked cruise ships at the port of barcelona from october 2017 to march 2020 both included fig b 8 analogously the distribution of number of flight opeartions at the airport of barcelona over the october 2017 to march 2020 period both included is shown in fig b9 fig b 9 distribution of simultaneously docked cruise ships at the port of barcelona from october 2017 to march 2020 both included fig b 9 appendix c learner benchmark the comparison of performance for each optimized learner is shown in fig c10 using the mean squared error for each pollutant averaged across stations fig c 10 mean squared error for each pollutant across all stations for each learner using a 5 fold cross validation resampling fig c 10 
25853,despite the critical role that replication plays in the advancement of science its presence in modelling literature is rare to encourage others to conduct replication and report success and challenges that facilitate or hinder replication we present the replication of an agent based model abm of residential sprawl using the replication standard replication results achieved relational equivalence through the replication process issues with the original research were identified and corrected in an improved model which qualitatively supported original results a specific challenge affecting alignment of original and replicate models included capturing model output variability and publishing all original output data for statistical analysis through the replication of agent based models additional confidence in model behaviour can be garnered and replicated abms can become accredited for reuse by others future research in the development and refinement of replication methodology and assessments should be cultivated along with a culture of value for replication efforts keywords model replication agent based modelling replication standard verification 1 introduction modelling is prevalent in almost every area of science ritchey 2012 from mathematics to psychology from engineering to geography and from archaeology to philosophy among these and other disciplines modelling provides four key functions 1 explanation of existing and potential phenomena 2 the ability to make predictions 3 increased decision making capacity and 4 a method to communicate knowledge schichl 2004 due to the usefulness of modelling scientists have spent a large amount of time on and their grant agencies on funding the construction testing and promotion of new models and enhancements to the modelling process each of these tasks are indispensable and time consuming however they fail to explicitly incorporate a critical requirement for scientific advancement which is replication replication is critical to scientific advancement easley et al 2000 latour and woolgar 1979 popper 1959 seagren 2015 as it is the only way to verify the authenticity and reliability of published research and corroborate challenges to current theory it has been argued that scientific results should not be viewed as real science if they cannot be reproduced by others cooper 2018 mckubre 2008 norton 2015 and that replication should be treated as the gold standard for scientific research casadevall and fang 2010 jasny et al 2011 despite substantial benefits arising from model replication why is it that replication has not been adequately recognized as a valuable part of theory development madden et al 1995 accreditation and verification of published results or been explicitly part of any call or requirement for academic funding an increasing number of scientists are coming to the realization that model replication is critical to the advancement of science donkin et al 2017 this realization is partly due to the high volume of models being produced that in many cases duplicate existing work evans et al 2013 the failure of some disciplines to coalesce on one or more community models which contrasts with the climate science and ecosystem modelling communities and the desire to reuse specific model components e g bell et al 2015 borrow bermejo et al 2017 and integrate models e g sturley et al 2018 and construct modelling tools e g tisue and wilensky 2004 to speed up model development and spend more time focusing on the advancement of science furthermore a successful model replication can prove that results are not an exceptional case wilensky and rand 2007 and contribute to theory development thiele and grimm 2015 the construction of a model is part of an iterative process rounsevell et al 2012 however the objective of replication is absent from any modelling process framework known to the authors furthermore a literature review of replication related research in six prominent journals with a focus on agent based modelling yielded poor results using keywords replicate and agent based model we retrieved 348 articles from the journal of artificial societies and social simulation 96 environmental modelling software 68 computers environment and urban systems 37 ecology and society 98 ecological economics 23 and journal of land use science 26 of these 348 articles only nine 2 6 replicated an existing model partially e g ronald et al 2016 or entirely e g donkin et al 2017 among the 9 articles that replicated an abm appendix a only five wilensky and rand 2007 janssen 2009 miodownik et al 2010 radax and rengs 2010 donkin et al 2017 incorporated replication as a primary objective and three journals yielded no abm replication research ecology and society ecological economics journal of land use science we sought to contribute to the gap in model replication literature a to provide a pedagogical example for graduate students that published model results in science need to be reproducible and that all published content requires scrutiny despite the peer review process b to update a 15 year old model that is well cited and remains scientifically relevant brown and robinson 2006 and c present an application of model replication that may be used to guide and encourage other model replication initiatives the process was also an effort to train undergraduate students and reflect on previous nascent modelling efforts that may have unknown flaws therefore the replication effort was also driven out of a desire to ensure model credibility given its longevity and use by others while most scientific replications are conducted by those not directly affiliated with the original experiments i e a third party in many cases third party individuals communicate with the original authors e g wilensky and rand 2007 the need to work with original abm creators is often the case even if a model is merely used for an alternative application bell et al 2015 the presented replication process was led by students guided by an original author of the model s use and collaborator in its development and demonstrates the challenges of replicating published model results even when detailed knowledge of the model and computer code are available through the replication process we sought to determine 1 what errors if any exist in the original model 2 what challenges and difficulties occur during the replication of the model and its associated published experiments and 3 what lessons were learned that may help others seeking to replicate a model 2 methods 2 1 replication process our replication methodology involved six steps understand the conceptual model collect resources verify model agreement replicate model construction model to model comparison conclusions and documentation 2 1 1 step 1 understand the conceptual model the purpose of a conceptual model is to formalize a high level understanding of the system being modelled wagner 2014 in doing so the creators imbed their biases perspectives experiences and knowledge of the system in a generalized format that could be applied to many situations or locations as such a conceptual model defines the scope of a system studied and the components of relevance to the type of research pursued robinson et al 2015 furthermore it can form the basis for other models that use different approaches to operationalize implement components of the conceptual model pennington 2008 ambrose 2015 hereafter we use the term conceptual model in a broad context to also include aspects of the written model formulation such as text equations figures and other model documentation excluding source code and software a successful replication of a model requires an understanding of its conceptual model as it can facilitate comprehension of existing model components their code and sequence and choice of code implementation understanding of the conceptual model becomes more imperative when the original code is not available and replicators must construct the model based solely on text and figures in a pool of equifinality given the likely unattainable goal of having full comprehension of the conceptual model and no uncertainty or questions about the conceptual model a number of methods could be used to assist in this understanding pseudo code could be generated that formalizes the behaviour of the conceptual model e g muller et al 2014 new visualizations created e g sequence diagram of potential activities e g li et al 2004 lin 2008 and descriptive protocols e g grimm et al 2006 2 1 2 step 2 collect resources while understanding the conceptual model should be based on text and figure documentation probability of replication success will be increased if the replicators can acquire as much of the original source code and data as possible radax and rengs 2010 donkin et al 2017 questioning and leveraging published science is one of the reasons why scientific publications include the contact information of authors teunis et al 2015 therefore replicators should contact original authors and attempt to acquire original programming packages comprising code random seed numbers input and output data and information about the hardware on which the models were run as differences in hardware have been shown to affect model outcomes e g wilensky and rand 2007 janssen 2009 2 1 3 step 3 verify model agreement the overarching goal of this step is to ensure agreement between the conceptual model and a published text equations and algorithms and b the implemented model fig 1 step 3 when model source code is not available the replicator should assess that the logic and outcomes of documented components align with existing literature and expert opinion when model source code is available specific code blocks corresponding to text equations and algorithms in published materials should be identified and assessed for their degree of alignment as well as correspondence to existing literature and expert opinion if the agreement between the conceptual and implemented models is satisfactorily verified then the process may move to step 4 otherwise an assessment about which of the two models conceptual or implemented is in err agreement between the conceptual and implemented models can reside in one of four cases case 1 the conceptual and implemented model are in agreement case 2 a flaw is discovered with the conceptual model that differs from the implemented model case 3 a flaw is discovered with the implemented model that differs from the conceptual model case 4 flaws are discovered with both the conceptual and implemented models if the implemented model code is available then the replication has a high probability of success since the primary goal of the replication process is to reproduce the results of the original model while the conceptual model represents the original developers perspective the replication process may uncover unintended mistakes in the communication of the conceptual model 2 1 4 step 4 replicate model construction when the source code of the implemented model is available it may be used to replicate published results e g izquierdo et al 2008 however the purpose of replication is typically for integration with other models or expansion to incorporate new components e g kacmar et al 1999 under these conditions new software and libraries are taken advantage of and a replicate model is constructed while a variety of platforms explicit to agent based modelling exist e g mason repast netlogo salgado and gilbert 2013 other platforms have also proved successful e g r statistics package thiele 2014 matlab macal and howe 2005 2 1 5 step 5 model to model comparison one approach to conducting a comparison between an original model and a replicate model is to use the replication standard rs axtell et al 1996 the rs consists of three potential outcomes 1 relational equivalence requires the replicated model to present similar relationships between input and output variables as the original model 2 distributional equivalence requires the results derived from two models to be statistically indistinguishable to each other and 3 numerical identity requires that the replicated model has the same numerical results with the original model which is the most difficult outcome to achieve it may not be possible to achieve numerical identity due to platform of replication loss or degradation of data and information over time or other factors therefore the replication effort should seek a systematic approach starting with relational equivalence and work toward numerical identity 2 1 6 step 6 conclusions and documentation after the entire replication process no matter whether the expected replicated results are acquired there must be some valuable findings to be concluded a successful replication can evaluate the overall behaviours the reliability and reproducibility of the original model a failure also provides lessons which can be the most valuable outcome of the replication process 3 replication case study 3 1 step 1 understand the conceptual model we apply the aforementioned methodology to replicate a simple agent based model of residential sprawl sluce s original model for exploration some the some model was designed to understand the effects of trade offs among preferences for aesthetic quality nearness to service centers and neighbourhood similarity on spatial patterns of residential development in a hypothetical landscape brown and robinson 2006 empirically informed agents using household survey data from south eastern michigan fernandez et al 2005 marans 2003 to evaluate how categorization of agents binning agents into groups affected spatial patterns of residential development relative to continuous variation in agent site location preferences as part of our efforts to understand the conceptual model literature related to the model were reviewed brown et al 2004 brown et al 2005 robinson and brown 2009 including project proposals nsf n d and other documents originally posted online in addition to one of the coauthors others related to the original model development and its application were contacted to clarify understanding and interpretation 3 2 step 2 collect resources a variety of resources were collected to facilitate the replication process prior to a decommissioning of the sluce project website in 2019 source code in swarm objective c http swarm org 2019 for the original implemented model was acquired 43 separate files in addition control files specifying the parameter settings for five published computational experiments were acquired three files describing instructions for visualizing results the input parameters and output variables as well as a map of values used to represent the landscape raw output data were unattainable and only results reported in brown and robinson 2006 were available 3 3 step 3 verify model agreement through the model verification process we discovered two errors the first was a case 2 error whereby the conceptual model was incorrectly communicated but correctly implemented specifically a function used to calculate the similarity in an agent s preferences to those of its neighbours differed between the conceptual and implemented models table 1 equation 1 corresponding to the conceptual model brown and robinson 2006 allows the value of neighbourhood similarity i e γ 3 r x y to range from 1 3 to infinity while equation 2 is used in the implemented model and constrains the range to between 0 and 1 table 1 neighbourhood similarity equations where n is the number of agents occupying neighbouring locations α1r α2r α3r represent the preference weights for distance to service areas landscape aesthetic and neighbourhood similarity for agent r and j represents the values for the neighbourhood to which r is making the comparison a second error identified in our verification of model agreement was a case 3 error whereby the conceptual model was incorrectly represented in the implemented model in the conceptual model brown and robinson 2006 state every agent must have a preference weight for each component i e αir and the preference weights across the three components were constrained to sum to one however the implemented model did not yield any code that rescaled preference weights to sum to one 3 4 step 4 replicate model construction we created our replicate model in netlogo which has a large user community is maintained and regularly updated is relatively simple to use and share among other researchers and has been integrated with r thiele et al 2012 thiele 2014 python jaxa rozen and kwakkel 2018 and mathematica bakshy and wilensky 2007 among other scientific data analysis tools we initially attempted to construct the replicate model based on only the conceptual model outlined by brown and robinson 2006 to ensure the replicate code was independent from the original code thiele and grimm 2015 however there was insufficient conceptual model documentation requiring us to iteratively revisit the implemented model code and revise our replicate model 3 5 step 5 model to model comparison by constructing the replicate model to create the same spatial and aspatial i e utility outputs as the implemented model we were able to directly compare the two models along the gradient of matching provided by the replication standard to mimic the procedures of the original publication we ran the same five computational experiments using the same parameter settings and landscapes whereby each experiment was run 30 times to produce an average and standard deviation of output metrics results are presented as brown and robinson 2006 in aggregate table 2 and by agent category table 3 table 2 spatial pattern and utility metrics of five experimental preference weight settings for the implemented model replicate model and improved model means and standard deviations s d of 30 runs since the implemented and replicate models include stochastic processes the replication standard of numerical identity cannot be achieved therefore we sought to establish relational equivalence and then test for distributional equivalence different approaches can be used to determine relational equivalence e g regression kleijnen 1998 however with access only to the aggregate output data of the implemented model means and standard deviations we used 1 percent similarity between model results to qualitatively assess relational equivalence using a threshold of 20 0 2 spearman s rank correlation coefficient to assess if output metric values were similarly ordered across our experiments for both models and 3 if within model outputs have the same relationships between models this latter method was achieved by calculating the spearman s rank correlation coefficient between outputs for a model and then comparing the resultant coefficients across models we did not to report p values which were highly variable as they are unreliable given our small sample sizes of five and seven for aggregate and categorical results respectively halsey et al 2015 prior to comparing our model results for distributional equivalence we tested for normality using the shapiro wilk test ghasemi and zahediasl 2012 seven of forty tests 17 5 among aggregate metrics and one of forty two tests 2 0 among categorical result metrics showed non normal metric distributions appendix b given the dominance of normality among our results and our limitation to mean and standard deviations from the original publication we used a student s t test to determine if results from our replicate model were statistically significantly similar α 0 05 to the implemented model 3 5 1 agent aggregated results replicate and implemented model outcomes under the aggregate population context i e no agent categories showed a high level of similarity table 2 among 40 output metric comparisons only 10 were less than 20 0 similar appendix c1 of these 10 five were associated with mean nearest neighbor mnn calculations and three with variation in residential utility vru in addition to similarity the spearman s rank correlation values ranged from strong to very strong positive correlation 0 60 0 97 between the implemented and replicate models for all aggregate results appendix d1 comparison of relationships between model outputs resulted in 31 of 56 comparisons 55 4 yielding identical spearman s rank correlation coefficients appendix e1 differences were minor for 17 comparisons 30 4 that had the same direction of correlation with only a slightly different strength of correlation differences where the direction of correlation differed only occurred with mnn which may be a function of different parameter settings in the software used to calculate spatial pattern fragstats mcgarigal et al 2012 that were not recorded the outcome of these comparisons demonstrate that relational equivalence was achieved between the implemented and replicate models at the aggregate level tests for distributional equivalence were less systematic and demonstrate that this level of replication could not be achieved of 40 t tests 33 82 5 showed significantly different results between the implemented and replicate aggregate model results appendix f1 3 5 2 agent category results segmenting the agent population into seven clusters where agent preferences were more similar to those within a cluster than between clusters yielded similar results to our aggregate outcomes table 3 outputs were highly similar with all 42 output metric comparisons being within 20 0 of each other appendix c2 the rank correlation between the implemented and replicate models was also very strong with positive correlation coefficient values ranging from 0 84 1 00 appendix d2 unlike the aggregate results the relationship comparison among the three categorical modelling outputs were much more variable appendix e2 however all comparisons had the same direction of correlation and six of twelve comparisons were within 0 1 and no comparison had a difference greater than 0 4 like the aggregate model results these suggest relational equivalence between the implemented and replicate model was achieved tests for distributional equivalence were strong for por whereby 13 of 14 t tests were significantly similar between the implemented and replicate models appendix f2 however this similarity was not present for mru and gini output comparisons therefore we conclude that distributional equivalence was not achieved at the categorical level table 3 results of proportion outside radius por of sprawl boundary mean residential utility mru and gini values for the implemented model replicate model and improved model agent populations are segmented into one of seven clusters means and standard deviations s d of 30 runs 3 6 step 6 conclusions and documentation the presented replication methodology was applied to a simple model of residential sprawl and assigned the outcome to one of the three levels of the replication standard axtell et al 1996 our replication effort achieved relational equivalence since similar relationships between the implemented and replicate models were observed but distributional equivalence could not be achieved through the replication process we identified two issues with the original research first there was a case 2 error whereby the equation used for calculating similarity among agents was incorrectly communicated in the conceptual model but correct in the implemented model second a case 3 error whereby the conceptual model described a process of normalizing agent preference weights that was not represented in the implemented model given discrepancies between the conceptual and implemented models we would not have been able to achieve any outcome of the replication standard without the original code while we were able to acquire the original input data experiment parameter files and source code among other files we were not able to obtain output data aggregated to report mean and standard deviations of different experiments in brown and robinson 2006 the lack of these data limited the types of statistical approaches that could be used to test for distributional equivalence 3 6 1 improved model having found case 2 and case 3 errors in our replication process we sought to create an improved model that rectified these errors and could be used to evaluate the credibility of the implemented model results we revisited steps 1 6 of the replication process and used the correct similarity equation and normalized the preference weights producing a new improved model as expected distributional equivalence could not be achieved at the aggregate 33 of 40 t tests 82 5 showed significantly different results or categorical level between the implemented and improved models appendix f however there was evidence of relational equivalence albeit it was weaker for the improved model than the replicate model at the aggregate level 19 of 40 output metric comparison were within the 20 similarity threshold appendix c1 spearman s rank correlation values ranged from 0 56 0 97 appendix d1 and comparison of relationships between model output resulted in 14 of 56 comparisons 25 having identical spearman s rank correlation coefficients appendix e1 differences were minor for 35 comparisons 62 5 which had the same direction of correlation with slightly different strength of correlation 0 4 similar levels of equivalence were acquired at the categorical level table 3 whereby 16 of 42 38 output metric comparisons were within 20 of each other appendix c2 rank correlation values were strong and positive 0 68 1 00 appendix d2 and comparison of relationships between model outputs all had the same direction with 8 of 12 comparisons within 0 1 and no comparisons had a difference greater than 0 3 appendix e2 these overarching results demonstrate that the improved model differed more from the implemented model than the replicate model however whether relational equivalence was achieved could be debated the two dominant results of the implemented model brown and robinson 2006 were 1 that introduction of variable agents results in more sprawl and agents were able to achieve higher and more uniform utility levels and 2 variability in preference weights had a much stronger influence on results than did categorization the first finding was achieved by the improved model whereby spatial measurements showed a decrease in clustering and increase in sprawl from the homogenous experiment to group means followed by the normal group normals and uniform experiments i e lpi mps decreased and ed and dor increased table 2 utility measurements followed suit with general increases in mru vru and gini across the same order of experiments the second finding was also upheld by the improved model whereby increases in sprawl and agent utility were greater between the homogeneous and normal experiments than between the homogeneous and group mean experiments while the presented research did not discredit the implemented model moving forward those interested in the some model should use the improved version a review of the 140 citations scopus june 17 2020 of brown and robinson 2006 identifies a variety of different reasons for citing brown and robinson 2006 yet not one of the citing articles identified either of the errors found through our replication process appendix g the findings of our replication effort and the results of reviewing the citing literature of brown and robinson 2006 highlight the challenges associated with peer review of models and their results as well as the need for model users to spend additional time and effort verifying all aspects of a model conceptual and implemented intended for use 3 7 replication documentation to assist others with similar replication efforts or to use the replicate and improved models we have published both models online through comses network at comses net zhang and robinson 2020 the protocol for publishing online with comses network is based on the fair data principles for access and reuse of models wilkinson et al 2016 to facilitate understanding and reuse of the replicate and improved models we document their contents following the overview design concepts and details odd protocol grimm et al 2006 grimm et al 2010 grimm et al 2020 comses puts constraints in place that require all aspects of the modelling process to be uploaded and reviewed by comses before a doi is assigned to the model submission these types of archives and protocols are needed to advance the science of abm rollins et al 2014 when publishers and funding agencies lack inclusion of these requirements the science community can facilitate this change by making it a de facto standard upon manuscript and grant review stodden 2010 which can over time become formalized 4 discussion we introduced and applied a replication methodology to reproduce a simple agent based model abm of residential location named some through this process we faced a number of challenges which included time hardware programming languages toolkits i e platforms and algorithms wilensky and rand 2007 among others although the implemented model was programmed using objective c and swarm libraries swarm org 2019 and could be reproduced in netlogo the language primitives and structure differ between the two modelling platforms for example netlogo uses its own language that is composed of higher level functions and is less verbose and flexible than objective c donkin et al 2017 railsback et al 2006 the different languages and platforms like others used for abm mason mason 2019 repast simphony repast 2019 mesa mesa readthedocs io 2019 often differ in how agents are represented and actions are scheduled railsback et al 2006 for example how agents are sorted shuffled and move bajracharya and duboz 2013 these differences can impede the replication process and make the achievement of numerical identity or distributional equivalence highly unlikely in addition to different model outcomes due to software choice differences in hardware e g processor size of random access memory and clock speed may also alter the performance of the same model wilensky and rand 2007 issues surrounding differences in floating point operations on hardware such as rounding continue to occur despite efforts for standardization e g ieee 754 1985 754 2008 iec 1989 while these issues often go unnoticed and most abm programmers modellers are not trained to determine if rounding errors are affecting their results kahan 2005 polhill et al 2005 show that rounding errors affect abm results despite arguments that measurement and data errors are to be greater than the effects of rounding errors the outcome of these differences in software and hardware create different initial conditions knight et al 2007 which both chaos theory and complexity science have shown can lead to significantly different outcomes epstein and axtell 1996 achieving numerical identity or distributional equivalence can be further impeded when models include stochastic processes axtell et al 1996 note that numerical identity is not expected to be achieved between any models that have stochastic elements pg 135 however computational stochasticity is based on random number generators and the stochasticity can be captured if the random seeds of model runs are recorded and the same random number generator used in replication kaplan 1981 in the absence of acquiring seeds and the same random number generator distributional equivalence may be achieved if the variability of model output is adequately captured however investigation of our inability to achieve distributional equivalence identified that the 30 model runs reported for the implemented model were insufficient to capture the variation in model output we found that a minimum of 100 model runs were required to capture the variability produced by the some model fig 2 appendix f therefore the mean and standard deviations provided by brown and robinson 2006 are from non representative samples of the population of model outcomes for each experiment and any alternative selection of 30 runs cannot be expected to be distributionally similar 4 1 motivation for replication model replication has not historically been viewed as valuable by the scientific community madden et al 1995 which is partly due to the mandate of funding agencies to explore new scientific frontiers rather than exploit and interrogate contemporary knowledge based on existing models a similar perspective has been the focus of journal publishers and only recently has value been placed on good science that strengthens our current understanding e g plos one or replication to achieve scientific consensus e g ais transactions on replication research however the process of model replication is very challenging especially when the goal is to replicate an entire original model legéndi et al 2013 lacking traditional incentives for replication grant funding or publishing replication can face scrutiny regardless of if it is unsuccessful or successful if the original model is proven un replicable the reason is typically inclined to be that the replication experiment is flawed rather than the original model easley et al 2000 even though laboratory study as normally conducted is often un replicable epstein 1980 when replication is successful the work can be dismissed as not creative easley et al 2000 or not being the first to make a novel scientific claim or discovery brown et al 2016 given the lack of rewards and acknowledgement of effort for replication both within an academic institution and by the broader scientific community most cannot afford the time to repeat previous research madden et al 1995 overcoming the lack of motivation for model replication could be critical to advancing the sciences using agent based modelling abm approaches through replication the abm community can create a suite of accredited models or model components for reuse bell et al 2015 that can prevent reinventing the wheel evans et al 2013 o sullivan et al 2015 accredited models could be applied to novel contexts be extended in new ways or be integrated into more complex models furthermore replication and accreditation could facilitate consolidation around several community models or frameworks like those found in climate e g collins et al 2006 and vegetation modelling e g lpj guess smith 2001 tang et al 2015 biome bgc hidy et al 2012 robinson et al 2013 community models have been shown to foster methodological assessments and advances e g trolle et al 2012 provide improved educational opportunities and create new prospects for coupled model intercomparison projects cmip n d e g taylor et al 2012 which in the abm context could focus on human decision making or coupling human natural systems robinson et al 2018 collectively these initiatives will help the sciences using abm to synthesize existing research into conceptual models or theories that can be exploited for scientific gains while also providing certainty to models and components that will allow for greater exploration march 1991 of new frontiers in methodology technology empirical data collection and model integration among other areas 4 2 lessons learned many of the lessons learned during the presented replication research can be found in existing literature for example the probability of a successful replication would be increased by improving documentation through the use of descriptive model protocols e g overview design concepts and details odd grimm et al 2006 2020 trace schmolke et al 2010 grimm et al 2014 which can aid conceptual interpretation of the model donkin et al 2017 and foster meta analyses of published models e g cipollina and salvatici 2010 similarly registering the model or its components with digital object identifiers dois can foster publishing of code and open access the benefit to the original modellers is recognition of their work and increased scientific impact our replication experience demonstrated that while time spent on understanding the conceptual model seemed to be the most important step access to the source code was critical to replication success however there is a trade off when immediately investigating existing source code that could bias the path of replicate model development there is a need to verify scientific results and develop new methods based on diverse perspectives that can only be achieved by starting anew furthermore there may be research needs that require the code to be updated or generated in a new platform only by working with both the conceptual model and the code were we able to identify errors replicate the original model results make improvements upon the original model and provide credibility to the original results despite these errors establishing communication with original model developers is important for replication success ambiguity in the communication of a model can lead to multiple interpretations and subsequently different trajectories of replication efforts e g wilensky and rand 2007 while model builders usually have the best understanding of their own models and can provide reasoning for modelling choices they may not be approachable or available for contact while model repositories e g comses net csdms model repository can increase model accessibility it may be useful to design and include inclusivity documentation alongside model archives or publications that state the openness of the model builders to questions bug reports applications and other model uses while bell et al 2015 found that all reviewed abm applications involved at least one of the individual developers as a coauthor replication and inclusivity documentation may foster expanded model use that would promote clusters of model applications in the absence of the original developers we classified the outcome of our replication within the replication standard axtell et al 1996 however there is a need for other standards to be developed and for modification of the replication standard further development of replication assessments like the replication standard would contribute to the development of a more formalized terminology and set of replication evaluation and testing processes that would make model accreditation through replication more transparent and comparable terminology and conceptual developments in ecological modelling e g augusiak et al 2014 could offer a guide in combination with descriptive protocols for comprehensive model evaluation validation and justifying model design choices e g trace schmolke et al 2010 grimm et al 2014 5 conclusion replication is a meaningful component of the research processes but is rarely conducted axelrod 1997 many scientists seek to create new models in an effort to advance science but few replicate their own existing models or those of others seagren 2015 in an effort to contribute to this gap we presented a methodology for replication that can be scrutinized and improved upon by others to guide and standardize model replication initiatives to demonstrate the application of the methodology we sought to replicate a simple agent based model abm of residential location results of our replication effort were able to achieve relational equivalence which is one of three levels of replication offered by the replication standard axtell et al 1996 the other two levels require statistical similarity distributional equivalence or identical results numerical identity through the replication process we uncovered errors in communication of the conceptual model and the code of the implemented model by brown and robinson 2006 we also discovered that brown and robinson did not capture the range of variation in model outputs which limited our ability to achieve distributional equivalence furthermore the presentation of aggregated results means and standard deviations limited the statistical methods that could be applied to assess distributional equivalence if the raw output data of original model were available a more rigorous comparison of the distributions of output values from the original and replicate models could have been conducted given these findings an improved model was created that produced outputs that were qualitatively supportive of the results of brown and robinson 2006 the presented case study offers a pedagogical example of model replication for others and highlights that peer reviewed publications and models are not always free of errors however both success and failure of replication are required to advance science success facilitates model accreditation and reliability garson 2009 while failure may breed novel insights and improvements to model structure methodology or data requirements in the future a systematic framework of complex model replication procedures should be composed and perhaps more importantly a culture of replication in the modelling sciences should be cultivated declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank undergraduate students duo zhang junzheng zhang and zixiang liang for their contributions to the initial phases of the presented replication process we would like to acknowledge with gratitude the funding provided by natural sciences and engineering research council nserc of canada 06252 2014 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105016 
25853,despite the critical role that replication plays in the advancement of science its presence in modelling literature is rare to encourage others to conduct replication and report success and challenges that facilitate or hinder replication we present the replication of an agent based model abm of residential sprawl using the replication standard replication results achieved relational equivalence through the replication process issues with the original research were identified and corrected in an improved model which qualitatively supported original results a specific challenge affecting alignment of original and replicate models included capturing model output variability and publishing all original output data for statistical analysis through the replication of agent based models additional confidence in model behaviour can be garnered and replicated abms can become accredited for reuse by others future research in the development and refinement of replication methodology and assessments should be cultivated along with a culture of value for replication efforts keywords model replication agent based modelling replication standard verification 1 introduction modelling is prevalent in almost every area of science ritchey 2012 from mathematics to psychology from engineering to geography and from archaeology to philosophy among these and other disciplines modelling provides four key functions 1 explanation of existing and potential phenomena 2 the ability to make predictions 3 increased decision making capacity and 4 a method to communicate knowledge schichl 2004 due to the usefulness of modelling scientists have spent a large amount of time on and their grant agencies on funding the construction testing and promotion of new models and enhancements to the modelling process each of these tasks are indispensable and time consuming however they fail to explicitly incorporate a critical requirement for scientific advancement which is replication replication is critical to scientific advancement easley et al 2000 latour and woolgar 1979 popper 1959 seagren 2015 as it is the only way to verify the authenticity and reliability of published research and corroborate challenges to current theory it has been argued that scientific results should not be viewed as real science if they cannot be reproduced by others cooper 2018 mckubre 2008 norton 2015 and that replication should be treated as the gold standard for scientific research casadevall and fang 2010 jasny et al 2011 despite substantial benefits arising from model replication why is it that replication has not been adequately recognized as a valuable part of theory development madden et al 1995 accreditation and verification of published results or been explicitly part of any call or requirement for academic funding an increasing number of scientists are coming to the realization that model replication is critical to the advancement of science donkin et al 2017 this realization is partly due to the high volume of models being produced that in many cases duplicate existing work evans et al 2013 the failure of some disciplines to coalesce on one or more community models which contrasts with the climate science and ecosystem modelling communities and the desire to reuse specific model components e g bell et al 2015 borrow bermejo et al 2017 and integrate models e g sturley et al 2018 and construct modelling tools e g tisue and wilensky 2004 to speed up model development and spend more time focusing on the advancement of science furthermore a successful model replication can prove that results are not an exceptional case wilensky and rand 2007 and contribute to theory development thiele and grimm 2015 the construction of a model is part of an iterative process rounsevell et al 2012 however the objective of replication is absent from any modelling process framework known to the authors furthermore a literature review of replication related research in six prominent journals with a focus on agent based modelling yielded poor results using keywords replicate and agent based model we retrieved 348 articles from the journal of artificial societies and social simulation 96 environmental modelling software 68 computers environment and urban systems 37 ecology and society 98 ecological economics 23 and journal of land use science 26 of these 348 articles only nine 2 6 replicated an existing model partially e g ronald et al 2016 or entirely e g donkin et al 2017 among the 9 articles that replicated an abm appendix a only five wilensky and rand 2007 janssen 2009 miodownik et al 2010 radax and rengs 2010 donkin et al 2017 incorporated replication as a primary objective and three journals yielded no abm replication research ecology and society ecological economics journal of land use science we sought to contribute to the gap in model replication literature a to provide a pedagogical example for graduate students that published model results in science need to be reproducible and that all published content requires scrutiny despite the peer review process b to update a 15 year old model that is well cited and remains scientifically relevant brown and robinson 2006 and c present an application of model replication that may be used to guide and encourage other model replication initiatives the process was also an effort to train undergraduate students and reflect on previous nascent modelling efforts that may have unknown flaws therefore the replication effort was also driven out of a desire to ensure model credibility given its longevity and use by others while most scientific replications are conducted by those not directly affiliated with the original experiments i e a third party in many cases third party individuals communicate with the original authors e g wilensky and rand 2007 the need to work with original abm creators is often the case even if a model is merely used for an alternative application bell et al 2015 the presented replication process was led by students guided by an original author of the model s use and collaborator in its development and demonstrates the challenges of replicating published model results even when detailed knowledge of the model and computer code are available through the replication process we sought to determine 1 what errors if any exist in the original model 2 what challenges and difficulties occur during the replication of the model and its associated published experiments and 3 what lessons were learned that may help others seeking to replicate a model 2 methods 2 1 replication process our replication methodology involved six steps understand the conceptual model collect resources verify model agreement replicate model construction model to model comparison conclusions and documentation 2 1 1 step 1 understand the conceptual model the purpose of a conceptual model is to formalize a high level understanding of the system being modelled wagner 2014 in doing so the creators imbed their biases perspectives experiences and knowledge of the system in a generalized format that could be applied to many situations or locations as such a conceptual model defines the scope of a system studied and the components of relevance to the type of research pursued robinson et al 2015 furthermore it can form the basis for other models that use different approaches to operationalize implement components of the conceptual model pennington 2008 ambrose 2015 hereafter we use the term conceptual model in a broad context to also include aspects of the written model formulation such as text equations figures and other model documentation excluding source code and software a successful replication of a model requires an understanding of its conceptual model as it can facilitate comprehension of existing model components their code and sequence and choice of code implementation understanding of the conceptual model becomes more imperative when the original code is not available and replicators must construct the model based solely on text and figures in a pool of equifinality given the likely unattainable goal of having full comprehension of the conceptual model and no uncertainty or questions about the conceptual model a number of methods could be used to assist in this understanding pseudo code could be generated that formalizes the behaviour of the conceptual model e g muller et al 2014 new visualizations created e g sequence diagram of potential activities e g li et al 2004 lin 2008 and descriptive protocols e g grimm et al 2006 2 1 2 step 2 collect resources while understanding the conceptual model should be based on text and figure documentation probability of replication success will be increased if the replicators can acquire as much of the original source code and data as possible radax and rengs 2010 donkin et al 2017 questioning and leveraging published science is one of the reasons why scientific publications include the contact information of authors teunis et al 2015 therefore replicators should contact original authors and attempt to acquire original programming packages comprising code random seed numbers input and output data and information about the hardware on which the models were run as differences in hardware have been shown to affect model outcomes e g wilensky and rand 2007 janssen 2009 2 1 3 step 3 verify model agreement the overarching goal of this step is to ensure agreement between the conceptual model and a published text equations and algorithms and b the implemented model fig 1 step 3 when model source code is not available the replicator should assess that the logic and outcomes of documented components align with existing literature and expert opinion when model source code is available specific code blocks corresponding to text equations and algorithms in published materials should be identified and assessed for their degree of alignment as well as correspondence to existing literature and expert opinion if the agreement between the conceptual and implemented models is satisfactorily verified then the process may move to step 4 otherwise an assessment about which of the two models conceptual or implemented is in err agreement between the conceptual and implemented models can reside in one of four cases case 1 the conceptual and implemented model are in agreement case 2 a flaw is discovered with the conceptual model that differs from the implemented model case 3 a flaw is discovered with the implemented model that differs from the conceptual model case 4 flaws are discovered with both the conceptual and implemented models if the implemented model code is available then the replication has a high probability of success since the primary goal of the replication process is to reproduce the results of the original model while the conceptual model represents the original developers perspective the replication process may uncover unintended mistakes in the communication of the conceptual model 2 1 4 step 4 replicate model construction when the source code of the implemented model is available it may be used to replicate published results e g izquierdo et al 2008 however the purpose of replication is typically for integration with other models or expansion to incorporate new components e g kacmar et al 1999 under these conditions new software and libraries are taken advantage of and a replicate model is constructed while a variety of platforms explicit to agent based modelling exist e g mason repast netlogo salgado and gilbert 2013 other platforms have also proved successful e g r statistics package thiele 2014 matlab macal and howe 2005 2 1 5 step 5 model to model comparison one approach to conducting a comparison between an original model and a replicate model is to use the replication standard rs axtell et al 1996 the rs consists of three potential outcomes 1 relational equivalence requires the replicated model to present similar relationships between input and output variables as the original model 2 distributional equivalence requires the results derived from two models to be statistically indistinguishable to each other and 3 numerical identity requires that the replicated model has the same numerical results with the original model which is the most difficult outcome to achieve it may not be possible to achieve numerical identity due to platform of replication loss or degradation of data and information over time or other factors therefore the replication effort should seek a systematic approach starting with relational equivalence and work toward numerical identity 2 1 6 step 6 conclusions and documentation after the entire replication process no matter whether the expected replicated results are acquired there must be some valuable findings to be concluded a successful replication can evaluate the overall behaviours the reliability and reproducibility of the original model a failure also provides lessons which can be the most valuable outcome of the replication process 3 replication case study 3 1 step 1 understand the conceptual model we apply the aforementioned methodology to replicate a simple agent based model of residential sprawl sluce s original model for exploration some the some model was designed to understand the effects of trade offs among preferences for aesthetic quality nearness to service centers and neighbourhood similarity on spatial patterns of residential development in a hypothetical landscape brown and robinson 2006 empirically informed agents using household survey data from south eastern michigan fernandez et al 2005 marans 2003 to evaluate how categorization of agents binning agents into groups affected spatial patterns of residential development relative to continuous variation in agent site location preferences as part of our efforts to understand the conceptual model literature related to the model were reviewed brown et al 2004 brown et al 2005 robinson and brown 2009 including project proposals nsf n d and other documents originally posted online in addition to one of the coauthors others related to the original model development and its application were contacted to clarify understanding and interpretation 3 2 step 2 collect resources a variety of resources were collected to facilitate the replication process prior to a decommissioning of the sluce project website in 2019 source code in swarm objective c http swarm org 2019 for the original implemented model was acquired 43 separate files in addition control files specifying the parameter settings for five published computational experiments were acquired three files describing instructions for visualizing results the input parameters and output variables as well as a map of values used to represent the landscape raw output data were unattainable and only results reported in brown and robinson 2006 were available 3 3 step 3 verify model agreement through the model verification process we discovered two errors the first was a case 2 error whereby the conceptual model was incorrectly communicated but correctly implemented specifically a function used to calculate the similarity in an agent s preferences to those of its neighbours differed between the conceptual and implemented models table 1 equation 1 corresponding to the conceptual model brown and robinson 2006 allows the value of neighbourhood similarity i e γ 3 r x y to range from 1 3 to infinity while equation 2 is used in the implemented model and constrains the range to between 0 and 1 table 1 neighbourhood similarity equations where n is the number of agents occupying neighbouring locations α1r α2r α3r represent the preference weights for distance to service areas landscape aesthetic and neighbourhood similarity for agent r and j represents the values for the neighbourhood to which r is making the comparison a second error identified in our verification of model agreement was a case 3 error whereby the conceptual model was incorrectly represented in the implemented model in the conceptual model brown and robinson 2006 state every agent must have a preference weight for each component i e αir and the preference weights across the three components were constrained to sum to one however the implemented model did not yield any code that rescaled preference weights to sum to one 3 4 step 4 replicate model construction we created our replicate model in netlogo which has a large user community is maintained and regularly updated is relatively simple to use and share among other researchers and has been integrated with r thiele et al 2012 thiele 2014 python jaxa rozen and kwakkel 2018 and mathematica bakshy and wilensky 2007 among other scientific data analysis tools we initially attempted to construct the replicate model based on only the conceptual model outlined by brown and robinson 2006 to ensure the replicate code was independent from the original code thiele and grimm 2015 however there was insufficient conceptual model documentation requiring us to iteratively revisit the implemented model code and revise our replicate model 3 5 step 5 model to model comparison by constructing the replicate model to create the same spatial and aspatial i e utility outputs as the implemented model we were able to directly compare the two models along the gradient of matching provided by the replication standard to mimic the procedures of the original publication we ran the same five computational experiments using the same parameter settings and landscapes whereby each experiment was run 30 times to produce an average and standard deviation of output metrics results are presented as brown and robinson 2006 in aggregate table 2 and by agent category table 3 table 2 spatial pattern and utility metrics of five experimental preference weight settings for the implemented model replicate model and improved model means and standard deviations s d of 30 runs since the implemented and replicate models include stochastic processes the replication standard of numerical identity cannot be achieved therefore we sought to establish relational equivalence and then test for distributional equivalence different approaches can be used to determine relational equivalence e g regression kleijnen 1998 however with access only to the aggregate output data of the implemented model means and standard deviations we used 1 percent similarity between model results to qualitatively assess relational equivalence using a threshold of 20 0 2 spearman s rank correlation coefficient to assess if output metric values were similarly ordered across our experiments for both models and 3 if within model outputs have the same relationships between models this latter method was achieved by calculating the spearman s rank correlation coefficient between outputs for a model and then comparing the resultant coefficients across models we did not to report p values which were highly variable as they are unreliable given our small sample sizes of five and seven for aggregate and categorical results respectively halsey et al 2015 prior to comparing our model results for distributional equivalence we tested for normality using the shapiro wilk test ghasemi and zahediasl 2012 seven of forty tests 17 5 among aggregate metrics and one of forty two tests 2 0 among categorical result metrics showed non normal metric distributions appendix b given the dominance of normality among our results and our limitation to mean and standard deviations from the original publication we used a student s t test to determine if results from our replicate model were statistically significantly similar α 0 05 to the implemented model 3 5 1 agent aggregated results replicate and implemented model outcomes under the aggregate population context i e no agent categories showed a high level of similarity table 2 among 40 output metric comparisons only 10 were less than 20 0 similar appendix c1 of these 10 five were associated with mean nearest neighbor mnn calculations and three with variation in residential utility vru in addition to similarity the spearman s rank correlation values ranged from strong to very strong positive correlation 0 60 0 97 between the implemented and replicate models for all aggregate results appendix d1 comparison of relationships between model outputs resulted in 31 of 56 comparisons 55 4 yielding identical spearman s rank correlation coefficients appendix e1 differences were minor for 17 comparisons 30 4 that had the same direction of correlation with only a slightly different strength of correlation differences where the direction of correlation differed only occurred with mnn which may be a function of different parameter settings in the software used to calculate spatial pattern fragstats mcgarigal et al 2012 that were not recorded the outcome of these comparisons demonstrate that relational equivalence was achieved between the implemented and replicate models at the aggregate level tests for distributional equivalence were less systematic and demonstrate that this level of replication could not be achieved of 40 t tests 33 82 5 showed significantly different results between the implemented and replicate aggregate model results appendix f1 3 5 2 agent category results segmenting the agent population into seven clusters where agent preferences were more similar to those within a cluster than between clusters yielded similar results to our aggregate outcomes table 3 outputs were highly similar with all 42 output metric comparisons being within 20 0 of each other appendix c2 the rank correlation between the implemented and replicate models was also very strong with positive correlation coefficient values ranging from 0 84 1 00 appendix d2 unlike the aggregate results the relationship comparison among the three categorical modelling outputs were much more variable appendix e2 however all comparisons had the same direction of correlation and six of twelve comparisons were within 0 1 and no comparison had a difference greater than 0 4 like the aggregate model results these suggest relational equivalence between the implemented and replicate model was achieved tests for distributional equivalence were strong for por whereby 13 of 14 t tests were significantly similar between the implemented and replicate models appendix f2 however this similarity was not present for mru and gini output comparisons therefore we conclude that distributional equivalence was not achieved at the categorical level table 3 results of proportion outside radius por of sprawl boundary mean residential utility mru and gini values for the implemented model replicate model and improved model agent populations are segmented into one of seven clusters means and standard deviations s d of 30 runs 3 6 step 6 conclusions and documentation the presented replication methodology was applied to a simple model of residential sprawl and assigned the outcome to one of the three levels of the replication standard axtell et al 1996 our replication effort achieved relational equivalence since similar relationships between the implemented and replicate models were observed but distributional equivalence could not be achieved through the replication process we identified two issues with the original research first there was a case 2 error whereby the equation used for calculating similarity among agents was incorrectly communicated in the conceptual model but correct in the implemented model second a case 3 error whereby the conceptual model described a process of normalizing agent preference weights that was not represented in the implemented model given discrepancies between the conceptual and implemented models we would not have been able to achieve any outcome of the replication standard without the original code while we were able to acquire the original input data experiment parameter files and source code among other files we were not able to obtain output data aggregated to report mean and standard deviations of different experiments in brown and robinson 2006 the lack of these data limited the types of statistical approaches that could be used to test for distributional equivalence 3 6 1 improved model having found case 2 and case 3 errors in our replication process we sought to create an improved model that rectified these errors and could be used to evaluate the credibility of the implemented model results we revisited steps 1 6 of the replication process and used the correct similarity equation and normalized the preference weights producing a new improved model as expected distributional equivalence could not be achieved at the aggregate 33 of 40 t tests 82 5 showed significantly different results or categorical level between the implemented and improved models appendix f however there was evidence of relational equivalence albeit it was weaker for the improved model than the replicate model at the aggregate level 19 of 40 output metric comparison were within the 20 similarity threshold appendix c1 spearman s rank correlation values ranged from 0 56 0 97 appendix d1 and comparison of relationships between model output resulted in 14 of 56 comparisons 25 having identical spearman s rank correlation coefficients appendix e1 differences were minor for 35 comparisons 62 5 which had the same direction of correlation with slightly different strength of correlation 0 4 similar levels of equivalence were acquired at the categorical level table 3 whereby 16 of 42 38 output metric comparisons were within 20 of each other appendix c2 rank correlation values were strong and positive 0 68 1 00 appendix d2 and comparison of relationships between model outputs all had the same direction with 8 of 12 comparisons within 0 1 and no comparisons had a difference greater than 0 3 appendix e2 these overarching results demonstrate that the improved model differed more from the implemented model than the replicate model however whether relational equivalence was achieved could be debated the two dominant results of the implemented model brown and robinson 2006 were 1 that introduction of variable agents results in more sprawl and agents were able to achieve higher and more uniform utility levels and 2 variability in preference weights had a much stronger influence on results than did categorization the first finding was achieved by the improved model whereby spatial measurements showed a decrease in clustering and increase in sprawl from the homogenous experiment to group means followed by the normal group normals and uniform experiments i e lpi mps decreased and ed and dor increased table 2 utility measurements followed suit with general increases in mru vru and gini across the same order of experiments the second finding was also upheld by the improved model whereby increases in sprawl and agent utility were greater between the homogeneous and normal experiments than between the homogeneous and group mean experiments while the presented research did not discredit the implemented model moving forward those interested in the some model should use the improved version a review of the 140 citations scopus june 17 2020 of brown and robinson 2006 identifies a variety of different reasons for citing brown and robinson 2006 yet not one of the citing articles identified either of the errors found through our replication process appendix g the findings of our replication effort and the results of reviewing the citing literature of brown and robinson 2006 highlight the challenges associated with peer review of models and their results as well as the need for model users to spend additional time and effort verifying all aspects of a model conceptual and implemented intended for use 3 7 replication documentation to assist others with similar replication efforts or to use the replicate and improved models we have published both models online through comses network at comses net zhang and robinson 2020 the protocol for publishing online with comses network is based on the fair data principles for access and reuse of models wilkinson et al 2016 to facilitate understanding and reuse of the replicate and improved models we document their contents following the overview design concepts and details odd protocol grimm et al 2006 grimm et al 2010 grimm et al 2020 comses puts constraints in place that require all aspects of the modelling process to be uploaded and reviewed by comses before a doi is assigned to the model submission these types of archives and protocols are needed to advance the science of abm rollins et al 2014 when publishers and funding agencies lack inclusion of these requirements the science community can facilitate this change by making it a de facto standard upon manuscript and grant review stodden 2010 which can over time become formalized 4 discussion we introduced and applied a replication methodology to reproduce a simple agent based model abm of residential location named some through this process we faced a number of challenges which included time hardware programming languages toolkits i e platforms and algorithms wilensky and rand 2007 among others although the implemented model was programmed using objective c and swarm libraries swarm org 2019 and could be reproduced in netlogo the language primitives and structure differ between the two modelling platforms for example netlogo uses its own language that is composed of higher level functions and is less verbose and flexible than objective c donkin et al 2017 railsback et al 2006 the different languages and platforms like others used for abm mason mason 2019 repast simphony repast 2019 mesa mesa readthedocs io 2019 often differ in how agents are represented and actions are scheduled railsback et al 2006 for example how agents are sorted shuffled and move bajracharya and duboz 2013 these differences can impede the replication process and make the achievement of numerical identity or distributional equivalence highly unlikely in addition to different model outcomes due to software choice differences in hardware e g processor size of random access memory and clock speed may also alter the performance of the same model wilensky and rand 2007 issues surrounding differences in floating point operations on hardware such as rounding continue to occur despite efforts for standardization e g ieee 754 1985 754 2008 iec 1989 while these issues often go unnoticed and most abm programmers modellers are not trained to determine if rounding errors are affecting their results kahan 2005 polhill et al 2005 show that rounding errors affect abm results despite arguments that measurement and data errors are to be greater than the effects of rounding errors the outcome of these differences in software and hardware create different initial conditions knight et al 2007 which both chaos theory and complexity science have shown can lead to significantly different outcomes epstein and axtell 1996 achieving numerical identity or distributional equivalence can be further impeded when models include stochastic processes axtell et al 1996 note that numerical identity is not expected to be achieved between any models that have stochastic elements pg 135 however computational stochasticity is based on random number generators and the stochasticity can be captured if the random seeds of model runs are recorded and the same random number generator used in replication kaplan 1981 in the absence of acquiring seeds and the same random number generator distributional equivalence may be achieved if the variability of model output is adequately captured however investigation of our inability to achieve distributional equivalence identified that the 30 model runs reported for the implemented model were insufficient to capture the variation in model output we found that a minimum of 100 model runs were required to capture the variability produced by the some model fig 2 appendix f therefore the mean and standard deviations provided by brown and robinson 2006 are from non representative samples of the population of model outcomes for each experiment and any alternative selection of 30 runs cannot be expected to be distributionally similar 4 1 motivation for replication model replication has not historically been viewed as valuable by the scientific community madden et al 1995 which is partly due to the mandate of funding agencies to explore new scientific frontiers rather than exploit and interrogate contemporary knowledge based on existing models a similar perspective has been the focus of journal publishers and only recently has value been placed on good science that strengthens our current understanding e g plos one or replication to achieve scientific consensus e g ais transactions on replication research however the process of model replication is very challenging especially when the goal is to replicate an entire original model legéndi et al 2013 lacking traditional incentives for replication grant funding or publishing replication can face scrutiny regardless of if it is unsuccessful or successful if the original model is proven un replicable the reason is typically inclined to be that the replication experiment is flawed rather than the original model easley et al 2000 even though laboratory study as normally conducted is often un replicable epstein 1980 when replication is successful the work can be dismissed as not creative easley et al 2000 or not being the first to make a novel scientific claim or discovery brown et al 2016 given the lack of rewards and acknowledgement of effort for replication both within an academic institution and by the broader scientific community most cannot afford the time to repeat previous research madden et al 1995 overcoming the lack of motivation for model replication could be critical to advancing the sciences using agent based modelling abm approaches through replication the abm community can create a suite of accredited models or model components for reuse bell et al 2015 that can prevent reinventing the wheel evans et al 2013 o sullivan et al 2015 accredited models could be applied to novel contexts be extended in new ways or be integrated into more complex models furthermore replication and accreditation could facilitate consolidation around several community models or frameworks like those found in climate e g collins et al 2006 and vegetation modelling e g lpj guess smith 2001 tang et al 2015 biome bgc hidy et al 2012 robinson et al 2013 community models have been shown to foster methodological assessments and advances e g trolle et al 2012 provide improved educational opportunities and create new prospects for coupled model intercomparison projects cmip n d e g taylor et al 2012 which in the abm context could focus on human decision making or coupling human natural systems robinson et al 2018 collectively these initiatives will help the sciences using abm to synthesize existing research into conceptual models or theories that can be exploited for scientific gains while also providing certainty to models and components that will allow for greater exploration march 1991 of new frontiers in methodology technology empirical data collection and model integration among other areas 4 2 lessons learned many of the lessons learned during the presented replication research can be found in existing literature for example the probability of a successful replication would be increased by improving documentation through the use of descriptive model protocols e g overview design concepts and details odd grimm et al 2006 2020 trace schmolke et al 2010 grimm et al 2014 which can aid conceptual interpretation of the model donkin et al 2017 and foster meta analyses of published models e g cipollina and salvatici 2010 similarly registering the model or its components with digital object identifiers dois can foster publishing of code and open access the benefit to the original modellers is recognition of their work and increased scientific impact our replication experience demonstrated that while time spent on understanding the conceptual model seemed to be the most important step access to the source code was critical to replication success however there is a trade off when immediately investigating existing source code that could bias the path of replicate model development there is a need to verify scientific results and develop new methods based on diverse perspectives that can only be achieved by starting anew furthermore there may be research needs that require the code to be updated or generated in a new platform only by working with both the conceptual model and the code were we able to identify errors replicate the original model results make improvements upon the original model and provide credibility to the original results despite these errors establishing communication with original model developers is important for replication success ambiguity in the communication of a model can lead to multiple interpretations and subsequently different trajectories of replication efforts e g wilensky and rand 2007 while model builders usually have the best understanding of their own models and can provide reasoning for modelling choices they may not be approachable or available for contact while model repositories e g comses net csdms model repository can increase model accessibility it may be useful to design and include inclusivity documentation alongside model archives or publications that state the openness of the model builders to questions bug reports applications and other model uses while bell et al 2015 found that all reviewed abm applications involved at least one of the individual developers as a coauthor replication and inclusivity documentation may foster expanded model use that would promote clusters of model applications in the absence of the original developers we classified the outcome of our replication within the replication standard axtell et al 1996 however there is a need for other standards to be developed and for modification of the replication standard further development of replication assessments like the replication standard would contribute to the development of a more formalized terminology and set of replication evaluation and testing processes that would make model accreditation through replication more transparent and comparable terminology and conceptual developments in ecological modelling e g augusiak et al 2014 could offer a guide in combination with descriptive protocols for comprehensive model evaluation validation and justifying model design choices e g trace schmolke et al 2010 grimm et al 2014 5 conclusion replication is a meaningful component of the research processes but is rarely conducted axelrod 1997 many scientists seek to create new models in an effort to advance science but few replicate their own existing models or those of others seagren 2015 in an effort to contribute to this gap we presented a methodology for replication that can be scrutinized and improved upon by others to guide and standardize model replication initiatives to demonstrate the application of the methodology we sought to replicate a simple agent based model abm of residential location results of our replication effort were able to achieve relational equivalence which is one of three levels of replication offered by the replication standard axtell et al 1996 the other two levels require statistical similarity distributional equivalence or identical results numerical identity through the replication process we uncovered errors in communication of the conceptual model and the code of the implemented model by brown and robinson 2006 we also discovered that brown and robinson did not capture the range of variation in model outputs which limited our ability to achieve distributional equivalence furthermore the presentation of aggregated results means and standard deviations limited the statistical methods that could be applied to assess distributional equivalence if the raw output data of original model were available a more rigorous comparison of the distributions of output values from the original and replicate models could have been conducted given these findings an improved model was created that produced outputs that were qualitatively supportive of the results of brown and robinson 2006 the presented case study offers a pedagogical example of model replication for others and highlights that peer reviewed publications and models are not always free of errors however both success and failure of replication are required to advance science success facilitates model accreditation and reliability garson 2009 while failure may breed novel insights and improvements to model structure methodology or data requirements in the future a systematic framework of complex model replication procedures should be composed and perhaps more importantly a culture of replication in the modelling sciences should be cultivated declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank undergraduate students duo zhang junzheng zhang and zixiang liang for their contributions to the initial phases of the presented replication process we would like to acknowledge with gratitude the funding provided by natural sciences and engineering research council nserc of canada 06252 2014 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105016 
25854,synthesizing and communicating knowledge on climate change to policymakers and stakeholders is often difficult due to the complexity and diversity of underlying research as a translation instrument we present the mipplot tool an open source r package that can be used to visualize data of long term climate mitigation scenarios with simple commands and under a user friendly language platform this tool generates plots in multiple languages for any scenario dataset following the data submission format of the integrated assessment modeling consortium the plots can be generated both by function call in r environment and by an interactive interface in a web browser environment which offers greater flexibility of inputs and suits both experts and non experts moreover its capability of specifying aggregation rules and different display languages extends its applicability to a broader range of users keywords climate change mitigation visualization scenarios r package 1 introduction solutions to global environmental problems should be supported by policy analysis and scientific knowledge that are credible legitimate and salient cash et al 2003 or inclusive rigorous transparent and accessible donelly et al 2018 however synthesizing such knowledge to policymakers and stakeholders is often difficult due to inefficiencies in communication caused by the complexity and diversity of the underlying research there has been some progress regarding this issue on climate change particularly with regard to data standardization and visualization addressing climate change requires considerable reduction in greenhouse gas ghg emissions from human activities mainly from fossil fuel combustion and land use change such as the conversion of forests into agricultural land in this context future challenges for the reduction of ghg emissions and the corresponding effects on the economy society and environment are commonly assessed through future scenarios produced by integrated assessment models iams which are not without problems anderson and jewell 2019 iams have been criticized for being technocratic but still an indispensable tool to explore the reach of climate targets and to understand the related challenges the results from iams including the identification of drivers of socio economic development final energy consumption and emissions provide crucial information for policymakers to understand the possible outcomes of alternative development and climate policy scenarios for instance the shared socio economic pathways ssps which is a set of five representative global scenarios developed by the iam research community riahi et al 2017 have been widely used to assess alternative futures throughout the 21st century considering socio economic aspects such as population economic growth and energy use and emissions o neill et al 2020 ssps are used along with a set of scenarios of global ghg concentrations known as the representative concentration pathways rcps van vuuren et al 2011 to characterize emission pathways in terms of a level of global warming data from numerous climate mitigation scenarios generated by iams have become publicly available to ensure transparency and facilitate analyses the most notable example is the release of the databases by the intergovernmental panel on climate change ipcc regarding its 5th assessment report ar5 ipcc 2014 and more recently regarding the special report on 1 5 c global warming sr1 5 ipcc 2018 which presents information from several model intercomparison projects mips the effective dissemination and communication of this information from researchers can potentially provide integrated research insights and influence climate policies furthermore the increased availability of models and their results to non modelers can increase the reliability of policy focused research pfenninger 2017 the complexity and large amount of information from climate mitigation studies hinder a straightforward communication of main insights to policymakers unless there is a translation channel usually researchers with expertise in climate mitigation scenarios directly engage policymakers and disclose relevant information from iam analyses to them an alternative is to have the expert be part of the policy making body so that the policymaking side can handle the information without the support of researchers however it would be ideal if information was delivered in a format or media that allows policymakers to directly and flexibly manage process and communicate the results there is inconclusive discussion and dialogs among researchers policymakers and stakeholders due to the lack of translation of information which provides transparent standardization intercomparison and visualization of research results this challenge of developing such a translation tool has attracted research interest and has been investigated by many research groups for example in working group i wgi of the ipcc the earth system model evaluation tool esmvaltool has been developed to improve diagnosis and understanding of outcomes from earth system models eyring et al 2016a b under the coupled model intercomparison project cmip another example is the panoply which is a tool for visualization and analysis of geo referenced data from climate models nasa 2017 contrary to the esmvaltool which was targeted at climate modeling researchers panoply was designed for non researchers these data visualization tools enable policymakers and stakeholders to select or even generate specific data of their concern and visualize such data in a format that allows them to obtain possible insights from climate mitigation scenarios by themselves more specifically in the field of iam scenario results we reviewed some of the existing scenario data plotting tools previous efforts on such scenario data processing and visualization are shown in table 1 the tools shown in table 1 are described by type browser python r based access usage and dataset applicability in addition to the aforementioned tools the project senses also developed a web embedded interactive framework to increase the accessibility of iam scenario outcomes to non experts project senses 2011 in the japan 2050 low carbon navigator project project low carbon navigator 2014 a user friendly web based tool was also developed to display the simulation results for japan it was aimed at facilitating multi stakeholder discussions and educating university students and the general public on the mechanisms through which japan can achieve a low carbon pathway in the near future based on the current status of scenario data plot tools promoting the application of iam scenario results requires tools that i are open source to provide users especially those from developing regions full access to the data and functions using a free and user friendly language platform ii present function calls and interactive interfaces to ensure that experts scientists and educators and non expert users stakeholders and policymakers can plot data of their own concerns and iii provide multi language outputs to ensure that research results from iam teams in different countries can be easily accessed by local policymakers and stakeholders moreover it is essential to compile and display data from multiple models in a consistent format to enhance the effective dissemination of model insights to policymakers and academics involved in climate policy motivated by the aforementioned academic political and educational requirements we developed the mipplot package this tool was developed to serve a two fold purpose the gathering of iam scenario data into a consistent dataset and provide a straightforward visualization of key data from multiple models the tool is open source flexible can be modified by users adaptable can be applied to scenario data from different sources and capable of displaying plots from large scenario datasets from multiple scenarios and models in multiple languages the initial audience of the mipplot tool are researchers and students interested in climate change mitigation scenarios expected to be able to handle r packages via function calls the tool s design also aims to reach stakeholders without technical expertise on model or data manipulation by including an interactive interface compared to other tools mipplot is the first r package for visualization of scenario results that enhances the flexible interactive usability by function calls including multi language outputs 2 methodology the mipplot was developed as a software package for r statistical programming r core team 2015 which is an open source software platform users can download the mipplot package free of cost utokyo mip 2018 from the code sharing platform github once the tool is accessed and installed in the library it can be used and developed without any restrictions users can check the complete code and make modifications to suit their own needs 2 1 outline the inputs functions and outputs of mipplot are shown in fig 1 inputs of this package require the scenario data following the standard data submission format of the integrated assessment modeling consortium iamc and the table of additivity rules rule table the package contains several functions that can be individually applied after installing the library these functions can clarify missing variables implement conversion to standard formats and check the additivity of data inputs which can be a powerful tool for users to organize model comparison exercises each time they check the data submitted by a modeling team any inconsistency in the additivity of the disaggregated variables to their corresponding totals can be easily verified the mipplot can generate visual diagnostic plots of major variables under different scenarios such as primary energy supply co2 emissions and carbon prices which are the main functions of this package the generation can be executed using function calls or interactive interfaces the interactive function can greatly accelerate the generation of a set of plots for comparison outputs of this package include the visual diagnostic plots together with its full codes and the results of data consistency 2 2 inputs the package requires two types of inputs the scenario dataset and its additivity rule table a sample of the scenario data following iamc data format is shown in table 2 it contains model scenario region variable unit and the value of each year it is worth noting the naming of variable to name a variable the iamc data format uses a vertical bar to explain the inclusion relationship for example primary energy is used for primary energy data and primary energy biomass is used for primary energy supplied by biomass energy carriers we developed the additivity check function following such naming feature to easily group variable items and check additivity of the data as an input of this function we created the additivity rule table a sample of the additivity rule table is shown in table 3 it contains three items left side right side and color code the sum of variables from the right side under the same aggregation rule should equal the sum of variables from the left side the sample rule tables for the ar5 and sr1 5 datasets are provided in the package by default the color code indicates the corresponding color of the right side variables following the same processes users can create new rule tables to meet customized requirements 2 3 functions data input and format conversion functions are listed as follows mipplot read iamc r mipplot read ruletab r the mipplot read iamc function transforms raw data in iamc format into a format that suits the plotting functions in the r programming language the mipplot read ruletab function reads the table of additivity rule and defines the rule id i e see rt in the application example in section 3 1 these functions are required for the generation of figures in mipplot and they can be used for the design of customized plots the default color codes in the rule table follow the ipcc color palettes nevertheless it is possible to set customized color codes as the mipplot is a completely open source package data plot functions are listed as follows mipplot line mipplot bar mipplot area mipplot point these plot functions cover the main figure forms such as line charts stacked bar charts stacked area charts and point charts developed based on the ggplot2 package wickham 2016 we enhanced the usability of scenario data which was originally in a format that could not be used by r plotting functions thereby allowing the comparison of the main interests scenarios models regions variables and periods of stakeholders by assigning figure types to time series disaggregated variables considering the graphic grammar wickman 2010 the mipplot line is applicable to the time series data of a single variable and it depicts the long term changes of specific variables that users are concerned about as well as the variety among models or scenarios the visualization is enhanced by discriminating data according to the type of line and points the mipplot bar is applicable to disaggregated variables of a single period and it shows the structure of chosen aggregated variables as well as the variety among models or scenarios this function follows the additivity rule defined by mipplot read ruletab the mipplot area is applicable to the time series data of disaggregated variables and it shows the long term changes in the structures of specific aggregated variables that users are concerned about this function follows the additivity rule defined by mipplot read ruletab the mipplot point is applicable to a single variable in a single period and it shows the variation among models or scenarios of a specific variable that users are concerned about the default type of the plot output file is pdf nevertheless all file types of plot outputs supported by the r language can be used as the output file type of mipplot the data consistency check functions are listed as follows mipplot additivity check this function checks the additivity of disaggregated variables to their corresponding totals and it generates error sets of disaggregated variables and their corresponding plots this function is a powerful tool for users who organize model comparison exercises whenever they check the data submitted by a modeling team all possible inconsistencies in additivity can be easily verified it is also possible for users to set their error ranges e g 1 5 the data interactive plot functions are listed as follows mipplot interactive line mipplot interactive bar mipplot interactive area these functions provide an interactive interface based on the r package shiny of the plot functions the data scope including scenarios models regions variables and periods can be selected and directly adjusted through the interface the visualization was also improved by limiting the number of scenario options no more than 15 scenarios in one plot and by setting rotatable text angles on the x axis all function calls executed in the r environment return ggplot2 objects which can be further modified by ggplot2 functions along with each plot the interactive function also provides the entire code this enables non expert users such as students to access the research outcomes from iam climate mitigation scenarios without technical barriers a website browser beta version of the interactive interface is under development mipplot 0 2 0 beta version the website browser version of the interactive functions mentioned above was published in a rental server and it can be accessed by a url link a limited number of accounts were provided by the developers to ensure that specific stakeholders policymakers who do not use r programming software can simultaneously access the mip scenario results in the website and check the plots via the website interactive interface 2 4 multi language outputs the developed tool can display outputs in several languages which is normally limited in other scenario data plot tools both plot and interactive functions allow users to choose the language preferences of the output data this can be done by specifying an additional argument in any of the plotting functions the ipcc reports are translated into six official un languages although the database website is provided only in english the current version mipplot v0 2 0 supports english japanese chinese simplified chinese traditional and spanish with the standard iamc format data input in english mipplot translates all headers e g model scenario period etc in all languages regions e g world oecd asia etc in all languages and variables e g emissions co2 carbon capture and storage industry primary energy wind etc in all languages in the plots the availability of plots in different languages facilitates the communication of iam research outcomes to a wider audience scope particularly to local stakeholders and students from different countries 3 application example with sr1 5 scenario data the scenario data from the ipcc special report on global warming of 1 5 c ipcc 2018 was used as an application case of mipplot v0 2 0 3 1 example of plots four plots namely a line chart a stacked bar chart a stacked area chart and a point chart were generated by the four plot functions as shown in fig 2 in mipplot v0 2 0 the commands to generate the plots are simple image 8 as long as users are aware of the element names in the input dataset scenario model region variable they can specify any element or group of elements in a single command users can also specify multiple elements e g a list of regions and or variables to generate a list of plots 3 2 example of interactive function the interactive interface by function call is shown in fig 3 the command for calling the interactive function interfaces is also simple image 9 the interactive interface on website browser beta version is shown in fig 4 this beta version can be accessed by url link with limited accounts provided by developers currently the interactive functions area line and bar plots are selected first followed by the same interactive interface as the function call which is executed in the r environment the scenario results can be plotted directly on the website without the need for additional data input result data or rule table 3 3 example of multi language the example of multi language outputs are shown in fig 5 this function can be applied by specifying an additional argument in any of the plotting functions as follows image 10 it can also be selected in the interactive interface 4 discussion the r package mipplot can plot figures through function call or interactive interface based on data from long term climate mitigation scenarios this package provides full access to users and can be downloaded free of cost the outputs can be shown in multiple languages to ensure that the research results from iam teams in different countries can be accessed by more local policy makers and stakeholders however current functions should be improved to better serve the needs of both experts and non experts for experts the current data consistency checks functions can be extended except for the additivity checks a function of scope can be added to check if the value of one variable or is outside an expected range e g per capita energy consumption below maximum historical values how different iam teams define the same variable by mipplot guidance e g whether the variable final energy industry includes non energy use or not whether final energy transportation includes international bunkers or not etc should be clarified also if mipplot also provides the information of the differences between how local statistics and iea ipcc statistics define specific variables local policymakers may be more willing to take the results from international iam teams as reference the visualization of a large set of scenarios and models could be simplified by grouping data into categories e g scenarios by climate target models by approach and displaying the uncertainty ranges for each of those categories for non experts the rules of additivity should be explained in detail there can be multiple ways to disaggregate variables which is not evident in the naming of iamc format variables for example final energy can be aggregated by source e g final energy coal final energy solar and final energy others or sector e g final energy industry and final energy buildings such multiple additivity rules should be explained to non experts the messages that can be derived from a selected set of scenarios should be explained in detail the application of the tool requires some basic knowledge from non experts researchers and non researchers unfamiliar with the concepts of long term scenarios of climate change mitigation which includes the range and name of items included in the scenario datasets models scenarios regions and variables and the definition of major variables e g emissions treated by gas and emitting sectors primary and final energy supply and demand sectors and aggregation rules for variables e g discrimination between emissions or energy disaggregated by economic sectors or fuels we plan to add indicator functions and display them in the interactive interface for example indicators of renewable energy share share of all renewable energy sources to total energy supply industrial electrification rate share of electricity consumption to total industrial final energy and decarbonization degree ratio of carbon emissions to gdp it should be noted that the metadata selection by climate assessment categories e g scenarios by climate target namely scenario lower 2 c and scenario of 1 5 c with high probability of temporary overshoot is not currently included in the mipplot in next steps the plots should be displayed in more languages to cover a wider public range 5 conclusion in this paper we presented the mipplot tool an open source tool for visualizing data from long term climate mitigation scenarios with simple commands under a user friendly language platform any scenario dataset following the standard format of the data submission to the iamc can be applied to generate plots in multiple languages this tool can be used to communicate the outcomes of scenario analysis and facilitate discussions not only among researchers working with models and scenarios but also among researchers from other fields students policymakers and other potential stakeholders interested in climate policies aimed to mitigate ghg emissions the inputs of this package require the iamc format scenario data and table of additivity rules the package can then clarify missing variables implement conversions to standard formats and check the additivity of data inputs the plot functions enable the generation of visual diagnostic plots such as line charts stacked bar charts stacked area charts and point charts the plots can be generated by command codes or by an interactive function call and there is also a website browser beta version of the interactive interface the interactive function can greatly accelerate the creation of a set of plots for comparison it facilitates the discussion and dialog between experts and non experts by displaying results requested by the users in real time for example users can answer questions such as what would be the major source of co2 emissions by 2030 in asia when aiming at a 2 c target such simultaneous illustrations can avoid one way communication and ensure efficiency with all of its installed functions the mipplot can greatly improve the flexibility of access to the outcomes of energy economic and integrated assessment research users can download the mipplot free of cost and obtain full access the software capability of specifying aggregation rules and different display languages extends its applicability to a broader range of users additionally its capability to check the consistency and completeness of scenario data is powerful for experts who organize model intercomparison projects the current functions can still be improved to better serve the needs of both experts and non experts for experts the current data consistency check functions can be extended e g check if the value of one variable or indication is outside a reasonable range the detailed definition of the same variable from different iam teams can be clarified and the visualization of a large set of scenarios and models can be simplified by grouping data into categories e g scenarios by climate target and models by approach for non experts more detailed guidance of additivity rules and scenario filters should be provided in the future we also plan to add indicator functions and respective displays to the interactive interface e g indicators of renewable energy share industrial electrification rate and degree of decarbonization 6 software availability software mipplot v0 2 0 description the mipplot software is available as a free r package it contains generic functions to produce area bar box line plots of data following the iamc submission format the tool can be applied to datasets of climate mitigation scenarios emission scenarios generated by means of iam and energy economic models that follow the format adopted for data submissions contributing to ipcc assessment reports a beta version which is under development can be accessed by the url link by a limited number of accounts provided by the developers please contact the corresponding authors for access main developers masahiro sugiyama diego silva herran wang jiayang akimitsu inoue and ju yiyi source language r availability https github com utokyo mip mipplot author contributions ms dsh and wj conceived of the study dsh ai ms wj and jy contributed to the development of the r package jy dsh and ms wrote the manuscript which was edited and approved by all the authors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the environment research and technology development fund jpmeerf20172004 of the environmental restoration and conservation agency dsh was supported by the institute for global environmental strategies through its strategic research fund srf 
25854,synthesizing and communicating knowledge on climate change to policymakers and stakeholders is often difficult due to the complexity and diversity of underlying research as a translation instrument we present the mipplot tool an open source r package that can be used to visualize data of long term climate mitigation scenarios with simple commands and under a user friendly language platform this tool generates plots in multiple languages for any scenario dataset following the data submission format of the integrated assessment modeling consortium the plots can be generated both by function call in r environment and by an interactive interface in a web browser environment which offers greater flexibility of inputs and suits both experts and non experts moreover its capability of specifying aggregation rules and different display languages extends its applicability to a broader range of users keywords climate change mitigation visualization scenarios r package 1 introduction solutions to global environmental problems should be supported by policy analysis and scientific knowledge that are credible legitimate and salient cash et al 2003 or inclusive rigorous transparent and accessible donelly et al 2018 however synthesizing such knowledge to policymakers and stakeholders is often difficult due to inefficiencies in communication caused by the complexity and diversity of the underlying research there has been some progress regarding this issue on climate change particularly with regard to data standardization and visualization addressing climate change requires considerable reduction in greenhouse gas ghg emissions from human activities mainly from fossil fuel combustion and land use change such as the conversion of forests into agricultural land in this context future challenges for the reduction of ghg emissions and the corresponding effects on the economy society and environment are commonly assessed through future scenarios produced by integrated assessment models iams which are not without problems anderson and jewell 2019 iams have been criticized for being technocratic but still an indispensable tool to explore the reach of climate targets and to understand the related challenges the results from iams including the identification of drivers of socio economic development final energy consumption and emissions provide crucial information for policymakers to understand the possible outcomes of alternative development and climate policy scenarios for instance the shared socio economic pathways ssps which is a set of five representative global scenarios developed by the iam research community riahi et al 2017 have been widely used to assess alternative futures throughout the 21st century considering socio economic aspects such as population economic growth and energy use and emissions o neill et al 2020 ssps are used along with a set of scenarios of global ghg concentrations known as the representative concentration pathways rcps van vuuren et al 2011 to characterize emission pathways in terms of a level of global warming data from numerous climate mitigation scenarios generated by iams have become publicly available to ensure transparency and facilitate analyses the most notable example is the release of the databases by the intergovernmental panel on climate change ipcc regarding its 5th assessment report ar5 ipcc 2014 and more recently regarding the special report on 1 5 c global warming sr1 5 ipcc 2018 which presents information from several model intercomparison projects mips the effective dissemination and communication of this information from researchers can potentially provide integrated research insights and influence climate policies furthermore the increased availability of models and their results to non modelers can increase the reliability of policy focused research pfenninger 2017 the complexity and large amount of information from climate mitigation studies hinder a straightforward communication of main insights to policymakers unless there is a translation channel usually researchers with expertise in climate mitigation scenarios directly engage policymakers and disclose relevant information from iam analyses to them an alternative is to have the expert be part of the policy making body so that the policymaking side can handle the information without the support of researchers however it would be ideal if information was delivered in a format or media that allows policymakers to directly and flexibly manage process and communicate the results there is inconclusive discussion and dialogs among researchers policymakers and stakeholders due to the lack of translation of information which provides transparent standardization intercomparison and visualization of research results this challenge of developing such a translation tool has attracted research interest and has been investigated by many research groups for example in working group i wgi of the ipcc the earth system model evaluation tool esmvaltool has been developed to improve diagnosis and understanding of outcomes from earth system models eyring et al 2016a b under the coupled model intercomparison project cmip another example is the panoply which is a tool for visualization and analysis of geo referenced data from climate models nasa 2017 contrary to the esmvaltool which was targeted at climate modeling researchers panoply was designed for non researchers these data visualization tools enable policymakers and stakeholders to select or even generate specific data of their concern and visualize such data in a format that allows them to obtain possible insights from climate mitigation scenarios by themselves more specifically in the field of iam scenario results we reviewed some of the existing scenario data plotting tools previous efforts on such scenario data processing and visualization are shown in table 1 the tools shown in table 1 are described by type browser python r based access usage and dataset applicability in addition to the aforementioned tools the project senses also developed a web embedded interactive framework to increase the accessibility of iam scenario outcomes to non experts project senses 2011 in the japan 2050 low carbon navigator project project low carbon navigator 2014 a user friendly web based tool was also developed to display the simulation results for japan it was aimed at facilitating multi stakeholder discussions and educating university students and the general public on the mechanisms through which japan can achieve a low carbon pathway in the near future based on the current status of scenario data plot tools promoting the application of iam scenario results requires tools that i are open source to provide users especially those from developing regions full access to the data and functions using a free and user friendly language platform ii present function calls and interactive interfaces to ensure that experts scientists and educators and non expert users stakeholders and policymakers can plot data of their own concerns and iii provide multi language outputs to ensure that research results from iam teams in different countries can be easily accessed by local policymakers and stakeholders moreover it is essential to compile and display data from multiple models in a consistent format to enhance the effective dissemination of model insights to policymakers and academics involved in climate policy motivated by the aforementioned academic political and educational requirements we developed the mipplot package this tool was developed to serve a two fold purpose the gathering of iam scenario data into a consistent dataset and provide a straightforward visualization of key data from multiple models the tool is open source flexible can be modified by users adaptable can be applied to scenario data from different sources and capable of displaying plots from large scenario datasets from multiple scenarios and models in multiple languages the initial audience of the mipplot tool are researchers and students interested in climate change mitigation scenarios expected to be able to handle r packages via function calls the tool s design also aims to reach stakeholders without technical expertise on model or data manipulation by including an interactive interface compared to other tools mipplot is the first r package for visualization of scenario results that enhances the flexible interactive usability by function calls including multi language outputs 2 methodology the mipplot was developed as a software package for r statistical programming r core team 2015 which is an open source software platform users can download the mipplot package free of cost utokyo mip 2018 from the code sharing platform github once the tool is accessed and installed in the library it can be used and developed without any restrictions users can check the complete code and make modifications to suit their own needs 2 1 outline the inputs functions and outputs of mipplot are shown in fig 1 inputs of this package require the scenario data following the standard data submission format of the integrated assessment modeling consortium iamc and the table of additivity rules rule table the package contains several functions that can be individually applied after installing the library these functions can clarify missing variables implement conversion to standard formats and check the additivity of data inputs which can be a powerful tool for users to organize model comparison exercises each time they check the data submitted by a modeling team any inconsistency in the additivity of the disaggregated variables to their corresponding totals can be easily verified the mipplot can generate visual diagnostic plots of major variables under different scenarios such as primary energy supply co2 emissions and carbon prices which are the main functions of this package the generation can be executed using function calls or interactive interfaces the interactive function can greatly accelerate the generation of a set of plots for comparison outputs of this package include the visual diagnostic plots together with its full codes and the results of data consistency 2 2 inputs the package requires two types of inputs the scenario dataset and its additivity rule table a sample of the scenario data following iamc data format is shown in table 2 it contains model scenario region variable unit and the value of each year it is worth noting the naming of variable to name a variable the iamc data format uses a vertical bar to explain the inclusion relationship for example primary energy is used for primary energy data and primary energy biomass is used for primary energy supplied by biomass energy carriers we developed the additivity check function following such naming feature to easily group variable items and check additivity of the data as an input of this function we created the additivity rule table a sample of the additivity rule table is shown in table 3 it contains three items left side right side and color code the sum of variables from the right side under the same aggregation rule should equal the sum of variables from the left side the sample rule tables for the ar5 and sr1 5 datasets are provided in the package by default the color code indicates the corresponding color of the right side variables following the same processes users can create new rule tables to meet customized requirements 2 3 functions data input and format conversion functions are listed as follows mipplot read iamc r mipplot read ruletab r the mipplot read iamc function transforms raw data in iamc format into a format that suits the plotting functions in the r programming language the mipplot read ruletab function reads the table of additivity rule and defines the rule id i e see rt in the application example in section 3 1 these functions are required for the generation of figures in mipplot and they can be used for the design of customized plots the default color codes in the rule table follow the ipcc color palettes nevertheless it is possible to set customized color codes as the mipplot is a completely open source package data plot functions are listed as follows mipplot line mipplot bar mipplot area mipplot point these plot functions cover the main figure forms such as line charts stacked bar charts stacked area charts and point charts developed based on the ggplot2 package wickham 2016 we enhanced the usability of scenario data which was originally in a format that could not be used by r plotting functions thereby allowing the comparison of the main interests scenarios models regions variables and periods of stakeholders by assigning figure types to time series disaggregated variables considering the graphic grammar wickman 2010 the mipplot line is applicable to the time series data of a single variable and it depicts the long term changes of specific variables that users are concerned about as well as the variety among models or scenarios the visualization is enhanced by discriminating data according to the type of line and points the mipplot bar is applicable to disaggregated variables of a single period and it shows the structure of chosen aggregated variables as well as the variety among models or scenarios this function follows the additivity rule defined by mipplot read ruletab the mipplot area is applicable to the time series data of disaggregated variables and it shows the long term changes in the structures of specific aggregated variables that users are concerned about this function follows the additivity rule defined by mipplot read ruletab the mipplot point is applicable to a single variable in a single period and it shows the variation among models or scenarios of a specific variable that users are concerned about the default type of the plot output file is pdf nevertheless all file types of plot outputs supported by the r language can be used as the output file type of mipplot the data consistency check functions are listed as follows mipplot additivity check this function checks the additivity of disaggregated variables to their corresponding totals and it generates error sets of disaggregated variables and their corresponding plots this function is a powerful tool for users who organize model comparison exercises whenever they check the data submitted by a modeling team all possible inconsistencies in additivity can be easily verified it is also possible for users to set their error ranges e g 1 5 the data interactive plot functions are listed as follows mipplot interactive line mipplot interactive bar mipplot interactive area these functions provide an interactive interface based on the r package shiny of the plot functions the data scope including scenarios models regions variables and periods can be selected and directly adjusted through the interface the visualization was also improved by limiting the number of scenario options no more than 15 scenarios in one plot and by setting rotatable text angles on the x axis all function calls executed in the r environment return ggplot2 objects which can be further modified by ggplot2 functions along with each plot the interactive function also provides the entire code this enables non expert users such as students to access the research outcomes from iam climate mitigation scenarios without technical barriers a website browser beta version of the interactive interface is under development mipplot 0 2 0 beta version the website browser version of the interactive functions mentioned above was published in a rental server and it can be accessed by a url link a limited number of accounts were provided by the developers to ensure that specific stakeholders policymakers who do not use r programming software can simultaneously access the mip scenario results in the website and check the plots via the website interactive interface 2 4 multi language outputs the developed tool can display outputs in several languages which is normally limited in other scenario data plot tools both plot and interactive functions allow users to choose the language preferences of the output data this can be done by specifying an additional argument in any of the plotting functions the ipcc reports are translated into six official un languages although the database website is provided only in english the current version mipplot v0 2 0 supports english japanese chinese simplified chinese traditional and spanish with the standard iamc format data input in english mipplot translates all headers e g model scenario period etc in all languages regions e g world oecd asia etc in all languages and variables e g emissions co2 carbon capture and storage industry primary energy wind etc in all languages in the plots the availability of plots in different languages facilitates the communication of iam research outcomes to a wider audience scope particularly to local stakeholders and students from different countries 3 application example with sr1 5 scenario data the scenario data from the ipcc special report on global warming of 1 5 c ipcc 2018 was used as an application case of mipplot v0 2 0 3 1 example of plots four plots namely a line chart a stacked bar chart a stacked area chart and a point chart were generated by the four plot functions as shown in fig 2 in mipplot v0 2 0 the commands to generate the plots are simple image 8 as long as users are aware of the element names in the input dataset scenario model region variable they can specify any element or group of elements in a single command users can also specify multiple elements e g a list of regions and or variables to generate a list of plots 3 2 example of interactive function the interactive interface by function call is shown in fig 3 the command for calling the interactive function interfaces is also simple image 9 the interactive interface on website browser beta version is shown in fig 4 this beta version can be accessed by url link with limited accounts provided by developers currently the interactive functions area line and bar plots are selected first followed by the same interactive interface as the function call which is executed in the r environment the scenario results can be plotted directly on the website without the need for additional data input result data or rule table 3 3 example of multi language the example of multi language outputs are shown in fig 5 this function can be applied by specifying an additional argument in any of the plotting functions as follows image 10 it can also be selected in the interactive interface 4 discussion the r package mipplot can plot figures through function call or interactive interface based on data from long term climate mitigation scenarios this package provides full access to users and can be downloaded free of cost the outputs can be shown in multiple languages to ensure that the research results from iam teams in different countries can be accessed by more local policy makers and stakeholders however current functions should be improved to better serve the needs of both experts and non experts for experts the current data consistency checks functions can be extended except for the additivity checks a function of scope can be added to check if the value of one variable or is outside an expected range e g per capita energy consumption below maximum historical values how different iam teams define the same variable by mipplot guidance e g whether the variable final energy industry includes non energy use or not whether final energy transportation includes international bunkers or not etc should be clarified also if mipplot also provides the information of the differences between how local statistics and iea ipcc statistics define specific variables local policymakers may be more willing to take the results from international iam teams as reference the visualization of a large set of scenarios and models could be simplified by grouping data into categories e g scenarios by climate target models by approach and displaying the uncertainty ranges for each of those categories for non experts the rules of additivity should be explained in detail there can be multiple ways to disaggregate variables which is not evident in the naming of iamc format variables for example final energy can be aggregated by source e g final energy coal final energy solar and final energy others or sector e g final energy industry and final energy buildings such multiple additivity rules should be explained to non experts the messages that can be derived from a selected set of scenarios should be explained in detail the application of the tool requires some basic knowledge from non experts researchers and non researchers unfamiliar with the concepts of long term scenarios of climate change mitigation which includes the range and name of items included in the scenario datasets models scenarios regions and variables and the definition of major variables e g emissions treated by gas and emitting sectors primary and final energy supply and demand sectors and aggregation rules for variables e g discrimination between emissions or energy disaggregated by economic sectors or fuels we plan to add indicator functions and display them in the interactive interface for example indicators of renewable energy share share of all renewable energy sources to total energy supply industrial electrification rate share of electricity consumption to total industrial final energy and decarbonization degree ratio of carbon emissions to gdp it should be noted that the metadata selection by climate assessment categories e g scenarios by climate target namely scenario lower 2 c and scenario of 1 5 c with high probability of temporary overshoot is not currently included in the mipplot in next steps the plots should be displayed in more languages to cover a wider public range 5 conclusion in this paper we presented the mipplot tool an open source tool for visualizing data from long term climate mitigation scenarios with simple commands under a user friendly language platform any scenario dataset following the standard format of the data submission to the iamc can be applied to generate plots in multiple languages this tool can be used to communicate the outcomes of scenario analysis and facilitate discussions not only among researchers working with models and scenarios but also among researchers from other fields students policymakers and other potential stakeholders interested in climate policies aimed to mitigate ghg emissions the inputs of this package require the iamc format scenario data and table of additivity rules the package can then clarify missing variables implement conversions to standard formats and check the additivity of data inputs the plot functions enable the generation of visual diagnostic plots such as line charts stacked bar charts stacked area charts and point charts the plots can be generated by command codes or by an interactive function call and there is also a website browser beta version of the interactive interface the interactive function can greatly accelerate the creation of a set of plots for comparison it facilitates the discussion and dialog between experts and non experts by displaying results requested by the users in real time for example users can answer questions such as what would be the major source of co2 emissions by 2030 in asia when aiming at a 2 c target such simultaneous illustrations can avoid one way communication and ensure efficiency with all of its installed functions the mipplot can greatly improve the flexibility of access to the outcomes of energy economic and integrated assessment research users can download the mipplot free of cost and obtain full access the software capability of specifying aggregation rules and different display languages extends its applicability to a broader range of users additionally its capability to check the consistency and completeness of scenario data is powerful for experts who organize model intercomparison projects the current functions can still be improved to better serve the needs of both experts and non experts for experts the current data consistency check functions can be extended e g check if the value of one variable or indication is outside a reasonable range the detailed definition of the same variable from different iam teams can be clarified and the visualization of a large set of scenarios and models can be simplified by grouping data into categories e g scenarios by climate target and models by approach for non experts more detailed guidance of additivity rules and scenario filters should be provided in the future we also plan to add indicator functions and respective displays to the interactive interface e g indicators of renewable energy share industrial electrification rate and degree of decarbonization 6 software availability software mipplot v0 2 0 description the mipplot software is available as a free r package it contains generic functions to produce area bar box line plots of data following the iamc submission format the tool can be applied to datasets of climate mitigation scenarios emission scenarios generated by means of iam and energy economic models that follow the format adopted for data submissions contributing to ipcc assessment reports a beta version which is under development can be accessed by the url link by a limited number of accounts provided by the developers please contact the corresponding authors for access main developers masahiro sugiyama diego silva herran wang jiayang akimitsu inoue and ju yiyi source language r availability https github com utokyo mip mipplot author contributions ms dsh and wj conceived of the study dsh ai ms wj and jy contributed to the development of the r package jy dsh and ms wrote the manuscript which was edited and approved by all the authors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the environment research and technology development fund jpmeerf20172004 of the environmental restoration and conservation agency dsh was supported by the institute for global environmental strategies through its strategic research fund srf 
