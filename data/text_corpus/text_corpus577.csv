index,text
2885,vegetation conditions are deeply dependent on water availability conversely vegetation can also affect terrestrial water conditions by modifying evapotranspiration and other hydrological processes knowledge of the interactions between vegetation and land water conditions are required for understanding and simulating terrestrial water and carbon budgets in response to climate change for the first time in this study a non linear granger causality approach is applied to investigate the water vegetation interactions over australia they are characterized by respectively the normalized difference vegetation index ndvi and the gravity recovery and climate experiment grace terrestrial water storage tws the results indicate that non linear interactions between tws and ndvi occur in over half 58 4 of the continent which are more likely to take place for grasslands followed by shrublands agricultural lands savannas and forests a revisit analysis of the dominant driver among water temperature and radiation for vegetation conditions confirms that australia is mostly water limited although the total area of primarily water limited regions has slightly decreased from 67 0 in the earlier sub period 1985 1999 to 65 2 in the more recent sub period 2000 2015 it accounts for a much larger proportion of the continent than that of temperature and radiation limited regions in both sub periods the spatial coverage of regions where temperature is the dominant factor has nearly tripled for the recent sub period this could most likely be due to the warmer climate that exacerbates the limitation of high temperature on vegetation growth and functioning these results contribute to an improved understanding of australia s terrestrial water and carbon cycles the grace tws shows to be highly relevant data for such investigations at a continental scale keywords grace tws water vegetation interaction granger causality analysis australia data availability the authors do not have permission to share data 1 introduction australia s ecosystems play an important role in influencing the global carbon and water cycles poulter et al 2014 ahlström et al 2015 cleverly et al 2016 for example during the 2010 2011 la niña event australia showed a unique influence on a significant drop in global sea level fasullo et al 2013 also it contributed to around 60 of the global net carbon sink anomaly 1 1 3 pgc of 2011 poulter et al 2014 as the frequency and severity of extreme hydroclimatic events are predicted to increase in future decades easterling et al 2000 chou et al 2013 it is urgently needed to improve our understanding of the interactions between vegetation and hydrological conditions the fact that vegetation condition depends on water availability especially in water limited regions is well known nemani et al 2003 how land greenness affects the global water cycle and regional terrestrial water balance e g by regulating evapotranspiration and altering the terrestrial water cycle has also been investigated in recent years e g feng et al 2016 koirala et al 2017 zeng et al 2018 wei et al 2018 chen et al 2021 for australia previous studies have investigated how vegetation responds to land water conditions yang et al 2014 andrew et al 2017 xie et al 2019 however the two way relationship between vegetation and land water storage in this region remains to be understood vegetation takes up water from the root zone in which soil moisture is replenished by precipitation thus precipitation and soil water content data are commonly used to represent the water condition on which vegetation relies however a lack of continuous soil moisture observations in both time and space is a major impediment for large scale investigation of vegetation and soil water relationships seneviratne et al 2010 in situ measurements of soil moisture are costly and lack broad spatial coverage robock et al 2000 robinson et al 2008 outputs from land surface models such as the global land data assimilation system gldas rodell et al 2004 and the global soil wetness project 2 gswp 2 dirmeyer et al 2006 are dependent on the quality of forcing data remote sensing soil moisture products such as the soil moisture and oceanic salinity mission smos kerr et al 2016 and the soil moisture active passive mission smap entekhabi et al 2010 only provide direct sensing of soil moisture of the top 5 cm of the soil column in the recent two decades the gravity recovery and climate experiment grace terrestrial water storage tws has emerged as a useful data source for investigating vegetation soil moisture relationships yang et al 2014 reported that grace tws is a better hydrological indicator for explaining variability in vegetation dynamics in australia than precipitation xie et al 2019 detected hotspots of interactions between vegetation greenness and grace tws at the global scale based on a linear relationship and reported that the response of vegetation greenness to tws is more rapid than that to precipitation previous studies suggest that only part of the total precipitation is accessed by vegetation and thus precipitation only provides indirect information on the plant water conditions chen et al 2013 yang et al 2014 on the other hand tws is a more direct indicator of soil moisture available for plant growth and is thus associated with vegetation variation to a larger degree therefore the grace tws is adopted in this study to investigate the relationship between vegetation and land water conditions in addition to water availability radiation and temperature are also important drivers for vegetation growth and functioning previous global scale studies suggest that vegetation growth is limited by radiation in rainforests and by temperature at high northern latitudes nemani et al 2003 over approximately half of the earth s vegetated surface vegetation growth is driven by the availability of water nemani et al 2003 seddon et al 2016 including most parts of australia since the spatial patterns of water limitation on vegetation compared to temperature and radiation in australia may have changed in the face of climate change in recent years it is meaningful to revisit the spatial patterns and detect the corresponding changes in different time periods the interactions between vegetation and climate conditions are most likely non linear foley et al 1998 zeng 2002 papagiannopoulou et al 2017 proposed a non linear granger causality framework to investigate the climate vegetation dynamics the granger causality test was first proposed by the economist granger 1969 and has been applied to investigate the vegetation climate interactions for recent years e g jiang et al 2015 kong et al 2018 xie et al 2019 this method assumes that variable x granger causes variable y if the past time series of variable x improves predicting the future time series of variable y although the granger causality test does not confirm a direct physical mechanism between two variables it provides implications of possible causality links from a statistical perspective here we present the first study on non linear interactions between land water conditions represented by the grace tws and vegetation conditions indicated by the normalized difference vegetation index ndvi in australia the objectives of this study are 1 to revisit the spatial patterns of water limitation on vegetation in comparison to temperature and radiation in australia and 2 to quantify the non linear interaction between vegetation and land water conditions with respect to different vegetation types 2 methodology 2 1 data a 31 year 1985 2015 reconstructed grace tws dataset humphrey et al 2017 provided by the institute for atmospheric and climate science eth is used in this study the 14 year 2003 2016 original grace tws dataset rl06m mscnv01 watkins et al 2015 wiese et al 2016 provided by the nasa jet propulsion laboratory jpl is also considered for comparison the reconstruction of the 31 year tws data set was based on empirical relationships between decomposed jpl grace tws temporal variations and the main atmospheric drivers precipitation and temperature this reconstructed tws time series are more robust and perform better than four state of the art land surface models humphrey et al 2017 we used the global inventory monitoring and modelling system gimms ndvi data tucker et al 2005 pinzon tucker 2014 for the same period as the reconstructed grace tws data the moderate resolution imaging spectroradiometer modis land cover classification data are used to identify different vegetation types across australia including forest shrubland savanna grassland and agricultural land fig 1 in addition as temperature and radiation are important drivers for vegetation conditions the era5 air temperature and net radiation datasets are also used in this study all the datasets are used at a 0 5 0 5 spatial resolution as they are or resampled to this resolution if necessary 2 2 statistical methods the granger causality method can test possible causal relationships e g forcing and feedback between vegetation and climate variables jiang et al 2015 kong et al 2018 the non linear granger causality analysis papagiannopoulou et al 2017 is applied in this study to investigate the interaction between vegetation and land water conditions over australia variable x granger causes variable y if the autoregressive forecast of y is improved by considering the information about x a linear vector autoregressive model can be used to derive the predictions required to determine granger causality which is represented as 1 y t x t β 01 β 02 p 1 p β 11 p β 12 p β 21 p β 22 p y t p x t p 1 2 where β ij are parameters that need to be estimated and 1 and 2 are white noise error terms p is the length of the lag time moving window if at least one of the parameters β 12 p at any p significantly differs from 0 it can be concluded that time series x granger causes time series y the following two models compare the two situations of including information of both x and y eq 2 versus only considering information about y eq 3 2 y t y t 1 β 01 p 1 p β 11 p y t p β 12 p x t p 1 3 y t y t 1 β 01 p 1 p β 11 p y t p 1 here we replace the traditional autoregressive models eq 2 and 3 in the granger causality framework with a non linear machine learning model i e the random forest the scikit learn library pedregosa et al 2011 is used to implement the random forest regression at each grid cell with the number of trees equal to 100 and the maximum number of predictor variables per node equal to the square root of the total number of predictor variables in addition the 5 fold cross validation is used to assess the model performance more details can be found in papagiannopoulou et al 2017 the authors proposed a performance measure based on the coefficient of determination r2 to evaluate the forecast it is defined as 4 r 2 y y 1 rss tss 1 i p 1 n y i y i 2 i p 1 n y i y 2 where y is the mean of y and y is the predicted time series of y obtained from a given forecasting model rss is the residual sum of squares and tss is the total sum of squares n is the length of the time series p 12 months is adopted in this study 12 predictor variables are included in the non linear predictive model as time windows longer than 12 months do not improve the predictions papagiannopoulou et al 2017 the quantification of the granger causality is represented by r2 which increases as the model performance improves a positive r2 means x granger causes y and a negative r2 means predictions are less representative of the observations than the mean of the observations in addition due to the existence of trends and seasonality in ndvi and tws time series parts of them might be non stationary and unsuitable for direct application of the granger causality test kong et al 2018 after the trends and seasonality are removed all the time series are stationary which is tested by the augmented dickey fuller adf unit root test after the processing the non linear granger causality test is applied in addition to the non linear granger causality analysis the pearson linear correlation is applied to examine how temperature might impact i e promote or impede vegetation growth and functioning in australia as well as the spatial patterns of tws ndvi and radiation ndvi correlations over australia for these analyses the time series of monthly anomalies of ndvi tws temperature and radiation are applied considering the autocorrelation of datasets the effective sample size bretherton et al 1999 is used in the statistical significance testing for the correlation coefficient between two time series the 0 05 significance level is adopted in this study 3 results and discussion 3 1 comparison of the influence of water temperature and radiation on vegetation condition in australia water availability temperature and radiation are important drivers for vegetation conditions which are commonly considered in investigating the effect of climate change on global ecosystems and the corresponding feedback nemani et al 2003 and seddon et al 2016 identified dominant drivers for vegetation productivity among those three factors at the global scale for 1982 1999 and 2000 2013 respectively water limited regions in australia seem to have become smaller in recent years compared to that of last century here we provide a revisit on the spatial patterns of water limitation on vegetation in comparison to temperature and radiation in australia by quantifying the impacts of those three factors on vegetation based on the non linear granger causality anlysis we split the 31 year data period into two sub periods i e 1985 1999 and 2000 2015 to detect the changes in spatial patterns of water temperature and radiation limitation on vegetation figs 2 a c show that tws air temperature and net radiation respectively granger causes ndvi in 91 9 82 8 and 62 4 of the grid cells and those spatial patterns are generally consistent with the pearson correlation analysis results shown in fig s1 by comparing the non linear granger causality r2 to ndvi from each of tws air temperature and net radiation ndvi for 61 6 of the grid cells in australia is dominantly driven by tws fig 2 d this confirms that australia is primarily water limited temperature is identified to be the dominant driver for 19 1 of the grid cells in this study fig 2 d when temperature is a constraint for plant growth like at high northern latitudes the temperature is too low to sustain a normal level of plant physiological activities in our study the regions where the temperature is a dominant factor show significant negative temperature ndvi correlation fig s1 b which means that vegetation greenness is impeded by high temperature qiu et al 2020 yang et al 2019 high temperature can directly limit plant growth by leading to changes in physiology haldimann and feller 2004 prasad et al 2017 bheemanahalli et al 2019 in some situations high temperature may increase evapotranspiration leading to a quick depletion of soil moisture consequently it will indirectly limit vegetation photosynthesis and growth rate yang et al 2019 the adverse effects of high temperature on vegetation conditions in australia should be taken seriously as the occurrence of simultaneous drought and heatwaves are likely to cause serious damage mazdiyasni and aghakouchak 2015 radiation dominates ndvi changes in 14 9 of the grid cells which are mostly located in the northwestern part of australia here significant positive correlations between net radiation and ndvi are observed fig s1 c we calculate annual average precipitation annual average evapotranspiration during 1985 2015 for each grid cell to identify energy limited and water limited regions fig s2 by following mcvicar et al 2012 radiation limited regions shown in fig 2 c correspond to the energy limited regions shown in fig s2 the spatial distribution of those radiation limited regions is also generally consistent with a previous study seddon et al 2016 on identifying dominant drivers for vegetation productivity among three climate variables water temperature and cloud cover at the global scale fig 3 compares the spatial pattern of the largest non linear granger causality among water temperature and radiation for vegetation conditions during 1985 1999 fig 3 a and 2000 2015 fig 3 b the percentage of regions in which temperature is the dominant driver for vegetation increased from 7 3 to 19 9 this result implies that the limitation of high temperature on vegetation conditions become stronger with the warmer climate during the more recent period this notion is supported by the results shown in fig s3 that the negative correlations between temperature and ndvi in the northern and eastern parts of australia appear to be stronger in the period of 2000 2015 than that in 1985 1999 the percentage of radiation limited regions decreased from 22 6 to 13 4 but the spatial distribution patterns in the two periods are similar which are mostly located in the northern part the northwestern part and the southeastern coastal areas of australia although the total area of water limited regions slightly decreased from 67 0 to 65 2 in the recent period it accounts for a much larger proportion of australia than the total area where temperature and radiation are the dominant factor in both sub periods therefore we focus on the interaction between vegetation and water conditions in australia in the next section 3 2 unidirectional and bidirectional granger causality relationships between vegetation and land water conditions with respect to different vegetation types this study defines the unidirectional and bidirectional granger causality relationships between vegetation and land water conditions following xie et al 2019 at each grid cell if the prediction of ndvi is improved by including tws as a predictor but the prediction of tws is not improved by including ndvi as a predictor then tws is said to be the unidirectional granger cause for ndvi and vice versa if the predictions are improved both ways then it is considered that there is a bidirectional granger causality relationship between tws and ndvi the unidirectional effect from tws on ndvi is observed in 33 5 of the grid cells fig 4 a and the effect of the opposite direction is only observed in 2 5 of the grid cells fig 4 b this is explained by the fact that in most cases where ndvi granger causes tws tws granger causes ndvi too bidirectional granger causality relationship between tws and ndvi is observed over half 58 4 of the study area fig 4 c fig 4 d shows the proportions of the unidirectional and bidirectional relationships between tws and ndvi for different vegetation types the highest proportion of tws ndvi interactions is observed in grasslands followed by shrublands agricultural lands and savannas and the lowest in forests the largest proportion of grid cells without a detected tws ndvi interaction is observed for the forest class this apparent result may be because australia is prone to bushfires which could disturb the detection of the effect from ndvi to tws these results suggest that vegetation cover and terrestrial water storage are closely coupled in a recent study we reported a coupling mechanism leading to a continental scale tws seesaw phenomenon in australia chen et al 2021 knowledge on such vegetation cover and tws interactions is important for modelling vegetation feedbacks to the carbon and hydrological cycles quantification of ndvi impacts on tws also provides a reference for improving large scale land surface models based on the reconstructed tws dataset 1985 2015 the non linear granger causality tests show that tws granger causes the variability in ndvi for 91 9 of the total number of grid cells fig 2 a while ndvi granger causes tws in 60 9 of the grid cells in australia figs 4 b c we also repeat for comparison a non linear granger causality analysis based on the original grace tws data from 2003 to 2016 the results are shown in fig 5 based on the 14 year data tws granger causes ndvi in 85 5 of the grid cells and ndvi granger causes tws in 66 0 of the grid cells the granger cause from ndvi to tws is mainly observed in eastern australia figs 4 b c and fig 5 it may be partially explained by the relatively larger plant available water capacity see fig s4 a larger plant available water capacity is often assoicated with thicker soils wetter climate and denser vegetation in those regions vegetation plays an important role in influencing the permeability and water retention capacity of the soils e g poor vegetation cover tends to favour soil erosion and reduce water storage capacity of the soil mohammad adam 2010 the apparent blocky pattern observed in fig 2 a and d fig 3 figs 4 a c and fig 5 is very likely an imprint of the original 3 3 resolution of the grace tws watkins et al 2015 thus the tws signal of the downscaled 0 5 0 5 grid is dependent on the surrounding grids in addition humphrey et al 2017 reported that the reconstructed 31 year tws data have larger predictive uncertainty than jpl grace data in the equatorial regions including some northern parts of australia due to lower quality of the atmospheric reanalysis in those regions however the larger uncertainty of the reconstructed tws data in the equatorial region of australia does not affect the non linear granger causality analysis result because both original tws fig 5 and reconstructed tws fig 4 show granger causality relationship with ndvi for very few grid cells in those regions it should also be noted that only precipitation and temperature were used to reconstruct the 31 year tws dataset based on the jpl grace tws data while human influences e g groundwater extraction and dam management were not considered therefore uncertainties exist in the reconstructed tws data as well as the analysis results based on that data for example in revisiting the dominant drivers among tws air temperature and net radiation for vegetation conditions in australia the results may differ from the actual situation to some extent as the reconstructed tws are not independent of temperature these limitations are expected to be overcome by the increase of the availability and reliability of the original grace tws data with the accumulation of data and improved grace resolution in the future the proportion of non linear granger causality relationships between tws and ndvi in australia reported in this study appears to be larger than that detected by the linear granger causality analysis on a global scale xie et al 2019 here we compare the percentages of grid cells with granger causality relationships between tws and ndvi based on non linear versus linear granger causality analysis during 1985 2015 fig 6 results show that both directions of granger causality relationship between tws and ndvi can be better revealed by the non linear granger causality analysis than the linear granger causality analysis such results highlight the importance of the non linear relationships between vegetation and land water conditions 4 conclusions in this study we aimed to revisit the dominant driver i e tws air temperature and net radiation for vegetation conditions in australia based on a non linear granger causality framework the analysis results confirm that australia is mostly water limited as tws dominantly drives ndvi variation for 61 6 of the grid cells during 1985 2015 the difference in the spatial patterns of water temperature and radiation limitation on vegetation conditions in two sub periods i e 1985 1999 vs 2000 2015 suggests that water limited regions has slightly decreased from 67 0 to 65 2 the total area of the regions where temperature is the dominant factor has nearly tripled for the recent sub period most likely due to the warmer climate which exacerbates the stress of high temperature on vegetation growth and functioning the unidirectional and bidirectional granger causality relationships between ndvi and tws with respect to different vegetation types show that tws is granger causing changes in vegetation for 91 9 of the grid cells ndvi granger causes tws in 60 8 of the grid cells tws ndvi interactions are detected in over half 58 4 of the continent which are more likely to occur in grasslands followed by shrublands agricultural lands savannas and forests the interaction established herein between tws and ndvi may help to improve our understanding of the continental terrestrial water and carbon cycles credit authorship contribution statement ajiao chen conceptualization methodology formal analysis investigation resources writing original draft writing review editing huade guan conceptualization methodology resources writing review editing supervision okke batelaan conceptualization methodology writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we confirm that none of the authors has a conflict of interest to declare and agree with the contents the datasets used in this study are available online jpl grace tws 2003 2016 https grace jpl nasa gov reconstructed tws grace rec v01 1985 2015 http rossa prod ap21 ethz ch delivery deliverymanagerservlet dps pid ie5766472 gimms ndvi https climatedataguide ucar edu climate data ndvi normalized difference vegetation index 3rd generation nasagfsc gimms era5 2 meter temperature and net radiation 10 24381 cds f17050d7 modis land cover classification https neo sci gsfc nasa gov view php datasetid mcd12c1 t1 we are thankful to the respective organizations for providing listed data the first author acknowledges support from the china scholarship council ministry of education china and flinders university australia for providing a phd scholarship appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128336 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2885,vegetation conditions are deeply dependent on water availability conversely vegetation can also affect terrestrial water conditions by modifying evapotranspiration and other hydrological processes knowledge of the interactions between vegetation and land water conditions are required for understanding and simulating terrestrial water and carbon budgets in response to climate change for the first time in this study a non linear granger causality approach is applied to investigate the water vegetation interactions over australia they are characterized by respectively the normalized difference vegetation index ndvi and the gravity recovery and climate experiment grace terrestrial water storage tws the results indicate that non linear interactions between tws and ndvi occur in over half 58 4 of the continent which are more likely to take place for grasslands followed by shrublands agricultural lands savannas and forests a revisit analysis of the dominant driver among water temperature and radiation for vegetation conditions confirms that australia is mostly water limited although the total area of primarily water limited regions has slightly decreased from 67 0 in the earlier sub period 1985 1999 to 65 2 in the more recent sub period 2000 2015 it accounts for a much larger proportion of the continent than that of temperature and radiation limited regions in both sub periods the spatial coverage of regions where temperature is the dominant factor has nearly tripled for the recent sub period this could most likely be due to the warmer climate that exacerbates the limitation of high temperature on vegetation growth and functioning these results contribute to an improved understanding of australia s terrestrial water and carbon cycles the grace tws shows to be highly relevant data for such investigations at a continental scale keywords grace tws water vegetation interaction granger causality analysis australia data availability the authors do not have permission to share data 1 introduction australia s ecosystems play an important role in influencing the global carbon and water cycles poulter et al 2014 ahlström et al 2015 cleverly et al 2016 for example during the 2010 2011 la niña event australia showed a unique influence on a significant drop in global sea level fasullo et al 2013 also it contributed to around 60 of the global net carbon sink anomaly 1 1 3 pgc of 2011 poulter et al 2014 as the frequency and severity of extreme hydroclimatic events are predicted to increase in future decades easterling et al 2000 chou et al 2013 it is urgently needed to improve our understanding of the interactions between vegetation and hydrological conditions the fact that vegetation condition depends on water availability especially in water limited regions is well known nemani et al 2003 how land greenness affects the global water cycle and regional terrestrial water balance e g by regulating evapotranspiration and altering the terrestrial water cycle has also been investigated in recent years e g feng et al 2016 koirala et al 2017 zeng et al 2018 wei et al 2018 chen et al 2021 for australia previous studies have investigated how vegetation responds to land water conditions yang et al 2014 andrew et al 2017 xie et al 2019 however the two way relationship between vegetation and land water storage in this region remains to be understood vegetation takes up water from the root zone in which soil moisture is replenished by precipitation thus precipitation and soil water content data are commonly used to represent the water condition on which vegetation relies however a lack of continuous soil moisture observations in both time and space is a major impediment for large scale investigation of vegetation and soil water relationships seneviratne et al 2010 in situ measurements of soil moisture are costly and lack broad spatial coverage robock et al 2000 robinson et al 2008 outputs from land surface models such as the global land data assimilation system gldas rodell et al 2004 and the global soil wetness project 2 gswp 2 dirmeyer et al 2006 are dependent on the quality of forcing data remote sensing soil moisture products such as the soil moisture and oceanic salinity mission smos kerr et al 2016 and the soil moisture active passive mission smap entekhabi et al 2010 only provide direct sensing of soil moisture of the top 5 cm of the soil column in the recent two decades the gravity recovery and climate experiment grace terrestrial water storage tws has emerged as a useful data source for investigating vegetation soil moisture relationships yang et al 2014 reported that grace tws is a better hydrological indicator for explaining variability in vegetation dynamics in australia than precipitation xie et al 2019 detected hotspots of interactions between vegetation greenness and grace tws at the global scale based on a linear relationship and reported that the response of vegetation greenness to tws is more rapid than that to precipitation previous studies suggest that only part of the total precipitation is accessed by vegetation and thus precipitation only provides indirect information on the plant water conditions chen et al 2013 yang et al 2014 on the other hand tws is a more direct indicator of soil moisture available for plant growth and is thus associated with vegetation variation to a larger degree therefore the grace tws is adopted in this study to investigate the relationship between vegetation and land water conditions in addition to water availability radiation and temperature are also important drivers for vegetation growth and functioning previous global scale studies suggest that vegetation growth is limited by radiation in rainforests and by temperature at high northern latitudes nemani et al 2003 over approximately half of the earth s vegetated surface vegetation growth is driven by the availability of water nemani et al 2003 seddon et al 2016 including most parts of australia since the spatial patterns of water limitation on vegetation compared to temperature and radiation in australia may have changed in the face of climate change in recent years it is meaningful to revisit the spatial patterns and detect the corresponding changes in different time periods the interactions between vegetation and climate conditions are most likely non linear foley et al 1998 zeng 2002 papagiannopoulou et al 2017 proposed a non linear granger causality framework to investigate the climate vegetation dynamics the granger causality test was first proposed by the economist granger 1969 and has been applied to investigate the vegetation climate interactions for recent years e g jiang et al 2015 kong et al 2018 xie et al 2019 this method assumes that variable x granger causes variable y if the past time series of variable x improves predicting the future time series of variable y although the granger causality test does not confirm a direct physical mechanism between two variables it provides implications of possible causality links from a statistical perspective here we present the first study on non linear interactions between land water conditions represented by the grace tws and vegetation conditions indicated by the normalized difference vegetation index ndvi in australia the objectives of this study are 1 to revisit the spatial patterns of water limitation on vegetation in comparison to temperature and radiation in australia and 2 to quantify the non linear interaction between vegetation and land water conditions with respect to different vegetation types 2 methodology 2 1 data a 31 year 1985 2015 reconstructed grace tws dataset humphrey et al 2017 provided by the institute for atmospheric and climate science eth is used in this study the 14 year 2003 2016 original grace tws dataset rl06m mscnv01 watkins et al 2015 wiese et al 2016 provided by the nasa jet propulsion laboratory jpl is also considered for comparison the reconstruction of the 31 year tws data set was based on empirical relationships between decomposed jpl grace tws temporal variations and the main atmospheric drivers precipitation and temperature this reconstructed tws time series are more robust and perform better than four state of the art land surface models humphrey et al 2017 we used the global inventory monitoring and modelling system gimms ndvi data tucker et al 2005 pinzon tucker 2014 for the same period as the reconstructed grace tws data the moderate resolution imaging spectroradiometer modis land cover classification data are used to identify different vegetation types across australia including forest shrubland savanna grassland and agricultural land fig 1 in addition as temperature and radiation are important drivers for vegetation conditions the era5 air temperature and net radiation datasets are also used in this study all the datasets are used at a 0 5 0 5 spatial resolution as they are or resampled to this resolution if necessary 2 2 statistical methods the granger causality method can test possible causal relationships e g forcing and feedback between vegetation and climate variables jiang et al 2015 kong et al 2018 the non linear granger causality analysis papagiannopoulou et al 2017 is applied in this study to investigate the interaction between vegetation and land water conditions over australia variable x granger causes variable y if the autoregressive forecast of y is improved by considering the information about x a linear vector autoregressive model can be used to derive the predictions required to determine granger causality which is represented as 1 y t x t β 01 β 02 p 1 p β 11 p β 12 p β 21 p β 22 p y t p x t p 1 2 where β ij are parameters that need to be estimated and 1 and 2 are white noise error terms p is the length of the lag time moving window if at least one of the parameters β 12 p at any p significantly differs from 0 it can be concluded that time series x granger causes time series y the following two models compare the two situations of including information of both x and y eq 2 versus only considering information about y eq 3 2 y t y t 1 β 01 p 1 p β 11 p y t p β 12 p x t p 1 3 y t y t 1 β 01 p 1 p β 11 p y t p 1 here we replace the traditional autoregressive models eq 2 and 3 in the granger causality framework with a non linear machine learning model i e the random forest the scikit learn library pedregosa et al 2011 is used to implement the random forest regression at each grid cell with the number of trees equal to 100 and the maximum number of predictor variables per node equal to the square root of the total number of predictor variables in addition the 5 fold cross validation is used to assess the model performance more details can be found in papagiannopoulou et al 2017 the authors proposed a performance measure based on the coefficient of determination r2 to evaluate the forecast it is defined as 4 r 2 y y 1 rss tss 1 i p 1 n y i y i 2 i p 1 n y i y 2 where y is the mean of y and y is the predicted time series of y obtained from a given forecasting model rss is the residual sum of squares and tss is the total sum of squares n is the length of the time series p 12 months is adopted in this study 12 predictor variables are included in the non linear predictive model as time windows longer than 12 months do not improve the predictions papagiannopoulou et al 2017 the quantification of the granger causality is represented by r2 which increases as the model performance improves a positive r2 means x granger causes y and a negative r2 means predictions are less representative of the observations than the mean of the observations in addition due to the existence of trends and seasonality in ndvi and tws time series parts of them might be non stationary and unsuitable for direct application of the granger causality test kong et al 2018 after the trends and seasonality are removed all the time series are stationary which is tested by the augmented dickey fuller adf unit root test after the processing the non linear granger causality test is applied in addition to the non linear granger causality analysis the pearson linear correlation is applied to examine how temperature might impact i e promote or impede vegetation growth and functioning in australia as well as the spatial patterns of tws ndvi and radiation ndvi correlations over australia for these analyses the time series of monthly anomalies of ndvi tws temperature and radiation are applied considering the autocorrelation of datasets the effective sample size bretherton et al 1999 is used in the statistical significance testing for the correlation coefficient between two time series the 0 05 significance level is adopted in this study 3 results and discussion 3 1 comparison of the influence of water temperature and radiation on vegetation condition in australia water availability temperature and radiation are important drivers for vegetation conditions which are commonly considered in investigating the effect of climate change on global ecosystems and the corresponding feedback nemani et al 2003 and seddon et al 2016 identified dominant drivers for vegetation productivity among those three factors at the global scale for 1982 1999 and 2000 2013 respectively water limited regions in australia seem to have become smaller in recent years compared to that of last century here we provide a revisit on the spatial patterns of water limitation on vegetation in comparison to temperature and radiation in australia by quantifying the impacts of those three factors on vegetation based on the non linear granger causality anlysis we split the 31 year data period into two sub periods i e 1985 1999 and 2000 2015 to detect the changes in spatial patterns of water temperature and radiation limitation on vegetation figs 2 a c show that tws air temperature and net radiation respectively granger causes ndvi in 91 9 82 8 and 62 4 of the grid cells and those spatial patterns are generally consistent with the pearson correlation analysis results shown in fig s1 by comparing the non linear granger causality r2 to ndvi from each of tws air temperature and net radiation ndvi for 61 6 of the grid cells in australia is dominantly driven by tws fig 2 d this confirms that australia is primarily water limited temperature is identified to be the dominant driver for 19 1 of the grid cells in this study fig 2 d when temperature is a constraint for plant growth like at high northern latitudes the temperature is too low to sustain a normal level of plant physiological activities in our study the regions where the temperature is a dominant factor show significant negative temperature ndvi correlation fig s1 b which means that vegetation greenness is impeded by high temperature qiu et al 2020 yang et al 2019 high temperature can directly limit plant growth by leading to changes in physiology haldimann and feller 2004 prasad et al 2017 bheemanahalli et al 2019 in some situations high temperature may increase evapotranspiration leading to a quick depletion of soil moisture consequently it will indirectly limit vegetation photosynthesis and growth rate yang et al 2019 the adverse effects of high temperature on vegetation conditions in australia should be taken seriously as the occurrence of simultaneous drought and heatwaves are likely to cause serious damage mazdiyasni and aghakouchak 2015 radiation dominates ndvi changes in 14 9 of the grid cells which are mostly located in the northwestern part of australia here significant positive correlations between net radiation and ndvi are observed fig s1 c we calculate annual average precipitation annual average evapotranspiration during 1985 2015 for each grid cell to identify energy limited and water limited regions fig s2 by following mcvicar et al 2012 radiation limited regions shown in fig 2 c correspond to the energy limited regions shown in fig s2 the spatial distribution of those radiation limited regions is also generally consistent with a previous study seddon et al 2016 on identifying dominant drivers for vegetation productivity among three climate variables water temperature and cloud cover at the global scale fig 3 compares the spatial pattern of the largest non linear granger causality among water temperature and radiation for vegetation conditions during 1985 1999 fig 3 a and 2000 2015 fig 3 b the percentage of regions in which temperature is the dominant driver for vegetation increased from 7 3 to 19 9 this result implies that the limitation of high temperature on vegetation conditions become stronger with the warmer climate during the more recent period this notion is supported by the results shown in fig s3 that the negative correlations between temperature and ndvi in the northern and eastern parts of australia appear to be stronger in the period of 2000 2015 than that in 1985 1999 the percentage of radiation limited regions decreased from 22 6 to 13 4 but the spatial distribution patterns in the two periods are similar which are mostly located in the northern part the northwestern part and the southeastern coastal areas of australia although the total area of water limited regions slightly decreased from 67 0 to 65 2 in the recent period it accounts for a much larger proportion of australia than the total area where temperature and radiation are the dominant factor in both sub periods therefore we focus on the interaction between vegetation and water conditions in australia in the next section 3 2 unidirectional and bidirectional granger causality relationships between vegetation and land water conditions with respect to different vegetation types this study defines the unidirectional and bidirectional granger causality relationships between vegetation and land water conditions following xie et al 2019 at each grid cell if the prediction of ndvi is improved by including tws as a predictor but the prediction of tws is not improved by including ndvi as a predictor then tws is said to be the unidirectional granger cause for ndvi and vice versa if the predictions are improved both ways then it is considered that there is a bidirectional granger causality relationship between tws and ndvi the unidirectional effect from tws on ndvi is observed in 33 5 of the grid cells fig 4 a and the effect of the opposite direction is only observed in 2 5 of the grid cells fig 4 b this is explained by the fact that in most cases where ndvi granger causes tws tws granger causes ndvi too bidirectional granger causality relationship between tws and ndvi is observed over half 58 4 of the study area fig 4 c fig 4 d shows the proportions of the unidirectional and bidirectional relationships between tws and ndvi for different vegetation types the highest proportion of tws ndvi interactions is observed in grasslands followed by shrublands agricultural lands and savannas and the lowest in forests the largest proportion of grid cells without a detected tws ndvi interaction is observed for the forest class this apparent result may be because australia is prone to bushfires which could disturb the detection of the effect from ndvi to tws these results suggest that vegetation cover and terrestrial water storage are closely coupled in a recent study we reported a coupling mechanism leading to a continental scale tws seesaw phenomenon in australia chen et al 2021 knowledge on such vegetation cover and tws interactions is important for modelling vegetation feedbacks to the carbon and hydrological cycles quantification of ndvi impacts on tws also provides a reference for improving large scale land surface models based on the reconstructed tws dataset 1985 2015 the non linear granger causality tests show that tws granger causes the variability in ndvi for 91 9 of the total number of grid cells fig 2 a while ndvi granger causes tws in 60 9 of the grid cells in australia figs 4 b c we also repeat for comparison a non linear granger causality analysis based on the original grace tws data from 2003 to 2016 the results are shown in fig 5 based on the 14 year data tws granger causes ndvi in 85 5 of the grid cells and ndvi granger causes tws in 66 0 of the grid cells the granger cause from ndvi to tws is mainly observed in eastern australia figs 4 b c and fig 5 it may be partially explained by the relatively larger plant available water capacity see fig s4 a larger plant available water capacity is often assoicated with thicker soils wetter climate and denser vegetation in those regions vegetation plays an important role in influencing the permeability and water retention capacity of the soils e g poor vegetation cover tends to favour soil erosion and reduce water storage capacity of the soil mohammad adam 2010 the apparent blocky pattern observed in fig 2 a and d fig 3 figs 4 a c and fig 5 is very likely an imprint of the original 3 3 resolution of the grace tws watkins et al 2015 thus the tws signal of the downscaled 0 5 0 5 grid is dependent on the surrounding grids in addition humphrey et al 2017 reported that the reconstructed 31 year tws data have larger predictive uncertainty than jpl grace data in the equatorial regions including some northern parts of australia due to lower quality of the atmospheric reanalysis in those regions however the larger uncertainty of the reconstructed tws data in the equatorial region of australia does not affect the non linear granger causality analysis result because both original tws fig 5 and reconstructed tws fig 4 show granger causality relationship with ndvi for very few grid cells in those regions it should also be noted that only precipitation and temperature were used to reconstruct the 31 year tws dataset based on the jpl grace tws data while human influences e g groundwater extraction and dam management were not considered therefore uncertainties exist in the reconstructed tws data as well as the analysis results based on that data for example in revisiting the dominant drivers among tws air temperature and net radiation for vegetation conditions in australia the results may differ from the actual situation to some extent as the reconstructed tws are not independent of temperature these limitations are expected to be overcome by the increase of the availability and reliability of the original grace tws data with the accumulation of data and improved grace resolution in the future the proportion of non linear granger causality relationships between tws and ndvi in australia reported in this study appears to be larger than that detected by the linear granger causality analysis on a global scale xie et al 2019 here we compare the percentages of grid cells with granger causality relationships between tws and ndvi based on non linear versus linear granger causality analysis during 1985 2015 fig 6 results show that both directions of granger causality relationship between tws and ndvi can be better revealed by the non linear granger causality analysis than the linear granger causality analysis such results highlight the importance of the non linear relationships between vegetation and land water conditions 4 conclusions in this study we aimed to revisit the dominant driver i e tws air temperature and net radiation for vegetation conditions in australia based on a non linear granger causality framework the analysis results confirm that australia is mostly water limited as tws dominantly drives ndvi variation for 61 6 of the grid cells during 1985 2015 the difference in the spatial patterns of water temperature and radiation limitation on vegetation conditions in two sub periods i e 1985 1999 vs 2000 2015 suggests that water limited regions has slightly decreased from 67 0 to 65 2 the total area of the regions where temperature is the dominant factor has nearly tripled for the recent sub period most likely due to the warmer climate which exacerbates the stress of high temperature on vegetation growth and functioning the unidirectional and bidirectional granger causality relationships between ndvi and tws with respect to different vegetation types show that tws is granger causing changes in vegetation for 91 9 of the grid cells ndvi granger causes tws in 60 8 of the grid cells tws ndvi interactions are detected in over half 58 4 of the continent which are more likely to occur in grasslands followed by shrublands agricultural lands savannas and forests the interaction established herein between tws and ndvi may help to improve our understanding of the continental terrestrial water and carbon cycles credit authorship contribution statement ajiao chen conceptualization methodology formal analysis investigation resources writing original draft writing review editing huade guan conceptualization methodology resources writing review editing supervision okke batelaan conceptualization methodology writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we confirm that none of the authors has a conflict of interest to declare and agree with the contents the datasets used in this study are available online jpl grace tws 2003 2016 https grace jpl nasa gov reconstructed tws grace rec v01 1985 2015 http rossa prod ap21 ethz ch delivery deliverymanagerservlet dps pid ie5766472 gimms ndvi https climatedataguide ucar edu climate data ndvi normalized difference vegetation index 3rd generation nasagfsc gimms era5 2 meter temperature and net radiation 10 24381 cds f17050d7 modis land cover classification https neo sci gsfc nasa gov view php datasetid mcd12c1 t1 we are thankful to the respective organizations for providing listed data the first author acknowledges support from the china scholarship council ministry of education china and flinders university australia for providing a phd scholarship appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128336 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2886,it is essential to have accurate and reliable daily inflow forecasting to improve short term hydropower scheduling this paper proposes a causal multivariate empirical mode decomposition ced framework as a complementary pre processing step for a day ahead inflow forecasting problem the idea behind ced is combining physics based causal inference with signal processing based decomposition to get the most relevant features among multiple time series to the inflow values the ced framework is validated for two areas in norway with different meteorological and hydrological conditions the validation results show that using ced as a pre processing step significantly enhances up to 70 the forecasting accuracy for various state of the art forecasting methods graphical abstract keywords inflow forecasting hydropower scheduling time series causal model multivariate decomposition machine learning information theory 1 introduction inflow forecasting improves operation and planning of hydropower stations while reducing the risk of flooding and reservoir rationing apaydin et al 2020 liao et al 2020 inflow forecasting approaches are categorized into two major groups 1 physical models and 2 data driven models physics based methods for inflow forecasting partly address nonlinearity and non stationary issues of inflow data using physical laws and catchment characteristics liao et al 2020 however the main challenge of such methods is their dependencies on initial conditions and input data bennett et al 2016 data driven methods for inflow forecasting include statistical and machine learning techniques such as linear regression lr kao et al 2015 fuzzy inference systems lohani et al 2014 spatial distribution based model tsai et al 2014 model tree jothiprakash and kote 2011 multilayer perception mlp golob et al 1998 coulibaly et al 2001 cheng et al 2015 abdellatif et al 2015 and support vector regression svr luo et al 2019 moazenzadeh et al 2018 tongal and booij 2018 in bordin et al 2020 a comprehensive review paper is provided in which all machine learning techniques used for inflow forecasting are listed recently deep learning methods such as long short term memory lstm apaydin et al 2020 kao et al 2020 herbert et al 2021 cheng et al 2021 recurrent neural network rnn ni et al 2020 sequential to sequential seq2seq network han et al 2021 kao et al 2021 yin et al 2021 and deep neural networks dnns have been used for inflow forecasting problems due to their capability to capture the nonlinearities and long term temporal dependencies learning yousefi et al 2020 however machine learning based models are facing some challenges for instance their performance is biased by training data and may suffer from over fitting information saturation and under fitting issues bai et al 2016 this is more problematic in the context of multivariate inflow forecasting problems where these techniques are unable to improve the inflow accurately when used as standalone models without any preprocessing techniques apaydin and sibtain 2021 feature extraction and selection as pre processing is key for any machine learning based forecasting model to address the issues mentioned above some prominent pre processing methods for time series analysis are inspired by the signal processing tool sets such as fast fourier transform fft wavelet transform wt empirical mode decomposition emd multivariate empirical mode decomposition memd etc for example wt has been used in hydrological studies for flood forecasting sehgal et al 2014a and river discharge forecasting sehgal et al 2014b however such methods are time consuming and require extensive computation power roushangar and alizadeh 2018 apaydin and sibtain 2021 in contrast to wt emd techniques work in time domain without the needing any preset basis functions or a mother wavelet emd can reduce the complexity of data set by breaking down the time series e g inflow historical values into different sub elements bai et al 2016 qi et al 2019 okkan and serbes 2013 apaydin and sibtain 2021 bai et al 2015 for example apaydin and sibtain 2021 proposes a multivariate streamflow forecasting model for inflow based on emd however the authors suggested an ad hoc process rather than an algorithmic approach to combine meteorological and hydrological data to forecast inflow in multivariate inflow forecasting decomposing original time series to its intrinsic mode functions imfs which carry different frequencies of the original series create a large set of features using large set of features without doing any feature selection techniques can introduce the risk of over fitting and information saturation in the machine learning process therefore feature selection techniques are usually used after decomposition techniques apaydin and sibtain 2021 bai et al 2016 in this paper we propose a causal inference based framework as a feature selection technique to find the most relevant features for inflow forecasting causal inference is an emerging concept within the machine learning community that characterizes the cause and effect relationship between distinct input and target variables based on directed acyclic graphs sriram et al 2018 this paper proposes a novel multivariate time series decomposition framework powered by causal inference for short term inflow forecasting application the proposed framework is referred to as causal empirical decomposition ced ced is a feature extraction and feature selection framework or a pre processing step that can be used for any forecasting problem with multiple input time series utilizing causal inference for time series feature extraction enables better exploitation of the inherent underlying dynamic behavior of a hydrological system to the best of the authors knowledge ced is the first attempt at causal based feature selection in inflow forecasting for hydropower applications the contribution of this paper are summarized as follows in terms of methodology a pre processing feature extracting and selection framework powered by decomposition and causality analysis is proposed that is applicable to any time series forecasting method from an application point of view the accuracy of short term inflow forecasting is considerably improved by using the proposed ced pre processing framework based on actual data from two different use cases in norway 2 use case all data used in this paper are collected from two main sources norway s regulatory agency nve and the norwegian center for climate services anon 0000a b two study areas in this paper as presented in fig 1 are located in vestlandet lærdal municipality and østlandet hemsedal municipality regions in norway the measurement stations and hydropower stations are presented with red and cyan circles respectively the first case study contains the borgund power station weather station filefjell kyrkjestølane and inflow measurement stations sula with the elevation of 1200 m likewise the second use case contains gjuva hydropower station inflow measurement station storeskar with the elevation of 850 m and weather stations hemsedal ski center and memesedal ii borgund hydropower plant was commissioned in 1974 with an installed capacity of 212 mw and annual production of 985 gwh gjuva hydropower plant located in hemsedal municipality has been in operation since 1957 with a maximum performance of 10 mw historical data from sula and storeskar inflow measurement stations are available from 1958 other data sources including meteorological and hydrological data are available from 2010 and 2013 for first and second use cases respectively the data set for both locations includes inflow m 3 s average temperature c maximum temperature c minimum temperature c precipitation mm snow depth cm and relative humidity the historical inflow measurements for two use cases are presented in fig 2 the inflow peak occurs at the end of spring or the beginning of summer when the temperature rises and causes snow to melt in these geographical locations there is also another peak at the beginning of fall due to increased precipitation furthermore the inflow variation in the second use case is significantly more notable than sula as its maximum inflow is 52 m 3 s while the maximum inflow for the sula is 16 72 m 3 s 3 methodology the overall architecture of the proposed ced framework together with the forecasting block is depicted in fig 3 the ced framework is comprised of four major steps decomposition information transfer modeling causal significant test and forecasting algorithms in the decomposition block the original time series is breaking down into three new features i e stochastic periodic and trend terms in this paper we have 7 time series with a length of n 3379 samples applied to the decomposition block the output is 21 new time series with the same length as the original time series the output of the decomposition step is sent to the second step which is a feature selection algorithm based on information theories in this step a greedy search algorithm is used to find the features lag values with the maximum contribution to the current state of inflow the objective function of the greedy search algorithm is to maximize the conditional mutual information and the transfer entropy the obtained causal candidate set is sent to the third step which is a pruning algorithm this step guarantees that the minimal subset of the causal candidate set is selected redundant variables are removed and only synergies variables are kept a causal network is obtained for each feature of inflow stochastic term periodic term and trend term these causal networks are used to forecast each feature of inflow once all three terms of inflow features are forecasted they are bundled to form the forecasted inflow values in the following subsections each step is briefly explained 3 1 step 1 time series decomposition this block utilizes a multivariate empirical mode decomposition memd method to split the original time series into intrinsic mode functions imfs while preserving all the original time series information memd is a self adaptive technique designed for nonlinear and non stationary data huang et al 2020 rehman and mandic 2010 memd decomposes a multivariate time series x m n including inflow precipitation snow depth relative humidity maximum temperature minimum temperature and average temperature where m is the number of variables n is the length of series and x m n is structured as follows 1 x m n x 1 x 2 x m x 1 1 x 1 2 x 1 n x 2 1 x 2 2 x 2 n x m 1 x m 2 x m n for the sake of better understanding the memd decomposition is expressed for only one variable of x m n as follows 2 x 1 j 0 m 1 i m f j 1 r m 1 j 0 m 1 i 0 n i m f j i 1 r m i 1 where i m f 1 r m n the residual r m is considered as the last imf is a two dimension sub matrix of a three dimension i m f matrix i m f i m f 1 i m f 2 i m f m i m f r m m n therefore each variable e g i m f 1 decomposed to m sub series i m f 1 1 i m f 2 1 i m f m 1 with the length of n same length with the original inflow after the decomposition of each variable fft analysis is applied on each of the imfs to sort them by three major features periodic stochastic and trend terms based on their frequencies the obtained periodic stochastic and trend features are fed into the forecasting engine to improve time series forecasting performance qi et al 2019 bai et al 2016 note that the obtained stochastic periodic and trend features are still in the time domain and not in the frequency domain each of these three features captures the underlying physical aspects the periodic term contains slow dynamics of an inflow time series depending on the seasonality and meteorological condition in a given geographical area the stochastic term represents the fast dynamics due to abrupt changes in meteorological parameters such as temperature and precipitation finally the trend term shows the average trend changes of the inflow during the decades showing the effect of long term phenomena such as global warming therefore variable x 1 can be represented by these three features as below 3 x 1 s x 1 p x 1 t x 1 where s x 1 p x 1 and t x 1 represent stochastic periodic and trend components of time series x 1 which are obtained from fft analysis as follows 4 s x 1 j 0 k i m f j 1 k 1 high frequencies p x 1 j k 1 k i m f j 1 k k m 1 mid frequencies t x 1 j k m 1 i m f j 1 r m 1 nearly zero frequencies where k is the number i m f j 1 that builds the stochastic term and k k is group of i m f j 1 that creates the periodic feature these parameters obtain from fft analysis as it is presented in the above equation the primary sub series of i m f 1 e g i m f 1 1 i m f 2 1 and i m f 3 1 represents high dynamics or high frequency components and it is sorted as the stochastic term the i m f 1 with nearly zero frequency are sorted as the trend term e g i m f m 1 1 and residual r m 1 and the rest of the sub series in i m f 1 constitute the periodic term 3 2 step 2 information transfer modeling this block employs a greedy search algorithm with a objective to maximize the transfer entropy between input features obtained from step 1 to the current target value e g inflow in this paper the greedy search algorithm searches among the decomposed features components with different time lags to find a causal candidate set for inflow features by maximizing information theory metrics such as transfer entropy te as its objective function te is based on conditional mutual information cmi cmi and te expression are explained as follows conditional mutual information cmi i x y z is similar to mutual information mi where all variables in an mi equation are conditioned to a third variable named z the cmi i x y z is an indicator that shows the uncertainty reduction in the variable y e g inflow from observing another variable x e g precipitation given the third variable z e g snow depth the cmi formulation is presented as novelli et al 2019 5 i y x z h y z h y x z 6 h y z y x z z p y z log 2 p y z p z where h y z and h y x z are the conditional entropy and p z and p y z are the marginal probability of z and joint probability between y and z respectively transfer entropy te t e x y is the degree of uncertainty reduction of variable y t by past values of x and y over and above the uncertainty reduction of y by its own past values alone bossomaier et al 2016 the expression of the t e x y is as novelli et al 2019 7 t e x y i y t x t 1 t k y t 1 t k h y t y t 1 t k h y t y t 1 t k x t 1 t k these metrics are used to identify the nonlinear and linear dependencies among different time series te is the transfer information among different source variables and a target variable i e inflow lizier 2012 in other words we find a minimal set of variables that collectively contribute to the target variable s next state the output of the second step is a subset of source variables and inflow with some time lags s s t in a predefined searching space for the set of candidates c s t that contribute to the current state of the inflow while fulfilling the statistical significance requirements finding the global optimal set among all the possible subsets of inflow time series together with other sources lagged time series is an np hard problem which exponentially grows with the size of the searching space sun et al 2015 therefore we use a sub optimal greedy search algorithm sun et al 2015 lizier 2012 for better clarification the second step is illustrated in algorithm 1 the greedy search algorithm starts with initializing s s t and c s t the algorithm selects inflow time series lags in i n f t c s t to perform self prediction wibral et al 2013 next it maximizes the cmi i c i n f t s s t at the beginning when s s t the value of i c i n f t s s t is equivalent to mutual information by adding one inflow lag to s s t the i c i n f t s s t will be equivalent to the cmi finally adding lagged time series from other sources turn the i c i n f t s s t into te this procedure is explained in algorithm 1 in fig 4 the procedure of obtaining s s t set is illustrated using a toy model in this example the greedy algorithm searches within the search space of l lags and it establishes the set of i n f t 1 i n f t 2 x 2 t 1 x 1 t 2 and x 2 t l as the causal candidate set s s t which contributes to the inflow current state i n f t 3 3 step 3 causal significance test in this step we check the causal strength of each variable in the causal candidate set s s t concerning the target variable inflow in our case by removing the influence of that variable in the set such a pruning process converges the inferred network to a causal network sun et al 2015 novelli et al 2019 the causal strength test is explained in algorithm 2 this process continues until all selected candidates in s s t are tested moreover an additional omnibus test have been done to secure the statistical significance against zero transfer entropy fig 5 depicts the outcome of step 3 on the ced framework for example x 2 t l is removed from obtained s s t in fig 5 because it is detected as a variable with non significant causal strength 3 4 step 4 forecasting in step 4 the obtained causal graph is used for target related inflow related feature selection then we feed significant causal features into a typical machine learning based forecasting algorithm in this paper we consider four widely used algorithms in the literature for inflow forecasting e g lr mlp rnn and lstm to save the space in the paper the description for each one of them is not provided here but can be find in qi et al 2019 abdellatif et al 2015 tayebiyan et al 2016 apaydin et al 2020 in addition these are the models developed in keras and sciket learn libraries pedregosa et al 2011 chollet et al 2015 4 results discussions the analysis is performed on a laptop with core i7 intel gpu 32 gb ram and nvidia geforce rtx 2080 the computation time of different scenarios ranges from less than one second for ced lr to 250 s for lstm with stand alone decomposition preprocessing techniques for training each model in addition the grid search algorithm is used for finding the hyperparameters of each models like lstm mlp and rnn for example the grid search set to the following ranges to find the optimal hyperparameters for the lstm model learning rate 0 1 0 01 0 001 activation functions relu tanh sigmoid first layer neurons 500 1000 1500 2000 second layer neurons 100 150 200 batch sizes 30 50 100 200 500 and optimizer adam adammax to save the space in the paper only one table of hyperparameters related to the sula area is presented see table 1 in the following sub sections the results are provided for each block presented in fig 3 the effectiveness of the proposed ced framework is validated for two different use cases hence the comparison is performed through two well known performance indicator the normalized root means square error nrmse and the coefficient of determination as follows 8 n r m e s t 1 n i n f t i n f t 2 inf 9 r 2 1 t 1 n i n f t i n f t 2 t 1 n i n f t inf 2 where i n f t and i n f t are actual inflow and forecasted inflow values at time t respectively inf is the expected value or mean of i n f nrmse is a normalized form of the root mean square error rmse the logic behind selecting rmse or mean square error mse is to penalize large error values which often occur during the peak of inflow exponentially more than many smaller errors for example 3 errors of 1 1 6 are significantly worse than 2 2 2 that lost value is 2 when a mean absolute error mae is used this property of assigning a higher loss value to larger errors is beneficial when more stability is wanted osberg 2020 in this paper for model training and validation mse is used to minimize large errors rather than minimizing small errors in inflow forecasting problems minimizing large errors have higher importance because of the cost of reservoir spillage and the environmental and social cost of the flood moreover nrmse and the coefficient of determination are used as the performance criteria for the validation of the models the best possible scores for r 2 and nrmse are 1 and 0 respectively the closer r 2 is to 1 or nrmse is to 0 the better trained the model is toward the unseen samples 4 1 multivariate decomposition the memd method is employed to decompose input variables time series into their imf sub series the projection number n u m d is a parameters of memd to extract proper number of i m f for a given time series rehman and mandic 2010 we select n u m d 256 based on our heuristic analysis and the literature rehman and mandic 2010 there are other parameters such as the stopping criteria condition for controlling the sifting process which is selected as a vector 0 05 0 05 0 05 as recommended in rilling et al 2003 the results of the multivariate decomposition block are presented for the inflow variable in fig 6 applying memd on inflow and other variables results in 21 sub series and a residual signal for the last imf as it can be seen in fig 6 c e and g the inflow stochastic feature is a summation of the seven first imf s i n f i m f 1 i n f i m f 2 i n f i m f 7 i n f the inflow s periodic feature is p i n f i m f 8 i n f i m f 9 i n f i m f 16 i n f and the inflow s trend feature is t i n f i m f 17 i n f i m f 18 i n f r i n f usually the first i m f i n f includes the highest dynamics of a time series while the last imf r i n f contains the lowest dynamics the s i n f p i n f and t i n f in fig 6 are obtained from the fft analysis by sorting the imfs based on their similar frequencies to form stochastic periodic and trend features of the inflow the fft analysis was applied to all variables time series but for visualization only inflow s fft conversion featured is presented in fig 6 b d f and h it shows that high frequencies are sorted in the stochastic feature representing daily dynamics with dominant frequencies from 0 1 to 0 5 1 d a y such dynamics range from one to two days the periodic features carry the main frequencies from 0 0027 to 0 03 1 d a y representing time series with yearly half yearly seasonal and monthly periods the last feature is the trend which contains nearly zero frequencies generally forecasting trend t x and periodic p x features are not challenging for ml forecasting algorithms because they are less noisy on the contrary the most challenging part is forecasting stochastic features because they are nonlinear and noisy therefore adding supplementary variables from meteorological and hydrological data such as temperature snow depth or precipitation may reduce the stochastic inflow feature forecasting uncertainties hence causality based analysis is essential to find the best set of variables that can improve the inflow forecasting performance a sensitivity analysis is provided in section 4 3 in which the forecasting accuracy of inflow features are compared with ced and without ced in table 3 and fig 9 4 2 causal graph from inflow features the feature decomposition block as explained in section 3 1 creates a data set with 21 variables see table 2 moreover some of these variables with a certain delay can significantly impact the inflow forecasting problem therefore these variables with their different lags need to be taken into account thereby let us assume all variables are markovian with l 30 lags a searching space with the size of 630 30 21 can be created however feeding all these variables to the ml forecasting algorithms may cause over fitting problem thus finding the correct variables with the right lags which can maximize the improvement impact on inflow forecasting performance is a key that can be done by steps 2 and 3 in the ced framework we assume 30 days for the maximum lag l which is based on our heuristic analysis and input from hydropower experts as explained in steps 2 and 3 of the ced algorithm in the methodology step 3 is a small set of variables out of the 630 variables based on their significant causal relationship with the inflow we illustrate the resultant causal models for the sula and storeskar use cases in fig 7 the dependencies between the original inflow time series with its lag values and the lags of other time series variables are presented in fig 7 a it shows the average temperature precipitation and humidity time series with their first lag one day ago values have the most contribution in forecasting the current state of the inflow moreover the first four lags of the historical inflow can significantly contribute to the current state of the inflow fig 7 b and c show the causal relationship between decomposed inflow features at the current time s i n f t t i n f t and p i n f t with their lags and lags of other resource variables for sula and storeskar areas respectively fig 7 b shows that the stochastic features of precipitation and humidity from the last day highly affect the stochastic inflow values at the current state moreover the inflow s stochastic feature has significant dependencies on its own recorded values until 15 days ago in addition the periodic feature of inflow at the current time depends on the p s n o t 1 p p r t 1 and s r h t 2 values this is because the periodic pattern of inflow shows the seasonality behavior including snowy snow melting rainy and dry periods since the dynamic in the inflow s periodic feature is lower than the stochastic feature its dependency on its past values is lower depends on its six lags than the stochastic feature in addition the trend feature of the inflow has almost no dynamic so it does not exhibit directed information with any exogenous variables in our data set instead it only depends on its first two past values another interesting observation from fig 7 b is that decomposition of inflow and other weather related enables us to explore more causal relationships between various components of input data and the current state of inflow for example snow depth was not found in causal graph without decomposition in fig 7 a while the snow depth periodic feature has an impact on the inflow periodic feature the decomposition process denoises and detrends the original signals strengthening steps 2 and 3 to discover more information rich variables for the inflow forecasting problem moreover the resultant causal model for decomposed values from the second use case storeskar area is more distinguishable from our first use case sula area see fig 7 c however the causal graph without decomposition step looks very similar for both use cases in fig 7 a in the next step the obtained causal graphs for each feature of inflow are employed for training different ml tools to forecast the inflow features 4 3 inflow forecasting in this step forecasting techniques such as lr mlp rnn and lstm are used to forecast s i n f p i n f and t i n f components for training and testing of the forecasting algorithms actual data from the case study areas presented in fig 1 are divided into three parts training validation and testing the period from 01 06 2019 to 31 12 2019 is selected as the testing data and the remaining time is selected as training and validation data sets with 90 and 10 ratio respectively the inputs for lr mlp rnn and lstm without ced is 30 past values of all the input variables presented in section 2 while the inputs with ced is obtained in section 4 2 and presented in fig 7 for each feature and use cases after training the model it is validated with the test data set as an example the results of inflow features forecasting are illustrated for the sula area using the lr model in fig 8 a b and c fig 8 shows that the obtained models accurately forecast the inflow features the scatter plot also shows a good match between the realized values and the forecasted values especially for the periodic and trend terms where r 2 0 99 the forecasted inflow is the summation of forecasted values of its stochastic periodic and trend terms see fig 8 d as it is observed the forecast performance of ced lr for sula area is better than storeskar area look at their r 2 scores because the inflow changes and nonlinearities in storeskar is much higher than sula area as explained and presented in section 2 and fig 2 respectively for the sake of better understanding the role of ced in improving forecasting accuracy of inflow features a sensitivity analysis have been done for two scenarios with ced mlp and mlp without ced stand alone mlp to show which feature of inflow is more challenging the results are provided in table 3 and fig 9 according to table 3 the accuracy of both trend and periodic features significantly improves by 66 and 97 5 for the rmse when ced is used as a pre processing step for mlp as presented in fig 9 both trend and periodic futures are less nonlinear and noisy represent nearly zero and slow inflow frequencies the challenging feature is the stochastic feature which has the fastest dynamics of inflow however using ced can improve the accuracy of the stochastic feature by almost 37 5 the next step is to validate the performance of the proposed ced framework by comparing it with other forecasting algorithms 4 4 ced validation to validate the performance of our proposed ced framework for time series feature selection we use it as a pre processing step with a number of widely used forecasting algorithms such as lr mlp rnn and lstm then the results are compared with the baseline no pre processing case the results in table 4 show that ced pre processing improves forecasting accuracy across different forecasting methods however in the case of using only decomposition pre processing removing steps 2 and 3 in fig 3 or only causal pre processing removing step 1 in fig 3 the inflow forecasting accuracy is less than using ced pre processing for example the ced pre processing with lstm forecasting method results in 70 64 27 relative improvement in reducing forecasting error nrmse in comparison to no pre processing decomposition pre processing and causal based pre processing respectively the improvement trend is also similar for the r 2 score as the ced mlp improves the forecasting accuracy by 30 compared with no preprocessing mlp it may be surprising why the lr outperforms the other models in the first column of table 4 using deep learning methods in a multivariate inflow forecasting problem when input is the raw data without pre processing feature extraction or selection increases the risk of over fitting and information saturation which might result in poorly trained models according to table 4 when only causality is used as the pre processing step the lstm performance outperforms all the other models however when only the decomposition technique is used lr outperforms all the others because the decomposition technique reduces the complexity and non linearities of the forecasting problem by breaking down the original time series into three terms stochastic periodic and trend while causality reduces the risk of over fitting and information saturation of the forecasting problem by finding the best informative lag values of each source moreover stand alone causality techniques may fail to distinguish crucial exogenous parameters in input data for the forecasting purpose see section 4 2 our proposed hybrid ced framework takes advantage of both techniques strengths and improves the inflow forecasting model s performance last but not least the ced is implemented for the second use case to validate the ced scalability for the inflow forecasting problem the results of the second use case are presented in table 5 we observe a similar enhancement in inflow forecasting for the second area using a ced feature selection framework it is worth mentioning some of the interesting observations in the presented results in tables 4 and 5 as follows feeding multiple time series from different domains e g meteorological hydrological etc into an inflow forecasting algorithm without pre processing does not necessarily improve the forecasting results our proposed ced framework highly improves inflow forecasting results with down selecting the most informative part of input data we picked four different forecasting methods ranging from classical algorithms such as lr and mlp to more complex techniques like rnn and lstm for ced validation all the selected state of the art forecasting methods exhibit relatively performance enhancement after using the ced pre processing step ced can serve as a complementary pre processing step for any forecasting method inflow forecasting results using ced mlp and ced lstm framework in comparison with mlp and lstm no pre processing are shown in fig 10 a and b for sula and storeskar use cases respectively as it is presented both ced mlp and ced lstm have a much better performance in forecasting the real values of inflow for different use cases another interesting observation in fig 10 is that ced mlp and ced lstm accurately forecast inflow s peak values which is a critical and challenging task for state of the art inflow forecasting methods and has significant consequences for short term hydropower scheduling another interesting aspect is to present the sensitivity of the ced algorithm to the size of training horizons and how often the model should be updated and retrained accordingly the sensitivity result of the ced mlp is presented in fig 11 with different lengths of historical data 2 months 6 months 1 year 5 years 10 years for day ahead inflow forecasting it shows after 5 years the forecast error improvement is marginal therefore 5 to 10 years of historical data is good enough to make a causal model in our use case it is worth mentioning that climate change may alter the historical temperature and precipitation patterns in different parts of the world therefore from a practical point of view it is worthwhile to update the causal models and retrain the model annually using a five to ten years historical horizon 5 practical consideration from a practical point of view accurate inflow forecasting models can significantly improve the hydropower scheduling performance flooding control measures water drinking and agriculture efficiency in this paper we focused on daily inflow forecasting mainly for short term hydro power scheduling problems with large storage reservoirs in norway usually most of the precipitation in norway especially close to the glaciers is in the form of snow looking at the historical values of inflow for two different use cases in fig 2 shows that the peak of inflow always happens at the end of spring or early summer when the temperature rise and snow starts to melt snow melting may occur in two weeks time period therefore it is crucial to estimate these periods precisely to avoid the risk of spillage by providing enough space for the maximum harnessing of inflow to the reservoirs our proposed ced pre processing framework enables the forecasting models to discover the dependencies of inflow to the lag values of other variables such as snow dept temperature and precipitation this feature improves the estimation of this period which is a critical factor in inflow forecasting in norway fig 10 a and b show that using ced pre processing with forecaster models lstm and mlp improves the forecasting accuracy at peak values in addition to meteorological data that we used in this paper other variables such as topography soil characteristics etc can impact the inflow forecasting the proposed ced pre processing framework is expandable to a higher number of input source time series more input variables as a direction for future works we will consider expanding source values beyond what we used in this paper 6 conclusion this paper proposes a causal multivariate empirical mode decomposition ced framework for multiple time series feature selection in other words ced is a pre processing step that is complementary to any machine learning based forecasting method we specifically developed ced for the inflow forecasting problem by taking advantage of both decomposition and causal inference approaches for time series at the same time we overcome the inherent weaknesses in both approaches in other words ced combines physics based causal inference with signal processing based decomposition to get the more relevant features among multiple time series to the target variable the inflow our proposed ced framework is validated for two different use cases related to hydropower reservoirs in norway the validation results show that using ced as a pre processing step significantly enhances the forecasting accuracy for various forecasting methods for example adding ced as a pre processing step to lstm and lr improved forecasting results by almost 70 we are trying to use more hydrological parameters as input and expand the ced framework s inflow forecasting horizon for future work credit authorship contribution statement mojtaba yousefi writing original draft writing review editing visualization methodology validation software investigation xiaomei cheng collecting data sorting data and cleaning data michele gazzea assisting in visualization august hubert wierling writing review editing providing technical comments and supervising jayaprakash rajasekharan writing review editing providing technical comments arild helseth providing technical and domain knowledge hossein farahmand project coordination project administration providing technical and domain knowledge writing review editing reza arghandeh supervision providing technical comment writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is partly supported by the research council of norway grant number 309997 and internal fund from western norway university of applied sciences hvl university project number 1030730 
2886,it is essential to have accurate and reliable daily inflow forecasting to improve short term hydropower scheduling this paper proposes a causal multivariate empirical mode decomposition ced framework as a complementary pre processing step for a day ahead inflow forecasting problem the idea behind ced is combining physics based causal inference with signal processing based decomposition to get the most relevant features among multiple time series to the inflow values the ced framework is validated for two areas in norway with different meteorological and hydrological conditions the validation results show that using ced as a pre processing step significantly enhances up to 70 the forecasting accuracy for various state of the art forecasting methods graphical abstract keywords inflow forecasting hydropower scheduling time series causal model multivariate decomposition machine learning information theory 1 introduction inflow forecasting improves operation and planning of hydropower stations while reducing the risk of flooding and reservoir rationing apaydin et al 2020 liao et al 2020 inflow forecasting approaches are categorized into two major groups 1 physical models and 2 data driven models physics based methods for inflow forecasting partly address nonlinearity and non stationary issues of inflow data using physical laws and catchment characteristics liao et al 2020 however the main challenge of such methods is their dependencies on initial conditions and input data bennett et al 2016 data driven methods for inflow forecasting include statistical and machine learning techniques such as linear regression lr kao et al 2015 fuzzy inference systems lohani et al 2014 spatial distribution based model tsai et al 2014 model tree jothiprakash and kote 2011 multilayer perception mlp golob et al 1998 coulibaly et al 2001 cheng et al 2015 abdellatif et al 2015 and support vector regression svr luo et al 2019 moazenzadeh et al 2018 tongal and booij 2018 in bordin et al 2020 a comprehensive review paper is provided in which all machine learning techniques used for inflow forecasting are listed recently deep learning methods such as long short term memory lstm apaydin et al 2020 kao et al 2020 herbert et al 2021 cheng et al 2021 recurrent neural network rnn ni et al 2020 sequential to sequential seq2seq network han et al 2021 kao et al 2021 yin et al 2021 and deep neural networks dnns have been used for inflow forecasting problems due to their capability to capture the nonlinearities and long term temporal dependencies learning yousefi et al 2020 however machine learning based models are facing some challenges for instance their performance is biased by training data and may suffer from over fitting information saturation and under fitting issues bai et al 2016 this is more problematic in the context of multivariate inflow forecasting problems where these techniques are unable to improve the inflow accurately when used as standalone models without any preprocessing techniques apaydin and sibtain 2021 feature extraction and selection as pre processing is key for any machine learning based forecasting model to address the issues mentioned above some prominent pre processing methods for time series analysis are inspired by the signal processing tool sets such as fast fourier transform fft wavelet transform wt empirical mode decomposition emd multivariate empirical mode decomposition memd etc for example wt has been used in hydrological studies for flood forecasting sehgal et al 2014a and river discharge forecasting sehgal et al 2014b however such methods are time consuming and require extensive computation power roushangar and alizadeh 2018 apaydin and sibtain 2021 in contrast to wt emd techniques work in time domain without the needing any preset basis functions or a mother wavelet emd can reduce the complexity of data set by breaking down the time series e g inflow historical values into different sub elements bai et al 2016 qi et al 2019 okkan and serbes 2013 apaydin and sibtain 2021 bai et al 2015 for example apaydin and sibtain 2021 proposes a multivariate streamflow forecasting model for inflow based on emd however the authors suggested an ad hoc process rather than an algorithmic approach to combine meteorological and hydrological data to forecast inflow in multivariate inflow forecasting decomposing original time series to its intrinsic mode functions imfs which carry different frequencies of the original series create a large set of features using large set of features without doing any feature selection techniques can introduce the risk of over fitting and information saturation in the machine learning process therefore feature selection techniques are usually used after decomposition techniques apaydin and sibtain 2021 bai et al 2016 in this paper we propose a causal inference based framework as a feature selection technique to find the most relevant features for inflow forecasting causal inference is an emerging concept within the machine learning community that characterizes the cause and effect relationship between distinct input and target variables based on directed acyclic graphs sriram et al 2018 this paper proposes a novel multivariate time series decomposition framework powered by causal inference for short term inflow forecasting application the proposed framework is referred to as causal empirical decomposition ced ced is a feature extraction and feature selection framework or a pre processing step that can be used for any forecasting problem with multiple input time series utilizing causal inference for time series feature extraction enables better exploitation of the inherent underlying dynamic behavior of a hydrological system to the best of the authors knowledge ced is the first attempt at causal based feature selection in inflow forecasting for hydropower applications the contribution of this paper are summarized as follows in terms of methodology a pre processing feature extracting and selection framework powered by decomposition and causality analysis is proposed that is applicable to any time series forecasting method from an application point of view the accuracy of short term inflow forecasting is considerably improved by using the proposed ced pre processing framework based on actual data from two different use cases in norway 2 use case all data used in this paper are collected from two main sources norway s regulatory agency nve and the norwegian center for climate services anon 0000a b two study areas in this paper as presented in fig 1 are located in vestlandet lærdal municipality and østlandet hemsedal municipality regions in norway the measurement stations and hydropower stations are presented with red and cyan circles respectively the first case study contains the borgund power station weather station filefjell kyrkjestølane and inflow measurement stations sula with the elevation of 1200 m likewise the second use case contains gjuva hydropower station inflow measurement station storeskar with the elevation of 850 m and weather stations hemsedal ski center and memesedal ii borgund hydropower plant was commissioned in 1974 with an installed capacity of 212 mw and annual production of 985 gwh gjuva hydropower plant located in hemsedal municipality has been in operation since 1957 with a maximum performance of 10 mw historical data from sula and storeskar inflow measurement stations are available from 1958 other data sources including meteorological and hydrological data are available from 2010 and 2013 for first and second use cases respectively the data set for both locations includes inflow m 3 s average temperature c maximum temperature c minimum temperature c precipitation mm snow depth cm and relative humidity the historical inflow measurements for two use cases are presented in fig 2 the inflow peak occurs at the end of spring or the beginning of summer when the temperature rises and causes snow to melt in these geographical locations there is also another peak at the beginning of fall due to increased precipitation furthermore the inflow variation in the second use case is significantly more notable than sula as its maximum inflow is 52 m 3 s while the maximum inflow for the sula is 16 72 m 3 s 3 methodology the overall architecture of the proposed ced framework together with the forecasting block is depicted in fig 3 the ced framework is comprised of four major steps decomposition information transfer modeling causal significant test and forecasting algorithms in the decomposition block the original time series is breaking down into three new features i e stochastic periodic and trend terms in this paper we have 7 time series with a length of n 3379 samples applied to the decomposition block the output is 21 new time series with the same length as the original time series the output of the decomposition step is sent to the second step which is a feature selection algorithm based on information theories in this step a greedy search algorithm is used to find the features lag values with the maximum contribution to the current state of inflow the objective function of the greedy search algorithm is to maximize the conditional mutual information and the transfer entropy the obtained causal candidate set is sent to the third step which is a pruning algorithm this step guarantees that the minimal subset of the causal candidate set is selected redundant variables are removed and only synergies variables are kept a causal network is obtained for each feature of inflow stochastic term periodic term and trend term these causal networks are used to forecast each feature of inflow once all three terms of inflow features are forecasted they are bundled to form the forecasted inflow values in the following subsections each step is briefly explained 3 1 step 1 time series decomposition this block utilizes a multivariate empirical mode decomposition memd method to split the original time series into intrinsic mode functions imfs while preserving all the original time series information memd is a self adaptive technique designed for nonlinear and non stationary data huang et al 2020 rehman and mandic 2010 memd decomposes a multivariate time series x m n including inflow precipitation snow depth relative humidity maximum temperature minimum temperature and average temperature where m is the number of variables n is the length of series and x m n is structured as follows 1 x m n x 1 x 2 x m x 1 1 x 1 2 x 1 n x 2 1 x 2 2 x 2 n x m 1 x m 2 x m n for the sake of better understanding the memd decomposition is expressed for only one variable of x m n as follows 2 x 1 j 0 m 1 i m f j 1 r m 1 j 0 m 1 i 0 n i m f j i 1 r m i 1 where i m f 1 r m n the residual r m is considered as the last imf is a two dimension sub matrix of a three dimension i m f matrix i m f i m f 1 i m f 2 i m f m i m f r m m n therefore each variable e g i m f 1 decomposed to m sub series i m f 1 1 i m f 2 1 i m f m 1 with the length of n same length with the original inflow after the decomposition of each variable fft analysis is applied on each of the imfs to sort them by three major features periodic stochastic and trend terms based on their frequencies the obtained periodic stochastic and trend features are fed into the forecasting engine to improve time series forecasting performance qi et al 2019 bai et al 2016 note that the obtained stochastic periodic and trend features are still in the time domain and not in the frequency domain each of these three features captures the underlying physical aspects the periodic term contains slow dynamics of an inflow time series depending on the seasonality and meteorological condition in a given geographical area the stochastic term represents the fast dynamics due to abrupt changes in meteorological parameters such as temperature and precipitation finally the trend term shows the average trend changes of the inflow during the decades showing the effect of long term phenomena such as global warming therefore variable x 1 can be represented by these three features as below 3 x 1 s x 1 p x 1 t x 1 where s x 1 p x 1 and t x 1 represent stochastic periodic and trend components of time series x 1 which are obtained from fft analysis as follows 4 s x 1 j 0 k i m f j 1 k 1 high frequencies p x 1 j k 1 k i m f j 1 k k m 1 mid frequencies t x 1 j k m 1 i m f j 1 r m 1 nearly zero frequencies where k is the number i m f j 1 that builds the stochastic term and k k is group of i m f j 1 that creates the periodic feature these parameters obtain from fft analysis as it is presented in the above equation the primary sub series of i m f 1 e g i m f 1 1 i m f 2 1 and i m f 3 1 represents high dynamics or high frequency components and it is sorted as the stochastic term the i m f 1 with nearly zero frequency are sorted as the trend term e g i m f m 1 1 and residual r m 1 and the rest of the sub series in i m f 1 constitute the periodic term 3 2 step 2 information transfer modeling this block employs a greedy search algorithm with a objective to maximize the transfer entropy between input features obtained from step 1 to the current target value e g inflow in this paper the greedy search algorithm searches among the decomposed features components with different time lags to find a causal candidate set for inflow features by maximizing information theory metrics such as transfer entropy te as its objective function te is based on conditional mutual information cmi cmi and te expression are explained as follows conditional mutual information cmi i x y z is similar to mutual information mi where all variables in an mi equation are conditioned to a third variable named z the cmi i x y z is an indicator that shows the uncertainty reduction in the variable y e g inflow from observing another variable x e g precipitation given the third variable z e g snow depth the cmi formulation is presented as novelli et al 2019 5 i y x z h y z h y x z 6 h y z y x z z p y z log 2 p y z p z where h y z and h y x z are the conditional entropy and p z and p y z are the marginal probability of z and joint probability between y and z respectively transfer entropy te t e x y is the degree of uncertainty reduction of variable y t by past values of x and y over and above the uncertainty reduction of y by its own past values alone bossomaier et al 2016 the expression of the t e x y is as novelli et al 2019 7 t e x y i y t x t 1 t k y t 1 t k h y t y t 1 t k h y t y t 1 t k x t 1 t k these metrics are used to identify the nonlinear and linear dependencies among different time series te is the transfer information among different source variables and a target variable i e inflow lizier 2012 in other words we find a minimal set of variables that collectively contribute to the target variable s next state the output of the second step is a subset of source variables and inflow with some time lags s s t in a predefined searching space for the set of candidates c s t that contribute to the current state of the inflow while fulfilling the statistical significance requirements finding the global optimal set among all the possible subsets of inflow time series together with other sources lagged time series is an np hard problem which exponentially grows with the size of the searching space sun et al 2015 therefore we use a sub optimal greedy search algorithm sun et al 2015 lizier 2012 for better clarification the second step is illustrated in algorithm 1 the greedy search algorithm starts with initializing s s t and c s t the algorithm selects inflow time series lags in i n f t c s t to perform self prediction wibral et al 2013 next it maximizes the cmi i c i n f t s s t at the beginning when s s t the value of i c i n f t s s t is equivalent to mutual information by adding one inflow lag to s s t the i c i n f t s s t will be equivalent to the cmi finally adding lagged time series from other sources turn the i c i n f t s s t into te this procedure is explained in algorithm 1 in fig 4 the procedure of obtaining s s t set is illustrated using a toy model in this example the greedy algorithm searches within the search space of l lags and it establishes the set of i n f t 1 i n f t 2 x 2 t 1 x 1 t 2 and x 2 t l as the causal candidate set s s t which contributes to the inflow current state i n f t 3 3 step 3 causal significance test in this step we check the causal strength of each variable in the causal candidate set s s t concerning the target variable inflow in our case by removing the influence of that variable in the set such a pruning process converges the inferred network to a causal network sun et al 2015 novelli et al 2019 the causal strength test is explained in algorithm 2 this process continues until all selected candidates in s s t are tested moreover an additional omnibus test have been done to secure the statistical significance against zero transfer entropy fig 5 depicts the outcome of step 3 on the ced framework for example x 2 t l is removed from obtained s s t in fig 5 because it is detected as a variable with non significant causal strength 3 4 step 4 forecasting in step 4 the obtained causal graph is used for target related inflow related feature selection then we feed significant causal features into a typical machine learning based forecasting algorithm in this paper we consider four widely used algorithms in the literature for inflow forecasting e g lr mlp rnn and lstm to save the space in the paper the description for each one of them is not provided here but can be find in qi et al 2019 abdellatif et al 2015 tayebiyan et al 2016 apaydin et al 2020 in addition these are the models developed in keras and sciket learn libraries pedregosa et al 2011 chollet et al 2015 4 results discussions the analysis is performed on a laptop with core i7 intel gpu 32 gb ram and nvidia geforce rtx 2080 the computation time of different scenarios ranges from less than one second for ced lr to 250 s for lstm with stand alone decomposition preprocessing techniques for training each model in addition the grid search algorithm is used for finding the hyperparameters of each models like lstm mlp and rnn for example the grid search set to the following ranges to find the optimal hyperparameters for the lstm model learning rate 0 1 0 01 0 001 activation functions relu tanh sigmoid first layer neurons 500 1000 1500 2000 second layer neurons 100 150 200 batch sizes 30 50 100 200 500 and optimizer adam adammax to save the space in the paper only one table of hyperparameters related to the sula area is presented see table 1 in the following sub sections the results are provided for each block presented in fig 3 the effectiveness of the proposed ced framework is validated for two different use cases hence the comparison is performed through two well known performance indicator the normalized root means square error nrmse and the coefficient of determination as follows 8 n r m e s t 1 n i n f t i n f t 2 inf 9 r 2 1 t 1 n i n f t i n f t 2 t 1 n i n f t inf 2 where i n f t and i n f t are actual inflow and forecasted inflow values at time t respectively inf is the expected value or mean of i n f nrmse is a normalized form of the root mean square error rmse the logic behind selecting rmse or mean square error mse is to penalize large error values which often occur during the peak of inflow exponentially more than many smaller errors for example 3 errors of 1 1 6 are significantly worse than 2 2 2 that lost value is 2 when a mean absolute error mae is used this property of assigning a higher loss value to larger errors is beneficial when more stability is wanted osberg 2020 in this paper for model training and validation mse is used to minimize large errors rather than minimizing small errors in inflow forecasting problems minimizing large errors have higher importance because of the cost of reservoir spillage and the environmental and social cost of the flood moreover nrmse and the coefficient of determination are used as the performance criteria for the validation of the models the best possible scores for r 2 and nrmse are 1 and 0 respectively the closer r 2 is to 1 or nrmse is to 0 the better trained the model is toward the unseen samples 4 1 multivariate decomposition the memd method is employed to decompose input variables time series into their imf sub series the projection number n u m d is a parameters of memd to extract proper number of i m f for a given time series rehman and mandic 2010 we select n u m d 256 based on our heuristic analysis and the literature rehman and mandic 2010 there are other parameters such as the stopping criteria condition for controlling the sifting process which is selected as a vector 0 05 0 05 0 05 as recommended in rilling et al 2003 the results of the multivariate decomposition block are presented for the inflow variable in fig 6 applying memd on inflow and other variables results in 21 sub series and a residual signal for the last imf as it can be seen in fig 6 c e and g the inflow stochastic feature is a summation of the seven first imf s i n f i m f 1 i n f i m f 2 i n f i m f 7 i n f the inflow s periodic feature is p i n f i m f 8 i n f i m f 9 i n f i m f 16 i n f and the inflow s trend feature is t i n f i m f 17 i n f i m f 18 i n f r i n f usually the first i m f i n f includes the highest dynamics of a time series while the last imf r i n f contains the lowest dynamics the s i n f p i n f and t i n f in fig 6 are obtained from the fft analysis by sorting the imfs based on their similar frequencies to form stochastic periodic and trend features of the inflow the fft analysis was applied to all variables time series but for visualization only inflow s fft conversion featured is presented in fig 6 b d f and h it shows that high frequencies are sorted in the stochastic feature representing daily dynamics with dominant frequencies from 0 1 to 0 5 1 d a y such dynamics range from one to two days the periodic features carry the main frequencies from 0 0027 to 0 03 1 d a y representing time series with yearly half yearly seasonal and monthly periods the last feature is the trend which contains nearly zero frequencies generally forecasting trend t x and periodic p x features are not challenging for ml forecasting algorithms because they are less noisy on the contrary the most challenging part is forecasting stochastic features because they are nonlinear and noisy therefore adding supplementary variables from meteorological and hydrological data such as temperature snow depth or precipitation may reduce the stochastic inflow feature forecasting uncertainties hence causality based analysis is essential to find the best set of variables that can improve the inflow forecasting performance a sensitivity analysis is provided in section 4 3 in which the forecasting accuracy of inflow features are compared with ced and without ced in table 3 and fig 9 4 2 causal graph from inflow features the feature decomposition block as explained in section 3 1 creates a data set with 21 variables see table 2 moreover some of these variables with a certain delay can significantly impact the inflow forecasting problem therefore these variables with their different lags need to be taken into account thereby let us assume all variables are markovian with l 30 lags a searching space with the size of 630 30 21 can be created however feeding all these variables to the ml forecasting algorithms may cause over fitting problem thus finding the correct variables with the right lags which can maximize the improvement impact on inflow forecasting performance is a key that can be done by steps 2 and 3 in the ced framework we assume 30 days for the maximum lag l which is based on our heuristic analysis and input from hydropower experts as explained in steps 2 and 3 of the ced algorithm in the methodology step 3 is a small set of variables out of the 630 variables based on their significant causal relationship with the inflow we illustrate the resultant causal models for the sula and storeskar use cases in fig 7 the dependencies between the original inflow time series with its lag values and the lags of other time series variables are presented in fig 7 a it shows the average temperature precipitation and humidity time series with their first lag one day ago values have the most contribution in forecasting the current state of the inflow moreover the first four lags of the historical inflow can significantly contribute to the current state of the inflow fig 7 b and c show the causal relationship between decomposed inflow features at the current time s i n f t t i n f t and p i n f t with their lags and lags of other resource variables for sula and storeskar areas respectively fig 7 b shows that the stochastic features of precipitation and humidity from the last day highly affect the stochastic inflow values at the current state moreover the inflow s stochastic feature has significant dependencies on its own recorded values until 15 days ago in addition the periodic feature of inflow at the current time depends on the p s n o t 1 p p r t 1 and s r h t 2 values this is because the periodic pattern of inflow shows the seasonality behavior including snowy snow melting rainy and dry periods since the dynamic in the inflow s periodic feature is lower than the stochastic feature its dependency on its past values is lower depends on its six lags than the stochastic feature in addition the trend feature of the inflow has almost no dynamic so it does not exhibit directed information with any exogenous variables in our data set instead it only depends on its first two past values another interesting observation from fig 7 b is that decomposition of inflow and other weather related enables us to explore more causal relationships between various components of input data and the current state of inflow for example snow depth was not found in causal graph without decomposition in fig 7 a while the snow depth periodic feature has an impact on the inflow periodic feature the decomposition process denoises and detrends the original signals strengthening steps 2 and 3 to discover more information rich variables for the inflow forecasting problem moreover the resultant causal model for decomposed values from the second use case storeskar area is more distinguishable from our first use case sula area see fig 7 c however the causal graph without decomposition step looks very similar for both use cases in fig 7 a in the next step the obtained causal graphs for each feature of inflow are employed for training different ml tools to forecast the inflow features 4 3 inflow forecasting in this step forecasting techniques such as lr mlp rnn and lstm are used to forecast s i n f p i n f and t i n f components for training and testing of the forecasting algorithms actual data from the case study areas presented in fig 1 are divided into three parts training validation and testing the period from 01 06 2019 to 31 12 2019 is selected as the testing data and the remaining time is selected as training and validation data sets with 90 and 10 ratio respectively the inputs for lr mlp rnn and lstm without ced is 30 past values of all the input variables presented in section 2 while the inputs with ced is obtained in section 4 2 and presented in fig 7 for each feature and use cases after training the model it is validated with the test data set as an example the results of inflow features forecasting are illustrated for the sula area using the lr model in fig 8 a b and c fig 8 shows that the obtained models accurately forecast the inflow features the scatter plot also shows a good match between the realized values and the forecasted values especially for the periodic and trend terms where r 2 0 99 the forecasted inflow is the summation of forecasted values of its stochastic periodic and trend terms see fig 8 d as it is observed the forecast performance of ced lr for sula area is better than storeskar area look at their r 2 scores because the inflow changes and nonlinearities in storeskar is much higher than sula area as explained and presented in section 2 and fig 2 respectively for the sake of better understanding the role of ced in improving forecasting accuracy of inflow features a sensitivity analysis have been done for two scenarios with ced mlp and mlp without ced stand alone mlp to show which feature of inflow is more challenging the results are provided in table 3 and fig 9 according to table 3 the accuracy of both trend and periodic features significantly improves by 66 and 97 5 for the rmse when ced is used as a pre processing step for mlp as presented in fig 9 both trend and periodic futures are less nonlinear and noisy represent nearly zero and slow inflow frequencies the challenging feature is the stochastic feature which has the fastest dynamics of inflow however using ced can improve the accuracy of the stochastic feature by almost 37 5 the next step is to validate the performance of the proposed ced framework by comparing it with other forecasting algorithms 4 4 ced validation to validate the performance of our proposed ced framework for time series feature selection we use it as a pre processing step with a number of widely used forecasting algorithms such as lr mlp rnn and lstm then the results are compared with the baseline no pre processing case the results in table 4 show that ced pre processing improves forecasting accuracy across different forecasting methods however in the case of using only decomposition pre processing removing steps 2 and 3 in fig 3 or only causal pre processing removing step 1 in fig 3 the inflow forecasting accuracy is less than using ced pre processing for example the ced pre processing with lstm forecasting method results in 70 64 27 relative improvement in reducing forecasting error nrmse in comparison to no pre processing decomposition pre processing and causal based pre processing respectively the improvement trend is also similar for the r 2 score as the ced mlp improves the forecasting accuracy by 30 compared with no preprocessing mlp it may be surprising why the lr outperforms the other models in the first column of table 4 using deep learning methods in a multivariate inflow forecasting problem when input is the raw data without pre processing feature extraction or selection increases the risk of over fitting and information saturation which might result in poorly trained models according to table 4 when only causality is used as the pre processing step the lstm performance outperforms all the other models however when only the decomposition technique is used lr outperforms all the others because the decomposition technique reduces the complexity and non linearities of the forecasting problem by breaking down the original time series into three terms stochastic periodic and trend while causality reduces the risk of over fitting and information saturation of the forecasting problem by finding the best informative lag values of each source moreover stand alone causality techniques may fail to distinguish crucial exogenous parameters in input data for the forecasting purpose see section 4 2 our proposed hybrid ced framework takes advantage of both techniques strengths and improves the inflow forecasting model s performance last but not least the ced is implemented for the second use case to validate the ced scalability for the inflow forecasting problem the results of the second use case are presented in table 5 we observe a similar enhancement in inflow forecasting for the second area using a ced feature selection framework it is worth mentioning some of the interesting observations in the presented results in tables 4 and 5 as follows feeding multiple time series from different domains e g meteorological hydrological etc into an inflow forecasting algorithm without pre processing does not necessarily improve the forecasting results our proposed ced framework highly improves inflow forecasting results with down selecting the most informative part of input data we picked four different forecasting methods ranging from classical algorithms such as lr and mlp to more complex techniques like rnn and lstm for ced validation all the selected state of the art forecasting methods exhibit relatively performance enhancement after using the ced pre processing step ced can serve as a complementary pre processing step for any forecasting method inflow forecasting results using ced mlp and ced lstm framework in comparison with mlp and lstm no pre processing are shown in fig 10 a and b for sula and storeskar use cases respectively as it is presented both ced mlp and ced lstm have a much better performance in forecasting the real values of inflow for different use cases another interesting observation in fig 10 is that ced mlp and ced lstm accurately forecast inflow s peak values which is a critical and challenging task for state of the art inflow forecasting methods and has significant consequences for short term hydropower scheduling another interesting aspect is to present the sensitivity of the ced algorithm to the size of training horizons and how often the model should be updated and retrained accordingly the sensitivity result of the ced mlp is presented in fig 11 with different lengths of historical data 2 months 6 months 1 year 5 years 10 years for day ahead inflow forecasting it shows after 5 years the forecast error improvement is marginal therefore 5 to 10 years of historical data is good enough to make a causal model in our use case it is worth mentioning that climate change may alter the historical temperature and precipitation patterns in different parts of the world therefore from a practical point of view it is worthwhile to update the causal models and retrain the model annually using a five to ten years historical horizon 5 practical consideration from a practical point of view accurate inflow forecasting models can significantly improve the hydropower scheduling performance flooding control measures water drinking and agriculture efficiency in this paper we focused on daily inflow forecasting mainly for short term hydro power scheduling problems with large storage reservoirs in norway usually most of the precipitation in norway especially close to the glaciers is in the form of snow looking at the historical values of inflow for two different use cases in fig 2 shows that the peak of inflow always happens at the end of spring or early summer when the temperature rise and snow starts to melt snow melting may occur in two weeks time period therefore it is crucial to estimate these periods precisely to avoid the risk of spillage by providing enough space for the maximum harnessing of inflow to the reservoirs our proposed ced pre processing framework enables the forecasting models to discover the dependencies of inflow to the lag values of other variables such as snow dept temperature and precipitation this feature improves the estimation of this period which is a critical factor in inflow forecasting in norway fig 10 a and b show that using ced pre processing with forecaster models lstm and mlp improves the forecasting accuracy at peak values in addition to meteorological data that we used in this paper other variables such as topography soil characteristics etc can impact the inflow forecasting the proposed ced pre processing framework is expandable to a higher number of input source time series more input variables as a direction for future works we will consider expanding source values beyond what we used in this paper 6 conclusion this paper proposes a causal multivariate empirical mode decomposition ced framework for multiple time series feature selection in other words ced is a pre processing step that is complementary to any machine learning based forecasting method we specifically developed ced for the inflow forecasting problem by taking advantage of both decomposition and causal inference approaches for time series at the same time we overcome the inherent weaknesses in both approaches in other words ced combines physics based causal inference with signal processing based decomposition to get the more relevant features among multiple time series to the target variable the inflow our proposed ced framework is validated for two different use cases related to hydropower reservoirs in norway the validation results show that using ced as a pre processing step significantly enhances the forecasting accuracy for various forecasting methods for example adding ced as a pre processing step to lstm and lr improved forecasting results by almost 70 we are trying to use more hydrological parameters as input and expand the ced framework s inflow forecasting horizon for future work credit authorship contribution statement mojtaba yousefi writing original draft writing review editing visualization methodology validation software investigation xiaomei cheng collecting data sorting data and cleaning data michele gazzea assisting in visualization august hubert wierling writing review editing providing technical comments and supervising jayaprakash rajasekharan writing review editing providing technical comments arild helseth providing technical and domain knowledge hossein farahmand project coordination project administration providing technical and domain knowledge writing review editing reza arghandeh supervision providing technical comment writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is partly supported by the research council of norway grant number 309997 and internal fund from western norway university of applied sciences hvl university project number 1030730 
2887,groundwater models require parameter optimization based on the minimization of objective functions describing for example the residual between observed and simulated groundwater head at larger scales constraining these models requires large datasets of groundwater head observations these observations are typically only available from databases comprised of varying quality data from a variety of sources and will be associated with unknown observational uncertainty at the same time the model structure especially the hydrogeological description will inevitably be a simplification of the complex natural system as a result calibration of groundwater models often results in parameter compensation for model structural deficiency or can be affected by observation errors this problem can be amplified by the application of common squared error based performance criteria which are most sensitive to the largest errors in our context we assume that the residuals that remain large during the optimization process likely do so because of either model structural error or observation error based on this assumption it is desirable to design an objective function that is less sensitive to these large residuals of low probability and instead favours the majority of observations that can fit the given model structure and likely are free of large observation errors we suggest a continuous ranked probability score crps based objective function that limits the influence of large residuals in the optimization process as the metric puts more emphasis on the position of the residual along the cumulative distribution function than on the magnitude of the residual the crps based objective function was applied in the calibration of regional scale coupled surface groundwater models and compared to conventional objective functions based on mean of squared absolute and root errors using synthetic as well as real observations the optimization tests illustrated that the novel crps based objective function successfully limited the dominance of large residuals in the optimization process and consistently reduced overall bias keywords hydrologic models parameter estimation calibration objective functions performance criteria 1 introduction numerical hydrological models are often complex physically based models that require substantial parametrization and evaluation against independent observations in virtually all cases at the local to regional and global scale real processes and structures are simplified even in most complex physically based models as a result model parameter values rarely represent a directly observable unit and typically must be estimated through optimization or indirect inversion for example hill and tiedeman 2007 poeter and hill 1997 commonly as in the case of this paper the calibration process is based on the minimization of the discrepancy between model output and observations resulting in effective parameter values for a given model setup and observational dataset the careful selection of calibration targets or objective functions is an important step in the calibration process as they have a large impact on the resulting optimized model in the context of hydrological modelling see for example demirel et al 2018 fowler et al 2018 gupta et al 2009 2012 krause et al 2005 likewise the selection of informative observations for model evaluation is a critical part of optimization design for example danapour et al 2019 hartmann et al 2017 pool et al 2017 seibert and vis 2016 for a more general discussion see gupta et al 2008 these issues are particularly relevant for regional and large scale groundwater models and when large observational datasets of head elevation are used as calibration targets to constrain the models as good as possible at larger scales outside dedicated research catchments or highly investigated sites the hydrogeological model will inevitably be a simplification based on interpolation of the underlying geological structures which is particularly evident for global products e g huscroft et al 2018 but remains valid also for regional and national scale geological mapping efforts e g thornton et al 2022 typically a hydrogeological model will consist of a limited set of units within which hydraulic properties are assumed to be uniform the uniformity can be circumvented by highly parameterized approaches such as pilot points ramarao et al 1995 doherty 2003 fienen et al 2009 which can increase computational burden as also demonstrated for the model system used in this work danapour et al 2019 however most groundwater models and coupled surface subsurface models still rely on a unit based approach where the lack of information and the implicit simplification in hydraulic parameter representation will lead to structural model inadequacies enemark et al 2019 such groundwater models require datasets of groundwater head observations to adequately constrain the model parameters compared to the uncertainty of simulated heads in groundwater flow models the measurement uncertainty on groundwater heads itself is usually small gelhar 1986 sonnenborg et al 2003 still certain observations can be unsuitable for parameter estimation due to several reasons this can be due to large elevation variations within a model grid especially in coarser large scale models where horizontal model resolution is in the range of tens to hundreds of metres within each such model cell the model can only represent one elevation and groundwater head not being able to account for small scale variations due to finer topography or other relevant heterogeneities moreover small scale geological features not described in the hydrogeological model or over simplification of geological units can result in models that cannot represent some measured groundwater heads other model uncertainties include inaccurate boundary conditions effects of groundwater pumping not accounted for in the models etc then there is observation related uncertainty a sufficiently large set of groundwater head observations distributed adequately across the study area is rarely obtainable within a given project due to both the required field campaigns and long time periods to be covered therefore head observations are often obtained from databases containing historic records of varying unknown quality and from various sources some of these observations can be affected by nearby pumping or will contain misinformation on aspects such as location observation time unit reference level or the measurement itself there are guidelines and methods for quantifying the uncertainty of head observations in relation to aspects of well construction general slope of groundwater head temporal representation or in relation to a given groundwater model depending on grid scale etc henriksen et al 2003 2017 hill and tiedeman 2007 chapter 11 these do however not always account for model structural errors and assigning individual uncertainties to each observation point is difficult which often leads to model aggregated uncertainty estimates disentangling different sources of uncertainty is a challenge in many hydrological model applications renard et al 2010 this means that not even measurement uncertainty can be determined for many large scale applications using large observation datasets originating from various sources lacking adequate metadata however an estimate of measurement uncertainty is crucial in weighting observations in calibrations ginn and cushman 1990 hill 1998 hence not all observations will be equally suitable for estimating optimal parameter values it is important to recognize that the optimal parameter values are not strictly defined as the values that result in the smallest deviations from observations but the values that best represent the true effective parameter values while minimizing the compensation for misrepresentation of measurements and structural model errors in other words minimizing of model error and tuning of parameter values should not compensate for model inadequacies or observation errors for example antonetti and zappa 2018 motavita et al 2019 white et al 2014 even if acknowledging that such parameter values are effective values valid in the given model environment in the context of this work focussing on large scale distributed and coupled surface groundwater models the term structural errors refers mainly to errors in the physical structure of the hydrogeological model similar errors in other model compartments such as vegetation distributions errors in boundary conditions or errors resulting from process simplification or misrepresentation common performance criteria in groundwater modelling contain a summed error term usually as squared errors to avoid cancellation of errors with opposite signs chai and draxler 2014 krause et al 2005 poeter and hill 1997 such performance criteria are sensitive towards outliers as large errors dominate the objective function i e it is common in groundwater model calibration that the objective function is dominated by a relatively small group of observations with large residuals in cases where large initial residuals owe to inappropriate parameter values in an ideal model structure these residuals can be minimized without significant trade off with other residuals however in regional scale groundwater models structural simplification and inadequacy will always cause trade offs between residuals at different locations therefore large errors should not necessarily be forcing the parameter optimization in a certain direction if these large errors are associated with either model structural error or observational outliers still squared error based performance metrics for example the root mean square error rmse sum of squared errors sse or nash sutcliffe efficiency nse are the most common criteria for the evaluation of hydrological models with observations gupta et al 2009 these challenges in dealing with outliers in data in general are well known and have been the motivation for various methods under the term robust regression rousseeuw and leroy 1987 some of these methods require iterative re weighting of residuals which are computationally expensive in model calibration also considerable efforts have been directed towards the better estimation of error distributions relaxing the need for explicit formulation of such and dealing with phenomena such as heteroscedasticity e g cheng et al 2018 schoups and vrugt 2010 sun et al 2017 vrugt and sadegh 2013 this also lead to the acceptance of using subjective likelihood measures beven and binley 1992 beven 2006 doherty and welter 2010 however determining error distributions remains non trivial and usually standard assumptions are used without the ability to formally evaluate them a solution that sometimes is employed to deal with observations that seem to be outliers or cannot be explained by the conceptual understanding of the hydrologist is to simply exclude them from the parameter optimization process or the general model evaluation sometimes this is performed based on simple prior information and assumptions or single model evaluations and residuals boldetti et al 2010 haaf and barthel 2018 højberg et al 2015 keating et al 2010 this can be considered undesired exclusion or lowering the weight of observations based on simple criteria such as residual size prior to calibration can easily lead to misinterpretation since it will be unknown if a given residual can be significantly reduced during optimization more rigorous methods of evaluating prior data conflicts typically require running an ensemble of models prior to calibration doherty and moore 2020 reitan and petersen øverleir 2008 hemmings et al 2020 e g using monte carlo analyses which can be hindering their application with computationally expensive models based on the challenges outlined above this study suggests an objective function that limits the impact on parameter identification originating from observation errors and model inadequacies at the same time the calibration framework avoids the necessity for excluding such observations from the optimization and model evaluation these observations can be informative with respect to model performance evaluation and should not be omitted based on arbitrary criteria but can hamper model parameter identification to achieve this goal we developed an objective function based on the continuous ranked probability score crps gneiting et al 2005 for describing deviations between simulated and observed groundwater head commonly the crps is used as an evaluation tool for probabilistic forecasts here we suggest a novel use of it in the context of an objective function for groundwater models with large sets of point observations we explain the crps based objective function particularly its inherent benefits in weighting between large and small errors we then apply the concept to the calibration of regional scale coupled groundwater surface water models in denmark and compare it to results obtained by using traditional objective functions based on mean squared error mse mean absolute error mae and mean root error mre 2 method the crps is a popular evaluation tool for probabilistic forecasts or model simulations gneiting et al 2005 it can be expressed as 1 crps p s x p o x 2 d x where ps is the empirical cumulative distribution function ecdf of the ensemble predictions for variable x at a certain timestep and po the respective ecdf of the observation or truth of variable x usually the observation is a discrete value hence its distribution is a heaviside step function that changes from 0 to 1 at the observed value this means that in fig 1 the crps can be interpreted as the area between the ecdf of the forecasts in blue and the respective step function of the expected value in red 0 in this case the optimum value of the crps is zero usually the crps is averaged over a series of timesteps it is attractive as it combines reliability and sharpness of forecasts in one indicator furthermore for deterministic forecasts the crps simplifies to the mae in this paper we suggest using the crps outside of its initially intended scope the evaluation of ensembles of forecasts or models instead of being applied to the value of a state variable across members of an ensemble forecast the principle of the crps can also be applied to the value of a state variable across a set of locations observation points which is evaluated against the output from an individual model that means instead of looking at the ecdf of the ensemble forecasts of a certain variable x for example the groundwater head at a certain point across an ensemble of models we consider the ecdf of a certain type of predictions across different points in space for example the groundwater head at several observation points across space in one individual model for this purpose since we have a different expected value or observation at each observation point we simply consider residuals model deviations from the observations instead of absolute values that leaves us with an expected or optimal value of 0 and the ecdf of the collective set of residuals in the model the particular properties of the crps and its differences to the mse are illustrated in fig 1 inspired by hersbach 2000 which shows the empirical cumulative distribution of a dataset of five predictions commonly in ensemble forecasting this would be predictions of the same variable from five members of a model ensemble in our case these are residuals of a simulated variable groundwater head at different observation locations the verification value or truth which is commonly an observation of the variable in question is marked by the red line when using residuals the expected or true value is 0 the crps can be represented by the sum of the areas dx dp2 in the left panel of fig 1 accordingly the crps for an empirical cumulative distribution can be written as 2 crps i 1 n d x i d p i 2 where dxi xi 1 xi and dpi is the cumulative probability of the ecdf not exceeding xi if xi is less than the expected value or 1 minus that cumulative probability if xi is greater than the expected value commonly the mse is given as 3 mse 1 n i 1 n x i o 2 where o is the expected value or observation in the right panel of fig 1 representing the mse in a notation similar to equation 2 xi o is represented by each dxi and dpi has a constant value of 1 n for an empirical cumulative distribution with n values accordingly the mse can be represented as the sum of areas dx2 dp in the right panel of fig 1 and rewritten as 4 mse i 1 n d x i 2 d p i correspondingly the mae is the sum of dx dp in either panel noteworthy are the different contributions of the single predictions to the aggregated crps or mse value grey areas in fig 1 the single prediction with the highest deviation from the truth is 4 and might be considered an outlier for the crps that prediction contributes a relatively small amount to the total crps 21 whereas for the mse that prediction accounts for 72 of the total mse in a model calibration using the mse or rmse as an objective function most algorithms will inevitably focus on improving the model fit for that particular point as it contributes such a large share to the total objective function however considering that such large deviations between model and observations at certain points often owe to observation uncertainty or issues with the model structures this could be undesired behaviour the crps is much less dominated by the largest deviations as it squares along dp equation 2 instead of dx as the mse equation 4 it assigns a relatively lower weight to the residuals at the outer ends of the ecdf these residuals are also the largest and in our context often those we have a low confidence in being informative to the model parameter estimation in contrast the crps favours sharp and reliable distributions of predictions because of the relatively large values assigned to predictions close to the truth as can be seen in fig 1 therefore we find it relevant to investigate the use of a crps based objective function for the optimization of large scale groundwater flow models 2 1 crps in comparison with other common performance criteria to illustrate the implications of using either crps or the mse as performance criteria some example error distributions based on gaussian distributions were compared being less dominated by large residuals the mae and mre were also included fig 2 shows those distributions and their respective performance criteria values the expected value red line always is 0 the modelled errors cumulative distribution function is given by the blue line for the reference top left this error distribution is a normal distribution with a standard deviation with a mean of 0 and a variance of 1 simply increasing the error top right by doubling the standard deviation leads to a crps and mae twice as large as in the reference an mre 1 4 times as large and an mse four times as large when a bias is introduced by changing the mean of the error distribution to 0 5 bottom left however the crps shows more sensitivity than the other metrics its value increases 1 4 fold over the crps in the reference whereas the mse mae and mre only increase 1 2 1 1 and 1 1 fold respectively over their values in the reference when outliers are being introduced to one end of the distribution bottom right by replacing 10 of the reference with absolute value samples from a normal distribution with a standard deviation of 5 the crps shows less sensitivity it only increases 1 2 fold compared to its value in the reference the mse increases 3 4 fold and even the mae is more sensitive than the crps increasing 1 4 fold compared to its reference value only the mre shows a similar sensitivity to these outliers as the crps by also increasing 1 2 fold 2 2 using the crps as an objective function for single model realizations we suggest using the crps outside its original scope of probabilistic forecasting but as an objective function for single model realizations that is we use the crps as a heuristic norm like one could also use the mse mae or mre related to the l2 l1 or l1 2 norm though the l2 norm is most commonly used different norms have been suggested in the context of inversion in hydrogeology e g the l1 norm in carrera et al 2005 still bringing the crps into the realm of single model realizations raises questions of its applicability as it in theory requires an assumption of ergodicity however its use also has been suggested in conjunction with non ergodic schlather models see yuen 2015 stochastic hydrology generally relies on ergodicity as an assumption e g gelhar 1986 however it is also acknowledged that ergodicity is hard to prove suciu et al 2006 sanchez vila and fernàndez garcia 2016 especially in many real world applications yet it commonly is assumed sanchez vila et al 2006 we assume that as we are using the crps like a norm see also fig 1 we can use it like other norms to inform a parameter estimation process such as the commonly used mse 3 model and data 3 1 the two study areas to illustrate the possible advantages of utilizing a crps based objective function we will evaluate the effect of different objective functions on the calibration of two real world regional scale coupled distributed groundwater surface water models the two study cases are situated in denmark the catchment of the river storå in western jutland and the catchment of the inner odense fjord on the island of funen which is dominated by the river odense å fig 3 both catchments are approximately 1 000 km2 large with generally gentle slopes and topography ranging from sea level to approximately 120 m geologically the storå catchment is more dominated by sandy soils especially in its western part whereas the odense catchment is dominated by clayey soils generally groundwater levels in denmark are shallow with moderate seasonal variations in the range of 1 m koch et al 2019 however several layers of aquifers can exist denmark has a temperate climate and the average yearly precipitation is roughly 1 000 mm and 800 mm in the storå and odense catchments respectively 3 2 hydrologic modelling framework both models were set up within the mike she hydrologic modelling software abbott et al 1986 mike she offers a transient fully distributed physically based description of the terrestrial part of the hydrological cycle it couples 3d subsurface flow 2d overland flow unsaturated zone processes and routing of surface water in streams the unsaturated zone is represented by the 2 layer method of mike she which includes the processes of interception ponding and evapotranspiration it simplifies the unsaturated zone to two layers the first layer representing the root zone which varies across vegetation types and seasons and the second representing the zone below the root zone down to the water table of the saturated zone dhi 2019b p 27 the two models are based on the national water resource model of denmark referred to as dk model developed at the geological survey of denmark and greenland henriksen et al 2003 højberg et al 2013 stisen et al 2019 the dk model is used in national water management and assessment of human and climate change impact which is reflected in the model evaluation against large national datasets of in situ groundwater head observations as well as streamflow observations with a focus on representing the overall water balance the two used submodels of the national model were originally setup within a project exploring possibilities of further developing the dk model stisen et al 2018 like the dk model the horizontal grid resolution is 500 m the description of the subsurface is based on a hydrogeological model covering all of denmark arvidsen et al 2020 stisen et al 2019 the parameterization is unit based i e each hydrogeological unit is assigned homogenous model parameters for hydraulic conductivity etc for reasons of computational efficiency some layers of the hydrogeological model have been combined resulting in seven computational layers in the storå model and ten in the odense model in mike she computational layers span the entire domain and are reduced to a minimum thickness 0 5 m in our case where the respective geological formation does not exist in those areas parameter values are taken from the respective nearest existing geological formation dhi 2019a p 313 artificial drainage widespread across denmark is included in the model snow accumulation and melt are incorporated with a degree day method meteorological forcing with daily timesteps of precipitation stisen et al 2012 reference evapotranspiration and air temperature is available from national gridded datasets scharling 1999b a further forcing include groundwater extractions for drinking water supply and irrigation the models are run with flexible timesteps allowing a maximum timestep of 24 h 3 3 hydrologic model calibration data as for the dk model the models were evaluated against groundwater head observations from a series of boreholes and runoff timeseries at stream stations displayed in fig 3 the dataset of groundwater head observations consists of borehole and groundwater level information from the public national danish database for boreholes and groundwater jupiter https eng geus dk products services facilities data and maps national well database jupiter in total the dataset for the storå model contains groundwater heads from 890 wells some boreholes have several observations in time and from different depths offering a total of 5 218 individual head observations for most wells only a single groundwater head observation exists only 30 of those wells offer time series with more than ten observations in time for the odense model data exists from 1 820 wells with 44 273 individual head observations 205 of those wells offer more than ten observations in time as with the dk model model simulations were also evaluated against discharge data based on timeseries of observed stream discharge with daily resolution from six stations in the storå catchment and nine in the odense catchment 3 4 hydrologic model calibration setup the models were calibrated with the ostrich optimization software toolkit matott 2017 using the pareto archived dynamically dimensioned search padds algorithm asadzadeh and tolson 2013 tolson and shoemaker 2007 padds is a global multi objective optimization algorithm that is geared towards effectively defining the pareto front for a set of objective functions in our case this allowed to simultaneously calibrate towards five objective functions discharge performance in streams and the mentioned four groundwater head objective functions all calibration and evaluation exercises were performed for the period 2000 to 2008 with a warm up period starting in 1990 discharge performance was included by using the kling gupta efficiency kge gupta et al 2009 5 kge 1 r 1 2 α 1 2 β 1 2 where r is the pearson correlation coefficient between observed and simulated discharge as daily timeseries in our case α is the standard deviation fraction and β the bias fraction of observed and simulated streamflow the groundwater observation dataset consists of observations of groundwater heads from boreholes each borehole can contain one or several observations in time and can contain one or several screen depths i e observations from different aquifers it was chosen to aggregate all observations within each single model grid cell and layer after calculating the individual residuals at their exact points in time and space this was done mostly because a model grid cell is the smallest unit the model can resolve individual head measurements within one model grid can contradict each other due to heterogeneities below the model scale and typical seasonal variations of groundwater heads are in the range of 1 m which is below the typical residuals similar aggregation is applied in other practical applications of large scale models with scattered distribution of observations in space and time e g sonnenborg et al 2003 weighting then was applied according to the number of observations per grid cell a relative weight of 1 was applied to cells with one observation a weight of 2 to cells with two to nine observations a weight of 3 to cells with ten to 99 observations and a weight of 5 to cells with 100 or more observations then the model was evaluated based on the mean error of all the observations within each model grid cell across time four different objective functions were included first a sum of squared errors sse 6 sse i 1 n m e i 2 where mei is the mean error of simulated groundwater heads in each of the n model grid cells containing observations second a sum of absolute errors sae 7 sae i 1 n m e i third a sum of root errors sre 8 sre i 1 n m e i 1 2 note that the terms sse and mse sae and mae and sre and mre can be used interchangeably in this context as mse 1 n sse etc as mse mae and mre are the more common terms we will use those throughout the rest of the paper lastly the crps based objective function see equation 1 using the ecdf of all model grid cells mei and a mean error of 0 as expected value in the storå model eight model parameters were calibrated six different geological units hydraulic conductivities the root depth and the saturated zone drain time constant for the odense model the calibration was set up with six free parameters four hydraulic conductivities the root depth and the drain time constant in the calibrations the padds algorithm was run with 5 000 model runs for the synthetic calibration experiments see section 3 5 below and 3 000 runs for the calibrations with real data parameter bounds were set to values between 1 10 7 and 1 10 2 m s for geological units representing sands 1 10 9 and 1 10 4 m s for geological units representing clays 1 10 10 and 1 10 5 s 1 for drain time constants and 200 and 2000 mm for root depths in its implementation in ostrich padds stores all solutions it explores along its search as we focus on the groundwater head objective functions we used the streamflow kge to remove non behavioural solutions that have a mean kge below 0 7 from the remaining solutions we picked the best performing solution in terms of each of the groundwater head metrics crps mse mae and mre defining four optimal solutions dependent on metric choice more information on the padds algorithm and the use of the pareto front it produces can be found in literature e g holmes et al 2020 koch et al 2022 3 5 synthetic hydrologic model calibration setup to further test the assumption that a crps based calibration is less likely to compensate for model structural errors or systematic observation errors by parameter compensation and similar we performed some synthetic calibration experiments the synthetic calibration experiments were performed with the storå model described above a certain realization i e with a specific set of parameters of the storå model served as a reference model or synthetic truth from a run of this reference model we sampled synthetic observations from the simulated groundwater heads those synthetic observations were sampled at the exact same time and locations where real observations are available normally distributed noise with zero mean and 0 5 m standard deviation was added to the synthetic observation to account for random observation error some of those observations were further perturbed potentially representing a scenario with larger yet undetected observation errors or a structural error in the model leading to a local inability of the model to preproduce observed groundwater heads in this example all observations within a rectangular spatial extent within the uppermost five layers of the model the quaternary layers and with an observed water level at least 5 0 m below the surface were perturbed this selection resulted in observations from 21 wells displayed in fig 4 being perturbed out of a total of 890 wells by adding 6 0 m to their observed value i e their synthetic truth then starting from a random initial parameter set the model was calibrated as described in section 3 4 using the synthetic observations with perturbations as described above again the four different groundwater head objective functions crps mse mae and mre were included 4 results after having shown illustratively how different objective functions react to hypothetic errors distributions in sections 2 and 2 1 now results of the calibration experiments of the hydrological models will be presented starting with the synthetic examples and then moving on to calibrations of the models against real world data 4 1 results of the synthetic hydrologic model calibration to display the benefits of the crps based objective function in a realistic but controlled model environment synthetic calibration experiments were carried out fig 4 shows the average deviation of simulated groundwater heads across all model layers from the reference model s simulated groundwater heads for the crps mse mae and mre optimal solution respectively the 21 wells with perturbed synthetic observations clearly have the largest effect in the mse optimal solution leading to the calibrated model with the largest deviation from the synthetic truth the mae mre and crps based calibrations all perform similarly well however the crps based calibration shows the lowest bias among the calibrated models furthermore the estimated parameters resulting from the crps based calibration are closest to the values of the reference model weighted by the parameter sensitivity the mean absolute deviation from the reference model s parameters for the crps optimal solution is 0 9 whereas it is 1 1 1 4 and 1 6 for the mae mre and mse optimal solutions respectively 4 2 results of the hydrologic model calibration advancing to the calibrations against real observations table 1 summarizes the results for the storå and odense models kge is included for reference purposes as mentioned the set from which the optimal solutions were picked was limited to those with an average kge of above 0 7 not surprisingly the best value for each performance metric is achieved for the best solution in terms of the respective objective even though the trade off between the four groundwater head objective functions is small especially in the storå model the solutions still are separated the resulting parameter values for all the picked solutions generally fall into expectable ranges with horizontal hydraulic conductivities in sand layers ranging from 1 10 2 to 1 10 4 m s with one exception of 1 10 7 m s in the deeper sand layers of storå and 6 10 5 to 1 10 8 m s in clay layers respectively the mean error me across all groundwater heads not included as an objective function also shows the expected behaviour in the odense model where the crps optimal solution clearly exhibits the lowest me or bias and the mse optimal solution the highest me for storå the behaviour is slightly erratic with the crps mre and mse optimal solutions performing similarly well in terms of me in the following we will focus on the differences between mse optimal solutions as this is the most common objective function and optimal solutions in terms of the proposed crps based objective function fig 5 displays the absolute mean errors of groundwater head in each grid cell with observations y axis in relation to the difference in mean error in each grid cell between the crps optimal and mse optimal solutions x axis for the majority of all grid cells the error of the crps optimal solution is slightly lower than the error of the mse optimal solution which places them in the left white half of the plot only few observations amongst those many with large errors exhibit a larger error in the crps optimal solution which places them in the right grey half of the plot this is in line with the results in table 1 the crps optimal solution results in a slightly higher rmse yet a lower me furthermore table 2 shows that the majority of cells with small error below 5 m exhibits smaller errors in the crps optimal solution compared to the mse optimal solution this applies to 57 and 67 of those grid cells in the storå and odense model respectively in contrast 77 and 84 of cells with large error above 10 m exhibit larger errors in the crps optimal solution the same can be shown for a comparison of the crps optimal with the mae optimal solution the picture only differs marginally from fig 5 etc hence the results are not shown here in the two maps in the top row fig 6 shows all grid cells with observations colour coded for their error in the crps optimal solution the middle map of each model displays only those cells which show a lower error in the crps optimal compared to the mse optimal solution the bottom map displays only those cells which show a larger error in the crps optimal compared to the mse optimal solution large errors dominate the bottom maps of both models reddish and blueish colours while small to moderate errors dominate the middle maps yellowish colours more interestingly some distinct areas with almost exclusively large errors become apparent in the storå model there appear to be specific issues in the northwest corner of the model where we overestimate groundwater heads blue colours and in the southwest corner where we underestimate groundwater heads red colours in the odense model there are two distinct regions in the south east and mid west where groundwater heads are underestimated by the model 5 discussion the issues arising from squared error based performance criteria being sensitive to even few outliers and extreme residuals are well known berthet et al 2010 legates and mccabe 1999 moriasi et al 2007 chai and draxler 2014 chen et al 2017 and also discussed in time series forecasting in general armstrong and collopy 1992 chen et al 2017 this also led to the recommendation of using multiple performance metrics as any single metric will provide information only on a certain aspect of the error characteristic chai and draxler 2014 also the popular nse exhibits similar issues of being highly influenced by outliers as it is also based on the squared error term krause et al 2005 mccuen et al 2006 for example bradley et al 2004 or berthet et al 2010 showed that mse based performance criteria are dominated by a small number of data points in cases where there are both some large negative as well as some large positive residuals as is often the case in groundwater head calibration a squared error based optimization is virtually limited to finding the best trade off between these two groups without much consideration to the majority of residuals which fall within a reasonable range and which should ideally inform parameter identification alternatively an outlier filtering can be performed however with the mentioned large non scientific datasets as used in our model case and when filtering is performed prior to the parameter estimation it is hard to determine which observations with large residuals represent observational outliers indicate model structural deficiencies or non optimal parameter values statistically sound outlier filtering often requires timeseries of observations jeong et al 2017 peterson et al 2018 whereas our observation dataset is comprised of many observation points with only a single or few observations in time and only few observation points with a whole time series outliers can also be detected based on spatial patterns bárdossy and kundzewicz 1990 however the dataset s spatial coverage can be too coarse and scattered compared to the variables actual spatial variability to allow for reliable outlier detection based on spatial patterns or such methods require assumptions about model structure helwig et al 2019 or still rely on a row of subjective criteria tremblay et al 2015 as mentioned in the introduction it appears that the scientific groundwater modelling literature assumes that data have been through a data control process where outliers have been identified and removed before the data are used in a modelling context however this is i not practical in case of data from thousands of observation wells from different sources with different and unknown data quality that are not originating from scientific monitoring programmes and ii risks omitting data containing valuable information about potentially deficient model parameters or model structures for such cases there is a lack of methodology metrics that can make use of all available data without allowing outliers to dominate already in its original context of ensemble forecasting the crps was considered robust to outliers gneiting and raftery 2007 the crps based objective function addresses the limitations of the squared error based objective functions by putting more emphasis on the position of each residual along the cumulative distribution than on the magnitude of each residual effectively this means that large less frequently occurring residuals are assigned a relatively lower weight than in squared error based metrics in our context it is an underlying assumption that during the optimization process the least probable residuals will coincide with the largest residuals for large datasets with some theoretic error distributions fig 2 it can be illustrated that the crps is less sensitive to outliers in an error distribution than squared error or even absolute error based performance criteria on the other hand it is more sensitive to the overall bias of all errors if used as an objective function the crps will focus on reaching a balanced result with little bias while accepting some potential outliers with large residuals the mse and also the mae will more likely result in a distribution with some overall bias if only the largest residuals can be reduced as those dominate the overall value of the metric the mre can be seen as similarly insensitive to large residuals as the crps however it is less sensitive to bias in real world cases with large observational dataset of mixed and uncertain data quality and even complex hydrological models exhibiting inevitable simplifications of processes and structures we have to accept that a part of the observational dataset will exhibit large residuals nonetheless we still assume that after parameter estimation the majority of the observations can fall within a reasonable margin of error and those large residuals are considered less informative to the parameter estimation process furthermore in coupled surface groundwater models a low overall bias potentially resulting in low water balance errors is of great interest this is where we can utilize the strength of crps based performance criteria over mse based criteria in synthetic calibration experiments it could be shown that perturbed observations have a lesser effect on the parameter estimation process when using the crps than when using the mse where the mre and mae performed similar to the crps when comparing the four performance criteria applied to the parameter estimation in two real world groundwater surface water models the crps optimal solutions showed an improvement of the majority of residuals which are mostly small compared to mse optimal solutions in line with expectations only few residuals are increasing in the crps optimal solution most of them large see fig 5 and table 2 with the given model structure based on geological units and units based parameters these large residuals especially in error hotspots as visible in fig 6 can indicate a general issue with the model structure for example the physical structure of the unit based hydrogeological model which likely can be considered the main structural error in our case højberg and refsgaard 2005 refsgaard et al 2012 or boundary conditions in parameter estimation we explicitly want to avoid compensating for issues such as model conceptual errors or observation errors by trying to tweak model parameters in other words such large residuals are not considered informative to the parameter estimation due to its characteristics of being comparably insensitive to outliers but more sensitive towards an overall bias a crps based calibration is less prone to show such undesired behaviour it will i maintain and make such error hotspots more pronounced allowing for easier identification of general issues with model structure ii not compensate as much for observations that are inadequate or incompatible with the model structure by tuning parameters values and iii obtain best model performance for the vast majority of observations this points in the direction of an optimization process where all available observations are included in a parameter estimation however observations that represent conditions not consistent with the model structure or have a large error do not dominate the outcome of the parameter optimization process to the same extent as when using mse based criteria subsequently these observations can be evaluated when they illuminate a systematic model structural deficiency they can lead to a re evaluation of the model structure prior to further parameter estimations in other cases for example where observations with large residuals are scattered randomly across the model domain they can be regarded as not representative of model scale and structure as observational outliers or simply as indicators of the overall model uncertainty it is worth highlighting that the crps based objective function proposed here for calibration of groundwater flow models with large observational datasets assumes that after adequate calibration only a limited number of large residuals remain the remaining residuals potentially can represent areas of either model structural uncertainty including issues such as errors in the hydrogeological model or inadequate boundary conditions or highlight observational errors and do not dominate the total number of observations our real world model applications are examples where this is the case compare fig 6 the proposed crps based groundwater head objective function can be used in multi objective settings in combination with other performance metrics such as streamflow metrics used in this article furthermore one can envision the use of the crps as a heuristic norm also for other types of observational datasets than groundwater heads as in this example 6 conclusions there are few examples in the literature that attempt to practically address the well known issues related to the use of squared error minimization in optimization problems these problems are specifically relevant for optimizations problems in models with uncertain model structures and large uncertain evaluation datasets both being common in large scale groundwater modelling squaring errors magnifies large values consequently a parameter estimation minimizing an objective function based on squared errors will focus on reducing the largest errors often the largest residuals are the result of model structural uncertainty or observation uncertainty i e they may not be informative to the parameter estimation process yield in effective parameters compensating for such issues and consequently risk producing a model with a skewed representation of the variable of interest groundwater head in our case with this paper we wanted to highlight those issues with squared error based performance criteria and compare an mse based criteria to a mae mre and crps based criteria in particular we suggested the use of a crps based performance criteria as a heuristic norm in the objective function of hydrological model parameter estimation the crps has some desirable properties that we demonstrated in synthetic and real calibration experiments compared to mse and to lesser extent also compared to mae the crps is less sensitive to large residuals at the same time it is more sensitive to the overall bias even compared to the mre the crps originates from the evaluation of probabilistic forecasts to our knowledge it has not been used before as performance criteria in our context of deterministic model optimization in practical applications such as in our example where we used a large dataset of groundwater head observations from a public database with varying and undetermined quality in combination with a regional scale groundwater model model structural uncertainty and observation uncertainty are difficult to quantify hindered for example by the computational and model setup effort related to ensembles representing various model structures ideally rigorous hydrologic model building and testing should include the evaluation of various model structures enemark et al 2019 the same applies to global scale hydrologic modelling where often creative calibration targets are used e g yang et al 2019 or where parameters are even estimated without calibration maxwell et al 2015 another example is a calibration of stream flow in the signature domain instead of the time domain assuming this makes the calibration more robust to objective function or data deficiencies fenicia et al 2018 westerberg and mcmillan 2015 the work presented in this paper is a practical way of dealing with observation uncertainty in cases where those uncertainties are hard to quantify it can also help shed light on areas of model inadequacy however mathematically sound accounting for such effects requires at least some idea of the magnitude of observation uncertainty or model inadequacy see for example the discussion of necessary prior knowledge about model discrepancy in brynjarsdóttir and ohagan 2014 yet in any case the influence of the chosen objective function due to its properties such as the magnification of large residuals should be considered the challenges mentioned related to large datasets with unknown observation uncertainty combined with model structures simplifying the real world due to scale issues data availability or computational limitations also occur in other areas than distributed hydrological models as in this example hence a look beyond squared error based performance is also expected to pay off in other modelling contexts and tackle the mentioned issues with squared error based performance criteria one could also envision the use of a robust objective function in the estimation of highly parameterized parameter fields such an application is beyond of the scope of this work the focus here was to shed light on the influence of the choice of the objective function on parameter estimation and certain implications of the most commonly used squared error based metrics in different contexts different objective functions might be best suited we have suggested a robust objective function that is beneficial in many practical applications of hydrological models credit authorship contribution statement raphael schneider methodology software investigation formal analysis data curation writing original draft writing review editing visualization hans jørgen henriksen supervision project administration funding acquisition writing review editing conceptualization simon stisen conceptualization methodology visualization writing original draft writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we want to gratefully acknowledge the fods 6 1 fasttrack metodeudvikling project headed by styrelsen for dataforsyning og effektivisering sdfe danish agency for data supply and efficiency in which context the presented method was developed furthermore we want to thank jens christian refsgaard for kindly providing helpful feedback to improve the manuscript and peter bauer gottwein for insightful thoughts on the topic data statement python scripts used to calculate the four different groundwater head objective functions including the crps based objective function are publicly available together with some example data https github com snowthe crps heuristic of the entire model and calibration setup can be made available upon personal request to the corresponding author without undue reservation 
2887,groundwater models require parameter optimization based on the minimization of objective functions describing for example the residual between observed and simulated groundwater head at larger scales constraining these models requires large datasets of groundwater head observations these observations are typically only available from databases comprised of varying quality data from a variety of sources and will be associated with unknown observational uncertainty at the same time the model structure especially the hydrogeological description will inevitably be a simplification of the complex natural system as a result calibration of groundwater models often results in parameter compensation for model structural deficiency or can be affected by observation errors this problem can be amplified by the application of common squared error based performance criteria which are most sensitive to the largest errors in our context we assume that the residuals that remain large during the optimization process likely do so because of either model structural error or observation error based on this assumption it is desirable to design an objective function that is less sensitive to these large residuals of low probability and instead favours the majority of observations that can fit the given model structure and likely are free of large observation errors we suggest a continuous ranked probability score crps based objective function that limits the influence of large residuals in the optimization process as the metric puts more emphasis on the position of the residual along the cumulative distribution function than on the magnitude of the residual the crps based objective function was applied in the calibration of regional scale coupled surface groundwater models and compared to conventional objective functions based on mean of squared absolute and root errors using synthetic as well as real observations the optimization tests illustrated that the novel crps based objective function successfully limited the dominance of large residuals in the optimization process and consistently reduced overall bias keywords hydrologic models parameter estimation calibration objective functions performance criteria 1 introduction numerical hydrological models are often complex physically based models that require substantial parametrization and evaluation against independent observations in virtually all cases at the local to regional and global scale real processes and structures are simplified even in most complex physically based models as a result model parameter values rarely represent a directly observable unit and typically must be estimated through optimization or indirect inversion for example hill and tiedeman 2007 poeter and hill 1997 commonly as in the case of this paper the calibration process is based on the minimization of the discrepancy between model output and observations resulting in effective parameter values for a given model setup and observational dataset the careful selection of calibration targets or objective functions is an important step in the calibration process as they have a large impact on the resulting optimized model in the context of hydrological modelling see for example demirel et al 2018 fowler et al 2018 gupta et al 2009 2012 krause et al 2005 likewise the selection of informative observations for model evaluation is a critical part of optimization design for example danapour et al 2019 hartmann et al 2017 pool et al 2017 seibert and vis 2016 for a more general discussion see gupta et al 2008 these issues are particularly relevant for regional and large scale groundwater models and when large observational datasets of head elevation are used as calibration targets to constrain the models as good as possible at larger scales outside dedicated research catchments or highly investigated sites the hydrogeological model will inevitably be a simplification based on interpolation of the underlying geological structures which is particularly evident for global products e g huscroft et al 2018 but remains valid also for regional and national scale geological mapping efforts e g thornton et al 2022 typically a hydrogeological model will consist of a limited set of units within which hydraulic properties are assumed to be uniform the uniformity can be circumvented by highly parameterized approaches such as pilot points ramarao et al 1995 doherty 2003 fienen et al 2009 which can increase computational burden as also demonstrated for the model system used in this work danapour et al 2019 however most groundwater models and coupled surface subsurface models still rely on a unit based approach where the lack of information and the implicit simplification in hydraulic parameter representation will lead to structural model inadequacies enemark et al 2019 such groundwater models require datasets of groundwater head observations to adequately constrain the model parameters compared to the uncertainty of simulated heads in groundwater flow models the measurement uncertainty on groundwater heads itself is usually small gelhar 1986 sonnenborg et al 2003 still certain observations can be unsuitable for parameter estimation due to several reasons this can be due to large elevation variations within a model grid especially in coarser large scale models where horizontal model resolution is in the range of tens to hundreds of metres within each such model cell the model can only represent one elevation and groundwater head not being able to account for small scale variations due to finer topography or other relevant heterogeneities moreover small scale geological features not described in the hydrogeological model or over simplification of geological units can result in models that cannot represent some measured groundwater heads other model uncertainties include inaccurate boundary conditions effects of groundwater pumping not accounted for in the models etc then there is observation related uncertainty a sufficiently large set of groundwater head observations distributed adequately across the study area is rarely obtainable within a given project due to both the required field campaigns and long time periods to be covered therefore head observations are often obtained from databases containing historic records of varying unknown quality and from various sources some of these observations can be affected by nearby pumping or will contain misinformation on aspects such as location observation time unit reference level or the measurement itself there are guidelines and methods for quantifying the uncertainty of head observations in relation to aspects of well construction general slope of groundwater head temporal representation or in relation to a given groundwater model depending on grid scale etc henriksen et al 2003 2017 hill and tiedeman 2007 chapter 11 these do however not always account for model structural errors and assigning individual uncertainties to each observation point is difficult which often leads to model aggregated uncertainty estimates disentangling different sources of uncertainty is a challenge in many hydrological model applications renard et al 2010 this means that not even measurement uncertainty can be determined for many large scale applications using large observation datasets originating from various sources lacking adequate metadata however an estimate of measurement uncertainty is crucial in weighting observations in calibrations ginn and cushman 1990 hill 1998 hence not all observations will be equally suitable for estimating optimal parameter values it is important to recognize that the optimal parameter values are not strictly defined as the values that result in the smallest deviations from observations but the values that best represent the true effective parameter values while minimizing the compensation for misrepresentation of measurements and structural model errors in other words minimizing of model error and tuning of parameter values should not compensate for model inadequacies or observation errors for example antonetti and zappa 2018 motavita et al 2019 white et al 2014 even if acknowledging that such parameter values are effective values valid in the given model environment in the context of this work focussing on large scale distributed and coupled surface groundwater models the term structural errors refers mainly to errors in the physical structure of the hydrogeological model similar errors in other model compartments such as vegetation distributions errors in boundary conditions or errors resulting from process simplification or misrepresentation common performance criteria in groundwater modelling contain a summed error term usually as squared errors to avoid cancellation of errors with opposite signs chai and draxler 2014 krause et al 2005 poeter and hill 1997 such performance criteria are sensitive towards outliers as large errors dominate the objective function i e it is common in groundwater model calibration that the objective function is dominated by a relatively small group of observations with large residuals in cases where large initial residuals owe to inappropriate parameter values in an ideal model structure these residuals can be minimized without significant trade off with other residuals however in regional scale groundwater models structural simplification and inadequacy will always cause trade offs between residuals at different locations therefore large errors should not necessarily be forcing the parameter optimization in a certain direction if these large errors are associated with either model structural error or observational outliers still squared error based performance metrics for example the root mean square error rmse sum of squared errors sse or nash sutcliffe efficiency nse are the most common criteria for the evaluation of hydrological models with observations gupta et al 2009 these challenges in dealing with outliers in data in general are well known and have been the motivation for various methods under the term robust regression rousseeuw and leroy 1987 some of these methods require iterative re weighting of residuals which are computationally expensive in model calibration also considerable efforts have been directed towards the better estimation of error distributions relaxing the need for explicit formulation of such and dealing with phenomena such as heteroscedasticity e g cheng et al 2018 schoups and vrugt 2010 sun et al 2017 vrugt and sadegh 2013 this also lead to the acceptance of using subjective likelihood measures beven and binley 1992 beven 2006 doherty and welter 2010 however determining error distributions remains non trivial and usually standard assumptions are used without the ability to formally evaluate them a solution that sometimes is employed to deal with observations that seem to be outliers or cannot be explained by the conceptual understanding of the hydrologist is to simply exclude them from the parameter optimization process or the general model evaluation sometimes this is performed based on simple prior information and assumptions or single model evaluations and residuals boldetti et al 2010 haaf and barthel 2018 højberg et al 2015 keating et al 2010 this can be considered undesired exclusion or lowering the weight of observations based on simple criteria such as residual size prior to calibration can easily lead to misinterpretation since it will be unknown if a given residual can be significantly reduced during optimization more rigorous methods of evaluating prior data conflicts typically require running an ensemble of models prior to calibration doherty and moore 2020 reitan and petersen øverleir 2008 hemmings et al 2020 e g using monte carlo analyses which can be hindering their application with computationally expensive models based on the challenges outlined above this study suggests an objective function that limits the impact on parameter identification originating from observation errors and model inadequacies at the same time the calibration framework avoids the necessity for excluding such observations from the optimization and model evaluation these observations can be informative with respect to model performance evaluation and should not be omitted based on arbitrary criteria but can hamper model parameter identification to achieve this goal we developed an objective function based on the continuous ranked probability score crps gneiting et al 2005 for describing deviations between simulated and observed groundwater head commonly the crps is used as an evaluation tool for probabilistic forecasts here we suggest a novel use of it in the context of an objective function for groundwater models with large sets of point observations we explain the crps based objective function particularly its inherent benefits in weighting between large and small errors we then apply the concept to the calibration of regional scale coupled groundwater surface water models in denmark and compare it to results obtained by using traditional objective functions based on mean squared error mse mean absolute error mae and mean root error mre 2 method the crps is a popular evaluation tool for probabilistic forecasts or model simulations gneiting et al 2005 it can be expressed as 1 crps p s x p o x 2 d x where ps is the empirical cumulative distribution function ecdf of the ensemble predictions for variable x at a certain timestep and po the respective ecdf of the observation or truth of variable x usually the observation is a discrete value hence its distribution is a heaviside step function that changes from 0 to 1 at the observed value this means that in fig 1 the crps can be interpreted as the area between the ecdf of the forecasts in blue and the respective step function of the expected value in red 0 in this case the optimum value of the crps is zero usually the crps is averaged over a series of timesteps it is attractive as it combines reliability and sharpness of forecasts in one indicator furthermore for deterministic forecasts the crps simplifies to the mae in this paper we suggest using the crps outside of its initially intended scope the evaluation of ensembles of forecasts or models instead of being applied to the value of a state variable across members of an ensemble forecast the principle of the crps can also be applied to the value of a state variable across a set of locations observation points which is evaluated against the output from an individual model that means instead of looking at the ecdf of the ensemble forecasts of a certain variable x for example the groundwater head at a certain point across an ensemble of models we consider the ecdf of a certain type of predictions across different points in space for example the groundwater head at several observation points across space in one individual model for this purpose since we have a different expected value or observation at each observation point we simply consider residuals model deviations from the observations instead of absolute values that leaves us with an expected or optimal value of 0 and the ecdf of the collective set of residuals in the model the particular properties of the crps and its differences to the mse are illustrated in fig 1 inspired by hersbach 2000 which shows the empirical cumulative distribution of a dataset of five predictions commonly in ensemble forecasting this would be predictions of the same variable from five members of a model ensemble in our case these are residuals of a simulated variable groundwater head at different observation locations the verification value or truth which is commonly an observation of the variable in question is marked by the red line when using residuals the expected or true value is 0 the crps can be represented by the sum of the areas dx dp2 in the left panel of fig 1 accordingly the crps for an empirical cumulative distribution can be written as 2 crps i 1 n d x i d p i 2 where dxi xi 1 xi and dpi is the cumulative probability of the ecdf not exceeding xi if xi is less than the expected value or 1 minus that cumulative probability if xi is greater than the expected value commonly the mse is given as 3 mse 1 n i 1 n x i o 2 where o is the expected value or observation in the right panel of fig 1 representing the mse in a notation similar to equation 2 xi o is represented by each dxi and dpi has a constant value of 1 n for an empirical cumulative distribution with n values accordingly the mse can be represented as the sum of areas dx2 dp in the right panel of fig 1 and rewritten as 4 mse i 1 n d x i 2 d p i correspondingly the mae is the sum of dx dp in either panel noteworthy are the different contributions of the single predictions to the aggregated crps or mse value grey areas in fig 1 the single prediction with the highest deviation from the truth is 4 and might be considered an outlier for the crps that prediction contributes a relatively small amount to the total crps 21 whereas for the mse that prediction accounts for 72 of the total mse in a model calibration using the mse or rmse as an objective function most algorithms will inevitably focus on improving the model fit for that particular point as it contributes such a large share to the total objective function however considering that such large deviations between model and observations at certain points often owe to observation uncertainty or issues with the model structures this could be undesired behaviour the crps is much less dominated by the largest deviations as it squares along dp equation 2 instead of dx as the mse equation 4 it assigns a relatively lower weight to the residuals at the outer ends of the ecdf these residuals are also the largest and in our context often those we have a low confidence in being informative to the model parameter estimation in contrast the crps favours sharp and reliable distributions of predictions because of the relatively large values assigned to predictions close to the truth as can be seen in fig 1 therefore we find it relevant to investigate the use of a crps based objective function for the optimization of large scale groundwater flow models 2 1 crps in comparison with other common performance criteria to illustrate the implications of using either crps or the mse as performance criteria some example error distributions based on gaussian distributions were compared being less dominated by large residuals the mae and mre were also included fig 2 shows those distributions and their respective performance criteria values the expected value red line always is 0 the modelled errors cumulative distribution function is given by the blue line for the reference top left this error distribution is a normal distribution with a standard deviation with a mean of 0 and a variance of 1 simply increasing the error top right by doubling the standard deviation leads to a crps and mae twice as large as in the reference an mre 1 4 times as large and an mse four times as large when a bias is introduced by changing the mean of the error distribution to 0 5 bottom left however the crps shows more sensitivity than the other metrics its value increases 1 4 fold over the crps in the reference whereas the mse mae and mre only increase 1 2 1 1 and 1 1 fold respectively over their values in the reference when outliers are being introduced to one end of the distribution bottom right by replacing 10 of the reference with absolute value samples from a normal distribution with a standard deviation of 5 the crps shows less sensitivity it only increases 1 2 fold compared to its value in the reference the mse increases 3 4 fold and even the mae is more sensitive than the crps increasing 1 4 fold compared to its reference value only the mre shows a similar sensitivity to these outliers as the crps by also increasing 1 2 fold 2 2 using the crps as an objective function for single model realizations we suggest using the crps outside its original scope of probabilistic forecasting but as an objective function for single model realizations that is we use the crps as a heuristic norm like one could also use the mse mae or mre related to the l2 l1 or l1 2 norm though the l2 norm is most commonly used different norms have been suggested in the context of inversion in hydrogeology e g the l1 norm in carrera et al 2005 still bringing the crps into the realm of single model realizations raises questions of its applicability as it in theory requires an assumption of ergodicity however its use also has been suggested in conjunction with non ergodic schlather models see yuen 2015 stochastic hydrology generally relies on ergodicity as an assumption e g gelhar 1986 however it is also acknowledged that ergodicity is hard to prove suciu et al 2006 sanchez vila and fernàndez garcia 2016 especially in many real world applications yet it commonly is assumed sanchez vila et al 2006 we assume that as we are using the crps like a norm see also fig 1 we can use it like other norms to inform a parameter estimation process such as the commonly used mse 3 model and data 3 1 the two study areas to illustrate the possible advantages of utilizing a crps based objective function we will evaluate the effect of different objective functions on the calibration of two real world regional scale coupled distributed groundwater surface water models the two study cases are situated in denmark the catchment of the river storå in western jutland and the catchment of the inner odense fjord on the island of funen which is dominated by the river odense å fig 3 both catchments are approximately 1 000 km2 large with generally gentle slopes and topography ranging from sea level to approximately 120 m geologically the storå catchment is more dominated by sandy soils especially in its western part whereas the odense catchment is dominated by clayey soils generally groundwater levels in denmark are shallow with moderate seasonal variations in the range of 1 m koch et al 2019 however several layers of aquifers can exist denmark has a temperate climate and the average yearly precipitation is roughly 1 000 mm and 800 mm in the storå and odense catchments respectively 3 2 hydrologic modelling framework both models were set up within the mike she hydrologic modelling software abbott et al 1986 mike she offers a transient fully distributed physically based description of the terrestrial part of the hydrological cycle it couples 3d subsurface flow 2d overland flow unsaturated zone processes and routing of surface water in streams the unsaturated zone is represented by the 2 layer method of mike she which includes the processes of interception ponding and evapotranspiration it simplifies the unsaturated zone to two layers the first layer representing the root zone which varies across vegetation types and seasons and the second representing the zone below the root zone down to the water table of the saturated zone dhi 2019b p 27 the two models are based on the national water resource model of denmark referred to as dk model developed at the geological survey of denmark and greenland henriksen et al 2003 højberg et al 2013 stisen et al 2019 the dk model is used in national water management and assessment of human and climate change impact which is reflected in the model evaluation against large national datasets of in situ groundwater head observations as well as streamflow observations with a focus on representing the overall water balance the two used submodels of the national model were originally setup within a project exploring possibilities of further developing the dk model stisen et al 2018 like the dk model the horizontal grid resolution is 500 m the description of the subsurface is based on a hydrogeological model covering all of denmark arvidsen et al 2020 stisen et al 2019 the parameterization is unit based i e each hydrogeological unit is assigned homogenous model parameters for hydraulic conductivity etc for reasons of computational efficiency some layers of the hydrogeological model have been combined resulting in seven computational layers in the storå model and ten in the odense model in mike she computational layers span the entire domain and are reduced to a minimum thickness 0 5 m in our case where the respective geological formation does not exist in those areas parameter values are taken from the respective nearest existing geological formation dhi 2019a p 313 artificial drainage widespread across denmark is included in the model snow accumulation and melt are incorporated with a degree day method meteorological forcing with daily timesteps of precipitation stisen et al 2012 reference evapotranspiration and air temperature is available from national gridded datasets scharling 1999b a further forcing include groundwater extractions for drinking water supply and irrigation the models are run with flexible timesteps allowing a maximum timestep of 24 h 3 3 hydrologic model calibration data as for the dk model the models were evaluated against groundwater head observations from a series of boreholes and runoff timeseries at stream stations displayed in fig 3 the dataset of groundwater head observations consists of borehole and groundwater level information from the public national danish database for boreholes and groundwater jupiter https eng geus dk products services facilities data and maps national well database jupiter in total the dataset for the storå model contains groundwater heads from 890 wells some boreholes have several observations in time and from different depths offering a total of 5 218 individual head observations for most wells only a single groundwater head observation exists only 30 of those wells offer time series with more than ten observations in time for the odense model data exists from 1 820 wells with 44 273 individual head observations 205 of those wells offer more than ten observations in time as with the dk model model simulations were also evaluated against discharge data based on timeseries of observed stream discharge with daily resolution from six stations in the storå catchment and nine in the odense catchment 3 4 hydrologic model calibration setup the models were calibrated with the ostrich optimization software toolkit matott 2017 using the pareto archived dynamically dimensioned search padds algorithm asadzadeh and tolson 2013 tolson and shoemaker 2007 padds is a global multi objective optimization algorithm that is geared towards effectively defining the pareto front for a set of objective functions in our case this allowed to simultaneously calibrate towards five objective functions discharge performance in streams and the mentioned four groundwater head objective functions all calibration and evaluation exercises were performed for the period 2000 to 2008 with a warm up period starting in 1990 discharge performance was included by using the kling gupta efficiency kge gupta et al 2009 5 kge 1 r 1 2 α 1 2 β 1 2 where r is the pearson correlation coefficient between observed and simulated discharge as daily timeseries in our case α is the standard deviation fraction and β the bias fraction of observed and simulated streamflow the groundwater observation dataset consists of observations of groundwater heads from boreholes each borehole can contain one or several observations in time and can contain one or several screen depths i e observations from different aquifers it was chosen to aggregate all observations within each single model grid cell and layer after calculating the individual residuals at their exact points in time and space this was done mostly because a model grid cell is the smallest unit the model can resolve individual head measurements within one model grid can contradict each other due to heterogeneities below the model scale and typical seasonal variations of groundwater heads are in the range of 1 m which is below the typical residuals similar aggregation is applied in other practical applications of large scale models with scattered distribution of observations in space and time e g sonnenborg et al 2003 weighting then was applied according to the number of observations per grid cell a relative weight of 1 was applied to cells with one observation a weight of 2 to cells with two to nine observations a weight of 3 to cells with ten to 99 observations and a weight of 5 to cells with 100 or more observations then the model was evaluated based on the mean error of all the observations within each model grid cell across time four different objective functions were included first a sum of squared errors sse 6 sse i 1 n m e i 2 where mei is the mean error of simulated groundwater heads in each of the n model grid cells containing observations second a sum of absolute errors sae 7 sae i 1 n m e i third a sum of root errors sre 8 sre i 1 n m e i 1 2 note that the terms sse and mse sae and mae and sre and mre can be used interchangeably in this context as mse 1 n sse etc as mse mae and mre are the more common terms we will use those throughout the rest of the paper lastly the crps based objective function see equation 1 using the ecdf of all model grid cells mei and a mean error of 0 as expected value in the storå model eight model parameters were calibrated six different geological units hydraulic conductivities the root depth and the saturated zone drain time constant for the odense model the calibration was set up with six free parameters four hydraulic conductivities the root depth and the drain time constant in the calibrations the padds algorithm was run with 5 000 model runs for the synthetic calibration experiments see section 3 5 below and 3 000 runs for the calibrations with real data parameter bounds were set to values between 1 10 7 and 1 10 2 m s for geological units representing sands 1 10 9 and 1 10 4 m s for geological units representing clays 1 10 10 and 1 10 5 s 1 for drain time constants and 200 and 2000 mm for root depths in its implementation in ostrich padds stores all solutions it explores along its search as we focus on the groundwater head objective functions we used the streamflow kge to remove non behavioural solutions that have a mean kge below 0 7 from the remaining solutions we picked the best performing solution in terms of each of the groundwater head metrics crps mse mae and mre defining four optimal solutions dependent on metric choice more information on the padds algorithm and the use of the pareto front it produces can be found in literature e g holmes et al 2020 koch et al 2022 3 5 synthetic hydrologic model calibration setup to further test the assumption that a crps based calibration is less likely to compensate for model structural errors or systematic observation errors by parameter compensation and similar we performed some synthetic calibration experiments the synthetic calibration experiments were performed with the storå model described above a certain realization i e with a specific set of parameters of the storå model served as a reference model or synthetic truth from a run of this reference model we sampled synthetic observations from the simulated groundwater heads those synthetic observations were sampled at the exact same time and locations where real observations are available normally distributed noise with zero mean and 0 5 m standard deviation was added to the synthetic observation to account for random observation error some of those observations were further perturbed potentially representing a scenario with larger yet undetected observation errors or a structural error in the model leading to a local inability of the model to preproduce observed groundwater heads in this example all observations within a rectangular spatial extent within the uppermost five layers of the model the quaternary layers and with an observed water level at least 5 0 m below the surface were perturbed this selection resulted in observations from 21 wells displayed in fig 4 being perturbed out of a total of 890 wells by adding 6 0 m to their observed value i e their synthetic truth then starting from a random initial parameter set the model was calibrated as described in section 3 4 using the synthetic observations with perturbations as described above again the four different groundwater head objective functions crps mse mae and mre were included 4 results after having shown illustratively how different objective functions react to hypothetic errors distributions in sections 2 and 2 1 now results of the calibration experiments of the hydrological models will be presented starting with the synthetic examples and then moving on to calibrations of the models against real world data 4 1 results of the synthetic hydrologic model calibration to display the benefits of the crps based objective function in a realistic but controlled model environment synthetic calibration experiments were carried out fig 4 shows the average deviation of simulated groundwater heads across all model layers from the reference model s simulated groundwater heads for the crps mse mae and mre optimal solution respectively the 21 wells with perturbed synthetic observations clearly have the largest effect in the mse optimal solution leading to the calibrated model with the largest deviation from the synthetic truth the mae mre and crps based calibrations all perform similarly well however the crps based calibration shows the lowest bias among the calibrated models furthermore the estimated parameters resulting from the crps based calibration are closest to the values of the reference model weighted by the parameter sensitivity the mean absolute deviation from the reference model s parameters for the crps optimal solution is 0 9 whereas it is 1 1 1 4 and 1 6 for the mae mre and mse optimal solutions respectively 4 2 results of the hydrologic model calibration advancing to the calibrations against real observations table 1 summarizes the results for the storå and odense models kge is included for reference purposes as mentioned the set from which the optimal solutions were picked was limited to those with an average kge of above 0 7 not surprisingly the best value for each performance metric is achieved for the best solution in terms of the respective objective even though the trade off between the four groundwater head objective functions is small especially in the storå model the solutions still are separated the resulting parameter values for all the picked solutions generally fall into expectable ranges with horizontal hydraulic conductivities in sand layers ranging from 1 10 2 to 1 10 4 m s with one exception of 1 10 7 m s in the deeper sand layers of storå and 6 10 5 to 1 10 8 m s in clay layers respectively the mean error me across all groundwater heads not included as an objective function also shows the expected behaviour in the odense model where the crps optimal solution clearly exhibits the lowest me or bias and the mse optimal solution the highest me for storå the behaviour is slightly erratic with the crps mre and mse optimal solutions performing similarly well in terms of me in the following we will focus on the differences between mse optimal solutions as this is the most common objective function and optimal solutions in terms of the proposed crps based objective function fig 5 displays the absolute mean errors of groundwater head in each grid cell with observations y axis in relation to the difference in mean error in each grid cell between the crps optimal and mse optimal solutions x axis for the majority of all grid cells the error of the crps optimal solution is slightly lower than the error of the mse optimal solution which places them in the left white half of the plot only few observations amongst those many with large errors exhibit a larger error in the crps optimal solution which places them in the right grey half of the plot this is in line with the results in table 1 the crps optimal solution results in a slightly higher rmse yet a lower me furthermore table 2 shows that the majority of cells with small error below 5 m exhibits smaller errors in the crps optimal solution compared to the mse optimal solution this applies to 57 and 67 of those grid cells in the storå and odense model respectively in contrast 77 and 84 of cells with large error above 10 m exhibit larger errors in the crps optimal solution the same can be shown for a comparison of the crps optimal with the mae optimal solution the picture only differs marginally from fig 5 etc hence the results are not shown here in the two maps in the top row fig 6 shows all grid cells with observations colour coded for their error in the crps optimal solution the middle map of each model displays only those cells which show a lower error in the crps optimal compared to the mse optimal solution the bottom map displays only those cells which show a larger error in the crps optimal compared to the mse optimal solution large errors dominate the bottom maps of both models reddish and blueish colours while small to moderate errors dominate the middle maps yellowish colours more interestingly some distinct areas with almost exclusively large errors become apparent in the storå model there appear to be specific issues in the northwest corner of the model where we overestimate groundwater heads blue colours and in the southwest corner where we underestimate groundwater heads red colours in the odense model there are two distinct regions in the south east and mid west where groundwater heads are underestimated by the model 5 discussion the issues arising from squared error based performance criteria being sensitive to even few outliers and extreme residuals are well known berthet et al 2010 legates and mccabe 1999 moriasi et al 2007 chai and draxler 2014 chen et al 2017 and also discussed in time series forecasting in general armstrong and collopy 1992 chen et al 2017 this also led to the recommendation of using multiple performance metrics as any single metric will provide information only on a certain aspect of the error characteristic chai and draxler 2014 also the popular nse exhibits similar issues of being highly influenced by outliers as it is also based on the squared error term krause et al 2005 mccuen et al 2006 for example bradley et al 2004 or berthet et al 2010 showed that mse based performance criteria are dominated by a small number of data points in cases where there are both some large negative as well as some large positive residuals as is often the case in groundwater head calibration a squared error based optimization is virtually limited to finding the best trade off between these two groups without much consideration to the majority of residuals which fall within a reasonable range and which should ideally inform parameter identification alternatively an outlier filtering can be performed however with the mentioned large non scientific datasets as used in our model case and when filtering is performed prior to the parameter estimation it is hard to determine which observations with large residuals represent observational outliers indicate model structural deficiencies or non optimal parameter values statistically sound outlier filtering often requires timeseries of observations jeong et al 2017 peterson et al 2018 whereas our observation dataset is comprised of many observation points with only a single or few observations in time and only few observation points with a whole time series outliers can also be detected based on spatial patterns bárdossy and kundzewicz 1990 however the dataset s spatial coverage can be too coarse and scattered compared to the variables actual spatial variability to allow for reliable outlier detection based on spatial patterns or such methods require assumptions about model structure helwig et al 2019 or still rely on a row of subjective criteria tremblay et al 2015 as mentioned in the introduction it appears that the scientific groundwater modelling literature assumes that data have been through a data control process where outliers have been identified and removed before the data are used in a modelling context however this is i not practical in case of data from thousands of observation wells from different sources with different and unknown data quality that are not originating from scientific monitoring programmes and ii risks omitting data containing valuable information about potentially deficient model parameters or model structures for such cases there is a lack of methodology metrics that can make use of all available data without allowing outliers to dominate already in its original context of ensemble forecasting the crps was considered robust to outliers gneiting and raftery 2007 the crps based objective function addresses the limitations of the squared error based objective functions by putting more emphasis on the position of each residual along the cumulative distribution than on the magnitude of each residual effectively this means that large less frequently occurring residuals are assigned a relatively lower weight than in squared error based metrics in our context it is an underlying assumption that during the optimization process the least probable residuals will coincide with the largest residuals for large datasets with some theoretic error distributions fig 2 it can be illustrated that the crps is less sensitive to outliers in an error distribution than squared error or even absolute error based performance criteria on the other hand it is more sensitive to the overall bias of all errors if used as an objective function the crps will focus on reaching a balanced result with little bias while accepting some potential outliers with large residuals the mse and also the mae will more likely result in a distribution with some overall bias if only the largest residuals can be reduced as those dominate the overall value of the metric the mre can be seen as similarly insensitive to large residuals as the crps however it is less sensitive to bias in real world cases with large observational dataset of mixed and uncertain data quality and even complex hydrological models exhibiting inevitable simplifications of processes and structures we have to accept that a part of the observational dataset will exhibit large residuals nonetheless we still assume that after parameter estimation the majority of the observations can fall within a reasonable margin of error and those large residuals are considered less informative to the parameter estimation process furthermore in coupled surface groundwater models a low overall bias potentially resulting in low water balance errors is of great interest this is where we can utilize the strength of crps based performance criteria over mse based criteria in synthetic calibration experiments it could be shown that perturbed observations have a lesser effect on the parameter estimation process when using the crps than when using the mse where the mre and mae performed similar to the crps when comparing the four performance criteria applied to the parameter estimation in two real world groundwater surface water models the crps optimal solutions showed an improvement of the majority of residuals which are mostly small compared to mse optimal solutions in line with expectations only few residuals are increasing in the crps optimal solution most of them large see fig 5 and table 2 with the given model structure based on geological units and units based parameters these large residuals especially in error hotspots as visible in fig 6 can indicate a general issue with the model structure for example the physical structure of the unit based hydrogeological model which likely can be considered the main structural error in our case højberg and refsgaard 2005 refsgaard et al 2012 or boundary conditions in parameter estimation we explicitly want to avoid compensating for issues such as model conceptual errors or observation errors by trying to tweak model parameters in other words such large residuals are not considered informative to the parameter estimation due to its characteristics of being comparably insensitive to outliers but more sensitive towards an overall bias a crps based calibration is less prone to show such undesired behaviour it will i maintain and make such error hotspots more pronounced allowing for easier identification of general issues with model structure ii not compensate as much for observations that are inadequate or incompatible with the model structure by tuning parameters values and iii obtain best model performance for the vast majority of observations this points in the direction of an optimization process where all available observations are included in a parameter estimation however observations that represent conditions not consistent with the model structure or have a large error do not dominate the outcome of the parameter optimization process to the same extent as when using mse based criteria subsequently these observations can be evaluated when they illuminate a systematic model structural deficiency they can lead to a re evaluation of the model structure prior to further parameter estimations in other cases for example where observations with large residuals are scattered randomly across the model domain they can be regarded as not representative of model scale and structure as observational outliers or simply as indicators of the overall model uncertainty it is worth highlighting that the crps based objective function proposed here for calibration of groundwater flow models with large observational datasets assumes that after adequate calibration only a limited number of large residuals remain the remaining residuals potentially can represent areas of either model structural uncertainty including issues such as errors in the hydrogeological model or inadequate boundary conditions or highlight observational errors and do not dominate the total number of observations our real world model applications are examples where this is the case compare fig 6 the proposed crps based groundwater head objective function can be used in multi objective settings in combination with other performance metrics such as streamflow metrics used in this article furthermore one can envision the use of the crps as a heuristic norm also for other types of observational datasets than groundwater heads as in this example 6 conclusions there are few examples in the literature that attempt to practically address the well known issues related to the use of squared error minimization in optimization problems these problems are specifically relevant for optimizations problems in models with uncertain model structures and large uncertain evaluation datasets both being common in large scale groundwater modelling squaring errors magnifies large values consequently a parameter estimation minimizing an objective function based on squared errors will focus on reducing the largest errors often the largest residuals are the result of model structural uncertainty or observation uncertainty i e they may not be informative to the parameter estimation process yield in effective parameters compensating for such issues and consequently risk producing a model with a skewed representation of the variable of interest groundwater head in our case with this paper we wanted to highlight those issues with squared error based performance criteria and compare an mse based criteria to a mae mre and crps based criteria in particular we suggested the use of a crps based performance criteria as a heuristic norm in the objective function of hydrological model parameter estimation the crps has some desirable properties that we demonstrated in synthetic and real calibration experiments compared to mse and to lesser extent also compared to mae the crps is less sensitive to large residuals at the same time it is more sensitive to the overall bias even compared to the mre the crps originates from the evaluation of probabilistic forecasts to our knowledge it has not been used before as performance criteria in our context of deterministic model optimization in practical applications such as in our example where we used a large dataset of groundwater head observations from a public database with varying and undetermined quality in combination with a regional scale groundwater model model structural uncertainty and observation uncertainty are difficult to quantify hindered for example by the computational and model setup effort related to ensembles representing various model structures ideally rigorous hydrologic model building and testing should include the evaluation of various model structures enemark et al 2019 the same applies to global scale hydrologic modelling where often creative calibration targets are used e g yang et al 2019 or where parameters are even estimated without calibration maxwell et al 2015 another example is a calibration of stream flow in the signature domain instead of the time domain assuming this makes the calibration more robust to objective function or data deficiencies fenicia et al 2018 westerberg and mcmillan 2015 the work presented in this paper is a practical way of dealing with observation uncertainty in cases where those uncertainties are hard to quantify it can also help shed light on areas of model inadequacy however mathematically sound accounting for such effects requires at least some idea of the magnitude of observation uncertainty or model inadequacy see for example the discussion of necessary prior knowledge about model discrepancy in brynjarsdóttir and ohagan 2014 yet in any case the influence of the chosen objective function due to its properties such as the magnification of large residuals should be considered the challenges mentioned related to large datasets with unknown observation uncertainty combined with model structures simplifying the real world due to scale issues data availability or computational limitations also occur in other areas than distributed hydrological models as in this example hence a look beyond squared error based performance is also expected to pay off in other modelling contexts and tackle the mentioned issues with squared error based performance criteria one could also envision the use of a robust objective function in the estimation of highly parameterized parameter fields such an application is beyond of the scope of this work the focus here was to shed light on the influence of the choice of the objective function on parameter estimation and certain implications of the most commonly used squared error based metrics in different contexts different objective functions might be best suited we have suggested a robust objective function that is beneficial in many practical applications of hydrological models credit authorship contribution statement raphael schneider methodology software investigation formal analysis data curation writing original draft writing review editing visualization hans jørgen henriksen supervision project administration funding acquisition writing review editing conceptualization simon stisen conceptualization methodology visualization writing original draft writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we want to gratefully acknowledge the fods 6 1 fasttrack metodeudvikling project headed by styrelsen for dataforsyning og effektivisering sdfe danish agency for data supply and efficiency in which context the presented method was developed furthermore we want to thank jens christian refsgaard for kindly providing helpful feedback to improve the manuscript and peter bauer gottwein for insightful thoughts on the topic data statement python scripts used to calculate the four different groundwater head objective functions including the crps based objective function are publicly available together with some example data https github com snowthe crps heuristic of the entire model and calibration setup can be made available upon personal request to the corresponding author without undue reservation 
2888,in groundwater protection zones water managers and water providers will typically need to know the time scale of recovery for aquifers contaminated by agrochemicals despite the large body of work on the topic in the scientific literature there still seems to be some need to clarify which waiting times are most relevant for water management planning and how they relate to metrics such as the mean transit time of tracer firstly we propose a simple nomenclature for the different waiting times and how they relate to the evolution of agrochemical concentration increasing and decreasing concentration trends trend reversal etc secondly we describe how to select and fit a lumped parameter model to contaminant time series and environmental isotopes in order to characterize aquifer response to agrochemical contamination thirdly we explain how waiting times can be calculated from the fitted lumped parameter model finally we present a case study focussing on the contamination of several springs of the city of luxembourg by a fungicide transformation product combining environmental tracers and pesticide measurements we find that on the study site the parameters estimated separately from the environmental tracers and from the pesticide data are nearly the same although they correspond to different areas within the groundwatershed although this could be incidental it might also indicate that robust waiting time estimates can be obtained from agrochemical time series and environmental tracer measurements at least in simple cases keywords lumped parameter models water protection zone contaminant transit time mean transit time waiting time trend reversal data availability the water quality data was uploaded as supplemental material 1 introduction waiting time estimates can play an important role in the management of diffuse groundwater contamination by agrochemicals typical situations faced by water providers or environmental agencies and involving waiting times are for instance mitigation measures implemented to reverse the upward concentration trend of a contaminant have not been followed by any observable improvement are the measures insufficient or is the aquifer acting as buffer a herbicide has been quantified in an aquifer at concentrations exceeding the drinking water quality standard thus forcing the local water provider to suspend the distribution of the contaminated groundwater subsequently all use of this herbicide has been forbidden within the water protection zone how long will the water provider have to wait until the concentrations have decreased sufficiently for the groundwater to be used again for drinking water purposes lumped parameter models originally developed to estimate aquifer parameters concerning aquifer storage and flow velocities from environmental tracer measurements eriksson 1958 małoszewski and zuber 1982 can easily be adapted to estimate groundwater waiting times and have the advantage of simplicity compared to a coupled flow and transport numerical model once parameterized a lumped parameter model can be used to quantify aquifer vulnerability to a contaminant input einsiedl et al 2009 to predict the evolution of diffuse pollution by agrochemicals małoszewski 2000 morgenstern et al 2015 visser et al 2007 and to estimate waiting times relevant for water managers thus bridging the gap between science and practice although a large body of literature has accumulated on the relationship between aquifer transit times and aquifer response to agrochemical contamination fovet et al 2015 molenat and gascuel odoux 2002 morgenstern et al 2015 owens et al 2008 schwientek et al 2009 tomer and burkart 2003 visser et al 2007 wassenaar et al 2006 there is still room for a method article written from the perspective of water managers needs in this article we wish to i clarify which waiting times are directly useful for water managers and how they differ from the classical groundwater transit time metrics such as the mean transit time of tracer ii show how these waiting times can be calculated from a parameterized lumped parameter model and iii demonstrate how existing water quality data can be used in combination with environmental tracer measurements for parameterization for the sake of illustration a case study focussing on the contamination of a luxembourgish aquifer by pesticide degradation products leached from commercial orchards is presented 2 groundwater dating basics we start first by reviewing briefly a few basic elements of groundwater dating a tracer is a measurable compound injected in the subsurface and from which useful aquifer parameters can be gained concerning solute storage and transit times an ideal tracer behaves strictly like a water molecule an environmental tracer is a tracer applied naturally by rainfall environmental tracers are injected over the entire surface of groundwatersheds albeit in a fashion that may vary spatially depending for instance on altitude or catchment size the travel time of a tracer or a contaminant from the point it is released from the soil to an outlet such as a pumping well or a spring will depend on the distance between the two points all else equal recharge rate porosity aquifer thickness the sampling point at any given time will usually intersect more than one flow line and thus the total flux at this point will cover a large range of individual transit times from the contributing surfaces the frequency distribution of the entire groundwatershed s transit times normalised to an integral of 1 will further be called transit time distribution and defines how an aquifer will release a tracer injected instantaneously and homogeneously over the entire recharge area in complex hydrogeological situations with multiple outlets both natural and man made such as pumping wells the transit time distribution of a single outlet will be different from the transit time distribution of the entire groundwatershed etcheverry 2001 in this paper only simple cases will be presented for which outlet and groundwatershed transit time distributions can be considered to be the same for the sake of mathematical analysis the transit time distribution is described by a model different model shapes have been described in the literature depending on the hydrogeological setting on the sampling situation and on transport processes amin and campana 1996 etcheverry 2001 małoszewski and zuber 1982 once a model is chosen its model parameters must be estimated or fitted from tracer measurements models have a different number of fitting parameters the main one being the mean transit time which is the weighted average of all flow lines individual transit time as we will show waiting times directly relatable to water management questions can be calculated easily after the model parameters of the transit time distribution have been estimated 3 waiting times of agrochemical contamination in groundwater protection zones in order to evaluate the efficiency of mitigation measures or to plan for technical remediation measures water managers need to know wassenaar et al 2006 i when concentrations at a sampling point should be expected to begin their decrease after the input has been reduced ii how long the decrease will last until a threshold concentration is reached either a water quality standard or another operational concentration and iii what the maximum contaminant concentration will be before trend reversal is reached point i is particularly important during the evaluation of mitigation measures point ii provides a useful basis when for instance deciding whether a treatment plant should be installed or if waiting for natural attenuation is a realistic alternative a typical application of point iii would be to answer whether maximum concentrations will or will not exceed the water quality standard in the case of an increasing trend fig 1 presents the idealised evolution of a contamination by agrochemicals measured in a drinking water uptake such as a well or a spring in response to a continued agrochemical leaching after leaching has begun there is a time lag before the contaminant first arrives at the uptake then a continuous increase up to a plateau concentration once the leaching ceases as a result of mitigation measures for instance another time lag is observed before concentrations start decreasing followed by a continuous decrease over time if leaching is stopped or reduced before a plateau is reached the upward trend will be followed directly by a downward trend and the point of inflexion will be called trend reversal the time lag between the enforcement of mitigation measures and the beginning of a decrease in concentration at the sampling point will be further referred to as reaction time treac the time lag between the beginning of a concentration decrease and the return of concentrations to a threshold value as self purification time tpurif and the total time between the implementation of mitigation measures and the achievement of a given water quality goal as recovery time trecovery these three waiting times are what water managers are directly interested in 4 model choice and relationship between model parameters and waiting times in the case of an agrochemical released from farmed areas four elements need to be taken into account to model the evolution of water quality at a drinking water outtake the time lag between the application of an agrochemical and its first arrival at the sampling point the range of travel times of an agrochemical at the outlet depending on the distance from each leaching point to the outlet dilution by recharge water from surfaces where the agrochemical is not applied degradation of the agrochemical below the root zone different models could be used to simulate these elements we choose here the combined exponential piston flow model epm the piston flow component simulates the first arrival time lag while the exponential component simulates the range of transport times from the contributing areas the epm is defined as follows modified from małoszewski and zuber 1982 1 g τ 0 if τ t pf g τ 1 t em exp τ t em t em t pf t em 1 if τ t pf with tem being the mean transit time of the exponential part t and tpf the piston flow transit time t if a spatially homogeneous input as well as steady state can be assumed tracer input injection in the unsaturated zone and output exit at the sampling point are related by the epm as follows modified after małoszewski and zuber 1982 2 c out t 0 c in t τ e x p λ τ g τ d τ with c out t being the tracer concentration at the system s outlet as a function of time m l3 c in t the tracer input as a function of time m l3 g the transfer function τ an integration constant equal to the total transit time of each flow line t and λ the decay or degradation constant respectively for radionuclides or bio degradable agrochemicals t 1 a dilution factor α must be integrated in the input time series c in of eq 2 to take into account surfaces not contributing any agrochemical α is assumed to be invariant over time because it depends primarily upon the ratio of the area of the contributing surface relative to the total surface area of the groundwatershed the smaller the surfaces contributing contaminant relatively to the size of the groundwatershed and the larger the dilution will be 3 c in t c leach t α with c leach being the soil leaching concentration of the agrochemical m l3 and α the dilution factor the experimenter must decide how to best approximate c leach depending on available data and study aims a constant annual leaching will suffice to predict the progressive concentration increase resulting from repeated annual applications of an agrochemichal over a period of many years whereas a step function is a simple way to simulate drastic mitigation measures leading to a significant reduction of the input farlin et al 2013b a numerical codes can also be used to calculate more complex leaching patterns taking into account application times and doses as well as changing environmental conditions and rainfall farlin et al 2018 in the case of environmental tracers tracer injection in the subsurface can be considered homogeneous over the entire recharge area as long as the groundwatershed is not too large and in the absence of an important altitude gradient degradation rates are usually smaller in the unsaturated and saturated zones than in the soil zone especially the topsoil where most of the microbial activity is concentrated chilton et al 2005 moreau and mouvet 1997 morvan et al 2006 pothuluri et al 1990 this is for instance the case where low doc content in the groundwater is insufficient to sustain substantial microbial activity even so degradation losses below the root zone can still be significant because transport times are usually much longer in the subsurface than in the soil for example it was calculated by farlin et al 2013c 2017 that up to 80 of the leached mass of atrazine and its transformation product desethylatrazine can be degraded during transport through the luxembourg sandstone first order degradation can be simulated using eq 2 by setting a positive degradation rate λ but at the cost of having to estimate its value from data all three waiting times relevant for water management can be obtained from eqs 1 3 and are given in eqs 4 6 the reaction time treac for a given contaminant within a groundwatershed depends on the distance between the treated surfaces and the outlet and on the lag time imparted by the soil column and by the unsaturated zone and is simply the piston flow component tpf of the epm 4 t reac t pf fig 2 presents an example of a configuration for which a lag time due to the distance between input and outlet should be expected this distance related shift in tracer response corresponds to the example presented in małoszewski and zuber 1982 fig 1 situation 3 for which the infinitesimally short flow lines do not contribute tracer to the outlet in małoszewski and zuber because of a impermeable overburden the self purification time tpurif will depend on the water quality goal to be reached and on the inertia of the aquifer which itself depends on the mean transit time of solute through the saturated zone here tem tpurif can be found using eq 2 by setting the water quality goal as c out and finding the corresponding t for the exponential model 86 of the total aquifer pore space mobile and immobile zones will have been flushed by recharge water after a time period twice the mean transit time and 95 after three mean transit times thus one can consider as a rule of thumb that three times the mean transit times of the saturated zone will be necessary to approach a new steady state concentration after a change in the input either a reduction or an increase eq 5 5 t purif 3 t em finally the total recovery time trecovery is the sum of the reaction time and the self purification time eq 6 6 t re cov e r y t reac t purif depending on the particulars waiting times can be given as a time span or in calendar years i e treac for instance can be either 10 years or 2030 for a starting date in 2020 5 calibration strategy in order to estimate treac tpurif and trecovery the two free parameters of the epm tem and tpf the dilution parameter α and the degradation parameter λ must be fitted to the known or reconstructed input and output time series by inverse modelling in many cases however the available data will be insufficient to calibrate both dilution and degradation parameters separately so that both must be lumped together to estimate the overall effect of dilution and degradation on output concentration yielding three fitting parameters tem tpf and α λ it is important to note that aquifer degradation will change the magnitude of agrochemical concentration in the output but not the shape of the output response farlin et al 2013b fig 3b thus when only the temporal evolution of an agrochemical concentration is needed and not its absolute concentration neglecting degradation will greatly simplify the calibration procedure when estimates of concentrations at the outlet are necessary neglecting degradation would yield conservative estimates corresponding to a worst case scenario once all parameters have been estimated the evolution of the concentration of other agrochemicals at the sampling point can be calculated by forward modelling małoszewski 2000 information gained from forward modelling is obviously what water managers will be interested in three different cases must be distinguished when calibrating the transit time distribution by inverse modelling depending on the available data case 1 the time series of the contaminant for which prediction is needed is sufficient to estimate all three fitting parameters case 2 another contaminant originating from the same source area can be used for parameterization case 3 only environmental tracer data and or a time series of contaminants originating from other source areas within the groundwatershed are available for inverse modelling case 1 applies when all parameters are estimated directly for the compound of interest but will be of limited practical interest since both maximum concentration and reaction time will be already known leaving only the self purification time to be predicted cases 2 and 3 are purely inverse modelling procedures designed to obtain the parameters of the transit time distribution if the contaminant data set is insufficient for that purpose in many cases such as for an incomplete increasing trend fitting three parameters to observations will make the inverse modelling problem non unique whenever this problem appears the following fitting scheme can be adopted 1 estimation of tem from an appropriate environmental tracer or a contaminant displaying a decreasing trend 2 use of the estimated tem in eq 2 and fitting of tpf and α λ to the data of the contaminant displaying the increasing trend waiting times can be estimated before the contaminant has even begun to appear at the sampling point this is only possible if another contaminant originating from the same sub catchment can be used for calibration let a be the contaminant already measured at the sampling point and b the contaminant not yet observable there in this case the calibration scheme is as follows 1 estimation of tem using either a if possible or environmental tracers or a contaminant displaying a decreasing trend 2 estimation of tpf and α λ using contaminant a 3 use of tem tpf and α λ in eq 2 yielding a predictable evolution of the concentrations of b as well as an estimate of c max it is important to note that the input of contaminant b c leach in eq 3 must be calculated or estimated separately for example using a leaching model farlin et al 2018 before using eq 2 for prediction furthermore it must be noted that in this case it is implicitly assumed that the degradation rate λ calibrated for a is approximately applicable to b and that retardation due to sorption effects is comparable for both compounds see appendix 2 6 case study 6 1 hydrogeological setting six of the springs providing drinking water to the city of luxembourg further referred to as springs 1 to 6 have been found to be contaminated by n n dimethylsulfamid ndms a transformation product of the fungicide tolylfluanid farlin et al 2013a tolylfluanid was widely used in europe for decades to protect fruit trees from specific fungi until its approval was cancelled in most countries of the european union in 2008 the springs drain the unconfined fractured rock aquifer known as the luxembourg sandstone locally the luxembourg sandstone formation is up to 70 m thick with a saturated zone of about 10 m the formation is doubly porous with estimated fracture and matrix porosities of between 1 5 and 5 22 respectively farlin et al 2013b tracer mean transit times had been previously estimated for all the springs draining the plateau using tritium and deuterium and vary between 10 and 16 years for the saturated zone farlin et al 2017 farlin et al 2013a thus recovery times from pesticide contamination are relatively long especially since back diffusion processes from the porous matrix into the fracture network may further slow down the observed concentration decreases farlin et al 2018 transit times through the unsaturated zone are probably much shorter according to a study made on behalf of the luxembourgish water agency farlin 2012 the transit times of water through the unsaturated zone of the luxembourg sandstone is at most 5 years with a median of 2 years the estimated recharge area is 1500 m long by up to 500 m wide for a surface area of 63 ha from the springs to the groundwatershed one finds first woodland 20 ha cropland 15 ha and the orchard 16 ha when the study began in 2018 ndms concentration in all six springs had largely exceeded the water quality standard of 100 ng l 644 ng l to 3320 ng l in august 2018 depending on the spring and was still increasing traces from the maize herbicide atrazine and its transformation product desethylatrazine dea were still present in all springs displaying a decreasing trend following a national atrazine ban in 2005 the water provider wished to know i when peak concentrations would be reached ii what the maximum concentrations would be and iii when ndms concentrations would return to the water quality standard of 100 ng l depending on these elements the water provider would consider planning the construction of a treatment plant in order to remove ndms by ultrafiltration 6 2 calibration of the transit time distribution two environmental tracers tritium and deuterium one pesticide atrazine and two pesticide transformation products dea and ndms were available to calibrate tem tpf and α λ for the orchard our example corresponds to case 3 above where the information of the contaminant of interest is insufficient to calibrate on its own the transit time distribution thus tem values used for predicting ndms evolution were those obtained from atrazine and dea fitting tritium measurements were used as additional independent tracer to verify the plausibility of the estimated tem values the tritium input function was obtained from the closest long term measuring station of the global network of isotopes in precipitation gnip https www naweb iaea org napc ih ihs resources gnip html located in trier germany completed with the vienna station using linear regression for the period 1962 1978 the tritium input function was further weighted by a recharge factor grabczak et al 1984 estimated from the difference between the annual deuterium average in spring water and in precipitation measured at the station trier stumpp et al 2014 tritium and deuterium were assumed to be applied homogeneously over the entire groundwatershed the cropland situated on the plateau between the orchards and the forested areas were the sole source area for atrazine and dea the orchards fungicides are mostly specific to fruit tree plantations for atrazine and dea steady state leaching was assumed from 1970 to 2007 the initial date of 1970 is arbitrary and was simply chosen to make sure that the plateau concentrations in spring water were reached in the simulations by 2007 when the first measurements of both compounds were made atrazine and dea leaching from the soil was assumed to have ceased in 2007 country wide ban of atrazine in 2005 plus two years of residual leaching as estimated by farlin et al 2013c assuming that in the unsaturated zone both water and tracer fluxes take place mostly through the fracture network with limited exchange with the matrix and that consequently tracer retardation relatively to water will be negligible the upper value for tpf was set to 5 years which is the maximum transit time of recharge water through the unsaturated zone estimated for the luxembourg sandstone the dilution and degradation parameters α and λ calculated for atrazine and dea are irrelevant for the orchard risk assessment since the source areas of atrazine or dea and that of ndms are different hence α λ was simply adjusted to fit an arbitrary steady state leaching of 1000 ng l just as for the dilution and degradation parameters tpf values obtained using tritium atrazine or dea are meaningless for the risk assessment it must be noted however that in the case of inverse modelling with tritium constraining tpf also constrains indirectly tem which is the only parameter estimated from atrazine dea and tritium further used for forward modelling using the tem value estimated from dea for each spring tpf and the dilution and degradation factor α λ were fitted to the ndms time series tem value obtained from the environmental isotopes were used as plausibility check one additional difficulty for ndms was the unknown year of first application of tolylfluanid in the orchards the last treatment was assumed to have taken place in 2007 the year before the tolylfluanid authorisation was revoked for increasing trends tpf depends directly on the time of first leaching and to a lesser extent on cmax as well in such a case the latest possible onset of ndms leaching can be found by trial and error we determined a minimum tpf corresponding to a leaching starting in 1995 shorter tpf predicted a trend reversal before 2018 which is not observed in spring water longer tpf values for orchard contaminants cannot be ruled out using only ndms data the best fit curves for all tracers are shown exemplarily for spring 1 on fig 3 the different tem estimates for a given spring from atrazine dea or tritium measurements are mostly within 2 years of each other tem is also similar from spring to spring the broadest range being obtained using tritium measurements and spanning values from 9 to 16 years table 1 summarizes the estimated parameters for each spring 6 3 predicting the evolution of orchard contaminants in spring water after estimating tem tpf and α λ for the orchard source area eq 2 was used to predict the evolution of ndms and calculate the waiting times and the probable maximum concentrations treac and trecovery as well as cmax estimated for ndms are summarized in table 1 the time to trend reversal treac and the total recovery time trecovery of ndms depend on the piston flow component tpf whose estimate depends itself on the length of the application period of ndms since the tpf values are minimum estimates a later date for treac and trecovery is possible the earliest date for treac is 2020 while the return to the water quality standard of 100 ng l trecovery depends on the spring and on cmax and should take place between 2050 and 2075 with maximum concentrations ranging from 1000 ng l to 4000 ng l table 1 the evolution of ndms concentrations for the earliest trend reversal in 2020 corresponding to the minimum tpf is shown exemplarily for spring 1 on fig 4 7 discussion the large time scales associated with aquifer contamination and remediation often pose a methodological challenge to water managers as groundwater waiting times are usually measured in years or even decades if not properly taken into account in long term planning this temporal component may on the one hand hide negative trends in groundwater until it is too late to prevent a serious and long lasting degradation of water quality parameters by a timely reduction of the agricultural input and on the other hand delay the positive effects of mitigation measures in this article we differentiate between three different characteristic times namely treac tpurif and trecovery far from being an academic differentiation it is the authors experience that both scientists and stakeholders such as water providers or farming advisors tend to either confuse waiting times with the mean transit time or use them interchangeably although they are relevant for different problems while the mean transit time is the only parameter gained directly from fitting a lumped parameter model it is treac tpurif and trecovery that water managers need the mean transit time tem will not give any information on the time to trend reversal for instance nor will it yield directly the recovery time trecovery which is so important for water managers facing the choice of either waiting for natural attenuation or planning for a technical solution even though the mean transit times and the reaction times are about the same in the study we present respectively 11 to 16 years and 12 to 15 years they need not be and the mean transit time which mostly depends upon tem could be very different from the time to trend reversal which depends upon tpf tpf for instance could be negligible if the contaminant source is close to the outlet and the unsaturated zone is shallow while tem could reach values of decades and more in the case of a thick saturated zone or low recharge rates or both combined scientists and engineers should be aware that only communicating the estimated mean transit time will not be sufficient for practitioners and that they will also need to calculate treac tpurif and trecovery or explain clearly the difference to water managers in any case assuming the mean transit time to be characteristic of contaminant waiting times is liable to lead to confusion and to erroneous management decisions the epm model chosen here is the most parcimonious possible if both a piston flow component and a range of contaminant transit times are to be simulated i e the contaminant input cannot be considered as a single point source other transit time distribution models could be used instead the dispersion or the gamma model could be chosen to include dispersion effects at the cost of one additional parameter whereas the partial exponential model baillieux et al 2015 jurgens et al 2016 could be an even more parcimonious choice if its parameters can be estimated independently from known screen depth this later model could also be used to simulate the superposition of spatially distinct contributing surfaces as shown by baillieux et al 2015 the case study we presented was in that regard a simple example with a homogeneous leaching over a single conterminous area within the groundwatershed given that even the epm has already four fitting parameters adding more free parameters should be weighted carefully depending on the data available estimating four parameters necessitates a time series including enough information about tem increase or decrease over time tpf time shift between input change and output response and α λ magnitude of dilution and degradation of a contaminant originating from the same subcatchment whenever this is not the case for instance because the leaching period cannot be constrained sufficiently or if the plateau concentration was not monitored environmental tracers or contaminants from other source areas can still in some cases lead to an approximate estimation furthermore including different tracers in the risk analysis allows to verify the plausibility of the estimated parameters and assumed leaching periods the main difference between environmental tracers and agrochemicals is that the formers are in good approximation applied uniformly over the entire recharge area at least for smaller groundwatersheds without significant altitude gradients whereas the agrochemical are only injected in the subsurface i e leached from treated surfaces within the recharge area in a way that may vary spatially baillieux et al 2015 eberts et al 2012 thus strictly speaking the transfer function of the saturated zone and hence the values of tem will be different for the entire groundwatershed and for arbitrary subcatchments within the recharge area in practice however if the surface of the area contributing agrochemicals is large relatively to the total recharge area sufficiently many flow lines captured at the outlet will originate from the subcatchment so that tem values for the subcatchment and for the entire groundwatershed may not differ considerably from one another this assumption can be verified by comparing the tem values obtained from water isotopes and from the decreasing trend of a contaminant as we showed in the case study this means on the one hand that environmental tracers could be used on their own whenever the aquifer reaction times must be estimated without any contaminant data to work with and on the other hand that with proper acknowledgement of the assumption made tem values estimated for a given source area could potentially be used for another source area within the groundwatershed in the absence of data specific to the latter 8 conclusion waiting time estimates can greatly help managers to make informed planning decisions such as building a treatment plant or a new water main or drilling new pumping wells before drinking water supply problems become acute as we have tried to show the waiting times estimates needed by water managers usually differ from the mean transit time of a tracer given the long delays involved in planning and constructing new drinking water infrastructures constraining sufficiently the estimates of the recovery time in particular will often not be problematic as good quality contaminant time series possibly combined with environmental isotopes can usually differentiate between recovery times of a few years and many decades the presented case study is a first indication that a simple lumped parameter model calibrated using agrochemical time series and environmental tracers might be sufficient for the task at least in simple hydrogeological setting and if the agrochemical source areas occupy a large portion of the entire groundwatershed the robustness of estimated waiting times should be tested further in other case studies credit authorship contribution statement j farlin conceptualization methodology investigation t gallé conceptualization investigation m bayerle investigation d pittois investigation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the case study on the evolution of a fungicide transformation product in the luxembourg sandstone aquifer was funded by the city of luxembourg insightful discussions on transit time estimates with participants of the european union funded cost action watson ca19120 is also gratefully acknowledged appendix a appendix 1 applicability and limits of the exponential model and of the steady state assumption the transit time distribution corresponding to contaminant transport through the saturated zone is simulated by an exponential model this model is the exact solution for semi confined aquifers of any shape as long as the equivalent water column in storage divided by the recharge rate is spatially constant but is also applicable to unconfined aquifers when the average saturated thickness is much larger than its spatial variations haitjema 1995 it must be noted however that this model assumes that tracer exchange between flow lines due to diffusion or mechanical dispersion is negligible eriksson 1958 eq 2 assumes a steady state flow field i e a constant hydraulic gradient and thus a constant water flux over time although aquifer storage will usually change seasonally with a maximum following winter recharge and a minimum following reduced summer infiltration however as long as the temporal variations in total head are small compared to the thickness of the aquifer the transit time distribution will only vary slightly for groundwater systems and the steady state assumption leads to negligible errors zuber et al 1986 agrochemical leaching will also follow a seasonal pattern but the resulting seasonal cycles will often be small compared to long time trends or will clearly appear superimposed upon them farlin et al 2018 appendix 2 soil and aquifer reaction time in the case of sorptive compounds the piston flow time lag is the sum of the transit time through the soil tsoil through the unsaturated zone tunsat and the time needed to cross the horizontal distance between the agricultural areas from which the contaminant is leached and the sampling point tdistance a thick unsaturated zone will lead to a significant shift between soil leaching and contamination of the aquifer schwientek et al 2009 even if the agricultural areas are close to the sampling point also the first arrival time will increase with increasing distance between the source area and the outlet we assume all three lag times to be piston flow i e the contaminant pulse is simply shifted in time thus the total lag time tpf is also piston flow and equal to the sum of the three terms 7 t pf t soil t unsat t d i s tan c e sorption effects will cause additional retardation for some agrochemicals compared to an ideal tracer 8 t soil c o n t a min a n t t soil t r a c e r r soil 9 t unsat c o n t a min a n t t unsat t r a c e r r unsat r can be estimated assuming a reversible linear sorption isotherm 10 c s k d c with cs contaminant concentration in the porous matrix mg kg c contaminant concentration in the water phase mg l and kd distribution coefficient l kg compound specific r is then małoszewski and zuber 1984 11 r 1 1 n n k d with r retardation factor and n porosity of the soil or the unsaturated zone tpf will often be estimated using a compound that undergoes sorption rather than from an ideal tracer if predictions are desired for another compound with a different sorption coefficient an adjustment must be made to account for this difference let b be the compound of interest and a the compound used for parameterization if k d b k d a the value of tpf b can be adjusted as follows 12 t pf b r a r b t pf a 1 1 n n k d a 1 1 n n k d b t pf a with kd1 distribution coefficient of the contaminant used for parameterization l kg and kd2 distribution coefficient of the contaminant of interest l kg fig 5 summarizes model parameters of the example shown on fig 2 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128306 appendix b supplementary data the following are the supplementary data to this article supplementary data 1 
2888,in groundwater protection zones water managers and water providers will typically need to know the time scale of recovery for aquifers contaminated by agrochemicals despite the large body of work on the topic in the scientific literature there still seems to be some need to clarify which waiting times are most relevant for water management planning and how they relate to metrics such as the mean transit time of tracer firstly we propose a simple nomenclature for the different waiting times and how they relate to the evolution of agrochemical concentration increasing and decreasing concentration trends trend reversal etc secondly we describe how to select and fit a lumped parameter model to contaminant time series and environmental isotopes in order to characterize aquifer response to agrochemical contamination thirdly we explain how waiting times can be calculated from the fitted lumped parameter model finally we present a case study focussing on the contamination of several springs of the city of luxembourg by a fungicide transformation product combining environmental tracers and pesticide measurements we find that on the study site the parameters estimated separately from the environmental tracers and from the pesticide data are nearly the same although they correspond to different areas within the groundwatershed although this could be incidental it might also indicate that robust waiting time estimates can be obtained from agrochemical time series and environmental tracer measurements at least in simple cases keywords lumped parameter models water protection zone contaminant transit time mean transit time waiting time trend reversal data availability the water quality data was uploaded as supplemental material 1 introduction waiting time estimates can play an important role in the management of diffuse groundwater contamination by agrochemicals typical situations faced by water providers or environmental agencies and involving waiting times are for instance mitigation measures implemented to reverse the upward concentration trend of a contaminant have not been followed by any observable improvement are the measures insufficient or is the aquifer acting as buffer a herbicide has been quantified in an aquifer at concentrations exceeding the drinking water quality standard thus forcing the local water provider to suspend the distribution of the contaminated groundwater subsequently all use of this herbicide has been forbidden within the water protection zone how long will the water provider have to wait until the concentrations have decreased sufficiently for the groundwater to be used again for drinking water purposes lumped parameter models originally developed to estimate aquifer parameters concerning aquifer storage and flow velocities from environmental tracer measurements eriksson 1958 małoszewski and zuber 1982 can easily be adapted to estimate groundwater waiting times and have the advantage of simplicity compared to a coupled flow and transport numerical model once parameterized a lumped parameter model can be used to quantify aquifer vulnerability to a contaminant input einsiedl et al 2009 to predict the evolution of diffuse pollution by agrochemicals małoszewski 2000 morgenstern et al 2015 visser et al 2007 and to estimate waiting times relevant for water managers thus bridging the gap between science and practice although a large body of literature has accumulated on the relationship between aquifer transit times and aquifer response to agrochemical contamination fovet et al 2015 molenat and gascuel odoux 2002 morgenstern et al 2015 owens et al 2008 schwientek et al 2009 tomer and burkart 2003 visser et al 2007 wassenaar et al 2006 there is still room for a method article written from the perspective of water managers needs in this article we wish to i clarify which waiting times are directly useful for water managers and how they differ from the classical groundwater transit time metrics such as the mean transit time of tracer ii show how these waiting times can be calculated from a parameterized lumped parameter model and iii demonstrate how existing water quality data can be used in combination with environmental tracer measurements for parameterization for the sake of illustration a case study focussing on the contamination of a luxembourgish aquifer by pesticide degradation products leached from commercial orchards is presented 2 groundwater dating basics we start first by reviewing briefly a few basic elements of groundwater dating a tracer is a measurable compound injected in the subsurface and from which useful aquifer parameters can be gained concerning solute storage and transit times an ideal tracer behaves strictly like a water molecule an environmental tracer is a tracer applied naturally by rainfall environmental tracers are injected over the entire surface of groundwatersheds albeit in a fashion that may vary spatially depending for instance on altitude or catchment size the travel time of a tracer or a contaminant from the point it is released from the soil to an outlet such as a pumping well or a spring will depend on the distance between the two points all else equal recharge rate porosity aquifer thickness the sampling point at any given time will usually intersect more than one flow line and thus the total flux at this point will cover a large range of individual transit times from the contributing surfaces the frequency distribution of the entire groundwatershed s transit times normalised to an integral of 1 will further be called transit time distribution and defines how an aquifer will release a tracer injected instantaneously and homogeneously over the entire recharge area in complex hydrogeological situations with multiple outlets both natural and man made such as pumping wells the transit time distribution of a single outlet will be different from the transit time distribution of the entire groundwatershed etcheverry 2001 in this paper only simple cases will be presented for which outlet and groundwatershed transit time distributions can be considered to be the same for the sake of mathematical analysis the transit time distribution is described by a model different model shapes have been described in the literature depending on the hydrogeological setting on the sampling situation and on transport processes amin and campana 1996 etcheverry 2001 małoszewski and zuber 1982 once a model is chosen its model parameters must be estimated or fitted from tracer measurements models have a different number of fitting parameters the main one being the mean transit time which is the weighted average of all flow lines individual transit time as we will show waiting times directly relatable to water management questions can be calculated easily after the model parameters of the transit time distribution have been estimated 3 waiting times of agrochemical contamination in groundwater protection zones in order to evaluate the efficiency of mitigation measures or to plan for technical remediation measures water managers need to know wassenaar et al 2006 i when concentrations at a sampling point should be expected to begin their decrease after the input has been reduced ii how long the decrease will last until a threshold concentration is reached either a water quality standard or another operational concentration and iii what the maximum contaminant concentration will be before trend reversal is reached point i is particularly important during the evaluation of mitigation measures point ii provides a useful basis when for instance deciding whether a treatment plant should be installed or if waiting for natural attenuation is a realistic alternative a typical application of point iii would be to answer whether maximum concentrations will or will not exceed the water quality standard in the case of an increasing trend fig 1 presents the idealised evolution of a contamination by agrochemicals measured in a drinking water uptake such as a well or a spring in response to a continued agrochemical leaching after leaching has begun there is a time lag before the contaminant first arrives at the uptake then a continuous increase up to a plateau concentration once the leaching ceases as a result of mitigation measures for instance another time lag is observed before concentrations start decreasing followed by a continuous decrease over time if leaching is stopped or reduced before a plateau is reached the upward trend will be followed directly by a downward trend and the point of inflexion will be called trend reversal the time lag between the enforcement of mitigation measures and the beginning of a decrease in concentration at the sampling point will be further referred to as reaction time treac the time lag between the beginning of a concentration decrease and the return of concentrations to a threshold value as self purification time tpurif and the total time between the implementation of mitigation measures and the achievement of a given water quality goal as recovery time trecovery these three waiting times are what water managers are directly interested in 4 model choice and relationship between model parameters and waiting times in the case of an agrochemical released from farmed areas four elements need to be taken into account to model the evolution of water quality at a drinking water outtake the time lag between the application of an agrochemical and its first arrival at the sampling point the range of travel times of an agrochemical at the outlet depending on the distance from each leaching point to the outlet dilution by recharge water from surfaces where the agrochemical is not applied degradation of the agrochemical below the root zone different models could be used to simulate these elements we choose here the combined exponential piston flow model epm the piston flow component simulates the first arrival time lag while the exponential component simulates the range of transport times from the contributing areas the epm is defined as follows modified from małoszewski and zuber 1982 1 g τ 0 if τ t pf g τ 1 t em exp τ t em t em t pf t em 1 if τ t pf with tem being the mean transit time of the exponential part t and tpf the piston flow transit time t if a spatially homogeneous input as well as steady state can be assumed tracer input injection in the unsaturated zone and output exit at the sampling point are related by the epm as follows modified after małoszewski and zuber 1982 2 c out t 0 c in t τ e x p λ τ g τ d τ with c out t being the tracer concentration at the system s outlet as a function of time m l3 c in t the tracer input as a function of time m l3 g the transfer function τ an integration constant equal to the total transit time of each flow line t and λ the decay or degradation constant respectively for radionuclides or bio degradable agrochemicals t 1 a dilution factor α must be integrated in the input time series c in of eq 2 to take into account surfaces not contributing any agrochemical α is assumed to be invariant over time because it depends primarily upon the ratio of the area of the contributing surface relative to the total surface area of the groundwatershed the smaller the surfaces contributing contaminant relatively to the size of the groundwatershed and the larger the dilution will be 3 c in t c leach t α with c leach being the soil leaching concentration of the agrochemical m l3 and α the dilution factor the experimenter must decide how to best approximate c leach depending on available data and study aims a constant annual leaching will suffice to predict the progressive concentration increase resulting from repeated annual applications of an agrochemichal over a period of many years whereas a step function is a simple way to simulate drastic mitigation measures leading to a significant reduction of the input farlin et al 2013b a numerical codes can also be used to calculate more complex leaching patterns taking into account application times and doses as well as changing environmental conditions and rainfall farlin et al 2018 in the case of environmental tracers tracer injection in the subsurface can be considered homogeneous over the entire recharge area as long as the groundwatershed is not too large and in the absence of an important altitude gradient degradation rates are usually smaller in the unsaturated and saturated zones than in the soil zone especially the topsoil where most of the microbial activity is concentrated chilton et al 2005 moreau and mouvet 1997 morvan et al 2006 pothuluri et al 1990 this is for instance the case where low doc content in the groundwater is insufficient to sustain substantial microbial activity even so degradation losses below the root zone can still be significant because transport times are usually much longer in the subsurface than in the soil for example it was calculated by farlin et al 2013c 2017 that up to 80 of the leached mass of atrazine and its transformation product desethylatrazine can be degraded during transport through the luxembourg sandstone first order degradation can be simulated using eq 2 by setting a positive degradation rate λ but at the cost of having to estimate its value from data all three waiting times relevant for water management can be obtained from eqs 1 3 and are given in eqs 4 6 the reaction time treac for a given contaminant within a groundwatershed depends on the distance between the treated surfaces and the outlet and on the lag time imparted by the soil column and by the unsaturated zone and is simply the piston flow component tpf of the epm 4 t reac t pf fig 2 presents an example of a configuration for which a lag time due to the distance between input and outlet should be expected this distance related shift in tracer response corresponds to the example presented in małoszewski and zuber 1982 fig 1 situation 3 for which the infinitesimally short flow lines do not contribute tracer to the outlet in małoszewski and zuber because of a impermeable overburden the self purification time tpurif will depend on the water quality goal to be reached and on the inertia of the aquifer which itself depends on the mean transit time of solute through the saturated zone here tem tpurif can be found using eq 2 by setting the water quality goal as c out and finding the corresponding t for the exponential model 86 of the total aquifer pore space mobile and immobile zones will have been flushed by recharge water after a time period twice the mean transit time and 95 after three mean transit times thus one can consider as a rule of thumb that three times the mean transit times of the saturated zone will be necessary to approach a new steady state concentration after a change in the input either a reduction or an increase eq 5 5 t purif 3 t em finally the total recovery time trecovery is the sum of the reaction time and the self purification time eq 6 6 t re cov e r y t reac t purif depending on the particulars waiting times can be given as a time span or in calendar years i e treac for instance can be either 10 years or 2030 for a starting date in 2020 5 calibration strategy in order to estimate treac tpurif and trecovery the two free parameters of the epm tem and tpf the dilution parameter α and the degradation parameter λ must be fitted to the known or reconstructed input and output time series by inverse modelling in many cases however the available data will be insufficient to calibrate both dilution and degradation parameters separately so that both must be lumped together to estimate the overall effect of dilution and degradation on output concentration yielding three fitting parameters tem tpf and α λ it is important to note that aquifer degradation will change the magnitude of agrochemical concentration in the output but not the shape of the output response farlin et al 2013b fig 3b thus when only the temporal evolution of an agrochemical concentration is needed and not its absolute concentration neglecting degradation will greatly simplify the calibration procedure when estimates of concentrations at the outlet are necessary neglecting degradation would yield conservative estimates corresponding to a worst case scenario once all parameters have been estimated the evolution of the concentration of other agrochemicals at the sampling point can be calculated by forward modelling małoszewski 2000 information gained from forward modelling is obviously what water managers will be interested in three different cases must be distinguished when calibrating the transit time distribution by inverse modelling depending on the available data case 1 the time series of the contaminant for which prediction is needed is sufficient to estimate all three fitting parameters case 2 another contaminant originating from the same source area can be used for parameterization case 3 only environmental tracer data and or a time series of contaminants originating from other source areas within the groundwatershed are available for inverse modelling case 1 applies when all parameters are estimated directly for the compound of interest but will be of limited practical interest since both maximum concentration and reaction time will be already known leaving only the self purification time to be predicted cases 2 and 3 are purely inverse modelling procedures designed to obtain the parameters of the transit time distribution if the contaminant data set is insufficient for that purpose in many cases such as for an incomplete increasing trend fitting three parameters to observations will make the inverse modelling problem non unique whenever this problem appears the following fitting scheme can be adopted 1 estimation of tem from an appropriate environmental tracer or a contaminant displaying a decreasing trend 2 use of the estimated tem in eq 2 and fitting of tpf and α λ to the data of the contaminant displaying the increasing trend waiting times can be estimated before the contaminant has even begun to appear at the sampling point this is only possible if another contaminant originating from the same sub catchment can be used for calibration let a be the contaminant already measured at the sampling point and b the contaminant not yet observable there in this case the calibration scheme is as follows 1 estimation of tem using either a if possible or environmental tracers or a contaminant displaying a decreasing trend 2 estimation of tpf and α λ using contaminant a 3 use of tem tpf and α λ in eq 2 yielding a predictable evolution of the concentrations of b as well as an estimate of c max it is important to note that the input of contaminant b c leach in eq 3 must be calculated or estimated separately for example using a leaching model farlin et al 2018 before using eq 2 for prediction furthermore it must be noted that in this case it is implicitly assumed that the degradation rate λ calibrated for a is approximately applicable to b and that retardation due to sorption effects is comparable for both compounds see appendix 2 6 case study 6 1 hydrogeological setting six of the springs providing drinking water to the city of luxembourg further referred to as springs 1 to 6 have been found to be contaminated by n n dimethylsulfamid ndms a transformation product of the fungicide tolylfluanid farlin et al 2013a tolylfluanid was widely used in europe for decades to protect fruit trees from specific fungi until its approval was cancelled in most countries of the european union in 2008 the springs drain the unconfined fractured rock aquifer known as the luxembourg sandstone locally the luxembourg sandstone formation is up to 70 m thick with a saturated zone of about 10 m the formation is doubly porous with estimated fracture and matrix porosities of between 1 5 and 5 22 respectively farlin et al 2013b tracer mean transit times had been previously estimated for all the springs draining the plateau using tritium and deuterium and vary between 10 and 16 years for the saturated zone farlin et al 2017 farlin et al 2013a thus recovery times from pesticide contamination are relatively long especially since back diffusion processes from the porous matrix into the fracture network may further slow down the observed concentration decreases farlin et al 2018 transit times through the unsaturated zone are probably much shorter according to a study made on behalf of the luxembourgish water agency farlin 2012 the transit times of water through the unsaturated zone of the luxembourg sandstone is at most 5 years with a median of 2 years the estimated recharge area is 1500 m long by up to 500 m wide for a surface area of 63 ha from the springs to the groundwatershed one finds first woodland 20 ha cropland 15 ha and the orchard 16 ha when the study began in 2018 ndms concentration in all six springs had largely exceeded the water quality standard of 100 ng l 644 ng l to 3320 ng l in august 2018 depending on the spring and was still increasing traces from the maize herbicide atrazine and its transformation product desethylatrazine dea were still present in all springs displaying a decreasing trend following a national atrazine ban in 2005 the water provider wished to know i when peak concentrations would be reached ii what the maximum concentrations would be and iii when ndms concentrations would return to the water quality standard of 100 ng l depending on these elements the water provider would consider planning the construction of a treatment plant in order to remove ndms by ultrafiltration 6 2 calibration of the transit time distribution two environmental tracers tritium and deuterium one pesticide atrazine and two pesticide transformation products dea and ndms were available to calibrate tem tpf and α λ for the orchard our example corresponds to case 3 above where the information of the contaminant of interest is insufficient to calibrate on its own the transit time distribution thus tem values used for predicting ndms evolution were those obtained from atrazine and dea fitting tritium measurements were used as additional independent tracer to verify the plausibility of the estimated tem values the tritium input function was obtained from the closest long term measuring station of the global network of isotopes in precipitation gnip https www naweb iaea org napc ih ihs resources gnip html located in trier germany completed with the vienna station using linear regression for the period 1962 1978 the tritium input function was further weighted by a recharge factor grabczak et al 1984 estimated from the difference between the annual deuterium average in spring water and in precipitation measured at the station trier stumpp et al 2014 tritium and deuterium were assumed to be applied homogeneously over the entire groundwatershed the cropland situated on the plateau between the orchards and the forested areas were the sole source area for atrazine and dea the orchards fungicides are mostly specific to fruit tree plantations for atrazine and dea steady state leaching was assumed from 1970 to 2007 the initial date of 1970 is arbitrary and was simply chosen to make sure that the plateau concentrations in spring water were reached in the simulations by 2007 when the first measurements of both compounds were made atrazine and dea leaching from the soil was assumed to have ceased in 2007 country wide ban of atrazine in 2005 plus two years of residual leaching as estimated by farlin et al 2013c assuming that in the unsaturated zone both water and tracer fluxes take place mostly through the fracture network with limited exchange with the matrix and that consequently tracer retardation relatively to water will be negligible the upper value for tpf was set to 5 years which is the maximum transit time of recharge water through the unsaturated zone estimated for the luxembourg sandstone the dilution and degradation parameters α and λ calculated for atrazine and dea are irrelevant for the orchard risk assessment since the source areas of atrazine or dea and that of ndms are different hence α λ was simply adjusted to fit an arbitrary steady state leaching of 1000 ng l just as for the dilution and degradation parameters tpf values obtained using tritium atrazine or dea are meaningless for the risk assessment it must be noted however that in the case of inverse modelling with tritium constraining tpf also constrains indirectly tem which is the only parameter estimated from atrazine dea and tritium further used for forward modelling using the tem value estimated from dea for each spring tpf and the dilution and degradation factor α λ were fitted to the ndms time series tem value obtained from the environmental isotopes were used as plausibility check one additional difficulty for ndms was the unknown year of first application of tolylfluanid in the orchards the last treatment was assumed to have taken place in 2007 the year before the tolylfluanid authorisation was revoked for increasing trends tpf depends directly on the time of first leaching and to a lesser extent on cmax as well in such a case the latest possible onset of ndms leaching can be found by trial and error we determined a minimum tpf corresponding to a leaching starting in 1995 shorter tpf predicted a trend reversal before 2018 which is not observed in spring water longer tpf values for orchard contaminants cannot be ruled out using only ndms data the best fit curves for all tracers are shown exemplarily for spring 1 on fig 3 the different tem estimates for a given spring from atrazine dea or tritium measurements are mostly within 2 years of each other tem is also similar from spring to spring the broadest range being obtained using tritium measurements and spanning values from 9 to 16 years table 1 summarizes the estimated parameters for each spring 6 3 predicting the evolution of orchard contaminants in spring water after estimating tem tpf and α λ for the orchard source area eq 2 was used to predict the evolution of ndms and calculate the waiting times and the probable maximum concentrations treac and trecovery as well as cmax estimated for ndms are summarized in table 1 the time to trend reversal treac and the total recovery time trecovery of ndms depend on the piston flow component tpf whose estimate depends itself on the length of the application period of ndms since the tpf values are minimum estimates a later date for treac and trecovery is possible the earliest date for treac is 2020 while the return to the water quality standard of 100 ng l trecovery depends on the spring and on cmax and should take place between 2050 and 2075 with maximum concentrations ranging from 1000 ng l to 4000 ng l table 1 the evolution of ndms concentrations for the earliest trend reversal in 2020 corresponding to the minimum tpf is shown exemplarily for spring 1 on fig 4 7 discussion the large time scales associated with aquifer contamination and remediation often pose a methodological challenge to water managers as groundwater waiting times are usually measured in years or even decades if not properly taken into account in long term planning this temporal component may on the one hand hide negative trends in groundwater until it is too late to prevent a serious and long lasting degradation of water quality parameters by a timely reduction of the agricultural input and on the other hand delay the positive effects of mitigation measures in this article we differentiate between three different characteristic times namely treac tpurif and trecovery far from being an academic differentiation it is the authors experience that both scientists and stakeholders such as water providers or farming advisors tend to either confuse waiting times with the mean transit time or use them interchangeably although they are relevant for different problems while the mean transit time is the only parameter gained directly from fitting a lumped parameter model it is treac tpurif and trecovery that water managers need the mean transit time tem will not give any information on the time to trend reversal for instance nor will it yield directly the recovery time trecovery which is so important for water managers facing the choice of either waiting for natural attenuation or planning for a technical solution even though the mean transit times and the reaction times are about the same in the study we present respectively 11 to 16 years and 12 to 15 years they need not be and the mean transit time which mostly depends upon tem could be very different from the time to trend reversal which depends upon tpf tpf for instance could be negligible if the contaminant source is close to the outlet and the unsaturated zone is shallow while tem could reach values of decades and more in the case of a thick saturated zone or low recharge rates or both combined scientists and engineers should be aware that only communicating the estimated mean transit time will not be sufficient for practitioners and that they will also need to calculate treac tpurif and trecovery or explain clearly the difference to water managers in any case assuming the mean transit time to be characteristic of contaminant waiting times is liable to lead to confusion and to erroneous management decisions the epm model chosen here is the most parcimonious possible if both a piston flow component and a range of contaminant transit times are to be simulated i e the contaminant input cannot be considered as a single point source other transit time distribution models could be used instead the dispersion or the gamma model could be chosen to include dispersion effects at the cost of one additional parameter whereas the partial exponential model baillieux et al 2015 jurgens et al 2016 could be an even more parcimonious choice if its parameters can be estimated independently from known screen depth this later model could also be used to simulate the superposition of spatially distinct contributing surfaces as shown by baillieux et al 2015 the case study we presented was in that regard a simple example with a homogeneous leaching over a single conterminous area within the groundwatershed given that even the epm has already four fitting parameters adding more free parameters should be weighted carefully depending on the data available estimating four parameters necessitates a time series including enough information about tem increase or decrease over time tpf time shift between input change and output response and α λ magnitude of dilution and degradation of a contaminant originating from the same subcatchment whenever this is not the case for instance because the leaching period cannot be constrained sufficiently or if the plateau concentration was not monitored environmental tracers or contaminants from other source areas can still in some cases lead to an approximate estimation furthermore including different tracers in the risk analysis allows to verify the plausibility of the estimated parameters and assumed leaching periods the main difference between environmental tracers and agrochemicals is that the formers are in good approximation applied uniformly over the entire recharge area at least for smaller groundwatersheds without significant altitude gradients whereas the agrochemical are only injected in the subsurface i e leached from treated surfaces within the recharge area in a way that may vary spatially baillieux et al 2015 eberts et al 2012 thus strictly speaking the transfer function of the saturated zone and hence the values of tem will be different for the entire groundwatershed and for arbitrary subcatchments within the recharge area in practice however if the surface of the area contributing agrochemicals is large relatively to the total recharge area sufficiently many flow lines captured at the outlet will originate from the subcatchment so that tem values for the subcatchment and for the entire groundwatershed may not differ considerably from one another this assumption can be verified by comparing the tem values obtained from water isotopes and from the decreasing trend of a contaminant as we showed in the case study this means on the one hand that environmental tracers could be used on their own whenever the aquifer reaction times must be estimated without any contaminant data to work with and on the other hand that with proper acknowledgement of the assumption made tem values estimated for a given source area could potentially be used for another source area within the groundwatershed in the absence of data specific to the latter 8 conclusion waiting time estimates can greatly help managers to make informed planning decisions such as building a treatment plant or a new water main or drilling new pumping wells before drinking water supply problems become acute as we have tried to show the waiting times estimates needed by water managers usually differ from the mean transit time of a tracer given the long delays involved in planning and constructing new drinking water infrastructures constraining sufficiently the estimates of the recovery time in particular will often not be problematic as good quality contaminant time series possibly combined with environmental isotopes can usually differentiate between recovery times of a few years and many decades the presented case study is a first indication that a simple lumped parameter model calibrated using agrochemical time series and environmental tracers might be sufficient for the task at least in simple hydrogeological setting and if the agrochemical source areas occupy a large portion of the entire groundwatershed the robustness of estimated waiting times should be tested further in other case studies credit authorship contribution statement j farlin conceptualization methodology investigation t gallé conceptualization investigation m bayerle investigation d pittois investigation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the case study on the evolution of a fungicide transformation product in the luxembourg sandstone aquifer was funded by the city of luxembourg insightful discussions on transit time estimates with participants of the european union funded cost action watson ca19120 is also gratefully acknowledged appendix a appendix 1 applicability and limits of the exponential model and of the steady state assumption the transit time distribution corresponding to contaminant transport through the saturated zone is simulated by an exponential model this model is the exact solution for semi confined aquifers of any shape as long as the equivalent water column in storage divided by the recharge rate is spatially constant but is also applicable to unconfined aquifers when the average saturated thickness is much larger than its spatial variations haitjema 1995 it must be noted however that this model assumes that tracer exchange between flow lines due to diffusion or mechanical dispersion is negligible eriksson 1958 eq 2 assumes a steady state flow field i e a constant hydraulic gradient and thus a constant water flux over time although aquifer storage will usually change seasonally with a maximum following winter recharge and a minimum following reduced summer infiltration however as long as the temporal variations in total head are small compared to the thickness of the aquifer the transit time distribution will only vary slightly for groundwater systems and the steady state assumption leads to negligible errors zuber et al 1986 agrochemical leaching will also follow a seasonal pattern but the resulting seasonal cycles will often be small compared to long time trends or will clearly appear superimposed upon them farlin et al 2018 appendix 2 soil and aquifer reaction time in the case of sorptive compounds the piston flow time lag is the sum of the transit time through the soil tsoil through the unsaturated zone tunsat and the time needed to cross the horizontal distance between the agricultural areas from which the contaminant is leached and the sampling point tdistance a thick unsaturated zone will lead to a significant shift between soil leaching and contamination of the aquifer schwientek et al 2009 even if the agricultural areas are close to the sampling point also the first arrival time will increase with increasing distance between the source area and the outlet we assume all three lag times to be piston flow i e the contaminant pulse is simply shifted in time thus the total lag time tpf is also piston flow and equal to the sum of the three terms 7 t pf t soil t unsat t d i s tan c e sorption effects will cause additional retardation for some agrochemicals compared to an ideal tracer 8 t soil c o n t a min a n t t soil t r a c e r r soil 9 t unsat c o n t a min a n t t unsat t r a c e r r unsat r can be estimated assuming a reversible linear sorption isotherm 10 c s k d c with cs contaminant concentration in the porous matrix mg kg c contaminant concentration in the water phase mg l and kd distribution coefficient l kg compound specific r is then małoszewski and zuber 1984 11 r 1 1 n n k d with r retardation factor and n porosity of the soil or the unsaturated zone tpf will often be estimated using a compound that undergoes sorption rather than from an ideal tracer if predictions are desired for another compound with a different sorption coefficient an adjustment must be made to account for this difference let b be the compound of interest and a the compound used for parameterization if k d b k d a the value of tpf b can be adjusted as follows 12 t pf b r a r b t pf a 1 1 n n k d a 1 1 n n k d b t pf a with kd1 distribution coefficient of the contaminant used for parameterization l kg and kd2 distribution coefficient of the contaminant of interest l kg fig 5 summarizes model parameters of the example shown on fig 2 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128306 appendix b supplementary data the following are the supplementary data to this article supplementary data 1 
2889,preparing for and minimizing the negative impacts of flooding requires knowing when floods are likely to occur however exploratory analysis of seasons may yield spurious conclusions and models of flood seasonality can be misleading if they fail to account for the cyclical nature of flood data or the possibility of multiple flood seasons in a year we modeled the arrival times of floods at 4 505 sites along rivers in the united states using statistical models comprised of weighted combinations of circular von mises distributions circular distributions address the periodicity of arrival time data while mixture models provide the flexibility to fit multiple seasons and a validation framework to test their significance nearly half of all sites we modeled optimized with at least a secondary flood season a fact hidden by analyses that neglect or conflate multiple seasons we found spatiotemporal patterns in the modeled dates of elevated flood hazard which point to common flood generating mechanisms partitioning sites by their modeled flood seasonalities yielded 6 7 clusters of sites along united states rivers with similar flood season numbers dates and durations results varied depending on whether floods were defined by annual maxima or peaks over threshold and on the threshold chosen models of river flood seasonality and the spatial patterns they reveal are potentially useful in informing the timing of infrastructure maintenance in risk sharing via financial instruments and in estimating the likelihood of compound events keywords flooding seasonality river flood circular statistics directional statistics statistical modeling mixtures of distributions data availability data are publicly available from the usgs nwis 1 introduction floods affect more people and cause more economic losses than any other natural disaster affecting 2 3 billion people from 1995 to 2015 wahlstrom and guha sapir 2015 and causing over one trillion usd in damages 2013 values between 1980 and 2013 winsemius et al 2016 globally economic damages due to floods are increasing with these increases largely attributable to population growth migration into hazard prone areas and increasing values of assets at risk bouwer 2011 tanoue et al 2016 in some regions however flood frequency is increasing mallakpour and villarini 2015 blöschl et al 2015 and further increases may be expected in the future milly et al 2002 dottori et al 2018 hirabayashi et al 2013 managing this risk requires knowledge not only of the likelihood of a flood of a given magnitude but also when that flood is likely to arrive the timing of high water affects preparation for emergency response planning of major infrastructure maintenance and the joint probability of coincident flood events although river flood seasons rarely have rigorous definitions times of increased flood hazard have been approximated by linking to the seasons of meteorological phenomena such as the spring freshet allison et al 2000 hurricanes magilligan and graber 1996 or rainfall black and werritty 1997 among others this approach neglects non weather hydrological phenomena which contribute to flooding such as antecedent soil moisture ye et al 2017 and is complicated by the fact that in many locations floods result from a variety of mechanisms hirschboeck 1991 this variety in flood generating mechanism causes the observed flood record to be comprised of mixed populations of data each with its own statistical distribution lecce 2000 smith et al 2011 villarini 2016 barth et al 2019 as a result these mixtures of populations violate the assumption that floods and their arrival times are independent identically distributed iid random variables that can be modeled by fitting unimodal probability density functions to observed data an assumption that underlies traditional flood frequency analysis techniques focusing on a primary flood season to the exclusion of secondary and tertiary and higher seasons could also leave communities vulnerable to unexpected flooding modeling river flood seasons under a framework that explicitly addresses multiple populations could allow for better preparation for extreme events and would more closely reflect the physical processes at play in addition to potentially containing multiple populations flood arrival dates are also an example of periodic data the upper and lower limits of the data range are equal this fact may create discontinuities in the probability density function if models developed for linear data are used without first transforming the data or performing other corrections periodic data also known as circular or directional data arise in many scientific fields most commonly when observations are recorded as times angles or directions examples of such data include wind and wave directions wang and gelfand 2014 mastrantonio et al 2016 geological strike angle pewsey et al 2013 animal movement directions eckert et al 2008 mclellan et al 2015 and the time of day or day of week for incidence of crime brunsdon and corcoran 2006 shirota and gelfand 2017 there also exists a rich literature of circular statistics applications to sinusoidal data particularly in medicine e g van oosterom et al 2000 pomfrett et al 1993 the periodicity of flood arrival times can be observed by considering that two floods occurring on december 30th and january 1st have a mean arrival date of july 2nd but a circular mean date of december 31st a much more useful interpretation of the data addressing this periodicity in the more familiar linear space is possible indeed this is the motivation for the concept of the water year but requires knowing a priori when to divide the data lecce 2000 for example modeled the arrival times of flooding by first breaking the year into seasons using a dataset from the state of north carolina but relying on site specific knowledge or visual inspection to determine the best dates to divide years into seasons could become overly subjective or cumbersome for large datasets in contrast circular statistical models treat data as points on a circle so periodicity is handled implicitly and temporal divisions are unnecessary as a result circular distributions are natural fits for repeating data including the dates of floods villarini 2016 black and werritty 1997 circular statistics address the periodicity of floods but cannot on their own address the issue of mixed populations to address both issues more flexible methods are needed such as the projected normal distribution mastrantonio et al 2015 optimized circular kernel density estimates dhakal et al 2015 or mixtures of circular statistical distributions rios gaona and villarini 2018 veatch and villarini 2020 in the present study we model the arrival dates of floods using mixtures of von mises distributions a mixture model of several von mises distributions allows the probability density of any point on the circle to be estimated as the weighted linear sum of multiple component distributions with their own parameterizations providing the flexibility to fit multimodal data thus mixture models of circular distributions provide an intuitive and flexible approach to modeling multimodal periodic data such as flood seasons while many others have worked to characterize the seasonality of river flooding e g archer 1981 mccuen and beighley 2003 beurton and thieken 2009 mediero et al 2015 klaus et al 2016 ryberg et al 2016 including by the use of circular statistical methods e g burn 1997 ouarda et al 2006 parajka et al 2009 chen et al 2010 chen et al 2013 jeneiová et al 2016 villarini 2016 blöschl et al 2017 ye et al 2017 hall and blöschl 2018 this study is to our knowledge the first application of mixture models of circular distributions to a large scale characterization of river flood seasonality by modeling flood seasons rather than relying on descriptive criteria such as circular mean and variance we can select the best fitting estimate of flood hazard and provide a simulation framework for forecasting filling of data gaps and scenario analysis our approach also reveals additional flood seasons that may be hidden in a single description of overall flood seasonality we apply these models to address three main questions 1 during what time s of the year is the likelihood of river flood greatest 2 what distinctions if any exist between regions of similar river flood seasonality both in the dates of peak flood hazard and the duration of elevated hazard and 3 how do these patterns change when considering annual maxima series versus peaks over threshold partial duration series 2 data and methods to reveal patterns in the seasons of river floods in the united states we fit mixture models to four river discharge datasets discharge data are from the united states geological survey usgs national water information system nwis us geological survey 2020 and were processed using the r programming language r core team 2020 in rstudio software rstudio team 2020 river discharge data were investigated for 23 253 sites with the present analysis limited to 4 505 which had sufficiently complete and recent data records to support analysis fig 1 the completeness criteria used in this study required that each gage include at least 90 daily completeness in 25 of its most recent 30 water years and that the last complete year in the record be water year 2013 from 1 october 2012 to 30 september 2013 or later these criteria ensured that results would be reliable and representative of close to present conditions annual maxima were extracted from each daily discharge dataset by water year peaks over threshold pot events were extracted with thresholds set to yield one two and four peaks per water year on average to ensure independence between events in the pot series we followed the guidance of the us water resources council 1981 designating peaks independent if they were separated by at least as many days as five plus the natural logarithm of the streamgage s watershed area in square miles and with the further constraint that the river discharge must fall to less than 75 of the lower of the two peaks before rising again gages without watershed area information available were excluded from the pot analysis these preparations yielded four datasets of river floods annual maxima and peaks over threshold with one two and four peaks per year hereafter referred to as pot1 pot2 and pot4 dates of peak river discharge were used to fit mixture models consisting of linear combinations of von mises distributions each with its own optimized parameter values the von mises distribution is the two dimensional projection of the von mises fisher distribution for data on the unit hypersphere it has two parameters mean and dispersion or equivalently angle and length of characteristic vector making it a kind of circular analogue to the normal distribution pewsey et al 2013 as a result it is the most prominent model used in circular statistics pewsey et al 2007 as detailed in qin et al 2016 a d dimensional unit vector θ describing a point on the unit hypersphere i e θ r d and θ 1 is said to follow the d variate von mises fisher distribution if its probability density function is given by 1 f θ μ κ c d κ exp κ μ t θ c d κ κ d 2 1 2 π d 2 i d 2 1 κ where μ 1 κ 0 d 2 μ t θ is the cosine similarity between θ and μ and id 2 1 κ refers to the modified bessel function of the first kind with order d 2 1 and argument κ for the special case where d 2 the von mises fisher distribution simplifies to the von mises distribution for data on the unit circle 2 f θ μ κ e κ cos θ μ 2 π i 0 κ the mixture model comprised of h unimodal von mises distributions f θ μ h κ h is then defined for h 1 h as 3 f θ α h μ h κ h h 1 h h 1 h α h f θ μ h κ h where αh are the mixture weights which are subject to the constraints 0 αh 1 and h α h 1 eq 3 has no analytical solution so the parameters α h μ h κ h h 1 h are computed via maximum likelihood estimation using the expectation maximization framework banerjee et al 2005 before using each dataset to fit a mixture model we tested it against the null hypothesis of circular uniformity with four tests rayleigh s test of the significance of the length of the sample data mean vector kuiper s one sample test of uniformity on the circle watson s goodness of fit test for the circular uniform distribution and rao s spacing test of uniformity jammalamadaka and sengupta 2001 we rejected the assumption of uniform circularity at sites with sufficient statistical evidence at an overall significance level of 5 when considering all four tests together with the bonferroni correction bonferroni 1936 applied to adjust for multiple comparisons for those sites with sufficient evidence to reject circular uniformity models example shown in fig 2 were fit consisting of one through four von mises distributions using hornik and grün s 2014 movmf package with the best model i e the optimal choice of h identified based on minimization of schwarz s bayesian information criterion bic schwarz 1978 selection based on minimizing bic allows additional component distributions to improve model performance while reducing the chance of over fitting fig 2 while bic gives a relative measure of model performance that is suitable for determining which among the candidate models is the best in a relative sense it does not indicate whether the chosen model performs well in an absolute sense as a more absolute measure of the goodness of model fit we constructed quantile quantile plots for each site comparing the quantiles of the observed data to those generated by the best fitting model fig 2 we generated empirical cumulative distribution functions to convert quantiles of observed data to probabilities then generated 1 000 exceedance probabilities from each best fitting model to create a lookup table of modeled quantiles as the mixture models used here do not have analytical distribution functions to invert for this purpose quantile quantile plots were then generated using the closest matching model quantile from the lookup table for each probability associated with each quantile of observed data with the correlation coefficient between these two datasets as a measure of goodness of model fit figs 2 5 and s2 once the models were fit and selected dates of peak flood hazard local maxima were identified from the fitted circular distributions with the date of greatest probability density designated as the primary flood season the next highest as the secondary season and so on to characterize the length of flood seasons we arbitrarily defined the beginning and end of a flood season as the points on the distribution surrounding a peak where the flood hazard is changing most rapidly i e the local maximum and minimum respectively of the distribution s first derivative fig 2 alternative definitions for a flood season undoubtedly exist but the exact meaning of a flood season length is less important in this case than a simple delineation allowing relative comparisons between seasons and sites to investigate spatial patterns in river flood seasonality we divisively clustered sites by their flood seasons as revealed by the fitted models we characterized the similarity between sites using gower s distance gower 1971 a distance metric for observations containing a mixture of diverse data types in the present study the use of this metric was necessitated by the inclusion of peak flood dates which are circular data and thus not appropriate for euclidean distances gower s distance is defined as the mean of a set of distance metrics each normalized by its maximum range 4 d g x i x j 1 q q 1 q d ijq x i x j 5 d ijq x iq x jq max x iq x jq where q is the maximum number of variables associated with each observation and dijq is the dissimilarity function most appropriate for each variable type x in the present study we characterized flood seasonality at each site using its number of flood seasons and each season s peak date and length in days the dissimilarity function for the number of seasons and the length of each season is the absolute difference while the dissimilarity of the peak flood season date required using the cosine dissimilarity reprojected onto 0 1 normalization by the maximum distance was performed using the maximum observed rather than maximum theoretical distance for each variable across all i j pairs normalization by the number of variables q is necessary for comparing partly observed responses e g comparing dates of peak flood hazard for two streamgages with different numbers of seasons sites and flood events were not restricted to nearby locations and no variable describing gage location e g latitude and longitude was included in the gower s distance computation ensuring that the resulting spatial patterns would be reflective of flood seasonality alone rather than geographic considerations this dissimilarity metric was then the basis of our divisive clustering of sites by partitioning around medoids pam kaufman and rousseeuw 1990 with the best number of clusters chosen by maximizing the average silhouette width kaufman and rousseeuw 1990 by partitioning around medoids representative members selected to denote the center of a cluster rather than means as in k means clustering pam avoids reliance on euclidean distances instead using only the provided dissimilarity matrix to assign cluster membership 3 results and discussion 3 1 spatial patterns across datasets this analysis reveals clear spatial patterns in flood seasonality along rivers in the united states circular mean dates of peak flow point to regional differences in flood timing and the circular variance of these dates to patterns in the dispersion of floods in time fig 3 moreover these metrics illustrate differences between annual maxima and pot series which tend to observe more events throughout the year and thus give a broader picture of potential flood hazard timing however this exploratory analysis though it addresses periodicity via usage of circular methods presents an incomplete and often misleading picture of flood seasonality as discussed below sites for which we had insufficient evidence to reject the null hypothesis of circular uniformity tend to be located in the piedmont plateau and coastal mid atlantic with additional clusters in florida the texas coast and the desert southwest fig 4 indicating that in these areas floods are equally likely or unlikely to occur throughout the year the number of sites without sufficient evidence to reject this hypothesis was the smallest in the pot4 dataset which logically contains the most events and therefore brings the greatest statistical power to bear on this test for sites with sufficient evidence to reject the uniformity assumption the lowest bic models fit observed data well correlation coefficients for quantile quantile plots comparing observed peak flow data from the annual maxima pot1 pot2 and pot4 datasets to the equivalent probability quantiles generated from the best fitting model averaged 0 97 0 06 0 96 0 06 0 98 0 03 and 0 99 0 01 respectively mean standard deviation model fits tended to be strongest in the south southeast and west coast with weaker correlations in the higher latitudes and mountain west fig 5 of the 4 505 sites modeled 37 44 optimized with at least a secondary flood season with the pot4 dataset having the highest percentage of sites with at least two seasons and the pot1 having the lowest fig 6 only 4 7 of models optimized with three or more seasons indicating that while many sites in the united states are subject to multiple flood seasons few experience more than two with so few sites having tertiary and higher seasons those seasons are not discussed further spatial patterns in the modeled dates of highest flood hazard point to regional differences in flood generating mechanisms fig 6 left column west coast gages show a primary flood season in the winter aligned with the usual timing of cold season atmospheric rivers lackmann and gyakum 1999 with the date of greatest flood hazard occurring later in the winter with decreasing latitude from the sierra nevada to the central lowlands floods primarily occur in the late spring along with peak rainfall intensity berghuijs et al 2016 with sites in the mountain west biased earlier toward the season of peak snowmelt floods in the southeastern states and ohio valley are most likely earlier in the spring when consistent spring rains lead to saturated soils berghuijs et al 2016 the upper midwest and new england states experience earlier primary flood seasons than neighboring areas of lower latitude and lesser snow accumulation only florida and select sites in the desert southwest show a primary season in the late summer likely the results of atlantic tropical waves smith et al 2011 villarini and smith 2010 and the southwest monsoon adams and comrie 1997 respectively in addition to the dates of peak flood hazard spatial patterns are also visible in the duration of elevated hazard fig 7 the mountain west upper midwest and new england states have the shortest primary flood seasons followed by the west coast sites the southeastern and ohio valley sites have the longest primary seasons of elevated flood hazard these patterns in season length are also congruent with the relative durations of suspected flood generating mechanisms peak snowmelt upper midwest mountain west and new england being the most temporally constrained followed by atmospheric rivers west coast with soil moisture accumulation punctuated by rainfall events southeast and ohio valley being the most temporally dispersed secondary seasons show a more mixed picture fig 6 right column than primary seasons distinct temporal patterns in the secondary season can be seen in the upper midwest summertime extending into autumn in the southern great plains the existence of these secondary seasons may be known to experienced managers but their statistical significance may be new information and their spatial distributions point to causal mechanisms the secondary season in the upper midwest aligns with the primary season for the rest of the midwest implying that the processes at play there also matter here though to a lesser degree than snowmelt does new england has a secondary flood season in the winter which may point to ice jam floods while the secondary flood seasons for florida and the atlantic coastal plain each match the other s primary season indicating that the same two mechanisms may be in play in both places but with their relative significances reversed the lengths of secondary seasons show little spatial pattern compared to primary seasons though secondary seasons are somewhat shorter overall ranging on average from 74 to 92 of the length of the primary season for sites with at least two seasons depending on the dataset this difference in length is the greatest for the pot4 dataset and the smallest for the annual maxima and pot1 datasets insights may also be gained by comparing the patterns in annual floods with those from partial duration series differences in modeled primary season are subtle especially when contrasted with the clear differences in circular mean flood date shown in fig 3 computing an overall average flood date from a partial duration series allows secondary season events which may rank among the largest events in the record without being the highest in their particular water year to bias the average toward a date that lies between two seasons but does not correspond to a time of elevated flood hazard examples include the apparent winter flood season shown in exploratory analysis of floods in georgia and south carolina fig 3 left column rows b d a time that lies between the late summer and early spring seasons revealed by the fitted circular models shown in fig 6 left column rows b d a similar effect may be observed in iowa and missouri where the two seasons of early summer and mid fall revealed by the fitted models are conflated into a single late summer season when computing an overall average from partial duration datasets in contrast to these distinctions average flood dates computed from annual maxima series generally show good agreement with the primary seasons revealed by fitted models implying that the overall average date of floods on a water yearly basis is often most reflective of the primary flood season and its generation mechanism however even in this series there is some risk of misinterpretation as with the upper midwest and new england sites where the overall average date of flood is an average of the two seasons revealed by statistical modeling while modeling the annual maximum series does capture secondary seasons fig 6 second column row a the revealed patterns in season are less spatially coherent than in the partial duration series and this coherence increases with the number of events included rows b d this implies that the partial duration series captures some secondary season events missed by the annual maximum series and that including more events in the partial duration series improves characterization of secondary flood seasons presumably this benefit would eventually diminish if the event threshold were lowered further to the point that too many non flood events were included in the analysis while we did not seek to optimize the threshold for inclusion of events in this study we show all three partial duration series to illustrate the effect of modifying that threshold a similar slight improvement can be observed in the depiction of the relative duration of the flood season with modeled primary flood season length showing more spatial coherence as more events are included in the partial duration series compared to the annual maximum series fig 7 first column this improved coherence is also reflected by the pot4 dataset yielding the highest average silhouette width of any dataset used for spatial clustering as described in the following section figure s1 supplementary material as a method of characterizing flood season variability modeled season durations represent a major improvement over circular variance as inclusion of many events tends to destroy all spatial patterns in flood season variance fig 3 right column row d 3 2 clustering analysis clustering of streamgages using the dates and durations of seasons extracted from fitted circular models formalizes the apparent spatial patterns described in the previous section clustering via pam and selecting the optimal number of clusters using maximal average silhouette width yields six clusters for the annual maximum pot1 and pot2 datasets and seven clusters for pot4 fig 8 gages with wider higher silhouette scores are more closely related smaller distance to members of their own cluster than they are to their next closest cluster therefore a higher mean silhouette width across all gages represents a measure of improvement in the choice of the number of clusters allowing more clusters tends to increase the mean silhouette width up to a point after which over fitting begins to reduce the mean width by dividing closely related groups figure s1 supplementary material the fact that all datasets optimized with six clusters with the exception of the pot4 dataset which optimized with seven indicates that spatial clustering of sites by flood seasonality is somewhat robust to the choice of dataset while these clusters are not perfectly consistent across datasets some common groupings emerge inland sites in the southeast and the ohio valley tend to cluster with sites on the west coast especially its southern extent sites in these areas tend to have a single flood season in the late winter though their generating mechanisms are presumably distinct only the pot4 dataset optimizing with an additional cluster splits the west coast sites into sub regions sites in the upper midwest tend to form a single coherent group with two flood seasons in the spring and summer despite their superficial similarity in modeled primary season fig 6 and average annual flood season fig 3 the upper midwest and new england sites secondary seasons are different enough to prevent them from clustering together sites along the middle mississippi river and missouri river drainages tend to cluster together despite their differences in primary flood season duration while sites along the south atlantic coastline stand out from the inland southeast for their biseasonality causing them to cluster with the biseasonal gages in the mountain west and southern great plains the florida and desert southwest sites with their dual flood seasons and primary summertime seasons also cluster together the clusters identified here are less spatially contiguous than but still bear a rough similarity to those identified by brunner et al 2020 in their analysis of regional flood events despite the fact that their analysis uses fewer sites and events and their clusters are based on rank order similarity of flood events rather than flood seasonality the fact that some similar clusters still emerge indicates underlying similarities across sites that control both their flood seasons and which floods events impact them most significantly 4 conclusion we fit mixture models of circular von mises distributions to the dates of peak river discharge at 4 505 river gage sites in the united states fitting mixtures of circular statistical models to the arrival dates of floods offers several potentially useful insights for flood risk management we found that nearly half of all river gages investigated have at least two seasons per year when residents and other stakeholders can expect elevated flood hazard while this multimodality of flood seasonality may in some cases be intuited from experience or inferred from exploratory analysis delineating these seasons using fitted models and selecting the best available model using a likelihood based criterion such as bic helps ensure that identified flood seasons are supported by data while reducing the conflation of multiple seasons we found that while a circular average date of annual flood arrival may do a passable job of estimating the peak date of the primary season in many cases secondary and higher seasons will remain hidden circular mean flood dates from partial duration series provide an even less reliable depiction of actual flood seasons while estimates of flood season variance from partial duration series are of almost no value at all in contrast fitted models of partial duration series provide the most spatially coherent patterns of flood season arrival date relative season length and season number this coherence increases as the flood threshold is lowered and more events are included in the series clustering river gage sites into groups based on the number timing and lengths of their flood seasons reveals regions of the country facing similar temporal patterns in their probability of flooding partitioning sites using pam and maximizing silhouette width optimizes these groupings to maximize intra cluster similarity relative to inter cluster differences providing a sounder basis for assessing similarity than relying on visually apparent commonalities some similar regions like the upper midwest can be explained by common flood generating mechanisms other groupings such as the inclusion of ohio valley sites with west coast sites may be more coincidental or point to indirect teleconnections whatever the reasons for their similarities flood risk managers in these areas may find opportunities to share knowledge best practices and lessons from past events similarly to the sharing within the i storm network of storm surge barrier operators lassing et al 2018 the criteria used here to rank flood seasons by importance and to measure their durations are admittedly arbitrary and their interpretation should be limited to relative comparisons across sites rather than ranking flood seasons by their maximum flood hazard we could have chosen an accumulation of overall hazard across the season rather than delineating flood seasons according to their derivatives we might have chosen an alternative approach to marking their starts and ends such as the second derivatives which would capture more of the shoulders of each period of elevated hazard as one example alternate approach cunderlik ouarda and bobée 2004 proposed and several have adopted an approach whereby flood seasons are defined in terms of relatively flood rich and flood poor months by testing the observed rate of flood occurrence against the circular uniform distribution while such process refinements could have some effect on the relative rankings of seasons by importance and the resulting clustering of sites the lack of an agreed upon definition for a season for flooding or other contexts means that any interpretations of flood seasonality in an absolute sense should be made with caution in cases where two flood seasons have nearly equal peak hazards the identification of primary and secondary may also be of little value for managers the most actionable finding here may be that multiple annual flood seasons which may or may not be apparent from visual inspection of hydrographs may now be evaluated formally and characterized by their statistical significance timing and duration the spatial clusters identified by partitioning of river gages around medoids describing their flood seasonalities present an opportunity for further research into flood generating mechanisms geomorphological and climatological similarities among sites and the potential for regionalization of statistical parameters for flood frequency analysis or other purposes as the earth s climate continues to evolve due to anthropogenic climate change research is also needed into the stationarity of flood seasons including changes that vary regionally for managers knowing when floods are most likely to occur may be valuable for design and maintenance of physical infrastructure construction of financial risk transfer instruments such as insurance policies and catastrophe bonds and assessment of the risks presented by compound and coincident events compound floods such as river floods and hurricane surges occurring simultaneously are perhaps the most prominent example of such events but many types of compound extremes exist which may complicate flood risk management zscheischler et al 2020 including wildfires extreme temperatures and even financial crises and hostile acts investigating these possibilities while equipped with knowledge of likely event timing could provide advantages over methods that generally require an independence assumption due to lack of data e g usace 1993 ch 11 credit authorship contribution statement william veatch conceptualization methodology software validation formal analysis data curation writing original draft writing review editing visualization funding acquisition gabriele villarini conceptualization writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements and data william veatch was supported by a grant from the office of the under secretary of defense for research and engineering usd r e national defense education program ndep ba 1 basic research gabriele villarini acknowledges support from the usace institute for water resources datasets for this research are available from the source cited as usgs 2020 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128330 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2889,preparing for and minimizing the negative impacts of flooding requires knowing when floods are likely to occur however exploratory analysis of seasons may yield spurious conclusions and models of flood seasonality can be misleading if they fail to account for the cyclical nature of flood data or the possibility of multiple flood seasons in a year we modeled the arrival times of floods at 4 505 sites along rivers in the united states using statistical models comprised of weighted combinations of circular von mises distributions circular distributions address the periodicity of arrival time data while mixture models provide the flexibility to fit multiple seasons and a validation framework to test their significance nearly half of all sites we modeled optimized with at least a secondary flood season a fact hidden by analyses that neglect or conflate multiple seasons we found spatiotemporal patterns in the modeled dates of elevated flood hazard which point to common flood generating mechanisms partitioning sites by their modeled flood seasonalities yielded 6 7 clusters of sites along united states rivers with similar flood season numbers dates and durations results varied depending on whether floods were defined by annual maxima or peaks over threshold and on the threshold chosen models of river flood seasonality and the spatial patterns they reveal are potentially useful in informing the timing of infrastructure maintenance in risk sharing via financial instruments and in estimating the likelihood of compound events keywords flooding seasonality river flood circular statistics directional statistics statistical modeling mixtures of distributions data availability data are publicly available from the usgs nwis 1 introduction floods affect more people and cause more economic losses than any other natural disaster affecting 2 3 billion people from 1995 to 2015 wahlstrom and guha sapir 2015 and causing over one trillion usd in damages 2013 values between 1980 and 2013 winsemius et al 2016 globally economic damages due to floods are increasing with these increases largely attributable to population growth migration into hazard prone areas and increasing values of assets at risk bouwer 2011 tanoue et al 2016 in some regions however flood frequency is increasing mallakpour and villarini 2015 blöschl et al 2015 and further increases may be expected in the future milly et al 2002 dottori et al 2018 hirabayashi et al 2013 managing this risk requires knowledge not only of the likelihood of a flood of a given magnitude but also when that flood is likely to arrive the timing of high water affects preparation for emergency response planning of major infrastructure maintenance and the joint probability of coincident flood events although river flood seasons rarely have rigorous definitions times of increased flood hazard have been approximated by linking to the seasons of meteorological phenomena such as the spring freshet allison et al 2000 hurricanes magilligan and graber 1996 or rainfall black and werritty 1997 among others this approach neglects non weather hydrological phenomena which contribute to flooding such as antecedent soil moisture ye et al 2017 and is complicated by the fact that in many locations floods result from a variety of mechanisms hirschboeck 1991 this variety in flood generating mechanism causes the observed flood record to be comprised of mixed populations of data each with its own statistical distribution lecce 2000 smith et al 2011 villarini 2016 barth et al 2019 as a result these mixtures of populations violate the assumption that floods and their arrival times are independent identically distributed iid random variables that can be modeled by fitting unimodal probability density functions to observed data an assumption that underlies traditional flood frequency analysis techniques focusing on a primary flood season to the exclusion of secondary and tertiary and higher seasons could also leave communities vulnerable to unexpected flooding modeling river flood seasons under a framework that explicitly addresses multiple populations could allow for better preparation for extreme events and would more closely reflect the physical processes at play in addition to potentially containing multiple populations flood arrival dates are also an example of periodic data the upper and lower limits of the data range are equal this fact may create discontinuities in the probability density function if models developed for linear data are used without first transforming the data or performing other corrections periodic data also known as circular or directional data arise in many scientific fields most commonly when observations are recorded as times angles or directions examples of such data include wind and wave directions wang and gelfand 2014 mastrantonio et al 2016 geological strike angle pewsey et al 2013 animal movement directions eckert et al 2008 mclellan et al 2015 and the time of day or day of week for incidence of crime brunsdon and corcoran 2006 shirota and gelfand 2017 there also exists a rich literature of circular statistics applications to sinusoidal data particularly in medicine e g van oosterom et al 2000 pomfrett et al 1993 the periodicity of flood arrival times can be observed by considering that two floods occurring on december 30th and january 1st have a mean arrival date of july 2nd but a circular mean date of december 31st a much more useful interpretation of the data addressing this periodicity in the more familiar linear space is possible indeed this is the motivation for the concept of the water year but requires knowing a priori when to divide the data lecce 2000 for example modeled the arrival times of flooding by first breaking the year into seasons using a dataset from the state of north carolina but relying on site specific knowledge or visual inspection to determine the best dates to divide years into seasons could become overly subjective or cumbersome for large datasets in contrast circular statistical models treat data as points on a circle so periodicity is handled implicitly and temporal divisions are unnecessary as a result circular distributions are natural fits for repeating data including the dates of floods villarini 2016 black and werritty 1997 circular statistics address the periodicity of floods but cannot on their own address the issue of mixed populations to address both issues more flexible methods are needed such as the projected normal distribution mastrantonio et al 2015 optimized circular kernel density estimates dhakal et al 2015 or mixtures of circular statistical distributions rios gaona and villarini 2018 veatch and villarini 2020 in the present study we model the arrival dates of floods using mixtures of von mises distributions a mixture model of several von mises distributions allows the probability density of any point on the circle to be estimated as the weighted linear sum of multiple component distributions with their own parameterizations providing the flexibility to fit multimodal data thus mixture models of circular distributions provide an intuitive and flexible approach to modeling multimodal periodic data such as flood seasons while many others have worked to characterize the seasonality of river flooding e g archer 1981 mccuen and beighley 2003 beurton and thieken 2009 mediero et al 2015 klaus et al 2016 ryberg et al 2016 including by the use of circular statistical methods e g burn 1997 ouarda et al 2006 parajka et al 2009 chen et al 2010 chen et al 2013 jeneiová et al 2016 villarini 2016 blöschl et al 2017 ye et al 2017 hall and blöschl 2018 this study is to our knowledge the first application of mixture models of circular distributions to a large scale characterization of river flood seasonality by modeling flood seasons rather than relying on descriptive criteria such as circular mean and variance we can select the best fitting estimate of flood hazard and provide a simulation framework for forecasting filling of data gaps and scenario analysis our approach also reveals additional flood seasons that may be hidden in a single description of overall flood seasonality we apply these models to address three main questions 1 during what time s of the year is the likelihood of river flood greatest 2 what distinctions if any exist between regions of similar river flood seasonality both in the dates of peak flood hazard and the duration of elevated hazard and 3 how do these patterns change when considering annual maxima series versus peaks over threshold partial duration series 2 data and methods to reveal patterns in the seasons of river floods in the united states we fit mixture models to four river discharge datasets discharge data are from the united states geological survey usgs national water information system nwis us geological survey 2020 and were processed using the r programming language r core team 2020 in rstudio software rstudio team 2020 river discharge data were investigated for 23 253 sites with the present analysis limited to 4 505 which had sufficiently complete and recent data records to support analysis fig 1 the completeness criteria used in this study required that each gage include at least 90 daily completeness in 25 of its most recent 30 water years and that the last complete year in the record be water year 2013 from 1 october 2012 to 30 september 2013 or later these criteria ensured that results would be reliable and representative of close to present conditions annual maxima were extracted from each daily discharge dataset by water year peaks over threshold pot events were extracted with thresholds set to yield one two and four peaks per water year on average to ensure independence between events in the pot series we followed the guidance of the us water resources council 1981 designating peaks independent if they were separated by at least as many days as five plus the natural logarithm of the streamgage s watershed area in square miles and with the further constraint that the river discharge must fall to less than 75 of the lower of the two peaks before rising again gages without watershed area information available were excluded from the pot analysis these preparations yielded four datasets of river floods annual maxima and peaks over threshold with one two and four peaks per year hereafter referred to as pot1 pot2 and pot4 dates of peak river discharge were used to fit mixture models consisting of linear combinations of von mises distributions each with its own optimized parameter values the von mises distribution is the two dimensional projection of the von mises fisher distribution for data on the unit hypersphere it has two parameters mean and dispersion or equivalently angle and length of characteristic vector making it a kind of circular analogue to the normal distribution pewsey et al 2013 as a result it is the most prominent model used in circular statistics pewsey et al 2007 as detailed in qin et al 2016 a d dimensional unit vector θ describing a point on the unit hypersphere i e θ r d and θ 1 is said to follow the d variate von mises fisher distribution if its probability density function is given by 1 f θ μ κ c d κ exp κ μ t θ c d κ κ d 2 1 2 π d 2 i d 2 1 κ where μ 1 κ 0 d 2 μ t θ is the cosine similarity between θ and μ and id 2 1 κ refers to the modified bessel function of the first kind with order d 2 1 and argument κ for the special case where d 2 the von mises fisher distribution simplifies to the von mises distribution for data on the unit circle 2 f θ μ κ e κ cos θ μ 2 π i 0 κ the mixture model comprised of h unimodal von mises distributions f θ μ h κ h is then defined for h 1 h as 3 f θ α h μ h κ h h 1 h h 1 h α h f θ μ h κ h where αh are the mixture weights which are subject to the constraints 0 αh 1 and h α h 1 eq 3 has no analytical solution so the parameters α h μ h κ h h 1 h are computed via maximum likelihood estimation using the expectation maximization framework banerjee et al 2005 before using each dataset to fit a mixture model we tested it against the null hypothesis of circular uniformity with four tests rayleigh s test of the significance of the length of the sample data mean vector kuiper s one sample test of uniformity on the circle watson s goodness of fit test for the circular uniform distribution and rao s spacing test of uniformity jammalamadaka and sengupta 2001 we rejected the assumption of uniform circularity at sites with sufficient statistical evidence at an overall significance level of 5 when considering all four tests together with the bonferroni correction bonferroni 1936 applied to adjust for multiple comparisons for those sites with sufficient evidence to reject circular uniformity models example shown in fig 2 were fit consisting of one through four von mises distributions using hornik and grün s 2014 movmf package with the best model i e the optimal choice of h identified based on minimization of schwarz s bayesian information criterion bic schwarz 1978 selection based on minimizing bic allows additional component distributions to improve model performance while reducing the chance of over fitting fig 2 while bic gives a relative measure of model performance that is suitable for determining which among the candidate models is the best in a relative sense it does not indicate whether the chosen model performs well in an absolute sense as a more absolute measure of the goodness of model fit we constructed quantile quantile plots for each site comparing the quantiles of the observed data to those generated by the best fitting model fig 2 we generated empirical cumulative distribution functions to convert quantiles of observed data to probabilities then generated 1 000 exceedance probabilities from each best fitting model to create a lookup table of modeled quantiles as the mixture models used here do not have analytical distribution functions to invert for this purpose quantile quantile plots were then generated using the closest matching model quantile from the lookup table for each probability associated with each quantile of observed data with the correlation coefficient between these two datasets as a measure of goodness of model fit figs 2 5 and s2 once the models were fit and selected dates of peak flood hazard local maxima were identified from the fitted circular distributions with the date of greatest probability density designated as the primary flood season the next highest as the secondary season and so on to characterize the length of flood seasons we arbitrarily defined the beginning and end of a flood season as the points on the distribution surrounding a peak where the flood hazard is changing most rapidly i e the local maximum and minimum respectively of the distribution s first derivative fig 2 alternative definitions for a flood season undoubtedly exist but the exact meaning of a flood season length is less important in this case than a simple delineation allowing relative comparisons between seasons and sites to investigate spatial patterns in river flood seasonality we divisively clustered sites by their flood seasons as revealed by the fitted models we characterized the similarity between sites using gower s distance gower 1971 a distance metric for observations containing a mixture of diverse data types in the present study the use of this metric was necessitated by the inclusion of peak flood dates which are circular data and thus not appropriate for euclidean distances gower s distance is defined as the mean of a set of distance metrics each normalized by its maximum range 4 d g x i x j 1 q q 1 q d ijq x i x j 5 d ijq x iq x jq max x iq x jq where q is the maximum number of variables associated with each observation and dijq is the dissimilarity function most appropriate for each variable type x in the present study we characterized flood seasonality at each site using its number of flood seasons and each season s peak date and length in days the dissimilarity function for the number of seasons and the length of each season is the absolute difference while the dissimilarity of the peak flood season date required using the cosine dissimilarity reprojected onto 0 1 normalization by the maximum distance was performed using the maximum observed rather than maximum theoretical distance for each variable across all i j pairs normalization by the number of variables q is necessary for comparing partly observed responses e g comparing dates of peak flood hazard for two streamgages with different numbers of seasons sites and flood events were not restricted to nearby locations and no variable describing gage location e g latitude and longitude was included in the gower s distance computation ensuring that the resulting spatial patterns would be reflective of flood seasonality alone rather than geographic considerations this dissimilarity metric was then the basis of our divisive clustering of sites by partitioning around medoids pam kaufman and rousseeuw 1990 with the best number of clusters chosen by maximizing the average silhouette width kaufman and rousseeuw 1990 by partitioning around medoids representative members selected to denote the center of a cluster rather than means as in k means clustering pam avoids reliance on euclidean distances instead using only the provided dissimilarity matrix to assign cluster membership 3 results and discussion 3 1 spatial patterns across datasets this analysis reveals clear spatial patterns in flood seasonality along rivers in the united states circular mean dates of peak flow point to regional differences in flood timing and the circular variance of these dates to patterns in the dispersion of floods in time fig 3 moreover these metrics illustrate differences between annual maxima and pot series which tend to observe more events throughout the year and thus give a broader picture of potential flood hazard timing however this exploratory analysis though it addresses periodicity via usage of circular methods presents an incomplete and often misleading picture of flood seasonality as discussed below sites for which we had insufficient evidence to reject the null hypothesis of circular uniformity tend to be located in the piedmont plateau and coastal mid atlantic with additional clusters in florida the texas coast and the desert southwest fig 4 indicating that in these areas floods are equally likely or unlikely to occur throughout the year the number of sites without sufficient evidence to reject this hypothesis was the smallest in the pot4 dataset which logically contains the most events and therefore brings the greatest statistical power to bear on this test for sites with sufficient evidence to reject the uniformity assumption the lowest bic models fit observed data well correlation coefficients for quantile quantile plots comparing observed peak flow data from the annual maxima pot1 pot2 and pot4 datasets to the equivalent probability quantiles generated from the best fitting model averaged 0 97 0 06 0 96 0 06 0 98 0 03 and 0 99 0 01 respectively mean standard deviation model fits tended to be strongest in the south southeast and west coast with weaker correlations in the higher latitudes and mountain west fig 5 of the 4 505 sites modeled 37 44 optimized with at least a secondary flood season with the pot4 dataset having the highest percentage of sites with at least two seasons and the pot1 having the lowest fig 6 only 4 7 of models optimized with three or more seasons indicating that while many sites in the united states are subject to multiple flood seasons few experience more than two with so few sites having tertiary and higher seasons those seasons are not discussed further spatial patterns in the modeled dates of highest flood hazard point to regional differences in flood generating mechanisms fig 6 left column west coast gages show a primary flood season in the winter aligned with the usual timing of cold season atmospheric rivers lackmann and gyakum 1999 with the date of greatest flood hazard occurring later in the winter with decreasing latitude from the sierra nevada to the central lowlands floods primarily occur in the late spring along with peak rainfall intensity berghuijs et al 2016 with sites in the mountain west biased earlier toward the season of peak snowmelt floods in the southeastern states and ohio valley are most likely earlier in the spring when consistent spring rains lead to saturated soils berghuijs et al 2016 the upper midwest and new england states experience earlier primary flood seasons than neighboring areas of lower latitude and lesser snow accumulation only florida and select sites in the desert southwest show a primary season in the late summer likely the results of atlantic tropical waves smith et al 2011 villarini and smith 2010 and the southwest monsoon adams and comrie 1997 respectively in addition to the dates of peak flood hazard spatial patterns are also visible in the duration of elevated hazard fig 7 the mountain west upper midwest and new england states have the shortest primary flood seasons followed by the west coast sites the southeastern and ohio valley sites have the longest primary seasons of elevated flood hazard these patterns in season length are also congruent with the relative durations of suspected flood generating mechanisms peak snowmelt upper midwest mountain west and new england being the most temporally constrained followed by atmospheric rivers west coast with soil moisture accumulation punctuated by rainfall events southeast and ohio valley being the most temporally dispersed secondary seasons show a more mixed picture fig 6 right column than primary seasons distinct temporal patterns in the secondary season can be seen in the upper midwest summertime extending into autumn in the southern great plains the existence of these secondary seasons may be known to experienced managers but their statistical significance may be new information and their spatial distributions point to causal mechanisms the secondary season in the upper midwest aligns with the primary season for the rest of the midwest implying that the processes at play there also matter here though to a lesser degree than snowmelt does new england has a secondary flood season in the winter which may point to ice jam floods while the secondary flood seasons for florida and the atlantic coastal plain each match the other s primary season indicating that the same two mechanisms may be in play in both places but with their relative significances reversed the lengths of secondary seasons show little spatial pattern compared to primary seasons though secondary seasons are somewhat shorter overall ranging on average from 74 to 92 of the length of the primary season for sites with at least two seasons depending on the dataset this difference in length is the greatest for the pot4 dataset and the smallest for the annual maxima and pot1 datasets insights may also be gained by comparing the patterns in annual floods with those from partial duration series differences in modeled primary season are subtle especially when contrasted with the clear differences in circular mean flood date shown in fig 3 computing an overall average flood date from a partial duration series allows secondary season events which may rank among the largest events in the record without being the highest in their particular water year to bias the average toward a date that lies between two seasons but does not correspond to a time of elevated flood hazard examples include the apparent winter flood season shown in exploratory analysis of floods in georgia and south carolina fig 3 left column rows b d a time that lies between the late summer and early spring seasons revealed by the fitted circular models shown in fig 6 left column rows b d a similar effect may be observed in iowa and missouri where the two seasons of early summer and mid fall revealed by the fitted models are conflated into a single late summer season when computing an overall average from partial duration datasets in contrast to these distinctions average flood dates computed from annual maxima series generally show good agreement with the primary seasons revealed by fitted models implying that the overall average date of floods on a water yearly basis is often most reflective of the primary flood season and its generation mechanism however even in this series there is some risk of misinterpretation as with the upper midwest and new england sites where the overall average date of flood is an average of the two seasons revealed by statistical modeling while modeling the annual maximum series does capture secondary seasons fig 6 second column row a the revealed patterns in season are less spatially coherent than in the partial duration series and this coherence increases with the number of events included rows b d this implies that the partial duration series captures some secondary season events missed by the annual maximum series and that including more events in the partial duration series improves characterization of secondary flood seasons presumably this benefit would eventually diminish if the event threshold were lowered further to the point that too many non flood events were included in the analysis while we did not seek to optimize the threshold for inclusion of events in this study we show all three partial duration series to illustrate the effect of modifying that threshold a similar slight improvement can be observed in the depiction of the relative duration of the flood season with modeled primary flood season length showing more spatial coherence as more events are included in the partial duration series compared to the annual maximum series fig 7 first column this improved coherence is also reflected by the pot4 dataset yielding the highest average silhouette width of any dataset used for spatial clustering as described in the following section figure s1 supplementary material as a method of characterizing flood season variability modeled season durations represent a major improvement over circular variance as inclusion of many events tends to destroy all spatial patterns in flood season variance fig 3 right column row d 3 2 clustering analysis clustering of streamgages using the dates and durations of seasons extracted from fitted circular models formalizes the apparent spatial patterns described in the previous section clustering via pam and selecting the optimal number of clusters using maximal average silhouette width yields six clusters for the annual maximum pot1 and pot2 datasets and seven clusters for pot4 fig 8 gages with wider higher silhouette scores are more closely related smaller distance to members of their own cluster than they are to their next closest cluster therefore a higher mean silhouette width across all gages represents a measure of improvement in the choice of the number of clusters allowing more clusters tends to increase the mean silhouette width up to a point after which over fitting begins to reduce the mean width by dividing closely related groups figure s1 supplementary material the fact that all datasets optimized with six clusters with the exception of the pot4 dataset which optimized with seven indicates that spatial clustering of sites by flood seasonality is somewhat robust to the choice of dataset while these clusters are not perfectly consistent across datasets some common groupings emerge inland sites in the southeast and the ohio valley tend to cluster with sites on the west coast especially its southern extent sites in these areas tend to have a single flood season in the late winter though their generating mechanisms are presumably distinct only the pot4 dataset optimizing with an additional cluster splits the west coast sites into sub regions sites in the upper midwest tend to form a single coherent group with two flood seasons in the spring and summer despite their superficial similarity in modeled primary season fig 6 and average annual flood season fig 3 the upper midwest and new england sites secondary seasons are different enough to prevent them from clustering together sites along the middle mississippi river and missouri river drainages tend to cluster together despite their differences in primary flood season duration while sites along the south atlantic coastline stand out from the inland southeast for their biseasonality causing them to cluster with the biseasonal gages in the mountain west and southern great plains the florida and desert southwest sites with their dual flood seasons and primary summertime seasons also cluster together the clusters identified here are less spatially contiguous than but still bear a rough similarity to those identified by brunner et al 2020 in their analysis of regional flood events despite the fact that their analysis uses fewer sites and events and their clusters are based on rank order similarity of flood events rather than flood seasonality the fact that some similar clusters still emerge indicates underlying similarities across sites that control both their flood seasons and which floods events impact them most significantly 4 conclusion we fit mixture models of circular von mises distributions to the dates of peak river discharge at 4 505 river gage sites in the united states fitting mixtures of circular statistical models to the arrival dates of floods offers several potentially useful insights for flood risk management we found that nearly half of all river gages investigated have at least two seasons per year when residents and other stakeholders can expect elevated flood hazard while this multimodality of flood seasonality may in some cases be intuited from experience or inferred from exploratory analysis delineating these seasons using fitted models and selecting the best available model using a likelihood based criterion such as bic helps ensure that identified flood seasons are supported by data while reducing the conflation of multiple seasons we found that while a circular average date of annual flood arrival may do a passable job of estimating the peak date of the primary season in many cases secondary and higher seasons will remain hidden circular mean flood dates from partial duration series provide an even less reliable depiction of actual flood seasons while estimates of flood season variance from partial duration series are of almost no value at all in contrast fitted models of partial duration series provide the most spatially coherent patterns of flood season arrival date relative season length and season number this coherence increases as the flood threshold is lowered and more events are included in the series clustering river gage sites into groups based on the number timing and lengths of their flood seasons reveals regions of the country facing similar temporal patterns in their probability of flooding partitioning sites using pam and maximizing silhouette width optimizes these groupings to maximize intra cluster similarity relative to inter cluster differences providing a sounder basis for assessing similarity than relying on visually apparent commonalities some similar regions like the upper midwest can be explained by common flood generating mechanisms other groupings such as the inclusion of ohio valley sites with west coast sites may be more coincidental or point to indirect teleconnections whatever the reasons for their similarities flood risk managers in these areas may find opportunities to share knowledge best practices and lessons from past events similarly to the sharing within the i storm network of storm surge barrier operators lassing et al 2018 the criteria used here to rank flood seasons by importance and to measure their durations are admittedly arbitrary and their interpretation should be limited to relative comparisons across sites rather than ranking flood seasons by their maximum flood hazard we could have chosen an accumulation of overall hazard across the season rather than delineating flood seasons according to their derivatives we might have chosen an alternative approach to marking their starts and ends such as the second derivatives which would capture more of the shoulders of each period of elevated hazard as one example alternate approach cunderlik ouarda and bobée 2004 proposed and several have adopted an approach whereby flood seasons are defined in terms of relatively flood rich and flood poor months by testing the observed rate of flood occurrence against the circular uniform distribution while such process refinements could have some effect on the relative rankings of seasons by importance and the resulting clustering of sites the lack of an agreed upon definition for a season for flooding or other contexts means that any interpretations of flood seasonality in an absolute sense should be made with caution in cases where two flood seasons have nearly equal peak hazards the identification of primary and secondary may also be of little value for managers the most actionable finding here may be that multiple annual flood seasons which may or may not be apparent from visual inspection of hydrographs may now be evaluated formally and characterized by their statistical significance timing and duration the spatial clusters identified by partitioning of river gages around medoids describing their flood seasonalities present an opportunity for further research into flood generating mechanisms geomorphological and climatological similarities among sites and the potential for regionalization of statistical parameters for flood frequency analysis or other purposes as the earth s climate continues to evolve due to anthropogenic climate change research is also needed into the stationarity of flood seasons including changes that vary regionally for managers knowing when floods are most likely to occur may be valuable for design and maintenance of physical infrastructure construction of financial risk transfer instruments such as insurance policies and catastrophe bonds and assessment of the risks presented by compound and coincident events compound floods such as river floods and hurricane surges occurring simultaneously are perhaps the most prominent example of such events but many types of compound extremes exist which may complicate flood risk management zscheischler et al 2020 including wildfires extreme temperatures and even financial crises and hostile acts investigating these possibilities while equipped with knowledge of likely event timing could provide advantages over methods that generally require an independence assumption due to lack of data e g usace 1993 ch 11 credit authorship contribution statement william veatch conceptualization methodology software validation formal analysis data curation writing original draft writing review editing visualization funding acquisition gabriele villarini conceptualization writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements and data william veatch was supported by a grant from the office of the under secretary of defense for research and engineering usd r e national defense education program ndep ba 1 basic research gabriele villarini acknowledges support from the usace institute for water resources datasets for this research are available from the source cited as usgs 2020 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128330 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
