index,text
0,although the fluid invasion process impacts the transport and dispersion of chemicals and contaminants in the resident phase of natural manufactured and biological unsaturated porous media these effects remain poorly understood in this study we investigate the role of hydrodynamic dispersion in different invasion patterns within the resident phase while nonidealities in pore structures contribute to mechanical dispersion our findings demonstrate that the interface front s morphology and the invading phase s distribution significantly influence mechanical dispersion within the resident phase our results show that the complex distribution of the invading phase under unstable displacement greatly affects mechanical dispersion measures such as velocity distribution variance and mean square displacement obtained through particle tracking the interface also alters hydrodynamic dispersion measures such as the local peclet number and generates extensive diffusion dominated regions across the domain this ultimately leads to solute trapping between pores that cannot reach the outlet however under stable invasion mechanical and hydrodynamic dispersion do not deviate significantly from the reference saturated condition in this scenario dispersion anomalies occur within a short distance ahead of the moving interface beyond which variations become similar to the reference media keywords transport in porous media unsaturated porous media hydrodynamic dispersion invasion process interface induced dispersion pore scale modeling data availability no data was used for the research described in the article 1 introduction hydrodynamic dispersion is a primary transport mechanisms occurring in diverse forms of porous media including natural e g aquifers and hydrocarbon reservoirs manufactured e g membranes and paper and biological e g bones and woods porous media anna et al 2021 essandoh et al 2013 hunt and sahimi 2017 puyguiraud et al 2021 sookhak lari et al 2019 wang et al 2014 the study of dispersion involving contaminants dissolved minerals pesticides fertilizers dissolved gasses and organic substances in the field of hydrogeology has gained significant attention due to the increased environmental concerns bear 2018 liu et al 2021 yan et al 2020 additionally advancements in numerical methods and experimental facilities have greatly contributed to the development of this knowledge blunt et al 2013 jahanbakhsh et al 2020 at its core hydrodynamic dispersion occurs in a porous medium due to two processes molecular diffusion which is the mixing caused by random molecular brownian motion and mechanical dispersion which results from variations in velocity causing mass to disperse as velocity changes dentz et al 2016 domenico and scbwartz 1982 fetter et al 1992 these two physical processes become more complex in a porous media due to intricate pore structures and pore space tortuosity leading to smaller values for molecular diffusion compared to non porous media moreover mechanical dispersion in porous media can be intensified by alterations in transport direction and flow rates caused by nonidealities in the porous medium fetter 1992 patrick a domenico 1998 less studies have explored dispersion under unsaturated conditions gong and piri 2020 jiménez martínez et al 2020 mohammadmoradi et al 2018 relative to saturated conditions dentz et al 2018 2016 puyguiraud et al 2021 2019 significant advances in mathematical models computational methods and experimental equipment have resulted in more accurate ways to understanding the underlying physics of this phenomena blunt et al blunt et al 2013 and wildenschild et al wildenschild et al 2002 have reviewed possible imaging and numerical techniques for simulating single and multiphase flow and transport simultaneously with studying dispersion in saturated media in recent years dentz et al 2011 kang et al 2014 souzy et al 2020 there has been a surge of interest in research on dispersion and its dependence on the saturation of the media in unsaturated porous media in an early study examining dispersion under unsaturated conditions sahimi et al 1986 explored both longitudinal and transverse dispersion across phases by analyzing the statistical distribution of two phase flow in a simple cube network of random radius pores their main finding was that the dispersivities were dependent on the phase distributions saturation and saturation history bekri 2002 used the immiscible lattice boltzmann and random walk methods to investigate the effect of peclet number and water saturation on the dispersion in two phase flow in a reconstructed porous media by utilizing peclet number and water saturation they established general correlations and identified asymptotic values for the longitudinal dispersion coefficient over extended timeframes later studies hasan et al 2019 jimenez martinez 2017 jimenez martinez et al 2015 karadimitriou et al 2017 have been conducted to demonstrate an increase of solute mixing in invading phase with a decrease of water content which was caused by an increase of flow velocity variation for various pathways however raoof and hassanizadeh 2013 revealed that the dispersity reaches a maximum value and then decreases with further decrease in saturation using pore scale modeling additionally many attempts have been made using direct numerical modeling to explore solute transport through various steady state fluid flow configurations aziz et al 2018 hasan et al 2020 jimenez martinez 2017 jiménez martínez et al 2020 almost all studies that explore two phase conditions have focused on the dispersion of solute within the invading phase furthermore these studies have not addressed dispersion under fully transient two phase flow missing the dynamic behavior of the system aziz et al 2018 gong and piri 2020 invasion of the fluid phases and associated movement of interfaces cause significant changes in the flow velocity field flow paths and tortuosity of the streamlines which controls transport of solute and colloids ahead of the interfaces the impact of interface dynamics in different invasion regimes on resident phase dispersion has not yet been investigated prior studies bekri 2002 sahimi et al 1986 do not clearly explain how interface movement alters mechanical and hydrodynamic dispersion and the associated length scales the absence of dynamic behavior in the interface front across various invasion regimes leaves the areas of the resident phase affected by the invading phase front and the depth of this impact uncertain gaining insight into the pore scale specifics of a real sand pack sample such as the effects of grain heterogeneity will aid in determining the essential parameters for continuum modeling as a result a pore scale examination of the invasion process is necessary for a comprehensive investigation of resident phase dispersion in this study we simulate the dynamics of different invasion regimes to examine the resulting changes in the flow field and solute transport within the resident phase the primary objective of this study is to assess the influence of the invading phase on the hydrodynamic dispersion of the resident phase by comparing the region ahead of the interface during the invasion process to saturated cases as a reference we explore the underlying physics of pore scale variations not previously addressed in the literature several simulations were conducted to perform a systematic study and quantitatively measure the impact of two phase regime effect the outcomes were then compared with the results obtained under saturated conditions for the same flow velocity serving as a reference case to examine the influence of various invasion patterns the advection effects and advection diffusion effects were explored separately the study is divided into three main sections section 2 presents an overview of the methods utilized to obtain the flow field and solute spreading section 3 discusses the results and their implications while section 4 provides the conclusions drawn from this study 2 methodology to investigate how the invasion process affects dispersion in the resident phase it is necessary to examine fluid flow and solute transport in both fully saturated and unsaturated media at the pore scale there are two commonly used methods for simulating pore scale flow network modeling and direct numerical modeling pore network modeling simplifies the complexity of pore structures into interconnected simple pores which can result in a loss of detailed information blunt et al 2013 shams et al 2018 to obtain a detailed understanding of the flow field this study uses direct numerical modeling in the following section we describe the governing equations computational procedure and properties of the computational domain used in this work 2 1 computational domain a two dimensional representation of the computed tomography image of a natural sample of granular sand pack from the previous published work vries et al 2022 is used to perform the direct numerical simulation as shown in fig 1 the computational domain and the generated numerical grid are kept identical throughout this study presence of a large number of grains in each principal direction provide a large enough domain for our simulations table 1 provides properties of the computational domain numerical discretization of the domain is performed using the openfoam utility snappyhexmesh which is capable of producing high quality grids within narrower pore throats and non smooth edges from complex porous media geometry to study grid independency of results we have performed simulation for different grid resolutions in a sub section of the whole domain resulting in the final number of grids in the table 1 which provides a minimum of ten numerical cells in the narrowest pore throats 2 2 governing equations and numerical methods of saturated simulation conservation of mass and momentum for isothermal and incompressible flow of newtonian fluid is governed by the navier stokes equations as 1 u 0 2 ρ u t ρ u u p 2 μ e f b where u denotes the fluid velocity p the static pressure ρ fluid density and μ fluid dynamic viscosity f b accounts for all body forces and e 1 2 u u t is the rate of strain tensor white 1991 in order to solve the advection diffusion of a passive tracer within the carrier flow eq 3 has been used 3 c t u c d m c 0 where c denotes concentration of solute and dm is molecular diffusion coefficient the numerical solution of saturated governing equations was performed using the openfoam open source toolbox jasak 1996 the steady state form of navier stokes eqs 1 and 2 was used to obtain the velocity field at the saturated medium using simplefoam solver next eq 3 is solved to obtain the transient evolution of solute transport within the steady state flow domain using the scalartransportfoam solver the advective term was interpolated using the upwind approach which is a first order and explicit method the diffusive term was discretized using a second order scheme known as gaussian linear corrected scheme and the time derivative was discretized using the euler scheme because of the sharp gradient of concentration between stagnant and flowing regions the transport matrix was solved using smoothsolver with a small relative tolerance of 10 16 aziz et al 2018 2 3 governing equations and numerical methods for invasion process fundamentally calculating the velocity field in immiscible two phase flow involves solving two separate navier stokes equations one for each primary phase a common boundary condition is employed to monitor the interface and depict the development of the two phase system however this approach necessitates solving the moving boundary problems for each phase which is challenging and extremely time intensive particularly for porous media problems involving intricate pore structures a more effective alternative for addressing the immiscible two phase flow is the whole domain formulation hirt and nichols 1981 this method treats the two phase system as a single phase system with location dependent properties and replaces the jump condition with an extra force that only operates at the interface the phase function f x characterizes the spatial distribution of both phases and is defined as follows 4 f x 1 with in the wett ing phase 0 with in the non wett ing phase the interface is represented by the surface of discontinuity of the phase function and it is indicated by the delta function concentrated on the two phase interface and space dependent properties are described by the following equations 5 ρ x f x ρ w 1 f x ρ n w 6 μ x f x μ w 1 f x μ n w ρ x and μ x are the density and the dynamic viscosity respectively considering the bulk fluid properties are assumed constant consequently the immiscible two phase flow is governed by only one set of navier stokes equations which is written as 7 ρ u t ρ u u p 2 μ e f b f s with an extra term f s which represents surface tension forces which is nonzero only at the phase interfaces and describes the effect of the laplace pressure in whole domain approach as numerical diffusion would smear the interface due to space truncation error a standard finite difference discretization cannot be used deshpande et al 2012 the numerical solution of governing equations of immiscible two phase flow was performed using the openfoam open source toolbox contrary to the conventional volume of fluid vof method hirt and nichols 1981 an efficient approach for keeping a sharp interface is applied in interfoam solver in openfoam deshpande et al 2012 henrik rusche 2002 as a nonlinear convective term to overcome the diffusion of the interface which is accomplished by computing the compression velocity with the multidimensional universal limiter with explicit solution mules deshpande et al 2012 to obtain solute transport within the resident phase an advection diffusion eq 3 equation is solved using phasescalartransport as a function object in openfoam toolbox a flowchart for the two main stages of solute transport in the invasion process is shown in fig 2 the distribution of the immiscible phases is solved first followed by the solute transport process within the resident phase in each time step 2 4 numerical conditions both the top and bottom boundaries of the domain are treated as no flow boundaries the right boundary maintains a constant pressure and for solute transport simulations a zero solute concentration gradient in the normal direction is assumed the left or inlet boundary has a set injection velocity and a consistent dimensionless solute concentration for solute transport initially the domain is completely saturated with the wetting phase and a contact angle of 45 is presumed the non wetting phase is introduced at a stable flow rate on the domain s inlet face various simulation scenarios with differing capillary numbers and viscosity ratios were executed the findings indicate that the new capillary and viscosity ratios did not have a significant impact on the analysis 2 5 validation of the numerical method to ensure reliable simulation results and analysis we evaluated the employed numerical schemes for immiscible two phase flow in invasion processes against experimental data to verify our model we utilized micromodel experiments conducted by yin et al 2018 which involved exploring two phase flow displacement in a porous medium by comparing our simulation results with the experimental observations we were able to validate our model and ensure its accuracy table 3 provides a summary of our model validation approach which involved applying similar geometries and conditions specifically we investigated the displacement of a resident phase water by a non wetting phase a flourinert solution over time in a porous domain we obtained the inlet boundary condition for our numerical simulation by analyzing the variable inlet flowrate in the yin et al experiment from the original images in the numerical simulation we assumed a relative zero pressure for the outlet boundary condition due to the experimental conditions the comparison of phase distributions during the invasion process as shown in fig 3 exhibits excellent agreement with the experimental observations it is worth noting that the validation of openfoam formulations has been the subject of various studies involving diverse porous media for simulating immiscible two phase flow deshpande et al 2012 ferrari et al 2015 ferrari and lunati 2013 3 results we aim to investigate how the invasion process affects the flow field and solute spreading within the resident phase previous studies by lenormand et al 1988 have highlighted the distinct invasion patterns that emerge based on the capillary number and viscosity ratio m of the two phases to conduct a thorough analysis under various flow regimes we conducted three simulations a saturated medium as the reference case along with stable and unstable invasion simulations table 2 presents the conditions and properties of these simulations because the simulation of different capillary numbers and viscosity ratios used in this study had no effect on the analysis the findings of a single set of capillary numbers and viscosity ratios are presented here during a stable invasion process which occurs under high capillary numbers and high viscosity ratios the front of the invading phase advances uniformly with minor irregularities typically only a few grains in size as depicted in fig 4 however a decrease in capillary number or viscosity ratio results in an unstable invasion which is characterized by the development of growing fingers within the medium these fingers can move either in the direction of the main flow or in the normal direction to it referred to as viscous fingering and capillary fingering respectively an et al 2020 tsuji et al 2016 in this study we consider both types of fingerings as unstable invasions and simulate viscous fingering as an example of an unstable invasion to ensure a fair comparison of different invasions against the reference case properties must be utilized that maintain the same velocity despite variations in capillary numbers the study findings are divided into two sections the first section focuses on the effect of invasion on mechanical dispersion while the second section examines the impact of invasion on hydrodynamic dispersion in the resident phase through this approach both aspects of the invasion s impact on dispersion are explored and discussed 3 1 mechanical dispersion the presence of nonidealities in pore structures such as variations in grain size distribution morphology and topology pore and grain arrangements can lead to velocity fluctuations and increase mechanical dispersion although the pore system parameters remain consistent across all models in this study variations in flow regime result in diverse flowlines local velocity distributions and ultimately discrepancies in mechanical dispersion the next section examines and compares the degree of mechanical dispersion in the resident phase with the reference case 3 1 1 velocity variations fig 5 a displays the velocity distribution within the resident phase during stable invasion stb over normalized time values of 0 08 0 20 0 32 and 0 44 compared to the reference case of flow under saturated conditions sat the corresponding invading phase distribution profiles during stable invasion are also shown although the fraction of velocities with negative values increases over time due to small irregularities there is no significant difference between the four stable displacements and the reference case for velocity distributions a rather uniform interface characterized by a stable invasion serves as a relatively uniform boundary condition driving the flow results in a minor disruption specifically less than 2 5 percent in the velocity field of the resident phase in fig 5b the velocity distribution under unstable invasion event unstb is shown indicating an increase in the generation of velocities with near zero and even negative values due to the generation of fingers and their impact on the velocity field within the resident phase the velocity distribution tends towards lower and even negative values resulting in transverse movement of fluid streamline in the medium and larger residence times for fluid particles in the domain contrary to the stable invasion scenario fig 5b demonstrates a noticeable shift in velocity distribution for the unstable invasion instance exhibiting a change of approximately 25 when compared to the reference case to assess the impact of the invasion process we calculated the variance of velocity values in the resident phase as illustrated in fig 6 under stable invasion the magnitude of velocity variance fluctuates around that of the reference case with less than a 1 difference between the two cases conversely during unstable invasion we observed a rapid decline in velocity variance due to the emergence of finger structures within the resident phase in an unstable invasion the variance values approach zero at the last time step as depicted in fig 6 with nearly 98 percent of the velocity values falling into bins close to zero the interquartile range which serves as a statistical indicator of local velocity distribution demonstrates a comparable pattern of reduced mechanical dispersion throughout the unstable invasion process as illustrated in fig 6 3 1 2 associated length scales of the invasion in the preceding section we investigated the effects of stable and unstable invasions on velocity variations in the resident phase over time as a measure of mechanical dispersion however that analysis did not account for the positions and spatial distribution effects within the resident phase and instead examined the deviation of the velocity distribution from the reference case across the entire domain in this section we aim to explore the length scale of interface induced velocity variations ahead of the interface during stable invasion we calculated the velocity variations within the resident phase as a function of longitudinal distance from the interface fig 7 shows the velocity variations over distance for different times we found that the majority of velocity variations occur within a short distance equivalent to a few grain sizes from the interface as a result there exists a clear characteristic length for interface induced velocity variations and over larger distances the velocity field converges to that of a saturated media indicating that the impact of interface induced variations diminishes in fig 7 the orange dashed line illustrates the position of the interface tip where the left side primarily signifies the invading phase and the right side mainly denotes the resident phase the green solid circles in fig 7 represent the point by point velocity deviation values from the reference case across the entire domain this figure reveals that significant velocity deviations are mostly focused near the interface front rather than being dispersed throughout the entire resident phase in other words there is a notable disparity between the orange and blue dashed lines as demonstrated in fig 7 the blue dashed line is consistently around 2 mm beyond the interface tip for all time steps the corresponding length scale or the gap between the orange and blue dashed lines which is roughly 2 mm is on the order of a few grain diameters based on the average grain diameter value found in table 2 due to the significant irregularity in the interface it is challenging to determine a distinct length scale for the effect of the invading phase on the resident phase in simpler terms the highly irregular shape of the interface front during unstable invasion makes it difficult to establish a well defined parameter such as the maximum velocity variation within a related length as was done in the stable invasion analysis consequently this examination is only applicable to the stable invasion process 3 1 3 lagrangian particle tracking the quantification of mechanical dispersion in porous media can be accomplished through lagrangian particle tracking to this end we have initialized 25 thousand non diffusive point particles close to the entrance of the resident phase at the onset of the invasion process by tracking the transient location of each particle we have calculated the longitudinal and transverse mean square displacement msd as a measure of mechanical dispersion while disregarding the effect of diffusion bijeljic et al 2004 bijeljic and blunt 2007 fig 8 a displays the trajectories of the particles and fig 8b presents the longitudinal and transverse msd for all simulated scenarios our findings indicate that the spreading of particles within the resident phase is similar under stable invasion and reference conditions but deviates in unstable invasion scenarios while high velocity regions are scarce in unstable invasion low velocity regions are abundant throughout the pore space leading to a reduction in the msd magnitude over time and hence a decrease in mechanical dispersion the zones with dashed lines in fig 8a highlight the areas where the particle velocity is comparatively high in stable invasion 3 2 solute transport in section 3 1 we investigated and discussed the impact of the invasion process on the advection component of hydrodynamic dispersion specifically mechanical dispersion in the subsequent section we will examine the impact of the diffusion process to accomplish this we present and analyze the results of a local peclet distribution analysis and solute transport simulation 3 2 1 local peclet number distribution the porous medium is composed of a large number of interconnected pores each of which can be analyzed separately to determine its local peclet number defined as p e l o c a l u d d m where u is the mean cross sectional velocity within the pore d is the diameter of the pore and dm is the molecular diffusion coefficient low peclet number values indicate diffusion dominated transport while high values indicate advection dominated transport transitional values result in a mixture of the two dentz et al 2018 as shown in fig 9 the invasion process can influence the value of individual peclet numbers and their distribution within the resident phase depending on the type of invasion regime the distribution of local peclet numbers changes as the interface advances across the medium producing variations in hydrodynamic dispersion despite the fact that the macroscopic peclet number p e v d d m remains constant throughout all invasion cases where v is the average velocity d is the average grain diameter and dm is the molecular diffusion coefficient the local peclet number distributions in fig 9 show similarities at early stages of invasion but deviate from each other at later stages as time progresses unstable invasions tend to result in smaller local peclet numbers due to the evolution of complex fingers and the formation of stagnant regions resulting in a diffusion dominated transport mechanism conversely stable invasions maintain a relatively constant distribution over time in order to quantitatively evaluate the differences in local peclet distributions we present skewness and kurtosis values as indicators of distribution symmetry and tail behavior for each time step in fig 9 skewness values can fluctuate by as much as 70 during unsaturated invasion while they only change by around 20 in stable invasion scenarios the deviation of kurtosis values from the mean in stable invasion is approximately 37 whereas in other cases kurtosis values can deviate from the average by as much as 150 the local peclet number distributions exhibit greater similarities during the early stages of invasion however as time progresses the distributions deviate from each other unstable invasions lead to smaller local peclet numbers due to the formation of stagnant regions and the evolution of complex fingers as a result diffusion dominates the majority of pore spaces in unstable invasions conversely stable invasions maintain a relatively constant distribution over time the 2d profiles in each plot illustrate the invasion progress under stable conditions at each time step the value of skewness and kurtosis are presented in each time step to compare the symmetry and the tail of distribution respectively the terms stb and unstb stand for stable invasion and unstable invasion respectively 3 2 2 solute transport simulation to examine how the invasion process affects the spreading of solute in the resident phase we injected a pulse of solute that was initially distributed over a vertical strip within the pore spaces we then conducted simulations to observe the transient solute spreading under different invasion regimes the simulation results for the reference case stable invasion and unstable invasion are presented in fig 10 notably solute spreading in the resident phase was significantly different under unstable invasion compared to the reference case whereas in the case of stable invasion the concentration distribution was relatively similar to the reference case the reason for this is that the invading phase did not significantly impact the resident phase under stable invasion leading to minimal changes in the flow field and comparable advection and diffusion processes as illustrated in fig 10 fig 11 compares the longitudinal msd calculated with particle tracking to the longitudinal msd calculated with solute transport simulation which show the same trend particularly at the end times the early difference in fig 11 is caused by different solute and particle configurations at the beginning of the msd calculation in addition to examining the spatial distribution of concentrations see fig 10 comparisons between simulations can also be made by analyzing the outflow from different domains such as concentration breakthrough curves btcs fig 12 shows a comparison of the btcs of concentrations measured at the outlet face of the sample under stable and unstable invasions as well as the reference case our results indicate that during unstable invasion a significant portion of the solute is confined within the resident phase resulting in only a small amount of solute reaching the outlet this confinement causes the concentration values to be significantly smaller than those obtained under stable invasion and the reference case although the distribution of solute under stable invasion is similar to the reference case there are considerable differences between the two these differences are mainly due to solute confinement within trapped pores in the resident phase as the invading phase advances a percentage of the resident phase becomes trapped between the grains resulting in the confined solute not reaching the outlet to further analyze our findings we calculated the total area of the domain covered by concentration values ranging from 0 1 to 1 the evolution of this area in different regimes reveals that solute spreading is smaller in stable invasion than in the reference case but considerably larger values are observed for unstable invasion due to solute confinement within trapped regions in the resident phase in all scenarios the total area increases due to hydrodynamic dispersion although the slope varies during unstable invasion as a large amount of solute remains stationary and confined within the pore spaces the plot of the total area reached a relatively horizontal slope over time 4 conclusion in this study we employed a validated openfoam code to investigate the influence of two phase flow invasion processes on hydrodynamic dispersion within the resident phase a systematic investigation was conducted by performing multiple simulations and quantitatively assessing the invasion effects results were compared to those obtained under saturated conditions at the same flow velocity which served as a reference case although a single pore structure was utilized for simulations variations in flow regime led to different invasion patterns resulting in diverse flow lines local velocity distributions and ultimately mechanical dispersion a relatively uniform invasion front and interface during stable invasion induced minor variations in the resident phase velocity field compared to the reference case in contrast unstable invasion led to significant velocity fluctuations within the resident phase with values often falling within lower or even negative ranges causing transverse fluid particle movement within the medium a sharp decline in velocity variations as determined by interquartile range and variance parameters was supported by results from lagrangian tracking of fluid particles examination of spatial distributions revealed the associated length scale for interface induced velocity variations with major deviations occurring a few grain sizes ahead of the interface beyond this point deviations diminished and local velocities resembled those of the saturated reference case the pore space was segmented into pore elements and the distribution of local péclet numbers within these pores served as a metric for hydrodynamic dispersion throughout the entire resident phase our findings indicated that the local péclet number distribution tended towards lower or even zero value ranges resulting in an increased prevalence of diffusion dominated pores during unstable invasion solute transport simulations and concentration breakthrough curves validated this observation under unstable invasion the concentration breakthrough curve revealed the arrival of only a small solute fraction as the majority of mass remained behind the invading front in the case of stable invasion solute distribution resembled that of the reference case but concentration values were reduced due to solute confinement within trapped pores in the resident phase these results demonstrated that unstable invasion strongly impacts dispersion mechanisms whereas the influence of stable invasion is confined to a small length scale with deviations gradually diminishing beyond this range declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper author statement all persons who meet authorship criteria are listed as authors and all authors certify that they have participated sufficiently in the work to take public responsibility for the content including participation in the concept design analysis writing or revision of the manuscript 
1,in order to investigate the migration of nanoparticles in porous media the model developed by katzourakis and chrysikopoulos 2021 is applied to simulate the transport of aggregating nanoparticles under various initial conditions in the aforementioned model nanoparticles may collide with each other and form larger particle structures with different mobility and reactivity characteristics individual particles as well as aggregates can be found suspended in aqueous phase or attached reversibly and or irreversibly on the solid matrix the aggregation process modelled after the smoluchowski population balanced equation pbe is coupled with the conventional advection dispersion attachment ada equation to form a system of coupled equations that govern the transport of aggregating nanoparticles particle collisions are expected to increase exponentially with increasing initial number of injected particles n0 therefore substantially pronounced aggregation is expected when n0 is increased similarly the initial particle diameter distribution of the injected particles is expected to affect the average size of aggregates and in turn influence their mobility in a porous medium several model simulations were performed with different n0 and particle diameter distributions the results indicated the strong importance of taking into account the initial particle concentration and realistic particle diameter population distribution into consideration graphical abstract image graphical abstract keywords nanoparticles transport aggregation fitting porous media mathematical modeling data availability data will be made available on request 1 introduction in recent years the development of nanoparticles has attracted the attention of the scientific community because nanoparticles are used in a wide variety of applications in sectors such as energy health electronic pharmaceuticals cosmetics nutrition biomedicine oil and gas hubbard et al 2020 malandrakis et al 2022 mohajerani et al 2019 simonsen et al 2018 wang et al 2016 the unique structure of nanoparticles makes them special purohit et al 2019 simonsen et al 2018 nanoparticles have one dimension less than 100 nm and large surface to volume ratio nowack and bucheli 2007 due to their large surface areas nanoparticles can act as carriers for other substances while at the same time they are very reactive which is very useful for a number of catalytic applications purohit et al 2019 nanoparticles occur naturally from geological processes wind erosion and glaciations incomplete combustion volcano activity and forest fires and biochemical cycling performed by bacteria plants and fungi malakar and snow 2020 nanoparticles can also enter the environment from inappropriate disposal of industrial substances agricultural activities pesticides and fertilizers automobiles and remediation efforts water and soil decontamination buzea and pacheco 2017 mohajerani et al 2019 furthermore there is a constant flux of natural nanoparticles from the earth s surface to the atmosphere which is calculated to be 342 mt year while for artificially generated nanoparticles this flux is estimated to be 10 3 mt year hochella et al 2019 nanoparticles suspended in the atmosphere subsequently precipitate with rain deposit onto surface waters hochella et al 2019 and eventually reach the groundwater nanoparticles can be very reactive and may cause toxic or carcinogenic effects on human beings and other living organisms ganguly et al 2018 for example titanium dioxide nanoparticles and gold nanoparticles can cause genotoxicity damage the dna and create inflammation chen et al 2014 thusly understanding the mechanics of nanoparticle transport in environmental systems is of great importance as it can help to better predict and restrict water contamination a basic and extremely significant characteristic of nanoparticle transport in environmental systems is aggregation nanoparticles tend to contact each other often stick together and form larger structures aggregates with different mobility and reactive behaviour arosio et al 2012 babakhani et al 2019 solovitch et al 2010 typical mathematical models developed for nanoparticle transport in porous media often ignore the aggregation process these models use the filtration theory ft or blocking equations which fail to capture the physicochemical processes that nanoparticles undergo during their migration goldberg et al 2014 recently developed models may include an expression for aggregation coupled with the transport equation but they do not account for appropriate particle dispersion repulsive interactions between aggregates and realistic attachment or they do not provide an explicit mathematical formulation for the resulting governing transport babakhani 2019 babakhani et al 2018 li and prigiobbe 2021 2020 quik et al 2015 taghavy et al 2015 in typical bench scale column experiments nanoparticle aggregation erroneously may appear to be insignificant because the experimental conditions may promote strong repulsive forces between particles the initial particle concentration is low and the initial nanoparticle size distribution is ignored in view of these reasons particle aggregation is discouraged in this study the nanoparticle transport model developed by katzourakis and chrysikopoulos 2021 will be employed to highlight the importance of the initial nanoparticle concentration and size distribution on nanoparticle transport in porous media saturated with pure water to the knowledge of the authors of this study such investigation has not been explored previously in literature 2 mathematical modelling 2 1 general transport equations the current study employs the mathematical model developed by katzourakis and chrysikopoulos 2021 which briefly assumes that nanoparticles can be found suspended in the aqueous phase with number concentration nk npk l3 or attached onto the matrix of the porous medium with number concentration n k npk ms where ms is the mass of the solid matrix k is the class number that indicates how many single nanoparticles each aggregate has and npk is the number of aggregates of class k classifying nanoparticles into k classes is necessary because during transport aggregates are formed with different transport characteristics the governing equation that describes the migration of nanoparticles in homogeneous water saturated porous media accounting for both aggregation and non equilibrium attachment onto the solid matrix can be written as katzourakis and chrysikopoulos 2021 2014 lee et al 2000 sabelfeld and kolodko 2002 1 n k t x t ρ b θ n k t x t d x k 2 n k t x x 2 u n k t x x f n k t x a n k t x where u l t is the average interstitial velocity dx k l2 t is the longitudinal hydrodynamic dispersion coefficient of the suspended nanoparticles that belong to class k ρb ms l3 is the bulk density of the solid matrix θ is the porosity of the porous medium fn k t x npk l3t is a general source configuration form of nanoparticles which belong to class k t t is time x l is the spatial coordinate in the longitudinal direction and an k t x npk l3t is the number concentration of nanoparticles attached onto the solid matrix n k npk ms is the sum of irreversibly n k i npk ms and of the reversibly n k r npk ms attached particles the corresponding accumulation term in eq 1 can be expressed by a two site attachment model as 2 n k t n k r t n k i t the reversible attachment rate component can be expressed by a nonequilibrium equation as sim and chrysikopoulos 1998 1999 3 ρ b θ n k r t r n k n k r n k r n k r n k ρ b θ n k r where r n k n k r 1 t is the rate coefficient of reversible nanoparticle attachment onto the solid matrix and r n k r n k 1 t is the rate coefficient of reversible nanoparticle detachment from the solid matrix similarly the irreversible attachment rate component in eq 2 can be written as compère et al 2001 katzourakis and chrysikopoulos 2014 4 ρ b θ n k i t r n k n k i n k where r n k n k i 1 t is the rate coefficient of irreversible nanoparticle attachment onto the solid matrix furthermore the nanoparticle aggregation term found on the right hand side of eq 1 which might act either as source or sink depending the class k can be modelled after the smoluchowski population balance equation pbe smoluchowski 1916 5 a n k d n k dt 1 2 i 1 k 1 b i k i n i n k i n k i 1 b k i n i where the term bi k represents the collision rate between particles that belong to classes i and k assuming laminar flow one of the common collision kernels bi k for diffusion limited aggregation dla processes which accounts for collisions resulting from brownian diffusion while ignoring negligible contributions from fluid shear and sedimentation li and prigiobbe 2020 petosa et al 2010 taghavy et al 2015 is axford 1997 von smoluchowski 1917 6 b i k dla 2 k b t 3 μ w r i r k 2 r i r k where t k is temperature k b m l2 t2 t is the boltzmann constant μ w m t l is the dynamic viscosity of water and rk l is the radius of a nanoparticle that belongs to class k the nanoparticles source term found on the right hand side of eq 1 can be written as sim and chrysikopoulos 1999 7 f n k t x g k t w x where w x 1 l refers to a point source geometry 8 w x δ x x 0 where δ x x0 1 l is the dirac delta function x0 l is the cartesian x coordinate of the source center the gk t npk l2t term found in eq 7 is the particle release function which for an instantaneous source is given by 9 g k t n i n j k a c θ δ t where ninj k npk is the injected number of particles which belong to class k and ac l2 is the cross sectional area of the porous medium 2 2 filtration theory the effect of particle size change onto the forward attachment rate can be approached with the use of the well established filtration theory ft the attachment rate term found on the right hand side of eq 3 can be expressed as sim and chrysikopoulos 1995 10 r n k n k r u φ f n k where f n k is the dynamic blocking function which accounts for porosity variations when particle attachment increases however for submicron particles such as nanoparticles it can be assumed that the porous medium is clean and thusly f n k 1 the φ 1 l parameter is the filter coefficient which can be expressed as rajagopalan and tien 1976 11 φ 3 1 θ 2 d c η where dc l is the collector average diameter and η is the single collector removal efficiency yao et al 1971 12 η α η o where α is the collision efficiency and ηo is the single collector contact efficiency which can be estimated with the correlation developed by tufenkji and elimelech 2004 2 3 aggregate structure the coalesced sphere assumption dictates that when two spherical particles collide they form a new spherical aggregate with mass equal to the sum of the masses of the two initial particles while the same is true for their volumes therefore the density of the aggregate is maintained constant however in reality the newly formed aggregates contain void spaces the relationship between the initial monomer dp 1 l and the diameter of the final aggregate that belongs to class k dp k l can be written as feder 1988 lee et al 2000 13 n p k ζ d p k d p 1 d f where np k npk is the number of particles present in an aggregate that belong to class k df is the fractal dimension of an aggregate and depends on the type of aggregation ζ is the packing factor which accounts for the void pore space within the spherical aggregate and depends on the shape of both monomers and aggregates 2 4 initial and boundary equations to solve the system of differential eqs 1 12 it is essential to present the appropriate initial and the boundary conditions for a one dimensional confined aquifer 14 n k 0 x 0 15 n k t 0 n k 0 t t p 0 t t p 16 n k t 0 0 17 n k 2 t l x d x 2 0 where lx l is the length of the porous medium and tp t is the source release period over which nanoparticles enter the porous medium it is noted that eqs 14 17 refer to two possible source configurations for the first one nanoparticles are introduced into the aquifer through a broad pulse source located at the inlet of the aquifer with source concentration n k 0 npk and pulse duration tp t see eq 15 while for the second one nanoparticles are instantaneously injected at a specified location within the aquifer and only clean water enters the aquifer see eqs 9 16 the initial condition eq 14 clarifies that initially there are no nanoparticles inside the porous medium the boundary condition eq 17 establishes that at the downstream end of the finite aquifer a concentration slope continuity is preserved shamir and harleman 1967 finally eqs 14 17 are applied k times once for each class 3 methods solving the nanoparticle transport model eqs 1 17 is not an easy task because several physical processes are involved advection dispersion aggregation etc forming a family of coupled partial differential equations applying direct conventional numerical approaches would require enormous amount of memory by introducing large matrices directly correlated to the total number of classes kmax instead the physical processes were decoupled through symmetrically weighted sequential splitting operator sws methods barry et al 2000 kanney et al 2003 steefel and macquarrie 1996 wood and baptista 1993 and subsequently each process was solved individually furthermore to restrict the total relative error an adaptive time step scheme was adopted the selection of the correct number of classes k is of great importance setting kmax too low fails to properly account for the formation of larger nanoparticles however setting it too high exponentially increases the computation costs here kmax was selected so that the maximum relative error on the non negligible concentrations between the different models runs is lower than 2 additional details of the solution procedure can be found in the supplementary information si section in this study a one dimensional confined porous medium with length lx 0 6 m was considered nanoparticles can enter the aquifer either through the inlet at x0 0 m over the duration of a broad pulse tp t or instantaneously at a preselected point x0 0 1 m in the first case while t tp the nanoparticle concentration n k 0 is constant and after t tp only clean water enters the aquifer in the second case there is an instantaneous mass injection while clean water continuously enters the aquifer from the inlet in order to highlight the impact of initial concentration on the nanoparticle transport two different simulations were performed with different initial concentrations for both source configurations furthermore to emphasize the impact that the initial particle diameter distribution has on the nanoparticle transport two simulations were performed in which equal amount of mass was instantaneously injected note that in the first case the nanoparticles injected have different sizes and in the second case the nanoparticles injected are of the same size six distinct simulations were conducted four to investigate the effect of the initial source concentration and two to explore whether the source distribution affects nanoparticle transport the mathematical model was incorporated in a fortran code which was used for all the numerical simulations conducted in this study the fortran code was compiled by intel fortran compiler classic on a conventional pc system build upon an amd ryzen 5 3600 cpu and paired with fast 16 gb of ddr4 3200 mhz memory due to the extensive parallel programming that was implemented through the use of the multithreading openmp protocol the cpu utilization during simulations averaged at 90 simulation times were rather lengthy and varied depending on the extend of aggregation and the number of classes used kmax higher rates of aggregation required finer time steps resulting in increased simulation times the total run time for cases of lower aggregation rates e g lower initial concentration was trun 256 minutes and for higher rates was trun 1361 minutes as previously stated the forward attachment rate of nanoparticles can be modeled after the ft this means that the r n k n k r parameter can be calculated for any class k as a function of the particle diameter with eqs 10 12 the relationship between r n k n k r rate and dp k diameter is illustrated in fig 1 note that the r n k n k r decreases monotically till dp 1 800 nm and then monotically increases thusly it is expected that particles of size dp 1 800 nm experience reduction in the attachment rate with increasing particle diameter while particles of size dp 1 850 nm display an increasing attachment rate with increasing particle diameter in this study two particle diameters were selected for simulations dp 1 39 9 and 850 nm to highlight each one of the two cases a further increase in particle size due to aggregation causes decrease in attachment rate and b further increase in particle size increases attachment rate for the calculation of r n k n k r parameter in fig 1 the following parameters were used collision efficiency α 0 009 syngouna and chrysikopoulos 2012 collector grain diameter dc 6 10 4 m and interstitial velocity u 0 3 m hr in order to highlight the effects of initial concentration on the nanoparticle transport two simulations were performed using an instantaneous source for two different initial injection concentrations ninj 1 1 109 np1 and ninj 1 1 1010 np1 the nanoparticle model described by eqs 1 14 16 17 and the model developed by katzourakis and chrysikopoulos 2015 subsequently this biocolloid transport model will be referred to as kc model were applied to particles with dp 1 39 9 nm which are injected at x0 0 10 m of the 1 d aquifer it should be noted here that the kc model can simulate the transport of colloids in a water saturated homogeneous porous media without considering particle aggregation both models kc and current model take into account the same physical process but the kc model ignores completely particle aggregation for the nanoparticle transport model the forward reversible attachment rate for k 1 was set to r n 1 n 1 r 0 331 1 hr see fig 1 while for the kc model the forward reversible attachment rate was also set to r n n r 0 331 1 hr the rest of the required model parameters are listed in table 1 the total number of suspended nanoparticles of class k 1 divided by ninj 1 injection number of nanoparticles introduced instantaneously n 1 t n i n j 1 is shown in figs 2 a b at three different locations within the 1 d aquifer x 0 25 0 4 and 0 6 m it should be noted that n 1 t np1 l3 is the total number concentration of suspended nanoparticles or equivalently the sum of nanoparticles initially present in class k 1 which at subsequent times contribute to formation of aggregates in various classes similarly and for the same locations the respective dimensionless average size of suspended aggregates d p d p 1 are shown in figs 2c d the nanoparticle model eqs 1 6 10 15 17 which accounts for a source with duration of tp 28 hr was applied to the 1 d aquifer for two different source concentrations n 1 0 3 1014 np1 m3 and n 1 0 8 1014 np1 m3 assuming that nanoparticles with diameter dp 1 850 nm enter the aquifer at x 0 m the forward reversible attachment rate for k 1 was set to r n 1 n 1 r 0 042 1 hr all other required model parameter values are listed in table 1 the kc model was also applied for the same source configuration with attachment rate independent of aggregate size r n n r 0 042 1 hr 4 simulation results and discussion 4 1 instantaneous source the results of fig 2 indicate that the concentrations n 1 t n i n j 1 decrease with increasing time and distance from the source as expected furthermore concentrations produced by the present model were higher than those produced by the kc model see figs 2a b this discrepancy is attributed to the fact that as aggregates are formed the average particle size increases and thusly the average attachment rate decreases see fig 1 for particle size dp 1 39 9 nm the kc model employs a constant attachment rate the differences between the current model and kc model are pronounced even further with the increase of the initial injection concentration see shaded areas in fig 2a b this difference can be attributed to the nature of the pbe eq 5 which establises that the rate of collisions between particles increases exponentially with increasing class concentration to summarize increasing the initial injected concentration increases the aggregation process which in turn decreases the average particle attachment rate and creates higher n 1 t n i n j 1 peaks additionally this observation is also supported by the values of d p d p 1 ratios seen in figs 2c d the average particle size which reflects the extend of the aggregation process becomes greater as time passes and as the distance from the source increases following the shape of the n 1 t n i n j 1 curve however it becomes much greater when the initial particle injection concentration increases see figs 2c d indicating that the aggregation process is sensitive to the source configuration figs 2c d suggest that after the d p d p 1 curves reach a peak they start to descend till they obtain an almost constant value for the rest of the simulation time as nanoparticles migrate downstream they attach onto the solid matrix of the porous medium with varying rates the smaller nanoparticles attach at higher rates compared to the larger ones see fig 1 for dp 39 9 nm after the main plume of nanoparticles exits the aquifer detaching nanoparticles have on the average smaller diameter than the respective suspended ones which keep aggregating as they migrate downstream consequently the average size of nanoparticles decreases forcing the d p d p 1 curves to acquire a negative slope of course at large times the suspended concentration becomes very small the aggregation process effectively stops and the ratio d p d p 1 is controlled by the diameter of the detaching aggregates which is mostly constant and reflects the time average of all particles that migrated through the aquifer in order to investigate the effects that a possible existing initial particle diameter distribution might have on nanoparticle transport two simulations were performed for the case of an instantaneous source for two different initial source configurations in the first simulation the diameters of nanoparticles injected followed a normal distribution with average diameter d p i n j 39 9 nm and standard deviation σ d p i n j 10 nm while in the second simulation the injected nanoparticles were all of the same size dp 1 39 9 nm for both source configurations the same nanoparticle model described by eqs 1 14 16 17 was used and the same amount of nanoparticle mass was injected into the aquifer the exact particle number ninj k for each class k injected in the aquifer for the first simulation following the normal distribution can be seen in fig 3 while for the second simulation ninj 1 3 13 108 np1 the mass based concentration of nanoparticles cm mn l3 with mn referring to the total mass of all suspended particles divided by the minj mass of nanoparticles introduced instantaneously is shown in fig 4 a at three different locations within the 1 d aquifer x 0 25 0 4 and 0 6 m the respective dimensionless average size of suspended aggregates d p d p 1 are shown at the same locations in fig 4b the cm minj ratios for the first source loading the one with the diameter distribution exhibit higher peaks than the ones with fixed initial diameter the reason for this is that despite both source configurations having the same amount of mass injected and the same average diameter dp 1 39 9 nm the total number of all distinct particles of all classes for the first source is n inj k t 1 6 109 npk which is larger than the total number of all distinct particles for the second source which is ninj 1 3 13 108 np1 the increased injected number of nanoparticles causes increased aggregation rate which in turn decreases the average particle attachment rate see fig 1 and creates higher cm minj peaks furthermore the difference in particle numbers between the two sources is caused by the presence of voids in the aggregate structures the first source consists of a size distribution with particles of various sizes monomers dimmers trimmers etc which may contain voids in their structure however the second source consists only of particles of a single size monomers which are dense the existence of voids means that for the same volume less nanoparticle mass is required and results to an increased distinct particle number the d p d p 1 ratios as shown in in fig 4b exhibit larger values in the presence of an initial nanoparticle size distribution than when injected particles are all of the same size this observation clearly supports the already established argument that the aggregation process is more pronounced when injected particles possess a size distribution finally it is noted that both source configurations considered here refer to monodisperse particles therefore the injected nanoparticles even if they have already formed aggregates at the time of injection can be decomposed to particles of the same size called monomers 4 2 broadpulse source the total number of suspended nanoparticles of class k 1 n 1 t n 1 0 is shown in figs 5 a b c at three different locations within the 1 d aquifer x 0 2 0 35 and 0 6 m and in figs 6 a b c at three different times t 3 28 and 32 hr the results from fig 5 indicate that the simulated breakthrough curves reach peak concentrations slower and exhibit more pronounced tailing than the kc model see figs 5a b c similarly it is evident from fig 6 that suspended nanoparticle concentrations simulated by the kc model expand faster downstream and exhibit higher concentration levels than the respective nanoparticle models see figs 6a b while after the end of the broad pulse tp 28 hr the breakthrough curves simulated by the kc model exit the aquifer faster see fig c these discrepancies between the two models are caused by the increased attachment rate the current nanoparticle model accounts for due to aggregation while the kc model ignores aggregation as seen in fig 1 for dp 850 nm further increase in particle size increases the attachment rate furthermore the differences between the current model and kc model are even more pronounced when the source concentration n 1 0 is increased the dimensionless average size of the suspended aggregates ratios d p d p 1 are presend in figs 5d e f at three different locations within the 1 d aquifer x 0 2 0 35 and 0 6 m and in figs 6d e f at three different times t 3 28 and 32 hr as expected the d p d p 1 ratio in fig 5 follows the respective concentration curves n 1 t see fig 5a b c and increases markedly up to a 8 fold however at the end of tp 28 hr the d p d p 1 ratios instead of decreasing as the respective concentration curves do they increase this behavior is explained by recognizing that during the simulation larger particles attach with higher rates onto the aquifer see fig 1 for dp 850 nm and that at the end of broad pulse injection at tp 28 hr they detach and produce steep positive d p d p 1 slopes this process is more pronounced exactly after t tp causing the formation of a step in d p d p 1 slopes at which point maximum detachment rate is achieved as indicated by the kinetic eq 3 max detachment rate is attained when the suspended concentration is zero and the attached concentration has reached its max value both conditions are satisfied at the end of broad pulse injection tp 28 hr and subsequently only clean water enters the aquifer furthermore the increase in d p d p 1 with distance along the 1 d aquifer seen in figs 5d e f as well as in figs 6d e f is attributed to the fact that as the nanoparticles move downstream they aggregate and consequently increase in size finally the increase in source concentration n 1 0 increases the collision rate see eq 5 promotes aggregation and increases all d p d p 1 ratios as seen in figs 5d e f and 6d e f in order to investigate the effect that kmax has on nanoparticle transport several simulations were performed for various k values which are presented in fig 7 the total number of suspended nanoparticles of class k 1 n 1 t n 1 0 is shown in figs 7a b as a function of time for two different initial number concentrations n 1 0 8 1014 and 3 1014 np1 m3 at x 0 6 m the absolute relative average error rae for the respective initial number concentrations is shown in fig 7c the results suggest that as the kmax parameter increases the breakthrough curves become smaller and exhibit increased tailing which is a consequence of increased attachment therefore as the k increases the extend of aggregation that can be accounted for also increases see fig 1 for dp 850 nm the rae can be calculated as follows 18 rae i 1 m x i k max x i 200 x i 200 m where m is the number of observations that participate in the calculation of the error and x i k max np1 l3 is the concentration of observation i calculated with the use of kmax classes it should be noted here that a value of kmax 200 classes was used as point of reference for calculating the rae it is clearly shown in fig 7c that increasing kmax causes the rae to decrease fast at early times but slowly at late times note that the y axis is on a log scale the slopes of the two rae lines suggest that if the initial concentration n 1 0 is increased the same error can be achieved by increasing the number of classes kmax 4 3 general observations from simulation results the discrepancies between models accounting for aggregation and those that do not varied spatially and temporally and were more pronounced as the initial injected concentration was increased furthermore all the graphs of the ratio d p d p 1 see figs 2c d 4b 5d e f and 6d e f which reflect the extend of aggregation process indicated that aggregation progressed became greater with increasing time and distance from the source additionally d p d p 1 ratios in conjunction with the filtration theory can be used to characterize the average attachment rate of nanoparticles onto the solid matrix of the porous medium it is evident that the attachment rate is not constant but varies spatially and temporally depending on the initial particle size the attachment rate increases or decreases in a practically linear fashion with increasing distance from the source see figs 6d e f certainly the impact of aggregation on nanoparticle transport cannot be discarded as negligible consequently proper modeling of aggregation is recommended 4 4 comparison with similar transport models babakhani 2019 developed a nanoparticle transport model that considers particle aggregation and evaluates size exclusion and concluded that accounting for particle aggregation the predictive ability of the model is improved similarly li and prigiobbe 2021 2020 developed a mechanistic model to describe the transport of nanaoparticles in the presence of foam the model takes into account nanoparticle aggeregation and particle attachment detachment onto the solid matrix it was shown that foam can be used to enchance nanoparticle delivery even in a low permeability medium within a shallow subsurface however both models mentioned above do not clearly describe how the transport and aggregation equations are coupled a governing mass transport equation that incorporates aggregation was not explicitly provided futhermore the first model overlooked the importance of particle attachment onto the solid matrix and the latter one considered particle hydrodynamic dispersion as negligible compared to particle advection particle dispersion might be negligible in cases where foam 90 vol gas is employed but certainly it can not be ignored in cases of low velocity laminar flow commonly occurring in water satured aquifers in this study the two site attachment model was adopted compère et al 2001 katzourakis and chrysikopoulos 2014 which has been proven to work very well with colloid biocolloid nanoparticle transport and cotransport experiments chrysikopoulos and katzourakis 2015 katzourakis and chrysikopoulos 2014 stefanarou et al 2023 the two site approach allows particles to be attached reveresibly or irreversibly onto the solid matrix of the porous medium if particles are attached reversibly then there is a chance depending on the suspended concentration to detach and transfer again into the aqueous phase however if particles are attached irreversibly then they remain attached till the end of the experiment contributing to the total column retention particles of the first type usually attach on the secondary minimum reversible attachment and of the second type on the primary minimum irreversible attachment tufenkji and elimelech 2005 finally in some cases katzourakis and chrysikopoulos 2014 the presence of irreversible particle attachment was absolutely necessary for the model to reduce extended tailing 5 summary and conclusions the conceptual model of katzourakis and chrysikopoulos 2021 was applied to simulate the transport of aggregating nanoparticles under various initial conditions in order to investigate the effects of initial particle concentration and particle diameter distribution on nanoparticle transport the results from several simulations suggested that nanoparticle aggregation can affect significantly the nanoparticle migration in porous media either hindering it or enhancing it depending on the initial particle diameter if further increase of the initial particle size causes decreasing attachment rates then aggregation enhances nanoparticle transport whereas if further increase leads to increasing attachment rates then aggregation hinders nanoparticle transport furthermore the increase of the source concentration significantly affects the aggregation rate additionally the existence of an initial particle diameter distribution can increase the aggregation rate and alter the transport characteristics of nanoparticles both observations are of great importance because they highlight the effects that the source configuration might have on particle transport and are often overlooked it should be noted that by changing the source configuration the shape of the resulting breakthrough curves might change considerably therefore ignoring nanoparticle aggregation or neglecting to take into account the actual source configuration might lead to erroneous results credit authorship contribution statement vasileios e katzourakis writing original draft methodology software constantinos v chrysikopoulos conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research has received funding from khalifa university grant award number fsu 2023 12 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2023 104475 appendix supplementary materials image application 1 
2,in august 2019 this journal published the paper water sorptivity of unsaturated fractured sandstone fractal modeling and neutron radiography experiment which reports on the experimental monitoring and analytical calculation of capillary moisture absorption in fractured sandstone this discussion considered a post publication review raises a number of concerns in relation to this paper these concern the observed deviation between the visual and numeric results of the moisture front penetrations the suboptimal character of the power law equation for the temporal evolutions of the moisture penetrations and the doubtful physical foundation for the pore fractal dimension and pore size ratio applied these concerns threaten the validity of the disputed paper s findings and conclusions and this discussion therefore invites its authors to refute these in their reply keywords critique bundle of tubes model capillary moisture absorption moisture front penetration fractal tortuosity data availability data will be made available on request 1 introduction in august 2019 this journal published the paper water sorptivity of unsaturated fractured sandstone fractal modeling and neutron radiography experiment zhao et al 2019 which reports on the experimental monitoring and analytical calculation of capillary moisture absorption in fractured sandstone for the experimental monitoring a neutron radiography set up is used for the analytical calculation a fractal imbibition model is developed the study visualises the capillary moisture absorption in the matrix via the bottom and fracture surface as well as in the fracture with neutron radiography the resulting temporal evolution of the moisture penetrations is then fitted with a base version of the analytical equation to attain the time exponents involved once these are found the sorptivities of matrix and fracture are predicted with the full analytical equation and the good agreement with their experimental counterparts is confirmed this discussion which is considered a post publication review however brings about a number of concerns with respect to zhao et al 2019 in section 2 it is firstly revealed that the values quantifying the moisture absorptions in the matrix reported in the paper s table 3 do mostly not correspond with the visual data illustrated in the paper s figures 7 8 section 3 secondly establishes that more proper fits of the temporal evolutions of the moisture penetrations are possible and argues that the fit for the fracture should have omitted the final four data points in section 4 thirdly it is reasoned that the values adopted for the sorptivity predictions are physically tenuous downgrading the paper s sorptivity prediction model to a humble calibratable formula this review is finally concluded with a discussion which formulates a number of concrete questions on the disputed paper and invites its authors to respond to these in their reply 2 moisture penetrations in matrix the paper s table 3 collects the temporal evolutions of the penetration distances of the moisture fronts originating from the bottom and fracture surfaces respectively lm1 lm2 and hm1 hm2 see the paper s figure 8a this table should hence be the quantification of the neutron radiography images shown in the paper s figures 7 8 the validity of the visual data in the paper s figures 7 8 is confirmed by the agreement of figure 7 s red lines for the fracture water level heights figure 7 s red values for these fracture water level heights and figure 10 s blue triangles for these fracture water level heights table 1 repeats the paper s table 3 s values at 0 4 and 0 8 s which should directly connect to the paper s figures 8a and 7 bottom right respectively table 1 shows that the penetrations via the fracture surface hm1 hm2 are about double as large as the penetrations via the bottom surface lm1 lm2 that conclusion is however contradicted by the visual information in the paper s figures 7 8 which suggest that the penetrations via bottom and fracture surfaces are very similar in extent as sample height and width are 35 mm and 41 mm respectively a ruler based evaluation allows quantifying that visual information which is equally collected in table 1 these values on the one hand highly differ from these gathered in the paper s table 3 and are on the other hand much more similar for bottom and fracture surfaces it is unclear how the paper s table 3 s values have been obtained but it is clear that these do not agree with the original visual data in the paper s figures 7 8 given that the paper s table 3 forms the basis for the further fits of the time exponents and predictions of the matrix sorptivities it cannot but be concluded that the derived results can neither be reliable given this divergence between the paper s table 3 and figures 7 8 3 time exponents of penetrations to predict the matrix sorptivities the time exponents of the moisture penetrations are necessary and these are obtained by fitting the paper s equation 13 to the measured moisture penetrations see the paper s figures 9 10 and tables 4 5 it should be noted that this constitutes a circularity as one needs to first measure the sorptivity in order to then predict the sorptivity that being said the typical time exponent for moisture accumulation and moisture penetration in capillary absorption experiments is 0 5 feng and janssen 2018 ren et al 2019 this typical value is however not obtained in the disputed paper as the fits lead to values ranging from 0 33 to 0 46 see the paper s figures 9 10 and tables 4 5 according to the paper the deviating exponents can be explained by fractal tortuosity see the paper s equation 7 the literature however contradicts that finding and that reason since there is ample proof for the standard linear with square root of time progression of capillary absorption in sandstone thomachot schneider et al 2008 al naddaf 2011 fu et al 2021 fig 1 confirms that such is also the case for the sandstone considered here the top graph reproduces the paper s power law fits see its equation 13 based on the values reported in its table 3 the center graph applies these same values but now fits a square root of time relation via the square root of time horizontal axis x hence equals t0 5 the obtained r² values are typically better except for hm but the difference is minimal for the square root of time fits than for the power law fits indicating that the latter are suboptimal in this respect it can however be noted that the fits for hm1 and hm2 are not perfect as it appears that the measured data sets do not cross the origin given the findings in section 1 above though there is severe distrust in the validity of these values the bottom graph hence fits the square root of time law to the corrected data see table 1 confirming that the moisture penetrations in the sandstone considered most plausibly behave standardly hence linear with square root of time while it could be countered that the fits in the bottom graph are established on two data points only their near perfect alignment with a square root of time expression without intercept as expected from the state of the art substantiates their plausibility this implies that the α values reported for the matrix moisture penetrations in the disputed paper are doubtful since it is far more likely that they are simply equal to 0 5 this moreover infers that also the sorptivities reported in the disputed paper are flawed since fig 1 bottom reveals sorptivities between 4 and 5 mm s0 5 instead of the earlier values ranging from 2 to 5 mm sα additionally it should be noted here that also the fit for the fracture is far from optimal the paper s figure 7 reveals that the moisture in the fracture reaches the top at around 0 65 s suggesting that the data points from 0 6 s onwards are already affected by this end of sample effect which is not considered in the analytical calculation the fit should have hence been restricted to the data points up to 0 5 s instead of using the full data set based on the values depicted in the paper s figure 7 a power law fit would yield time exponent 0 59 and sorptivity 52 2 mm s0 59 with r² 0 9769 whereas a square root of time fit would lead to time exponent 0 5 and sorptivity 51 5 mm s0 5 with r² 0 9826 also in this case thus the square root of time fit gives a more optimal fit what is more the power law fit results in a time exponent above 0 5 which is physically impossible 4 analytical prediction of sorptivity 4 1 introduction the paper s equations 13 and 15 describe the sorptivity relation adopted in the disputed paper 1 l sm σ cos θ 4 μ α d λ max 1 α 1 d α β d β 1 α t α with lsm m the moisture penetration σ n m surface tension θ contact angle μ pa s dynamic viscosity α time exponent d pore fractal dimension λmax m maximum pore size β ratio of smallest and largest pore size t s time this expression is based on a bundle of tubes model which simplifies the pore structure of the porous material to a set of independent parallel tubes in the disputed paper the tubes radii and tortuosities are governed by fractal concepts see the paper s equations 13 and 9 these concepts require values for the tortuosity fractal dimension dt pore fractal dimension d pore size ratio β and maximum pore size λmax dt is hidden in α as dt equals 1 2α where the latter is obtained as the time exponent fitted to the moisture penetration d is valued via fractal box counting executed on the paper s figure 6b while β is simply quantified based on literature values λmax finally is calculated with the paper s equation 14 with the values assumed in the disputed paper it finds a nice agreement between experiment and calculation for the moisture absorption via the bottom surface while the agreement is less solid for the moisture absorption via the fracture surface see its table 7 the latter deviation is explained in the disputed paper by microcracking near the fracture surfaces it should be stated from the start that such success of the bundle of tubes method is surprising given the current insights on the reliability of such models for moisture transfer in porous media such approximation of porous media s pore structures with bundle of tubes surrogates and their subsequent translation to moisture storage and transport properties has a long history already the early papers in this field stem from the 1950 s purcell 1949 burdine et al 1950 1953 that bundle of tubes approach however received criticism from the start as fatt 1956 avowed that the two models used in the past the sphere pack and the bundle of tubes have been too simple and as a result the equations derived from them have failed to predict observed properties agreement between theory and observation has been achieved for these models by inserting parameters of doubtful physical significance that invalidity of bundle of tubes models for moisture transfer in porous media has since been confirmed many times over for multiple material categories construction materials reservoir rocks soil strata fischer and celia 1999 scheffler and plagge 2010 hunt et al 2013 in recent decades hence the application of these overly simplified pore structure representations has been abandoned in favour of models using a more complex and complete topological and geometrical model for the pore space islahuddin and janssen 2019 among very many others the good agreement between experiment and calculation as reported in the disputed paper is however already degraded by the findings in sections 2 and 3 above fig 1 bottom reveals measured sorptivities of 4 to 5 mm s0 5 and exponents of 0 5 predictions with equation 1 using these corrected α values lead to 2 8 mm s0 5 hence strongly different already from their measured counterparts these predictions however still apply d equal to 1 84 and β equal to 0 01 values which are both queried in the two subsections below 4 2 fractal pore dimension in the disputed paper the fractal pore dimension d is determined via fractal box counting of the ct slice shown in the paper s figure 6b based on the scale indicated the largest pore feature observable in that ct slice is below 100 µm based on the voxel count versus that scale indicated the voxel size is about 1 25 µm implying that the smallest pore feature detectible in that ct slice is above 3 µm that infers that the fractal box counting only considers a part of the pore structure given that the paper s figure 4 presenting the mercury intrusion porosimetry results reveals pore sizes running from below 0 003 µm to over 300 µm certainly at the lower end a lot of pore features are not represented in the ct slice in the paper s figure 6b moreover it is impossible to account for tortuosity from a single two dimensional ct slice while that tortuosity plays a large role in the disputed paper the resulting d value is hence not necessarily representative for the actual pore structure of the sandstone following wang et al 2019 the pore fractal dimension can be derived from the mercury intrusion results as well fractal theory imposes this correlation between the number of pores with diameter larger than λ n λ and pore diameter λ 2 n λ λ d assuming a bundle of tubes model with fractal tortuosity the number of pores with diameter λ n λ m² per unit cross section of the sample can be calculated as 3 n λ 4 v λ l s π λ 2 τ λ l s 4 v λ π λ 2 τ λ 4 v λ l s 1 d t π λ 2 λ 1 d t with v λ m³ m³ the volume of mercury associated with pores with diameter λ per unit volume of material τ λ tortuosity of pores with diameter λ and ls m sample height this computation assumes a sample with cross section 1 m² and height ls this sample height ls thus also forms the net length of the pores stretching tortuously from bottom to top of the sample plotting pore number n λ and pore diameter λ in a log log graph then permits determining the pore fractal dimension d from the slope of the curve s as d s wang et al 2019 see fig 2 that figure represents the application of eq 3 to the paper s mercury intrusion data shown in its figure 4 two variants are assessed for the first α equals 0 41 the average time exponent obtained for the moisture penetrations in the matrix in the disputed paper while for the second α equals 0 5 as argued in section 3 which respectively translate to dt s of 1 22 and 1 00 fig 2 reveals that the resulting values for the pore fractal dimension d are respectively 1 55 and 1 77 both different from the 1 84 given in the disputed paper fig 2 moreover exposes that there is some turmoil in the two curves in the 10 to 100 µm range which brings further doubts on the reliability of the box counting approach applied in the disputed paper since the considered ct slice primarily comprises pore features in that spectrum fortunately the impact of these corrected d values on the predicted sorptivities is not overly big for d equal to 1 77 and α equal to 0 5 on the one hand the predictions reduce to 2 6 mm s0 5 for d equal to 1 55 and α equal to 0 41 on the other hand the predictions lower to 1 7 mm s0 41 instead of the 2 8 mm s0 5 obtained in section 4 1 4 3 pore size ratio fig 2 however also sheds light on the pore size ratio β in the disputed paper this is presumed 0 01 based on literature and not on physical information concerning the sandstone considered actually this value is used as a calibration parameter as the disputed paper states that when β equals 10 4 or 10 3 the predicted values of the developed model significantly underestimate the sorptivity of matrix when β 10 2 the matrix sorptivity predicted by the developed model is close to the experimental nonlinear fitting value such calibratable parameter reduces the paper s prediction model to a humble calibratable formula as no argumentation is provided on how to dependably and independently determine β this incidentally introduces another circularity in the prediction model see also the time exponent in section 3 as one again needs to first measure the sorptivity in order to then predict the sorptivity fig 2 however invalidates β being equal to 0 01 as it exposes that the pore features come in sizes ranging 5 orders of magnitude given the smallest and largest pore diameters are respectively below 0 003 µm and above 300 µm and unfortunately β does have critical impacts on the sorptivity predictions with β equal to 10 5 and α equal to 0 5 d equal to 1 77 the predicted sorptivities go down to 0 08 mm s0 5 hence roughly 50 to 60 times below the measured sorptivities of 4 to 5 mm s0 5 this outcome of course utterly invalidates the paper s matching measured and predicted values and thus nullifies the reliability of the sorptivity prediction model in zhao et al 2019 this of course does not come as a surprise given the state of the art with respect to bundle of tubes models for the quantification of moisture transfer in porous media see section 4 1 in this case β is fatt s 1956 parameter of doubtful physical significance inserted to match measurements and simulations 5 discussion the observations discussed above in sections 2 3 and 4 lead to a number of serious concerns on zhao et al 20 109 and this discussion therefore invites its authors to address these in their reply the following concrete questions may serve as guideline for such reply 1 can zhao et al 2019 explain the deviations observed between the visual findings in its figures 7 8 and the quantitative information in their table 3 see section 2 2 can zhao et al 2019 support why their power law fits contradicting the state of the art with respect to moisture absorption in sandstone should be preferred over the more optimal square root of time fits and more in line with the state of the art see section 3 3 can zhao et al 2019 defend their choices with respect to d and β in light of the physical information available for the sandstone see sections 4 2 4 3 4 can zhao et al 2019 reflect over their defining their equation 15 as a sorptivity prediction model given that determination of the parameters α and β in their actually requires performing the capillary absorption experiment 6 conclusions in august 2019 this journal published the paper water sorptivity of unsaturated fractured sandstone fractal modeling and neutron radiography experiment zhao et al 2019 which reports on the experimental monitoring and analytical calculation of capillary moisture absorption in fractured sandstone this discussion considered a post publication review however brings about a number of concerns in relation to zhao et al 2019 concerning 1 the divergence between visual and numeric results of the moisture front penetrations 2 the optimality of standard square root of time fits over the applied power law fits 3 the deviation between physical data and preferred d and β values 4 the contradiction that the prediction of sorptivity requires measurement of sorptivity these concerns threaten the validity of zhao et al 2019 findings and conclusions and the authors are therefore invited to refute these in their reply declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
3,reservoir management and contaminant transport simulations rely on accurate modeling of the subsurface this task becomes more challenging when the reservoir of interest also has a large amount of fractures inverse modeling of these fractured media involves first performing multiple field tests such as pumping tests tracer tests or seismic surveys inverse modeling methods then are used to find potential geologic models that produce simulated results that match field observations popular methods include the ensemble kalman filter markov chain monte carlo and simulated annealing a common challenge these methods face is that inverse modeling of fractured media is inherently a multiobjective optimization task the inverse modeling method must find a discrete fracture model that produces the same flow characteristics as observed in field pumping or tracer tests but it also must find a discrete fracture model with a fracture network that matches the fracture parameter distribution observed by the field measurements such as seismic surveys this challenge can be approached in two steps the first step is producing discrete fracture network models with parameter distributions that match field observations from seismic surveys this study focused on the second step involving the development of a method that can take a population of discrete fracture networks and generate new fracture networks in a way that preserves the fracture parameter distribution this was done using a genetic algorithm modified to apply to the domain of discrete fracture networks during genetic mixing the fractures of the child model are generated by randomly copying over fractures from the parent models this process ensures the child model adopts the fracture parameter distribution of the parent models this study tests the effectiveness of this genetic algorithm on a synthetic example the results of the experiment show the genetic algorithm is able to effectively produce a population of discrete fracture models with breakthrough curves that match the curves of the reference model keywords discrete fracture networks genetic algorithms stochastic inversion subsurface flow and transport data availability no data was used for the research described in the article 1 introduction fracture network models are important tools across multiple industries the most prominent use of fracture network models is in the petroleum industry where they are used for determining the optimal methods for fracturing and recovering petroleum from reservoirs another application for fracture network models is geothermal energy production to design effective methods for producing fractures that transfer thermal energy from the crust to the fluids flowing through the fracture network researchers rely on accurate fracture network models a wide variety of conceptual models exist for simulating flows through fractured rock concepts include modeling the rock is a discrete network of fractures another as nonuniform continuum of hydraulic parameters or even as a hybrid where the rock is modeled as a nonuniform continuum that contains a small number of dominant discrete features neuman 2005 such variety exists because of the different geologic contexts that produces a range of flow behavior this study focuses on the use discrete fracture network models to simulate flows through fractured rock this conceptual model would be suitable to simulating flows through impermeable crystalline rock unlike traditional groundwater flow simulations that discretize the domain as a grid of cells each with its hydraulic properties discrete fracture networks describe fractures as a set of finite planes with a defined shapes sizes locations and orientations the fracture parameters often are defined on the basis of a probability distribution that imitates distributions of fractures measured in the field during flow simulations the fluids strictly flow through these fracture planes with the flow behavior governed by the hydraulic properties of the fracture planes and how the fracture planes are connected note that for real fractured rock fluid exchange can exist between the fracture and the matrix for this work we assume that most of the fluid dynamics is governed by the geometry of fractures and how they are connected to each other the goal is to investigate how the complexities of geometry dominated flow affect the performance of a proposed method using genetic algorithms thanks to the flexibility of genetic algorithms the proposed method can be adjusted to handle scenarios where the fluid exchange between fractures and the rock becomes more significant even with a method for accurately simulating the flow of fluids through the fracture network researchers must still somehow convert field measurements into a set of discrete fracture network parameters such that the simulated flow model produces results that match observations from the field this task of inverse modeling is known as history matching and researchers have deployed a wide range of methods to accomplish it methods include ensemble kalman filters ping and zhang 2013 nejadi et al 2017 markov chain monte carlo optimization ma et al 2008 xu et al 2013 and simulated annealing tran 2007 mahmoodpour and masihi 2016 an important method for converting a set of well observations into a hydraulic model for fractured rock is hydraulic tomography illman et al 2009 illman 2014 zha et al 2015 klepikova et al 2020 ringel et al 2021 the process for hydraulic tomography begins by first acquiring data from pumping tests applied to a well field at the site of interest these tests must be designed in a way that properly extracts information about the hydraulic properties of the rock between the wells the tests usually involve selecting a well to pressurize then monitoring the pressure of adjacent wells through time to see how the pressure from the injection well propagates through the rock once data is collected a variety of inversion methods can be used to convert the collected data into hydraulic models for klepikova et al 2020 a misfit function was first defined to measure the differences between the simulated and observed the parameters of the model were then tuned using the nelder mead simplex algorithm mckinnon 1998 in order to find the optimal parameter values that minimizes the misfit function zha et al 2015 performed their inversion step using the simultaneous successive linear estimator algorithm xiang et al 2009 where a successive bayesian linear estimator derives the mean parameter fields using the results of field tests and assumes prior knowledge of the mean value and spatial structures of estimated parameters to perform inversion for a three dimensional discrete fracture network ringel et al 2021 used a reversible jump markov chain monte carlo method fan and sisson 2011 to calculate the likelihood a given change to a proposed model such as adding or removing a fracture yields a new model that is most likely to produce pressure transients that match with field tests note that many of these methods have the same underlying process of running multiple fluid flow simulations to find a group of model parameters that produce results that fit field observations often these simulations are large and computationally expensive so researchers use a variety of techniques and heuristics to efficiently find solutions while minimizing the required amount of computational resources another popular family of methods used by researchers includes evolutionary techniques which are optimization algorithms with heuristics that are inspired by biological processes in nature one of the most popular heuristics is the concept of natural selection possible solutions to an optimization problem are represented as individuals in a population the fitness of these individuals is determined by how well they solve the optimization problem individuals that perform poorly are assigned a low fitness value and removed from the population but individuals that perform well are assigned a high fitness value and are allowed to stay in the population after the weakest individuals have been removed from the population the remaining individuals then are allowed to reproduce and create a new generation of possible solutions during the reproduction process the child solutions adopt characteristics from each of the parent solutions the idea is that the children will adopt the beneficial characteristics of their parents ideally producing a new generation of solutions that will perform better than the previous generation evolutionary methods such as differential evolution das and suganthan 2010 pant et al 2020 and genetic algorithms mitchell 1998 sivanandam and deepa 2008 rely on this main heuristic of natural selection researchers then can use these techniques to find the optimal set of fracture network parameters that fits simulation results to field observations cadini et al 2013 liu and reynolds 2019 zhang et al 2019 maucec et al 2020 although it is important to be able to generate a set of possible fracture network parameters that produce simulation results that fit with field observations it is just as important that the distribution of the fracture parameters also match what is observed in the rock to effectively produce a set of fractures that imitate the parameter distributions in real life stochastic fracture network generation methods can be used j p 2005 bonneau et al 2013 these methods often deploy simple generating heuristics inspired by the rock mechanics involved with fracture formation and propagation these simple rules can generate fractures with parameters such as fracture shape size orientation or spatial density with the same distributions as observed in the field using techniques like well logging and micro seismics willis et al 2006 han et al 2021 kennedy et al 2022 because fracture networks involve complexity at multiple scales fractal dimensions are another popular method used to ensure that generated fracture models imitate distributions found in the field liu et al 2015 zhang et al 2019 overall researchers have many methods available for them to produce realistic and accurate distributions of fractures though seismic surveys can be used to generate a discrete fracture network sicking and malin 2019 this does not necessarily mean the generated model will produce simulated well test results that will match with all field observations such as well pressure tests instead it is best to create discrete fracture network models using results from both seismic surveys and well pressure tests mayerhofer et al 2006 for example seismic survey data can used to define the number of fractures and the density of fractures for a given study volume hydraulic tomography can then use well pressure tests to determine remaining model parameters such fracture connectivity orientations and size once discrete fracture networks can be generated to have some of the same parameter distributions measured from seismic surveys it can be challenging to also modify the generate models such that they also match the flow characteristics measured from pumping and tracer tests performed in the field when naively changing the parameters of the fracture network to better fit with measured flow characteristics this often can cause the distribution of fracture parameters to deviate from distributions measured in the field to deal with this need for a fracture model with both accurate fracture distributions and accurate flow characteristics researchers use optimization techniques that can handle multiobjective functions maucec et al 2020 instead of using techniques that iteratively generate a population of fracture models that converge toward both accurate distributions and flow characteristics at the same time this work proposes a different method that can meet this multiobjective goal in two distinct and straightforward steps the first step involves establishing a method for readily generating a set of fracture models with fracture parameter distributions that match field observations this step can be done by simply using any of the popular methods in the literature such as stochastic generation or using fractal dimensions the second critical step is to use the population of generated fracture networks to create new fracture models that are more likely to match flow characteristics but in a way that preserves the fracture distributions of the previously generated fracture network models this is done by creating a genetic algorithm optimization architecture that is designed to preserve the fracture parameter distribution of the population with each generation this work includes a synthetic experiment to investigate the effectiveness of this technique 2 methodology 2 1 discrete fracture network to model the discrete fracture networks the software package dfnworks by the los alamos national laboratory was used hyman et al 2015 dfnworks is composed of multiple software packages combined to form a seamless software suite for generating three dimensional models of discrete fracture networks as well as simulating flow and transport of particles through these fracture networks to generate a discrete fracture network dfnworks first uses a feature rejection algorithm for meshing fram method to generate the underlying geometric model for the discrete fracture network at this stage fractures are represented as flat polygons of various sizes and are distributed across a three dimensional volume each of the polygons is assigned additional attributes such as aperture size to generate a fracture network the distribution of the fracture network parameters are first defined once defined the distributions then are sampled and the fracture network is iteratively built one fracture at a time after each fracture is added the fram method checks if a series of constraints are met constraints such as proximity to other fractures or proximity to the model domain boundary are considered if the added fracture fails to meet a criterion then the fracture is rejected and a new one is created otherwise the fracture is kept within the fracture model and the process continues when the growing fracture model achieves a certain stopping criterion such as a set number of fractures the iteration loop halts and the geometric fracture model is presented as the final model ready for the next step of the process after the geometric fracture model is created the lagrit meshing tool converts the geometric model into a mesh model the meshing process produces a delaunay triangulation mesh that is ideal for parallel computations during the fracture generation phase the user has the freedom to define a probability distribution from which the software can sample to generate the fractures for fracture radius dfnworks provides options for a log normal distribution an exponential distribution a truncated power law distribution or simply a constant for this study the truncated power law distribution was used 1 p r α r 0 r r 0 1 α 1 r u r 0 α in this equation the probability density function p r is parameterized with the maximum fracture radius r u the minimum fracture radius r 0 and an exponent α that defines the shape of the power law distribution the truncated power law distribution was used because its additional parameters allow greater control on the shape of the distribution that controls the fracture radius di federico and neuman 1997 di federico et al 1999 neuman 2008 but the proposed method does not rely on a specific distribution to function so the approach will still work when an alternate fracture distribution function is used for the fracture orientation the orientation vector normal to the fracture plane must be sampled by essentially using a gaussian distribution that is embedded onto the surface of a three dimensional sphere this was done by using the three dimensional von mises fisher distribution 2 f x μ κ κ e x p κ μ t x 4 π sinh κ in this equation the probability distribution function f x is parameterized by μ and κ μ is the vector for the mean orientation with t indicating that the vector is transposed κ is the parameter that represents the variance of the distribution around the mean orientation dfnworks samples this distribution by using the algorithm outlined by wood 1994 2 2 flow and transport simulation the dfnworks suite hyman et al 2015 also includes software for running flow and transport simulations using the discrete fracture network called dfnflow the software takes the meshed fracture network model and uses pflotran to compute the pressure field across the network pflotran lichtner et al 2015 is an open source code base that was written by core developers from the u s department of national laboratories such as the los alamos national laboratory sandia national laboratory lawrence berkeley national laboratory and oak ridge national laboratory as well as contributions from universities and research labs around the world pflotran is capable of massively parallel operations that can operate at multiple scales and physics pflotran also can solve differential equations for non isothermal multiphase flow reactive transport and geomechanics in porous media this study uses pflotran to solve for the single phase flow across the fractures during steady state pflotran does this by solving the three dimensional richards equation richards 1931 the mixed form of the richards equation proposed by celia et al 1990 yields robust numerical solutions and maintains mass balance for unsaturated flow problems wu et al 2021 3 t ϕ s η η q q w in this equation t is time ϕ is the porosity of the soil matrix s is the water saturation η is the molar water density q is the darcy flux and q w is a positive source or a negative sink of water the darcy flux q is calculated using darcy s law 4 q k k r μ p w w η g z where k is the intrinsic permeability k r is the relative permeability μ is the water viscosity p is the pressure w w is the formula weight of water g is the acceleration of gravity and z is the vertical component of the position vector in this study the van genuchten soil water retention curve van genuchten 1980 and the mualem relative permeability function mualem 1976 was used to calculate the water saturation and the relative permeability for this study the boundary conditions for the model will be a no flow boundary condition for all boundaries except for the injection and extraction well the two wells will be set to a boundary condition of a constant pressure the permeability of the fractures are derived as a function of the aperture of the fracture this is done by first assuming the fluid flows between two smooth impermeable parallel plates this assumption allows the boussinesq equation boussinesq 1868 to yield the volumetric flow rate q per unit fracture width normal to the direction flow hyman et al 2016 5 q b 3 ρ g 12 ν h in this equation b is the fracture aperture ρ is the fluid density g is gravitational acceleration and h is hydraulic head this relationship between fracture aperture and flow rate can be used to derive a relationship between aperture and transmissivity hyman et al 2016 6 t b 3 ρ g 12 ν this equation is referred to as the cubic law witherspoon et al 1980 pflotran uses this relationship to convert fracture aperture into permeability note that these equations assume that the flow through the fracture is homogeneous and that there is no fluid exchange between the fracture and the matrix such an assumption can be justified when modeling flow through impermeable crystalline rock for this study focus is on investigating whether the genetic algorithm is capable of tuning a model with its flow dynamics dominated by the geometry and connectivity of the fractures however the methods developed in this work can be extended to study sites where there is significant fluid exchange between the fracture and the matrix after the flow and pressure field is calculated this information is sent to the software called dfntrans for particle transport calculations during this step dfntrans adopts the lagrangian approach for calculating the path that a cluster of non reactive indivisible particles take through the discrete fracture model to calculate the path the particles take the fluid velocity field first is calculated using the pressure field result from pflotran as well as other flow attributes derived from the fracture network to calculate the velocity field pflotran was set to use a constant porosity of 25 since the goal of this work was to test whether the proposed method can tune a model with flows dominated by fracture geometry and connectivity porosity was simply set to a constant the proposed method also does not rely on a constant porosity so it can be extended to handle distributions where porosity does become a function of aperture after the velocity field is solved the particle paths can be calculated by numerically integrating the trajectory equation for each particle at the intersection of fractures it is assumed that complete mixing occurs this means when a traveling particle meets an intersection a flux weighted stochastic method is used to determine whether or not the particle stays within the fracture plane or would flow into the intersecting fracture plane because of the stochastic nature of flow through fracture intersections the general flow structure of the entire fracture network can be studied by using many tracer particles or with multiple runs of the transport simulation 2 3 genetic algorithm for fracture networks genetic algorithms refer to a class of optimization algorithms in which a family of optimal solutions is found by iteratively applying the process of natural selection in this work we show that genetic algorithms can be used to tune a dfnm where the genetic algorithm is tasked with finding the correct distribution of fractures such that it produces simulated tracer test results that match with observations the process begins with generating an initial population of potential solutions to an optimization problem each member of the population is ranked with a fitness function a fitness function is a function designed to convert the performance of single candidate solution into a single number this fitness function is used to rank members of the population candidate solutions by their ability to perform the task solutions that do a superior job of optimizing the given problem will receive a high fitness score while solutions that do a poor job solving the optimization problem are given a low fitness score below a certain fitness threshold solutions with a lower fitness score are removed from the population the remaining solutions then are used to generate a new generation of potential solutions this is done by selecting high fitness parents and mixing their genetic information to produce new children solutions with a greater chance of yielding a high performance score after repeating this process for multiple generations the population evolves into a family of high fitness solutions that effectively solve the optimization problem one important factor in a successful genetic algorithm is having a well designed method for encoding the solutions as a genetic code if done correctly the genetic mixing process can properly explore the solution space and quickly find the optimal solution within the context of discrete fracture networks this study proposes to represent each fracture as a single genetic base within the complete genetic code of a discrete fracture network model fig 1 for example a discrete fracture network with 300 fractures will have a genetic code that is 300 bases long to create new discrete fracture networks the genetic code of the child discrete fracture network will be composed of the genetic bases fractures randomly sampled from the genetic code of high fitness parent discrete fracture networks the key idea is that this method of genetic mixing preserves the distributions of the fracture parameters that means if the genetic algorithm process begins with a fracture network model population that was all generated with the same parameter distribution then after multiple generations of applying the genetic algorithm the final population will have the same fracture parameter distribution the main difference between the initial and final generation is that the final population of discrete fracture networks will be able to produce simulated flow behaviors that best match with field observations note that the genetic mixing process does not create new fractures after many iterations of the genetic algorithm the population becomes filled with copies of the same genetic code although the genetic algorithm process will converge quickly it will also have a high risk of converging prematurely and stopping at a suboptimal solution to prevent the genetic algorithm from halting at a locally optimum solution new fractures must be added to the population the new fractures must be added in a way that does not change the parameter distribution of the fractures this is done by adding newly generated fracture networks midway through the process these new fracture networks are generated using the same parameter distributions as were used to generate the initial population if these new fracture network models were simply added to the population then these models would be removed quickly from the population because of a low fitness value that could not compete with models that have already evolved to force the genetic influence of the newly added models the genetic mixing process allows the combination of genetic code between high fitness models and newly generated models by controlling the size and frequency the freshly generated models that are added to the population the user can increase the likelihood that the genetic algorithm would converge to the global optimum a complete description of the discrete fracture genetic algorithm used in this study begins with generating a population of discrete fracture network models for this study a population of 20 models was maintained each of the fracture network models was generated with the same parameter distribution for the fracture radius and fracture orientation a fitness value then was calculated for each member of the population for this study fitness was determined by how well a given fracture network could recreate the time versus concentration curves of a tracer transport test measured on a reference fracture model the individuals of the population then were ranked by their fitness value and a certain fraction of the lowest performing individuals was removed from the population to maintain the size of the population newly generated discrete fracture models then were added to the population some of these new models were generated using the same parameter distributions as the initial generation other new models were generated by randomly selecting two parent models then randomly copying the genetic code from each of the parent models to produce the genetic code for the child model the length of the genetic code of the child model the number of fractures in the child model was set randomly as a number between the number of fractures for each of the parent models for example for parent models with 180 and 200 fractures then the generated child model could have anywhere from 180 to 200 fractures newly generated models were added until the number of models was equal to the set population number as an example for a current generation of 20 models the next generation could contain ten copies of the best models from the previous generation six new models generated using genetic mixing and four models generated from sampling the same parameter distribution as used to generate the initial population the fitness of the new generation then was calculated and the entire process can be repeated multiple times to create many generations the entire loop then could be halted when the fitness of subsequent generations no longer improved a complete summary of the genetic algorithm is presented in algorithm 1 a flowchart of the process is also presented on fig 4 2 4 synthetic fracture model experiment for this study the discrete fracture genetic algorithm was tested on a synthetic fracture model in which tracer particles were injected simulated to flow through the fracture network and extracted through two wells fig 2 the domain of the fracture model was in the form of a cube with a side length of 15 m all the sides of the domain were set to a no flow boundary condition there was also no water exchange between the fractures and the matrix water only flows through the fractures water was only allowed to enter and exit the model through two wells that intersect the model domain the two wells were vertically oriented centered along the diagonal of the model and spaced 14 meters apart to create flow the wells were set with a constant pressure difference of 4 0 1 0 6 pascals to generate the discrete fracture network within the model domain the dfngen function which uses fram and lagrit of dfnworks was used in the geometric model fractures were modeled as two dimensional octagons with their radius and orientation determined by sampling defined parameter distributions the fracture radius was sampled from a truncated power law distribution using a maximum fracture radius of r u 5 0 meters a minimum fracture radius of r 0 1 0 meters and an exponent of α 2 6 that defines the shape of the power law distribution the fracture orientation was sampled using the three dimensional von mises fisher distribution with the mean orientation vector μ set to a vertically oriented normal vector and the orientation variance set to κ 1 0 the aperture of the fractures was set to a constant width of 1 0 1 0 5 meters note that a constant fracture aperture is a valid simplification for this study this work aims to test how a genetic algorithm can handle discrete fracture network models where the flow is mainly governed by how the fractures are connected to each other to focus on this mechanism all fractures were given a constant width the proposed method can handle variations in fracture aperture but this simplification is valid for initial tests of the proposed method dfngen was instructed to randomly insert 2100 fractures into the model domain this number of fractures was determined by the computational time and resources available for this work for the given model dimension and constraints this was the minimum number of fractures needed to study how the genetic algorithm would perform when tuning a discrete fracture network model the proposed technique can be scaled to handle models with a larger number of fractures this number of fractures was also chosen to ensure that any randomly generated fracture model would be likely to hydraulically connect the two wells to run the particle transport simulation the flow simulation was run first until it reached steady state afterward tracer particles were added to the model through the injection well fig 3 ten tracer particles were added to every point where a fracture intersected the wells dfntrans then calculated the trajectory of the particles as the transport simulation progressed dfntrans recorded the time it took for each particle to reach the extraction well the transport simulation ended when all particles reached the extracting well the recorded arrival times then could be used to create the breakthrough curves for the given model note that the breakthrough curve derived using arrival times of simulated particles can be used as a proxy for the cumulative molar amount of tracer recovered at the extraction well recorded over time for example recovering 25 out of 50 simulated tracer particles collected over the span of one simulated week would be the same as recovering 0 5 moles out 1 0 moles of an injected tracer compound collected over a span of a week to select the reference model one of the randomly generated fracture models was chosen as the reference model the goal of the genetic algorithm is to look for a discrete fracture model that produces the same breakthrough curves as the reference model to do this the genetic algorithm needs a fitness function that can rank how well each of the breakthrough curves matches with the breakthrough curves of the reference model this is done by first calculating the quantiles of the arrival times of the tracer particles for this study 11 quantiles were calculated 0 10 20 and so on up to 100 this converts the breakthrough curve into an 11 dimensional vector to calculate the fitness function the l2 distance is calculated between the quantile vector of the tested model and the reference model this means taking the difference between the quantile vectors for the two models summing the squares of all the elements of the new difference vector and finally taking the square root of this final sum note that this metric is an error value models that yield a low value are the best fit with the reference model and so are most likely to be kept in the population conversely models with a high value poorly match the reference model so they are most likely to be removed from the population after the fitness of each model is evaluated half of the population s worst performing models are removed from the population newly generated models then are added until the number of models in the population is the same as the starting population for this study the population was initialized with 20 models with each new generation ten models are copies of the previous generation s best models nine are newly generated from genetic mixing of models from the previous generation and one model is generated using the same parameter distribution as was used to create the initial population during model generation heavy emphasis was placed on genetic mixing because this ensures the genetic algorithm can converge quickly toward optimal solutions the genetic evolution process was repeated until no further improvement was observed from subsequent generations in this study the process continued for 40 generations for this study it took 11 days to complete the entire experiment with dfnworks running single threaded on an amd ryzen 3900x processor the runtime for this method can be significantly reduced by running multiple parallel processes when calculating the breakthrough curves for each of the candidate solutions fig 4 2 since genetic algorithms are a method that can easily take advantage of parallel processing this method can computational scale as well as other methods 3 results during the genetic algorithm loop the performance of the models within each generation was recorded fig 5 plots the distribution of the model performances within each generation of 20 models the graph includes the 10 50 and 90 quantiles of the distribution the model error value is the value from the fitness functions recall that this fitness function is based on the differences in the tracer arrival time quantiles between the tested reference model the graph shows that starting with a median model error of 3 5 successive iterations of the genetic algorithm led to a population of models with a median model error of 0 75 the population reached this value by 15 generations beyond 15 generations the population performance did not improve but instead remained at this performance level fig 6 shows that the variance of a population s model performances also evolved throughout the iterations of genetic evolution at the start the initial generated models had a model error variance of 0 4 then during the genetic algorithm process each iteration yielded a very different variance value although the variance of the model error fluctuated widely from generation to generation overall the population s variance trended downward the downward trend stopped at generation 15 the same generation that the median model error reached its steady state value after 15 generations the variance no longer decreased but instead stayed at a value of 0 09 the volatility of the model error variance also decreased substantially beyond 15 generations during the experiment the breakthrough curves for each of the models for each generation were recorded fig 7 shows the breakthrough curves recorded for the first generation the final generation and the reference model the breakthrough curves are jagged in appearance because of the relatively small number of tracer particles used such simulated breakthrough curves are expected to be more smooth with the use of more tracer particles note that for the initial population of generated models the majority of the models had an early breakthrough curve with the median of the curves having their 50 point at 2 4 1 0 3 years to test the genetic algorithm s ability to generate models with behaviors outside of the initial generated distribution note that the selected reference model has a breakthrough curve with a 50 point at 8 4 1 0 3 years after 40 generations of applying the genetic algorithm the final population of models successfully shifted right toward the breakthrough curve of the reference model after 40 generations the median of the curves had their 50 point occur at 8 5 1 0 3 years which is essentially the same as the breakthrough curve for the reference model although the final curves fit well at the 50 point the slope of the final breakthrough curves did not match well with the curves of the reference model at the particle recovery amount of 10 the reference model s breakthrough curve reached this point at 2 8 1 0 3 years yet after 40 generations the median of breakthrough curves reached this point at 4 0 1 0 3 years for the tracer recovery percentage of 90 the reference model s breakthrough curve reached this percentage at 3 7 1 0 2 years yet after 40 generations the median of breakthrough curves reached this percentage at 2 4 1 0 2 years 4 discussion overall the results show that the genetic algorithm was able to successfully evolve the population to produce discrete fracture models with tracer breakthrough curves that matched with an observed breakthrough curve fig 5 shows that the algorithm was able to reach convergence to a set of optimal solutions within 15 generations by modifying a population of 20 models note that the population model error decreased until it reaches a limit of 0 75 recall that with hundreds of fractures and with each fracture having its own set of parameters the overall discrete fracture model was heavily parameterized this means that the genetic algorithm theoretically should have the ability to produce discrete fracture models with breakthrough curves that perfectly match the curve of the reference model and achieve a model error of zero yet the algorithm only achieved a minimum of 0 75 one reason why the genetic algorithm failed to achieve a lower model error is that the simulation of particle transport was not completely deterministic if the particle transport simulation were run twice on the same discrete fracture model it would yield slightly different results the reason is that at every fracture intersection the path the particle would take is a stochastic process with its likelihoods weighted by flux because the fitness function was calculated by using the results of the particle transport simulation the fitness function adopted its stochastic value so evaluating the fitness function on the same model multiple times produced different results the uncertainty of the function limited the genetic algorithm s ability to find the most optimal model so the genetic algorithm generated a population of models that had a high likelihood of producing a good fitness score the uncertainty associated with the transport simulation can be reduced by either increasing the number of particles added to the simulation or by running the transport simulation multiple times and recording the average result another reason why model error did not reach zero is because of forcing the genetic mixing between high fitness models and models that are newly generated recall that to prevent the genetic algorithm from prematurely stopping at a local optimum genetic diversity was introduced to the population by forcing the high fitness models to genetically mix with models that were newly generated when producing new child models as a population of optimal models evolves the genetic variation of the models gets reduced and at every step genetic diversity is injected into the population from newly generated discrete fracture models at a certain point the reduction in genetic diversity caused by removing the least fit models is equal to the genetic diversity added by the newly generated models and so the genetic diversity reaches a steady state the aforementioned reasons also explain why the variance of the model error did not reach zero fig 6 the stochastic nature of the fitness function and the consistent addition of genetic diversity to the population prevented the variance of the model error from reaching zero the breakthrough curves of fig 7 also show that the genetic algorithm was able to adjust the models successfully so that they produced curves that better followed what was produced by the reference model after 40 generations the breakthrough curves shifted toward the right meaning that the tracer particles arrived at the extraction well later than earlier generations of models the genetic algorithm achieved this by adjusting populations of fracture parameters until the bulk hydraulic conductivity of the fracture network was increased this led to a lower overall flow rate and so a later arrival time for the particles fig 5 also shows that the average slope of the breakthrough curves of the final generation did not match the slope of the breakthrough curve of the reference model for the first 10 of the arriving particles the particles in the final generation models arrived later than the particles for the reference models for the last 10 percent of the arriving particles the particles in the final generation models arrived earlier than the particles for the reference model this behavior can be attributed to the uncertainty of the fitness function during the start of the genetic algorithm the fitness functions could produce an error signal that was much greater than the uncertainty of the fitness function this allowed the genetic algorithm to quickly distinguish which members of the population were high performing at this stage errors such as the temporal shift of the breakthrough curve could be fitted quickly but as evolution progressed the fitness function produced smaller error signals until finally the error signal was smaller than the noise generated by the fitness function at that stage the slopes between breakthrough curves became difficult to distinguish from each other thereby inhibiting further improvement one limitation for the results of these experiments is that there is no guarantee that the calibrated models will generalize beyond the specific placement of the test wells during the calibration process this means that if a tracer test was performed on the calibrated model with the test wells in a new orientation then the resulting breakthrough curve may be different than the breakthrough curves from applying the tracer test on the ground truth model with the same new orientation of the test wells since information about the hydraulic structure of the fractures comes solely from hydraulic tests with the wells the tracer tests essentially become blind to any regions in the fracture network that are not hydraulically connected to the wells a similar limitation applies to fracture networks which are anisotropic if tracer tests were performed on wells that were installed in an anisotropic fracture matrix in only a single orientation rotating the wells by 90 degrees would yield a different breakthrough curve to reduce the risk of discrete fracture models overfitting to the biases introduced by well placement multiple wells can be installed in multiple orientations this allows for a more comprehensive measurement of the hydraulic structure of the fracture network and helps to mitigate the limitations of wells installed in a single orientation 5 conclusion this study introduces a method for generating a population of discrete fracture models that produce simulated results that match with field observations the method can achieve this model tuning capability without changing the distribution of the fracture parameters this allows the method to not only produce models with flow characteristics that match field pumping tests but to do so in a way that creates a population of fractures that can match fracture distribution parameters observed by field seismic surveys note that this study does not use any data from a seismic survey the key idea is that if there is prior knowledge about the distribution of a fracture parameter then the proposed method can generate possible solutions that preserve this distribution this prior knowledge of parameter distributions may come from analysis of seismic surveys but can also come from other methods the method used in this study is the genetic algorithm modified in a way that can handle discrete fracture networks this was done by encoding every model s fracture as a genetic base in the model s genome during genetic mixing the child model is generated by randomly copying genetic bases from each of the parent models this process allows the fracture parameter distribution of the child model to be the same as the distribution of the parent models to test how well a genetic algorithm modified for discrete fracture networks can perform the genetic algorithm was applied to a synthetic case in which the goal was to find a population of discrete fracture models that when run with particle transport simulations can produce breakthrough curves that match the observed breakthrough curve from a reference model the results show that the genetic algorithm was able to successfully produce a population of discrete fracture models with breakthrough curves that closely match the reference model s breakthrough curve within the span of 15 generations the genetic algorithm reduced the model error and variance to a minimum that was bounded by the uncertainty of the fitness function and by the algorithm injecting genetic diversity into the population the genetic algorithm was found to excel at adjusting the fracture network s bulk hydraulic conductivity to temporally shift the breakthrough curve until it closely matched the breakthrough curve of the reference model the genetic algorithm changed the bulk hydraulic conductivity of the model by adjusting the number of fractures the orientation of the fractures the orientation of the fractures and the location of the fractures by changing these parameters the connectivity of the fractures changes thereby changing the bulk hydraulic conductivity of the model the genetic algorithm also was found to struggle with adjusting the population s breakthrough curves to match the slope of the reference breakthrough curve many issues caused by the uncertainty of the fitness function can be resolved by increasing the number of particles used in the transport simulation or by re running the simulation and using the average simulation result future work for this study includes testing the discrete fracture genetic algorithm on models with a different fracture parameter distribution for example this study involved fractures that have a relatively even distribution of orientations but there are subsurface reservoirs with fractures that are heavily biased toward one or two orientations such fracture models with multiple families of fracture orientations might change the effectiveness of the genetic algorithm another path of study is the development and testing of new fitness functions that are based on different pumping or tracer tests because this study found that the performance can be bounded by the uncertainty associated with the fitness function future work could focus on developing better fitness functions for applications with discrete fracture networks related work also could involve development of better genetic mixing strategies the current strategy randomly selects fractures to be copied over to the child model this method ignores all the other fractures connected to the copied fracture and so that connectivity information can be destroyed during the genetic mixing process future work could help develop a better genetic mixing scheme that considers the hydraulic connections made by adjacent fractures any future improvement of the discrete fracture genetic algorithm could help researchers quickly and more efficiently solve the important problem of inverse modeling of fractured subsurface reservoirs credit authorship contribution statement fleford redoloza conceptulization methodology investigation writing original draft liangping li conceptulization supervision funding acquisition arden davis writing review editing declaration of competing interest liangping li fleford redoloza and arden davis declares that they have no conflict of interest acknowledgments this work has been supported through a grant from the national science foundation united states oia 1833069 the authors wish to thank the associate editor as well as two anonymous reviewers for their comments which substantially helped to improve the final version of the manuscript 
4,this study introduces firm information gain for model discrimination based on shannon entropy and worst case scenario experimental design firm information gain is the minimal additional information gained by an experimental design with respect to existing information robust experimental design aims to maximize the firm information gain by searching for the least number of new pumping wells and observation wells robust experimental design includes a bayes factor threshold to ensure that new data provide strong evidence for model discrimination to maximize the firm information gain a framework is proposed that combines the parallel sequential genetic algorithm ga for parallel computing and the nested quadrature rule for efficiently solving multidimensional integrals the numerical experiment involves the true model for the purpose of verification the results show that using a full covariance matrix is imperative to avoid exaggerating firm information gain collecting new groundwater data is prioritized over exploring additional pumping wells maximizing firm information gain is able to identify the same and true model keywords robust experimental design entropy information theory model discrimination uncertainty data availability data will be made available on request 1 introduction groundwater is a crucial source of freshwater throughout the world for both hydrologic and human systems alley et al 2002 giordano 2009 siebert et al 2010 groundwater modeling has been widely used for decades as essential tools for the planning and management of groundwater resources gleeson et al 2012 wada et al 2010 however developing a groundwater model has never been an easy task as groundwater data is always sparse and uncertainty always exists multiple conceptualizations of a groundwater system are often investigated yet considering too many conceptual models indicates high model prediction uncertainty and may lose the purpose of model development bredehoeft 2005 højberg and refsgaard 2005 collecting and incorporating new data into groundwater models helps advance conceptual understanding and management of groundwater resources kikuchi 2017 and in turn reduces the number of models nevertheless collecting groundwater data is usually costly and optimal experimental design techniques are often conducted before data collection to gain the maximum amount of information given a pre defined monitoring objective according to sun 1994 experimental design in groundwater modeling generally falls into two parts the observation part e g state variables to be observed the number and locations of observation wells and observation frequency and the excitation part e g the number and locations of extraction and injection wells pumping and injection rates and periods of extraction and injection if the excitation part is predetermined and only the observation part is considered the experimental design is referred to as an observation network design observation network designs have been studied extensively in the literature a variety of methodologies have been introduced to design a groundwater observation network kollat et al 2011 loaiciga et al 1992 mogheir et al 2006 among these methods physically based simulation approaches cieniawski et al 1995 cleveland and yeh 1990 dhar and datta 2007 hudak and loaiciga 1992 mckinney and loucks 1992 meyer et al 1994 reed et al 2000 storck et al 1997 and information theory entropy based method alfonso et al 2010 mogheir et al 2006 mogheir and singh 2002 nowak and guthke 2016 poeter and anderson 2005 are commonly employed owing to their flexibility in examining design scenarios and design constraints the objectives of observation network designs are usually to 1 improve parameter estimation altmann dieses et al 2002 chang et al 2005 cleveland and yeh 1990 herrera and pinder 2005 hsu and yeh 1989 sciortino et al 2002 siade et al 2017 sun and yeh 2007 ushijima and yeh 2015 2 minimize prediction uncertainty chadalavada and datta 2008 janssen et al 2008 mckinney and loucks 1992 nowak et al 2010 wagner 1995 wöhling et al 2016 3 detect plumes bode et al 2019 dhar and datta 2007 dokou and pinder 2009 kim and lee 2007 leube et al 2012 meyer and brill 1988 storck et al 1997 and 4 to discriminate among candidate models and identify the most probable model kikuchi et al 2015 knopman and voss 1988 pham and tsai 2016 2015 usunoff et al 1992 yakirevich et al 2013 readers are referred to several in depth review articles hassan 2003 kollat et al 2011 loaiciga et al 1992 minsker 2003 to achieve the objective of model discrimination observation networks aim to provide the most useful information with respect to model discrimination several criteria have been developed for model discrimination in optimal observation network designs based on the maximum differences between model predictions knopman et al 1991 knopman and voss 1988 nordqvist and voss 1996 usunoff et al 1992 the maximum kullback leibler information kikuchi et al 2015 nowak and guthke 2016 yakirevich et al 2013 the maximum change in entropy box and hill 1967 alfonso et al 2010 and the maximation of posterior model probability pham and tsai 2016 2015 the basic concept underlying all these criteria is to sample the state variable s at spatiotemporal locations i e predicted data where the variance among the ensemble of proposed competing model s predictions is maximized the worth of new data has been analyzed in various water related problems such as prediction uncertainty reduction dausman et al 2010 feyen and gorelick 2005 freer et al 1996 gates and kisiel 1974 rojas et al 2010 sohn and small 2000 tiedeman et al 2004 2003 yokota and thompson 2004 model selection wöhling et al 2015 decision making ben zvi et al 1988 davis and dvoranchik 1971 james et al 1996 reichard and evans 1989 and cost effectiveness james and gorelick 1994 neuman et al 2012 norberg and rosén 2006 wagner 1999 though optimal observation network designs have been studied extensively in the past there is still a lack of clear understanding of the amount and the worth of new data required to justify a certain level of model discrimination and identify the most probable model besides none of these studies could guarantee identifying the same most probable model moreover all these studies only considered the observation part of the experimental design in this study we introduce a robust experimental design for model discrimination based on the shannon entropy shannon 1948 and the worst case scenario experimental design sun and yeh 2007 first we introduce a firm information gain concept and derive a new model discrimination criterion based on the shannon entropy and the bayes factor the firm information is defined to be the minimum information guaranteed from an experimental design the crux of experimental design based on firm information gain is that the design objective can be achieved with the least information as a result any other experimental designs under the same experimental conditions will result in higher information gain and therefore guarantee the same design outcome according to sun and yeh 2007 an experimental design is considered robust if it accounts for both the excitation part pumping activities and observation part observation activities to maximize the firm information gain this is achieved through a max min optimization problem to improve model discrimination while using the fewest possible pumping and observation wells in the context of robust experimental design robust means that the optimized pumping well network performs well across all possible observation well networks as it performs well even with the worst case observation well network we hypothesize that the same most probable model can be identified from a pool of competing models by maximizing the firm information gain for a system e g a set of conceptual groundwater models that differ in boundary conditions geological structures etc and satisfying a bayes factor threshold any other experimental design solutions having information negative shannon entropy higher than the firm information will result in the same most probable model second we introduce a parallel computing framework that combines the parallel sequential ga carroll 1996 and the nested quadrature rule genz and keister 1996 to efficiently solve the time consuming max min optimization problem finally we test the proposed framework and conduct the robust experimental design on a hypothetical numerical example where nine competing groundwater models were generated and a robust experimental design is needed to discriminate among the models and identify the same most probable model the robust experimental design in this study is different from that in box and hill 1967 and pham and tsai 2016 first this study considers measurement errors and data correlation in the experimental design second a new model discrimination criterion is introduced that maximizes firm information gain to obtain the robust experimental design instead of finding an upper bound of the expected information gain usually referred to the box hill discrimination function 2 methodology 2 1 shannon entropy and expected information gain shannon entropy shannon 1948 provides a measure of the information value of a system using probabilities of the occurrence of events in the system consider that a set of m candidate models m m1 m2 m m represents the events of the system e g candidate models are groundwater models that differ in model conceptualizations such as boundary conditions geological structures and parameter structures their posterior model probabilities are pr m i δ obs given existing observation data δ obs the shannon entropy of the system is 1 s m δ obs i 1 m pr m i δ obs lnpr m i δ obs where lnpr m i δ obs is the information of the model m i negative entropy s represents the average amount of information i provided by all candidate models 2 i m δ obs s m δ obs the least information corresponds to the maximum entropy when all models have an equal posterior model probability the maximum information from the system corresponds to the minimum entropy when one model has a 100 posterior model probability and other models have zero posterior model probability the main purpose of an experimental design d for model discrimination is to maximize information gain through acquiring new data such that the most probable model can be identified from a pool of candidate models of the system the information gain is defined as follows 3 i g i m δ d new i m δ obs where i g is the information gain after an experimental design δ d new r n is a vector of n new data and i m δ d new represents the combined information obtained from both the new and existing data new data are unknown and uncertain before sampling this study proposes an expected information gain of the new data for the experimental design 4 i g e i m δ d new i m δ obs where i g is the expected information gain and e is the expectation operator the expected new information under a probability distribution function of new data δ d new is 5 e i m δ d new i 1 m pr m i δ d new ln pr m i δ d new q δ d new d δ d new where pr m i δ d new are the posterior model probabilities given new data δ d new and q δ d new is the averaged probability density function of new data δ d new via bayesian model averaging 6 q δ d new j 1 m pr m j δ obs p δ d new m j where p δ d new m j is the probability density function of predicted new data δ d new using the model m j inserting eqs 2 and 5 into the eq 4 appendix a shows the expected information gain as follows 7 i g i 1 m pr m i δ obs p δ d new m i ln p δ d new m i q δ d new d δ d new the integral in the eq 7 is the kullback leibler kl divergence that measures the difference between the bma weighted probability distribution q δ d new and the probability distribution p δ d new m i maximizing i g enhances the diversity of probability distributions of each model s prediction compared to the bma weighted probability distribution the expected information gain is the averaged kl divergence weighted by pr m i δ obs because the kl divergence is always non negative the expected information gain is always non negative to solve eq 7 we need to know the probability density function p δ d new m j of predicted new data δ d new considering that new data are correlated and multivariate gaussian the probability density function p δ d new m i is 8 p δ d new m i 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i where δ i are the expected values of new data estimated by the model m i σ i σ σ ε is the total covariance matrix of new data involving the use of the model m i which is the sum of the covariance matrix of the estimated new data representing parameter and model structure uncertainties and the covariance matrix of measurement errors in new data this study considers correlated data which results in a full covariance matrix appendix b further expands the eq 7 with the multivariate gaussian distribution as follows 9 i g i 1 m pr m i δ obs ln 2 π n 2 σ i 1 2 n 2 e δ d new m i ln q δ d new e δ d new m i ln q δ d new in the eq 9 is the expectation of ln q δ d new under randomness of δ d new given model m i which is 10 e δ d new m i ln q δ d new p δ d new m i ln q δ d new d δ d new for one dimensional integral i e only one new observation is collected gaussian quadrature rules and monte carlo methods are powerful however when the new data are in high dimensions and correlated these approaches become impractical due to prohibitive computing costs rising exponentially with the number of dimensions and there is no analytical solution for the eq 10 as far as authors knowledge it is noted that the method presented in this study is not limited to the gaussian distribution of the predicted new data the general form of the expected information gain is in eq 7 as soon as one knows the probability density function p δ d new m j eq 7 can be solved assuming the gaussian distribution is for the convenience purpose that the general form of i g in eq 7 is reduced to a simplifier form in eqs 9 and 10 genz and keister 1996 presented a nested quadrature rule to efficiently calculate high dimensional integrals for the multivariate normal distribution with zero means and an identity matrix unfortunately the integral in eq 10 was for multivariate gaussian with a non zero mean and a full covariance matrix therefore this study adopted the cholesky decomposition to transform e δ d new m i ln q δ d new into a multivariate normal distribution with zero means and an identity matrix i e the covariance matrix is an identity matrix in which all the diagonal elements are ones and all off diagonal elements are zeros and used genz and keister 1996 approach to calculate e δ d new m i ln q δ d new numerically as shown in appendix c 2 2 max min information gain criterion for model discrimination consider an experimental design d that includes a pumping design and an observation design to collect new groundwater level data using the least number of pumping wells and observation wells the new head observation locations serve to obtain firm information gain while the new pumping test locations serve to maximize the firm information gain data from new head observation wells stimulated by new pumping test locations will serve to discriminate groundwater models such that the same most probable groundwater model can be identified this study adopts the bayes factor as a model discrimination function to achieve the design objective the max min optimization problem to maximize firm information gain is introduced for the robust experimental design as follows 11 max d q min d δ i g d where dq are the pumping design and d δ are the observation design eq 11 is subject to 12 min b f k i p δ d new m k p δ d new m i i 1 2 m and i k γ where m i n i g d is the firm information gain from experimental design bf ki is the bayes factor which is the likelihood ratio of the most probable model m k having the highest posterior model probability against other models m i and γ is a bayes factor threshold p δ d new m i is the likelihood that new data are predicted using the model m i p δ d new m k is the highest likelihood among m models given pumping locations in an experimental design d m i n i g d can be obtained by minimizing i g given new observation data as dependent variables the maximum of m i n i g d in eq 11 can be solved by solving the maximization optimization problem where the dependent variables are pumping locations eq 12 ensures that the same most probable model has sufficient evidence to be discriminated from all other models the classification of harold jeffreys jeffreys 1998 presents how strong the new data evidence supports one model over other models the higher the γ value the stronger the data evidence that supports one model over the other competing models for example when the bayes factor is between 5 and 10 the data evidence is classified as substantial when bayes factor is greater than 10 the data evidence is classified as strong jeffreys 1998 2 3 total covariance matrix σ i for new observation data the total covariance matrix of new data includes the covariance matrix of measurement errors in new data and the covariance matrix of the estimated new data random measurement errors are usually modeled by uncorrelated gaussian noise with zero means therefore the covariance matrix of measurement errors in new data can be σ ε σ ε 2 i where σ ε 2 is a constant error variance and i is an identity matrix monte carlo simulation on model parameters is adopted to calculate the covariance matrix of the expected values of new data estimated by model m i 13 i 1 q 1 q 1 q δ β i q δ i δ β i q δ i t where q is the number of realizations of model parameters β i q of the model m i these parameter realizations are sampled from the posterior distribution upon history matching for each model δ i is the mean of the new data simulated by model m i 14 δ i 1 q q 1 q δ β i q the bma method hoeting et al 1999 draper 1995 is used to calculate the covariance matrix σof the estimated new data as follows 15 bma i 1 m i δ β i δ d new δ β i δ d new t pr m i δ obs where δ β i is the predicted new data using the model m i and the estimated model parameters β i of the model m i δ d new is the bma mean of the predicted new data the total covariance matrix for δ d new is σ i σ σ ε σ bma σ ε 2 4 model calibration and posterior model probability the covariance matrix adaptation evolution strategy cma es hansen and ostermeier 2001 hansen et al 2003 is employed to estimate model parameters and to obtain a covariance matrix for the estimated model parameters model parameters are estimated by minimizing the root mean square error rmse between calculated and observed heads the cma es is a global local stochastic derivative free algorithm that was parallelized for time consuming groundwater model calibration and uncertainty analysis elshall et al 2013 once the estimated parameters and their covariance matrix are obtained by the cma es the marginal likelihood function is calculated as follows for existing observation data δ obs which is similar to the eq 8 16 p δ obs m i 2 π n 1 2 σ ε σ i 1 2 exp 1 2 δ obs δ i t σ ε σ i 1 δ obs δ i where n1 is the number of existing observation data σ ε is the covariance matrix of measurement errors δ i is the simulated observation data using the model m i with the estimated model parameters β i obtained by the cma es and σ i is the covariance matrix of simulated observation data which is calculated by the monte carlo simulation based on the estimated model parameters and their covariance matrix obtained by the cma es it is important to acknowledge that the covariance matrix generated by the cma es is merely an estimate it is crucial to confirm the accuracy of the covariance matrix obtained through this method this can be achieved by running simulations using realizations of model parameters and verifying that the resulting root mean square errors rmses are comparable to those obtained using the estimated model parameters the posterior model probability for each groundwater model is commonly calculated the same as the likelihood given the assumption that all models have the same prior model probability other than the cma es the null space monte carlo method siade et al 2017 and the iterative ensemble smoother method white 2018 can also quantify model output uncertainty 3 numerical example this study uses a steady state groundwater flow condition in a 5 layer synthetic anisotropic confined aquifer to illustrate the robust experimental design based on the firm information gain the size of the aquifer is 5 km by 5 km and is discretized into 5 layers 25 rows and 25 columns see fig 1 the cell size is 200 m by 200 m with variable thickness there are two pumping wells pws screened at layer 1 and layer 5 and one injection well iw screened at layer 3 fig 1 shows the well locations and pump rates the true constant head boundary condition of 50 m is assigned to the boundary cells of all layers at the south boundary no flow boundary condition is assigned to the north east and west boundaries to allow better variations higher sensitivities in simulated heads upon pumping fig 2 a shows the true aquifer structure table 1 lists the true model parameters the usgs modflow 2005 harbaugh 2005 is adopted to simulate true steady state groundwater levels at the 5 existing observation wells in the model domain see fig 1 gaussian noises of a zero mean and a standard deviation of 0 1 m are added to the groundwater level data to simulate measurement errors we pretend that we do not know the true aquifer structure i e the geometry of the aquifer system and lithology the true constant head boundary value and the true horizontal hydraulic conductivity three dimensional geometry views of three aquifer structures are given in fig 2 denoted as g1 g2 and g3 respectively these aquifer structures were extracted from the real world case study of the baton rouge aquifer system using three different geostatistical methods the generalized parameterization the indicator zonation methods and the indicator kriging respectively pham and tsai 2016 2015 g3 is a highly connected aquifer system following up with g1 and g2 the number of active model cells is 2018 1566 and 2021 for gp iz and ik respectively three head values 49 50 and 51 m for the south boundary are considered and denoted as b1 b2 and b3 respectively the number of boundary cells at the south boundary for g1 g2 and g3 are 30 25 and 29 respectively these boundary cells are only in layers from 3 to 5 for all three aquifer structures the connections between the south boundary cells and the aquifer are weaker in the iz structure in comparison to the gp and ik structures combinations of three aquifer structures and three head boundary values result in nine conceptual groundwater models 4 solving the max min optimization problem to obtain robust experimental design this study solved eqs 11 and 12 to identify the robust experimental design for model discrimination to identify the most probable groundwater model the bayes factor threshold was set to be 10 such that the most probable model will be at least strongly discriminated from the other eight competing models decision variables were the number of new pumping wells and the number of new observation wells a pumping rate of 200 m3 day was assigned for all new pumping wells experimental designs were conducted by gradually increasing the number of pumping wells and the number of observation wells of the system the robust experimental design was the one that optimizes eq 11 using the least number of new pumping wells and new observation wells until eq 12 is satisfied to maximize the firm information gain in eq 11 the max min optimization problem this study utilized a parallel sequential genetic algorithm ga optimization scheme given a number of pumping wells and observation wells a parallel ga was employed to optimize pumping locations the outer loop of the max min optimization problem and under the parallel ga a sequential ga the inner loop was employed to optimize observation locations the ga code of carroll 1996 was employed to solve the max min optimization problem and was parallelized to be run in supermic a supercomputer at louisiana state university using an embarrassingly parallel technique a population size of 80 i e used 80 cores was assigned to the parallel ga and a population size of five was assigned to the sequential ga micro ga the number of generations was 50 for the parallel ga and 500 for the sequential ga other default settings were set the same in the ga code 5 results 5 1 model calibration posterior model probability and entropy of the current system table 2 shows model calibration results the posterior model probability and bayes factor for each groundwater model the parallel cma es estimated horizontal hydraulic conductivity for all layers using the five noisy head observation data the top five models g1b1 g1b2 g1b3 g3b1 and g3b2 showed comparably small rmses the three conceptual models with g2 aquifer structure resulted in a much larger rmse it indicates that the aquifer structure significantly affected the model calibration results the models with the g1 and g3 aquifer structures better represented the aquifer than the g2 aquifer structure the top five models had posterior model probabilities greater than 17 g3b2 model had the highest posterior model probability but did not have the lowest rmse because of the impact of the covariance matrix σ i in eq 16 insufficient observation data used in the model calibration prevented the true model g1b2 from having the highest posterior model probability and outperforming the other models nevertheless the bayes factor suggested that the current data did not discriminate g3b2 model from the other top four models the entropy of the system was 1 748 nat calculated using eq 1 and the posterior model probabilities in table 2 the nat the natural unit of information is the natural unit for information entropy given a system of nine models the entropy of the system is between zero highest information and 2 197 nat lowest information therefore 1 748 nat 79 6 of the maximal entropy of the system was a high value this indicates that more data are needed to reduce the entropy increase information of the system and to identify the most probable model 5 2 information gain and data correlation evaluation using the current system in this section we intend to study the changes in expected information gain i g and firm information gain min i g d by systematically adding new observation data before conducting an exhaustive robust experimental design no new pumping and injection wells were added we only draw new head data out of active cells that are in common in three aquifer structures i e 1024 possible locations additionally we investigate the impacts of data correlation on min i g d fig 3 shows the spatial distributions of expected information gain i g from drawing one new head data in layers 2 to 5 i g was found varied between 0 723 nat and 1 472 nat high expected information gain occurred in the areas near the constant head boundary and near the injection well where heads predicted by the candidate models were quite different drawing one additional head data for either layer 3 4 or 5 gained higher i g than that from layer 2 data collected from different locations provided different i g robust experimental designs are needed to identify optimal locations fig 4 compares firm information gain min i g d calculated by using an experimental design d i e using the existing pumping and injection wells and adding one to five new head data for both cases of uncorrelated and correlated heads the result indicates that min i g d increased as the size of new head data increased experimental designs considering uncorrelated new data overestimated min i g d the degree of overestimation increased dramatically with the size of uncorrelated data data correlation significantly impacted on min i g d therefore this study will only focus on experimental designs utilizing correlated data in the later sections experimental designs using one to five new head data and the existing pumping and injection wells were unable to reach the highest possible information gain i g of 1 748 nat the bayes factor threshold eq 12 was also not satisfied the bayes factor values will be presented in section 5 5 the experimental designs using one to five new head data showed that new pumping wells are needed 5 3 information gain using one new well detailed maximum firm information gain was illustrated by the case of searching for one optimal pumping location and one optimal observation location although there are 1024 possible locations model cells available for installing new pumping and observation wells only 256 locations every other model cell were considered for potential pumping locations to ensure the experimental design remains tractable the potential pumping locations were indexed from 1 to 256 the potential head observation locations are indexed from 1 to 1024 fig 5 a shows the firm information gain min i g d given by each of the 256 potential pumping well locations for each new pumping location 1024 alternatives of new observation wells were investigated and the observation location that resulted in min i g d was recorded the result showed that min i g d were varied from 0 532 nat to 0 684 nat not much changes in min i g d were detected if placing a new pumping well in layer 1 layer 2 layer 4 or layer 5 however large changes in min i g d were found if placing a new pumping well in layer 3 the maximum change of min i g d was found to be 0 684 nat occurring at pumping location index 141 in layer 3 which is denoted as circle a in fig 5 a and c the horizontal coordinates of the optimal pumping location are x 4300 m y 3100 m given the optimal pumping location fig 5 b shows the expected information gain i g for each of the 1024 potential observation locations the result showed that i g were varied from 0 684 nat to 1 110 nat drawing a new head data from layer 3 low conductivity generally provided higher i g than other layers some observation locations in layers 4 and 5 also provided higher i g the firm information gain of 0 684 nat was obtained at the observation location index 548 in layer 3 and denoted at circle b in fig 5 b and d the horizontal coordinates of the optimal observation location are also x 500 m y 2100 m the result verified that all experimental designs using one new head observation and one new pumping well at pumping location index 141 in layer 3 resulted in higher i g than 0 684 nat similar to section 5 2 experimental designs using one new pumping well and one new head data were unable to reach the highest possible information gain i g of 1 748 nat and failed to meet the bayes factor threshold two or more pumping wells are needed to achieve the design objective 5 4 data worth of adding new pumping wells versus new observation wells adding more pumping wells or adding more observation wells showed different maximize firm information gain max min i g d as illustrated in fig 6 red circles show max min i g d by increasing the number of new pumping wells up to five while keeping the number of new observation wells to be one yellow squares show max min i g d by increasing the number of new observation wells up to five while keeping the number of new pumping wells to be one given the same number of new wells e g one new pumping well or one new observation well adding new observation wells always resulted in higher max min i g d than adding new pumping wells fig 6 suggested that experimental designs should emphasize new head data collection before exploring new pumping wells we acknowledge that this observation may vary depending on the specific case in this numerical example where the model domain is relatively small the addition of a single pumping well can potentially influence the entire model domain therefore incorporating additional observation wells would be a more effective strategy than adding more pumping wells 5 5 robust experimental designs fig 7 shows whether the first rank model can or cannot be discriminated from others considering only adding up to five new head data bf12 is the bayes factor of the first rank model to the second rank model bf13 is the bayes factor of the first rank model to the third rank model and so forth the model rank is determined by the likelihood after new data are acquired the rank may change for different scenarios fig 7 a presents the bayes factors for the status quo no new pumping well and no new head data the result showed that adding a new head data discriminated the first rank model from the last three models by adding two new head data the first rank model was discriminated from the last two models adding up to five new head data only discriminated the first rank model from the last five models experimental designs using up to 5 new observations and the current system were unable to discriminate the most probable model from the other eight competing models if one new pumping well was added fig 7 b showed that adding a new head data discriminated the first rank model from six other models adding two new head data discriminated the first rank model from five other models the results also indicate that increasing new head data increased the maximum firm information gain see fig 6b but might not increase the number of models to be discriminated against similar to fig 7 a experimental designs using one new pumping well and up to five new observations were unable to discriminate the most probable model from the other eight competing models if two new pumping wells were added the same most probable model can be identified by using two to five new head data as shown in fig 7 c where the first rank model dominated all other models the most probable model was the g1b2 model which was the true model the robust experimental design found that two new pumping wells and two new head observation wells sufficed with firm information gain of 1 707 nat and reduce the entropy of the system to 0 041 nat the minimum bayes factor of 152 98 exceeded the selected threshold of 10 given the optimal locations of the two pumping wells we verified all possible locations of two new head observation wells produced entropy of the system less than 0 041 nat all identified most probable models which met the bayes factor threshold were the g1b2 model the true model this verification indicates that the same most probable model can be consistently identified regardless of sample locations 6 discussion the presence of head data correlation attributable to several factors like spatiotemporal location model domain size boundary conditions and model parameterizations showed significant impacts on firm information gain min i g d and should be considered in experimental designs this is because the most probable model tends to receive overwhelming posterior model probability close to 100 when the data size is large and the data are assumed uncorrelated this finding is consistent with lu et al 2013 that suggests accounting for the correlation of model data errors in the covariance matrix to avoid deriving unrealistic posterior model probabilities for this study it poses a serious concern that exaggerated min i g d by assuming data uncorrelated may eventually fail the experimental designs due to low information gain in actual data collection to gain maximum firm information this study found that the best locations to draw new pumping wells are in low hydraulic conductivity zones i e layer 3 in this case study see fig 5a and 5c and the best locations to draw new observation wells are the areas that are far from the pumping wells see fig 5d this is because pumping in these areas tends to generate high variation in groundwater levels in these low conductivity zones and thereafter provides higher expected information gain in comparison with pumping in high conductivity zones therefore this numerical example suggests drawing new pumping wells in the low conductivity zone and observe at a far enough distance from the pumping wells to obtain firm information for model discrimination and identification it is noted that different aquifer settings e g boundary conditions will result in different design outcomes groundwater systems are highly heterogeneous and nonlinear different locations of pumping wells and observation wells yield different information given a design objective determining the best pumping and observation locations is an important step before any field data collection as pumping tests are costly and time consuming considering only the observation part e g adding new observation wells was not a good strategy for this case study potentially because many new observation locations might yield similar information i e did not help to increase firm information gain simultaneously accounting for both the observation part and the excitation part e g adding new pumping wells was found a more efficient way to obtain new additional information the robust experimental design was succeeded in determining the optimal locations to draw new pumping wells and measure groundwater levels to achieve the design objective using the least number of wells after the robust experimental design was succeeded all other designs using the same number of wells such as 2 pumping wells and two observation wells identified the same most probable groundwater model which differs from the author s previous model discrimination criterion based on posterior model probability pham and tsai 2016 2015 where the most probable model was varied by design alternatives it is important to recognize that drilling a new pumping well is generally more expensive than drilling a new observation well and groundwater managers are not typically interested in drilling a new pumping well solely for model discrimination purposes consequently in a real world application it is more feasible to apply the method to the existing pumping network and concentrate on drilling new observation wells only the nested quadrature rule genz and keister 1996 was found an efficient approach to calculate high dimensional integrals such as e δ d new m i ln q δ d new for deriving the expected information gain i g in this study using 5 nodes and searching for one new observation at a time the dimension of δ d new is one calculating e δ d new m i ln q δ d new required nine samples of δ d new calculating e δ d new m i ln q δ d new required sample sizes of 37 93 201 and 401 when the dimension δ d new increased from 2 to 5 the computation time dramatically increased with δ d new dimension yet the number of samples required was small in comparison with the traditional monte carlo simulation approaches that usually require thousands of samples solving the max min programming problem to maximize the firm information gain was extremely time consuming even with the hypothetical case study where a single model run was less than one minute for example using the parallel ga to search for two new pumping wells the computation time for solving the max min programming problem was 6 68 7 86 8 87 11 5 and 16 4 h for δ d new size to be 1 2 3 4 and 5 using 80 cores computing time grew substantially by just increasing a few new observation data the most time consuming part came from e δ d new m i ln q δ d new calculations combining the parallel sequential ga and the nested quadrature rule efficiently solved the time consuming max min optimization problem the presented methodology assumed that the probability distribution function of observable states i e groundwater level given a realization of model events follows a multivariate gaussian distribution this assumption may not hold for the nonlinear groundwater problem e g the reactive transport model shi et al 2014 and may have an impact on the results of the robust experimental design however assuming the multivariate gaussian allows transferring the complicated multiple integrals of eq 7 into an easier form of eqs 9 and 10 these equations can accurately and efficiently be solved by utilizing the cholesky decomposition and the nested quadrature rule this gaussian assumption can be resolved by using monte carlo approaches such as the dream package vrugt 2016 however this approach requires high computational cost e g requires thousands of sample sizes and is not suitable for solving the max min optimization program even with the hypothetical numerical example in this study the robust experimental design may not guarantee a global optimal solution when the search dimension increases e g greater than five because solving the nonlinear and non convex max min problem is challenging and the computation time increases exponentially with the increase in search dimension to increase the global search capability in finding the global optimal solution one can increase the population size in the ga however this will significantly increase the computation time therefore we limited our search dimensions to less than five i e less than five new wells to avoid potential numerical issues in the numerical calculation of e δ d new m i ln q δ d new and make our optimization problem trackable the computational burden of the robust experimental design may be reduced by using surrogate modeling approaches also known as reduced order model where a complex model is replaced with an approximate but computationally efficient model ushijima and yeh 2013 asher et al 2015 jefferson et al 2015 yin and tsai 2020 for the numerical example in this study the true model was added to a pool of 9 competing models for verification purposes when the true model was removed from the robust experimental design presented in section 5 5 the minimum bayes factor decreased to 44 73 which was almost 3 42 times less than when the true model was included consequently the most probable model became g1b3 it is worth noting that a true model is typically unknown and including it does not reflect a realistic scenario however whether or not the true model is included has no impact on the methodology but it could affect the optimal locations for pumping and observation wells and the number of wells used in the robust experimental design if a model that is close to the true model is included among the competing models there is a higher likelihood of obtaining a robust experimental design with lower costs i e using fewer pumping and observation wells while achieving the minimum bayes factor of 10 was possible in the numerical example expensive experimental designs may be resulted for real world applications it is analyst s discretion in setting the bayes factor to achieve a certain level of model discrimination jeffreys 1998 7 conclusions incorporating the concept of firm information gain nominally the minimum expected information gain in the robust experimental design reveals the minimum information required while acquiring new data to identify the most probable model this is a robust approach and places the experimental design in the context of information theory the bayes factor threshold of 10 in the robust experimental design ensures that new data provides strong evidence to discriminate the most probable model from other candidate models considering a full covariance matrix of data substantially affects the calculation of firm information gain the full covariance matrix in this study is comprehensive which accounts for measurement errors and errors from model conceptualization and model parameters the bayesian model averaging method and the monte carlo approach are suitable to quantify covariances due to conceptual uncertainty and parametric uncertainty respectively neglecting covariances between data tends to exaggerate true firm information gain and results in unrealistic bayes factor values maximizing the firm information gain in the robust experimental design is a unique choice and results in more direct solutions than those from maximizing the value of the box hill discrimination function an upper bound of the expected information gain however calculating the firm information gain is not straightforward this study found that the genz keister genz and keister 1996 method can efficiently calculate the multi dimensional integral in the expected information gain when data size is small this study also found that the parallel sequential genetic algorithm scheme is an efficient scheme to maximize the firm information gain which is posed as a max min programming problem through the numerical groundwater example this study found that 1 maximum firm information gain grows faster with the size of new head data than with the number of new pumping wells in other words this study suggests that experimental designs should emphasize new head data collection before exploring new pumping wells for this specific numerical example and 2 the same most probable groundwater model could be identified as long as solutions of experimental designs result in higher than firm information gain and satisfy a bayes factor threshold future research should focus on evaluating the impacts of gaussian assumption on the robust experimental design and comparing the informatics metric proposed in this study with available metrics presented in the introduction section credit authorship contribution statement hai v pham conceptualization methodology software validation formal analysis investigation visualization writing original draft frank t c tsai conceptualization methodology writing original draft resources supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported in part by the u s geological survey under grant cooperative agreement no g21ap10577 and the u s national science foundation award no 2019561 lsu high performance computing and lsu center for computation technology are acknowledged for providing a supercomputer for this study all of the numerical data are provided in the tables and figures produced by solving the equations in the paper appendix a derivations for expected information gain the expected information gain i g in the eq 4 can be further expanded as a1 i g i 1 m pr m i δ new lnpr m i δ new q δ d new d δ d new i 1 m pr m i δ obs lnpr m i δ obs where a2 q δ d new j 1 m pr m j δ obs p δ d new m j and a3 pr m i δ new p δ new m i pr m i δ obs q δ d new inserting eqs a3 and a2 into a1 we have a4 i g i 1 m p δ new m i pr m i δ obs lnpr m i δ obs d δ d new i 1 m p δ new m i pr m i δ obs ln p δ new m i q δ d new d δ d new i 1 m pr m i δ obs lnpr m i δ obs then a5 i g i 1 m pr m i δ obs p δ d new m i ln p δ d new m i q δ d new d δ d new appendix b expected information gain for correlated multivariate gaussian data the expected information gain is b1 i g i 1 m pr m i δ obs p δ d new m i ln p δ d new m i d δ d new i 1 m pr m i δ obs p δ d new m i ln q δ d new d δ d new the probability density function is b2 p δ d new m i 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i where σ i σ σ ε is the total covariance matrix for δ d new when model m i is used substituting p δ d new m i in b1 with b2 the first integral in b1 is b3 p δ d new m i ln p δ d new m i d δ d new e ln p δ d new m i e ln 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i ln 2 π n 2 σ i 1 2 1 2 e δ d new δ i t σ i 1 δ d new δ i ln 2 π n 2 σ i 1 2 n 2 where e is the expectation operator and b4 e δ d new δ i t σ i 1 δ d new δ i e tr δ d new δ i t σ i 1 δ d new δ i e tr σ i 1 δ d new δ i δ d new δ i t tr e σ i 1 δ d new δ i δ d new δ i t tr σ i 1 e δ d new δ i δ d new δ i t tr σ i 1 σ i tr i n where tr is the trace of a square matrix and i is the identity matrix the second integral in b1 is b5 e δ d new m i ln q δ d new p δ d new m i ln q δ d new d δ d new appendix c calculate e δ d new m i ln q δ d new the correlated new data δ d new are transformed into uncorrelated random variables x by the cholesky decomposition since the covariance matrices σ i are positive definite and symmetric let σ i l i l i t and δ d new l i x δ i where l i is a lower triangular matrix with real and positive diagonal entries the random variables x have zero means and an identity matrix for the covariance matrix δ d new obtained through x include measurement errors the probability density function p δ d new m i in terms of x is c1 p δ d new m i 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i 2 π n 2 l i l i t 1 2 e 1 2 l i x t l i l i t 1 l i x 2 π n 2 l i 1 e 1 2 x t x therefore q δ d new can be calculated in terms of x c2 q δ d new j 1 m pr m j δ obs p δ d new m j j 1 m pr m j δ obs 2 π n 2 σ j 1 2 e 1 2 δ d new δ j t σ j 1 δ d new δ j j 1 m pr m j δ obs 2 π n 2 σ j 1 2 e 1 2 l i x δ i δ j t σ j 1 l i x δ i δ j q x the transformation from d δ d new to d x needs the jacobian which is the determinant of l i d δ d new abs l i d x where abs l i is the absolute value of the determinant of l i since positive diagonal entries in l i it becomes δ d new l i d x therefore c3 e δ d new m i ln q δ d new p δ d new m i ln q δ d new d δ d new 2 π n 2 l i 1 e 1 2 x t x ln q x l i d x 2 π n 2 e 1 2 x t x ln q x d x the nested quadrature rule for n dimensional numerical integration genz and keister 1996 is c4 e δ d new m i ln q δ d new r 1 r ln q x r 1 x r n w r where n is the number of dimensions of δ d new r 1 r where r is the number of nodes after removing duplicates heiss and winschel 2008 and x r n is a set of nodes and wr is a set of weights sampling x r 1 x r n from a sparse grid we get δ d new 
5,watersheds act as low pass filters damping and attenuating climatic signals as water moves through the surface and subsurface this is a well observed phenomenon however the ways in which watershed properties control the nature of this filtering are less well documented especially with respect to groundwater surface water interactions here we use a physically based groundwater surface water model to simulate idealized hillslope ensembles with varying watershed properties hillslope slope hydraulic conductivity and precipitation magnitude to quantitively explore the impact of watershed configuration on temporal filtering in both the surface and subsurface to limit the complexities of this system an idealized titled v domain is used multi decadal simulations 95 years are run and then power spectral densities and transfer functions are used to quantify the temporal dynamics and damping of each simulation overall we show that the degree of filtering and the degree of signal transformation is controlled by the total time spent in the subsurface and the degree of groundwater surface water exchanges the ratio of precipitation to hydraulic conductivity controls the partitioning between infiltration and runoff greater infiltration results in less filtering in the subsurface and more filtering in streamflow for a given precipitation conductivity ratio deeper water table depths lead to greater streamflow filtering for periods less than 5 years for time periods greater than 5 years the streamflow filtering is most strongly related to hydraulic conductivity which controls the baseflow dynamics the majority of the input signal is filtered in the subsurface for short periods less than one year for longer time scales hydraulic conductivity is found to be the primary control of filtering and power shift taking place in the subsurface with larger conductivities correlated to less filtering and less of a signal transformation deeper water table depths lead to more signal transformation in saturated storage but are not correlated to filtering in unsaturated storage this is likely due to counteracting effects of higher conductivity which decreases filtering and deeper water table depths which increase filtering data availability data will be made available on request 1 introduction watersheds can be conceptualized as a series of filters that damp and attenuate climatic signals precipitation signals i e watershed inputs are often considered to be white noise delworth manabe 1988 katul et al 2007 however the resulting watershed outflow time series are more organized with greater memory and decreased noise input signals get progressively filtered as they move through the watershed system resulting in decreased high frequency variability moving downstream along a river or working down into the subsurface li zhang 2007 matsoukas et al 2000 yang et al 2018 while high frequency filtering is a well observed phenomenon delworth manabe 1988 katul et al 2007 xiuyu liang et al 2016 the watershed characteristics which control the properties of this filtering are less well understood and often difficult to parse out in real world complex systems sauquet et al 2008 previous work has utilized both numerical and observational data to demonstrate this filtering katul et al 2007 li zhang 2007 matsoukas et al 2000 pandey et al 1998 yang et al 2018 zhang yang 2010 however considering integrated groundwater behavior with numerical simulations has not yet been quantitatively explored as noted above filtering occurs progressively as signals propagate through watersheds resulting in a spectral shift towards low frequency variability streamflow responds almost immediately to precipitation and as a result precipitation generated runoff has a greater variance and is more representative of the climatic input than other hydrologic signals still filtering and signal transformations do occur as this precipitation works its way through the stream network and slow processes like baseflow further smooth the streamflow time series as a result streamflow still exhibits filtering of the high frequency variability and longer memory than precipitation gall et al 2013 guan et al 2011 pandey et al 1998 sauquet et al 2008 tessier et al 1996 the subsurface is one of the most significant sources of damping in the system it has been demonstrated that the white noise precipitation signal gets reddened as the high frequencies are damped even in shallow soil moisture time series delworth manabe 1988 katul et al 2007 vinnikov et al 1996 as we move deeper into soil profile the soil moisture variability and correlation with climatic signal decreases indicating greater buffering as high frequency fluctuations are damped amenu et al 2005 entin et al 2000 manfreda 2007 wu et al 2002 it has been shown that the decay time scales time for a signal to decrease to 1 e its original signal of soil moisture as well as the temporal scaling both increase with depth in the subsurface wu dickinson 2004 yang et al 2018 moving deeper into the subsurface the temporal scaling continues to increase with depth yang et al 2018 and subsurface acts as a fractal filter li zhang 2007 yang et al 2018 zhang schilling 2005 while the general trends of increased filtering and reddening of the signal with depth are well established the impact of watershed characteristics on this behavior is less well understood many observational studies of streamflow have found that there is temporal scaling present in the streamflow spectra and that there exists a breakpoint or crossover point where the scaling exponent shifts gall et al 2013 guan et al 2011 pandey et al 1998 sauquet et al 2008 tessier et al 1996 however the value of the scaling exponent of streamflow and the frequency of the break point vary from study to study based on the temporal resolution of the data and the differences in the catchments studied some studies have found that basin size does not affect the spectral scaling of stream flow over a large spread of basin sizes pandey et al 1998 tessier et al 1996 while another study found the size of the drainage area does affect the scaling parameters özger et al 2013 similarly several studies have explored the controls of temporal scaling in groundwater condon and maxwell 2014 compared scaling responses of latent heat and water table depth to changes in irrigation they found both that water table depth exhibited more scaling than latent heat and that breaks in scaling for the water table depth depended on proximity to the river areas more closely connected to the river had breaks in scaling whereas other areas did not have scaling breaks zhang and li 2005 used a 1d transient model with both homogenous and heterogenous hydraulic conductivities to look at the temporal scaling of groundwater head they found that switching between the two subsurface configurations has little effect on the scaling factor of groundwater head in xiuyu liang and zhang 2013 a bounded unconfined aquifer was investigated by developing a theoretical formula for the input and output spectra of their domain they found that the characteristic time of an aquifer controls the scaling of the hydraulic head which is a function of both the drainage area hydraulic conductivity and specific yield related studies x liang zhang 2015 xiuyu liang et al 2016 found that while heterogeneity and boundary conditions affect the hydraulic head scaling the areal recharge has a more significant influence furthermore zhang and yang 2010 systematically altered hydraulic conductivity for a watershed simulation using modflow 2000 to explore connections between conductivity and temporal scaling in the subsurface they found that the influence of hydraulic conductivity on groundwater head scaling varied based on the scenario constant river stage vs varied river stage and homogeneous vs heterogeneous subsurface for the homogenous scenario with constant river stage it was found that as k increased the groundwater head spectra shifted down indicating more filtering occurred across all frequencies still few studies have systematically explored the impact of groundwater and groundwater surface water interactions on temporal scaling and memory across both surface and subsurface systems conceptually we expect groundwater configuration to influence streamflow scaling we know that water table configuration and groundwater fluxes are dependent on topography recharge rate and hydraulic conductivity freeze witherspoon 1967 gleeson manning 2008 gleeson et al 2011 haitjema mitchell bruker 2005 reed m maxwell et al 2016 tóth 1963 high recharge rates low hydraulic conductivities and shallow topography generally cause shallower water tables whereas low recharge rates high hydraulic conductivities and steep topography generally cause deep water table depths furthermore its established that shallow water tables lead to more local flow paths and deeper water tables lead to more regional flow paths freeze witherspoon 1967 gleeson manning 2008 haitjema mitchell bruker 2005 tóth 1963 and we would expect to see greater damping for longer flow paths than shorter flow paths li zhang 2007 yang et al 2018 due to the complexity heterogeneity and interconnectedness of watersheds the controlling characteristics of the temporal scaling in a watershed can be difficult to isolate this study builds upon this previous research by using a simple idealized tilted v watershed domain and a fully integrated hydrology model to quantify changes in temporal scaling as a function of both surface and subsurface watershed characteristics there are many interesting hydraulic components which could be analyzed in this research we elect to explore the scaling behaviors in streamflow soil moisture unsaturated and saturated groundwater storage and compare the dynamics of these system components as well as the varying impacts of watershed characteristics on their signal transformation and filtering properties 2 methods an ensemble of idealized simulations was designed to investigate the relationship between watershed properties signal filtering and memory the simulations were run using an idealized tilted v domain section 2 1 and an integrated physical hydrology model parflow section 2 2 adjustable parameters of the watershed such as hydraulic conductivity precipitation and topographic slopes section 2 3 were altered to develop an ensemble of simulations section 2 4 hydrologic responses from the simulations such as streamflow storages water table depth were calculated for each of the runs section 2 5 and the temporal dynamics of the simulations were then quantitatively evaluated using a range of spectral methods including power spectral density section 2 6 and transfer functions section 2 7 finally memory metrics were computed to compare the input precipitation time series with the resulting streamflow and storage time series section 2 8 2 1 baseline configuration in order to investigate the hydrologic controls of filtering we chose this simple watershed design so we could systematically manipulate the characteristics with a small number of parameters and because the tilted v domain is a commonly used benchmark problem in hydrology carlier et al 2018 s kollet et al 2017 reed m maxwell et al 2014 rahman et al 2019 the tilted v shown in fig 1 has two hillslopes of equal length and width 2000m with a river channel 40m running between the two hillslopes and is of uniform thickness 100m the domain was discretized into cells with lengths and widths of 40m dx dy 40m and thicknesses of 2 5m dz 2 5m the slope of the two hillslopes hs varied while the slope along the axis of the river river slope rs was constant at 0 01 the subsurface was kept homogenous for this study with the same subsurface properties used for both the channel and hillslope as has been implemented in previous tilted v studies s kollet et al 2017 rahman et al 2019 additionally for all simulations the porosity is 0 4 and the van genuchten parameters are alpha equal to 6 and n equal to 2 these additional parameters for the subsurface parameters can alter the domain s response however they were unchanged in this study 2 2 hydrologic model and boundary conditions all the simulations were run using the integrated physical hydrologic model parflow which solves three dimensional variably saturated groundwater flow and surface water flow the subsurface is simulated with richards equation and overland flow is simulated using the two dimensional kinematic wave equation the details of parflow are provided in ashby and falgout 1996 jones and woodward 2001 s j kollet and maxwell 2006 and reed m maxwell 2013 we chose to use an integrated hydrologic model because we want to explore the role of subsurface buffering on streamflow dynamics therefore it is important to select a hydrologic model which simulates lateral groundwater flow and captures the dynamics of groundwater surface water interactions with parflow there is a terrain following grid option which was implemented in this study with the assumption that our 100m domain is sufficiently deep for this configuration to not affect the results for our simulations the surface boundary conditions of the tilted v domain included is a free surface overland flow boundary condition in the subsurface no flow boundary conditions were implemented across the domain with the surface being the only outlet in the system parflow fully integrates the groundwater surface water systems with a free surface overland flow boundary condition this approach dynamically swaps to solving the overland flow equations when and wherever the pressure is greater than zero at the land surface this boundary condition allowing groundwater surface water interactions to evolve dynamically over the course of a simulation i e for streams to form and disappear depending on saturation levels tilted v domains similar to the one being used here have been thoroughly validated in previous model benchmarking and intercomparison studies s kollet et al 2017 reed m maxwell et al 2014 and are included in parflow s standard test suite 2 3 adjustable parameters the conditions outlined in section 2 1 and section 2 2 remained constant throughout this study and across all the simulations in the ensemble that was created three model parameters were adjusted as follows to better understand their influence of the power shift on the input precipitation signal 1 three hillslope hs values were used 0 02 0 05 and 0 08 these values are consistent with other hillslope or tilted v studies bearup et al 2016 reed m maxwell 2010 mikkelson et al 2013 2 hydraulic conductivity k values range from 0 5 to 15 m day this range was selected to span common literature values for k 3 five precipitation scalers pscale were tested 0 1 0 25 0 5 0 75 and 1 pscale values are applied by simply multiplying the entire precipitation time series by this value our approach treats the precipitation signal as an approximately white noise input signal the slope of the log log periodogram for this signal is 0 12 and our goal is to study its evolution through the watershed it should be noted that other processes like evapotranspiration and snowpack would also modify the input moisture signal to a watershed for the purposes of this study we don t consider this input signal variability though we focus on the ways in which topography and geology transform a white noise input our full ensemble includes 59 simulations and is outlined in table 1 we test every pscale and hs combination however we run transient simulations only for the k values that resulted in reasonable groundwater configurations in our initial spin up runs i e water table depths with minimal ponding occurring outside the river and where there was still sufficient water for a river to form 2 4 ensemble spin up protocol and transient simulations for each simulation the following spin up protocol was followed to initialize the groundwater configuration first we run 100 year simulations with a daily time step and a constant precipitation forcing equal to the long term average of precipitation to reach steady state groundwater configurations next using the water table configurations from the steady state simulations as the initial condition an additional 30 year transient spin up simulation was run with the first two years of the full precipitation time series being repeated 15 times also with a daily timestep after the transient spin ups were complete 95 year transient simulations were run utilizing a historically based precipitation time series applied uniformly over the domain here too all simulations are completed with a daily timestep the original precipitation time series is a 95 year daily record from rain gauge data from fredonia ny from the climate data online database through the noaa national centers for environmental information this dataset was selected because it had a continuous record of rainfall measurements for a sufficiently long record as previously discussed the precipitation time series was altered by multiplicative factors of 0 1 0 25 0 5 0 75 and 1 it is important to note that evapotranspiration was not modeled in this study as we used this precipitation time series to represent a white noise input signal 2 5 model output calculations streamflow storage and soil moisture from the parflow model outputs several time series and metrics were directly calculated from the parflow simulation gridded outputs of pressure and saturation the methods for each of these calculations are outlined individually streamflow for each daily timestep of the transient simulations the streamflow leaving the domain was calculated streamflow is the sum of any overland flow exiting the domain at any cell along the bottom edge y 0 with the overland flow formulation eq 1 1 q p 5 3 s y 1 2 s x 2 s y 2 n d x where q is the daily streamflow m3 d p is the ponded pressure head of the cell m sx and sy are the slopes in the x and y direction n is manning s coefficient and dx is the cell width in m storage the total storage m3 of the domain was calculated at each timestep as the sum of subsurface storage both saturated and unsaturated as well as surface storage surface storage found by multiplying the cell area dx dy by the pressure value for each cell with ponded water for the subsurface storage the saturation was multiplied by the porosity 0 4 and then by the cell volume dx dy dz to find the volume of incompressible storage in each cell subsurface storage is divided into several components for analysis soil moisture storage the storage in the top 2 5m of the domain i e just the top layer note that this can include both saturated and unsaturated grid cells saturated storage the storage in cells with a saturation value of 1 the location of these cells may change over the course of the simulation as the subsurface drains and fills thus there is not constant spatial definition here unsaturated storage the storage in cells with saturation values less than 1 water table depth wtd for each simulation the wtd was calculated for each timestep as the average distance from the surface to the water table for the entire domain these values were then averaged across the time series of the whole simulation so that there is a single wtd value for each simulation 2 6 power spectral density power spectral densities psds and power spectra are established methods to explore the temporal scaling behavior of physical and chemical watershed signals previous researchers have applied spectral methods to analyze the temporal scaling behavior of streamflow gall et al 2013 guan et al 2011 pandey et al 1998 sauquet et al 2008 tessier et al 1996 soil moisture amenu et al 2005 d odorico rodrı guez iturbe 2000 wu et al 2002 water table depth condon maxwell 2014 hydraulic head and groundwater well levels li zhang 2007 schilling zhang 2012 zhang li 2005 zhang schilling 2004 zhang yang 2010 latent heat condon maxwell 2014 little bloomfield 2010 chemical responses c j duffy gelhar 1985 christopher j duffy gelhar 1986 chloride concentrations kirchner et al 2000 and nitrogen concentrations zhang schilling 2005 essentially this approach consists of a fourier transform to shift time series to the frequency domain as illustrated in fig 2 the power spectral density psd represents the power of the signal present at each frequency and it squares the magnitude of a discrete fourier transform higher values in the power spectrum indicate that a given frequency is contributing more to the variance in the signal there are numerous methods to estimate the psd of a time series here the psd of the input precipitation and output streamflow unsaturated storage and saturated storage time series were generated utilizing the scipy signal periodogram function in python and applied the hann window in order for the psd to not be a function of the size of the storage values the psds were normalized by the maximum psd value so that each periodogram had a range of 0 to 1 however we did not normalize the areas under the curves 2 7 transfer functions transfer functions tf are a method to quantify the differences between two power spectral densities here we use this method to explore shifts in the temporal dynamics between the input precipitation signal and the simulated streamflow transfer functions have been applied in hydrologic context to analytical solutions of aquifer models of varying complexity gelhar 1974 russian et al 2013 and have also been used for parameter estimation pedretti et al 2016 traditionally transfer functions are derived from stationary linear systems this is of course not true for hydrologic systems however our goal is not to derive transfer functions that can fully describe the system rather we are simply using the transfer function as a method to investigate the controls of signal filtering our method of calculating transfer functions from the input psd and output psd is consistent with pedretti et al 2016 and schuite et al 2019 the transfer function is calculated as the ratio of the psd of the output signal to the input signal pedretti et al 2016 schuite et al 2019 as in eq 2 2 t f p s d o u t p u t p s d i n p u t where the output is the streamflow psd and the input is the precipitation psd for the same frequency range here the transfer function is first calculated across all frequencies in the psds and is then smoothed using a simple moving average following the approach of schuite et al 2019 here we also average the tf across specific period windows of interest 3 14d 14d 3m 3m 1y 1 5y 5 10y 10 30y to provide summary metrics of high and low frequency damping 2 8 soil moisture and total storage memory in addition to spectral methods which can provide information about individual timescales of variability memory metrics can provide an intuitive way to quantify overall signal persistence the memory of a hydrologic variable is generally a measure of how that variable is likely to evolve in time and the timescale at which an anomaly is likely to persist there are many ways to calculate memory for example calculating the time needed for an anomaly to dissipate using an autocorrelation calculation finding the mean persistence time or length of time the value spends over a certain threshold or calculating remaining precipitation fractions fp for this study the surface storage total storage and soil moisture storage memory were evaluated by calculating the fraction of precipitation anomalies fp that remain after a given time lag as presented by mccoll et al 2017 more specifically the remaining precipitation fraction is defined as the ratio of positive changes for a given storage time series e g soil moisture divided by the total amount of precipitation in this study fp was calculated for the time series of both soil moisture storage and total storage for a given time series f the calculation for fp is found using eq 3 for a storage moisture time series θ with t timesteps days 3 f p f i 1 ft δ θ i 0 t p t dt where δθ i is the positive change in the soil moisture δθ i 0 when there is no change or a negative change in storage dt is the frequency of the time series 1 once per day and the denominator is the total precipitation across the domain for the entire time series the sampling frequency or time lag i was altered by changing the number of days between the positive changes in soil moisture note that fp is dependent on the frequency that the time series is sampled which corresponds to the time lag that is being evaluated i e the value of i here we calculate fp over a range of memory sampling frequencies or time lags to explore how much of the signal is left at various time scales fp was calculated at the sampling frequencies of 1 2 3 5 7 10 14 30 60 and 90 day s for a memory sampling frequency of 1 day the δθ i is calculated between each day whereas for a memory sampling frequency of 7 days the δθ i value would only be calculated each 7 days we selected this approach based on the findings of mccoll et al 2017 who compared the fp method to various other memory metrics and demonstrated that many of the historically used methods such as autocorrelations with fixed time lag overestimate soil moisture memory additionally we could readily apply the remaining precipitation fraction method to other time series other than soil moisture such as total storage 3 results 3 1 general trends three representative cases are selected in fig 3 to illustrate a range of response behaviors in the surface and subsurface for a given precipitation input for every simulation in fig 3 there are increases in structure and reddening of the signal from the initial noisy precipitation input moving through the domain from streamflow to soil moisture unsaturated storage and finally saturated storage this illustrates the increased filtering that occurs with depth as signals move through the subsurface and more energy is removed in all three of the saturated storage signals shown in fig 3 hourly and daily variability is almost entirely absent the difference between the cases i e columns shown in fig 3 highlight the impact of watershed characteristics on the filtering process the left b e h k simulation has the lowest k value in fig 3 which causes the most energy to be removed in the subsurface and by the time the signal has reached the saturated storage the high frequency components of the input signal have been removed and results in the smoothest unsaturated and saturated storage signal additionally with this lower k value we would expect this system to be the most stable and which can be observed as there is an observed attenuation in the peak value between 60 and 80 years the center c f i l and right c g j m simulations are flatter and drier than the first column and both have hs values of 0 02 relative to the first case with hs 0 08 the right and left simulations of streamflow signals are similar however have very different subsurface signals the only difference between the center and right cases are their k values the center simulation has a high k value resulting in a very responsive streamflow signal whereas the right simulation has a more stable streamflow baseline with larger daily peaks from larger overland flow events when comparing the saturated storages both the center and right simulations have higher k values and therefore have more variability in the storage signals less energy is lost as it moves through the subsurface the raw time series of these select simulations illustrates qualitatively the impact that watershed properties can have on the degree of filtering and how it varies with depth to evaluate ensemble characteristics more quantitatively the memory of soil moisture surface storage unsaturated storage and saturated storage were all calculated using the fraction of precipitation fp mccoll et al 2017 at various sampling frequencies as the sampling frequency decreases i e the time scales increase the memory intuitively decreases consistent with the idea that signals are dampened out over time the ranges of fp for each sampling frequency across the entire ensemble of simulations are plotted in fig 4 the surface storage is the smallest storage body with the fastest response times its fp values are significantly smaller than the other parts of the system and decay to zero much more quickly the subsurface storages unsaturated saturated and soil moisture all have larger values for daily sampling frequencies and maintain memory much longer than the surface system memory is calculated based on the precipitation signal the unsaturated zone responds most quickly to this signal and has the greatest memory due to the long term correlation with the precipitation signal the soil moisture is the top 2 5m of the domain and can contain both saturated and unsaturated cells and therefore is in between the saturated and unsaturated storage memories the saturated storage memory drops off more quickly and is lower than the unsaturated and soil moisture memories this is consistent with fig 3 where we demonstrate significant dampening in the saturated storage component of the system also due to the larger role of lateral redistribution that happens with the saturate storage flow in the unsaturated zone is primarily vertical we expect for the direct correlation to precipitation to be weaker in this component of the system 3 2 connection between watershed properties and degree of filtering next we explore how watershed properties affect the temporal filtering properties using transfer functions as described in the methods section we are presenting the average transfer functions over a preselected set of time periods for streamflow unsaturated storage and saturated storage it is important to note that in these results transfer functions are used as a method to understand the signal transformation of the input signal that takes place transfer function values greater than one indicate a shift in power or signal transformation whereas transfer function less than one indicate filtering has occurred as is consistent with the analysis of schuite et al 2019 we showed that the cumulative power spectrum for streamflow is expected to asymptote at a value of one but other variables in our case saturated and unsaturated storage approach values greater than one base on previous studies such as yang et al 2018 filtering and memory increase as we move deeper into the subsurface as such relative to streamflow we expect a larger proportion of groundwater signal to be in the longer periods fig 5 shows the average transfer function i e portion of the signal that is maintained for the entire time series the values are colored by the ratio of the pscale over k this ratio was chosen because it reflects the degree of partitioning between runoff and infiltration recall the larger transfer functions indicate that less filtering has occurred and smaller transfer function values indicate more filtering or signal transformation has occurred for streamflow fig 5a steeper hillslopes lead to deeper wtd which in turn correlates to a larger unsaturated zone buffer and greater filtering fig 5a shows three distinct curves depending on the slope of the watershed that illustrate this relationship for a given water table depth steeper slopes result in less filtering additionally larger pscale over k ratios lead to more runoff and smaller values leading to more infiltration thus as the pscale over k ratio increases darker colors in fig 5a there is less filtering due to more of the precipitation generating runoff rather than infiltrating conversely for unsaturated storage fig 5b these relationships are flipped here there is a slight positive relationship between wtd and transfer function values indicating less filtering with deeper water table depths this relationship is much weaker than what was observed for streamflow though also we see a positive relationship between the pscale ratio and filtering i e less filtering when the pscale over k ratio is smaller this makes sense as precipitation partitioning will shift toward infiltration as the pscale over k ratio decreases leading to greater infiltration and a noisier signal in the subsurface finally for saturated storage fig 5c the degree of filtering is much greater than unsaturated or streamflow components roughly an order of magnitude higher as can be seen from the x axis ranges this is consistent with fig 3 that illustrated the significant filtering of the storage signal interestingly we do not see a relation to wtd for saturated storage once again the pscale over k ratio color gradient shows that as the ratio increases the filtering increases which is once again due to increasing infiltration as the ratio decreases to further investigate the controls of the filtering at different temporal scales the transfer functions for each metric are binned and averaged over periods of interest fig 6 shows the degree of filtering at six different period windows for the streamflow signals of the entire ensemble for streamflow less than 5 years fig 6a d there is a direct relation between wtd and the amount of filtering that we see deeper water tables lead to more filtering smaller transfer function values and shallower water tables lead to less filtering furthermore at these periods less than 5 years as the pscale to k ratio increases there is also a decrease in the amount of filtering this indicates that configurations with higher precipitation and lower k values have less filtering occurring and is consistent with increased runoff partitioning for infiltration excess overland flow when precipitation exceeds the infiltration rate controlled by k for longer periods greater than 5 years the relationship between water table depth and degree of signal transformation is less clear note that the tf values especially for the periods over 10 years for are often greater than one indicating the power shift rather than true filtering taking place at these longer timescales there is stronger connection to precipitation here with higher precipitation correlating to less filtering i e a noisier streamflow signal the same period windows are used in fig 7 for the unsaturated storage transfer function averages however the points are shaded by their k values as opposed to the pscale k ratio because stronger relationships were found for k in this part of the system for the shorter periods less than 3 months fig 7a b there is significant filtering and a very small portion of the original signal remains less than 1 nearly all the power at these periods is removed from the signal as it is filtered through the subsurface generally for all period windows filtering decreases i e transfer function values increase as k increases this makes sense because higher k values allow for greater infiltration and for the signal to transfer more quickly through the subsurface looking at the periods greater than 3 months there is still a positive inverse relationship between k and the degree of filtering or signal transformation taking place as with the previous plots steeper slopes lead to deeper water tables there is not a clear relationship between water table depth and filtering for periods less than one year however for a given water table higher k values result in less filtering for the longest periods fig 7d e f we can again see distinct trends emerging between water table depth and degree of signal transformation for a given slope value the deeper the water table leads to greater tf values this seems counterintuitive as we would expect to see greater filtering with depth however wtd is also a function of k and indeed we see a positive correlation in the coloring with higher k values for the deeper wtd of a given slope here we are likely seeing the nonlinear impacts of increasing k in the unsaturated zone which is controlling the signal transformation and filtering finally fig 8 shows the degree of filtering or the power shift taking place over the six period windows for the saturated storage component of the domain similar to the unsaturated storage in fig 7 the shorter periods experience significant filtering and nearly all the power at these periods is removed from the signal less than 1 remains however since the signal must move through more of the subsurface before reaching the saturated storage and given the dampening impacts of lateral flow even more filtering and damping occurs for this reason the saturated transfer function values are smaller than the unsaturated storage value at all time scales and the periods of significant damping includes all periods less than 1 year fig 8 a c note that for unsaturated storage this was only up to 3 months focusing on the relationships for periods greater than 1 year once again the filtering is inversely correlated to the k values simulations with the high k values exhibit the least filtering across all periods in the saturated storage once again the slopes are still positively correlated to the wtd and for a given wtd the k value determines the degree of filtering interestingly though for a given slope and k value e g the red squares or the purple dots in fig 8e f we now see an inverse relationship between wtd and tf indicating more filtering or more of a transformation with deeper water table depths this is consistent with the idea of increased filtering and more of a power shift in the subsurface with depth as noted above we hypothesize that the nonlinear relationship between saturation and hydraulic conductivity in the unsaturated zone increases the relative importance of k leading to decreased filtering with increased depths figs 6 8 show the controls of the filtering and signal transformations for the entire ensemble to further illustrate how the watershed parameters affect the signal transformation behavior representative cross sections of the ensemble were selected where only a single variable is altered in fig 9 only the k values change over an order of magnitude as the pscale and hs values are held constant for the streamflow fig 9a higher k values lead to more filtering with the strongest relationship occurring for periods less than 5 years for the longest period 10 30y this relation flips and higher k values lead to less filtering for periods 10 30 years conversely for both unsaturated storage fig 9b and saturated storage fig 9c the relation is consistent across nearly all periods with higher k values leading to higher tf values these findings are consistent with what was discussed in figs 7 and 8 and the basic physical controls of these system components at the land surface higher k values lead to more infiltration less overland flow and a larger baseflow component i e greater filtering whereas in the subsurface the k value controls the flux rates fig 10 shows a different cross section where the k and hs values are held constant and the pscale values change as shown here both streamflow and saturated storage have inverse relations between the amount of precipitation and filtering as the precipitation increases there is less filtering larger transfer function across all the scales and for all pscale values interestingly for unsaturated storage fig 10b this is true for pscale values 0 5 after which the trend reverses and there is actually more filtering for larger pscale values for hs there was a larger variety of trends across the cross sections of the full ensemble and a single representative trend was not possible 4 discussion previous research has demonstrated that filtering and damping of the precipitation signal increases moving deeper into the subsurface li zhang 2007 yang et al 2018 here we explore the way that watershed characteristics control the degree of signal transformation moving from streamflow to unsaturated and saturated storage through a series of controlled numerical experiments it should be noted that the storages which we investigated are not independent variables and the dynamics of saturated storage depends on infiltration recharge and baseflow our goal in this study is not to fully attribute all sources of variability but rather to understand the interactions between subsurface storage dynamics and streamflow in order to achieve well controlled numerical experiments numerous simplifications were made in our models first we used a real world precipitation and with this input signal all of the precipitation adjustments are scalers we did not explore changing precipitation variability and instead focus on the ways in which topography and geology transform a white noise input however changes in the temporal distribution of rainfall events and not just their magnitude would alter the driving frequencies of the domain and could influence our results we also assumed a homogenous and idealized domain to limit the topographic and hydrogeologic complexity more complex watersheds configurations would impart additional temporal structure through the close interactions between groundwater configuration geology and topography we also excluded evapotranspiration from our simulations this could also be spatially variably depending on land cover type however we recognize that evapotranspiration is an important hydrologic control especially for real world studies furthermore these simulations were run at a daily scale so that we could run sufficiently long simulations to analyze the long term trends however running at this temporal resolution limits our ability to analyze short term phenomena like pressure propagation finally the baseline configuration and boundary conditions and resolutions also are considerations that would alter the results as stated in the methods section 2 2 no flow boundary conditions were set in the subsurface using these boundary conditions our water tables might be shallower than if another boundary condition was used and then alter the streamflow responses of the simulations furthermore in this study some of the lowest wtd simulations resulted in considerable excess ponding outside of the river channels one area where excess ponding could affect our results is in the fp of saturated memory the ensemble member with the highest saturated storage fp at the daily scale had considerable ponding without this member or runs with similar ponding we d expect the range of saturated storage fp to be reduced even in this relatively simple system the range of controls is complex and varies by the temporal scale looking at individual parameters k pscale or hs can sometimes explain the filtering behaviors and signal transformations of the system but a combination of these parameters better encapsulates the physical processes at work such as wtd or pscale k table 2 provides a conceptual summary of the short term and long term signal transformation and filtering controls based on the results in figs 6 8 for streamflow the filtering controls vary depending on the period length for short periods 3d 5y we show that as the wtd increases the degree of filtering increases this is consistent with a deeper and more filtered baseflow component with deeper water table depths the other key control for these periods is the pscale k ratio which determines the amount of infiltration versus overland flow we would expect to see with higher pscale k values there is more overland flow and less infiltration leading to less filtering fig 6 for longer term streamflow filtering greater than 5 years wtd and pscale k ratio are no longer the key controls of filtering rather here we see a positive relationship between filtering and k indicating that it is instead how readily the precipitation signal can propagate through the surface that controls the degree of filtering higher k values allow for less energy to be lost in the subsurface and less filtering of the precipitation signal furthermore we see less filtering with the higher pscale values as the increased precipitation leads to larger gradients in the subsurface which maintain more of the signal the changes in relationship seen with longer time scales are likely due to the increased importance of baseflow dynamics in determining low frequency streamflow variability the increased filtering behavior at shorter timescales is consistent with the findings of li and zhang 2007 they looked at how the scaling of different hydrological signals rainfall streamflow groundwater baseflow compared and then also analyzed how they changed at different timescales they similarly found that the scaling increased as the subsurface buffer increased and that this trend could be attributed to the dampening of the signal additionally they found that the scaling decreases at longer timescales while they considered significantly smaller time scales less than 1 year and did not directly analyze the unsaturated zone the general findings are consistent with our results for both saturated and unsaturated storage nearly all of the signal is removed for the shortest periods 3d 3m this agrees with previous findings of yang et al 2018 that the greatest signal filtering occurs in the unsaturated zone at the longer scales k and slope appear to be the primary controls this is consistent with the drivers of infiltration k determines the amount of energy lost as water passes through the subsurface furthermore the impact of k can be amplified in the unsaturated zone where k varies nonlinearly as a function of saturation hs has an inverse relation with the degree of filtering as steeper slopes lead to higher gradients the primary role of k and hs in the unsaturated zone indicates that filtering here is a function of how quickly the signal moves through the subsurface increased k and steeper gradients both allow the signal to move more quickly and experience less filtering perhaps counterintuitively though it should be noted that higher k s are also correlated with deeper water table depths and therefore a deeper thicker unsaturated zone we also find decreased filtering with increased water table depth in this part of the system this would seem to contradict the logic of more filtering happening with more time spent in the subsurface as you move deeper we hypothesize that this relationship occurs because the nonlinear relationship between k and saturation has a stronger impact on the filtering than wtd close inspection of fig 7 shows that for a constant k and slope values there is actually more filtering with deeper water tables similar to unsaturated storage saturated storage also exhibits nearly complete filtering for short time periods which was expected as it is well established that there is increased filtering with depth an important difference for saturated storage is that the periods of nearly complete filtering extend further to 3d 1y once again in the subsurface k is a key control and as k increases less energy is lost and the signal can more readily move through the subsurface and less filtering occurs for saturated storage neither wtd pscale k nor hs have a strong influence on the signal filtering but pscale does have some influence unlike the unsaturated storage the deeper wtd of the same k shows more filtering for saturated storage as was shown in fig 8 as for pscale as it increases the system is transmitting more water and the larger fluxes lead to larger gradients which in turn leads to high velocities and more noise can make it through the system like the unsaturated storage it seems the wtd is not as important as the time spent in the subsurface 5 conclusions in this study the temporal dynamics of streamflow soil moisture and groundwater storage are simulated with an integrated hydrologic model and through the use of spectral methods the degree of filtering and signal transformation and how the transformation evolves in time is examined and trends at the short and long term timescales consistent with previous work we see persistent damping and attenuation as precipitation moves through the watershed with the largest filtering occurring in the deepest parts of the subsurface especially at the higher frequencies our work extends beyond previous studies to evaluate the connection between watershed characteristics and temporal filtering across times scales and at different parts of the system overall we show that filtering and power shift between frequencies is controlled by the amount of time spent in the subsurface and the degree of groundwater surface water exchanges for the streamflow signal the short term and long term controls vary by time period and represent the two different streamflow generation mechanisms runoff and baseflow for short periods the primary controls of filtering are those most closely linked to runoff generation i e pscale k ratio and wtd which can influence infiltration excess and saturation excess overland flow at longer periods the signal transformation is more strongly correlated to variables that control baseflow we found that the higher k and higher pscale values causes less filtering higher k values lead to faster signal transmission in the subsurface and larger pscale values increase the amplitude of the total signal in the subsurface in the subsurface we found that both the saturated and unsaturated zones have significant filtering at the shorter timescales and nearly all the high frequency variability is removed additionally across all time scales k was the primary control of the subsurface filtering however there were some differences between the saturated and unsaturated zone and their filtering responses to pscale and wtd specifically there is more filtering in the saturated zone and for a wider range of timescales furthermore the two storages show different signal transformation responses to wtd with unsaturated storage showing less filtering for deeper wtd and saturated storages showing more filtering for increases in wtd this work highlights the complexity of temporal filtering and signal transformations across hydrologic systems we demonstrate differences in the physical controls across time scales and consider different components of the system additional studies looking at a wider range of watershed variables and their influence on filtering is needed for a better understanding on how hydrologic variables control signal filtering this study attempts to minimize the complexities of the hydrologic system with an idealized watershed in order to learn about the controls in real world watersheds these analyses should be applied to more complex and differing watersheds finally this research did not implement particle tracking software coupling similar research to particle tracking could provide additional information about how these watershed variables alter the filtering ability of the subsurface credit authorship contribution statement abram farley software formal analysis investigation data curation visualization writing original draft laura e condon conceptualization methodology writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the national science foundation nsf award number 1835794 supported this research additionally all of the simulations and model runs were completed on the university of arizona s high performance computing uahpc resources 
6,in dam break problems two dimensional 2d models on the vertical plane appropriately capture the non hydrostatic pressure distribution but with a high computational cost this study presents a hybrid lagrangian solver of one dimensional 1d shallow water flow and a 2d large eddy simulation with a sub particle scale to simulate dam break flows over frictional beds the proposed 1d 2d hybrid scheme is a useful trade off between the accuracy of the 2d models on the vertical plane and the computational efficiency of 1d shallow water equations models the configuration of sub domains is established so that the 1d solves the regions with dominantly hydrostatic pressure while the 2d solver accounts for the non hydrostaticity a moving coupling method is introduced based on two layer shallow water equations with a uniform velocity distribution that constructs two sub domains including inner and outer zones in the section coupling model scm or upper and lower layers within the layer coupling model lcm then the region with high vertical accelerations is substituted by the 2d model in the scm or mixed scm lcm in which the overlapping zone and boundary interface move with time based on the 1d flow feature also a new method is proposed to deal with irregular boundary interfaces in the lcm while satisfying the momentum transfer the present hybrid model is advantageous as it meets mass conservation and there is no need for typical inflow outflow boundary conditions at the coupling interfaces for simulating dam break waves moving particle simulation mps is utilized to solve the lagrangian equations which is advantageous in dealing with free surface problems with large deformation the model validation reveals that the hybrid results of water depths are in very good agreement with observed data and those obtained from the 2d model for selected dam break flows with the ratio of initial depths downstream to upstream up to 0 4 in this respect the average root mean square error rmse is computed at 8 09 mm in the scm 8 16 mm in the mixed scm lcm and 7 48 mm in the pure 2d model while both hybrid models save the computational time by 55 compared to the 2d model moreover the mixed scm lcm preserves the nonlinearity of the wavefront over long times whereas the scm converts the wavefront into a shockwave considering the same number of fluid particles keywords hybrid 1d 2d numerical model dam break two layer shallow water large eddy simulation section coupling model layer coupling model irregular boundary interface 1 introduction dam break flow is a free surface unsteady flow resulting from the sudden removal of a vertical barrier of a reservoir which is basically a two dimensional problem and the downstream bottom is often dry in practical applications stansby et al 1998 the problem has commonly titled bore propagation in case the dam break wave propagates on a wet bed khayyer and gotoh 2010 wüthrich et al 2018 notably rapid continuous and discontinuous waves related to the dam break problem i e rarefaction and shock waves respectively castro orgaz and hager 2019 are not restricted merely to structural failure but also the rapid operation of hydraulic control structures cozzolino et al 2015 hashemy et al 2013 ritter 1892 and stoker 1957 pioneered analytical solutions by solving de saint venant equations also named one dimensional shallow water equations swe for frictionless dam break flows over horizontal dry and wet beds respectively however the analytical solutions are inconsistent with the experimental data at the early dam break stages due to the hydrodynamic pressure effect and demonstrate a two dimensional 2d behaviour hu et al 2018 ozmen cagatay and kocaman 2010 stansby et al 1998 regarding 2d dam break numerical modelling on the vertical plane stansby et al 1998 proposed an innovative solver based on the potential flow solutions at the initial stages they state that the wavefront profile mimics a horizontal jet for the ratio of initial depths downstream hd to upstream h 0 equal to zero i e r hd h 0 0 in contrast a vertical mushroom like jet occurs for r up to 0 4 mohapatra et al 1999 introduced a finite difference solver to simulate 2d dam break flows based on the euler equations and a free slip boundary condition and reported that the non hydrostatic pressure is significant in the initial stages while the pressure shows a hydrostatic distribution in the long term simulation although the aforesaid 2d models ignore the effect of shear stresses they provide valuable insights into understanding the impact of the non hydrostatic pressure of the dam break problem in addition to the hydrodynamic pressure the bed friction and turbulence cannot be omitted at the initial instants of the dam collapse which is the main reason for wave breaking castro orgaz and chanson 2017 jánosi et al 2004 conducted extensive laboratory experiments on dam break flows they reported that the turbulence effect is vigorous at the initial stages where the flow is very fast mainly the turbulence impact is more significant for wet bed cases as the downstream depth tends to obstruct the flow in general the most reliable tools to include fully non hydrostatic pressure and shear stresses in the dam break simulations are reynolds averaged navier stokes rans equations and large eddy simulation les coupled with a turbulence closure problem to model turbulence the turbulence models comprise k ε and k ω in rans and sub grid scale sgs in les or sub particle scale sps in meshless les methods gotoh et al 2001 which have been widely utilized in the literature for dam break problems using mesh based khoshkonesh et al 2019 ozmen cagatay and kocaman 2010 shigematsu et al 2004 wang et al 2020 and meshfree fu and jin 2014 leroy et al 2016 meringolo et al 2019 shao and gotoh 2005 numerical methods the no slip boundary condition usually is used for open channel beds pope 2000 on the other hand there is a possibility to enhance swe type models for long waves weakly dispersive by accounting for the non hydrostaticity in the system of equations referred to as boussinesq type models cantero chinchilla et al 2019 by doing this the propagation of surges can be reasonably tracked with considerably low computational time in large scale cases without resorting to the 2d approaches castro orgaz and chanson 2021 in this respect weakly dispersive waves are obtained from adding non hydrostatic terms to de saint venant equations that are classified into weakly and fully nonlinear boussinesq models according to the type of prescribed velocity and pressure profiles cantero chinchilla et al 2016 castro orgaz and hager 2017 comprehensively described boussinesq type models in the literature and stated that the most practical such models are fully nonlinear procedures including serre green naghdi sgn equations green and naghdi 1976 serre 1953 and vertically averaged and moment vam equations khan and steffler 1996 steffler and jin 1993 the advantage of vam to sgn is that the system of equations does not include higher order derivatives the frictional effect is considered and there is no need to introduce an empirical formula to mimic wave breaking unlike sgn equations cantero chinchilla et al 2020a cantero chinchilla et al 2018 in the last decade hybrid models have become popular in the hydrodynamic modelling of free surface flows in which hydrostatic swes simulate flows in regions neglecting vertical accelerations while boussinesq type or navier stokes ns equations are used in zones where the non hydrostatic effects are relevant as regards the boussinsq type flow castro orgaz and chanson 2020 introduced a hybrid model of sgn swe so that the sgn equations simulate undular waves whereas the swes transform the broken wave into a shockwave they reported the hybrid solver efficiency even for wave breaking in hybrid sw ns equations models the most common coupling method is accomplished by an interface dividing the solution domain longitudinally into sub models and transferring the flow information along the wave direction referred to as the section coupling model scm ni et al 2020 this technique has been pioneered to simulate solitary wave propagation using finite difference pringle et al 2016 and finite element takase et al 2016 solvers by applying the hydrostatic sub model to offshore areas while the non hydrostatic sub model simulates runup over sloping beaches and submerged breakwaters also the flow information is transferred by assigning a fixed interface region between the two constituent sub models without defining open boundary conditions at the interface in addition the hybrid sw ns models were used to simulate dam break flows using a finite difference model mintgen and manhart 2018 as well as the smoothed particle hydrodynamics sph ni et al 2020 where the transfer of flow information is achieved via inflow outflow boundary conditions that are re evaluated according to subcritical and supercritical flow regimes at each time step other coupling algorithms are also available for fully 2d lagrangian eulerian hybrid solvers of free surface flows in this respect the layer coupling model lcm splits the domain into two regions located above and below an interface and flow information is transferred transversely using a fixed boundary interface for example marrone et al 2016 introduced a 2d hybrid solver in which the sph tracks the free surface with large front deformation in the upper layer while the finite volume fv sub model deals with the lower shear flow moreover the coupling settings can be performed through a bi directional system by means of fixed longitudinal and vertical interfaces referred to as a mixed scm lcm typically the sph model simulates the free surface flow whereas the fv scheme simulates the near wall flow chiron et al 2018 napoli et al 2016 the main objective of the present study is to propose a hybrid sw ns equations based solver to simulate dam break flows using moving particle simulation mps as a mesh free numerical scheme the mps was initially introduced by koshizuka and oka 1996 and later employed in free surface problems such as dam break waves over fixed khayyer and gotoh 2010 and movable beds shakibaeinia and jin 2011 swash morphodynamics harada et al 2019a 2019b multiphase flows fu and jin 2016 khayyer et al 2019a shakibaeinia and jin 2012 granular flows jandaghian et al 2021 xu and jin 2021 ice wave dynamics junior et al 2021 fluid structure interactions khayyer et al 2021 2019b and shallow water flows sarkhosh and jin 2022 2021 the proposed hybrid solver applies the les method and the sub particle scale sps turbulence model to the 2d sub domain as they agree well with experimental data for 2d dam break flows larocque et al 2013 wang et al 2020 the present work is novel as a straightforward 1d two layer shallow water method provides boundary conditions for the 2d sub domain and can constitute inner and outer zones in the section by section setup or upper and lower layers within the layer by layer arrangement this is advantageous to transfer the flow information in the overlapping zone through a moving scm without resorting to typical fixed inflow outflow boundary conditions in the existing hybrid 1d 2d solvers for dam break waves as the movement of the inner zone of the 1d sub model automatically determines the moving scm moreover a moving lcm is introduced to reduce the computational cost by prescribing a known velocity profile within the low cost 1d sub layer unlike the existing lcms in which fully 2d sub models are placed at the upper and lower areas of a fixed boundary interface in the results section the model verification is conducted to ensure that the moving coupling method is performed appropriately the hybrid solver is then validated and applied to simulate various frictional dam break problems 2 governing equations the proposed hybrid scheme incorporates shallow water and navier stokes equations for dam break flow modelling in regions with hydrostatic and non hydrostatic pressure distributions respectively the corresponding governing equations are described in the following sections 2 1 two layer shallow water flow with uniform velocity distribution if the shear stress between a two layer shallow water flow is neglected the two layer 1d swe is simplified to three equations with a uniform velocity distribution chen et al 2007 fig 1 shows the sketch of a two layer dam break problem in which the sublayer depth is involved in defining the interface of the moving coupling model the velocity distribution is uniform on the whole depth for an inviscid two layer dam break flow as a result the continuity and momentum equations are adopted for a single layer in addition to an advection equation that contains the continuity equation for calculating the sublayer water depth in the following lagrangian framework 1 d h d t h u x 2 d u d t g h z x g s f 3 d d t h 1 h 0 where d dt is the total derivative t u x u is the shallow water flow velocity m s h is the water depth m h 1 is the sublayer water depth m see fig 1 z is the bed elevation m g is the gravitational acceleration m s2 x m and t s are spatial and temporal independent variables respectively in the lagrangian approach the particle position is tracked by dx dt u the bed friction sf is obtained from the manning formula assuming a wide rectangular channel 4 s f u u n m 2 h 4 3 where nm is the manning coefficient m 1 3s 2 2 large eddy simulation with sub particle scale turbulence model the les mass and momentum conservation equations derive a convolution filter applied to the unsteady navier stokes equations for an incompressible fluid flow lubin et al 2010 the so called spatial filtering approach which removes small scale eddies in the following lagrangian framework gotoh et al 2004 5 u 0 o r 1 ρ d ρ d t 0 6 d u d t 1 ρ p g ν 0 2 u 1 ρ τ where the bar symbol indicates particle scale variables d dt is the total derivative equal to t u u the velocity vector m s the gradient operator ρ the fluid density kg m3 p the pressure pa g the gravitational acceleration vector m s2 ν0 the laminar kinematic viscosity m2 s and 2 the laplacian operator also the particle position is obtained from the movement of particles d r d t u where r is the position vector m in eq 6 τ ρ u u is a sub particle scale sps turbulent stress tensor and captures small scale eddies with each element characterized by 7 τ i j ρ u i u j ρ u i u j u i u j where the prime symbol refers to sub particle scale components following the boussinesq 1877 hypothesis of the turbulence eddy viscosity the sps turbulent components are defined as a linear function of the mean velocity gradients therefore 8 τ ρ 2 ν t s 2 3 k i where ν t turbulent eddy viscosity i the 2 2 identity matrix and s the sps mean rate of strain tensor with each element is defined by 9 s i j 1 2 u i x j u j x i in eq 8 k u i u i 2 t r τ ρ is the sps turbulent kinetic energy which can be incorporated into the pressure term for computational ease i e p p 2 ρ k 3 fu and jin 2013 shao and gotoh 2005 in the present study a compound of the smagorinsky model and the mixing length theory is utilized that is applicable to open channel flows bradbrook et al 2000 gotoh et al 2004 10 ν t min c s δ k d 2 s where cs is the smagorinsky constant usually varying from 0 1 to 0 15 kazemi et al 2017 δ the filter size that is equal to initial particle size dl2d k the von karman constant 0 4 d the normal distance to the closest wall and s the magnitude of the mean rate of strain tensor defined by 11 s 2 s i j s i j 1 2 gotoh et al 2004 state that the first term of eq 10 in the round bracket stands for the flow far away from solid boundaries while the second term dominates near bed flows they also suggest that the filter size δ 0 01 m is enough spatial resolution to resolve the turbulent kinetic energy in the experimental studies of waves moreover cs 0 1 is suitable for mps simulating dam break problems shao and gotoh 2005 in addition to the turbulence model the bed roughness effect is paramount in open channel flows this study adopts the mirror particles technique colagrossi and landrini 2003 garoosi and shakibaeinia 2020 lee et al 2011 marrone et al 2011 with modifications to account for the no slip boundary and introduces wall function treatment for different coupling models as discussed in section 4 3 mps spatial integration the following sections describe the mps spatial discretization for the sub models in mps partial differential equations are spatially integrated via a weighted average of flow variables derivatives over a support domain unlike the sph which differentiates the weighted average function without operating differencing schemes in the flow variables koshizuka et al 2018 the mps swe is the solver for shallow water flows sarkhosh and jin 2021 a weakly compressible mps scheme wc mps shakibaeinia and jin 2009 is adopted to solve the navier stokes equations 3 1 mps swe in mesh free swe solvers the water particles are interpreted as water columns the interaction of each target particle with neighbouring ones is traced within the support domain identified through the following relationship xia et al 2013 12 r e i 1 d r e i 0 1 d h i 0 h i where re is the searching radius which varies with time and space for each water particle wang and shen 1999 subscript i and superscript 0 denote the target particle and initial time respectively also r e i 0 1 d α 1 d d l 1 d is the initial searching radius where dl1d is the initial particle size and α1d is the searching radius parameter equal to 1 3 in this study as calibrated in the literature sarkhosh and jin 2021 the water depth is calculated using the algebraic density ratio formula introduced by sarkhosh and jin 2021 instead of the partial differential continuity equation since the mass is automatically satisfied 13 h i h i 0 n 0 1 d n i 1 d where n1d is unknown and defined as the one dimensional particle number density it is defined as the number of particles around a target particle and is calculated using the smoothing kernel function w1d 14 n i 1 d j i n w i j 1 d w i j 1 d w 1 d x i j r e i 1 d where j denotes neighbouring particles and xij the absolute value of the distance between the target particle and adjacent particles xij xj xi the spiky type kernel function is adopted in the mps swe sub model since it produces a robust repulsive force fredini and limache 2013 it is defined in the following one dimensional setting considering the normalization factor 1 8 r e i 1 d to satisfy the unity condition monaghan and lattanzio 1985 15 w i j 1 d 1 8 r e i 1 d 2 x i j r e i 1 d 3 0 x i j r e i 1 d 2 0 x i j r e i 1 d 2 notably the spatial derivative is applied to the water surface elevation in eq 2 h obtained from 16 h x i 1 n i 1 d j i n h j h i x i j 2 x j x i w i j 1 d 1 n i 1 d j i n f i j d s w i j 1 d where the hat symbol stands for the minimum value among the target and adjacent particles in the support domain to provide interparticle repulsive forces in the gradient operator koshizuka et al 1998 in addition f i j d s is the one dimensional dynamic stabilizing force inspired by the pioneering dynamic stabilizer tsuruta et al 2013 imposed from adjacent particles on the target particle to satisfy the linear momentum conservation 17 f i j d s u i j d s g δ t u i j d s 0 x i j 0 5 δ l i δ l j 0 else u i j d s u i h i u i n i 1 d h j u j n j 1 d h i n i 1 d h j n j 1 d x j x i x i j where u i j d s is the dynamic stabilizing velocity calculated from the theory of the inelastic collision with unequal masses and δli the particle length equal to 1 n i 1 d the velocity gradient is computed by koshizuka et al 2018 18 u x i 1 n i 1 d j i n u j u i x i j 2 x j x i w i j 1 d finally the advection equation eq 3 is simplified to an algebraic formula under the lagrangian framework for calculating the sub layer water depth 19 d d t h 1 h 0 h 1 i h 1 i 0 h 0 h i 3 2 wc mps in wc mps the particle number density is given by 20 n i 2 d j i n w i j 2 d w i j 2 d w 2 d r i j r e 2 d where rij r ij r j r i unlike the mps swe the searching radius re 2d is constant in the wc mps since a small density change is allowed in the weakly compressible mps and determined by re 2d α2d dl 2d and α2d 3 1 as recommended by koshizuka et al 2018 unlike the mps swe the spiky type kernel function adopted for the wc mps is free from any normalization factor as given by koshizuka et al 2018 shakibaeinia and jin 2009 shakibaeinia and jin 2009 21 w i j 2 d 1 r i j r e 2 d 3 0 r i j r e 2 d 1 0 r i j r e 2 d 1 the gradient correction scheme introduced by khayyer and gotoh 2011 is adopted for the pressure gradient which is taylor series consistent and unconditionally first order accurate 22 p i d s n 0 2 d j i n p j p i r i j 2 r i j c i w i j 2 d c i d s n 0 2 d j i n r i j r i j r i j 2 w i j 2 d 1 where ds is the space dimension equal to 2 for a two dimensional setting and ci is the corrective matrix khayyer and gotoh 2013 2011 state that eq 22 cannot produce repulsive interparticle forces attributed to anti symmetric particle distribution to solve this problem one option is the application of conditional collision force ccf together with the stabilization technique st xu and jin 2019 for this purpose ccf is incorporated into the pressure gradient to satisfy the momentum conservation formulated as 23 f i c c f 1 n 0 2 d j i n f i j c c f w i j 2 d f i j c c f ρ i ξ i j u i j δ t e i j r i j d c o l l u i j 0 0 else u i j u i ρ i u i ρ j u j ρ i ρ j e i j e i j r i j r i j where f i j c c f is the collision force vector f i c c f the vector of the weighted average of the collision force for each target particle u i j the relative velocity after the pairwise collision e ij the unit vector of r ij and dcoll the collision distance which usually sets 0 9dl 2d moreover ξ ij dcoll rij dcoll is a coefficient to minimize the impact of the collision model on energy conservation as xu 2021 recommended moreover the st modifies the velocities at the end of each time step during the calculation given by 24 u i s t u i δ u i δ u i δ r i 1 n 0 2 d j i n u i j w i j 2 d r i j where δri 0 01dl 2d based on the compressibility of the fluid less than 1 moreover the original divergence and laplacian mps operators are employed given by 25 u i d s n 0 2 d j i n u i j r i j 2 r i j w i j 2 d 26 2 φ i 2 d s λ n 0 2 d j i n φ i j w i j 2 d λ j i n r i j 2 w i j 2 d j i n w i j 2 d where φ is the particle scale flow variable the turbulent stress tensor in eq 6 is also integrated in a similar way to the mps divergence operator gotoh et al 2004 4 coupling models the neumann type boundary condition is imposed on the upstream and downstream solid walls by defining four layers of ghost particles so that the pressure value and free surface elevation are first calculated for the closest ghost particle named the wall particle which is later assigned to the other ghost particles this procedure is advantageous in reducing the searching radius and computational time as stated by koshizuka et al 2018 and sarkhosh and jin 2021 moreover the dirichlet boundary condition is applied to the free surface of the 2d sub domain by assigning zero pressure to the fluid particles in case n βn 0 β 0 97 koshizuka et al 2018 also the free slip boundary condition is prescribed on the internal vertical separation sections in the 2d sub domain for the velocity vectors in the present work two coupling methods are developed scm as shown in figs 2 a and 2b and mixed scm lcm which is also known as the bi directional coupling model as depicted in fig 2c as regards the first single and two moving overlapping zones provide vertical boundary conditions for the 2d sub domain in dry and wet bed conditions respectively in the mixed scm lcm a moving boundary interface is added to the scm that separates the 2d from the 1d sub domain at the bottom since the experimental results prove that the assumption of a prescribed velocity profile is feasible for dam break waves near the bottom wüthrich et al 2018 on the other hand the velocity profile is highly non uniform close to the free surface incorporating a backward negative flow velocity where the 2d sub model can capture this distortion castro orgaz et al 2022 castro orgaz and chanson 2022 the following sections describe how the flow information is transferred from the mps swe to the wc mps sub model 4 1 section coupling model scm the schematic of the scm is displayed in figs 2a and 2b the 2d area is accommodated on the right hand side and the middle of the solution domain for the above figures where the flow depth of the inner zone h 1 is set to zero at the initial time and l 0 is the 2d subdomain s length at the initial time the 2d subdomain s length is controlled automatically by the expansion or contraction of the 1d inner zone in the longitudinal direction which may vary with time based on the flow feature the 1d region is placed on the other side of the 2d sub domain where h 1 h 0 and h 1 hd upstream and downstream at the initial time respectively in the overlapping zone the horizontal velocity is assumed uniform and equal to the depth averaged velocity conversely a kinematic free surface boundary condition is adopted for the vertical velocity pringle et al 2016 the length of the overlapping zone is set to 1 5 re 2d which is the intermediate value of the recommended length in the scm literature varying from 3 dl to 6 dl fourtakas et al 2018 kassiotis et al 2011 ni et al 2020 the velocities in the overlapping zone are characterized by 27 u s c m u v s c m y u x where u s c m and v s c m are the 2d velocities along the x and y coordinate axes in the overlapping zone v s c m y u x can also be determined by combining the advection equation d y h dt 0 continuity equation eq 1 and vertical movement of particles v d y d t as shown in fig 3 a four layers of ghost particles are allocated to the 2d sub domain to ensure enough particles within the searching radius and avoid potential particle deficiency which is a numerical error of misestimating the weighted average function due to insufficient neighbouring particles within a support domain koshizuka et al 2018 the ghost particles are positioned at the whole bottom to ensure they cover the solution domain the vertical ghost particles move automatically according to the motion of the second shallow water layer with a flow depth equal to the first layer on the left and right hand sides of the 2d sub domain at the initial time but with zero value in the middle the normal and tangential velocity at the bed is set to zero to impose the no slip condition which leads to u w 0 moreover the velocities for the ghost particles are computed for the no slip boundary condition as follows bouscasse et al 2013 lee et al 2011 28 u g n 2 u w u m n u g t 2 u w u m t u w u w 0 u w 0 where n and t are the bed boundary normal and tangential vectors respectively also u g is the velocity vector for the ghost particle and u m is the mirrored velocity of the fluid particle into the ghost particle located on the other side of the boundary as shown in fig 3a this method is named the mirror particles technique marrone et al 2011 in which u m is evaluated using an interpolation technique such as a moving mean square interpolation koshizuka et al 2018 state that another interpolation technique could be a low cost weighted average therefore the following interpolation technique is presented in this study 29 r m 2 r w r g u m 1 n f 2 d m j f l u i d n u j w i j 2 d if r m r j r e 2 d n f 2 d m j f l u i d n w i j 2 d where r m is the position vector of a ghost particle mirrored into the fluid domain colagrossi and landrini 2003 also n f 2 d is the particle number density of the fluid particles with the position and velocity vectors r j and u j that surround the mirrored particle with the properties r m and u m for the pressure the neumann boundary condition is imposed on the bed boundary duan et al 2021 30 n p w a l l ρ g n compared to the conventional zero gradient condition the right hand side of the above equation stands for a hydrostatic pressure assumption near the bed boundary lee et al 2011 which diminishes a numerical boundary layer as discussed by hosseini and feng 2011 in other words this treatment helps to satisfy the momentum conservation in the projection methods due to omitting the pressure gradient for velocity calculation in the predictor stage the pressure for the ghost particles is computed similarly using eq 30 4 2 mixed scm lcm bi directional coupling model as illustrated in fig 2c the 2d sub domain is surrounded by the 1d model from the left hand side and the bottom assuming that the velocity profile away from the free surface and near the bed can be prescribed in the overlapping zone the momentum transfer is similar to the scm the lcm is characterized by h 1 ϕhd and 0 ϕ 1 as depicted in fig 3b ϕ could be constant or varying along the streamwise direction at the initial time since the two layer shallow water flow can simulate any initial configuration of h 1 the flow velocity is transferred from the 1d sub layer to the 2d sub domain at the lcm boundary interface using a prescribed velocity distribution the predefined velocity profile technique has been successfully applied to multi layer shallow water models to simulate non hydrostatic free surface flows xia and jin 2007 2006 wüthrich et al 2018 experimentally studied wave propagation generated by the vertical release of a water volume the streamwise velocity distribution measured in depth by an ultrasound velocity profiler clearly agreed with prandtl s power law for dry and wet bed cases as they documented thus the wall function following prandtl s power law is adopted in this study for the fully developed turbulent open channel flows bose and dey 2007 31 u l c m 1 m m u y h 1 m where y is the elevation of each fluid particle from the bed and m is the exponent depending on the bed friction and usually varying from 4 to 12 for rough and smooth beds chanson 2004 respectively herein m 8 is utilized as recommended by wüthrich et al 2018 for experimental studies of tsunami waves by means of a vertical water release the second term for computing the turbulent eddy viscosity in eq 10 kd is omitted since the wall function treats the bed effect in the lcm as discussed by bose and dey 2007 the vertical component of prandtl s power law must satisfy the divergence free condition u l c m 0 therefore 32 v l c m h u x y h 1 m m if the normal distance of a fluid particle to the lcm interface i e d becomes less than a threshold velocity vector u l c m u l c m v l c m follows prandtl s power law in the mps literature this threshold value represents a virtual wall boundary adjacent to the bed boundary that varies from 0 5dl 2d to dl 2d matsunaga et al 2020b zhang et al 2019 herein the threshold value is set to dl 2d the normal distance to the bed is calculated by d n r wi where n nx ny is the boundary normal vector determined by the geometry of the boundary and r wi r i r w where r w is the position vector of the boundary the geometry of the bottom in the 2d zone is arbitrary unlike the scm the prevalent techniques to treat the irregular wall boundaries in the mps comprise the wall particle method explicitly represented polygon wall boundary distance function based polygon and boundary integral based polygon koshizuka et al 2018 matsunaga et al 2020b mitsume et al 2015 in this work a combination of the wall particle method and explicitly represented polygon wall boundary is applied with some adjustments in the way that the 2d wall boundary would vary spatially and temporally proportional to the height of lcm h 1 z for this purpose the boundary tangential vector shown by t in fig 3b is first calculated 33 t cos θ e x sin θ e y θ arctan h 1 z x where e x and e y are the unit vectors along the x and y axis respectively therefore the boundary normal vector will be 34 n cos π 2 θ e x sin π 2 θ e y n sin θ e x cos θ e y since there is no guarantee that the real normal distance to the bed is obtained from d n r wi in arbitrary geometry the wall particles adjacent to the near the wall fluid particle contribute to finding the minimum normal distance to the wall boundary as depicted in fig 4 inspired by the explicitly represented polygon method introduced by mitsume et al 2018 2015 after calculating the boundary normal vector using eq 33 the normal distance will be 35 d m i n k l r k i n k l k r i r k r e 2 d where k 1 2 n denotes the particles on the boundary polygon l in the proximity of r i r k re 2d as illustrated in fig 4 the near the wall velocity vector is prescribed in case d dl 2d so that the normal component is equal to the lcm velocity to account for the impermeable boundary condition while the parallel component retains the magnitude and direction 36 u u l c m u n n u l c m n n u n l c m 0 u u n n u l c m u l c m n n combining the above equations yields 37 u u l c m equation 37 is a dirichlet type boundary condition referred to as the no slip boundary condition with moving walls matsunaga et al 2020a matsunaga and koshizuka 2022 that is consistent with the momentum transfer at the interface of lcm also the velocities of the ghost particles are obtained from eqs 31 and 32 furthermore a non homogenous neumann boundary condition is employed for the pressure normal to the wall to account for the velocity of the moving boundary with finite acceleration koshizuka et al 2018 matsunaga et al 2020a 38 n p w a l l ρ g d u w d t n u w u l c m the pressure for the ghost particles is computed similarly using eq 38 5 hybrid solver the fractional step algorithm introduced by chorin 1968 is adopted for time integration which divides each time level into the following predictor and corrector stages 5 1 predictor stage first the 1d momentum equation is discretized neglecting the free surface elevation time rate h z t to obtain the intermediate velocity u position x and particle number density n 1d as formulated in eq 39 a semi implicit scheme is utilized for the friction term for stability reasons buttinger kreuzhuber et al 2019 39 u u k g δ t u u k n m 2 h k 4 3 x x k u δ t n 1 d j i n w 1 d then the water depth h k 1 is obtained from the density ratio equation eq 13 using the newton raphson iteration as described by sarkhosh and jin 2022 in detail and h 1 k 1 using eq 19 the computed water depths are added to the bed elevation known based on the bed topography to obtain the free surface elevation h k 1 z k 1 and lcm height h 1 k 1 z k 1 the positions of 2d wall particles shown by red circles in fig 4 are then updated in this respect vertical wall particles above lcm move according to the longitudinal displacement of the 1d zone in the proximity of xl and xr of the scm as illustrated in fig 4 moreover the horizontal wall particles oscillate temporally along the y direction with a magnitude equal to h 1 z in lcm to reduce the computational time only those wall particles restricted in the vicinity of xl and xr oscillate moreover the magnitude and direction of the 2d ghost particles motion denoted by grey circles are equal to the wall particles displacement with the same y 0 for scm and x 0 for lcm the 2d momentum equation is discretized omitting the pressure gradient as follows 40 u u k δ t g δ t ν 0 2 u k δ t 1 ρ τ k r r k u δ t the momentum is transferred from the 1d model to the 2d sub domain using eq 27 in the overlapping zone and eq 36 at the lcm boundary interface in addition the velocities of the ghost particles at the bottom are obtained from eq 28 for the scm and eq 37 for the lcm 5 2 corrector stage in the mps there are several methods to calculate the pressure namely solving the pressure poisson equation ppe implicitly koshizuka and oka 1996 or explicitly ye et al 2020 zuo et al 2022 for incompressible flow and the equation of state shakibaeinia and jin 2010 or equation of state together with explicit ppe considering weakly compressible flow xiao and jin 2022 in this study the last technique is utilized for pressure calculation to do that the ppe is given by 41 2 p k 1 ρ δ t u xiao and jin 2022 state that the laplacian term can be solved explicitly in the appreciation of the intermediate pressure determined by the equation of state therefore the pressure is computed using eqs 25 and 26 as follows 42 p i ρ c 0 2 γ n i 2 d n 0 2 d 1 γ 7 c 0 10 u m a x p i k 1 2 d s λ n 0 2 d j i n p j w i j 2 d ρ δ t d s n 0 2 d j i n u i j r i j 2 r i j w i j 2 d 2 d s λ n 0 2 d j i n w i j 2 d where c 0 is the speed of sound using taylor series expansion the neumann boundary condition eq 30 and the non homogeneous neumann boundary condition eq 38 are imposed on the walls for the scm and lcm respectively 43 scm p w a l l k 1 p k 1 r w r ρ g lcm p w a l l k 1 p k 1 r w r ρ g d u w d t the velocities and positions of the 1d model are corrected 44 u g δ t h z x k 1 g δ t u u k n m 2 h k 4 3 u k 1 u u x k 1 x k u k 1 δ t similar to the predictor stage the positions of the 2d wall and ghost particles are corrected then the 2d velocities and positions are updated as 45 u δ t ρ p k 1 u k 1 u u r k 1 r k u k 1 δ t finally the momentum is transferred from the 1d model to the 2d sub domain similar to the predictor stage also the velocities of the wall and ghost particles are updated which is applied to compute the viscous and turbulent shear stresses for the next time step since the solver is based on an explicit scheme the time step must satisfy the courant friedrichs lewy cfl stability condition in this work the time steps are first calculated for the sub models afterward the minimum value is taken into account to assure numerical stability as follows 46 δ t cfl min δ l g h u m a x d l 2 d c 0 on the right hand side of eq 46 the first term inside the brackets refers to the mps swe time step obtained from the 1d particle length and wave speed whereas the second term represents the wc mps computed from the 2d particle size and sound speed in this study cfl 0 5 is selected to guarantee the stability of the sub models as recommended in the aforementioned mps solvers koshizuka et al 2018 sarkhosh and jin 2021 6 model verification first the two layer shallow water equations are solved using the mps swe and compared to the analytical solution chen et al 2007 in section 6 1 then the coupling method is analysed to investigate the motion of the coupling boundaries according to the two layer shallow water flow under a hydrostatic assumption for this purpose in sections 6 2 and 6 3 the correct assignment of the 1d flow velocity and the kinematic free surface boundary condition to the 2d sub domain is verified to ensure the model does not experience any unphysical instability at the moving interfaces 6 1 two layer shallow water equations with a uniform velocity distribution the two layer shallow water equations are solved using the mps swe to demonstrate the accuracy of the solver in simulating a frictionless dam break problem involving a two layer shallow water flow chen et al 2007 extended stoker 1957 solution to this case in this regard a numerical experiment is considered concerning a dam break problem over a wet bed with h 0 0 25 m and hd 0 1 m the channel length is 9 m over a horizontal bed with upstream and downstream boundaries closed by vertical walls the gate is placed at x 4 65 m and removed instantaneously the initial water depth of the lower layer h 1 is set equal to the downstream flow depth as shown in fig 1a in the mps swe dl 1d 0 005 m is utilized fig 5 compares the numerical results with stoker s analytical solution by chen et al 2007 the errors of the flow depths predicted by the numerical model at different times compared to the analytical solution are gathered in table 1 using root mean square error rmse revealing that rmse increases with time 6 2 analysis of the moving overlapping zones in the scm this section studies the moving coupling method in the scm hybrid model as shown in fig 2b in this analysis the velocities of the 2d sub model wc mps are computed using the swe information from eq 27 which means that the overlapping zone is assumed to cover the whole 2d sub domain under a hydrostatic pressure assumption for this purpose a dam break problem test is considered with the initial upstream and downstream flow depths and channel length described in section 6 1 at the initial time h 1 is set equal to zero in the interval between xl 3 5 m to xr 5 5 m while h 1 h on the left and right hand sides of the interval see fig 3a the criterium for selecting a proper initial 2d sub domain length is discussed in section 7 then the 2d subdomain s length is controlled automatically by the motion of the two layer shallow water flow based on the expansion or contraction of the 1d inner zone in the longitudinal direction the flow depth results and the velocity transfer from the 1d sub model to the 2d sub domain are depicted in fig 6 using dl 1d 0 005 m and dl 2d 0 005 m as shown the 2d sub domain expands with time and moves forward moreover the vertical velocity near the dam has high values at the initial times around 1 5 m s which later show lower values with approximately equal to zero and uniform velocities at t 2 s except for the shockwave with a high magnitude moreover the total volume of this 2d sub domain within the zone with h 1 0 at different times was calculated as 0 373 m2 per unit width using the trapezoidal rule which means its volume remains unchanged as a result mass conservation which is one of the challenging issues in the hybrid models is accurately satisfied in the present model 6 3 analysis of the moving boundaries in the mixed scm lcm this section investigates the moving coupling boundaries through the mixed scm lcm as shown in fig 2c a numerical experiment for a dam break problem is employed with the same upstream and downstream flow depths and channel length described in sections 6 1 and 6 2 and h 1 equal to 0 05 m in the interval between xl 3 5 m to the end of the channel i e xr 9 0 m see fig 3b also h 1 h on the left hand side of the interval fig 7 shows the flow depth results and the velocity transfer from the 1d sub model to the 2d sub domain using dl 1d 0 005 m and dl 2d 0 005 m during the wave propagation with time similarly the 2d sub domain volume was calculated as 0 447 m3 per unit width which proves that 2d occupies an identical area at different time levels and satisfies the mass conservation moreover the velocities are transferred appropriately to the 2d sub domain with no unphysical oscillations near the moving interfaces showing that the coupling boundaries perform well 7 model validation this section uses different coupling models to simulate real dam break waves with non hydrostatic pressure distribution over dry and wet beds 7 1 scm with a single overlapping zone the sketch of this hybrid configuration is depicted in fig 2a which is employed for the dry bed surge the sub domains are configured so that the wc mps solves regions with highly vertical accelerations whereas the rest is simulated through the mps swe the experimental data collected by ozmen cagatay and kocaman 2010 are adopted for model validation the experimental setup is characterized by an upstream water depth of h 0 0 25 m in a horizontal channel with a 9 m length bounded by walls upstream and downstream and a gate at x 4 65 m that is removed instantaneously also the bed downstream of the gate is dry the criterium of applicability of boussinesq type flows is utilized to determine the length of each sub domain as recommended in boussinesq type flows literature these equations are suitable with a minimum wavelength to depth ratio of six khan and steffler 1996 steffler and jin 1993 therefore at least the length of the 2d sub domain is considered six times the initial upstream flow depth i e l 0 6h 0 where l 0 is the 2d subdomain s length in this experiment three values of l 0 equal to 6 9 and 12h 0 are used in figs 8 and 9 the hybrid solver with l 0 h 0 6 is compared to the wc mps regarding the propagation of the dry bed surge using dl 1d and dl 2d 0 01 m and nm 0 01 m 1 3s which represents the bed roughness in a glass made flume as can be seen the hybrid model predicts the dam break wave with good agreement compared to the 2d model in terms of the pressure and velocity magnitude as the hybrid solver simulates this case mostly with the 2d sub model later in this section the hybrid model s performance is also tested for longer times considering the aforesaid numerical experiment to evaluate the 1d sub model contribution to the dam break flow because the wave propagation only affects the 2d sub domain at the initial times whereas the 1d model participates in mimicking the wave progression at the final steps the water depth profiles calculated from the hybrid model using various l 0 h 0 and the wc mps models are also compared with the observed data ozmen cagatay and kocaman 2010 in the forms of dimensionless water depth and times as depicted in fig 10 overall the hybrid solver could predict the dry bed surge as well as the wc mps compared to the observed data as seen the 1d sub model begins to simulate the rarefaction wave in the hybrid solver for l 0 h 0 6 setting at t 3 88 while the 1d sub domain affects the rarefaction wave for l 0 h 0 9 only at t 6 64 expectedly widening the range of use of the 2d sub model within the hybrid model improves the agreement with the fully 2d model that is why there is no discrepancy between the water depth profiles computed by the hybrid solver using l 0 h 0 12 with the wc mps the rmse of the flow depth profiles predicted by the hybrid and wc mps solvers compared to the observed data is shown in table 2 via two point linear interpolation of the two closest particles located upstream and downstream of the experimental data locations although the wc mps predicts the water profiles slightly better all models are approximately in the same order of accuracy moreover the percentage of reduced fluid particles and cpu time are shown in the last columns of table 2 overall the accuracy increases with increasing l 0 h 0 but with more fluid particles and computational time in addition the depth averaged velocities calculated from the hybrid and wc mps solvers are compared by the vertically averaged method given by cantero chinchilla et al 2018 47 u 1 y s 0 y s u d z v 1 y s 0 y s v d z where u and v are depth averaged velocities corresponding to u and v across the 2d free surface depth ys that is calculated by the 2d sub model the vertical velocity in the 1d sub model is zero while u equals the depth averaged velocity of the mps swe solver the comparison results are shown in figs 11 and 12 in dimensionless forms it shows that the wavefront has high velocities with steep slopes at the early stages which later becomes smoother with time also the vertical velocity is of high magnitude starting at around 0 4 m s and decreasing to 0 06 m s with downward and upward slopes in the proximities of rarefaction and wavefront respectively that is recognizable approximately near the initial gate position at x x 0 h 0 0 overall the hybrid solver agrees well with the wc mps particularly with increasing l 0 h 0 however l 0 h 0 6 shows a slightly higher speed of the wavefront at t 6 64 additionally the complete comparison of hybrid and wc mps solvers are provided in table 3 regarding calculated h u and v profiles expectedly the errors of the calculated depth averaged flow variables of the hybrid solver show higher accuracy with increasing l 0 h 0 so that the ratios of 9 and 12 indicate perfect agreement with the wc mps results the aforesaid numerical experiment is also tested at dimensionless times t 9 5 and 26 5 to evaluate the hybrid model s performance at long times for this purpose the pressure and velocity magnitude obtained from the hybrid solver are compared with the 2d model using various l 0 h 0 as illustrated in figs 13 and 15 furthermore the depth averaged flow variables are plotted in figs 14 and 16 for the time levels mentioned above at t 9 5 the results for l 0 h 0 9 and 12 clearly agree with those of the wc mps model in contrast the wavefront is slightly ahead in the hybrid solver with l 0 h 0 6 compared to the 2d model because the wave travels faster due to the excessive contribution of the 1d sub model to the longitudinal motion of the overlapping zone upstream of the 2d dub domain in the scm hybrid solver and so l 0 h 0 6 is not a suitable setting for the long times of the dam break also it produces the rarefaction wave mostly through the hydrostatic sub model the results at t 26 5 show the wave reflection against the downstream vertical end as can be noticed the pressure flow depth and velocities fit well with the 2d model for l 0 h 0 9 and 12 also the pressure and flow depth are slightly overestimated adjacent to the downstream wall in the hybrid solver with l 0 h 0 6 compared to the wc mps due to the fast movement of the overlapping zone in the longitudinal direction subject to the 1d sub model 7 2 scm with double overlapping zones this coupling configuration simulates the dam break problem over a wet bed as shown in fig 2b castro orgaz and chanson 2020 discuss that the surge is fully broken and well predicted by the swe for the downstream to upstream flow depth ratio r 0 1 since the shockwave resembles the wavefront as a discontinuity however dam break problem over a wet bed with ratios r 0 4 to 0 55 are of high importance in such cases a 2d model sufficiently describes the undular bore but the transformation of the undular bore into a broken surge could be well estimated by the swe cantero chinchilla et al 2020b castro orgaz and chanson 2020 in this section a dam break problem with r 0 4 is considered using the experimental data provided by ozmen cagatay and kocaman 2010 the upstream and downstream water depths are 0 25 and 0 1 m respectively resting in a horizontal channel with a 9 m length and bounded by walls upstream and downstream the gate is accommodated at x 4 65 m and removed instantaneously in the hybrid solver nm 0 01 m 1 3s dl 1d and dl 2d 0 01 m as discussed in section 7 1 the 2d sub domain configuration with l 0 h 0 9 predicts better thus three l 0 values equal to 9h 0 12h 0 and 15h 0 are employed in this case so that the 2d sub domain is initially established symmetrically upstream and downstream of the gate to this end three groups of vertical boundaries illustrated in fig 3a are set to xl 3 52 xr 5 78 m for l 0 9h 0 xl 3 15 xr 6 15 m for l 0 12h 0 and xl 2 77 xr 6 53 m for l 0 15h 0 the hybrid solution obtained with l 0 h 0 9 is compared to the wc mps in figs 17 and 18 the hybrid model simulates the surge propagation with almost identical accuracy to the 2d model in terms of the pressure and velocity magnitude at t 1 5 and 2 38 as the wave is fully simulated via the 2d sub model in these times the undular bore with high amplitude is initially developed which later is converted to a train of waves moreover the wave is significantly non linear and non hydrostatic at the stages mentioned earlier it is noticeable that the wave breaking occurs at t 4 01 at this time the advancing wave train is still modelled by the 2d sub model with perfect agreement with the wc mps however the rarefaction wave is affected by the 1d sub model but still agrees well with the wc mps as can be tracked in the wc mps model the wave breaking suppresses the undulation at the wavefront at t 6 51 and 8 9 so that the non hydrostaticity decreases with time at these time levels the hybrid solver converts the broken surge to a shockwave by the 1d sub model while still mimicking the wave train upstream of the shockwave by the 2d sub model the flow depth profiles obtained from the hybrid model using various l 0 h 0 and the wc mps models are plotted versus the observed data ozmen cagatay and kocaman 2010 in dimensionless forms as shown in fig 19 moreover depth averaged horizontal and vertical velocities are plotted in figs 20 and 21 from these figures it can be inferred that all configurations approximately give results in perfect agreement with the wc mps model at the early stages the contribution of the 1d sub domain to mimic the rarefaction wave begins at t 4 01 for l 0 h 0 9 at t 6 51 for l 0 h 0 12 and at t 8 9 for l 0 h 0 15 in addition the broken wave is transformed into a shockwave at t 6 51 for l 0 h 0 9 while at the final time level for the other settings the error in the water depths obtained from the hybrid and wc mps solvers are compared to the observed data in table 4 using rmse as can be noticed all models are about in the same order of accuracy while decreasing the percentage of reduced fluid particles and cpu time with reducing l 0 h 0 the depth averaged horizontal and vertical velocities of the hybrid and 2d models in figs 20 and 21 show that the velocity patterns are very similar at t 1 5 and 2 38 however u is slightly underestimated in the proximity of the rarefaction wave due to the application of the 1d sub model for l 0 h 0 9 at t 4 01 the velocities clearly agree with those calculated by the wc mps at t 6 51 for l 0 h 0 12 and 15 while the hybrid configuration with l 0 h 0 15 shows better agreement at t 8 9 it is worth noting that the depth averaged vertical velocity is 0 155 m s at t 8 9 in the wc mps model between x x 0 h 0 7 and 10 while the hybrid results have zero values since this area is simulated by the 1d sub model that transfers the wavefront into a shockwave with no vertical velocity information the hybrid errors of h u and v compared to the 2d solver are summarized in table 5 which decreases with increasing l 0 h 0 the wave reflection for the aforesaid numerical experiment is also evaluated at t 29 94 using the observed flow depths provided by kocaman and ozmen cagatay 2015 as shown in fig 22 the hybrid solver can predict the reflected wavefront in good agreement with the 2d model regarding the flow pressure and velocity furthermore flow depth and depth averaged velocities fit well with the 2d model near the wavefront as shown in fig 23 on the other hand the solution is simplified to a hydrostatic behaviour attributed to the swes on the left and right hand sides of the wavefront but still shows a reasonable agreement 7 3 mixed scm lcm bi directional coupling model this setting mimics the dam break problem over a wet bed with an overlapping zone upstream of the gate at xl see fig 2c moreover a boundary interface bisects the initial downstream depth and intersects the overlapping zone at xl considering ϕ 0 5 as illustrated in fig 3b the main idea of this coupling method is to simulate the whole train of waves without converting the wavefront to a shockwave unlike section 7 2 the same experimental setup as the previous section is considered for comparison three settings are considered with overlapping zones placed at xl 3 52 xl 3 15 and xl 2 77 as also employed in section 7 2 the configurations above can be characterized in dimensionless lengths equal to x 0 xl h 0 4 5 6 and 7 5 respectively moreover the initial height of the boundary surface is set to 0 05 m which is half the value of the initial hd in the hybrid solver nm 0 01 m 1 3s dl 1d and dl 2d 0 01 m the results of the water depths horizontal and vertical velocities calculated from the hybrid solver with x 0 xl h 0 4 5 are compared to the wc mps in figs 24 and 25 at different dimensionless times overall the hybrid solver models wave propagation with good agreement with the 2d solver at t 1 5 the undular bore is well simulated by the hybrid solver although u is slightly overestimated just upstream of the undular bore also the wavefront is a bit attenuated due to the prescribed velocity profile in the lcm at t 2 38 but the rarefaction wave agrees with the 2d result later the breaking wave occurs and a train of waves is formed at t 4 01 also the 1d sub model contributes to simulating the rarefaction wave at this time level notably the missing velocity information of the wavefront in the scm with l 0 h 0 9 is well estimated by the mixed scm lcm with x 0 xl h 0 4 5 at t 6 51 and 8 9 on the other hand the vertical velocity in the proximity of the rarefaction wave reduces with time showing that this area can be simulated according to the hydrostatic assumption over long times overall the hybrid solver slightly underestimates v in the wavefront area but still maintains the velocities information until the end of the wave propagation period which is an advantage over the scm the dimensionless velocity profiles in depth of the hybrid model using x 0 xl h 0 4 5 compared to the wc mps are depicted in figs 26 and 27 at different dimensionless times five stations are selected within the 2d sub domain to plot the velocity profiles which are x 3 75 4 25 4 75 5 25 and 5 75 m that can be rewritten in dimensionless forms equal to x h 0 15 17 19 21 and 23 from the figures it can be inferred that the velocity profiles obtained from the hybrid solver fit well with the wc mps in particular the agreement is better upstream of the gate at the initial times which is related to regions with smaller h 1 where the 2d sub model incorporates computing the flow variables more moreover the water surface profiles computed from the hybrid and wc mps solvers are plotted compared to the observed data ozmen cagatay and kocaman 2010 in fig 28 for this case the results are normalized with h 0 at different dimensionless times as can be seen from fig 28 the mixed scm lcm with x 0 xl h 0 4 5 begins to simulate the rarefaction wave affected by the 1d sub model at t 4 01 whereas the bidirectional coupling settings with x 0 xl h 0 6 and 7 5 are influenced by the hydrostatic sub model at t 6 51 on the other hand the wavefront is well predicted by the hybrid solver compared to the wc mps although attenuating the amplitude slightly at some time levels in addition the pattern of the wave train is maintained in the hybrid solver even using various coupling during the wave progression the rmse of water depth calculated by the hybrid solver using different ratios of x 0 xl h 0 are compared to the wc mps as well as experimental data in table 6 also the reduced fluid particles and cpu time are computed as provided in the last columns of table 6 for the studied dam break problem over a wet bed case the average error of the flow depth number of fluid particles and cpu time on the basis of the five times indicated in tables 5 and 6 are summarized in table 7 to understand the performance of various scm and mixed scm lcm settings compared to the wc mps it is worth noting that the solvers are executed on pc intel core i7 4790 cpu 3 60 ghz the average water depth errors of various configurations of the hybrid solver are also provided in fig 29 a at different dimensionless time levels using a bar chart to compare the coupling models performance better predictably more particles produce a better estimation of the flow depth but with higher computational cost as explained earlier xl the starting point of the overlapping zone on the left hand side of the 2d sub domain is identical for scm and the mixed scm lcm with the same coupling number e g scm 1 and scm lcm 1 in this respect scm 1 requires fewer fluid particles than the mixed scm lcm 1 however scm 2 and 3 need more particles than the corresponding mixed scm lcm 2 and 3 with increasing the 2d sub domain length in other words the 2d sub domain in the mixed scm lcm saves more fluid particles by expanding the 2d subdomain compared to the scm with identical xl as illustrated in fig 29a the scm predicts the water depth slightly better than the corresponding mixed scm lcm at t 1 5 and 2 38 at t 4 01 the hybrid settings with the same coupling number show similar accuracy however the mixed lcm scm can estimate the flow depth with higher precision at t 6 51 and 8 9 with even lower computational time moreover the relative number of fluid particles computational time and average rmse h applied in the hybrid solvers normalized using the wc mps properties are plotted in fig 29b 7 4 scm in the presence of an obstacle this section applies the scm hybrid model to a dam break over irregular bed topography the laboratory data collected by ozmen cagatay and kocaman 2011 is adopted for analysis the experimental setup consists of a horizontal rectangular channel 9 m long and 0 3 m wide that is made of glass with an open downstream end there is a trapezoidal obstacle in the channel with the following coordinates of vertices in meters 6 18 0 6 53 0 075 6 83 0 075 and 7 18 0 a gate is placed upstream of the obstacle at x 4 65 m and is removed instantaneously the initial upstream water depth is set to 0 25 m while the downstream bed is dry in the hybrid solver nm 0 01 m 1 3s dl 1d and dl 2d 0 01 m and the 2d portion is given by l 0 equal to 9h 0 which is a good choice providing a perfect agreement with the 2d model as discussed in section 7 1 the pressure and velocity magnitude results of the hybrid model are compared to the wc mps model in figs 30 and 31 at different dimensionless times overall the hybrid results agree with the full 2d model the dimensionless flow depths are also compared to the observed data ozmen cagatay and kocaman 2011 in fig 32 the two numerical results are in very good agreement with the observed data to simulate the wave traveling at different times although the hybrid solver converts the wave reflection to a shock wave at t 41 84 miliani et al 2021 state that the no slip boundary condition delays the dam break waves compared to the experimental results as it produces excessive resistance of the bed likewise the hybrid wave reflection matches the observed data better than the wc mps in this study it indicates that the assumption of a no slip boundary condition in the pure 2d model overestimates the bed friction since it does not represent the real physical properties of the bottom conversely the hybrid model admits more realistic bottom friction upstream of the channel through the manning formula in fig 32 the profiles of the bottom pressure head p b normalized by ρgh 0 are also provided the p b deviates from the hydrostatic pressure in the proximity of the obstacle where the hydraulic jump occurs since vertical accelerations are of high magnitude and provoke vertical pressures below hydrostatic behavior as the flow accelerates toward the obstacle crest to capture the hydrodynamic pressure the 2d portion of the hybrid model is configured so as to simulate the flow neighboring the obstacle and shows excellent agreement with the wc mps model in addition the 2d sub domain is more extended at the early stages which then reduces at the final times since the 2d sub region moves according to the motion of the 1d inner zone and gradually leaves the open downstream end this is in harmony with the physical point of view as the hydrodynamic pressure is more significant at early times conversely the swes can predict the wave reflection through a shockwave with a satisfactory agreement with the full 2d model at the end of the simulation except for near the obstacle that necessitates the use of a non hydrostatic model since the current example contains a transcritical flow regime flow transitions can be detected by the froude number fr variation along the channel at different times as illustrated in fig 33 at t 11 9 the flow regime changes from subcritical fr 1 to supercritical fr 1 that continues up to the end of the channel also the flow regime remains critical fr 1 at x x 0 h 0 0 throughout the simulations however the formation of the hydraulic jump is evident as the wave approaches the trapezoidal obstacle at the progressing times in addition the location of the hydraulic jump shifts upstream with time also the flow regime is critical over the crest which later becomes supercritical where the flow moves downstream of the trapezoidal block overall the flow regime of the full 2d model can be well predicted by the hybrid solver 8 conclusions this study presents a hybrid solver to simulate the dam break problem over a frictional bed two 1d regions are constructed by a two layer shallow water flow with a uniform velocity distribution by means of inner and outer zones in a section by section setup or upper and lower layers within a layer by layer arrangement the area with highly vertical accelerations near the dam site at the initial stages is then substituted by a 2d sub domain via the large eddy simulation les together with the sub particle scale sps a moving particle simulation mps is utilized for the spatial integration of the system of equations which helps to deal with problems with large deformation of the free surface two coupling models have been developed the first is the section coupling model scm in which the velocity information is transferred from the 1d to 2d sub domains through one or two moving overlapping zones the proposed scm is novel as it does not employ an open boundary coupling interface unlike the existing coupling models for dam break flows that necessitate the prescription of the flow at the inlet and outlet according to flow regimes at each time that is cumbersome as the flow is highly transient instead moving overlapping zones are adopted within a closed boundary condition at the interface that moves based on the 1d flow feature in addition the mass conservation is appropriately satisfied in the current model because the interfaces move with the flow and so there is no concern about the mass exchange at the interfaces this study proposes a layer coupling model lcm that reduces the 2d portion of the hybrid model from the bottom this is different from the typical lcm techniques found in literature which use fully 2d lagrangian eulerian hybrid solvers for free surface flow for this purpose the near the wall flow follows prandtl s power law and can be approximated by the 1d sub domain in contrast the velocity profile is non uniform close to the free surface and incorporates a backward negative flow velocity that can be captured by the 2d sub model moreover a robust technique has been introduced to treat the irregular wall boundaries at the lcm interface while preserving momentum conservation in the validation section the present model was compared with the 2d mps solver and experimental data to analyze the performance of sub domain configurations at selected times in terms of the dry bed surge the hydrodynamic pressure is significant in the vicinities of the wavefront and rarefaction wave where the 2d sub model is adopted and able to solve the non hydrostaticity the dry surge results reveal that the scm hybrid solver agrees well with observed data by the root mean square error rmse of free surface profiles equals 8 08 mm which is only 0 6 mm larger than the pure 2d model while saving the computational time by 55 concerning the wet bed surge the scm performs slightly better at early times in contrast the mixed scm lcm accomplishes more promising results over long times after the dam release the reason is that the scm converts the wavefront to a shockwave conducted by the 1d sub model while the second coupling strategy maintains the non linearity of the wavefront moreover it can be concluded that the scm based hybrid solver gives train of waves results almost as accurately as the wc mps at the initial times just after the dam collapse except for the wavefront conversely the mixed scm lcm solver can mimic the wave propagation with slightly better precision with progressing time compared to the scm solver however the two coupling models generally show somewhat similar results with rmses equal to 8 09 mm for the scm and 8 16 mm for the mixed scm lcm which is in conformity with a full 2d solver with an rmse of 7 48 mm finally it must be emphasized that the present hybrid model is restricted to simulating dam break flows regarding the hybrid configuration the 2d sub domain is established in the proximity where the gate was initially positioned and must be extended by at least six times the initial reservoir flow depth in the current solver wet bed simulation can be done using either scm or mixed scm lcm however only scm is used for dry surge modeling since the irregularity of the moving boundary interfaces in the mixed scm lcm model is erratic in the dry dam break flow and requires complex treatment in the future the current solver will be developed to a coupling of 1d mps boussinesq type with the 2d wc mps model to study highly non hydrostatic wave propagation in hydraulic and coastal engineering such as the solitary wave run up over inclined shorelines and breakwaters also it is recommended to extend the current coupling models to a hybrid 3d ns 2d sw equations solver to simulate dam break flows over real world bed topography credit authorship contribution statement payam sarkhosh conceptualization methodology software validation writing original draft visualization investigation yee chung jin conceptualization visualization investigation writing review editing funding acquisition supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests yee chung jin reports financial support was provided by natural sciences and engineering research council of canada acknowledgments this research was partly supported by the natural sciences and engineering research council of canada rgpin 2022 03432 
7,a hydrological model incurs three types of uncertainties measurement structural and parametric uncertainty for instance in rainfall runoff models measurement uncertainty exists due to errors in measurements of rainfall and streamflow data structural uncertainty exists due to errors in mathematical representation of hydrological processes parametric uncertainty is a consequence of our inability to measure effective model parameters limited data available to calibrate model parameters and measurement and structural uncertainties the existence of these predominantly epistemic uncertainties makes the model inference difficult limits of acceptability loa framework has been proposed in the literature for model inference under a rejectionist framework loas can be useful in model inference if they reflect the effect of errors in rainfall and streamflow measurements in this study the usefulness of quantile random forest qrf algorithm has been explored for constructing loas loas obtained by qrf were compared to the uncertainty bounds obtained by rating curve analysis and the loas obtained by runoff ratio method rating curve analysis yields uncertainty in streamflow measurements only and the runoff ratio method is expected to reflect uncertainty in rainfall and streamflow volume measurements loas obtained by using qrf were found to envelop the uncertainty bounds due to streamflow measurement errors loas obtained by qrf and runoff ratio methods were similar further qrf loas were scrutinized in terms of their ability to reflect the effect of rainfall uncertainty both qualitatively and quantitatively results indicate that qrf loas reflect the effect of rainfall uncertainty increase in standard deviation with increase in mean streamflow values and decrease in coefficient of variation with increase in mean streamflow values a mathematical analysis of the loas obtained by the qrf method is presented to provide a theoretical foundation keywords hydrological model uncertainty machine learning runoff ratio limits of acceptability model validation 1 introduction 1 1 background in a generic hydrological model 1 y g x θ δ ϵ δ and ε denote the effect of structural and measurement errors beven 2005 in the estimation of time series of observed hydrologic variables e g streamflow y by the approximate model g here x denotes model inputs such as rainfall and temperature and θ denotes the set of model parameters measurement errors refer to errors in measurements of rainfall and streamflow while structural errors refer to errors in the mathematical representation of hydrologic processes given a parameter set θ s the structural and measurement errors are estimated based on the residual time series y g x θ s if an appropriate probability distribution over δ and ε may be assumed the parameters of the distributions along with hydrologic model parameters can be obtained by using bayes theorem kennedy and o hagan 2001 however the use of formal probability distributions has its own challenges beven and smith 2015 often a probability distribution over the sum of δ and ε is assumed such as gaussian or generalized gaussian schoups and vrugt 2010 ammann et al 2019 smith et al 2015 but the residual time series can yield only an aggregate estimate of the effect of measurement and structural errors that is the quantities δ and ε are individually unidentifiable renard et al 2010 2011 brynjarsdóttir and oʼhagan 2014 separate identification of structural and measurement errors is required to determine what part of modeling exercise needs to be addressed to reduce total uncertainty the data or the model e g reichert and mieleitner 2009 and to facilitate rejection of bad models to identify structural uncertainty in a model strong prior information about measurement uncertainties is required renard et al 2010 mcmillan et al 2012 brynjarsdóttir and oʼhagan 2014 mcmillan et al 2018 and this information should be obtained before calibration and independent of the hydrologic model being used given information about measurement uncertainty and the residual time series corresponding to a model or model parameters a bayesian characterization of structural uncertainty is possible in the sense that one can obtain a probabilistic estimate of the effect of structural uncertainty conditioned upon each possible realization of rainfall and other inputs and streamflow time series priors over measurement uncertainty are typically constructed by making aleatoric assumptions about the nature of these errors for example one can obtain information about random measurement uncertainty in streamflow by using rating curve analysis kiang et al 2018 petersen øverleir et al 2009 reitan and petersen øverleir 2009 le coz et al 2014 or other probabilistic methods de oliveira and vrugt 2022 but epistemic uncertainties in streamflow such as those introduced by extrapolation of rating curve to gauge heights well above the observations may not be knowable reliable information about rainfall measurement uncertainty cannot be obtained in most situations for instance one may estimate the uncertainty in areal average rainfall by assuming that this uncertainty is dominated by spatial variability of rainfall and neglecting temporal errors and biases moulin et al 2009 renard et al 2011 spatial variability can be modeled using a statistical model such as kriging provided that enough data to estimate the parameters of the variogram are available this is further complicated as the parameters of the variogram will change from event to event in unknown ways precipitation data also incur timing errors which can be significant if the precipitation gauges are sparse or are located outside the watershed if the observed event seem to violate the principle of mass balance e g beven and westerberg 2011 one may expect errors in the measurements of either rainfall data or streamflow data or both such time periods in rainfall runoff time series are referred to as disinformative beven and westerberg 2011 which should be discarded before model fitting a disinformative event can introduce bias in the modeling effort because it violates mass balance and also because it affects the antecedent conditions for subsequent events beven and smith 2015 disinformative periods in a rainfall runoff dataset may be identified as the ones with exceptionally high and low runoff ratios beven and westerberg 2011 where runoff ratio of an event is defined as the ratio of total event streamflow to total event rainfall what is an exceptionally high or low value of runoff ratio may be determined using the knowledge about the rainfall runoff response of the watershed several other attempts have been made to characterize the uncertainty in hydrologic data and hydrologic modeling e g kuczera and parent 1998 kavetski et al 2006a 2006b gabellani et al 2007 gong et al 2013 mcmillan et al 2018 but it still remains an unsolved problem because of dominantly epistemic nature of these errors recently gupta and govindaraju 2022 noted that several methods have been proposed for uncertainty analysis in hydrology but there is no consensus on which method should be used recently the runoff ratio method has been proposed to construct limits of acceptability loa bounds on streamflow that could then be used to identify behavioral models beven 2019 a model or a model parameter set is considered behavioral if the streamflow simulated by it falls within the loa at some predefined timesteps beven et al 2022 depending on the purpose of the modeling exercise it is clear that loa should be such as to encompass the uncertainty due to measurement errors in rainfall and streamflow thus a model that properly accounts for streamflow dynamics within the margin of measurement errors would not be rejected and will be considered behavioral loas have also been defined using flow duration curves fdcs westerberg et al 2011 in this method measurement uncertainty over streamflow time series is obtained using rating curve analysis measurement uncertainty in streamflow is converted to an uncertainty bound over fdc a model or model parameter set is considered behavioral if the fdc simulated by it falls into the fdc uncertainty bound however this method only compares the probability distribution of observed and simulated streamflows and removes the temporal information from the streamflow time series also it does not account for rainfall measurement errors in fact most of the methods to derive loas are based on streamflow uncertainty only and neglect rainfall uncertainty e g krueger et al 2010 coxon et al 2014 to the best of author s knowledge the runoff ratio method is the only method that constructs loas while acknowledging uncertainty in both streamflow and rainfall measurements the runoff ratio method also has some limitations as discussed below fundamentally the loa method has been proposed in a rejectionist framework beven and lane 2019 which makes it different from bayesian methods wherein no models are explicitly rejected frequentist statistics also provides a model rejection framework such as the likelihood ratio test neyman and pearson 1933 fisherian hypothesis testing fisher 1956 and more recently evidential testing royall 2017 lele 2004 but these methods are based on aleatoric assumptions as are bayesian methods about various uncertainties and therefore are difficult to justify in hydrologic applications there have been a relatively few attempts in hydrology to use rigorous frequentist methods for model inference but see pande 2013a 2013b the loa framework provides an alternative to the formal statistical frameworks as it combines the elements of bayesian theory parameter update as the models are tested against more data and frequentist statistics model rejection loa can also be applied in a purely bayesian framework by defining an appropriate loa based likelihood function e g krueger et al 2010 the aim of this study was to explore the potential of using machine learning algorithms called decision tree dt and in particular quantile random forest qrf in constructing loas in gauged and ungauged locations 1 2 runoff ratio method and decision trees in runoff ratio method the rainfall and streamflow time series are divided into separate rainfall runoff events then the rainfall runoff events with similar characteristics are pooled together the main idea is that the two similar events should have similar runoff ratios of course no two events are exactly similar and there would be some differences in runoff ratios but the large differences can be at least partly attributed to either rainfall and or streamflow measurement errors the differences between runoff ratio values of two similar events may also result from imperfections in the methodology to compute runoff ratios multiplying a zero loss streamflow event with runoff ratios of all the similar events would result in an ensemble of corresponding streamflow hydrographs zero loss streamflow can be obtained by dividing the observed hydrograph by the corresponding runoff ratio beven 2019 suggested that the upper and lower bounds of these hydrographs be used as loa over the rainfall runoff event in question the different hydrographs in the ensemble can be assigned a weight based on the similarity of the corresponding event with the event for which loa is being constructed this method is described in more detail below the advantage of the runoff ratio method is that it allows to define a distribution of streamflow hydrographs for a given rainfall event and antecedent conditions based on available data a limitation of this method is that it is applicable to flashy watersheds only beven 2019 also this method cannot account for potential timing errors in precipitation it only accounts for errors in precipitation and streamflow volume and further can be applied only at an event timescale further this method cannot be used to construct loas at ungauged locations where streamflow data are unavailable for computing runoff ratios these limitations can be addressed by using a machine learning ml method while retaining the advantage of the runoff ratio method a direct mapping between relevant watershed attributes meteorological data and streamflow can be created by using an ml algorithm e g govindaraju 2000 zhang and govindaraju 2000 2003 iorgulescu and beven 2004 shortridge et al 2016 kratzert et al 2019 ml can be particularly useful in constructing loas for baseflow dominated watersheds where runoff ratio method is not applicable and to construct loas at ungauged locations further the ml approach allows defining loas at the scale of available data as discussed below ml algorithms called decision trees dts are particularly well suited in this regard another advantage of the ml approach is that data from several watersheds may be used to train the model and define loas data from different watersheds however may introduce disinformation because of watershed specific epistemic uncertainties beven 2020 but the hydrologically relevant information available from other watersheds may still be useful especially when loas are to be constructed for an ungauged watershed an ml algorithm such as dt will be able to identify hydrologically similar watersheds based on available watershed characteristics albeit that watersheds characteristics are typically represented by spatially averaged indices neglecting their spatial variation thus dts are natural candidates to consider for constructing loas as discussed below the uncertainties in hydrological data are predominantly epistemic which may change from event to event in unknown ways and the true statistical behavior of uncertainties will not be generally represented by the available data therefore dt would either overpredict or underpredict the effect of measurement errors while overprediction is acceptable underprediction may be problematic in many applications therefore one needs to allow for outliers while validating the models using the loa method as in beven et al 2022 further the dt model would compensate for systematic biases these systematic errors cannot be detected by a statistical approach a bias term can be introduced in statistical models but these models would not be able to differentiate between the bias in the data and the bias in the model simulations the classical method of finding uncertainty in the measurement of a phenomenon is to repeat the measurement process several times under identical conditions the repeated sampling method however is impossible for the measurements of environmental phenomena such as rainfall and streamflow mcmillan et al 2012 but an approximate repeated sampling method may be implemented for environmental measurements the main idea is to estimate the effect of measurement uncertainty using observations of rainfall streamflow events under similar conditions across several different events and or several different watersheds the runoff ratio and dt methods can be thought of as approximate repeated sampling techniques once the loas are obtained either formal or informal bayesian liu et al 2009 krueger et al 2010 beven and lane 2022 methodologies may be used for subsequent uncertainty analysis in informal methods one may define behavioral models and model parameters as ones that yield streamflow time series within the loa thus all the models with an inferior structure will eventually be rejected as more and more data are used at least that is the expectation one can also use the apparatus of formal bayesian theory for model or parameter inference using the loas in approximate bayesian computation framework nott et al 2012 sadegh and vrugt 2013 vrugt and sadegh 2013 vrugt and beven 2018 1 3 objectives the objective of this study is to develop a method for constructing loas that can account for both precipitation and streamflow measurement errors and can be used for ungauged catchments in this study we ask if a variant of dt called quantile random forest qrf may be used to construct meaningful loas a second question is if the loas obtained by qrf algorithm are comparable to those obtained by the runoff ratio method of beven 2019 the novelty of this study lies in using qrf model to construct loas that account for measurement uncertainty based on available data to address the objective of this study uncertainty bounds obtained by qrf model are scrutinized to check if they can be used as loas the uncertainties in real world data are however unknown therefore it is impossible to check if the uncertainty bounds obtained by any method represent true uncertainties some characteristics of the uncertainties can be obtained by using statistical methods based on aleatoric assumptions we test whether the qrf estimated loas reflect the effect of these uncertainties or not further this paper presents a mathematical analysis of the proposed hypothesis the goal of the mathematical analysis is 1 to show how decision trees such as qrf can be used to encompass measurement uncertainties due to errors in rainfall and streamflow measurements and 2 to clarify the logic and assumptions behind the proposed method in section 2 the theory behind dts and qrf algorithm are discussed along with the methodology to empirically test the proposed method section 3 discusses the results of the study section 4 presents a brief mathematical analysis of the qrf method in terms of defining loas section 5 concludes the paper 2 theory and methodology 2 1 study area data and the models developed in this study data from ohio river basin orb were used to calibrate and validate the qrf model this basin contains 431 usgs streamflow stations fig 1 the streamflow data were downloaded from usgs website for all the 431 stations data for these watersheds are available from water year 2011 to 2020 total drainage area of each usgs station was delineated on the 30m 30m resolution digital elevation model archuleta et al 2017 u s geological survey the national map 2017 by using the archydro toolbox for each of the drainage areas predictor variables listed in table 1 were computed or collected climate data were collected over the study area from historical climate network hcn stations available at national centers for environmental information ncei website to test the capability of the qrf model in capturing rainfall and streamflow measurement uncertainties data from st joseph river watershed sjrw were used as test cases sjrw is located just above the orb in northwest as indicated in fig 1 see also figure b1 in appendix b the drainage areas of the sjrw watersheds are listed in table 2 specifically qrf models were used to generate loas at four usgs streamflow stations located in sjrw three kinds of qrf models were developed 1 gauged single scenario in this case four individual qrf models were developed for each of the four sjrw watersheds using data from the watershed where the loas were to be constructed for example to construct loas at station 04180500 the data from only this station were used to train the qrf model these models are referred to as gauged single models 2 gauged scenario in this case a qrf model was trained using data from both the orb and the four sjrw watersheds the model thus trained is referred to as gauged model three kinds of models were developed in this scenario 2a qrf was trained using data from all the training watersheds referred to gauged all 2b qrf was trained using data from the 4 most similar watersheds to the watershed where loas are to be constructed referred to gauged 4 and 2c qrf was trained using the data from the 20 most similar watersheds referred to gauged 20 3 ungauged scenario in this case a qrf model was trained using data only from the orb watersheds without using the sjrw data the model thus trained will be referred to as ungauged model out of the 431 orb stations 80 of the stations were fixed for the calibration of qrf and the remaining stations were fixed for validation similar watersheds in the gauged scenario were selected based on the watershed static attributes and the mean climate mean precipitation and temperatures the first two scenarios allow us to test the usefulness of qrf approach in constructing loas at a gauged location and the third scenario allows us to test the usefulness of the approach at ungauged locations the comparison of the first two and the third scenario allows to test the usefulness of data across multiple watersheds in constructing loas 2 2 machine learning models to map predictor variables to streamflow the main idea behind ml algorithms is to create a mapping between predictor and response variables friedman et al 2001 chap 2 for most watershed scale rainfall runoff models the set of predictor variables constitutes meteorological data soil data land use data etc table 1 and the response variable typically is streamflow time series available data are divided into calibration and validation sets the samples contained in calibration set are used to create a mapping such that a loss function which is a function of the mapping is minimized and the samples contained in validation set are used to test the generalizability of the created mapping in this study qrf was used to create a mapping between predictor and response variables breiman et al 1984 breiman 2001 the basic building block of qrf is another ml algorithm called regression trees friedman et al 2001 chap 9 iorgulescu and beven 2004 regression trees create a non linear mapping between predictor and response variables in this method the space of predictor variables is divided into s contiguous subregions and in each subregion the response variable is approximated by a unique function let the set containing predictor and response variables be denoted by d each element of d represents a calibration training sample let the i th calibration sample be denoted by x i yi then d x 1 y 1 x 2 y 2 x n y n where n is the total number of calibration samples the vector x i is a p vector where p denotes the number of predictor variables that is x i x i1 x i2 xip and yi is a scalar that denotes the response variable corresponding to the i th sample in this study the i th response variable is streamflow at the outlet of a watershed at a particular time step the i th predictor vector includes static watershed attributes and meteorological data at multiple lags table 1 the regression tree is created using an iterative procedure in the first iteration the set d is divided into two or more subsets based on a randomly selected j th predictor variable let the two subsets be denoted by d 11 and d 12 then 2 d 11 x i y i x i j x j thresh d 12 d d 11 where x j thresh denotes a randomly chosen threshold for j th predictor variable in the second iteration the subsets d 11 and d 12 are further divided into smaller subsets and so on for subsequent iterations at the end of the iterative procedure s smaller subsets of d are obtained and each subset occupies a distinct region of the predictor space thus the regression tree algorithm divides the predictor space into s contiguous subregions this method is referred to as regression trees because the process of division of training samples into s subsets can be visualized as creating a tree fig 2 see also friedman et al 2001 pp 268 the tree grows deeper with each iteration therefore the number of iterations is also referred to as tree depth typically a maximum value of tree depth d is assigned to avoid overfitting the subsets obtained in the last iteration are also referred to as leaf nodes it is clear that there is a relationship between the number of leaf nodes s and maximum tree depth d an increase in d implies an increase in s note that once the tree is created each subregion can be identified by a set of rules on predictor variables after the tree is created response of a sample with predictor vector x is obtained as follows the first step is to identify the subregion of the predictor space to which the vector x belongs suppose that x belongs to the i th subregion corresponding to i th training subset denoted by si then the response variable corresponding to x is estimated as the average response of calibration samples contained in si 3 y x 1 l i j 1 l i y x j where li denotes the number of samples in si regression trees are developed so that the sum of square errors between observed and estimated responses is minimized with some regularization to avoid over fitting the averaging of data in the leaf node however neglects the variability in the data therefore not just the average but the entire distribution y x j for x j si were used to construct loas as explained below the method of regression trees is particularly suitable for the purpose of creating loas because it mimics the function of an approximate repeated sampler by grouping similar calibration samples similarity in predictor space together based on several watershed attributes thus enabling the accounting of measurement uncertainty due to errors in response and predictor variables regression trees have to be regularized to avoid overfitting therefore b regression trees are developed instead of a single one each of the b regression trees is created by randomly drawing k samples by bootstrapping from the calibration set d this yields an ensemble y x y 1 x y 2 x y b x of streamflow estimates corresponding to the predictor variable x where the b th estimate yb x obtained by eq 3 corresponds to the b th tree the average of values in y x is taken as the final estimate this method is known as random forest rf in this study the rf algorithm was used to create a mapping between predictor variables listed in table 1 and streamflow and the streamflow in each subregion of the predictor space was estimated as the average streamflow of calibration samples in that subregion eq 3 but as mentioned above taking averages of data in the leaf node neglects the variability in the leaf node which might contain important information about uncertainties therefore quantile random forest qrf technique was used to construct loas where quantiles instead of averages are computed in this technique the ensemble y qrf is constructed by using the entire distribution of data in leaf nodes if a given predictor say x falls into the i th leaf node of the b th tree denoted by s i b then the distribution of response variable in s i b can be represented as 4 y b x y j y j s i b thus we will have a distribution yb for each tree now the data from each yb s can be combined to form an ensemble y qrf x 5 y qrf x y j y j y b b 1 2 b note that the yj values contained in y qrf are observed values not the estimates qrf estimates different quantiles of the response for a given x by treating y qrf as the distribution of response in this study 2 5th and 97 5th percentiles obtained by qrf were used as lower and upper loas we found that these percentiles were typically adequate for constructing loas in the sense that most of the observations were enveloped by the loas but a few flow values could not be enveloped therefore in practical applications more extreme percentiles might be appropriate for creating loas if the premise the ensemble of estimated streamflow represents only measurement uncertainty were true then in the absence of measurement errors the different streamflow estimates in the ensemble would be approximately identical in practice however even in the absence of measurement errors the streamflow estimates in the ensemble would be different because of several reasons 1 imperfections in creating the regression trees these imperfections include selection of appropriate values of b number of regression trees and s number of leaf nodes a large value of s or large value of maximum tree depth d may result in an over estimation of measurement errors and conversely for a small value of s or small value of d in this study optimal values of b and d along with minimum number of samples in a leaf node were estimated by computing the out of bag oob error the oob error is the prediction error of calibrated rf from the left out training set an early stopping method searches for the optimal values of these parameters with the minimal oob error 2 small calibration set which is inadequate to represent the population of measurement errors calibration sets should be large enough such that the variability in measurement errors in rainfall and streamflow is captured in this study data from a total of 431 orb stations plus 4 sjrw stations were used out of which data from a total of 344 stations were used for calibration 3 the set of predictor variables used to train the ml algorithm is incomplete if a relevant predictor variable is missed in the set of predictor variables the uncertainty bound yielded by qrf would also contain structural errors the predictor variables used in this study are listed in table 1 though these predictors variable are incomplete they are still good enough to estimate the streamflow time series accurately in many watersheds as evident by high nse for some of the test stations shown in the results section even after taking all the precautions the loas created by qrf method would still contain structural errors qrf would be able to construct better loas as the sample size increases when the loas are to be constructed at a gauged location the longer length of data at the location will be more important than the data from other watersheds but data from other watersheds would be the only option when loas are to be constructed at an ungauged location the loas obtained by qrf were compared against the bounds obtained over streamflow measurements uncertainty which in turn were obtained by rating curve analysis if the loas obtained by qrf indeed reflect the effects of measurement uncertainties in rainfall and streamflows these should envelop the uncertainty bound obtained by rating curve analysis also we compared the bounds obtained by runoff ratio method to the bounds obtained by qrf method analysis of rating curve and runoff ratio results was carried out at the four usgs streamflow gauging stations within sjrw as indicated in table 2 sjrw is located immediately northwest of orb as indicated in fig 1 moreover the qrf loa should also reflect the effects of measurement uncertainty in rainfall in this study the measurement uncertainty in areal average rainfall was obtained using an empirical approach one challenge is that the rainfall uncertainty bounds cannot be directly compared to the loas since rainfall is processed through the watersheds in a highly non linear fashion before it reaches the watershed outlet there is no exact way of translating measurement uncertainty in rainfall to streamflow space therefore in this study the various realizations of rainfall were processed through the scs curve number cn formula for different values of cn to get an estimate of excess rainfall subsequently coefficient of variation of streamflow cv q were compared to the coefficient of variation of excess rainfall time series cv r 2 3 rating curve analysis to quantify uncertainties in measured streamflow the streamflow at a river cross section is estimated using the observed relationship between measured gage heights at the cross section and corresponding measured discharges this relationship is referred to as rating curve herschy 1993 commonly a rating curve is modeled as multiple power law segments le coz et al 2014 6 log q r h 0 h h 0 1 log a 1 b 1 log h h 0 1 h 0 1 h h s 1 log a 2 b 2 log h h 0 2 h s 1 h h s 2 log a m b m log h h 0 m h s m 1 h in eq 6 q r is the estimated streamflow h is measured gage height h 0 1 is the cease to flow parameter of lowest power law segment which corresponds to height of riverbed with respect to datum h s k is the upper bound of kth power law segment on h axis h 0 k is the cease to flow parameter of kth segment ak and bk are the multiplier and exponent parameters of the kth segment and m is the number of rating curve segments typically several gage heights are measured during a day which are then converted to streamflow using the rating curve eq 6 corresponds to manning equation sturm 2001 for flow in open channels with the assumption that hydraulic radius is approximately equal to depth le coz et al 2014 and is a frequently used relationship in hydraulic modeling errors in gage height measurements may be assumed negligible reitan and petersen øverleir 2009 thus uncertainties in estimated streamflow are mainly due to errors in direct measurements of streamflow that are used to construct the rating curve in this study the following model was used to quantify the uncertainties in estimated streamflow 7 q h q r h ϵ r where q r h is determined by eq 6 εr is the random measurement error in observed streamflow and q h is the observed streamflow further we assumed the εr s at different time steps to be distributed independently as skewed exponential power distribution fernández and steel 1998 also q h was truncated at zero which makes the probability density of q equal to 8 p q q 2 γ γ 1 f ϵ r ϵ r γ i 0 ϵ r f ϵ r γ ϵ r i 0 ϵ r 1 φ 0 q r ϕ β γ i 0 q where γ 0 is the skew parameter i denotes the indicator function φ 0 q r ϕ β γ is the probability that the value of untruncated q is less than zero and f ϵ r is the power exponential distribution with scale parameter ϕ and shape parameter β 1 1 9 f ϵ r ϵ r γ 1 1 2 1 β 2 1 2 1 β ϕ 1 exp 1 2 ϵ r ϕ 2 1 β the priors listed in table 3 were used as weakly informative priors over parameters of the models q r and εr following reitan and petersen øverleir 2009 strictly uniform priors over the parameters of q r are not non informative gupta et al 2022 this difference however would have minimal effect on our analysis as we are concerned only with the width of uncertainty bounds over streamflow time series not the probabilities assigned to different realizations of streamflow time series further we have not imposed any upper limit on the distribution of streamflow very low practically zero probability will be assigned beyond a certain magnitude of q irrespective of the prior distribution used the results obtained for the four sjrw stations confirm that absence of upper limit does not have any effect on the obtained uncertainty bounds validity of the error model of eq 8 was assessed a posteriori via qq plots the aleatoric assumption made in the analysis may not be valid during the peak events it has been shown using hydraulic modeling that uncertainty during peak events can be very high di baldassarre and montanari 2009 these uncertainties are epistemic in nature rather than aleatoric and therefore a formal statistical treatment of these uncertainties is difficult to test how well the qrf loas envelop the streamflow uncertainty due to these epistemic sources we computed the fraction of peaks enveloped by the qrf loas if the true peaks were some multiple f of the observed peaks with f varying from 1 1 to 2 we refer to this analysis as the multiplier analysis in this study only the peaks with flow values greater than 50 percentile were considered for this analysis the posterior distribution over parameters was computed using delayed rejection adaptive metropolis dram algorithm haario et al 2006 in an approximate bayes setting nott et al 2012 the approximate bayes computations facilitated faster convergence to a posterior distribution this method of rating curve analysis is same as that of reitan and petersen øverleir 2009 except that they used a multiplicative error model instead of an additive error model the multiplicative error model was considered unsuitable in this case because of the large range of streamflow values as opposed to that in reitan and petersen øverleir 2009 study a multiplicative error model would result in unrealistically high uncertainties at larger values of observed streamflow additive error structure used in this study was found to be appropriate by the way of qq plot test in the examples considered in this study convergence to posterior distribution was confirmed using r diagnostic statistic rd gelman and rubin 1992 markov chains were assumed to converge to posterior distribution if r d converged to a value below 1 1 and never increased on further simulations of the chains the posterior distribution was further processed to remove the parameter sets that yielded large deviations between observed and estimated streamflow the deviation between observed and estimated streamflow was measured using sum of square errors the computed posterior distribution over parameters of both q r and εr was used to simulate several streamflow time series that were assumed to represent random uncertainty in measurements of streamflow as obtained by the rating curve method 2 4 uncertainty bound in areal average rainfall the uncertainty in areal average rainfall exists due to errors in rainfall measurements at a gauging station and due to spatial interpolation errors in rainfall measurements at a gauging station are difficult to obtain due to lack of a simple error model the errors due to spatial interpolation are likely to dominate the total error in areal average rainfall e g renard et al 2011 therefore the errors in rainfall measured at a gauging station are neglected in this study and it is assumed that the errors in areal average rainfall exist solely due to spatial variability of rainfall several different models have been proposed to capture the spatial variation of rainfall such as cluster point poisson processes waymire and gupta 1981a b c random cascades gupta and waymire 1993 kriging moulin et al 2009 and conditional simulations renard et al 2011 all these models treat rainfall as a random field in space time domain but most of these models are based on strict assumptions about the covariance of spatial rainfall or error structure which are not justifiable in practice even if the assumptions are approximately true the rain gauge density is typically too small to reliably estimate the parameters of the covariance function this issue is further complicated as the covariance structure may vary from event to event in unknown ways depending upon the type of event therefore in this study an empirical approach was used to get an estimate of the uncertainty in areal average rainfall there were 6 rainfall gauging stations near the sjrw locations on these stations are shown in fig b1 at which daily timescale data were available typically data from the available rain gauges are used to compute a single areal average rainfall time series using the thiessen polygon interpolation method in this study all the 63 26 1 different combination of the 6 rain gauges were used to produce 63 realizations of areal average rainfall using the thiessen polygon method these 63 realizations represent an estimate of uncertainty in areal average rainfall 2 5 uncertainty bounds using runoff ratio method the qrf method does not allow one to incorporate a hydrologists knowledge about a watershed to construct the measurement uncertainty bounds one method that allows incorporation of such knowledge was proposed by beven 2019 using runoff ratios of observed rainfall runoff events in this method only the observed rainfall runoff data along with evaporation data of the watershed in question are used to create loas this method was used to derive loa estimates that were then compared to the loas estimated by the qrf algorithm in the first step the observed rainfall runoff data were separated into different rainfall runoff events this kind of hydrograph separation requires estimation of the recession curve to this end the master recession curve mrc technique was used lamb and beven 1997 mrc is a characteristic recession curve of the watershed tallaksen 1995 once an mrc is defined the streamflow time series can be divided into different rainfall runoff events in this study a rainfall value below 1mm day 1 was considered negligible and a new rainfall event was assumed to start if the rainfall was negligible for more than 7 consecutive days for example a new rainfall event started at time step tn if the rainfall values at the time steps t n 1 and t n 7 were less than 1mm day 1 the streamflow hydrograph corresponding to each rainfall event was assumed to start at the beginning of the rainfall event and end just before the start of next rainfall period next mrc was appropriately appended at the end of the streamflow hydrograph for each rainfall runoff event the number of rainfall runoff events thus obtained for four of the stations in sjrw are listed in table 4 in the second step the runoff ratio of each event was computed as the ratio of the total volume of event streamflow to the total volume of event rainfall where event streamflow refers to streamflow time series obtained after appending the mrc this resulted in an ensemble of runoff ratios in the third step loas were computed over each of the rainfall runoff events in an iterative manner to construct the loas over the ith event the events in the ensemble similar to the ith event were identified based on antecedent moisture condition and total volume of rainfall during the event as an estimate of the antecedent moisture conditions initial streamflow of the event was used thus the events that were closest to the ith event were identified by using the mahalanobis distance between the events using these two variables this is the k nearest neighbor approach used by beven 2019 appropriate value of the mahalanobis distance to define the closeness of two events is a subjective decision in this study we first computed the mahalanobis distance of the ith event from rest of the events and then normalized the distance values to lie between 0 and 1 now events similar to the ith event may be defined as the events that are d m n distance away from the ith event where d m n denotes normalized mahalanobis distance several values of d m n were used to analyze the impact of this threshold on uncertainty bound after the completion of the third step one obtains runoff ratios of the ith event and those of other ni events that are similar to the ith event in addition to the k nearest neighbor approach we also used decision tree approach to group similar events again based on antecedent moisture condition and total rainfall volume in what follows the abbreviations rr knn and rr qrf will be used to refer to runoff ratio method applied using k nearest neighbor method and qrf method respectively in the fourth step the streamflow time series of the ith event was divided by its runoff ratio ci thus yielding a zero loss streamflow time series of the ith event that would have been observed if the runoff ratio of the ith event was equal to 1 the zero loss streamflow time series was then multiplied by the largest and smallest runoff ratios to obtain upper and lower bounds of loa in rr knn method the largest and smallest runoff ratios were identified among the ni runoff ratios of the events similar to the i th event in rr qrf approach the largest and smallest runoff ratios were the 100th and 0th percentiles in the leaf node to which the i th event belonged rr qrf approach is more objective than the rr knn approach since the value of d m n needs to be specified subjectively in the latter however specification of appropriate percentiles in rr qrf incurs some subjectivity 3 experiments with rainfall runoff data 3 1 an evaluation of decision tree dts in terms of predicting streamflow fig 3 shows the nse values obtained by the rf ungauged model for the watersheds contained in the test set nse was greater than 0 60 for 55 of the watersheds and was greater than 0 5 for 80 of the test watersheds there were some systematic patterns in the spatial distribution of nse values nses were typically higher in the eastern part of the basin than those in the western part most watersheds in the eastern orb had nses greater than 0 5 for about 20 of all the test watersheds the nse was less than 0 5 it is likely that the rf algorithm could not identify the rainfall runoff relationship in these watersheds possibly because the hydrological behavior of these watersheds is not represented in the data overall the performance of the rf model was deemed acceptable for majority of the watersheds for which nse was greater than 0 50 it captured the rainfall runoff dynamics in the sense that its response to input rainfall is hydrologically consistent the term hydrologically consistent is used to refer to an expected behavior of hydrological models increasing streamflow with increasing rainfall under similar antecedent conditions one question is if qrf model can be used to construct loas in a watershed where the nse is low we note that low nse value can also be due to errors in streamflow or rainfall data but still the loas obtained for these watersheds may not be reliably used for model inference fig 4 shows the observed and predicted streamflow for the four stations located in sjrw nse was close to 0 6 for the three of the stations but was poor 0 36 for station 04178000 these values seem adequate for constructing measurement uncertainty bounds except for station 04178000 3 2 limits of acceptability loa constructed by the qrf models fig 5 shows the loas obtained by the qrf models trained under the first two scenarios gauged single and gauged along with the uncertainty bounds obtained by the rating curve analysis since rating curve analysis yields uncertainty due to errors in streamflow measurements only loas obtained by qrf should envelop the uncertainty bound obtained by rating curve analysis as shown in fig 5 a similar observation was made for the majority of the cases table 5 among the different qrf models qrf gauged all qrf gauged 20 qrf gauged 4 qrf single the loas obtained by the qrf gauged models were widest and the loas obtained by the qrf gauged 20 and qrf gauged 4 models were typically close to each other the qrf single model yields very narrow loas at the two peaks shown at time steps 410 and 438 these two peaks are among the highest flow values observed in these watersheds implying that more data are required to construct reliable loas for these peaks this illustrates the practical difficulty in constructing loas and highlights the need to allow for outliers when loas are used for model inference there would not be enough data to estimate loas for events with return period greater than 2 to 10 years in many instances the loas obtained by the three qrf gauged models qrf gauged all qrf gauged 20 qrf gauged 4 were very similar except at a few time steps as mentioned above the 4 and 20 most similar watersheds to train the qrf model were identified using static watershed attributes these static attributes are already used by the qrf method to partition the data into leaf nodes which explains the similarity of loas obtained by the three qrf gauged models the uncertainty bound obtained by rating curve analysis was significantly narrower at most of the time steps indicating that errors in rainfall measurements contribute more to measurement uncertainty than do the errors in streamflow measurements but the streamflow uncertainty bounds shown in fig 5 were obtained by making aleatoric assumptions the peak streamflow values may contain larger uncertainties fig 6 shows the fraction of peaks enveloped by upper bounds of loas if the observed peak magnitude were multiplied by a factor f as the multiplier f increases the fraction of peaks enveloped by the qrf uncertainty bound decreases this decrease however occurs at different rates for the three models interestingly the fractions of multiplied peaks enveloped by the loas were larger for the gauged single model than the ones obtained by the gauged all model this is likely from timing errors in precipitation data as discussed below the typical errors in peak streamflow have been reported to be 20 40 di baldassarre and montanari 2009 fig 6 shows that more than 55 of the peaks were enveloped in these ranges of errors by all the three models even for 100 errors more than 30 of the peaks are enveloped by the qrf loas across the three models one of the characteristics of the loa by the qrf method fig 5 is that it is very wide at the time steps corresponding to streamflow peaks and narrow at the time steps where streamflow is small although not shown here this pattern was visible throughout the study period fig 7 shows the standard deviations of streamflow obtained by qrf method plotted against streamflow the standard deviation increases as streamflow value increases in keeping with how rainfall uncertainty typically propagates to streamflow uncertainty moulin et al 2009 renard et al 2011 these observations suggest that qrf is able to account for the effect of uncertainty due to rainfall and streamflow measurement errors one seeming discrepancy to the pattern discussed above is the wide loa obtained by the qrf gauged method between time steps 410 and 420 even when the streamflow time series is in recession phase fig 5 this is especially the case for the stations 04180500 and 04178000 data show that some rain did fall over the watershed at these time steps fig 5 and this rain event was similar in magnitude to the rain event that generated the streamflow peak at time step 424 one possibility is that this rain event did not result in streamflow due to spatial location of the event rain event might be far from the watershed outlet the second possibility is that the rainfall measurement at the gauging station is erroneous the third source of error is the unknown true intensity of rainfall the observed rainfall data are at daily timescale and two events with same intensity at the daily timescale may have very different intensities at sub daily timescales which will result in different hydrographs these are examples of epistemic errors and the exact reason for these errors is difficult to know in fact we do not even know whether the measurement is actually erroneous a good hydrological model forced with this rain event and uninformed by true spatial distribution and true intensity of rainfall will still generate a streamflow event if the antecedent conditions allow it would be unwise to reject this model if these errors indeed exist this illustrates how qrf can account for epistemic errors similarly at time step 450 a wide loa was obtained by the qrf method for three of the stations whereas streamflow time series is in recession phase again a rainfall event was observed at this time step which apparently did not result in a streamflow peak and the same arguments apply in some of the events timing errors between observed peak and qrf simulated peak were observed primarily in the loas created by the qrf gauged model an example of such timing errors may be seen at time step 438 in fig 5 these timing errors occurred for less than 20 events per watershed see also fig 11 where loas for a few other time steps are also shown for the five peak events shown in fig 5 timing error occurs only for one event for the three stations 04180500 04180000 and 04179520 out of the two major peaks at time steps 410 and 438 timing errors at time step 438 are present for stations 04180500 and 04180000 there seem to be two possibilities behind these timing errors 1 disinformation introduced by the data from other watersheds or 2 timing errors in rainfall data for the stations 04180500 and 04180000 there is zero lag between rainfall and qrf obtained streamflow peak at time step 410 meanwhile at time step 438 a lag of 1 2 days between rainfall and streamflow peak is observed further the rainfall event at time step 438 is more intense at daily timescale and one would expect a smaller lag between rainfall and streamflow peaks for this event compared to the lag observed for the event at time step 410 therefore it seems more likely that the computed areal average rainfall has timing errors for this event we note that it is also possible that the sub daily timescale intensity of the event at time step 438 was low which would justify the delay in peak this is again an example of epistemic uncertainty the same arguments apply for the timing errors observed at the station 04178000 especially at time step 424 where the lag between computed areal rainfall peak and observed streamflow is 3 days the timing errors at the station 04178000 were more frequent which is partly the reason for poor validation nse value at this station fig 4 it is worth noting that the timing errors between loas constructed by qrf single model and observed streamflow were typically absent it is possible that the model has compensated for timing errors in precipitation potential for the timing errors in rainfall around time step 438 is also illustrated in fig 8 which shows all the different realizations of the areal average rainfall for the majority of the realizations the second precipitation peak occurs at time step 436 while for a few realizations the second peak occurs at time step 435 these realizations were constructed using six gauging stations which are located outside but near the sjrw watershed fig b1 if data from more stations were available some of the realization might have very well shown the second peak at the time step 437 it is worth noting that the information about consistent timing error may not be revealed by qrf single model as it will learn this as a behavior of the watershed thus this analysis illustrates the usefulness of data from different watersheds in constructing loas further the analysis also illustrates how the loas constructed using decision trees may potentially capture the effect of timing errors it is possible that the timing errors between the observed and qrf gauged model simulated peaks occur because of disinformation introduced by data from other watersheds therefore it seems prudent to construct multiple loas using data from different sets of watersheds and use a combination of these loas for model inference fig 9 shows the cv r coefficient of variation of areal average rainfall obtained by using the empirical approach described above the cv r values decrease as areal average rainfall increases at all the stations at first one may attribute this behavior to standard deviation of rainfall being constant irrespective of the mean rainfall value however it was observed that standard deviation of areal average rainfall increases with increasing mean rainfall values now shown similar to the standard deviation of streamflow the cv values of excess rainfall obtained by scs cn method also follow the same pattern as areal average rainfall but the cvs corresponding to excess rainfall were typically higher than the cvs corresponding to areal average rainfall the difference between excess and areal average rainfall cvs become smaller for higher values of areal average rainfall many of the small non zero areal average rainfall values produce no excess rainfall increased number of zeros in excess rainfall increases the cv fig 9 shows that variation of cv q with streamflow follows the same pattern as that of variation of cv r with mean areal average rainfall cv q decreases as mean streamflow increases for all the four stations the magnitudes of cvs are of similar order for the areal average rainfall and streamflow time series another pattern in cv r plots is that there is a larger smaller scatter in these values when mean rainfall is small large the same pattern can be seen in streamflow values also the rainfall time series is transformed non linearly through a watershed to yield streamflow the same rainfall event can result in very different streamflow hydrograph depending upon the spatial distribution of rain within the watershed and antecedent moisture conditions thus for a given rainfall magnitude many different values of streamflow are possible which explains the larger scatter in cv q fig 9 indicates that the statistical structure of rf uncertainty bound reflects the effect of rainfall uncertainty overall these results combined with the results discussed above indicate that the dts could account for the effect of uncertainty due to errors in rainfall and streamflow measurements further it can be argued that any model with heteroscedastic error structure would result in uncertainty bounds as shown in fig 5 the qrf method does not enforce heteroscedastic error structure rather this error structure was identified by the algorithm from the data the experiments with synthetic data showed results not shown that if the errors are homoscedastic qrf produces homoscedastic error structure and if the errors are heteroscedastic qrf produces a heteroscedastic error structure loas shown in fig 5 do not represent measurement uncertainty only it is likely that structural errors of qrf model are also contributing to these bounds 3 3 how do qrf loas compare to the loas obtained by the runoff ratio method fig 10 shows the loas obtained by the runoff ratio method along with the ensemble of runoff ratios at four of the gauging stations in sjrw ideally the runoff ratios should lie between 0 and 1 the errors in rainfall and streamflow measurements and inexactness of hydrograph separation method however may result in values of runoff ratios greater than one beven and westerberg 2011 indeed a few rainfall runoff events had runoff ratio values greater than 2 which are likely to have occurred due to significant biases in rainfall measurements these periods can be referred to as disinformative periods beven and westerberg 2011 which should not be used for parameter estimation and uncertainty analysis in this study however these events were kept for further analysis as the final aim is to compare the bounds obtained by different methods it may be noted that qrf will not recognize such disinformative periods but it will yield appropriate uncertainty bound for these events making it unlikely that a good model will be rejected by using the loas obtained by the qrf algorithm even if it includes disinformative periods for example if a rainfall event has large negative bias qrf will identify this event as similar to other events with small rainfall and the loas for this event will span a large range of streamflow values fig 10 shows the loas obtained by using the runoff ratio method where similar events were selected using knn method with two different distance thresholds d m n 0 2 and 0 3 and by using qrf method one expects the loas to envelop all the observations and the uncertainty bounds to become wider as the value of d m n increases this is indeed observed in fig 10 with the following special case the observations coincide with the upper loa at a few time steps for small d m n values these cases occur because of the small number of rainfall runoff events available at a station and even smaller number of similar rainfall runoff events this prohibits the construction of robust loas loas obtained by rr qrf method were typically wider than the those obtained by the rr knn method which is partly a consequence of using 0 and 100 percentile values of data in the leaf node for defining these bounds see section 2 5 qrf gauged algorithm yielded tigher loas compared to those obtained by runoff ratio method for a few time steps fig 11 but at other times steps e g between 400 and 420 the qrf loas were wider there is one general similarity between the loas obtained by qrf and runoff ratio method the width of both loas increase or decrease almost synchronously in time except for a few timing errors see above for a discussion of this issue this gives us further confidence that the loas obtained by qrf are able to capture general patterns of measurement uncertainty if the patterns of loas obtained by qrf and runoff ratio method were significantly different that would have disproved the usefulness of qrf in constructing loas 3 4 convergence of loas obtained by qrf algorithm to test the convergence properties of qrf estimated loas with increasing length of data several qrf models were developed using different lengths of training data in these experiments data from only that watershed where loas are to be constructed were used i e gauged single models were developed for each of the four test watersheds 12 different gauged single models were developed using 1 2 12 years of data fig 12 shows the 97 5th percentiles of loas thus obtained using different amounts of data for three stations 04180500 04180000 and 04178000 loa estimates at high flow time steps started to converge when more than three years of data were used but there were a few high flow time steps where loas did not converge at station 04179520 the convergence of loas seems to be much slower than the convergence at other stations loas appear to be converging for low flows as well but more data are required to achieve the final bounds 3 5 limits of acceptability loa created using the qrf ungauged model one of the major advantages of the qrf algorithm is that it can be used to construct loas at ungauged locations fig 13 shows the loas constructed by the qrf ungauged model along with loas constructed by the other models for comparison the loas obtained by the qrf ungauged model were typically wider than the loas obtained by the other models the timing errors between loas and observed streamflow can also be observed for the qrf ungauged model at time step 406 there exists a widening of loas along with a very small peak in observed streamflow but the observed precipitation is either zero or negligible this is clearly because of an error in precipitation magnitude it is likely that there was a small amount of precipitation in the watershed which was not recorded by the precipitation gauges there were a few other such events where very small observed precipitation corresponded to a significant observed streamflow resulting in very high runoff ratios as discussed above therefore depending upon the precipitation magnitudes during current and previous time steps qrf predicts a peak in streamflow such peaks would not have any impact on model inference in the sense that a hydrological model would not produce streamflow peaks in the absence of rainfall and the simulated streamflows would always be enveloped by the loas at these time steps fig 6 shows that more than 60 of the multiplied peaks were enveloped by the qrf loa even for 100 errors f 2 for the ungauged model the analysis suggests that loas obtained by the ungauged model are very conservative this is desirable when the loas are to be constructed at an ungauged location so as to include a large number of rainfall runoff behaviors the results of this analysis are encouraging in terms of usefulness of qrf approach in creating loas at both gauged and ungauged locations 4 logic behind the proposed method in this section a mathematical argument is presented for using dts for constructing loas we hypothesize that if infinite amount of hydrological data are available dt estimated loa will reflect the effect of uncertainty due to errors in rainfall and streamflow measurements this is a hypothetical scenario as infinite data are never available but it serves to illustrate the usefulness of dts in constructing loas and provides a theoretical basis in practical cases the dts would also reflect variability due to other sources as the number of calibration samples approaches infinity the error incurred by a dt approaches optimal bayes error denil et al 2014 which is the irreducible part of the error due to inherent variability in the process and due to measurement errors both epistemic and aleatoric assuming for the sake of discussion that there is no inherent variability in the hydrological processes more on this below then errors incurred by a decision tree approach measurement error as the samples size increases thus the results of denil et al 2014 suggest that decision tree can be used to account for measurement uncertainty even if it holds only for the hypothetical case of infinite data however it may not be immediately clear how the uncertainty bounds obtained by decision trees represent measurement uncertainty in case of infinite sample size here we answer this question and elucidate the logic behind the proposed hypothesis a formal analysis of the proposed hypothesis is provided in appendix a first consider the case where only the streamflow measurements are uncertain and the rainfall measurements are free of errors further assume that the errors in streamflow measurements are unbiased as the sample size increases the diameter of each leaf node approaches zero that is predictor vectors contained in a leaf node are approximately equal a formal proof of this statement is given in appendix a the true streamflow values corresponding to predictor vectors contained in a leaf node are approximately equal and any variations in the observed streamflow would be due to measurement errors thus given an infinite sample the minimum and maximum values contained in the leaf node represent lower and upper bounds over streamflow and the difference between these bounds is due to measurement uncertainty a formal analysis of this case is given in section a 1 second consider the case where only the rainfall measurements are uncertain and the streamflow measurements are error free in this case also the diameter of a leaf node would approach zero for the same reason as in the first case and predictor vectors contained in a leaf node would be near identical as the sample size approaches infinite but due to measurement errors the underlying true values of predictor vectors contained in a leaf node would be different more precisely the projections of predictor vectors on rainfall subspace will be different since there exists a streamflow value corresponding to each true predictor vector the set of streamflow values corresponding to true predictor vectors in a leaf node would represent the effect of measurement uncertainty in predictor vector on streamflow a formal analysis of the second case is given in section a 2 third consider the case where both rainfall and streamflow measurements are corrupted by errors the logic behind this case is similar to the logic discussed above for the first and second cases a formal analysis of this case is given in section a 3 finally we elaborate on inherent variability in hydrological processes the mathematical analyses provided above and in the appendix a implicitly assume that the predictors variables used to train the decision tree are complete in the sense that predictor variables contain all the information that is required to predict streamflow this however is not possible since the physical structure of the watershed itself will be changing continuously albeit only slowly with intermittent large disruptions which will change the hydrological response of the watershed this can be referred to as the inherent uncertainty in hydrological processes which is irreducible therefore given an infinite sample decision trees would also account for this inherent variability along with the measurement uncertainty both measurement uncertainty and inherent variability are generally dominated by epistemic errors since to construct loas only the upper and lower bounds on errors are required for a given rainfall runoff event it is sufficient that the errors incurred in a given event fall in the range of the errors incurred from other similar events further since the errors are epistemic and available data are finite in practice it is possible that the errors of some events do not fall in the range of errors represented in the data therefore accommodation for such outliers needs to be made while using loas for model inference typically 5 of the observations are allowed to fall outside the estimated uncertainty bound in hydrological applications these 5 outliers might well include the timesteps that one is most interested in e g high flows for flood modeling therefore a posteriori analysis of outliers should be carried out a model can be declared unfit for purpose if all or most of the 5 outliers belong to the timesteps of interest it is possible that all the models are rejected as unfit for purpose but nevertheless a model is required for some urgent practical application in this case some of the rejected models with least deviation from the loas might be used and the inverse of the magnitude of deviation can be used as the weight of that model in decision making alternatively instead of defining fraction of outliers beforehand one can report the accepted models for different fractions of outliers 5 summary and conclusions separation of structural and measurement uncertainty was recognized as one of the twenty three unsolved problems in hydrology by blöschl et al 2019 the only way to address this problem is to estimate measurement uncertainty before model calibration this is a difficult task given that statistical properties of rainfall and streamflow measurement uncertainty are poorly understood especially those of rainfall measurements there exist two dominant philosophies to address this problem 1 to assume statistical distributions over measurement uncertainty due to both rainfall and streamflow errors and 2 to construct limits of acceptability loa that provide some bounds on measurement uncertainty before any modeling exercise loa has been used within the glue framework however both of these philosophies may also be combined together in approximate bayes computation abc framework loa can also be used in a purely bayesian framework by defining a likelihood function that penalizes the simulations based on their deviations from the loa defined through a suitable metric the aim of this paper was to test the capability of decision tree algorithms in creating loas that provide meaningful bounds on measurement uncertainty in this study quantile random forest qrf method was used to construct loas the advantages of the qrf method are as follows 1 it can reflect the effect of both precipitation and streamflow measurement uncertainty 2 it can account for timing errors in precipitation 3 it can be applied at the timescale of available data and 4 it can be used to construct loas at ungauged catchments the results show that the loas obtained by using qrf enveloped the uncertainty bounds over streamflow observations measurement uncertainty in streamflow due to aleatory variability was found to be very small it was shown that the statistical structure of qrf uncertainty bound was similar to an uncertainty bound obtained by propagating rainfall uncertainty through a hydrological model some observations include 1 standard deviations of streamflow obtained by the qrf method increase with increasing values of observed streamflow 2 cvs of simulated rainfall time series and qrf uncertainty bound follow the same pattern they decrease with increasing value of rainfall and streamflow respectively 3 the general pattern of increase and decrease of width of uncertainty bound was similar for qrf and runoff ratio methods the qrf method does not contain any mechanism that induces the uncertainty bounds to follow any pre determined patterns therefore existence of these patterns suggests the qrf method is able to identify some of the characteristics of measurement uncertainty from data we cannot conclude that all the characteristics of measurement uncertainty were identified because qrf is unable to extract all the hydrological information from available data for the four sjrw watersheds used as test cases in this study indeed this is likely to be the case for most watersheds since data on all the factors determining the hydrological response of a watershed are not available a timing error between observed streamflow and the loas obtained by the qrf method was observed in all four test watersheds figs 5 and 11 in gauged all and ungauged cases these timing errors are likely due to timing errors in precipitation data figs 5 and 11 show that qrf can compensate for consistent precipitation timing errors in a watershed in gauged single case thus data from other similar watersheds can be useful in constructing loas that capture the effects of precipitation timing errors in general the shorter the length of data available to construct loas the more the data from other similar watersheds will be required the issue of choosing similar watersheds is discussed below another possible reason for timing errors in gauged all case is that data from other watersheds may have introduced disinformation into the loas therefore it appears that loas should be constructed using data from several sets of watersheds so that the effect of both the potential timing errors and disinformation can be accommodated this will in general mean a larger number of behavioral models and higher predictive uncertainty overall the results of this paper indicate potential for the qrf approach for constructing loas at both gauged and ungauged locations in the hypothetical scenario when infinite amount of hydrological data are available the qrf algorithm can actually reflect the effects of measurement uncertainty as shown in the mathematical analysis in appendix a this analysis used the following main assumptions to prove the proposed hypothesis 1 the relationship between predictor and response variables is one to one 2 the mapping between predictor and response variable is continuous 3 the errors in predictor and response variables are unbiased but otherwise the errors could be either aleatoric or epistemic 4 error can be assumed independently and identically distributed within a leaf node we note that assumption 1 was made for mathematical convenience a similar analysis can be carried out without this assumption for a finite sample size the uncertainty bounds obtained by a decision tree include contributions from structural uncertainty of qrf method along with measurement uncertainty a major advantage of qrf method and indeed the loa approach is that it is a non parametric approach for constructing loas and does not resort to strong assumptions on the statistical nature of streamflow and rainfall measurement errors overall the qrf method offers promise as a powerful tool in hydrological model inference rainfall runoff data may also contain disinformative periods to identify disinformation and biases one requires physical understanding of the rainfall runoff processes runoff ratio method is an example of using process based knowledge to identify biases but it is not applicable for baseflow dominated catchments and cannot be applied at ungauged locations moreover runoff ratio method can identify the effect of errors in streamflow and precipitation volume it cannot identify precipitation timing errors qrf method addresses these limitations of the runoff ratio method qrf will not explicitly identify disinformative periods but it will likely define loas for the disinformative periods such that a good model would not be rejected because of these periods further as noted it is possible that data from other watersheds introduce disinformation into the constructed loas an interesting future problem in this respect would be to combine qrf method with catchment similarity analysis such that data from only the watersheds which are known to be hydrologically similar to the parent watershed where loas are be constructed are used this would potentially reduce the disinformation introduced by the data from other catchments while yielding meaningful loas this technique can be particularly useful for prediction in ungauged basins in this paper catchment characteristics in the form of spatially averaged indices such as mean slope mean soil properties etc were used in the qrf method to identify similar catchments however methods based on hydrological process understanding e g wagener et al 2007 may prove to be better at identifying similar catchments one can also use other ml algorithms for creating loas in addition to the qrf method given a finite amount of data in practical applications different algorithms would extract different information from available data and hence a different estimate of loas would be obtained a combination of these different loas will be more desirable for model inference a problem to be explored in future data availability all the data used in this work are publicly available and can be downloaded from the doi https zenodo org record 7697209 zajtxh mkuk credit authorship contribution statement abhinav gupta conceptualization data curation formal analysis methodology software validation visualization writing original draft writing review editing rao s govindaraju conceptualization project administration resources supervision visualization writing review editing pin ching li methodology software venkatesh merwade writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements an earlier version of the paper was substantially revised based on the review comments of keith beven we are grateful to him for his insightful comments on the paper part of this manuscript was prepared when ag was a doctoral candidate at purdue university the rest of the manuscript was written when ag was a maki postdoctoral associate at dri where he was supported by dri s postdoctoral support funds and sulo and aileen maki postdoctoral fellowship this support is gratefully acknowledged pcl was supported by nsf award 1835822 and 2118329 during the period of this work appendix a mathematical analysis of the proposed hypothesis in this section a heuristic mathematical analysis in the support of the proposed hypothesis is provided the aim of the analysis is to clarify the assumptions behind the hypothesis and limitations in practical implementation specifically we show why the data in leaf nodes of a decision tree can be used to capture measurement uncertainty and under what condition structural uncertainty would be small the analysis is divided into three parts for convenience 1 when measurement errors occur in streamflow measurements only 2 when measurement errors occur in rainfall measurements only and 3 when both rainfall and streamflow measurements incur errors we note that the analysis provided below is valid for both aleatoric and epistemic errors a 1 case 1 only streamflow measurements are uncertain first we provide the analysis of the proposed hypothesis under the restriction that only the streamflow measurements contain errors and rainfall measurements are free of errors let x denote the predictor space x x denotes a point in the predictor space and n d x denote the d neighborhood of x in x where d is a suitable distance metric further let us define by y the set containing error corrupted value of a response variable as a1 y y x x x since y x is an error corrupted value it can be written as a2 y x y t x ϵ where y t x denotes the true but unobserved value of the response variable and ε denotes the measurement error in y here ε represent a general error term which can be a function of x and or y the data contained in a leaf node of a decision tree may be approximated as a neighborhood of the points close to its center for example if a leaf node constitutes the set x k x i x i x i 1 k and the point x m x k is close to its center then x k can be treated as a neighborhood of x m to define a neighborhood a distance metric is needed and distance metric chosen defines the shape of neighborhood in the analysis presented below a different distance metric might be required for different leaf nodes of the decision tree this does not pose any challenge to the generality of the analysis the approximation of a leaf node by the d neighborhood is made for the sake of mathematical convenience so that the analysis is manageable similar assumptions have been made by other authors e g denil et al 2014 assumption 1 the mapping between predictor and response variables is continuous assumption 2 the relationship between probability distribution of ε with x and y does not change significantly in a c ball b c x a3 b c x x i d x x i c where c is a sufficiently small number in other words the distribution of ε changes slowly over x assumption 3 without loss of generality we assume that the relationship between true values of predictor and true values of response variables is one to one this assumption is also made for analytical convenience assumption 4 the expected value of ε is zero assumption 5 the response variable y varies smoothly with the predictor variable x this is particularly true for rainfall runoff models where unit increase in rainfall can result in a maximum of unit increase in streamflow all else being equal for every x d n d x there exists a y d y by definition of y by virtue of eq a2 yd x d y t x d ε define y d as a4 y d y d x d x d n d x and define y d t as a5 y d t y t x d x d n d x further define the quantity a6 y d x 1 v o l n d x y d x d x where vol denotes volume assertion 1 the quantity y d x defined in eq a6 approaches the true value y t x as the number of samples increases the proof of this assertion along with technical conditions can be found in brieman et al 1984 and denil et al 2014 these references do not directly consider errors in measurement but the proofs provided in these references are still valid provided assumption 4 holds if assumption 4 is not valid then the prediction error obtained by a decision tree approaches the optimal bayes error note that the discrete version of the eq a6 is the response variable estimated by the rf algorithm therefore the structural errors in rf estimate would decrease arbitrarily as the sample size increases assertion 2 the diameter of the y d t is small if the sample size is large in other words the maximum difference between the y t values contained in y d t would be small let this difference be denoted by d i a y d t we note the following a decision tree aims to create leaf nodes so as to minimize some measure of prediction error such as mean square error on test set the estimated response by the decision tree is the average of the response values contained in a leaf node given by eq a6 and the leaf nodes create a partition of the predictor space x i e the subsets created by the leaf nodes are disjoint and cover the predictor space these requirements are met only if the quantity d i a y d t x is small for each x here y d t is denoted as a function of the argument x for consider n points x 1 x 2 x n x that constitute the training set with corresponding neighborhoods nd x 1 nd x 2 nd x n denote the number of leaf nodes created by the decision tree by m clearly m n further consider the expression for mean square error a7 m s e n 1 n i 1 n y x i y x i 2 where y x i is estimated response given by equation 23 the expression a7 is minimized when each term in the summation is minimized if m n there will be many out of n points that would fall into the same leaf node and therefore will have identical estimate of the response thus msen would not be minimized this seems to imply that for msen to be minimized we need m n due to measurement errors however minimization of msen on training set may not result in minimization of msen on test set and making m n is likely to result in overfitting therefore to satisfy the three conditions above the value of m must be less than n but not much smaller than n as n increases m should also increase otherwise m would become much smaller than n technically this condition translates to the following m and m n 0 as n in decision tree language as n increases the predictor space would be split into smaller and smaller partitioning subregions i e diameter of the leaf nodes would become smaller and smaller hence it follows that dia nd 0 as n if diameter of nd x is small then by assumption 5 and the assumption that values in nd x are error free the d i a y d t is also small in summary if the sample size is large then the decision tree would be able to create small leaf nodes in order to minimize mean square error more technically for n n a and δ 0 a8 d i a y d t δ where n a is some arbitrary large value theorem 1 the set y d approximately captures measurement uncertainty in response variable if the sample size is large proof the minimum value contained in y d is greater than or equal to min y d t ϵ l and the maximum value contained in y d is less than or equal to max y d t ϵ u here εl denotes a value in the left tail of the distribution of ε such that probability of ε taking a value less than or equal to ε l is γl similarly εu denotes a value in the right tail of the distribution of ε such that probability of ε taking a value greater than or equal to εu is γu note that εl and ε u are likely to be negative and positive quantities respectively by assertion 2 the difference between max y d t and min y d t is small for large n and therefore a9 min y d t max y d t y t x using eq a9 the minimum and maximum values contained in y d may be approximated by y t x εl and y t x εu these lower and upper bounds represent the bounds on measurement uncertainty due to errors in streamflow measurements as sample size increases the probabilities γl and γu would approach zero the approximation a9 would become more accurate and thus the proposed hypothesis would become more accurate this completed the analysis of the 1st case in the preceding paragraph we argued mathematically that as the sample size increases and the neighborhood n d x becomes smaller the set y d represents measurement uncertainty in y more accurately in reality n d x cannot be arbitrarily small and the sample size is finite thus y d represents both measurement and structural uncertainty however the structural uncertainty would still be small if the sample size is large enough so as to create small leaf nodes see assertion 2 above and eq a8 practically speaking one can aim only for the modest goal of obtaining an uncertainty bound where majority of width is due to measurement uncertainty fortunately this is useful in practice in the construction of loas as it helps avoid type 1 errors rejecting models with good structures at the cost of a few type 2 errors accepting a few models with bad structures this is a desirable property of the loas beven 2019 a 2 case 2 only rainfall measurements are uncertain let x denote the predictor space x x denote a point in the predictor space and n d x denote the d neighborhood of x in x where d is a suitable distance metric here x represents a vector containing rainfall and other relevant predictor variables let x r denote the component of x containing error corrupted current and time lagged rainfall values x r can be written as a10 x r x r t ϵ x r where x r t is the true value and εx is the error in x r denote by y the set containing y values as defined in eq a1 assumption 6 the expected value of εx is zero assumption 7 we assume that the probability distribution of ε x varies slowly within n d x the probability distribution of εx can be assumed independent and identically distributed within n d x for each x n d x there exists a true value x t and corresponding to each x t there exists a y t value thus we can define a set y d similar to that defined in eq a4 only difference being that the x values are error corrupted in this case assertion 3 the diameter of nd x approaches zero as the sample size increases this assertion follows from the proof of assertion 2 assertion 4 the true value of the values contained in n d x approximate the probability distribution of x for large sample large following assertion 3 it is reasonable to assume that values contained in nd x are approximately equal that is any x d nd x is approximately equal to x but the values contained in nd x are error corrupted therefore the true value corresponding to any x d nd x can be written as a11 x d t x d ϵ x x ϵ x from eq a11 it is clear that x d t is a random variable with mean value x and larger moments defined by εx hence assertion 4 follows corollary 1 the minimum and maximum values contained in n d x can be approximated by x εx l and x εx u respectively here εx l and εx u are defined similarly as εl and εu are defined in theorem 1 again εx l and εx u are likely to be negative and positive quantities respectively assertion 5 there exists a one to one mapping between nd x and y d it can be seen from eq a11 that there exists a unique true value corresponding to each x d nd x for two values contained in nd x to be identical the value of ε x will have to be identical but the probability of such an event is practically zero less than some arbitrarily small δ 0 to be more precise by assumption 3 there exists a one to one relationship between true value of predictor and response variables therefore there must exist a one one mapping between nd x and y d theorem 2 the set y d provides the effect of measurement uncertainty in rainfall on streamflow yt x the truth in this assertion stems from one to one mapping between the elements of n d x and y d assertion 5 and since by assertion 4 n d x provides measurement uncertainty in x y d yields the effect of measurement uncertainty in x on y x the set nd x contains several elements with approximately the same value x but these values are error corrupted the underlying true values will differ due to measurement uncertainty in x for each unique true value in nd x there exists a unique value of y in y d when we observe an error corrupted value x the corresponding response can be any value contained in y d depending upon the error in x therefore the loa corresponding to x should be min y d max y d this completes the analysis of 2nd case the above analysis is valid in the case of large number of samples with finite samples y d would capture measurement uncertainty and structural uncertainty because the diameter of nd x would not be small but a sufficiently large number of samples would result in small structural uncertainty a 3 case 3 both streamflow and rainfall measurements are uncertain here we consider the case where both the rainfall and streamflow measurements are corrupted by errors this case is a combination of case 1 and case 2 the notations and assumptions are same as in previous two cases consider x d nd x and the corresponding response variable y d y d the error corrupted x d and yd can be represented by eqs a2 and a10 respectively theorem 3 the set y d provides lower and upper measurement bounds due to errors in response measurements and the effect of errors in predictor measurements if the sample size is large from theorem 2 clearly y d t would yield the effect of errors in predictor variable measurements here y d t is defined as in eq a5 further note that since response measurement is also error corrupted the values contained in y d can be written as a12 y d x d y t x d ϵ x ϵ y t where x d nd x and y d y d are error corrupted values y t and x d εx are true values of predictor and response variables respectively the term ε represents measurement error in response variable which is a function of y t here ε cannot be assumed independent of y t values since the variation of y t within y d t is large in this case as opposed to that in case 1 denote the set containing true value y t corresponding to each true value in nd x by y d t as in eq a5 then the minimum and maximum values contained in y d are min y d t ϵ l min y d t and max y d t ϵ u max y d t here ϵ l min y d t is the value of ϵ min y d t in the left tail of the distribution such that probability of ϵ min y d t taking a value less than ϵ l min y d t is γl the term ϵ u max y d t is defined similarly for large sample the probability γl will approach 0 the quantities min y d t ϵ l min y d t and max y d t ϵ u max y d t are lower and upper bounds of total measurement uncertainty due to errors in predictor and response variables this completes the proof of case 3 appendix b 
8,per and polyfluoroalkyl substances pfass have become emerging contaminants of critical concern comprehensive understanding of the transport and fate of pfas in the vadose zone a type of water unsaturated porous media is key to determination of the risks of the pfas contamination in the subsurface and to the development of the effective remediation strategies accurate modeling of the pfas transport in the unsaturated porous media is still a challenge due to the variable surface tension induced by the adsorption of pfas to the air water interfaces in an effort to address this challenge we propose a multidimensional modeling framework for the transient pfas transport in the unsaturated porous media based on the second order accuracy finite volume method in the modeling the adsorption of pfas to the solid surfaces and to the air water interfaces is described by the two domain sorption kinetics model i e both the instantaneous and the rate limited pfas adsorptions are taken into account the diffusive and convective terms in the governing equations for the pfas transport and the water flow are discretized by the central difference and the quadratic upstream interpolation for convective kinetics schemes respectively we investigate the effects of the convergent criteria coupling method and variation of the surface tension on the average and the local pfas concentration and water content in the computational domain we find that the convergent criteria should be chosen carefully so as to get the accurate results the differences between the different coupling methods are affected by the boundary conditions the variation in the surface tension due to the variation of the pfas concentration cannot be neglected the dimensionless parameters relevant to the properties of porous media as well as the relation between the surface tension and the pfas concentration play an important role in transport of pfas in the unsaturated porous media the effects of the péclet number damköhler values and fraction of instantaneous sorption are not significant in the ranges studied in this work these findings provide a better understanding of transport of pfas in the vadose zone keywords transport of pfas in vadose zone two domain sorption kinetics model quick scheme variable surface tension data availability data will be made available on request synopsis unlabelled box a multidimensional modeling framework with the second order accuracy is proposed to simulate the transport of pfass emerging contaminants of critical concern in the vadose zone 1 introduction the industrial scale applications of pfass due to their some unique features e g strong chemical and thermal stability sharifan et al 2021 result in their widespread in the environment tokranov et al 2018 wang et al 2017 perfluorooctanoic acid pfoa and perfuorooctanesulfonic acid pfos are two primary pfass the observed concentrations of pfos and pfoa in groundwater are often orders of magnitude greater than the health advisory of 0 07 µg l combined total lyu et al 2018 it has been demonstrated that the vadose zone a type of water unsaturated porous medium can serve as a subsurface reservoir of pfas and thus as a long term contaminant resource to the groundwater anderson et al 2019 dauchy et al 2019 filipovic et al 2015 weber et al 2017 hence it is critical to reveal the transport of pfas in unsaturated porous media one feature for the transport of pfas in unsaturated porous media is that in addition to adsorption of pfas to solid surfaces lv et al 2018 lyu et al 2019 pfas also can accumulate at air water interfaces costanza et al 2019 the surface tension depends on the concentration of pfas at air water interfaces brusseau and van glubt 2019 which is determined by pfas adsorption processes the variation of the surface tension due to the change of the pfas concentration at air water interfaces affects the water flow which on the other hand impacts the pfas transport and hence the adsorption of pfas thus the water flow and the transport of pfas in unsaturated porous media are tightly coupled and interact with each other because of such coupling effects it is still a challenge for the accurate modeling of the pfas transport in unsaturated porous media with the variable surface tension addressing this issue is key to determination of the risks of the pfas contamination in the subsurface and to the development of the effective remediation strategies in an effort to address this issue a multidimensional modeling approach is proposed in the present study to simulate the transport of pfas in unsaturated porous media with the transient water flow and the variable surface tension the studies on modeling the water flow and the transport of pfas in the unsaturated porous media can be classified into two groups the modeling with the steady water flow brusseau et al 2019b brusseau 2020 and the modeling with the transient water flow guo et al 2020 silva et al 2020 zeng and guo 2021 the modeling with the steady water flow is commonly exploited to fit the breakthrough curves obtained from the experiments to determine the parameters depicting the adsorption of pfas to the solid surfaces and to the air water interfaces brusseau 2021 it has been revealed that both the instantaneous and the rate limited adsorptions i e two domain sorption kinetics model need to be considered so as to depict the pfas adsorption brusseau et al 2019a in these studies 1d simulations are usually performed in fact the water flow in the vadose zone is a transient and multidimensional process thus in order to explore the transport of pfas in the vadose zone numerically the multidimensional modeling with the transient water flow is a must in order to develop an accurate multidimensional modeling approach for the transport of pfas in the vadose zone with the transient water flow it is necessary to address the following issues the first is the description of the adsorption of pfas to the air water interfaces since it influences the pfas concentration and hence the surface tension and the water flow the second is the numerical schemes to solve the governing equations for the water flow and the transport of pfas in unsaturated porous media since it determines the accuracy of simulation results the third is the method to couple calculations of the water flow and of the pfas transport since the water flow and the pfas transport interact with each other note that pfas is a type of surfactant for the water flow in unsaturated porous media due to the surfactant induced variation of the surface tension two methods have been employed to couple the water flow and the surfactant transport in the first method the water flow and the surfactant transport are solved within the same iterative loop i e the surfactant concentration at the current time step is used for the water flow calculation simith and gillham 1994 1999 in the second method the surfactant concentration in the last time step is used for the calculation of the water flow henry et al 2001 here the first and second methods are called the iterative coupling ic and time lagged coupling tlc methods respectively the tcl method is more computationally efficient than the ic method and hence has been commonly used e g in the utchem model bhuyan et al 1990 although the ic method is more accurate than the tlc method in order to employ the tlc method it is needed to show that the results obtained from the tlc and ic methods are almost the same it has been revealed that for the 1d surfactant induced water flow in unsaturated porous media the results obtained by these two coupling methods are in good agreement henry et al 2002 but the results from these two coupling methods have not been compared for the 2d cases weather these two coupling methods yield the same results for the multidimensional pfas transport in unsaturated porous media still needs to be clarified up to now only several studies have been reported for the multidimensional modeling of transport of pfas in unsaturated porous media silva et al 2020 modified the unsaturated flow and the transport models in the software of hydrus and realized the 2d simulation of the pfas transport but as the authors stated in their literature they limit their presentation to the pfas transport for the solution concentrations in which the effects of the surface tension and the viscosity are negligible silva et al 2020 that is to say variation of surface tension is not fully considered moreover only the instantaneous adsorption of pfas to the air water interfaces is considered in their studies the coupling between the calculations of water flow and pfas transport is not specified silva et al 2020 zeng and guo simulated the transport of pfas in multi dimensional heterogeneous unsaturated porous media with the variable surface tension zeng and guo 2021 in their study both the water flow and the pfas transport equations are calculated at the same time step however only the instantaneous adsorption is considered and first order upwind scheme is employed to discretize the advection terms in the governing equations zeng and guo 2021 it should be noted that the first order accurate numerical schemes are not enough to get the accurate results versteeg and malalasekera 2007 thus higher order accurate numerical schemes are needed as illustrated in figs s1 in si supporting information because of the limitations of the existing multidimensional modeling for the transport of pfas in unsaturated porous media with the transient water flow and the variable surface tension we propose a multidimensional modeling approach based on the second order accurate implicit finite volume method fvm the fvm has been employed for solving the transient water flow in unsaturated porous media das and nassehi 2003 dey and dhar 2022 lai and ogden 2015 the features of our modeling approach are as follows first the two domain sorption kinetics model is employed to describe the adsorption of pfas to the solid surfaces and to the air water interfaces i e both the instantaneous and the rate limited pfas adsorptions are taken into account to our best knowledge the instantaneous and the rate limited pfas adsorptions are seldom considered simultaneously in the existing studies for modeling the pfas transport in unsaturated porous media brusseau 2020 guo et al 2020 silva et al 2020 zeng and guo 2021 second the diffusive and the convective terms in the governing equations for the water flow and the pfas transport are discretized by the central difference and the quadratic upstream interpolation for convective kinetics quick schemes respectively these two schemes have at least the second order accuracy third the results obtained based on different coupling methods i e ic and tlc are compared and it is revealed that the difference between them can be significant when the boundary conditions e g the inlet flow rate are always changing because of these features the multidimensional modeling framework proposed in the present study paves a way towards the detailed study of the pfas transport in the vadose zone based on the developed modeling framework we also explore the influences of the parameters e g soil properties flow conditions pfas properties on the pfas transport the paper is organized as follows in section 2 the mathematical models are presented the numerical methods are detailed in section 3 the results are presented and discussed in section 4 finally the conclusions are drawn in section 5 2 mathematical models 2 1 water flow the following equation is employed to depict the water flow in unsaturated porous media 1 θ t q 0 where θ is the water content t the time s and q the darcian flux m s the water content is θ sw ϕ for which sw is the water saturation and ϕ is the porosity of the porous medium the darcian flux is equal to 2 q k h z where h is the water pressure head m k the unsaturated hydraulic conductivity m s and z the spatial coordinate m assumed positive upward combing eqs 1 and 2 leads to the mixed form richards equation celia et al 1990 3 θ t k h z 0 to solve eq 3 it is needed to know the relationship between θ and h the relationship between the water content and the pressure head of pure water without pfas h0 has been proposed by van genuchten 1980 4 θ θ s θ r 1 α h 0 n m θ r h 0 0 θ s h 0 0 where θ s is the saturated volumetric water content θ r the residual volumetric water content and α m 1 n and m the retention curve fitting parameters note that h0 in eq 4 represents the pressure head of pure water not equal to the pressure head of water with pfas h as in previous studies simith and gillham 1994 the pressure head of water with pfas can be expressed as 5 h h 0 σ 0 σ where σ0 and σ are surface tensions of water without and with pfas nm 1 respectively the surface tension σ depends on the pfas concentration c mol m3 and can be determined as adamson and gast 1997 chang and franses 1995 6 σ σ 0 1 b l n 1 c a where a mol m3 and b are the curve fitting parameters here we assume that the contact angle is unchanged as the pfas concentration varies substituting eqs 5 and 6 into eq 4 yields 7 θ θ s θ r 1 α h 1 b l n 1 c a n m h 0 θ s h 0 where m 1 1 n the unsaturated hydraulic conductivity k m s in eq 2 can be calculated by the empirical mualem function 1976 8 k θ k s θ l 1 1 θ 1 m m 2 where ks is saturated hydraulic conductivity m s m and l the fitting parameters and θ is the effective water saturation 9 θ θ θ r θ s θ r 2 2 pfas transport the transport of pfas in porous media can be described by the following equation 10 j θ c t r where j is the mass flux of pfas mol m2 s and r is the source term due to adsorption of pfas to the solid surfaces and to the air water interfaces mol m3 s the mass flux of pfas includes the convective flux and the hydrodynamic dispersion flux and is determined as 11 j q c d i j θ c where dij is the hydrodynamic dispersion coefficient m2 s determined as bear 1979 12 θ d i j d t q δ i j d l d t q i q j q θ τ d 0 δ i j where τ θ 7 3 θ s 2 is the tortuosity factor millington and quirk 1961 d0 the diffusion coefficient of pfas in water m2 s and dl and dt the longitudinal and transverse dispersivities m respectively here we assume that the longitudinal and transverse dispersivities are same to each other and equal to 0 2 l 0 44 for which l is the characteristic length m schulze makuch 2005 in eq 12 δij is the kronecker delta function δij 0 if i j δij 1 if i j note that dispersivities of porous media are affected by man factors e g the water content roof and hassanizadeh 2013 topologies of pore spaces meigel et al 2022 and density gradient of fluids nick et al 2009 the correlations between dispersivities and these factors however are still scarce due to lack of the correlations the effects of these factors are not considered in the present study although the models to determine dispersivities can be incorporated into eq 12 easily in our calculations it has been reported that longitudinal dispersivity is larger than the transverse one delgado 2007 the difference between them is still unclear however for this reason they are assumed to be equal the source term r in eq 10 is determined as 13 r ρ b c s t c a w t where cs is the concentration adsorbed pfas on the solid surfaces mol kg ρ b the density of the porous medium kg m3 caw the concentration of pfas adsorbed at the air water interfaces mol m3 the two domain sorption kinetics model is employed to describe the adsorption of pfas to the solid surfaces and to the air water interfaces this model has been successfully applied to simulate the transport of pfos in porous media with the steady water flow brusseau et al 2019 thus the following equations are employed to determine cs and caw in eq 13 14 c s c s 1 c s 2 15 c a w c a w 1 c a w 2 where c s 1 is the adsorption concentration at the solid surfaces in the instantaneous sorption domain mol kg c s 2 is the adsorption concentration at the solid surfaces in the kinetic domain mol kg c aw 1 is the adsorption concentration at the air water interfaces with the instantaneous sorption mol m3 c aw 2 is adsorption concentration at the air water interfaces with the sorption being limited by diffusive mass transfer c s 1 and c s 2 in eq 14 are determined as guo et al 2020 16 c s 1 f s k f c n 17 d c s 2 d t α s 1 f s k f c n c s 2 where fs is the fraction of sorbent for the instantaneous sorption α s the first order rate constant 1 s and kf m3n moln 1 kg and n the fitting parameters c aw 1 and c aw 2 in eq 15 are determined as guo et al 2020 18 c a w 1 f a w a a w k a w c 19 d c a w 2 d t α a w 1 f a w a a w k a w c c a w 2 where faw is the fraction of air water interfaces with instantaneous sorption aaw is the specific air water interfacial area 1 m kaw the adsorption coefficient at the air water interface m and α aw the first order rate constant 1 s the specific air water interfacial area in eqs 18 and 19 is expressed as guo et al 2020 20 a a w x 2 s w 2 x 1 s w x 0 where x 0 x 1 and x 2 are the fitting parameters 1 m the adsorption coefficient kaw in eqs 18 and 19 is determined as adamson and gast 1997 chang and franses 1995 guo et al 2020 21 k a w 1 r t c σ l n c t 1 r t σ 0 b a c where r is the universal gas constant 8 314 j mol k and t is the temperature k the relationship between σ and c is presented in eq 6 by combining eqs 10 19 we can get the equation to describe the transport of pfas in the unsaturated porous media 22 θ c t ρ b f s k f c n t ρ b α s 1 f s k f c n c s 2 f aw a aw k aw c t α a w 1 f a w a a w k a w c c a w 2 d θ c q c 2 3 dimensionless equations as noted transport of pfas in unsaturated porous media is a multiple process including the water flow as well as the pfas transport and adsorption in the studies of problems involving multiple processes dimensionless equations are commonly employed e g in boufadel et al 1999 the advantages of dimensionless equations are 1 the solutions of dimensionless equations depict a family of problems instead of a particular one 2 the key parameters controlling the processes can be obtained i e the dimensionless parameters in dimensionless equations for these reasons dimensionless equations are also employed in the present study the dimensionless equations for the water flow and pfas transport are established based on the following dimensionless parameters 23 t t l u 0 l z z l h h l c c c 0 a a c 0 α α l k s k s u 0 k k s s w e l 1 1 s w e 1 m m 2 a a w a a w l k a w k a w k a w 0 c s 2 ρ b c s 2 c 0 c a w 2 c a w 2 c 0 q q u 0 where l is the characteristic length m equal to the domain size u0 is the value of the mean velocity at the inlet of the domain m s z is the spatial coordinate m assumed positive upward c0 is the inlet pfas concentration mol m3 ks is equal to ks u0 m and l are the fitting parameters sw e is the effective water saturation defined as s w e θ θ r θ s θ r kaw 0 is equal to 1 r t σ 0 b a c 0 substituting eq 23 into eqs 3 and 22 yields 24 θ t k h z 0 25 θ c t r s f s c n t r a w f a w a a w k a w c t r s d a s 1 f s c n d a s c s 2 r a w d a a w 1 f a w a a w k a w c d a a w c a w 2 α l q τ θ p e c q c where rs and raw are the retardation values das and daaw are the damköhler values and pe is the péclet number r s ρ b k f c 0 n 1 represents the ratio of the concentration of pfas adsorbed to the solid surfaces to the concentration of pfas r a w k a w 0 l σ 0 b l r t a c 0 represents the ratio of the concentration of pfas adsorbed to the water air interfaces to the pfas concentration d a s α s l u 0 represents the ratio of the adsorption rate to the solid surfaces to the convective mass transport rate d a a w α a w l u 0 represents the ratio of the adsorption rate to the water air interfaces to the convective mass transport rate p e u 0 l d 0 represents the ratio of the convective transport rate to the diffusive transport rate the relationship between θ and h in eq 24 is described as 26 θ θ s θ r 1 α h 1 b l n 1 c a n m θ r h 0 θ s h 0 cs 2 and caw 2 in eq 25 are described as 27 d c s 2 d t r s d a s 1 f s c n d a s c s 2 28 d c a w 2 d t r a w d a a w 1 f a w a a w k a w c d a a w c a w 2 aaw in eq 25 is determined as 29 a a w x 2 s w 2 x 1 s w x 0 where sw is the water saturation equal to θ θs x0 x0l x1 x1l and x2 x2l 3 numerical methods 3 1 discretization the finite volume method fvm is employed to solve eqs 24 and 25 in fvm the computational domain is discretized into a number of grid cells each grid cell contains a central node and has six faces figs s2 in si the scalar variables such as the water content θ and the dimensionless water pressure head h are stored at the central nodes of the grid cells the velocities are defined at the faces between the grid cells the diffusive and convective terms in eqs 24 and 25 are discretized based on the central difference and quick quadratic upstream interpolation for convective kinetics schemes respectively both of these two schemes have at least the second order accuracy versteeg and malalasekera 2007 to guarantee the numerical solution stable the source term is negatively linearized patankar 1980 as mentioned two types of coupling methods are employed in the present study in the tlc method each variable is defined at a time step n and an iteration step m in the ic method each variable is defined at a time step n a couple step k and an iteration step m for simplicity in the following description in this subsection each variable is defined at a time step n and an iteration step m as that in the tlc method see fig 1 the tlc and ic methods are detailed in the following subsections as in the study of celia et al 1990 the backward euler approximation is employed for the time discretization in this way eq 24 can be discretized to the following linear equation 30 a h i j k n 1 m h i j k n 1 m 1 a h i 1 j k n 1 m h i 1 j k n 1 m 1 a h i 1 j k n 1 m h i 1 j k n 1 m 1 a h i j 1 k n 1 m h i j 1 k n 1 m 1 a h i j 1 k n 1 m h i j 1 k n 1 m 1 a h i j k 1 n 1 m h i j k 1 n 1 m 1 a h i j k 1 n 1 m h i j k 1 n 1 m 1 s h i j k 1 n 1 m s h i j k 2 n 1 m s h i j k 3 n 1 m where the subscripts i j and k represent the coordinates of the grid cells the superscripts n and m represent the time step and the iteration step respectively ah is the coefficient m2 s and sh is the source term m3 s the values of ah and sh are determined by eqs s12 s22 in text s1 in si to determine ah in eq 30 it is needed to know the hydraulic conductivity at the interface between two adjacent grid cells see eqs s12 s17 in text s1 in si two methods can be used to determine the hydraulic conductivities at the interfaces between two adjacent grid cells take k i 1 2 j k n 1 m m s at the interface between grid cells i j k and i 1 j k as an example the first is the arithmetic average method i e k i 1 2 j k n 1 m k i j k n 1 m k i 1 j k n 1 m 2 the second is the weighted harmonic method in this method k i 1 2 j k n 1 m is determined based on the fact that the flow rate from grid cell i j k to i 1 j k is equal to that from grid cell i j k to face i 1 2 j k and also the same as that from face i 1 2 j k to grid cell i 1 j k 31 k i 1 2 j k a i 1 2 j k h i j k h i 1 j k δ x i j k i 1 j k k i j k a i j k h i j k h i 1 2 j k δ x i j k i 1 2 j k k i 1 j k a i 1 j k h i 1 2 j k h i 1 j k δ x i 1 2 j k i 1 j k thus the hydraulic conductivity k i 1 2 j k can be written as 32 k i 1 2 j k δ x i j k i 1 j k δ x i j k i 1 2 j k k i j k δ x i 1 2 j k i 1 j k k i 1 j k the hydraulic conductivity at the face between two grids determined based on the weighted harmonic method i e eq 32 is close to the smaller one of these two grid hydraulic conductivities the smaller the face hydraulic conductivity is the lower the darcian flux and therefore the slower the liquid penetration into the porous media in the present study the initial water content near the liquid injection point of the unsaturated porous media is low based on the properties of the porous media used in the present study we find that the hydraulic conductivity at a grid face determined by the weighted harmonic method eq 32 is rather small when the water content in one of its adjacent grids is low e g close to the minimum water content when the hydraulic conductivity is small liquid shall accumulate near the injection point and a huge computational time is needed for liquid to penetrate into the porous media thus the arithmetic average method i e k i 1 2 j k n 1 m k i j k n 1 m k i 1 j k n 1 m 2 is employed to determine the hydraulic conductivity at the faces between grid cells in our simulation so as to save the computation time however it should be noted that the hydraulic conductivity at a grid face determined by the weighted harmonic method eq 32 is more accurate than the one obtained by the arithmetic average method similarly eq 25 can be discretized into the following linear equation 33 a c i j k n 1 m c i j k n 1 m 1 a c i 1 j k n 1 m c i 1 j k n 1 m 1 a c i 1 j k n 1 m c i 1 j k n 1 m 1 a c i j 1 k n 1 m c i j 1 k n 1 m 1 a c i j 1 k n 1 m c i j 1 k n 1 m 1 a c i j k 1 n 1 m c i j k 1 n 1 m 1 a c i j k 1 n 1 m c i j k 1 n 1 m 1 s c i j k l 1 n 1 m s c i j k l 2 n 1 m where ac is the coefficient m3 and sc is the source term mol the values of ac and sc are determined by eqs s36 s56 in text s2 in si as mentioned quick scheme is employed discretize the convective term in eq 33 in the quick scheme the variable i e the pfas concentration at the interface between two neighboring grid cells is determined by those of three grid cells close to this interface thus to determine the pfas concentration at the interface near the boundary of the computational domain the pfas concentration in the mirror grid cell outside of the computational domain is needed see figs s3 in si its value is determined based on the linear extrapolation approach leonard 1979 although the quick scheme has an accuracy of third order in terms of taylor series truncation error on a uniform mesh implementation of boundary conditions can be problematic with such high order schemes versteeg and malalasekera 2007 our calculations show that oscillatory behavior is observed for the solutions of the case in fig s1b if the number of grid cells is reduced the fact that the quick scheme can give undershoots and overshoots has led to the development of the second order schemes that avoid these issues versteeg and malalasekera 2007 for instance the tvd total variation diminishing scheme has been developed to achieve oscillation free solutions nevertheless it should be noted that if used with care and judgement the quick scheme can give very accurate solutions of convection diffusion problems versteeg and malalasekera 2007 3 2 numerical scheme by applying eqs 30 and 33 to each grid cell in the computational domain the linear equations for the normalized water pressure head and the normalized pfas concentration are established the bicgstab bi conjugate gradient stabilized method van der vorst 1992 is employed to solve the linear equations the procedure to solve the linear equations for the dimensionless water pressure head eq 30 is illustrated in the red box in fig 1 the procedure to solve the linear equations for the dimensionless pfas concentration eq 33 is illustrated in the blue box in fig 1 the detail procedures are presented in text s3 in si in our calculations the discretized dimensionless water flow equation eq 30 and the discretized dimensionless pfas transport equation eq 33 are solved separately but as we mentioned above the water flow and the pfas transport are coupled two coupling methods tlc and ic are employed in the tlc method the dimensionless water pressure head water content and dimensionless pfas concentration at are calculated as follows see fig 1a 1 based on the dimensionless pfas concentration at the time step n c n the dimensionless water pressure heat at the time step n 1 is determined 2 the darcian flux and the water content at the time step n 1 q n 1 and θ n 1 are obtained by using eqs 2 and 26 respectively 3 based on the determined θ n 1 and q n 1 c n 1 is determined 4 the steps 1 3 are repeated until the total injection time is reached in the tlc method the dimensionless water pressure heat at the time step n 1 is obtained based on the dimensionless pfas concentration at the time step n as compared to the tlc method one more calculation iteration is added in the ic method see fig 1b this added calculation iteration is used to couple the water flow and the pfas transport at the same time step in the ic method the dimensionless water pressure head water content and dimensionless pfas concentration are calculated as follows 1 the dimensionless water pressure head water content and dimensionless pfas concentration at the time step n 1 and the couple step k are updated h n 1 k θ n 1 k and c n 1 k are updated at the initial couple step i e k 0 h n 1 k 0 hn θ n 1 k 0 θ n and c n 1 k 0 cn 2 based on c n 1 k the water pressure head at the time step n 1 and the couple step k 1 h n 1 k 1 is obtained 3 the darcian flux and the water content at the time step n 1 and the couple step k 1 q n 1 k 1 and θ n 1 k 1 are obtained by using eqs 2 and 26 respectively 4 based on q n 1 k 1 and θ n 1 k 1 the dimensionless pfas concentration at the time step n 1 and the couple step k 1 c n 1 k 1 is determined 5 steps 2 5 are repeated until the convergent criterion is achieved the convergent criterion is that the difference in the dimensionless pressure head and the dimensionless pfas concentration obtained between two successive couple steps is smaller than the prescribed value 6 the steps 1 5 are repeated until the total injection time is reached 3 3 numerical validation to validate the proposed numerical method mentioned above we compare in figs s4 in si our simulation results for the transient water flow in the unsaturated porous media against the results reported in celia et al 1990 and our simulation results for the transport and reaction processes in saturated porous media against the analytical results presented in corapcioglu and haridas 1985 in these two previous studies only one dimensional equations are solved thus in our calculations the number of grid cells is 1 in the x and y directions and 100 in the z direction the initial and boundary conditions as well as the detailed governing equations are detailed in text s4 in si figs s4a in si shows the profile of the water pressure head at the time of 360 s figs s4b in si shows the profiles of the normalized solute concentration at the time of 100 and 1000s in figs s4 in si the horizontal axis is the distance from the inlet where water or solution is injected into the computational domain the vertical coordinate is the water pressure head in figs 4a in si and in figs 4b in si it is the normalized solute concentration the normalized solute concentration is the solute concentration divided by the one at the inlet as shown in figs s4 in si the present simulation results agree well with the results presented in the previous literatures such agreement demonstrates the validity of the proposed numerical method in order to validate that our proposed model can depict the transport of pfas in unsaturated porous media we also perform 1d simulation and compare the results against the experimental results of the transient transport of pfas in an unsaturated porous material with constant water content for various cation types and ionic strengths all parameters except faw and αaw used in simulations are determined in accordance with the experiment since the experimental results show that adsorption of pfas to solid surfaces is negligible it is not considered in the simulation by adjusting the values of faw and αaw the simulation results can match the experimental data as illustrated in figs s5 in si such matching validates the effectiveness of our proposed model 3 4 computational domain considering the computational costs we use the quasi 2d square computational domain the number of grid cells in x direction is 1 in our calculations in the following description we focus on the y z plane the schematic of the computational domain is illustrated in fig 2 the boundary conditions are also illustrated in this figure here the top bottom left and right boundaries are at z 1 z 0 y 0 and y 1 respectively the left and right boundaries are impermeable thus at y 0 and y 1 qy 0 and c y 0 the bottom boundary is fully saturated with water i e θ θ s at z 0 in addition c z 0 at z 0 the pfas solution is injected at the top boundary the injection zone is at the middle of the top boundary and the length of the injection zone in the y direction is 1 5 of the side length of the computational domain at the injection zone qz 1 and c 1 the non injection part of the top boundary is impermeable i e qz 0 and c z 0 the initial conditions are as follows the concentrations of pfas in water at solid surfaces and air water interfaces are zero i e c cs caw 0 in the computational domain the initial water content in the computational domain is determined by solving the equation k h z 0 with the following boundary conditions the bottom boundary z 0 is saturated with water whereas the others are impermeable 4 results and discussion the quasi 2d square computational domain is divided into 1 50 50 grid cells in x y and z directions in the following description on the computational domain we focus on only the plane in the y and z directions the characteristic length l is taken as the side length of the computational domain the dimensionless of each grid cell is 1 50 in the present study we explore not only the numerical method for the solution of transport of pfas in unsaturated porous media but also the influences of the parameters e g soil properties flow conditions pfas properties on the pfas transport the pfas transport depends on the water flow as indicated by eq 24 the water flow in the unsaturated porous media is controlled by k as well as the relation between θ and h k depends on u0 at the inlet boundary as well as ks which is the intrinsic property of the porous material the relation between θ and h depends on not only the intrinsic property of the porous material e g the dimensionless parameters α n m θr and θs in eq 26 but also the relation between the surface tension and the pfas concentration e g the dimensionless parameters a b and c in eq 26 since α αl a a c0 and c c c0 the relationship between θ and h also depends on the size of the domain l and the inlet pfas concentration c0 as indicated by eq 25 the pfas transport in the unsaturated porous media depends on the dimensionless numbers da s da aw rs raw and pe the dimensionless parameters fs faw and the relation between the dimensionless specific air water interfacial area and the water saturation i e x0 x1 and x2 in eq 29 here x0 x1 and x2 are the intrinsic properties of porous media collectively the dimensionless parameters and numbers controlling the pfas transport in the unsaturated porous media can be clarified into three categories 1 the ones relevant to the intrinsic properties of porous media k α n m θr θs x0 x1 and x2 2 the ones relevant to the relation between the surface tension and the pfas concentration a and b and 3 the dimensionless numbers da s da aw rs raw and pe and the dimensionless parameters n fs and faw relevant to the pfas transport and reaction in the present study x0 x1 x2 θr θs and n are assumed to be constant see table s1 in addition m 1 1 n the values of ks α and n are different for various porous media as for soils ks can vary from 5 10 8 to 5 10 4 m s carsel and parrish 1988 leij 1996 bonan 2019 α from 0 8 to 14 5 m 1 carsel and parrish 1988 and n from 1 09 to 4 guo et al 2020 carsel and parrish 1988 in our study n varies from 2 to 3 and 4 α αl varies from 1 12 to 28 based on the assumption that the domain size is l 2 m ks varies from 0 01 to 44 based on the assumption that the value of the inlet mean velocity is u0 200 cm day the values of a and b in the relation between the surface tension and the pfas concentration depends on the chemistry of the pfas solution e g the ionic strength and ph it is reported that as the ionic strength varies a can range from 7 41 to 47 4 µg l li et al 2021 and b from 0 03 to 0 25 li et al 2021 in the present study a varies from 0 004 to 0 4 based on the assumption that the inlet pfas concentration is 1 mol m3 and b varies from 0 01 to 0 107 in this work daaw varies from 0 008 to 8 this range is similar to that in brusseau et al 2020 the variation of das is the same as daaw kf is reported to be 0 055 10 3 mol kg mol m3 n with n being 0 85 guo et al 2020 following this point the value of rs varies from 0 091 to 10 based on the variation range of a and b mentioned above the value of raw is varied from 10 8 to 1 552 10 6 the value of pe ranges from 10 to 85 733 the values of fs and faw are in the range 0 1 the reference values of the dimensionless parameters mentioned above can be found in table s1 in the following sections 4 1 4 3 the effects of numerical methods on simulation results are explored and the dimensionless parameters used for different numerical methods are the same the dimensionless step time is δt 4 63 10 5 the dimensionless total injection time is ttotal 4 8 in the sections 4 4 4 6 the effects of the dimensionless parameters mentioned above on the water content and pfas concentration in porous media are studied in order to reveal the roles of these dimensionless parameters only the one under investigation is varied whereas the others are the reference values the dimensionless time step is δt 1 157 10 4 the dimensionless total injection time is ttotal 12 in order to achieve the steady state of pfas transport the convergent criterion is that the sum of the relative error for each grid cell between two successive calculations is not more than 10 5 tlc method is employed to couple the water flow and the pfas transport since as disclosed below simulation results obtained from tlc and ic are similar if the boundary conditions are not varied tcl method is more computationally efficient variation of the surface tension with the pfas concentration is considered 4 1 effects of the convergent criteria two types of convergent criteria are compared the first type denoted as cc1 is that the sum of the relative error for each grid cell between two successive calculations is not more than 10 5 the second type denoted as cc2 is that the largest relative error for the grid cell is not more than 10 5 for the calculations to illustrate the influences of the convergent criteria in this subsection the ic method is employed and the step time is δt 4 63 10 5 the influences of the convergent criteria on the dimensionless average pfas concentration and the average water content are illustrated in fig 3 the average water content is defined as the ratio of the water volume to the volume of the porous material i e the volume of the computational domain 34 θ a v e v w v p m v i j k θ i j k v p m where vw is the water volume vpm the volume of the porous material and vi j k and θi j k the volume and the water content of the grid cell with the coordinate i j k respectively the normalized average water content is θ a v e θave θs where θ s 0 294 is the saturated volumetric water content similarly the dimensionless average pfas concentration is defined as 35 c a v e n p f a s v w c i j k v i j k θ i j k v i j k θ i j k where c i j k is the dimensionless pfas concentration in the grid cell with the coordinate i j k as shown in fig 3a the effects of the convergent criteria on the dimensionless average pfas concentration are significant the relative error for the dimensionless average pfas concentration between cc1 and cc2 i e ε c a v e 100 c a v e c c 1 c a v e c c 2 c a v e c c 1 can be as large as 35 at the time of t 1 by contrast the normalized average water contents obtained from cc1 and cc2 are similar and the relative error ε θ a v e 100 θ a v e c c 1 θ a v e c c 2 θ a v e c c 1 is not more than 4 1 see fig 3b we also compare in fig 4 the variations with the dimensionless injection time t of the distributions of the dimensionless pfas concentrations c obtained from cc1 and cc2 the relative error for the local dimensionless pfas concentration ε c l o c can reach to 83 at the injection time of t 4 in particular for the cc2 case c can reach to 1 79 at t 4 this is unrealistic since c in the computational domain cannot be larger than 1 e g see the case of cc1 at t 4 8 ε c l o c can reach to 46 even though the relative error for the dimensionless average pfas concentration ε c a v e is not more than 4 see fig 3 for local water content the relative error obtained with cc1 and cc2 couldn t be neglected either as illustrated in figs s6 in si thus cc1 gives a tighter convergence with less errors as compared to cc2 and the convergent criteria need to be selected carefully 4 2 effects of the coupling method as we mentioned above two types of coupling methods i e tlc and ic are employed to couple the water flow and the pfas transport the ic method has one more calculation iteration than the tlc method hence the computational time for the ic method 48 48 h is longer than that for the tlc method 26 64 h we find that the dimensionless average pfas concentration and the normalized average water content obtained with these two iterative schemes are almost the same see fig 5 the results shown in these figures are obtained with cc1 and δt 4 63 10 5 as can be seen the influence of the coupling method is trivial the results presented in fig 5 are obtained with δt 4 63 10 5 we also wonder whether the pfas concentrations and the water contents obtained from the tlc and ic methods are similar or not when the step time is larger since the larger step time is usually employed in the modeling of transport processes in the vadose zone we compare in figs s7 in si the variations with the dimensionless injection time t of the dimensionless average pfas concentration c a v e and of the normalized average water content θ a v e obtained from the tlc and ic methods with cc1 and δt 4 63 10 2 the relative error is very small except for the low dimensionless pfas concentration if the boundary conditions are constant the distributions of the pfas concentration and of the water content will reach the steady state as the pfas injection continues the distributions of the pfas concentration and of the water content at the steady state are supposed to be the same for isa and isb as a result the difference in the pfas concentration and in the water content between the tlc and ic methods diminishes as the pfas continues to flow see figs s8 and s9 in si on the other hand if the boundary conditions are not constant e g the inlet pfas concentration is always changing during the pfas injection then the pfas concentration and water content may not reach a steady state in this case the difference in the pfas and in the water content between the tlc and ic methods may not diminish to demonstrate this conjecture we perform the calculations with the inlet pfas concentration varied periodically while other conditions are same with the base case the convergent criterion is cc1 and the step time is δt 4 63 10 2 each period is δt period 0 213 in the first 0 037 dimensionless time of each period c inlet 1 while c inlet 0 in the remaining time of each period the variations with the injection time t of the dimensionless average pfas concentrations c a v e and of the normalized average water contents θ a v e obtained from the tlc and ic methods are compared in fig 6 the relative error for the normalized average water content ε θ a v e obtained from tlc and ic fluctuate around 2 5 by contrast the relative error for the dimensionless average pfas concentration ε c a v e can be significant from this point of view we infer that the difference between c a v e obtained from the tlc and ic methods may not diminish the variations with the injection time t of the distributions of the dimensionless pfas concentration c for the variable boundary conditions are compared in fig 7 at the early stage of the injection of pfas e g t 0 2316 the maximum relative error for the local dimensionless pfas concentration can reach to 44 015 4 such high ε c l o c is mainly attributed to the low pfas concentration at the early stage of the pfas injection as we discussed above after this early stage of pfas injection ε c l o c is decreased but can still be more than 40 the variations with the injection time t of the distributions of the normalized water contents θ for the variable boundary conditions are compared in figs s10 in si the relative error for the local water content ε θ l o c can be more than 10 thus the differences in the pfas concentration and in the water content between the tlc and ic methods are affected by the boundary conditions the pfas concentration and the water content obtained from the tlc and ic methods are rather similar when the injection velocity and inlet pfas concentration are constant but in the case of the variable inlet pfas concentration and the constant injection velocity the difference in the pfas concentration between the tlc and ic methods cannot be neglected in fact the boundary conditions e g the inlet pfas concentration for the vadose zone are variable thus the ic method needs to be employed for modeling of the transport of pfas in the vadose zone 4 3 effects of the variable surface tension to illustrate the effects of the variable surface tension induced by the change of the pfas concentration at the air water interfaces we compare the pfas concentration and the water content obtained from the constant and the variable surface tensions cst and vst in the calculations the step time coupling method and convergent criterion are δt 4 63 10 5 ic and cc1 respectively in the cases of cst the surface tension with pfas is assumed to be the surface tension without pfas σ0 the variations with the injection time t of the dimensionless average pfas concentrations c a v e and of the normalized average water contents θ a v e obtained from cst and vst are compared in fig 8 as shown in fig 8a the relative error for the dimensionless average pfas concentration ε θ a v e between vst and cst peaks at the very beginning of the pfas injection i e 190 6 when the dimensionless pfas concentration is relatively small as pfas injection continues the ε θ a v e decreases and becomes smaller than 1 the initial average water content is 0 254 the normalized average water content is θave 0 864 after the pfas is injected the normalized average water content increases slightly and then reaches a stable level quickly in the cst case in the vst case the normalized average water content reduces however see fig 8b the explanation is as follows in the vst case the retention curve is sensitive to the pfas concentration as mentioned above for a given water pressure head the water content decreases as the pfas concentration increases fig 9 as pfas injection continues the average pfas concentration increases first and then levels offs see fig 8a as a result the average water content decreases in the vst case by contrast in the cst case the retention curve is not affected by the pfas concentration therefore the average water content increases as water flows as shown in fig 8b the relative error for the normalized average water content ε θ a v e obtained from cst and vst increases first and then levels off at about 73 the distributions of the normalized water content θ at t 4 8 obtained from cst and vst are presented in fig 10 the bottom of the computational domain is fully saturated with water where the dimensionless water pressure head h is zero the distribution of the dimensionless water pressure head at t 4 8 is shown in fig 11 as shown in fig 9 the effects of the dimensionless pfas concentration on the retention curve are negligible when h 0 1 for z 0 1 h ranges from 0 1 to 0 for z 0 1 fig 11 this contributes to explain why the distribution of the normalized water contents for the cst and vst cases are similar in the zone of z 0 1 in the regions with z 0 2 the dimensionless water pressure head ranges from 1 to 0 2 in this h range the dimensionless pfas concentration is important to the retention curve fig 9 thus the difference between the local water contents obtained from cst and vst are significant in the regions of z 0 2 the maximum relative error for the local water content obtained from cst and vst is 363 1 since the average water content obtained with cst and vst can be quite different the variation of the surface tension should be taken into account to describe the transport of pfas in the vadose zone 4 4 effects of k α and n effects of the dimensionless parameters relevant to the properties of porous media i e k α and n on the water flow and transport of pfas are illustrated in fig 12 values of cave and θave with t depend on the values of k α and n the effects of k α and n on transport of pfas in unsaturated porous media cannot be neglected as shown in fig 12b and 12d with the increase of t θave increases first and then decreases the increase of θave is owing to the injection of pfas solution with pfas injection the pfas concentration increases which in turn results in the decrease of θave the reason is that for a given dimensionless water pressure head h θ is lower for higher c fig 9 note that θave remains almost constant after t 2 indicating that the water flow reaches the steady state the relevant variations of cave are presented in fig 12a and 12c it is indicated that after t 2 cave is greater for the higher θave the reason could be as follows the higher θ is the lower aaw eq 29 thereby leading to less pfas adsorbed to the water air interfaces and consequently higher pfas concentration as illustrated in fig 12b and 12d for a given t θave decreases with the increase of α and n this stems from the fact that for a given h and c θ decreases with the increase of α and n see eq 26 the variations of cave and θave with t for different k are illustrated in fig 12e and 12f as mentioned k is related to the saturated hydraulic conductivity of porous media in the case of low k e g k 0 1 and 0 01 the flow resistance is higher thereby hindering the flow of the pfas solution into the deep of porous media this in turn results in the accumulation of pfas solution near the injection site see fig s11 if the water content at the injection site increases to a critical value then the constant flow boundary condition at the injection site is shifted to the constant water head boundary condition for the constant water head boundary condition at the injection site the flow rate of the pfas solution is smaller for the lower k as a result at the beginning of pfas injection θave increases faster in the case of higher k as illustrated in fig 12f attributed to the higher pfas flow rate because of the increased pfas flow rate cave also increases faster in the case of higher k at the beginning of pfas injection as shown in fig 12e as a result cave at a given t is greater with higher k 4 5 effects of a and b the effects of a and b relevant to the relation between the surface tension and the pfas concentration on cave and θave are presented in fig 13 at the beginning of pfas injection i e t 0 25 the effects of a on cave and θave are trivial after t 1 θave for a given t increases with the increase of a fig 13b after t 0 25 θave for a given t increases with the decrease of b fig 13d the reasons are as follows for a given h and c θ increases with the increase of a and decrease of b eq 26 as θ increases aaw decreases eq 29 thereby reducing the adsorption of pfas to the water air interfaces this explains why for a given t cave increases with the increase of a and the decrease of b after t 1 see fig 13a and 13c 4 6 effects of das daaw rs raw pe faw and fs the effects of the dimensionless parameters relevant to the pfas transport and reaction on the water flow and pfas transport are explored in this subsection the effects of the retardation numbers rs and raw on cave and θave are presented in fig 14 the retardation numbers represent the ratio of the concentration of adsorbed pfas to the concentration of the pfas solution thus the higher the retardation number is the lower cave as shown in fig 14 as illustrated in fig 14b after t 1 θave increases with the increase of rs mainly attributed to the decreased cave by contrast θave remains almost constant regardless of raw after t 6 fig 14d the influences of pe varied from 10 to 85 733 on cave and θave are not significant see fig s12 the main reason is that τ θ p e is smaller than α l q in the first term at the right hand side of eq 25 we find that when effects of pe on cave and θave can be neglected when pe 100 the effects of the damköhler numbers das and daaw as well as faw and fs on cave and θave are not significant see figs s13 s15 especially after t 5 cave and θave are almost the constant regardless of faw and fs it should be noted that das fs daaw and faw characterize the kinetic solid phase adsorption and the kinetic air water interfacial adsorption this thus indicate that the effects of the kinetic adsorption on the pfas transport are not significant in agreement with the study reported in zeng et al 2021 5 conclusions in this work a two dimensional modeling framework with the second order accurate is proposed to investigate the numerical methods for coupled solution of water flow and transport of pfas in unsaturated porous media as well as the influences of the parameters e g soil properties flow conditions pfas properties on the pfas transport as regarding to the numerical method the effects of the convergent criteria coupling method and variation of the surface tension are explored on the average and the local pfas concentration as well as the water content in the computational domain two types of convergent criteria i e cc1 and cc2 are employed the difference between the pfas concentrations obtained with cc1 and cc2 could be not neglected thus cautions are needed to set the convergent criteria to couple the pfas transport and the water flow in unsaturated porous media two types of coupling methods are used i e the tlc and ic methods the results obtained from the tlc and ic methods are almost the same if the boundary conditions are constant but for the variable boundary conditions e g the periodic variation of the inlet pfas concentration the difference between the pfas concentrations in the computational domain obtained from the tlc and ic methods cannot be neglected this implies that the ic method needs to be employed to model the transport of pfas in the vadose zone since the boundary conditions for the vadose zone are always changing in the true cases the effects of the variation of the surface tension are also explored by comparing the results obtained with the cst and vst the difference between the water contents obtained from cst and vst cannot be neglected although the pfas concentration obtained from cst and vst is trivial variation in the surface tension due to the variation of the pfas concentration needs to be considered if the water content is the objective of the study the parameters that control transport of pfas in porous media can be classified into three categories 1 those relevant to properties of porous media 2 those relevant to relationships between surface tension and concentrations of pfas and 3 those relevant to transport of pfas and reactions with surfaces exploring the role of these parameters contribute to reveal the primary factors controlling the water flow and transport of pfas in the vadose zone our studies show that k α and n in category 1 a and b in category 2 and rs and raw in category 3 can play important roles in flow of water and thus transport of pfas in unsaturated porous media alternatively effects of das daaw pe faw and fs relevant to transport of pfas and their reactions are not significant since αs is included only in das αaw only in daaw d0 only in pe we infer that the effects of αs αaw d0 faw and fs on transport of pfas in unsaturated porous media are not significant at least in the parameter ranges studied in the present study among the dimensionless parameters k α n a b rs and raw which are important in describing transport of pfas k α n and rs actually depend on the physical and chemical properties of the porous media the domain size l and the inlet boundary conditions u0 and c0 alternatively a b and raw depend on domain size l the inlet concentration of pfas c0 as well as a and b it must be noted that a and b are parameters for the relationship between surface tension and concentration of pfas thus the main factors controlling transport of pfas in unsaturated porous media are the physical and chemical properties of the porous media the domain size the inlet pfas injection velocity and the inlet concentration of pfas as well as the parameters a and b in the relation between the surface tension and the pfas concentration which depend on both properties of pfas and water chemistry the parameters a and b are a function of not only properties of pfas but also water chemistry including ionic strength and types of cations le et al 2021 and 2022 li et al 2021 which are not uniform in the vadose zone and its distribution depends on the water flow which is coupled to transport of pfas thus in the vadose zone the variation of the water chemistry the water flow and transport of pfas are coupled and interact with each other to disclose the characteristics of transport of pfas in the vadose zone it is a necessity to take into account the effects of water chemistry pfas also can be absorbed to the interfaces between water and non aqueous phase liquids napls developing the mathematical models including these factors will be an extension of the present study moreover the computational efficiency needs to be further improved these issues will be explored in detail in the future studies credit author statement rui wu coding writing review editing xiaoxing li formal analysis original draft yuanyuan sun validation piotr szymczak methodology review wentao jiao conceptualization review editing text s1 presents the detailed discretization of the flow equation text s2 presents the detailed discretization of the pfas transport equation text s3 presents the validations of the proposed numerical method fig s1 illustrates the superior of the second order accurate numerical scheme as compared to the first order accurate numerical scheme mirror grid cell used in our numerical method figs s2 and s3 illustrate the grid cells figs s4 and s5 show the validation of the present model and numerical methods figs s6 to s10 are the results regarding the effects of the convergent criteria coupling method and variation of the surface tension on the average and the local pfas concentration and water content figs s11 to s15 are the results regarding the effects of dimensionless parameters k pe das daaw fs and faw on the pfas concentration and water content declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests yuan sun reports administrative support was provided by national natural science foundation of china acknowledgments rui wu is grateful for the support of sino german center m 0545 yuanyuan sun is grateful for the support of the national natural science foundation of china no 42077109 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2023 104490 appendix supplementary materials image application 1 
9,interactions between the tide and sloping sea boundary make watertable fluctuations in coastal unconfined aquifers complicated based on a perturbation method we derived a new analytical solution to predict watertable fluctuations for coastal unconfined aquifers with a sloping sea boundary following validation with a numerical model the analytical solution was used to explore the effects of the vertical flow in the saturated zone and dynamic effective porosity on watertable fluctuations results show that the new analytical solution accurately predicts watertable fluctuations for coastal unconfined aquifers with a sloping sea boundary compared with sand coastal unconfined aquifers both vertical flow and dynamic effective porosity effects on watertable fluctuations are more pronounced for loam coastal unconfined aquifers vertical flow has a minor influence on the fluctuation amplitude while it significantly decreases the phase lag of the watertable fluctuation at a given location in contrast to vertical flow accounting for the dynamic effective porosity not only decreases the phase lag but also significantly amplifies the fluctuation amplitude for a given location which enables watertable wave propagation further inland increasing the beach slope weakens the effects of the vertical flow and dynamic effective porosity on watertable fluctuations furthermore including either the vertical flow or dynamic effective porosity effects leads to a lower watertable overheight these results highlight the importance of vertical flow and dynamic effective porosity effects in models of watertable fluctuations keywords analytical solution numerical simulation amplitude decay phase lag overheight 1 introduction coastal unconfined aquifers are hotspots for land ocean interactions subject to oceanic oscillations e g tides and waves water periodically flows into or drains out of coastal unconfined aquifers leading to watertable fluctuations that directly impact a range of groundwater dependent processes such as beach erosion seawater intrusion submarine groundwater discharge solute transport and chemical loading to the ocean e g parlange et al 1984 li et al 1999 moore 2010 xin et al 2010 bakhtyar et al 2011 werner et al 2013 robinson et al 2018 an accurate prediction of watertable fluctuations is essential for understanding these groundwater dependent processes previously substantial efforts were devoted to predicting watertable fluctuations in coastal unconfined aquifers e g parlange et al 1984 nielsen 1990 barry et al 1996 raubenheimer et al 1999 li et al 2000a b robinson et al 2006 heiss and michael 2014 shoushtari et al 2016 luo et al 2023 most of these studies investigated watertable fluctuations based on the one dimensional boussinesq equation since it is computationally efficient and reveals explicit relations between parameters that affect watertable fluctuations initially the boussinesq equation was used to describe groundwater flow in the saturated zone e g parlange et al 1984 nielsen 1990 bear 2012 with time more laboratory and field data were presented and it was found that the predictions of the classical boussinesq equation deviate from measurements therefore the boussinesq equation was extended to account for different factors including the vertical flow in the saturated zone e g nielsen et al 1997 li et al 2000b unsaturated flow e g parlange and brutsaert 1987 barry et al 1996 li et al 1997 jeng et al 2005b kong et al 2013 2015 and hysteresis e g kong et al 2016a watertable fluctuations in coastal unconfined aquifers predicted using boussinesq based equations show an asymptotic amplitude decay rate and zero phase lag increase rate standing wave behavior of watertable waves with increasing neωd ks where ne is the static effective porosity ω 2π t t 1 is the angular frequency with t t being the fluctuation period d l is the mean sea level height and ks lt 1 is the saturated hydraulic conductivity e g barry et al 1996 nielsen et al 1997 li et al 2000a kong et al 2013 2015 however this limiting behavior was not found in the laboratory experiments of shoushtari et al 2016 in which fluctuation period effects i e neωd ks on watertable wave propagation in the unconfined aquifer with a vertical sea boundary were considered shoushtari et al 2016 observed an increase of both the amplitude decay rate and phase lag increase rate of watertable waves with increasing neωd ks and hence concluded that all existing boussinesq equations cannot predict experimental results correctly effective porosity is the volume of water that an aquifer drains or imbibes per unit surface area of aquifer per unit change of the watertable height childs 1960 it is usually treated as a constant e g barry et al 1996 nielsen et al 1997 li et al 2000a kong et al 2013 2015 nevertheless experimental field and numerical evidence show that the effective porosity depends on the porewater velocity during watertable fluctuations e g cartwright et al 2005 rabinovich et al 2015 pozdniakov et al 2019 more recently luo et al 2023 developed a modified boussinesq equation that includes vertical flow and dynamic effective porosity effects and showed that its predictions compare well with a wide range of experimental data compiled from parlange et al 1984 cartwright et al 2003 shoushtari et al 2016 and kong et al 2016a i e the modified boussinesq equation accurately predicts watertable fluctuations for coastal unconfined aquifers with a vertical sea boundary including for large neωd ks most of abovementioned work regarding watertable fluctuations assumes a vertical sea boundary although sloping beaches are common vos et al 2020 nielsen 1990 developed an analytical approximation to predict watertable fluctuations in coastal unconfined aquifers with a sloping sea boundary using a perturbation method however this analytical solution only matches the time varying sea boundary condition approximately to overcome this shortcoming li et al 2000a revisited the same linearized boussinesq equation and derived an analytical solution using a coordinate transformation that enables inclusion of moving boundary effects kong et al 2011 obtained an analytical solution based on a nonlinear boussinesq equation by a perturbation method with two perturbation parameters alleviating high order term problems in the analytical solutions of nielsen 1990 and li et al 2000a in addition to solutions based on the boussinesq equation teo et al 2003 and jeng et al 2005a b derived analytical solutions based on laplace s equation for coastal unconfined aquifers with a sloping sea boundary they did not however consider the vertical flow and dynamic effective porosity that for a vertical sea boundary affect watertable fluctuations luo et al 2023 as mentioned above here based on a modified boussinesq equation that includes vertical flow and dynamic effective porosity effects luo et al 2023 an analytical solution is derived for coastal unconfined aquifers with a sloping sea boundary by a perturbation method after comparison with a well validated numerical model the new analytical solution is used to explore the effects of vertical flow and dynamic effective porosity on watertable fluctuations for coastal unconfined aquifers with a sloping sea boundary 2 theory 2 1 governing equation to account for the vertical flow and dynamic effective porosity effects watertable fluctuations in homogeneous coastal unconfined aquifers with a sloping sea boundary fig 1 can be described by a modified boussinesq equation luo et al 2023 1a n t h t k s d 2 h x 2 k s d 3 4 h x 4 with 1b n t n e 1 exp a τ ω b 1c τ ω n e h ψ k s 1 ω n e ω h ψ k s 1d h ψ 0 θ θ r θ s θ r d ψ where h l is the watertable elevation relative to the mean sea level t t is time x l is the horizontal distance from the origin nt is the dynamic effective porosity τω is a dimensionless parameter related to the watertable fluctuation frequency and soil properties a and b are fitting parameters hψ l is a measure of the equivalent saturated height of the unsaturated zone θ is the soil water content related to the capillary suction ψ l and θs and θr are the saturated and residual soil water contents respectively note that the fourth order term in the right hand side and nt in eq 1a represent vertical flow and dynamic effective porosity effects respectively various models have been proposed to describe the relation between θ and ψ among these van genuchten 1980 s model vg model is widely used and can be written as 2 θ θ s θ r s e θ r θ s θ r 1 α ψ n 1 1 n θ r where α l 1 and n are the fitting parameters related to soil properties and se is the effective saturation however substituting eq 2 into eq 1d does not lead to simple analytical expression but requires numerical treatment therefore a modified van genuchten model mvg model is adopted to describe the relation between θ and ψ troch 1993 kong et al 2016b luo et al 2019 3 θ θ s θ r s e θ r θ s θ r 1 α 1 ψ n 1 1 1 n 1 θ r where α 1 l 1 and n 1 are fitting parameters different from α and n the comparison between the vg and mvg model will be presented in the following section by comparison the difference between the mvg model and vg model is the exponent which makes the mvg model integrable to attain simple analytical expression i e substituting eq 3 into eq 1d gives luo et al 2019 4 h ψ 1 α 1 2 2 analytical derivation in contrast to aquifers with a vertical sea boundary sea level oscillations on a sloping beach generate a moving boundary condition 5 h cot β η t t η t where η m is the sea level relative to the mean sea level and β is the beach slope following li et al 2000a the moving boundary is considered by transforming the coordinate as 6 z x cot β η t for simplicity a driving head modeled as a cosine function is assumed at the sea boundary 7 h 0 t η t a cos ω t where a l is the amplitude of the driving head combining eqs 6 and 7 and substituting into eq 1a gives 8 h t k s d n t 2 h z 2 k s d 3 3 n t 4 h z 4 a ω cot β sin ω t h z the perturbation parameter used in approximating the solution to eq 8 subject to eq 7 is 9 ε a n t ω 2 k s d cot β compared with the perturbation parameter adopted in previous studies nielsen 1990 li et al 2000a nt rather than ne is involved in eq 9 therefore the modified ε is less than or equal to the original one because of a smaller nt note that ε is required to be less than unity when using the perturbation approach physically ε is the ratio of tidal excursion to the wavelength of primary wave li et al 2000a combining eqs 8 and 9 gives 10 h t k s d n t 2 h z 2 k s d 3 3 n t 4 h z 4 2 k s ω d n t ε sin ω t h z consistent with li et al 2000a we seek a solution of the form 11 h h 0 ε h 1 o ε 2 where h 0 and h 1 are the zeroth and first order solutions respectively 2 3 zeroth order solution by substituting eq 10 into eq 11 the following zeroth order perturbation equation is obtained 12 h 0 t k s d n t 2 h 0 z 2 k s d 3 3 n t 4 h 0 z 4 the corresponding boundary condition is 13 h 0 0 t a cos ω t the solution of eq 12 subject to eq 13 is nielsen et al 1997 luo et al 2023 14a h 0 a exp k 1 z cos ω t k 2 z with 14b k d k 1 k 2 i d 3 2 1 1 4 3 n t ω d k s i where k k 1 ik 2 is the watertable wave number with i 1 the real k 1 d and imaginary k 2 d parts represent the amplitude decay rate and phase lag increase rate of the zeroth order wave respectively 2 4 first order solution similarly substituting eq 10 into eq 11 gives the following first order perturbation equation 15 h 1 t k s d n t 2 h 1 z 2 k s d 3 3 n t 4 h 1 z 4 2 k s ω d n t sin ω t h 0 z the corresponding boundary condition is 16 h 1 0 t 0 since the amplitude of the first order wave is much less than that of the zeroth order wave the second term on the right hand side of eq 15 can be ignored and hence eq 15 is approximated as 17 h 1 t k s d n t 2 h 1 z 2 2 k s ω d n t sin ω t h 0 z substituting eq 14a into eq 17 yields 18 h 1 t k s d n t 2 h 1 z 2 j 1 2 m j exp v j z sin w j t p j z n j exp v j z cos w j t p j z where j 1 or 2 and expressions for mj nj vj wj and pj are given in table 1 since eq 18 is linear h 1 can be divided into 19 h 1 h 11 h 12 the solution to h 1 j j 1 or 2 is 20a h 1 j y exp v j z cos w j t p j z y exp f z cos w j t f z γ exp v j z sin w j t p j z γ exp f z sin w j t f z with 20b e k s d n t 20c y v j 2 e p j 2 e n j 2 p j v j e w j m j v j 2 e p j 2 e 2 2 p j v j e w j 2 20d γ 2 p j v j e w j n j v j 2 e p j 2 e m j v j 2 e p j 2 e 2 2 p j v j e w j 2 20e f w j 2 e substituting eqs 14 19 and 20 into eq 11 gives the analytical expression of h note that eqs 14a and 20a include the independent variable z to obtain the analytical solution in the original x coordinate one only needs to substitute eq 6 into eqs 14a and 20a respectively 3 analytical validation in contrast to various existing laboratory experiments concerning watertable fluctuations in coastal unconfined aquifers with a vertical beach e g parlange et al 1984 cartwright et al 2003 kong et al 2016b shoushtari et al 2016 experimental measurements with a sloping beach are scarce therefore we first validate the numerical model by a laboratory experiment and then compare the analytical solution with the validated numerical model 3 1 comparison between measurements and numerical simulations cartwright et al 2004 conducted laboratory experiments of watertable wave propagation in the coastal unconfined aquifer with a sloping beach of 11 7 in their experiment the sand flume had dimensions of 9 m length 0 12 m width 1 5 m height the mean sea level was set to 1 01 m from the aquifer bottom a driving head with an amplitude of 0 204 m and a period of 348 s was imposed on the sea boundary creating a moving boundary because of the sloping beach the sand used in the experiment had an average saturated hydraulic conductivity of 1 32 10 4 m s 1 and a static effective porosity of 0 3 the vg model fitting parameters were 1 7 m 1 and 9 fitting the mvg model to the vg model gives α 1 1 63 m 1 and n 1 8 2 with which the mvg model matches well with the vg model table 2 and fig 2 the saturated and residual soil water contents were 0 38 and 0 08 respectively based on the experimental setup ε was 4 44 without considering the dynamic effective porosity a smaller ε results for the dynamic effective porosity but it was still close to unity with the a and b values determined in the next section thus the new analytical solution cannot be used to predict the experiment of cartwright et al 2004 directly a 3 d variable saturation and variable density groundwater model sutra voss and provost 2010 was used to simulate the experiment of cartwright et al 2004 using the setup in fig 1 in the numerical simulations mrq was treated as a time varying boundary condition mn was the atmospheric boundary while qp and pn were no flow boundaries initially the pressure distribution was hydrostatic relative to the sea level the seepage face on mrq was implemented into the numerical model following the method of xin et al 2010 the model domain was discretized with a horizontal distance of 0 02 m 450 columns and vertical distance varying from 0 0071 to 0 015 m 100 layers case base in table 3 the numerical simulation was run to get quasi steady state with a time step of 4 s different mesh sizes and time steps were tested to ensure grid independent numerical results fig 3 compares simulated and measured watertables at different locations in general the numerical predictions are in good agreement with the measurements of cartwright et al 2004 especially during the watertable raising stage however there are deviations between predicted and measured results during the falling watertable stage suggesting the numerical model cannot capture the seepage face accurately this is likely because truncation of the unsaturated zone in the sloping beach area may have introduced errors in predicting the watertable dynamics based on the original richards equation zheng et al 2022 overall the comparison between the numerical model and experiment enhances the confidence to adopt numerical results of sutra as benchmarks 3 2 comparison between numerical simulations and analytical solutions after being validated by the experiment we used the numerical model to evaluate the new analytical solution previous studies indicated that the fluctuation period plays an important role in affecting watertable wave propagation in coastal unconfined aquifers e g li et al 2000a shoushtari et al 2016 luo et al 2023 therefore based on the experimental setup of cartwright et al 2004 four different fluctuation periods were considered 1800 7200 21 600 and 43 200 s as the fluctuation period increases the watertable wave propagates further inland to decrease possible reflection effects from the inland boundary the length of the sand flume was extended to 50 m which allows the amplitude of watertable wave at the inland boundary to be damped to within 1 of the amplitude of the driving head other parameters were the same as those in the experiment of cartwright et al 2004 for all numerical simulations the grid size was the same as the base case for the first 9 m from the sea whereas it became 0 2 m horizontal 0 015 m vertical for the rest of the model domain the time step was 4 8 15 and 30 s when the fluctuation period was 1800 7200 21 600 and 43 200 s respectively all parameters adopted in numerical simulations are summarized in table 3 before applying the new analytical solution to predict watertable fluctuations the two fitting parameters a and b must be determined due to a lack of measured data relating nt and ω a and b were determined by fitting the present analytical solution to the numerical results for period t 1800s fig 4 shows watertable predicted by the numerical simulation and the present analytical solution with a 0 0335 and b 0 7444 for this case the results of the present analytical solution are generally in good agreement with the numerical predictions at four locations nevertheless there are deviations between the analytical and numerical predictions especially during the watertable falling stage one possibility for this mismatch is that eq 1d is proposed based on one dimensional sand column experiments for which the fluctuation amplitude is constant and only vertical flow occurs this is contrary to the watertable wave propagation in coastal unconfined aquifers with both horizontal and vertical flows and amplitude decay along the inland in addition watertable wave propagation becomes more complicated because of the nonlinear interactions between the driving signal and sloping beach i e truncation of the unsaturated zone and seepage face development these nonlinear interactions are ignored when deriving the present analytical solution subsequently a 0 0335 and b 0 7444 were substituted into the new analytical solution to predict watertable fluctuations with t 7200 21 600 and 43 200 s figs 5 7 the results from the analytical solutions of nielsen 1990 and li et al 2000a are included for comparison as can be seen the new analytical solution performs well in reproducing the numerical results while the nielsen 1990 and li et al 2000a analytical solutions significantly deviate from numerical predictions for all three cases figs 5 7 this is because the analytical solutions of nielsen 1990 and li et al 2000a are derived from the original boussinesq equation when predicting watertable fluctuations from analytical solutions based on the original boussinesq equation the amplitude decay rate is equal to the phase lag increase rate for the zeroth order fluctuation this is contrary to the evidence from both field observations and laboratory experiments where the amplitude decay rate is different from the phase lag increase rate raubenheimer et al 1999 shoushtari et al 2016 luo et al 2023 moreover neglecting the dynamic effective porosity leads to larger ε which can increase errors of the analytical solution without the dynamic effective porosity ε was calculated to be 0 9764 0 5637 and 0 3986 for t 7200 21 600 and 43 200 s respectively whereas it decreased to 0 2026 0 1777 and 01 616 after considering the dynamic effective porosity note that predictions from the analytical solutions of nielsen 1990 and li et al 2000a are close to each other even though the latter captures moving boundary effects while the former does not given that aquifers are usually heterogeneous we further generated a log normal hydraulic conductivity field with a mean value of 1 32 10 4 m s fig 8 consistent with yu et al 2023 the variance and horizontal correction length of the log normal hydraulic conductivity field were 1 and 5 m respectively based on this log normal hydraulic conductivity field cases 1 4 were re simulated with other parameters kept the same as can be seen from figs 4 7 the heterogeneity has minor influence on watertable for all cases consequently the new analytical solution matches well with the numerical simulation even though the heterogeneity is considered the good performance of the new analytical solution confirms the applicability of eqs 1a and 1d even for the aquifer with a sloping sea boundary here a 0 0335 and b 0 7444 were adopted to predict watertable fluctuations for the aquifer with a sloping beach of 11 7 this differs from luo et al 2023 who found that eq 1a with a 0 0335 and b 0 4444 can predict watertable wave propagation in coastal unconfined aquifers with a vertical sea boundary this difference in the value of b occurs because the nonlinear interaction between the driving head and sloping beach affects watertable wave propagation and the water exchange between the saturated and unsaturated zones thus leading to a different dynamic effective porosity despite this with the groundwater level measured at a single location a and b values can be readily determined by fitting the present analytical solution to watertable measurements therefore the analytical solution presented here could be useful in real applications 4 hypothesized scenarios hypothesized scenarios were designed to illustrate the effects of the vertical flow and dynamic effective porosity on watertable fluctuations in coastal unconfined aquifers with a sloping sea boundary in reality tides usually have an amplitude ranging from several centimeters to a few meters and a period of several hours therefore a driving head with an amplitude of 1 m and a period of 12 h was imposed on the sea boundary the mean sea level was set to 5 m from the aquifer bottom since the response time of the unsaturated zone is related to soil properties two different soils were considered sand with a weak capillarity and loam with a strong capillarity table 2 again the mvg model matches well with the vg model for these two soils fig 2 four different beach slopes were assumed 30 45 60 and 90 based on the above parameters ε was calculated to be less than unity even with the static effective porosity and thus both the previous and present analytical solutions are applicable for all scenarios 5 results and discussion 5 1 vertical flow effects on watertable fluctuations compared with previous analytical solutions nielsen 1990 li et al 2000a the new analytical solution involves not only vertical flow but also the dynamic effective porosity therefore we first examined vertical flow effects on watertable fluctuations based on the new analytical solution by assuming nt ne fig 9 displays watertable elevation distributions calculated by different analytical solutions at five typical time slots i e t 0 t 8 t 4 3t 8 and t 2 for sand coastal unconfined aquifers with different sea boundary slopes note that only the results from the analytical solution of li et al 2000a without accounting for the vertical flow are presented for comparison since the predictions from the analytical solution of nielsen 1990 are similar to those from the analytical solution of li et al 2000a as anticipated whether considering vertical flow or not the fluctuation amplitude attenuates as the watertable waves propagate inland eventually becoming negligible at the same time watertable overheight the average inland watertable elevation during one period is larger than the mean sea level occurs fig 9 note that both analytical solutions predict a zero overheight for β 90 because a linearized governing equation describes watertable fluctuations todd and mays 2004 by comparison predictions with the vertical flow are almost identical to the predictions of the li et al 2000a approximation for all beach slopes suggesting that effects of the vertical flow on watertable elevations are negligible for sand coastal unconfined aquifers in contrast to sand aquifers watertable elevation distributions predicted by the analytical solution with the vertical flow greatly differs from that predicted by the analytical solution without vertical flow for loam coastal unconfined aquifers regardless of the beach slope fig 10 according to li et al 2000b the effects of the vertical flow depend on the value of neωd ks specifically the greater neωd ks is the more significant are the vertical flow effects for sand and loam coastal aquifers considered here neωd ks is 0 1 and 6 19 respectively confirming the observed increased importance of vertical flow for loam coastal unconfined aquifers compared with the predictions of li et al 2000a including the vertical flow leads to a smaller overheight these results imply that neglecting vertical flow effects may lead to an inappropriate estimate of watertable elevations especially for aquifers with large neωd ks to further illustrate vertical flow effects the predicted watertable elevation time series at x 5 and 50 m for both sand and loam aquifers are presented in fig 11 as expected the watertable wave propagates inland with amplitude decay and phase lag regardless of whether vertical flow is included moreover both watertable elevation and overheight decrease with increasing the beach slope this is because increasing the beach slope leads to a decrease of the moving boundary range which inhibits watertable wave propagation li et al 2000a teo et al 2003 compared with sand aquifers the fluctuation amplitude is smaller at a given location for loam aquifers whether predicted by the present analytical solution or that of li et al 2000a however the predicted overheight is greater for loam aquifers again the difference between the predictions from the previous analytical solution and the present analytical solution can be ignored at both locations for sand coastal unconfined aquifers nevertheless it becomes significant for loam coastal unconfined aquifers specifically the vertical flow has a mild impact on the fluctuation amplitude while it significantly decreases the phase lag at x 5 m consequently the watertable elevation peaks earlier with vertical flow included in other words vertical flow can accelerate watertable wave propagation as the beach slope increases the deviation between the predictions with and without accounting for vertical flow decreases this implies that increasing the beach slope weakens vertical flow effects on watertable fluctuations furthermore for loam coastal unconfined aquifers at x 50 m the difference between overheights predicted with and without including vertical flow decreases with increasing of the beach slope 5 2 dynamic effective porosity effects on watertable fluctuations we investigated dynamic effective porosity effects on watertable fluctuations based on the new analytical solution luo et al 2023 recommended a 0 0335 and b 0 4444 for coastal unconfined aquifers with a vertical sea boundary and the same values are used here fig 12 compares the watertable elevation distributions for sand coastal unconfined aquifers calculated with the new analytical solution with and without the dynamic effective porosity similarly to the case of vertical flow there is no discernible difference between the predictions with and without the dynamic effective porosity at the five times considered i e t 0 t 8 t 4 3t 8 and t 2 for all beach slopes however for loam coastal unconfined aquifers the predicted watertable elevation distributions with and without consideration of the dynamic effective porosity noticeably deviate fig 13 this emphasizes the critical role played by the dynamic effective porosity in predictions of watertable wave propagation compared with the results that ignore the dynamic effective porosity the fluctuation amplitude is larger at the same location and watertable waves propagate further inland by a factor of approximately 2 with the dynamic effective porosity included in addition including the dynamic effective porosity results in a smaller watertable overheight as mentioned earlier a zero overheight for β 90 results from using a linearized governing equation to describe watertable fluctuations the effective porosity reflects the vertical water exchange between the saturated and vadose zones during watertable fluctuations the dynamic effective porosity is usually less than the static effective porosity since the vadose zone has insufficient time to reach equilibrium a smaller effective porosity corresponds to reduced vertical water exchange between the saturated and vadose zones and hence watertable waves can propagate further landward li et al 1997 luo et al 2023 for sand coastal unconfined aquifers considered here neωhψ ks is calculated to be 0 002 which gives a dynamic effective porosity of 0 39 almost equal to the static effective porosity 0 4 however for loam coastal aquifers neωhψ ks is 0 82 and the dynamic effective porosity 0 059 is much less than the static porosity 0 23 this means that due to capillarity the vadose zone has insufficient time to respond to watertable fluctuations for loam coastal unconfined aquifers therefore the dynamic effective porosity plays a more important role in affecting watertable fluctuations for loam coastal unconfined aquifers this result is consistent with the finding of luo et al 2023 who indicated that the effects of the dynamic effective porosity depend on the value of neωhψ ks the greater neωhψ ks is the more significant dynamic effective porosity effects are similarly to further examine the effects of the dynamic effective porosity on watertable fluctuations watertable elevation time series at x 5 and 50 m are plotted for both sand and loam coastal unconfined aquifers fig 14 as can be seen the difference between the predictions from the analytical solutions with and without considering the dynamic effective porosity can be ignored for both locations of sand coastal unconfined aquifers while it is pronounced for loam coastal unconfined aquifers at x 5 m the dynamic effective porosity not only decreases the phase lag but also significantly amplifies the fluctuation amplitude for loam coastal unconfined aquifers regardless of the beach slope consequently watertable fluctuations can propagate further inland if the dynamic effective porosity is accounted for this differs from the role played by the vertical flow which mainly alters the phase lag in general the deviation between the predictions with and without consideration of the dynamic effective porosity decreases with increasing beach slope implying that a steep beach slope weakens dynamic effective porosity effects on watertable fluctuations additionally at x 50 m for the loam coastal unconfined aquifer the difference between overheights calculated with and without the dynamic effective porosity decreased with increasing beach slope 6 conclusions this study evaluated the effects of the vertical flow and dynamic effective porosity on watertable fluctuations in coastal unconfined aquifers with a sloping sea boundary using a perturbation method we derived an analytical solution to the modified boussinesq equation developed by luo et al 2023 for coastal unconfined aquifers with a sloping sea boundary after comparing with a well validated numerical model this analytical solution was used to explore the effects of the vertical flow and dynamic effective porosity on watertable fluctuations the following conclusions can be reached 1 the new analytical solution accurately predicts watertable fluctuations in coastal unconfined aquifers with a sloping sea boundary highlighting the importance of vertical flow and dynamic effective porosity effects 2 vertical flow has a minor influence on the watertable fluctuation amplitude while it significantly decreases the phase lag at a given location regardless of the beach slope compared with sand coastal unconfined aquifers vertical flow effects on watertable fluctuations are more significant for loam coastal unconfined aquifers 3 in contrast to the vertical flow considering the dynamic effective porosity not only accentuates the fluctuation amplitude but also decreases the phase lag at a given location compared with sand aquifers due to capillarity the dynamic effective porosity plays a more important role in affecting watertable fluctuations for loam coastal unconfined aquifers 4 increasing the beach slope weakens the effects of the vertical flow and dynamic effective porosity on watertable fluctuations 5 including either the vertical flow or dynamic effective porosity leads to a lower phase averaged watertable overheight moreover the difference between overheights for sand and loam aquifers calculated with and without the vertical flow dynamic effective porosity decreases with increasing the beach slope in reality coastal unconfined aquifers are expected to have complex geometries that lead to three dimensional groundwater flow in addition the sea boundary is subject to waves rather than a regular signal considered here which makes watertable fluctuations more complicated nielsen 2009 despite this the new analytical solution can be used as a simple tool to predict watertable fluctuations that are fundamental for understanding many groundwater dependent processes in coastal unconfined aquifers with sloping sea boundaries data availability the experimental data used in this study are compiled from cartwright et al 2004 the analytical solution code can be found at https doi org 10 5281 zenodo 8019310 credit authorship contribution statement zhaoyang luo conceptualization methodology visualization writing original draft writing review editing jun kong methodology supervision writing review editing d a barry methodology supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments jk acknowledges the national natural science foundation of china 51979095 the authors thank xiayang yu for generating the log normal hydraulic conductivity field 
